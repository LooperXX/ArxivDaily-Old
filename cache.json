{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.08264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.",
          "link": "http://arxiv.org/abs/2107.08264",
          "publishedOn": "2021-08-03T02:06:30.405Z",
          "wordCount": 730,
          "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1\">Brandon Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_S/0/1/0/all/0/1\">Sophie Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>",
          "description": "Citation context analysis (CCA) is an important task in natural language\nprocessing that studies how and why scholars discuss each others' work. Despite\ndecades of study, traditional frameworks for CCA have largely relied on\noverly-simplistic assumptions of how authors cite, which ignore several\nimportant phenomena. For instance, scholarly papers often contain rich\ndiscussions of cited work that span multiple sentences and express multiple\nintents concurrently. Yet, CCA is typically approached as a single-sentence,\nsingle-label classification task, and thus existing datasets fail to capture\nthis interesting discourse. In our work, we address this research gap by\nproposing a novel framework for CCA as a document-level context extraction and\nlabeling task. We release MultiCite, a new dataset of 12,653 citation contexts\nfrom over 1,200 computational linguistics papers. Not only is it the largest\ncollection of expert-annotated citation contexts to-date, MultiCite contains\nmulti-sentence, multi-label citation contexts within full paper texts. Finally,\nwe demonstrate how our dataset, while still usable for training classic CCA\nmodels, also supports the development of new types of models for CCA beyond\nfixed-width text classification. We release our code and dataset at\nhttps://github.com/allenai/multicite.",
          "link": "http://arxiv.org/abs/2107.00414",
          "publishedOn": "2021-08-03T02:06:30.299Z",
          "wordCount": 658,
          "title": "MultiCite: Modeling realistic citations requires moving beyond the single-sentence single-label setting. (arXiv:2107.00414v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1\">Devamanyu Hazarika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>",
          "description": "The majority of existing methods for empathetic response generation rely on\nthe emotion of the context to generate empathetic responses. However, empathy\nis much more than generating responses with an appropriate emotion. It also\noften entails subtle expressions of understanding and personal resonance with\nthe situation of the other interlocutor. Unfortunately, such qualities are\ndifficult to quantify and the datasets lack the relevant annotations. To\naddress this issue, in this paper we propose an approach that relies on\nexemplars to cue the generative model on fine stylistic properties that signal\nempathy to the interlocutor. To this end, we employ dense passage retrieval to\nextract relevant exemplary responses from the training set. Three elements of\nhuman communication -- emotional presence, interpretation, and exploration, and\nsentiment are additionally introduced using synthetic labels to guide the\ngeneration towards empathy. The human evaluation is also extended by these\nelements of human communication. We empirically show that these approaches\nyield significant improvements in empathetic response quality in terms of both\nautomated and human-evaluated metrics. The implementation is available at\nhttps://github.com/declare-lab/exemplary-empathy.",
          "link": "http://arxiv.org/abs/2106.11791",
          "publishedOn": "2021-08-03T02:06:30.292Z",
          "wordCount": 654,
          "title": "Exemplars-guided Empathetic Response Generation Controlled by the Elements of Human Communication. (arXiv:2106.11791v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_R/0/1/0/all/0/1\">Rachit Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiwelekar_A/0/1/0/all/0/1\">Arvind W Kiwelekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netak_L/0/1/0/all/0/1\">Laxman D Netak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodake_A/0/1/0/all/0/1\">Akshay Ghodake</a>",
          "description": "Although most logistics and freight forwarding organizations, in one way or\nanother, claim to have core values. The engagement of employees is a vast\nstructure that affects almost every part of the company's core environmental\nvalues. There is little theoretical knowledge about the relationship between\nfirms and the engagement of employees. Based on research literature, this paper\naims to provide a novel approach for insight around employee engagement in a\nlogistics organization by implementing deep natural language processing\nconcepts. The artificial intelligence-enabled solution named Intelligent Pulse\n(I-Pulse) can evaluate hundreds and thousands of pulse survey comments and\nprovides the actionable insights and gist of employee feedback. I-Pulse allows\nthe stakeholders to think in new ways in their organization, helping them to\nhave a powerful influence on employee engagement, retention, and efficiency.\nThis study is of corresponding interest to researchers and practitioners.",
          "link": "http://arxiv.org/abs/2106.07341",
          "publishedOn": "2021-08-03T02:06:30.172Z",
          "wordCount": 618,
          "title": "i-Pulse: A NLP based novel approach for employee engagement in logistics organization. (arXiv:2106.07341v1 [cs.SI] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Palliser_Sans_R/0/1/0/all/0/1\">Rafel Palliser-Sans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rial_Farras_A/0/1/0/all/0/1\">Albert Rial-Farr&#xe0;s</a>",
          "description": "This paper presents our submission to SemEval-2021 Task 5: Toxic Spans\nDetection. The purpose of this task is to detect the spans that make a text\ntoxic, which is a complex labour for several reasons. Firstly, because of the\nintrinsic subjectivity of toxicity, and secondly, due to toxicity not always\ncoming from single words like insults or offends, but sometimes from whole\nexpressions formed by words that may not be toxic individually. Following this\nidea of focusing on both single words and multi-word expressions, we study the\nimpact of using a multi-depth DistilBERT model, which uses embeddings from\ndifferent layers to estimate the final per-token toxicity. Our quantitative\nresults show that using information from multiple depths boosts the performance\nof the model. Finally, we also analyze our best model qualitatively.",
          "link": "http://arxiv.org/abs/2104.00639",
          "publishedOn": "2021-08-03T02:06:30.151Z",
          "wordCount": 617,
          "title": "HLE-UPC at SemEval-2021 Task 5: Multi-Depth DistilBERT for Toxic Spans Detection. (arXiv:2104.00639v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parry_H/0/1/0/all/0/1\">Hishan Parry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xun_L/0/1/0/all/0/1\">Lei Xun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_A/0/1/0/all/0/1\">Amin Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jia Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrett_G/0/1/0/all/0/1\">Geoff V. Merrett</a>",
          "description": "The Transformer architecture is widely used for machine translation tasks.\nHowever, its resource-intensive nature makes it challenging to implement on\nconstrained embedded devices, particularly where available hardware resources\ncan vary at run-time. We propose a dynamic machine translation model that\nscales the Transformer architecture based on the available resources at any\nparticular time. The proposed approach, 'Dynamic-HAT', uses a HAT\nSuperTransformer as the backbone to search for SubTransformers with different\naccuracy-latency trade-offs at design time. The optimal SubTransformers are\nsampled from the SuperTransformer at run-time, depending on latency\nconstraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses\ninherited SubTransformers sampled directly from the SuperTransformer with a\nswitching time of <1s. Using inherited SubTransformers results in a BLEU score\nloss of <1.5% because the SubTransformer configuration is not retrained from\nscratch after sampling. However, to recover this loss in performance, the\ndimensions of the design space can be reduced to tailor it to a family of\ntarget hardware. The new reduced design space results in a BLEU score increase\nof approximately 1% for sub-optimal models from the original design space, with\na wide range for performance scaling between 0.356s - 1.526s for the GPU and\n2.9s - 7.31s for the CPU.",
          "link": "http://arxiv.org/abs/2107.08199",
          "publishedOn": "2021-08-03T02:06:30.134Z",
          "wordCount": 680,
          "title": "Dynamic Transformer for Efficient Machine Translation on Embedded Devices. (arXiv:2107.08199v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kaiyu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayazit_D/0/1/0/all/0/1\">Deniz Bayazit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_R/0/1/0/all/0/1\">Rebecca Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1\">Stefanie Tellex</a>",
          "description": "Humans use spatial language to naturally describe object locations and their\nrelations. Interpreting spatial language not only adds a perceptual modality\nfor robots, but also reduces the barrier of interfacing with humans. Previous\nwork primarily considers spatial language as goal specification for instruction\nfollowing tasks in fully observable domains, often paired with reference paths\nfor reward-based learning. However, spatial language is inherently subjective\nand potentially ambiguous or misleading. Hence, in this paper, we consider\nspatial language as a form of stochastic observation. We propose SLOOP (Spatial\nLanguage Object-Oriented POMDP), a new framework for partially observable\ndecision making with a probabilistic observation model for spatial language. We\napply SLOOP to object search in city-scale environments. To interpret\nambiguous, context-dependent prepositions (e.g. front), we design a simple\nconvolutional neural network that predicts the language provider's latent frame\nof reference (FoR) given the environment context. Search strategies are\ncomputed via an online POMDP planner based on Monte Carlo Tree Search.\nEvaluation based on crowdsourced language data, collected over areas of five\ncities in OpenStreetMap, shows that our approach achieves faster search and\nhigher success rate compared to baselines, with a wider margin as the spatial\nlanguage becomes more complex. Finally, we demonstrate the proposed method in\nAirSim, a realistic simulator where a drone is tasked to find cars in a\nneighborhood environment.",
          "link": "http://arxiv.org/abs/2012.02705",
          "publishedOn": "2021-08-03T02:06:30.126Z",
          "wordCount": 717,
          "title": "Spatial Language Understanding for Object Search in Partially Observed City-scale Environments. (arXiv:2012.02705v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balali_A/0/1/0/all/0/1\">Ali Balali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadpour_M/0/1/0/all/0/1\">Masoud Asadpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafari_S/0/1/0/all/0/1\">Seyed Hossein Jafari</a>",
          "description": "Data is published on the web over time in great volumes, but majority of the\ndata is unstructured, making it hard to understand and difficult to interpret.\nInformation Extraction (IE) methods extract structured information from\nunstructured data. One of the challenging IE tasks is Event Extraction (EE)\nwhich seeks to derive information about specific incidents and their actors\nfrom the text. EE is useful in many domains such as building a knowledge base,\ninformation retrieval, summarization and online monitoring systems. In the past\ndecades, some event ontologies like ACE, CAMEO and ICEWS were developed to\ndefine event forms, actors and dimensions of events observed in the text. These\nevent ontologies still have some shortcomings such as covering only a few\ntopics like political events, having inflexible structure in defining argument\nroles, lack of analytical dimensions, and complexity in choosing event\nsub-types. To address these concerns, we propose an event ontology, namely\nCOfEE, that incorporates both expert domain knowledge, previous ontologies and\na data-driven approach for identifying events from text. COfEE consists of two\nhierarchy levels (event types and event sub-types) that include new categories\nrelating to environmental issues, cyberspace, criminal activity and natural\ndisasters which need to be monitored instantly. Also, dynamic roles according\nto each event sub-type are defined to capture various dimensions of events. In\na follow-up experiment, the proposed ontology is evaluated on Wikipedia events,\nand it is shown to be general and comprehensive. Moreover, in order to\nfacilitate the preparation of gold-standard data for event extraction, a\nlanguage-independent online tool is presented based on COfEE.",
          "link": "http://arxiv.org/abs/2107.10326",
          "publishedOn": "2021-08-03T02:06:30.107Z",
          "wordCount": 733,
          "title": "COfEE: A Comprehensive Ontology for Event Extraction from text, with an online annotation tool. (arXiv:2107.10326v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>",
          "description": "Weak supervision has shown promising results in many natural language\nprocessing tasks, such as Named Entity Recognition (NER). Existing work mainly\nfocuses on learning deep NER models only with weak supervision, i.e., without\nany human annotation, and shows that by merely using weakly labeled data, one\ncan achieve good performance, though still underperforms fully supervised NER\nwith manually/strongly labeled data. In this paper, we consider a more\npractical scenario, where we have both a small amount of strongly labeled data\nand a large amount of weakly labeled data. Unfortunately, we observe that\nweakly labeled data does not necessarily improve, or even deteriorate the model\nperformance (due to the extensive noise in the weak labels) when we train deep\nNER models over a simple or weighted combination of the strongly labeled and\nweakly labeled data. To address this issue, we propose a new multi-stage\ncomputational framework -- NEEDLE with three essential ingredients: (1) weak\nlabel completion, (2) noise-aware loss function, and (3) final fine-tuning over\nthe strongly labeled data. Through experiments on E-commerce query NER and\nBiomedical NER, we demonstrate that NEEDLE can effectively suppress the noise\nof the weak labels and outperforms existing methods. In particular, we achieve\nnew SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74,\nBC5CDR-disease 90.69, NCBI-disease 92.28.",
          "link": "http://arxiv.org/abs/2106.08977",
          "publishedOn": "2021-08-03T02:06:30.097Z",
          "wordCount": 705,
          "title": "Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data. (arXiv:2106.08977v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1\">Henning Wachsmuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>",
          "description": "Despite extensive research efforts in the recent years, computational\nmodeling of argumentation remains one of the most challenging areas of natural\nlanguage processing (NLP). This is primarily due to inherent complexity of the\ncognitive processes behind human argumentation, which commonly combine and\nintegrate plethora of different types of knowledge, requiring from\ncomputational models capabilities that are far beyond what is needed for most\nother (i.e., simpler) natural language understanding tasks. The existing large\nbody of work on mining, assessing, generating, and reasoning over arguments\nlargely acknowledges that much more common sense and world knowledge needs to\nbe integrated into computational models that would accurately model\nargumentation. A systematic overview and organization of the types of knowledge\nintroduced in existing models of computational argumentation (CA) is, however,\nmissing and this hinders targeted progress in the field. In this survey paper,\nwe fill this gap by (1) proposing a pyramid of types of knowledge required in\nCA tasks, (2) analysing the state of the art with respect to the reliance and\nexploitation of these types of knowledge, for each of the for main research\nareas in CA, and (3) outlining and discussing directions for future research\nefforts in CA.",
          "link": "http://arxiv.org/abs/2107.00281",
          "publishedOn": "2021-08-03T02:06:30.076Z",
          "wordCount": 665,
          "title": "Scientia Potentia Est -- On the Role of Knowledge in Computational Argumentation. (arXiv:2107.00281v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Weidong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mingjun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lusheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jinwen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jianbo Tang</a>",
          "description": "Language model pre-training based on large corpora has achieved tremendous\nsuccess in terms of constructing enriched contextual representations and has\nled to significant performance gains on a diverse range of Natural Language\nUnderstanding (NLU) tasks. Despite the success, most current pre-trained\nlanguage models, such as BERT, are trained based on single-grained\ntokenization, usually with fine-grained characters or sub-words, making it hard\nfor them to learn the precise meaning of coarse-grained words and phrases. In\nthis paper, we propose a simple yet effective pre-training method named LICHEE\nto efficiently incorporate multi-grained information of input text. Our method\ncan be applied to various pre-trained language models and improve their\nrepresentation capability. Extensive experiments conducted on CLUE and\nSuperGLUE demonstrate that our method achieves comprehensive improvements on a\nwide variety of NLU tasks in both Chinese and English with little extra\ninference cost incurred, and that our best ensemble model achieves the\nstate-of-the-art performance on CLUE benchmark competition.",
          "link": "http://arxiv.org/abs/2108.00801",
          "publishedOn": "2021-08-03T02:06:30.069Z",
          "wordCount": 604,
          "title": "LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization. (arXiv:2108.00801v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>",
          "description": "Pre-trained language models (PrLM) has been shown powerful in enhancing a\nbroad range of downstream tasks including various dialogue related ones.\nHowever, PrLMs are usually trained on general plain text with common language\nmodel (LM) training objectives, which cannot sufficiently capture dialogue\nexclusive features due to the limitation of such training setting, so that\nthere is an immediate need to fill the gap between a specific dialogue task and\nthe LM task. As it is unlikely to collect huge dialogue data for\ndialogue-oriented pre-training, in this paper, we propose three strategies to\nsimulate the conversation features on general plain text. Our proposed method\ndiffers from existing post-training methods that it may yield a general-purpose\nPrLM and does not individualize to any detailed task while keeping the\ncapability of learning dialogue related features including speaker awareness,\ncontinuity and consistency. The resulted Dialog-PrLM is fine-tuned on three\npublic multi-turn dialogue datasets and helps achieve significant and\nconsistent improvement over the plain PrLMs.",
          "link": "http://arxiv.org/abs/2106.00420",
          "publishedOn": "2021-08-03T02:06:30.051Z",
          "wordCount": 603,
          "title": "Dialogue-oriented Pre-training. (arXiv:2106.00420v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tianming Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1\">Gaurav Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Maozu Guo</a>",
          "description": "Label noise and long-tailed distributions are two major challenges in\ndistantly supervised relation extraction. Recent studies have shown great\nprogress on denoising, but pay little attention to the problem of long-tailed\nrelations. In this paper, we introduce constraint graphs to model the\ndependencies between relation labels. On top of that, we further propose a\nnovel constraint graph-based relation extraction framework(CGRE) to handle the\ntwo challenges simultaneously. CGRE employs graph convolution networks (GCNs)\nto propagate information from data-rich relation nodes to data-poor relation\nnodes, and thus boosts the representation learning of long-tailed relations. To\nfurther improve the noise immunity, a constraint-aware attention module is\ndesigned in CGRE to integrate the constraint information. Experimental results\non a widely-used benchmark dataset indicate that our approach achieves\nsignificant improvements over the previous methods for both denoising and\nlong-tailed relation extraction. Our dataset and codes are available at\nhttps://github.com/tmliang/CGRE.",
          "link": "http://arxiv.org/abs/2105.11225",
          "publishedOn": "2021-08-03T02:06:30.021Z",
          "wordCount": 615,
          "title": "Distantly-Supervised Long-Tailed Relation Extraction Using Constraint Graphs. (arXiv:2105.11225v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Amish Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1\">Sourav Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datar_A/0/1/0/all/0/1\">Arnhav Datar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadiwala_J/0/1/0/all/0/1\">Juned Kadiwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1\">Hrithwik Shalu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1\">Jimson Mathew</a>",
          "description": "Reliable detection of the prodromal stages of Alzheimer's disease (AD)\nremains difficult even today because, unlike other neurocognitive impairments,\nthere is no definitive diagnosis of AD in vivo. In this context, existing\nresearch has shown that patients often develop language impairment even in mild\nAD conditions. We propose a multimodal deep learning method that utilizes\nspeech and the corresponding transcript simultaneously to detect AD. For audio\nsignals, the proposed audio-based network, a convolutional neural network (CNN)\nbased model, predicts the diagnosis for multiple speech segments, which are\ncombined for the final prediction. Similarly, we use contextual embedding\nextracted from BERT concatenated with a CNN-generated embedding for classifying\nthe transcript. The individual predictions of the two models are then combined\nto make the final classification. We also perform experiments to analyze the\nmodel performance when Automated Speech Recognition (ASR) system generated\ntranscripts are used instead of manual transcription in the text-based model.\nThe proposed method achieves 85.3% 10-fold cross-validation accuracy when\ntrained and evaluated on the Dementiabank Pitt corpus.",
          "link": "http://arxiv.org/abs/2012.00096",
          "publishedOn": "2021-08-03T02:06:30.014Z",
          "wordCount": 651,
          "title": "Multi-Modal Detection of Alzheimer's Disease from Speech and Text. (arXiv:2012.00096v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vassilev_A/0/1/0/all/0/1\">Apostol Vassilev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Munawar Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Honglan Jin</a>",
          "description": "When people try to understand nuanced language they typically process\nmultiple input sensor modalities to complete this cognitive task. It turns out\nthe human brain has even a specialized neuron formation, called sagittal\nstratum, to help us understand sarcasm. We use this biological formation as the\ninspiration for designing a neural network architecture that combines\npredictions of different models on the same text to construct robust, accurate\nand computationally efficient classifiers for sentiment analysis and study\nseveral different realizations. Among them, we propose a systematic new\napproach to combining multiple predictions based on a dedicated neural network\nand develop mathematical analysis of it along with state-of-the-art\nexperimental results. We also propose a heuristic-hybrid technique for\ncombining models and back it up with experimental results on a representative\nbenchmark dataset and comparisons to other methods to show the advantages of\nthe new approaches.",
          "link": "http://arxiv.org/abs/2006.12958",
          "publishedOn": "2021-08-03T02:06:30.007Z",
          "wordCount": 640,
          "title": "Can you tell? SSNet -- a Sagittal Stratum-inspired Neural Network Framework for Sentiment Analysis. (arXiv:2006.12958v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>",
          "description": "Natural language to SQL (NL2SQL) aims to parse a natural language with a\ngiven database into a SQL query, which widely appears in practical Internet\napplications. Jointly encode database schema and question utterance is a\ndifficult but important task in NL2SQL. One solution is to treat the input as a\nheterogeneous graph. However, it failed to learn good word representation in\nquestion utterance. Learning better word representation is important for\nconstructing a well-designed NL2SQL system. To solve the challenging task, we\npresent a Relation aware Semi-autogressive Semantic Parsing (\\MODN) ~framework,\nwhich is more adaptable for NL2SQL. It first learns relation embedding over the\nschema entities and question words with predefined schema relations with\nELECTRA and relation aware transformer layer as backbone. Then we decode the\nquery SQL with a semi-autoregressive parser and predefined SQL syntax. From\nempirical results and case study, our model shows its effectiveness in learning\nbetter word representation in NL2SQL.",
          "link": "http://arxiv.org/abs/2108.00804",
          "publishedOn": "2021-08-03T02:06:29.997Z",
          "wordCount": 588,
          "title": "Relation Aware Semi-autoregressive Semantic Parsing for NL2SQL. (arXiv:2108.00804v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stergiadis_E/0/1/0/all/0/1\">Emmanouil Stergiadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Satendra Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovalev_F/0/1/0/all/0/1\">Fedor Kovalev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levin_P/0/1/0/all/0/1\">Pavel Levin</a>",
          "description": "While NMT has achieved remarkable results in the last 5 years, production\nsystems come with strict quality requirements in arbitrarily niche domains that\nare not always adequately covered by readily available parallel corpora. This\nis typically addressed by training domain specific models, using fine-tuning\nmethods and some variation of back-translation on top of in-domain monolingual\ncorpora. However, industrial practitioners can rarely afford to focus on a\nsingle domain. A far more typical scenario includes a set of closely related,\nyet succinctly different sub-domains. At Booking.com, we need to translate\nproperty descriptions, user reviews, as well as messages, (for example those\nsent between a customer and an agent or property manager). An editor might need\nto translate articles across a set of different topics. An e-commerce platform\nwould typically need to translate both the description of each item and the\nuser generated content related to them. To this end, we propose MDT: a novel\nmethod to simultaneously fine-tune on several sub-domains by passing\nmultidimensional sentence-level information to the model during training and\ninference. We show that MDT achieves results competitive to N specialist models\neach fine-tuned on a single constituent domain, while effectively serving all N\nsub-domains, therefore cutting development and maintenance costs by the same\nfactor. Besides BLEU (industry standard automatic evaluation metric known to\nonly weakly correlate with human judgement) we also report rigorous human\nevaluation results for all models and sub-domains as well as specific examples\nthat better contextualise the performance of each model in terms of adequacy\nand fluency. To facilitate further research, we plan to make the code available\nupon acceptance.",
          "link": "http://arxiv.org/abs/2102.10160",
          "publishedOn": "2021-08-03T02:06:29.968Z",
          "wordCount": 729,
          "title": "Multi-Domain Adaptation in Neural Machine Translation Through Multidimensional Tagging. (arXiv:2102.10160v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1\">Omar Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>",
          "description": "Systems for Open-Domain Question Answering (OpenQA) generally depend on a\nretriever for finding candidate passages in a large corpus and a reader for\nextracting answers from those passages. In much recent work, the retriever is a\nlearned component that uses coarse-grained vector representations of questions\nand passages. We argue that this modeling choice is insufficiently expressive\nfor dealing with the complexity of natural language questions. To address this,\nwe define ColBERT-QA, which adapts the scalable neural retrieval model ColBERT\nto OpenQA. ColBERT creates fine-grained interactions between questions and\npassages. We propose an efficient weak supervision strategy that iteratively\nuses ColBERT to create its own training data. This greatly improves OpenQA\nretrieval on Natural Questions, SQuAD, and TriviaQA, and the resulting system\nattains state-of-the-art extractive OpenQA performance on all three datasets.",
          "link": "http://arxiv.org/abs/2007.00814",
          "publishedOn": "2021-08-03T02:06:29.947Z",
          "wordCount": 609,
          "title": "Relevance-guided Supervision for OpenQA with ColBERT. (arXiv:2007.00814v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grundmann_P/0/1/0/all/0/1\">Paul Grundmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_S/0/1/0/all/0/1\">Sebastian Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loser_A/0/1/0/all/0/1\">Alexander L&#xf6;ser</a>",
          "description": "Retrieving answer passages from long documents is a complex task requiring\nsemantic understanding of both discourse and document context. We approach this\nchallenge specifically in a clinical scenario, where doctors retrieve cohorts\nof patients based on diagnoses and other latent medical aspects. We introduce\nCAPR, a rule-based self-supervision objective for training Transformer language\nmodels for domain-specific passage matching. In addition, we contribute a novel\nretrieval dataset based on clinical notes to simulate this scenario on a large\ncorpus of clinical notes. We apply our objective in four Transformer-based\narchitectures: Contextual Document Vectors, Bi-, Poly- and Cross-encoders. From\nour extensive evaluation on MIMIC-III and three other healthcare datasets, we\nreport that CAPR outperforms strong baselines in the retrieval of\ndomain-specific passages and effectively generalizes across rule-based and\nhuman-labeled passages. This makes the model powerful especially in zero-shot\nscenarios where only limited training data is available.",
          "link": "http://arxiv.org/abs/2108.00775",
          "publishedOn": "2021-08-03T02:06:29.941Z",
          "wordCount": 572,
          "title": "Self-supervised Answer Retrieval on Clinical Notes. (arXiv:2108.00775v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2001.01037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiamei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1\">Sebastian Lapuschkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1\">Wojciech Samek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1\">Alexander Binder</a>",
          "description": "This paper analyzes the predictions of image captioning models with attention\nmechanisms beyond visualizing the attention itself. We develop variants of\nlayer-wise relevance propagation (LRP) and gradient-based explanation methods,\ntailored to image captioning models with attention mechanisms. We compare the\ninterpretability of attention heatmaps systematically against the explanations\nprovided by explanation methods such as LRP, Grad-CAM, and Guided Grad-CAM. We\nshow that explanation methods provide simultaneously pixel-wise image\nexplanations (supporting and opposing pixels of the input image) and linguistic\nexplanations (supporting and opposing words of the preceding sequence) for each\nword in the predicted captions. We demonstrate with extensive experiments that\nexplanation methods 1) can reveal additional evidence used by the model to make\ndecisions compared to attention; 2) correlate to object locations with high\nprecision; 3) are helpful to \"debug\" the model, e.g. by analyzing the reasons\nfor hallucinated object words. With the observed properties of explanations, we\nfurther design an LRP-inference fine-tuning strategy that reduces the issue of\nobject hallucination in image captioning models, and meanwhile, maintains the\nsentence fluency. We conduct experiments with two widely used attention\nmechanisms: the adaptive attention mechanism calculated with the additive\nattention and the multi-head attention mechanism calculated with the scaled dot\nproduct.",
          "link": "http://arxiv.org/abs/2001.01037",
          "publishedOn": "2021-08-03T02:06:29.934Z",
          "wordCount": 703,
          "title": "Explain and Improve: LRP-Inference Fine-Tuning for Image Captioning Models. (arXiv:2001.01037v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Nupur Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_A/0/1/0/all/0/1\">Anshul Rai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Gaurav Kumar</a>",
          "description": "To truly grasp reasoning ability, a Natural Language Inference model should\nbe evaluated on counterfactual data. TabPert facilitates this by assisting in\nthe generation of such counterfactual data for assessing model tabular\nreasoning issues. TabPert allows a user to update a table, change its\nassociated hypotheses, change their labels, and highlight rows that are\nimportant for hypothesis classification. TabPert also captures information\nabout the techniques used to automatically produce the table, as well as the\nstrategies employed to generate the challenging hypotheses. These\ncounterfactual tables and hypotheses, as well as the metadata, can then be used\nto explore an existing model's shortcomings methodically and quantitatively.",
          "link": "http://arxiv.org/abs/2108.00603",
          "publishedOn": "2021-08-03T02:06:29.926Z",
          "wordCount": 553,
          "title": "TabPert: An Effective Platform for Tabular Perturbation. (arXiv:2108.00603v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhongkun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>",
          "description": "Complex reasoning aims to draw a correct inference based on complex rules. As\na hallmark of human intelligence, it involves a degree of explicit reading\ncomprehension, interpretation of logical knowledge and complex rule\napplication. In this paper, we take a step forward in complex reasoning by\nsystematically studying the three challenging and domain-general tasks of the\nLaw School Admission Test (LSAT), including analytical reasoning, logical\nreasoning and reading comprehension. We propose a hybrid reasoning system to\nintegrate these three tasks and achieve impressive overall performance on the\nLSAT tests. The experimental results demonstrate that our system endows itself\na certain complex reasoning ability, especially the fundamental reading\ncomprehension and challenging logical reasoning capacities. Further analysis\nalso shows the effectiveness of combining the pre-trained models with the\ntask-specific reasoning module, and integrating symbolic knowledge into\ndiscrete interpretable reasoning steps in complex reasoning. We further shed a\nlight on the potential future directions, like unsupervised symbolic knowledge\nextraction, model interpretability, few-shot learning and comprehensive\nbenchmark for complex reasoning.",
          "link": "http://arxiv.org/abs/2108.00648",
          "publishedOn": "2021-08-03T02:06:29.896Z",
          "wordCount": 626,
          "title": "From LSAT: The Progress and Challenges of Complex Reasoning. (arXiv:2108.00648v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kellert_O/0/1/0/all/0/1\">Olga Kellert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matlis_N/0/1/0/all/0/1\">Nicholas H. Matlis</a>",
          "description": "The explosion in the availability of natural language data in the era of\nsocial media has given rise to a host of applications such as sentiment\nanalysis and opinion mining. Simultaneously, the growing availability of\nprecise geolocation information is enabling visualization of global phenomena\nsuch as environmental changes and disease propagation. Opportunities for\ntracking spatial variations in language use, however, have largely been\noverlooked, especially on small spatial scales. Here we explore the use of\nTwitter data with precise geolocation information to resolve spatial variations\nin language use on an urban scale down to single city blocks. We identify\nseveral categories of language tokens likely to show distinctive patterns of\nuse and develop quantitative methods to visualize the spatial distributions\nassociated with these patterns. Our analysis concentrates on comparison of\ncontrasting pairs of Tweet distributions from the same category, each defined\nby a set of tokens. Our work shows that analysis of small-scale variations can\nprovide unique information on correlations between language use and social\ncontext which are highly valuable to a wide range of fields from linguistic\nscience and commercial advertising to social services.",
          "link": "http://arxiv.org/abs/2108.00533",
          "publishedOn": "2021-08-03T02:06:29.874Z",
          "wordCount": 618,
          "title": "Geolocation differences of language use in urban areas. (arXiv:2108.00533v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongkyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhiliang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Lanqing Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nevin L. Zhang</a>",
          "description": "Text style transfer aims to alter the style (e.g., sentiment) of a sentence\nwhile preserving its content. A common approach is to map a given sentence to\ncontent representation that is free of style, and the content representation is\nfed to a decoder with a target style. Previous methods in filtering style\ncompletely remove tokens with style at the token level, which incurs the loss\nof content information. In this paper, we propose to enhance content\npreservation by implicitly removing the style information of each token with\nreverse attention, and thereby retain the content. Furthermore, we fuse content\ninformation when building the target style representation, making it dynamic\nwith respect to the content. Our method creates not only style-independent\ncontent representation, but also content-dependent style representation in\ntransferring style. Empirical results show that our method outperforms the\nstate-of-the-art baselines by a large margin in terms of content preservation.\nIn addition, it is also competitive in terms of style transfer accuracy and\nfluency.",
          "link": "http://arxiv.org/abs/2108.00449",
          "publishedOn": "2021-08-03T02:06:29.866Z",
          "wordCount": 615,
          "title": "Enhancing Content Preservation in Text Style Transfer Using Reverse Attention and Conditional Layer Normalization. (arXiv:2108.00449v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1\">Mithun Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1\">Punyajoy Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutt_R/0/1/0/all/0/1\">Ritam Dutt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_B/0/1/0/all/0/1\">Binny Mathew</a>",
          "description": "Hate speech is regarded as one of the crucial issues plaguing the online\nsocial media. The current literature on hate speech detection leverages\nprimarily the textual content to find hateful posts and subsequently identify\nhateful users. However, this methodology disregards the social connections\nbetween users. In this paper, we run a detailed exploration of the problem\nspace and investigate an array of models ranging from purely textual to graph\nbased to finally semi-supervised techniques using Graph Neural Networks (GNN)\nthat utilize both textual and graph-based features. We run exhaustive\nexperiments on two datasets -- Gab, which is loosely moderated and Twitter,\nwhich is strictly moderated. Overall the AGNN model achieves 0.791 macro\nF1-score on the Gab dataset and 0.780 macro F1-score on the Twitter dataset\nusing only 5% of the labeled instances, considerably outperforming all the\nother models including the fully supervised ones. We perform detailed error\nanalysis on the best performing text and graph based models and observe that\nhateful users have unique network neighborhood signatures and the AGNN model\nbenefits by paying attention to these signatures. This property, as we observe,\nalso allows the model to generalize well across platforms in a zero-shot\nsetting. Lastly, we utilize the best performing GNN model to analyze the\nevolution of hateful users and their targets over time in Gab.",
          "link": "http://arxiv.org/abs/2108.00524",
          "publishedOn": "2021-08-03T02:06:29.855Z",
          "wordCount": 694,
          "title": "You too Brutus! Trapping Hateful Users in Social Media: Challenges, Solutions & Insights. (arXiv:2108.00524v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henao_P/0/1/0/all/0/1\">Pablo Restrepo Henao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischbach_J/0/1/0/all/0/1\">Jannik Fischbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spies_D/0/1/0/all/0/1\">Dominik Spies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frattini_J/0/1/0/all/0/1\">Julian Frattini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelsang_A/0/1/0/all/0/1\">Andreas Vogelsang</a>",
          "description": "Identifying feature requests and bug reports in user comments holds great\npotential for development teams. However, automated mining of RE-related\ninformation from social media and app stores is challenging since (1) about 70%\nof user comments contain noisy, irrelevant information, (2) the amount of user\ncomments grows daily making manual analysis unfeasible, and (3) user comments\nare written in different languages. Existing approaches build on traditional\nmachine learning (ML) and deep learning (DL), but fail to detect feature\nrequests and bug reports with high Recall and acceptable Precision which is\nnecessary for this task. In this paper, we investigate the potential of\ntransfer learning (TL) for the classification of user comments. Specifically,\nwe train both monolingual and multilingual BERT models and compare the\nperformance with state-of-the-art methods. We found that monolingual BERT\nmodels outperform existing baseline methods in the classification of English\nApp Reviews as well as English and Italian Tweets. However, we also observed\nthat the application of heavyweight TL models does not necessarily lead to\nbetter performance. In fact, our multilingual BERT models perform worse than\ntraditional ML methods.",
          "link": "http://arxiv.org/abs/2108.00663",
          "publishedOn": "2021-08-03T02:06:29.849Z",
          "wordCount": 634,
          "title": "Transfer Learning for Mining Feature Requests and Bug Reports from Tweets and App Store Reviews. (arXiv:2108.00663v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1\">Yuval Pinter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stent_A/0/1/0/all/0/1\">Amanda Stent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1\">Mark Dredze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>",
          "description": "Commonly-used transformer language models depend on a tokenization schema\nwhich sets an unchangeable subword vocabulary prior to pre-training, destined\nto be applied to all downstream tasks regardless of domain shift, novel word\nformations, or other sources of vocabulary mismatch. Recent work has shown that\n\"token-free\" models can be trained directly on characters or bytes, but\ntraining these models from scratch requires substantial computational\nresources, and this implies discarding the many domain-specific models that\nwere trained on tokens. In this paper, we present XRayEmb, a method for\nretrofitting existing token-based models with character-level information.\nXRayEmb is composed of a character-level \"encoder\" that computes vector\nrepresentations of character sequences, and a generative component that decodes\nfrom the internal representation to a character sequence. We show that\nincorporating XRayEmb's learned vectors into sequences of pre-trained token\nembeddings helps performance on both autoregressive and masked pre-trained\ntransformer architectures and on both sequence-level and sequence tagging\ntasks, particularly on non-standard English text.",
          "link": "http://arxiv.org/abs/2108.00391",
          "publishedOn": "2021-08-03T02:06:29.842Z",
          "wordCount": 592,
          "title": "Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information. (arXiv:2108.00391v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1\">Harsh Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>",
          "description": "To build challenging multi-hop question answering datasets, we propose a\nbottom-up semi-automatic process of constructing multi-hop question via\ncomposition of single-hop questions. Constructing multi-hop questions as\ncomposition of single-hop questions allows us to exercise greater control over\nthe quality of the resulting multi-hop questions. This process allows building\na dataset with (i) connected reasoning where each step needs the answer from a\nprevious step; (ii) minimal train-test leakage by eliminating even partial\noverlap of reasoning steps; (iii) variable number of hops and composition\nstructures; and (iv) contrasting unanswerable questions by modifying the\ncontext. We use this process to construct a new multihop QA dataset:\nMuSiQue-Ans with ~25K 2-4 hop questions using seed questions from 5 existing\nsingle-hop datasets. Our experiments demonstrate that MuSique is challenging\nfor state-of-the-art QA models (e.g., human-machine gap of $~$30 F1 pts),\nsignificantly harder than existing datasets (2x human-machine gap), and\nsubstantially less cheatable (e.g., a single-hop model is worse by 30 F1 pts).\nWe also build an even more challenging dataset, MuSiQue-Full, consisting of\nanswerable and unanswerable contrast question pairs, where model performance\ndrops further by 13+ F1 pts. For data and code, see\n\\url{https://github.com/stonybrooknlp/musique}.",
          "link": "http://arxiv.org/abs/2108.00573",
          "publishedOn": "2021-08-03T02:06:29.835Z",
          "wordCount": 626,
          "title": "MuSiQue: Multi-hop Questions via Single-hop Question Composition. (arXiv:2108.00573v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1\">Chang Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiangyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>",
          "description": "Text generation from semantic parses is to generate textual descriptions for\nformal representation inputs such as logic forms and SQL queries. This is\nchallenging due to two reasons: (1) the complex and intensive inner logic with\nthe data scarcity constraint, (2) the lack of automatic evaluation metrics for\nlogic consistency. To address these two challenges, this paper first proposes\nSNOWBALL, a framework for logic consistent text generation from semantic parses\nthat employs an iterative training procedure by recursively augmenting the\ntraining set with quality control. Second, we propose a novel automatic metric,\nBLEC, for evaluating the logical consistency between the semantic parses and\ngenerated texts. The experimental results on two benchmark datasets, Logic2Text\nand Spider, demonstrate the SNOWBALL framework enhances the logic consistency\non both BLEC and human evaluation. Furthermore, our statistical analysis\nreveals that BLEC is more logically consistent with human evaluation than\ngeneral-purpose automatic metrics including BLEU, ROUGE and, BLEURT. Our data\nand code are available at https://github.com/Ciaranshu/relogic.",
          "link": "http://arxiv.org/abs/2108.00577",
          "publishedOn": "2021-08-03T02:06:29.733Z",
          "wordCount": 597,
          "title": "Logic-Consistency Text Generation from Semantic Parses. (arXiv:2108.00577v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1\">Riyaz A. Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_A/0/1/0/all/0/1\">Atreya Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1\">Manish Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Maneesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>",
          "description": "While neural models routinely report state-of-the-art performance across NLP\ntasks involving reasoning, their outputs are often observed to not properly use\nand reason on the evidence presented to them in the inputs. A model that\nreasons properly is expected to attend to the right parts of the input, be\nself-consistent in its predictions across examples, avoid spurious patterns in\ninputs, and to ignore biasing from its underlying pre-trained language model in\na nuanced, context-sensitive fashion (e.g. handling counterfactuals). Do\ntoday's models do so? In this paper, we study this question using the problem\nof reasoning on tabular data. The tabular nature of the input is particularly\nsuited for the study as it admits systematic probes targeting the properties\nlisted above. Our experiments demonstrate that a BERT-based model\nrepresentative of today's state-of-the-art fails to properly reason on the\nfollowing counts: it often (a) misses the relevant evidence, (b) suffers from\nhypothesis and knowledge biases, and, (c) relies on annotation artifacts and\nknowledge from pre-trained language models as primary evidence rather than\nrelying on reasoning on the premises in the tabular input.",
          "link": "http://arxiv.org/abs/2108.00578",
          "publishedOn": "2021-08-03T02:06:29.726Z",
          "wordCount": 639,
          "title": "Is My Model Using The Right Evidence? Systematic Probes for Examining Evidence-Based Tabular Reasoning. (arXiv:2108.00578v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Anshul Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahiya_K/0/1/0/all/0/1\">Kunal Dahiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sheshansh Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saini_D/0/1/0/all/0/1\">Deepak Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sumeet Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1\">Purushottam Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1\">Manik Varma</a>",
          "description": "Extreme multi-label classification (XML) involves tagging a data point with\nits most relevant subset of labels from an extremely large label set, with\nseveral applications such as product-to-product recommendation with millions of\nproducts. Although leading XML algorithms scale to millions of labels, they\nlargely ignore label meta-data such as textual descriptions of the labels. On\nthe other hand, classical techniques that can utilize label metadata via\nrepresentation learning using deep networks struggle in extreme settings. This\npaper develops the DECAF algorithm that addresses these challenges by learning\nmodels enriched by label metadata that jointly learn model parameters and\nfeature representations using deep networks and offer accurate classification\nat the scale of millions of labels. DECAF makes specific contributions to model\narchitecture design, initialization, and training, enabling it to offer up to\n2-6% more accurate prediction than leading extreme classifiers on publicly\navailable benchmark product-to-product recommendation datasets, such as\nLF-AmazonTitles-1.3M. At the same time, DECAF was found to be up to 22x faster\nat inference than leading deep extreme classifiers, which makes it suitable for\nreal-time applications that require predictions within a few milliseconds. The\ncode for DECAF is available at the following URL\nhttps://github.com/Extreme-classification/DECAF.",
          "link": "http://arxiv.org/abs/2108.00368",
          "publishedOn": "2021-08-03T02:06:29.720Z",
          "wordCount": 654,
          "title": "DECAF: Deep Extreme Classification with Label Features. (arXiv:2108.00368v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00480",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Rahimikia_E/0/1/0/all/0/1\">Eghbal Rahimikia</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zohren_S/0/1/0/all/0/1\">Stefan Zohren</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Poon_S/0/1/0/all/0/1\">Ser-Huang Poon</a>",
          "description": "We develop FinText, a novel, state-of-the-art, financial word embedding from\nDow Jones Newswires Text News Feed Database. Incorporating this word embedding\nin a machine learning model produces a substantial increase in volatility\nforecasting performance on days with volatility jumps for 23 NASDAQ stocks from\n27 July 2007 to 18 November 2016. A simple ensemble model, combining our word\nembedding and another machine learning model that uses limit order book data,\nprovides the best forecasting performance for both normal and jump volatility\ndays. Finally, we use Integrated Gradients and SHAP (SHapley Additive\nexPlanations) to make the results more 'explainable' and the model comparisons\nmore transparent.",
          "link": "http://arxiv.org/abs/2108.00480",
          "publishedOn": "2021-08-03T02:06:29.702Z",
          "wordCount": 549,
          "title": "Realised Volatility Forecasting: Machine Learning via Financial Word Embedding. (arXiv:2108.00480v1 [q-fin.CP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bruyn_M/0/1/0/all/0/1\">Maxime De Bruyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotfi_E/0/1/0/all/0/1\">Ehsan Lotfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhmann_J/0/1/0/all/0/1\">Jeska Buhmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daelemans_W/0/1/0/all/0/1\">Walter Daelemans</a>",
          "description": "Knowledgeable FAQ chatbots are a valuable resource to any organization.\nUnlike traditional call centers or FAQ web pages, they provide instant\nresponses and are always available. Our experience running a COVID19 chatbot\nrevealed the lack of resources available for FAQ answering in non-English\nlanguages. While powerful and efficient retrieval-based models exist for\nEnglish, it is rarely the case for other languages which do not have the same\namount of training data available. In this work, we propose a novel pretaining\nprocedure to adapt ConveRT, an English SOTA conversational agent, to other\nlanguages with less training data available. We apply it for the first time to\nthe task of Dutch FAQ answering related to the COVID19 vaccine. We show it\nperforms better than an open-source alternative in a low-data regime and\nhigh-data regime.",
          "link": "http://arxiv.org/abs/2108.00719",
          "publishedOn": "2021-08-03T02:06:29.688Z",
          "wordCount": 606,
          "title": "ConveRT, an Application to FAQ Answering. (arXiv:2108.00719v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Joy T. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agu_N/0/1/0/all/0/1\">Nkechinyere N. Agu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1\">Ismini Lourentzou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Arjun Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paguio_J/0/1/0/all/0/1\">Joseph A. Paguio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jasper S. Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dee_E/0/1/0/all/0/1\">Edward C. Dee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_W/0/1/0/all/0/1\">William Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashyap_S/0/1/0/all/0/1\">Satyananda Kashyap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giovannini_A/0/1/0/all/0/1\">Andrea Giovannini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1\">Leo A. Celi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Mehdi Moradi</a>",
          "description": "Despite the progress in automatic detection of radiologic findings from chest\nX-ray (CXR) images in recent years, a quantitative evaluation of the\nexplainability of these models is hampered by the lack of locally labeled\ndatasets for different findings. With the exception of a few expert-labeled\nsmall-scale datasets for specific findings, such as pneumonia and pneumothorax,\nmost of the CXR deep learning models to date are trained on global \"weak\"\nlabels extracted from text reports, or trained via a joint image and\nunstructured text learning strategy. Inspired by the Visual Genome effort in\nthe computer vision community, we constructed the first Chest ImaGenome dataset\nwith a scene graph data structure to describe $242,072$ images. Local\nannotations are automatically produced using a joint rule-based natural\nlanguage processing (NLP) and atlas-based bounding box detection pipeline.\nThrough a radiologist constructed CXR ontology, the annotations for each CXR\nare connected as an anatomy-centered scene graph, useful for image-level\nreasoning and multimodal fusion applications. Overall, we provide: i) $1,256$\ncombinations of relation annotations between $29$ CXR anatomical locations\n(objects with bounding box coordinates) and their attributes, structured as a\nscene graph per image, ii) over $670,000$ localized comparison relations (for\nimproved, worsened, or no change) between the anatomical locations across\nsequential exams, as well as ii) a manually annotated gold standard scene graph\ndataset from $500$ unique patients.",
          "link": "http://arxiv.org/abs/2108.00316",
          "publishedOn": "2021-08-03T02:06:29.681Z",
          "wordCount": 697,
          "title": "Chest ImaGenome Dataset for Clinical Reasoning. (arXiv:2108.00316v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00513",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Tian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_K/0/1/0/all/0/1\">Khushbu Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1\">Sutanay Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chandan K. Reddy</a>",
          "description": "Question Answering (QA) in clinical notes has gained a lot of attention in\nthe past few years. Existing machine reading comprehension approaches in\nclinical domain can only handle questions about a single block of clinical\ntexts and fail to retrieve information about different patients and clinical\nnotes. To handle more complex questions, we aim at creating knowledge base from\nclinical notes to link different patients and clinical notes, and performing\nknowledge base question answering (KBQA). Based on the expert annotations in\nn2c2, we first created the ClinicalKBQA dataset that includes 8,952 QA pairs\nand covers questions about seven medical topics through 322 question templates.\nThen, we proposed an attention-based aspect reasoning (AAR) method for KBQA and\ninvestigated the impact of different aspects of answers (e.g., entity, type,\npath, and context) for prediction. The AAR method achieves better performance\ndue to the well-designed encoder and attention mechanism. In the experiments,\nwe find that both aspects, type and path, enable the model to identify answers\nsatisfying the general conditions and produce lower precision and higher\nrecall. On the other hand, the aspects, entity and context, limit the answers\nby node-specific information and lead to higher precision and lower recall.",
          "link": "http://arxiv.org/abs/2108.00513",
          "publishedOn": "2021-08-03T02:06:29.652Z",
          "wordCount": 638,
          "title": "Attention-based Aspect Reasoning for Knowledge Base Question Answering on Clinical Notes. (arXiv:2108.00513v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binlong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>",
          "description": "Chinese sentiment analysis (CSA) has always been one of the challenges in\nnatural language processing due to its complexity and uncertainty. Transformer\nhas succeeded in capturing semantic features, but it uses position encoding to\ncapture sequence features, which has great shortcomings compared with the\nrecurrent model. In this paper, we propose T-E-GRU for Chinese sentiment\nanalysis, which combine transformer encoder and GRU. We conducted experiments\non three Chinese comment datasets. In view of the confusion of punctuation\nmarks in Chinese comment texts, we selectively retain some punctuation marks\nwith sentence segmentation ability. The experimental results show that T-E-GRU\noutperforms classic recurrent model and recurrent model with attention.",
          "link": "http://arxiv.org/abs/2108.00400",
          "publishedOn": "2021-08-03T02:06:29.607Z",
          "wordCount": 542,
          "title": "Transformer-Encoder-GRU (T-E-GRU) for Chinese Sentiment Analysis on Chinese Comment Text. (arXiv:2108.00400v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gadde_R/0/1/0/all/0/1\">Ravi Teja Gadde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1\">Ivan Bulyko</a>",
          "description": "Neural language models (LM) trained on diverse corpora are known to work well\non previously seen entities, however, updating these models with dynamically\nchanging entities such as place names, song titles and shopping items requires\nre-training from scratch and collecting full sentences containing these\nentities. We aim to address this issue, by introducing entity-aware language\nmodels (EALM), where we integrate entity models trained on catalogues of\nentities into the pre-trained LMs. Our combined language model adaptively adds\ninformation from the entity models into the pre-trained LM depending on the\nsentence context. Our entity models can be updated independently of the\npre-trained LM, enabling us to influence the distribution of entities output by\nthe final LM, without any further training of the pre-trained LM. We show\nsignificant perplexity improvements on task-oriented dialogue datasets,\nespecially on long-tailed utterances, with an ability to continually adapt to\nnew entities (to an extent).",
          "link": "http://arxiv.org/abs/2108.00082",
          "publishedOn": "2021-08-03T02:06:29.557Z",
          "wordCount": 595,
          "title": "Towards Continual Entity Learning in Language Models for Conversational Agents. (arXiv:2108.00082v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Podina_I/0/1/0/all/0/1\">Ioana R. Podin&#x103;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1\">Liviu P. Dinu</a>",
          "description": "In this work, we provide an extensive part-of-speech analysis of the\ndiscourse of social media users with depression. Research in psychology\nrevealed that depressed users tend to be self-focused, more preoccupied with\nthemselves and ruminate more about their lives and emotions. Our work aims to\nmake use of large-scale datasets and computational methods for a quantitative\nexploration of discourse. We use the publicly available depression dataset from\nthe Early Risk Prediction on the Internet Workshop (eRisk) 2018 and extract\npart-of-speech features and several indices based on them. Our results reveal\nstatistically significant differences between the depressed and non-depressed\nindividuals confirming findings from the existing psychology literature. Our\nwork provides insights regarding the way in which depressed individuals are\nexpressing themselves on social media platforms, allowing for better-informed\ncomputational models to help monitor and prevent mental illnesses.",
          "link": "http://arxiv.org/abs/2108.00279",
          "publishedOn": "2021-08-03T02:06:29.550Z",
          "wordCount": 576,
          "title": "A Psychologically Informed Part-of-Speech Analysis of Depression in Social Media. (arXiv:2108.00279v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Anshul Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_N/0/1/0/all/0/1\">Noveen Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sheshansh Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sumeet Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1\">Purushottam Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1\">Manik Varma</a>",
          "description": "Deep extreme classification (XC) seeks to train deep architectures that can\ntag a data point with its most relevant subset of labels from an extremely\nlarge label set. The core utility of XC comes from predicting labels that are\nrarely seen during training. Such rare labels hold the key to personalized\nrecommendations that can delight and surprise a user. However, the large number\nof rare labels and small amount of training data per rare label offer\nsignificant statistical and computational challenges. State-of-the-art deep XC\nmethods attempt to remedy this by incorporating textual descriptions of labels\nbut do not adequately address the problem. This paper presents ECLARE, a\nscalable deep learning architecture that incorporates not only label text, but\nalso label correlations, to offer accurate real-time predictions within a few\nmilliseconds. Core contributions of ECLARE include a frugal architecture and\nscalable techniques to train deep models along with label correlation graphs at\nthe scale of millions of labels. In particular, ECLARE offers predictions that\nare 2 to 14% more accurate on both publicly available benchmark datasets as\nwell as proprietary datasets for a related products recommendation task sourced\nfrom the Bing search engine. Code for ECLARE is available at\nhttps://github.com/Extreme-classification/ECLARE.",
          "link": "http://arxiv.org/abs/2108.00261",
          "publishedOn": "2021-08-03T02:06:29.542Z",
          "wordCount": 655,
          "title": "ECLARE: Extreme Classification with Label Graph Correlations. (arXiv:2108.00261v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suresh_V/0/1/0/all/0/1\">Varsha Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_D/0/1/0/all/0/1\">Desmond C. Ong</a>",
          "description": "Modern emotion recognition systems are trained to recognize only a small set\nof emotions, and hence fail to capture the broad spectrum of emotions people\nexperience and express in daily life. In order to engage in more empathetic\ninteractions, future AI has to perform \\textit{fine-grained} emotion\nrecognition, distinguishing between many more varied emotions. Here, we focus\non improving fine-grained emotion recognition by introducing external knowledge\ninto a pre-trained self-attention model. We propose Knowledge-Embedded\nAttention (KEA) to use knowledge from emotion lexicons to augment the\ncontextual representations from pre-trained ELECTRA and BERT models. Our\nresults and error analyses outperform previous models on several datasets, and\nis better able to differentiate closely-confusable emotions, such as afraid and\nterrified.",
          "link": "http://arxiv.org/abs/2108.00194",
          "publishedOn": "2021-08-03T02:06:29.510Z",
          "wordCount": 567,
          "title": "Using Knowledge-Embedded Attention to Augment Pre-trained Language Models for Fine-Grained Emotion Recognition. (arXiv:2108.00194v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1\">AbdelRahim Elmadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1\">El Moatez Billah Nagoudi</a>",
          "description": "Masked language models (MLMs) are pretrained with a denoising objective that,\nwhile useful, is in a mismatch with the objective of downstream fine-tuning. We\npropose pragmatic masking and surrogate fine-tuning as two strategies that\nexploit social cues to drive pre-trained representations toward a broad set of\nconcepts useful for a wide class of social meaning tasks. To test our methods,\nwe introduce a new benchmark of 15 different Twitter datasets for social\nmeaning detection. Our methods achieve 2.34% F1 over a competitive baseline,\nwhile outperforming other transfer learning methods such as multi-task learning\nand domain-specific language models pretrained on large datasets. With only 5%\nof training data (severely few-shot), our methods enable an impressive 68.74%\naverage F1, and we observe promising results in a zero-shot setting involving\nsix datasets from three different languages.",
          "link": "http://arxiv.org/abs/2108.00356",
          "publishedOn": "2021-08-03T02:06:29.447Z",
          "wordCount": 577,
          "title": "Improving Social Meaning Detection with Pragmatic Masking and Surrogate Fine-Tuning. (arXiv:2108.00356v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00104",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_P/0/1/0/all/0/1\">Peng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseem_T/0/1/0/all/0/1\">Tahira Naseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Astudillo_R/0/1/0/all/0/1\">Ram&#xf3;n Fernandez Astudillo</a>",
          "description": "Transformer-based language models pre-trained on large amounts of text data\nhave proven remarkably successful in learning generic transferable linguistic\nrepresentations. Here we study whether structural guidance leads to more\nhuman-like systematic linguistic generalization in Transformer language models\nwithout resorting to pre-training on very large amounts of data. We explore two\ngeneral ideas. The \"Generative Parsing\" idea jointly models the incremental\nparse and word sequence as part of the same sequence modeling task. The\n\"Structural Scaffold\" idea guides the language model's representation via\nadditional structure loss that separately predicts the incremental constituency\nparse. We train the proposed models along with a vanilla Transformer language\nmodel baseline on a 14 million-token and a 46 million-token subset of the BLLIP\ndataset, and evaluate models' syntactic generalization performances on SG Test\nSuites and sized BLiMP. Experiment results across two benchmarks suggest\nconverging evidence that generative structural supervisions can induce more\nrobust and humanlike linguistic generalization in Transformer language models\nwithout the need for data intensive pre-training.",
          "link": "http://arxiv.org/abs/2108.00104",
          "publishedOn": "2021-08-03T02:06:29.439Z",
          "wordCount": 601,
          "title": "Structural Guidance for Transformer Language Models. (arXiv:2108.00104v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara L. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "We introduce mTVR, a large-scale multilingual video moment retrieval dataset,\ncontaining 218K English and Chinese queries from 21.8K TV show video clips. The\ndataset is collected by extending the popular TVR dataset (in English) with\npaired Chinese queries and subtitles. Compared to existing moment retrieval\ndatasets, mTVR is multilingual, larger, and comes with diverse annotations. We\nfurther propose mXML, a multilingual moment retrieval model that learns and\noperates on data from both languages, via encoder parameter sharing and\nlanguage neighborhood constraints. We demonstrate the effectiveness of mXML on\nthe newly collected MTVR dataset, where mXML outperforms strong monolingual\nbaselines while using fewer parameters. In addition, we also provide detailed\ndataset analyses and model ablations. Data and code are publicly available at\nhttps://github.com/jayleicn/mTVRetrieval",
          "link": "http://arxiv.org/abs/2108.00061",
          "publishedOn": "2021-08-03T02:06:29.423Z",
          "wordCount": 569,
          "title": "MTVR: Multilingual Moment Retrieval in Videos. (arXiv:2108.00061v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tumarada_K/0/1/0/all/0/1\">Kishore Tumarada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dr. Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragut_D/0/1/0/all/0/1\">Dr. Eduard Dragut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gnawali_D/0/1/0/all/0/1\">Dr. Omprakash Gnawali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_D/0/1/0/all/0/1\">Dr. Arjun Mukherjee</a>",
          "description": "Opinion prediction is an emerging research area with diverse real-world\napplications, such as market research and situational awareness. We identify\ntwo lines of approaches to the problem of opinion prediction. One uses\ntopic-based sentiment analysis with time-series modeling, while the other uses\nstatic embedding of text. The latter approaches seek user-specific solutions by\ngenerating user fingerprints. Such approaches are useful in predicting user's\nreactions to unseen content. In this work, we propose a novel dynamic\nfingerprinting method that leverages contextual embedding of user's comments\nconditioned on relevant user's reading history. We integrate BERT variants with\na recurrent neural network to generate predictions. The results show up to 13\\%\nimprovement in micro F1-score compared to previous approaches. Experimental\nresults show novel insights that were previously unknown such as better\npredictions for an increase in dynamic history length, the impact of the nature\nof the article on performance, thereby laying the foundation for further\nresearch.",
          "link": "http://arxiv.org/abs/2108.00270",
          "publishedOn": "2021-08-03T02:06:29.400Z",
          "wordCount": 599,
          "title": "Opinion Prediction with User Fingerprinting. (arXiv:2108.00270v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1\">Mika H&#xe4;m&#xe4;l&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1\">Khalid Alnajjar</a>",
          "description": "We survey human evaluation in papers presenting work on creative natural\nlanguage generation that have been published in INLG 2020 and ICCC 2020. The\nmost typical human evaluation method is a scaled survey, typically on a 5 point\nscale, while many other less common methods exist. The most commonly evaluated\nparameters are meaning, syntactic correctness, novelty, relevance and emotional\nvalue, among many others. Our guidelines for future evaluation include clearly\ndefining the goal of the generative system, asking questions as concrete as\npossible, testing the evaluation setup, using multiple different evaluation\nsetups, reporting the entire evaluation process and potential biases clearly,\nand finally analyzing the evaluation results in a more profound way than merely\nreporting the most typical statistics.",
          "link": "http://arxiv.org/abs/2108.00308",
          "publishedOn": "2021-08-03T02:06:29.385Z",
          "wordCount": 570,
          "title": "Human Evaluation of Creative NLG Systems: An Interdisciplinary Survey on Recent Papers. (arXiv:2108.00308v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1\">Joseph Marvin Imperial</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1\">Ethel Ong</a>",
          "description": "In order to ensure quality and effective learning, fluency, and\ncomprehension, the proper identification of the difficulty levels of reading\nmaterials should be observed. In this paper, we describe the development of\nautomatic machine learning-based readability assessment models for educational\nFilipino texts using the most diverse set of linguistic features for the\nlanguage. Results show that using a Random Forest model obtained a high\nperformance of 62.7% in terms of accuracy, and 66.1% when using the optimal\ncombination of feature sets consisting of traditional and syllable\npattern-based predictors.",
          "link": "http://arxiv.org/abs/2108.00241",
          "publishedOn": "2021-08-03T02:06:29.368Z",
          "wordCount": 531,
          "title": "Diverse Linguistic Features for Assessing Reading Difficulty of Educational Filipino Texts. (arXiv:2108.00241v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Heng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1\">Yew-Soon Ong</a>",
          "description": "Current one-stage methods for visual grounding encode the language query as\none holistic sentence embedding before fusion with visual feature. Such a\nformulation does not treat each word of a query sentence on par when modeling\nlanguage to visual attention, therefore prone to neglect words which are less\nimportant for sentence embedding but critical for visual grounding. In this\npaper we propose Word2Pix: a one-stage visual grounding network based on\nencoder-decoder transformer architecture that enables learning for textual to\nvisual feature correspondence via word to pixel attention. The embedding of\neach word from the query sentence is treated alike by attending to visual\npixels individually instead of single holistic sentence embedding. In this way,\neach word is given equivalent opportunity to adjust the language to vision\nattention towards the referent target through multiple stacks of transformer\ndecoder layers. We conduct the experiments on RefCOCO, RefCOCO+ and RefCOCOg\ndatasets and the proposed Word2Pix outperforms existing one-stage methods by a\nnotable margin. The results obtained also show that Word2Pix surpasses\ntwo-stage visual grounding models, while at the same time keeping the merits of\none-stage paradigm namely end-to-end training and real-time inference speed\nintact.",
          "link": "http://arxiv.org/abs/2108.00205",
          "publishedOn": "2021-08-03T02:06:29.360Z",
          "wordCount": 639,
          "title": "Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding. (arXiv:2108.00205v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hannun_A/0/1/0/all/0/1\">Awni Hannun</a>",
          "description": "The decade from 2010 to 2020 saw remarkable improvements in automatic speech\nrecognition. Many people now use speech recognition on a daily basis, for\nexample to perform voice search queries, send text messages, and interact with\nvoice assistants like Amazon Alexa and Siri by Apple. Before 2010 most people\nrarely used speech recognition. Given the remarkable changes in the state of\nspeech recognition over the previous decade, what can we expect over the coming\ndecade? I attempt to forecast the state of speech recognition research and\napplications by the year 2030. While the changes to general speech recognition\naccuracy will not be as dramatic as in the previous decade, I suggest we have\nan exciting decade of progress in speech technology ahead of us.",
          "link": "http://arxiv.org/abs/2108.00084",
          "publishedOn": "2021-08-03T02:06:29.238Z",
          "wordCount": 551,
          "title": "The History of Speech Recognition to the Year 2030. (arXiv:2108.00084v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morgan_S/0/1/0/all/0/1\">Skye Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>",
          "description": "This paper addresses the identification of toxic, engaging, and fact-claiming\ncomments on social media. We used the dataset made available by the organizers\nof the GermEval-2021 shared task containing over 3,000 manually annotated\nFacebook comments in German. Considering the relatedness of the three tasks, we\napproached the problem using large pre-trained transformer models and multitask\nlearning. Our results indicate that multitask learning achieves performance\nsuperior to the more common single task learning approach in all three tasks.\nWe submit our best systems to GermEval-2021 under the team name WLV-RIT.",
          "link": "http://arxiv.org/abs/2108.00057",
          "publishedOn": "2021-08-03T02:06:29.145Z",
          "wordCount": 553,
          "title": "WLV-RIT at GermEval 2021: Multitask Learning with Transformers to Detect Toxic, Engaging, and Fact-Claiming Comments. (arXiv:2108.00057v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1\">Joseph Marvin Imperial</a>",
          "description": "Automatic readability assessment (ARA) is the task of evaluating the level of\nease or difficulty of text documents for a target audience. For researchers,\none of the many open problems in the field is to make such models trained for\nthe task show efficacy even for low-resource languages. In this study, we\npropose an alternative way of utilizing the information-rich embeddings of BERT\nmodels with handcrafted linguistic features through a combined method for\nreadability assessment. Results show that the proposed method outperforms\nclassical approaches in readability assessment using English and Filipino\ndatasets, obtaining as high as 12.4% increase in F1 performance. We also show\nthat the general information encoded in BERT embeddings can be used as a\nsubstitute feature set for low-resource languages like Filipino with limited\nsemantic and syntactic NLP tools to explicitly extract feature values for the\ntask.",
          "link": "http://arxiv.org/abs/2106.07935",
          "publishedOn": "2021-08-02T01:58:23.538Z",
          "wordCount": 592,
          "title": "BERT Embeddings for Automatic Readability Assessment. (arXiv:2106.07935v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohitash Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_A/0/1/0/all/0/1\">Aswin Krishna</a>",
          "description": "Social scientists and psychologists take interest in understanding how people\nexpress emotions and sentiments when dealing with catastrophic events such as\nnatural disasters, political unrest, and terrorism. The COVID-19 pandemic is a\ncatastrophic event that has raised a number of psychological issues such as\ndepression given abrupt social changes and lack of employment. Advancements of\ndeep learning-based language models have been promising for sentiment analysis\nwith data from social networks such as Twitter. Given the situation with\nCOVID-19 pandemic, different countries had different peaks where the rise and\nfall of new cases affected lock-downs which directly affected the economy and\nemployment. During the rise of COVID-19 cases with stricter lock-downs, people\nhave been expressing their sentiments in social media. This can provide a deep\nunderstanding of human psychology during catastrophic events. In this paper, we\npresent a framework that employs deep learning-based language models via long\nshort-term memory (LSTM) recurrent neural networks for sentiment analysis\nduring the rise of novel COVID-19 cases in India. The framework features LSTM\nlanguage model with a global vector embedding and state-of-art BERT language\nmodel. We review the sentiments expressed for selective months in 2020 which\ncovers the first major peak of novel cases in India. Our framework utilises\nmulti-label sentiment classification where more than one sentiment can be\nexpressed at once. Our results indicate that the majority of the tweets have\nbeen positive with high levels of optimism during the rise of the novel\nCOVID-19 cases and the number of tweets significantly lowered towards the peak.\nThe predictions generally indicate that although the majority have been\noptimistic, a significant group of population has been annoyed towards the way\nthe pandemic was handled by the authorities.",
          "link": "http://arxiv.org/abs/2104.10662",
          "publishedOn": "2021-08-02T01:58:23.508Z",
          "wordCount": 798,
          "title": "COVID-19 sentiment analysis via deep learning during the rise of novel cases. (arXiv:2104.10662v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ambroszkiewicz_S/0/1/0/all/0/1\">Stanislaw Ambroszkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartyna_W/0/1/0/all/0/1\">Waldemar Bartyna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bylka_S/0/1/0/all/0/1\">Stanislaw Bylka</a>",
          "description": "Cloud Native Application CNApp (as a distributed system) is a collection of\nindependent components (micro-services) interacting via communication\nprotocols. This gives rise to present an abstract architecture of CNApp as\ndynamically re-configurable acyclic directed multi graph where vertices are\nmicroservices, and edges are the protocols. Generic mechanisms for such\nreconfigurations evidently correspond to higher-level functions (functionals).\nThis implies also internal abstract architecture of microservice as a\ncollection of event-triggered serverless functions (including functions\nimplementing the protocols) that are dynamically composed into event-dependent\ndata-flow graphs. Again, generic mechanisms for such compositions correspond to\ncalculus of functionals and relations.",
          "link": "http://arxiv.org/abs/2105.10362",
          "publishedOn": "2021-08-02T01:58:23.482Z",
          "wordCount": 589,
          "title": "Functionals in the Clouds: An abstract architecture of serverless Cloud-Native Apps. (arXiv:2105.10362v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1\">Matteo Stefanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1\">Silvia Cascianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Connecting Vision and Language plays an essential role in Generative\nIntelligence. For this reason, large research efforts have been devoted to\nimage captioning, i.e. describing images with syntactically and semantically\nmeaningful sentences. Starting from 2015 the task has generally been addressed\nwith pipelines composed of a visual encoder and a language model for text\ngeneration. During these years, both components have evolved considerably\nthrough the exploitation of object regions, attributes, the introduction of\nmulti-modal connections, fully-attentive approaches, and BERT-like early-fusion\nstrategies. However, regardless of the impressive results, research in image\ncaptioning has not reached a conclusive answer yet. This work aims at providing\na comprehensive overview of image captioning approaches, from visual encoding\nand text generation to training strategies, datasets, and evaluation metrics.\nIn this respect, we quantitatively compare many relevant state-of-the-art\napproaches to identify the most impactful technical innovations in\narchitectures and training strategies. Moreover, many variants of the problem\nand its open challenges are discussed. The final goal of this work is to serve\nas a tool for understanding the existing literature and highlighting the future\ndirections for a research area where Computer Vision and Natural Language\nProcessing can find an optimal synergy.",
          "link": "http://arxiv.org/abs/2107.06912",
          "publishedOn": "2021-08-02T01:58:23.400Z",
          "wordCount": 659,
          "title": "From Show to Tell: A Survey on Image Captioning. (arXiv:2107.06912v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boenninghoff_B/0/1/0/all/0/1\">Benedikt Boenninghoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickel_R/0/1/0/all/0/1\">Robert M. Nickel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolossa_D/0/1/0/all/0/1\">Dorothea Kolossa</a>",
          "description": "The PAN 2021 authorship verification (AV) challenge is part of a three-year\nstrategy, moving from a cross-topic/closed-set AV task to a\ncross-topic/open-set AV task over a collection of fanfiction texts. In this\nwork, we present a novel hybrid neural-probabilistic framework that is designed\nto tackle the challenges of the 2021 task. Our system is based on our 2020\nwinning submission, with updates to significantly reduce sensitivities to\ntopical variations and to further improve the system's calibration by means of\nan uncertainty-adaptation layer. Our framework additionally includes an\nout-of-distribution detector (O2D2) for defining non-responses. Our proposed\nsystem outperformed all other systems that participated in the PAN 2021 AV\ntask.",
          "link": "http://arxiv.org/abs/2106.15825",
          "publishedOn": "2021-08-02T01:58:23.394Z",
          "wordCount": 571,
          "title": "O2D2: Out-Of-Distribution Detector to Capture Undecidable Trials in Authorship Verification. (arXiv:2106.15825v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>",
          "description": "We present our works on SemEval-2021 Task 5 about Toxic Spans Detection. This\ntask aims to build a model for identifying toxic words in whole posts. We use\nthe BiLSTM-CRF model combining with ToxicBERT Classification to train the\ndetection model for identifying toxic words in posts. Our model achieves 62.23%\nby F1-score on the Toxic Spans Detection task.",
          "link": "http://arxiv.org/abs/2104.10100",
          "publishedOn": "2021-08-02T01:58:23.371Z",
          "wordCount": 555,
          "title": "UIT-ISE-NLP at SemEval-2021 Task 5: Toxic Spans Detection with BiLSTM-CRF and ToxicBERT Comment Classification. (arXiv:2104.10100v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cetoli_A/0/1/0/all/0/1\">Alberto Cetoli</a>",
          "description": "This paper proposes a message-passing mechanism to address language\nmodelling. A new layer type is introduced that aims to substitute\nself-attention for unidirectional sequence generation tasks. The system is\nshown to be competitive with existing methods: Given N tokens, the\ncomputational complexity is O(N logN) and the memory complexity is O(N) under\nreasonable assumptions. In the end, the Dispatcher layer is seen to achieve\ncomparable perplexity to prior results while being more efficient.",
          "link": "http://arxiv.org/abs/2105.03994",
          "publishedOn": "2021-08-02T01:58:23.363Z",
          "wordCount": 522,
          "title": "Dispatcher: A Message-Passing Approach To Language Modelling. (arXiv:2105.03994v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>",
          "description": "Recently, the development of pre-trained language models has brought natural\nlanguage processing (NLP) tasks to the new state-of-the-art. In this paper we\nexplore the efficiency of various pre-trained language models. We pre-train a\nlist of transformer-based models with the same amount of text and the same\ntraining steps. The experimental results shows that the most improvement upon\nthe origin BERT is adding the RNN-layer to capture more contextual information\nfor the transformer-encoder layers.",
          "link": "http://arxiv.org/abs/2106.11483",
          "publishedOn": "2021-08-02T01:58:23.356Z",
          "wordCount": 527,
          "title": "A Comprehensive Exploration of Pre-training Language Models. (arXiv:2106.11483v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.06402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allein_L/0/1/0/all/0/1\">Liesbeth Allein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>",
          "description": "Truth can vary over time. Fact-checking decisions on claim veracity should\ntherefore take into account temporal information of both the claim and\nsupporting or refuting evidence. In this work, we investigate the hypothesis\nthat the timestamp of a Web page is crucial to how it should be ranked for a\ngiven claim. We delineate four temporal ranking methods that constrain evidence\nranking differently and simulate hypothesis-specific evidence rankings given\nthe evidence timestamps as gold standard. Evidence ranking in three\nfact-checking models is ultimately optimized using a learning-to-rank loss\nfunction. Our study reveals that time-aware evidence ranking not only surpasses\nrelevance assumptions based purely on semantic similarity or position in a\nsearch results list, but also improves veracity predictions of time-sensitive\nclaims in particular.",
          "link": "http://arxiv.org/abs/2009.06402",
          "publishedOn": "2021-08-02T01:58:23.320Z",
          "wordCount": 595,
          "title": "Time-Aware Evidence Ranking for Fact-Checking. (arXiv:2009.06402v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frey_B/0/1/0/all/0/1\">Benjamin Frey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "We introduce ChrEnTranslate, an online machine translation demonstration\nsystem for translation between English and an endangered language Cherokee. It\nsupports both statistical and neural translation models as well as provides\nquality estimation to inform users of reliability, two user feedback interfaces\nfor experts and common users respectively, example inputs to collect human\ntranslations for monolingual data, word alignment visualization, and relevant\nterms from the Cherokee-English dictionary. The quantitative evaluation\ndemonstrates that our backbone translation models achieve state-of-the-art\ntranslation performance and our quality estimation well correlates with both\nBLEU and human judgment. By analyzing 216 pieces of expert feedback, we find\nthat NMT is preferable because it copies less than SMT, and, in general,\ncurrent models can translate fragments of the source sentence but make major\nmistakes. When we add these 216 expert-corrected parallel texts into the\ntraining set and retrain models, equal or slightly better performance is\nobserved, which demonstrates indicates the potential of human-in-the-loop\nlearning. Our online demo is at https://chren.cs.unc.edu/; our code is\nopen-sourced at https://github.com/ZhangShiyue/ChrEnTranslate; and our data is\navailable at https://github.com/ZhangShiyue/ChrEn.",
          "link": "http://arxiv.org/abs/2107.14800",
          "publishedOn": "2021-08-02T01:58:23.313Z",
          "wordCount": 626,
          "title": "ChrEnTranslate: Cherokee-English Machine Translation Demo with Quality Estimation and Corrective Feedback. (arXiv:2107.14800v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1\">Jean-Baptiste Alayrac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doersch_C/0/1/0/all/0/1\">Carl Doersch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_C/0/1/0/all/0/1\">Catalin Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">David Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1\">Skanda Koppula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andrew Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1\">Evan Shelhamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1\">Olivier H&#xe9;naff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1\">Matthew M. Botvinick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Jo&#xe3;o Carreira</a>",
          "description": "The recently-proposed Perceiver model obtains good results on several domains\n(images, audio, multimodal, point clouds) while scaling linearly in compute and\nmemory with the input size. While the Perceiver supports many kinds of inputs,\nit can only produce very simple outputs such as class scores. Perceiver IO\novercomes this limitation without sacrificing the original's appealing\nproperties by learning to flexibly query the model's latent space to produce\noutputs of arbitrary size and semantics. Perceiver IO still decouples model\ndepth from data size and still scales linearly with data size, but now with\nrespect to both input and output sizes. The full Perceiver IO model achieves\nstrong results on tasks with highly structured output spaces, such as natural\nlanguage and visual understanding, StarCraft II, and multi-task and multi-modal\ndomains. As highlights, Perceiver IO matches a Transformer-based BERT baseline\non the GLUE language benchmark without the need for input tokenization and\nachieves state-of-the-art performance on Sintel optical flow estimation.",
          "link": "http://arxiv.org/abs/2107.14795",
          "publishedOn": "2021-08-02T01:58:23.302Z",
          "wordCount": 639,
          "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs. (arXiv:2107.14795v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "Recent years have brought about an interest in the challenging task of\nsummarizing conversation threads (meetings, online discussions, etc.). Such\nsummaries help analysis of the long text to quickly catch up with the decisions\nmade and thus improve our work or communication efficiency. To spur research in\nthread summarization, we have developed an abstractive Email Thread\nSummarization (EmailSum) dataset, which contains human-annotated short (<30\nwords) and long (<100 words) summaries of 2549 email threads (each containing 3\nto 10 emails) over a wide variety of topics. We perform a comprehensive\nempirical study to explore different summarization techniques (including\nextractive and abstractive methods, single-document and hierarchical models, as\nwell as transfer and semisupervised learning) and conduct human evaluations on\nboth short and long summary generation tasks. Our results reveal the key\nchallenges of current abstractive summarization models in this task, such as\nunderstanding the sender's intent and identifying the roles of sender and\nreceiver. Furthermore, we find that widely used automatic evaluation metrics\n(ROUGE, BERTScore) are weakly correlated with human judgments on this email\nthread summarization task. Hence, we emphasize the importance of human\nevaluation and the development of better metrics by the community. Our code and\nsummary data have been made available at:\nhttps://github.com/ZhangShiyue/EmailSum",
          "link": "http://arxiv.org/abs/2107.14691",
          "publishedOn": "2021-08-02T01:58:23.296Z",
          "wordCount": 641,
          "title": "EmailSum: Abstractive Email Thread Summarization. (arXiv:2107.14691v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14638",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paula_A/0/1/0/all/0/1\">Amauri J Paula</a>",
          "description": "It is presented here a machine learning-based (ML) natural language\nprocessing (NLP) approach capable to automatically recognize and extract\ncategorical and numerical parameters from a corpus of articles. The approach\n(named a.RIX) operates with a concomitant/interchangeable use of ML models such\nas neuron networks (NNs), latent semantic analysis (LSA) and naive-Bayes\nclassifiers (NBC), and a pattern recognition model using regular expression\n(REGEX). To demonstrate the efficiency of the a.RIX engine, it was processed a\ncorpus of 7,873 scientific articles dealing with natural products (NPs). The\nengine automatically extracts categorical and numerical parameters such as (i)\nthe plant species from which active molecules are extracted, (ii) the\nmicroorganisms species for which active molecules can act against, and (iii)\nthe values of minimum inhibitory concentration (MIC) against these\nmicroorganisms. The parameters are extracted without part-of-speech tagging\n(POS) and named entity recognition (NER) approaches (i.e. without the need of\ntext annotation), and the models training is performed with unsupervised\napproaches. In this way, a.RIX can be essentially used on articles from any\nscientific field. Finally, it has a potential to make obsolete the currently\nused articles reviewing process in some areas, specially those in which texts\nstructure, text semantics and latent knowledge is captured by machine learning\nmodels.",
          "link": "http://arxiv.org/abs/2107.14638",
          "publishedOn": "2021-08-02T01:58:23.266Z",
          "wordCount": 646,
          "title": "An automated domain-independent text reading, interpreting and extracting approach for reviewing the scientific literature. (arXiv:2107.14638v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1911.12377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Landi_F/0/1/0/all/0/1\">Federico Landi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corsini_M/0/1/0/all/0/1\">Massimiliano Corsini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Vision-and-Language Navigation (VLN) is a challenging task in which an agent\nneeds to follow a language-specified path to reach a target destination. The\ngoal gets even harder as the actions available to the agent get simpler and\nmove towards low-level, atomic interactions with the environment. This setting\ntakes the name of low-level VLN. In this paper, we strive for the creation of\nan agent able to tackle three key issues: multi-modality, long-term\ndependencies, and adaptability towards different locomotive settings. To that\nend, we devise \"Perceive, Transform, and Act\" (PTA): a fully-attentive VLN\narchitecture that leaves the recurrent approach behind and the first\nTransformer-like architecture incorporating three different modalities -\nnatural language, images, and low-level actions for the agent control. In\nparticular, we adopt an early fusion strategy to merge lingual and visual\ninformation efficiently in our encoder. We then propose to refine the decoding\nphase with a late fusion extension between the agent's history of actions and\nthe perceptual modalities. We experimentally validate our model on two\ndatasets: PTA achieves promising results in low-level VLN on R2R and achieves\ngood performance in the recently proposed R4R benchmark. Our code is publicly\navailable at https://github.com/aimagelab/perceive-transform-and-act.",
          "link": "http://arxiv.org/abs/1911.12377",
          "publishedOn": "2021-08-02T01:58:23.225Z",
          "wordCount": 687,
          "title": "Multimodal Attention Networks for Low-Level Vision-and-Language Navigation. (arXiv:1911.12377v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_D/0/1/0/all/0/1\">Deepak Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casper_J/0/1/0/all/0/1\">Jared Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeGresley_P/0/1/0/all/0/1\">Patrick LeGresley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1\">Mostofa Patwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korthikanti_V/0/1/0/all/0/1\">Vijay Anand Korthikanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainbrand_D/0/1/0/all/0/1\">Dmitri Vainbrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashinkunti_P/0/1/0/all/0/1\">Prethvi Kashinkunti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernauer_J/0/1/0/all/0/1\">Julie Bernauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phanishayee_A/0/1/0/all/0/1\">Amar Phanishayee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>",
          "description": "Large language models have led to state-of-the-art accuracies across a range\nof tasks. However, training these models efficiently is challenging for two\nreasons: a) GPU memory capacity is limited, making it impossible to fit large\nmodels on even a multi-GPU server; b) the number of compute operations required\nto train these models can result in unrealistically long training times.\nConsequently, new methods of model parallelism such as tensor and pipeline\nparallelism have been proposed. Unfortunately, naive usage of these methods\nleads to fundamental scaling issues at thousands of GPUs, e.g., due to\nexpensive cross-node communication or devices spending significant time waiting\non other devices to make progress.\n\nIn this paper, we show how different types of parallelism methods (tensor,\npipeline, and data parallelism) can be composed to scale to thousands of GPUs\nand models with trillions of parameters. We survey techniques for pipeline\nparallelism and propose a novel interleaved pipeline parallelism schedule that\ncan improve throughput by 10+% with memory footprint comparable to existing\napproaches. We quantitatively study the trade-offs between tensor, pipeline,\nand data parallelism, and provide intuition as to how to configure distributed\ntraining of a large model. Our approach allows us to perform training\niterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs\nwith achieved per-GPU throughput of 52% of theoretical peak. Our code is open\nsourced at https://github.com/nvidia/megatron-lm.",
          "link": "http://arxiv.org/abs/2104.04473",
          "publishedOn": "2021-08-02T01:58:23.215Z",
          "wordCount": 734,
          "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM. (arXiv:2104.04473v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14596",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongtong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fangxiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>",
          "description": "Multimodal pre-training models, such as LXMERT, have achieved excellent\nresults in downstream tasks. However, current pre-trained models require large\namounts of training data and have huge model sizes, which make them difficult\nto apply in low-resource situations. How to obtain similar or even better\nperformance than a larger model under the premise of less pre-training data and\nsmaller model size has become an important problem. In this paper, we propose a\nnew Multi-stage Pre-training (MSP) method, which uses information at different\ngranularities from word, phrase to sentence in both texts and images to\npre-train the model in stages. We also design several different pre-training\ntasks suitable for the information granularity in different stage in order to\nefficiently capture the diverse knowledge from a limited corpus. We take a\nSimplified LXMERT (LXMERT- S), which has only 45.9% parameters of the original\nLXMERT model and 11.76% of the original pre-training data as the testbed of our\nMSP method. Experimental results show that our method achieves comparable\nperformance to the original LXMERT model in all downstream tasks, and even\noutperforms the original model in Image-Text Retrieval task.",
          "link": "http://arxiv.org/abs/2107.14596",
          "publishedOn": "2021-08-02T01:58:23.204Z",
          "wordCount": 617,
          "title": "Multi-stage Pre-training over Simplified Multimodal Pre-training Models. (arXiv:2107.14596v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hauer_B/0/1/0/all/0/1\">Bradley Hauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondrak_G/0/1/0/all/0/1\">Grzegorz Kondrak</a>",
          "description": "The WiC task has attracted considerable attention in the NLP community, as\ndemonstrated by the popularity of the recent MCL-WiC SemEval task. WSD systems\nand lexical resources have been used for the WiC task, as well as for WiC\ndataset construction. TSV is another task related to both WiC and WSD. We aim\nto establish the exact relationship between WiC, TSV, and WSD. We demonstrate\nthat these semantic classification problems can be pairwise reduced to each\nother, and so they are theoretically equivalent. We analyze the existing WiC\ndatasets to validate this equivalence hypothesis. We conclude that our\nunderstanding of semantic tasks can be increased through the applications of\ntools from theoretical computer science. Our findings also suggests that more\nefficient and simpler methods for one of these tasks could be successfully\napplied in the other two.",
          "link": "http://arxiv.org/abs/2107.14352",
          "publishedOn": "2021-08-02T01:58:23.189Z",
          "wordCount": 572,
          "title": "WiC = TSV = WSD: On the Equivalence of Three Semantic Tasks. (arXiv:2107.14352v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mandy Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>",
          "description": "In this work, we take the first steps towards building a universal rewriter:\na model capable of rewriting text in any language to exhibit a wide variety of\nattributes, including styles and languages, while preserving as much of the\noriginal semantics as possible. In addition to obtaining state-of-the-art\nresults on unsupervised translation, we also demonstrate the ability to do\nzero-shot sentiment transfer in non-English languages using only English\nexemplars for sentiment. We then show that our model is able to modify multiple\nattributes at once, for example adjusting both language and sentiment jointly.\nFinally, we show that our model is capable of performing zero-shot\nformality-sensitive translation.",
          "link": "http://arxiv.org/abs/2107.14749",
          "publishedOn": "2021-08-02T01:58:23.174Z",
          "wordCount": 534,
          "title": "Towards Universality in Multilingual Text Rewriting. (arXiv:2107.14749v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarap_A/0/1/0/all/0/1\">Abien Fred Agarap</a>",
          "description": "We define disentanglement as how far class-different data points from each\nother are, relative to the distances among class-similar data points. When\nmaximizing disentanglement during representation learning, we obtain a\ntransformed feature representation where the class memberships of the data\npoints are preserved. If the class memberships of the data points are\npreserved, we would have a feature representation space in which a nearest\nneighbour classifier or a clustering algorithm would perform well. We take\nadvantage of this method to learn better natural language representation, and\nemploy it on text classification and text clustering tasks. Through\ndisentanglement, we obtain text representations with better-defined clusters\nand improve text classification performance. Our approach had a test\nclassification accuracy of as high as 90.11% and test clustering accuracy of\n88% on the AG News dataset, outperforming our baseline models -- without any\nother training tricks or regularization.",
          "link": "http://arxiv.org/abs/2107.14597",
          "publishedOn": "2021-08-02T01:58:23.167Z",
          "wordCount": 584,
          "title": "Text Classification and Clustering with Annealing Soft Nearest Neighbor Loss. (arXiv:2107.14597v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daphne Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadrzadeh_M/0/1/0/all/0/1\">Mehrnoosh Sadrzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abramsky_S/0/1/0/all/0/1\">Samson Abramsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cervantes_V/0/1/0/all/0/1\">Victor H. Cervantes</a>",
          "description": "Language is contextual as meanings of words are dependent on their contexts.\nContextuality is, concomitantly, a well-defined concept in quantum mechanics\nwhere it is considered a major resource for quantum computations. We\ninvestigate whether natural language exhibits any of the quantum mechanics'\ncontextual features. We show that meaning combinations in ambiguous phrases can\nbe modelled in the sheaf-theoretic framework for quantum contextuality, where\nthey can become possibilistically contextual. Using the framework of\nContextuality-by-Default (CbD), we explore the probabilistic variants of these\nand show that CbD-contextuality is also possible.",
          "link": "http://arxiv.org/abs/2107.14589",
          "publishedOn": "2021-08-02T01:58:23.159Z",
          "wordCount": 528,
          "title": "On the Quantum-like Contextuality of Ambiguous Phrases. (arXiv:2107.14589v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_R/0/1/0/all/0/1\">Runzhe Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>",
          "description": "The high-quality translation results produced by machine translation (MT)\nsystems still pose a huge challenge for automatic evaluation. Current MT\nevaluation pays the same attention to each sentence component, while the\nquestions of real-world examinations (e.g., university examinations) have\ndifferent difficulties and weightings. In this paper, we propose a novel\ndifficulty-aware MT evaluation metric, expanding the evaluation dimension by\ntaking translation difficulty into consideration. A translation that fails to\nbe predicted by most MT systems will be treated as a difficult one and assigned\na large weight in the final score function, and conversely. Experimental\nresults on the WMT19 English-German Metrics shared tasks show that our proposed\nmethod outperforms commonly used MT metrics in terms of human correlation. In\nparticular, our proposed method performs well even when all the MT systems are\nvery competitive, which is when most existing metrics fail to distinguish\nbetween them. The source code is freely available at\nhttps://github.com/NLP2CT/Difficulty-Aware-MT-Evaluation.",
          "link": "http://arxiv.org/abs/2107.14402",
          "publishedOn": "2021-08-02T01:58:23.149Z",
          "wordCount": 589,
          "title": "Difficulty-Aware Machine Translation Evaluation. (arXiv:2107.14402v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mukhtar_S/0/1/0/all/0/1\">Shakeeb A. M. Mukhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joglekar_P/0/1/0/all/0/1\">Pushkar S. Joglekar</a>",
          "description": "One of the major problems writers and poets face is the writer's block. It is\na condition in which an author loses the ability to produce new work or\nexperiences a creative slowdown. The problem is more difficult in the context\nof poetry than prose, as in the latter case authors need not be very concise\nwhile expressing their ideas, also the various aspects such as rhyme, poetic\nmeters are not relevant for prose. One of the most effective ways to overcome\nthis writing block for poets can be, to have a prompt system, which would help\ntheir imagination and open their minds for new ideas. A prompt system can\npossibly generate one liner, two liner or full ghazals. The purpose of this\nwork is to give an ode to the Urdu, Hindi poets, and helping them start their\nnext line of poetry, a couplet or a complete ghazal considering various factors\nlike rhymes, refrain, and meters. The result will help aspiring poets to get\nnew ideas and help them overcome writer's block by auto-generating pieces of\npoetry using Deep Learning techniques. A concern with creative works like this,\nespecially in the literary context, is to ensure that the output is not\nplagiarized. This work also addresses the concern and makes sure that the\nresulting odes are not exact match with input data using parameters like\ntemperature and manual plagiarism check against input corpus. To the best of\nour knowledge, although the automatic text generation problem has been studied\nquite extensively in the literature, the specific problem of Urdu, Hindi poetry\ngeneration has not been explored much. Apart from developing system to\nauto-generate Urdu, Hindi poetry, another key contribution of our work is to\ncreate a cleaned and preprocessed corpus of Urdu, Hindi poetry (derived from\nauthentic resources) and making it freely available for researchers in the\narea.",
          "link": "http://arxiv.org/abs/2107.14587",
          "publishedOn": "2021-08-02T01:58:23.142Z",
          "wordCount": 741,
          "title": "Urdu & Hindi Poetry Generation using Neural Networks. (arXiv:2107.14587v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lamers_W/0/1/0/all/0/1\">Wout S. Lamers</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Boyack_K/0/1/0/all/0/1\">Kevin Boyack</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Lariviere_V/0/1/0/all/0/1\">Vincent Larivi&#xe8;re</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_C/0/1/0/all/0/1\">Cassidy R. Sugimoto</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Eck_N/0/1/0/all/0/1\">Nees Jan van Eck</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Waltman_L/0/1/0/all/0/1\">Ludo Waltman</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Murray_D/0/1/0/all/0/1\">Dakota Murray</a> (4) ((1) Centre for Science and Technology Studies, Leiden University, Leiden, Netherlands, (2) SciTech Strategies, Inc., Albuquerque, NM, USA, (3) &#xc9;cole de biblioth&#xe9;conomie et des sciences de l&#x27;information, Universit&#xe9; de Montr&#xe9;al, Canada, (4) School of Informatics, Computing, and Engineering, Indiana University Bloomington, IN, USA)",
          "description": "Disagreement is essential to scientific progress. However, the extent of\ndisagreement in science, its evolution over time, and the fields in which it\nhappens, remains largely unknown. Leveraging a massive collection of scientific\ntexts, we develop a cue-phrase based approach to identify instances of\ndisagreement citations across more than four million scientific articles. Using\nthis method, we construct an indicator of disagreement across scientific fields\nover the 2000-2015 period. In contrast with black-box text classification\nmethods, our framework is transparent and easily interpretable. We reveal a\ndisciplinary spectrum of disagreement, with higher disagreement in the social\nsciences and lower disagreement in physics and mathematics. However, detailed\ndisciplinary analysis demonstrates heterogeneity across sub-fields, revealing\nthe importance of local disciplinary cultures and epistemic characteristics of\ndisagreement. Paper-level analysis reveals notable episodes of disagreement in\nscience, and illustrates how methodological artefacts can confound analyses of\nscientific texts. These findings contribute to a broader understanding of\ndisagreement and establish a foundation for future research to understanding\nkey processes underlying scientific progress.",
          "link": "http://arxiv.org/abs/2107.14641",
          "publishedOn": "2021-08-02T01:58:23.132Z",
          "wordCount": 658,
          "title": "Measuring Disagreement in Science. (arXiv:2107.14641v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pillai_N/0/1/0/all/0/1\">Nisha Pillai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matuszek_C/0/1/0/all/0/1\">Cynthia Matuszek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1\">Francis Ferraro</a>",
          "description": "We propose a learning system in which language is grounded in visual percepts\nwithout specific pre-defined categories of terms. We present a unified\ngenerative method to acquire a shared semantic/visual embedding that enables\nthe learning of language about a wide range of real-world objects. We evaluate\nthe efficacy of this learning by predicting the semantics of objects and\ncomparing the performance with neural and non-neural inputs. We show that this\ngenerative approach exhibits promising results in language grounding without\npre-specifying visual categories under low resource settings. Our experiments\ndemonstrate that this approach is generalizable to multilingual, highly varied\ndatasets.",
          "link": "http://arxiv.org/abs/2107.14593",
          "publishedOn": "2021-08-02T01:58:23.110Z",
          "wordCount": 552,
          "title": "Neural Variational Learning for Grounded Language Acquisition. (arXiv:2107.14593v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14740",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1\">Shraey Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>",
          "description": "There is unison is the scientific community about human induced climate\nchange. Despite this, we see the web awash with claims around climate change\nscepticism, thus driving the need for fact checking them but at the same time\nproviding an explanation and justification for the fact check. Scientists and\nexperts have been trying to address it by providing manually written feedback\nfor these claims. In this paper, we try to aid them by automating generating\nexplanation for a predicted veracity label for a claim by deploying the\napproach used in open domain question answering of a fusion in decoder\naugmented with retrieved supporting passages from an external knowledge. We\nexperiment with different knowledge sources, retrievers, retriever depths and\ndemonstrate that even a small number of high quality manually written\nexplanations can help us in generating good explanations.",
          "link": "http://arxiv.org/abs/2107.14740",
          "publishedOn": "2021-08-02T01:58:23.097Z",
          "wordCount": 569,
          "title": "Automatic Claim Review for Climate Science via Explanation Generation. (arXiv:2107.14740v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baruah_A/0/1/0/all/0/1\">Arup Baruah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_K/0/1/0/all/0/1\">Kaushik Amar Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbhuiya_F/0/1/0/all/0/1\">Ferdous Ahmed Barbhuiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_K/0/1/0/all/0/1\">Kuntal Dey</a>",
          "description": "This paper presents the results obtained by our SVM and XLM-RoBERTa based\nclassifiers in the shared task Dravidian-CodeMix-HASOC 2020. The SVM classifier\ntrained using TF-IDF features of character and word n-grams performed the best\non the code-mixed Malayalam text. It obtained a weighted F1 score of 0.95 (1st\nRank) and 0.76 (3rd Rank) on the YouTube and Twitter dataset respectively. The\nXLM-RoBERTa based classifier performed the best on the code-mixed Tamil text.\nIt obtained a weighted F1 score of 0.87 (3rd Rank) on the code-mixed Tamil\nTwitter dataset.",
          "link": "http://arxiv.org/abs/2107.14336",
          "publishedOn": "2021-08-02T01:58:23.087Z",
          "wordCount": 525,
          "title": "IIITG-ADBU@HASOC-Dravidian-CodeMix-FIRE2020: Offensive Content Detection in Code-Mixed Dravidian Text. (arXiv:2107.14336v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">GuoLiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyang Li</a>",
          "description": "Although attention-based Neural Machine Translation has achieved remarkable\nprogress in recent layers, it still suffers from issue of making insufficient\nuse of the output of each layer. In transformer, it only uses the top layer of\nencoder and decoder in the subsequent process, which makes it impossible to\ntake advantage of the useful information in other layers. To address this\nissue, we propose a residual tree aggregation of layers for Transformer(RTAL),\nwhich helps to fuse information across layers. Specifically, we try to fuse the\ninformation across layers by constructing a post-order binary tree. In\nadditional to the last node, we add the residual connection to the process of\ngenerating child nodes. Our model is based on the Neural Machine Translation\nmodel Transformer and we conduct our experiments on WMT14 English-to-German and\nWMT17 English-to-France translation tasks. Experimental results across language\npairs show that the proposed approach outperforms the strong baseline model\nsignificantly",
          "link": "http://arxiv.org/abs/2107.14590",
          "publishedOn": "2021-08-02T01:58:23.066Z",
          "wordCount": 584,
          "title": "Residual Tree Aggregation of Layers for Neural Machine Translation. (arXiv:2107.14590v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Apostolova_E/0/1/0/all/0/1\">Emilia Apostolova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karim_F/0/1/0/all/0/1\">Fazle Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muscioni_G/0/1/0/all/0/1\">Guido Muscioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_A/0/1/0/all/0/1\">Anubhav Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clyman_J/0/1/0/all/0/1\">Jeffrey Clyman</a>",
          "description": "In this work, we modify and apply self-supervision techniques to the domain\nof medical health insurance claims. We model patients' healthcare claims\nhistory analogous to free-text narratives, and introduce pre-trained `prior\nknowledge', later utilized for patient outcome predictions on a challenging\ntask: predicting Covid-19 hospitalization, given a patient's pre-Covid-19\ninsurance claims history. Results suggest that pre-training on insurance claims\nnot only produces better prediction performance, but, more importantly,\nimproves the model's `clinical trustworthiness' and model\nstability/reliability.",
          "link": "http://arxiv.org/abs/2107.14591",
          "publishedOn": "2021-08-02T01:58:23.044Z",
          "wordCount": 569,
          "title": "Self-supervision for health insurance claims data: a Covid-19 use case. (arXiv:2107.14591v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lei Lin</a>",
          "description": "It is expensive to evaluate the results of Machine Translation(MT), which\nusually requires manual translation as a reference. Machine Translation Quality\nEstimation (QE) is a task of predicting the quality of machine translations\nwithout relying on any reference. Recently, the emergence of\npredictor-estimator framework which trains the predictor as a feature extractor\nand estimator as a QE predictor, and pre-trained language models(PLM) have\nachieved promising QE performance. However, we argue that there are still gaps\nbetween the predictor and the estimator in both data quality and training\nobjectives, which preclude QE models from benefiting from a large number of\nparallel corpora more directly. Based on previous related work that have\nalleviated gaps to some extent, we propose a novel framework that provides a\nmore accurate direct pretraining for QE tasks. In this framework, a generator\nis trained to produce pseudo data that is closer to the real QE data, and a\nestimator is pretrained on these data with novel objectives that are the same\nas the QE task. Experiments on widely used benchmarks show that our proposed\nframework outperforms existing methods, without using any pretraining models\nsuch as BERT.",
          "link": "http://arxiv.org/abs/2107.14600",
          "publishedOn": "2021-08-02T01:58:23.031Z",
          "wordCount": 632,
          "title": "MDQE: A More Accurate Direct Pretraining for Machine Translation Quality Estimation. (arXiv:2107.14600v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14586",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dupuy_C/0/1/0/all/0/1\">Christophe Dupuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arava_R/0/1/0/all/0/1\">Radhika Arava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>",
          "description": "Recent advances in deep learning have drastically improved performance on\nmany Natural Language Understanding (NLU) tasks. However, the data used to\ntrain NLU models may contain private information such as addresses or phone\nnumbers, particularly when drawn from human subjects. It is desirable that\nunderlying models do not expose private information contained in the training\ndata. Differentially Private Stochastic Gradient Descent (DP-SGD) has been\nproposed as a mechanism to build privacy-preserving models. However, DP-SGD can\nbe prohibitively slow to train. In this work, we propose a more efficient\nDP-SGD for training using a GPU infrastructure and apply it to fine-tuning\nmodels based on LSTM and transformer architectures. We report faster training\ntimes, alongside accuracy, theoretical privacy guarantees and success of\nMembership inference attacks for our models and observe that fine-tuning with\nproposed variant of DP-SGD can yield competitive models without significant\ndegradation in training time and improvement in privacy protection. We also\nmake observations such as looser theoretical $\\epsilon, \\delta$ can translate\ninto significant practical privacy gains.",
          "link": "http://arxiv.org/abs/2107.14586",
          "publishedOn": "2021-08-02T01:58:23.017Z",
          "wordCount": 611,
          "title": "An Efficient DP-SGD Mechanism for Large Scale NLP Models. (arXiv:2107.14586v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14419",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Musaev_M/0/1/0/all/0/1\">Muhammadjon Musaev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mussakhojayeva_S/0/1/0/all/0/1\">Saida Mussakhojayeva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khujayorov_I/0/1/0/all/0/1\">Ilyos Khujayorov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khassanov_Y/0/1/0/all/0/1\">Yerbolat Khassanov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ochilov_M/0/1/0/all/0/1\">Mannon Ochilov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varol_H/0/1/0/all/0/1\">Huseyin Atakan Varol</a>",
          "description": "We present a freely available speech corpus for the Uzbek language and report\npreliminary automatic speech recognition (ASR) results using both the deep\nneural network hidden Markov model (DNN-HMM) and end-to-end (E2E)\narchitectures. The Uzbek speech corpus (USC) comprises 958 different speakers\nwith a total of 105 hours of transcribed audio recordings. To the best of our\nknowledge, this is the first open-source Uzbek speech corpus dedicated to the\nASR task. To ensure high quality, the USC has been manually checked by native\nspeakers. We first describe the design and development procedures of the USC,\nand then explain the conducted ASR experiments in detail. The experimental\nresults demonstrate promising results for the applicability of the USC for ASR.\nSpecifically, 18.1% and 17.4% word error rates were achieved on the validation\nand test sets, respectively. To enable experiment reproducibility, we share the\nUSC dataset, pre-trained models, and training recipes in our GitHub repository.",
          "link": "http://arxiv.org/abs/2107.14419",
          "publishedOn": "2021-08-02T01:58:22.987Z",
          "wordCount": 616,
          "title": "USC: An Open-Source Uzbek Speech Corpus and Initial Speech Recognition Experiments. (arXiv:2107.14419v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Palen_Michel_C/0/1/0/all/0/1\">Chester Palen-Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holley_N/0/1/0/all/0/1\">Nolan Holley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lignos_C/0/1/0/all/0/1\">Constantine Lignos</a>",
          "description": "To address what we believe is a looming crisis of unreproducible evaluation\nfor named entity recognition tasks, we present guidelines for reproducible\nevaluation. The guidelines we propose are extremely simple, focusing on\ntransparency regarding how chunks are encoded and scored, but very few papers\ncurrently being published fully comply with them. We demonstrate that despite\nthe apparent simplicity of NER evaluation, unreported differences in the\nscoring procedure can result in changes to scores that are both of noticeable\nmagnitude and are statistically significant. We provide SeqScore, an open\nsource toolkit that addresses many of the issues that cause replication\nfailures and makes following our guidelines easy.",
          "link": "http://arxiv.org/abs/2107.14154",
          "publishedOn": "2021-07-30T02:13:27.900Z",
          "wordCount": 539,
          "title": "Addressing Barriers to Reproducible Named Entity Recognition Evaluation. (arXiv:2107.14154v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12470",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yadong Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jingzhen Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Duan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xiaoxi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zeng Zhao</a>",
          "description": "Recently, large-scale transformer-based models have been proven to be\neffective over a variety of tasks across many domains. Nevertheless, putting\nthem into production is very expensive, requiring comprehensive optimization\ntechniques to reduce inference costs. This paper introduces a series of\ntransformer inference optimization techniques that are both in algorithm level\nand hardware level. These techniques include a pre-padding decoding mechanism\nthat improves token parallelism for text generation, and highly optimized\nkernels designed for very long input length and large hidden size. On this\nbasis, we propose a transformer inference acceleration library -- Easy and\nEfficient Transformer (EET), which has a significant performance improvement\nover existing libraries. Compared to Faster Transformer v4.0's implementation\nfor GPT-2 layer on A100, EET achieves a 1.5-4.5x state-of-art speedup varying\nwith different context lengths. EET is available at\nhttps://github.com/NetEase-FuXi/EET. A demo video is available at\nhttps://youtu.be/22UPcNGcErg.",
          "link": "http://arxiv.org/abs/2104.12470",
          "publishedOn": "2021-07-30T02:13:27.864Z",
          "wordCount": 627,
          "title": "Easy and Efficient Transformer : Scalable Inference Solution For large NLP model. (arXiv:2104.12470v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfson_T/0/1/0/all/0/1\">Tomer Wolfson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>",
          "description": "Recent efforts to create challenge benchmarks that test the abilities of\nnatural language understanding models have largely depended on human\nannotations. In this work, we introduce the \"Break, Perturb, Build\" (BPB)\nframework for automatic reasoning-oriented perturbation of question-answer\npairs. BPB represents a question by decomposing it into the reasoning steps\nthat are required to answer it, symbolically perturbs the decomposition, and\nthen generates new question-answer pairs. We demonstrate the effectiveness of\nBPB by creating evaluation sets for three reading comprehension (RC)\nbenchmarks, generating thousands of high-quality examples without human\nintervention. We evaluate a range of RC models on our evaluation sets, which\nreveals large performance gaps on generated examples compared to the original\ndata. Moreover, symbolic perturbations enable fine-grained analysis of the\nstrengths and limitations of models. Last, augmenting the training data with\nexamples generated by BPB helps close performance gaps, without any drop on the\noriginal data distribution.",
          "link": "http://arxiv.org/abs/2107.13935",
          "publishedOn": "2021-07-30T02:13:27.852Z",
          "wordCount": 585,
          "title": "Break, Perturb, Build: Automatic Perturbation of Reasoning Paths through Question Decomposition. (arXiv:2107.13935v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oka_Y/0/1/0/all/0/1\">Yui Oka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudoh_K/0/1/0/all/0/1\">Katsuhito Sudoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>",
          "description": "Non-autoregressive neural machine translation (NAT) usually employs\nsequence-level knowledge distillation using autoregressive neural machine\ntranslation (AT) as its teacher model. However, a NAT model often outputs\nshorter sentences than an AT model. In this work, we propose sequence-level\nknowledge distillation (SKD) using perturbed length-aware positional encoding\nand apply it to a student model, the Levenshtein Transformer. Our method\noutperformed a standard Levenshtein Transformer by 2.5 points in bilingual\nevaluation understudy (BLEU) at maximum in a WMT14 German to English\ntranslation. The NAT model output longer sentences than the baseline NAT\nmodels.",
          "link": "http://arxiv.org/abs/2107.13689",
          "publishedOn": "2021-07-30T02:13:27.825Z",
          "wordCount": 536,
          "title": "Using Perturbed Length-aware Positional Encoding for Non-autoregressive Neural Machine Translation. (arXiv:2107.13689v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.11820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1\">Devamanyu Hazarika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1\">Rishabh Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_S/0/1/0/all/0/1\">Samson Yu Bai Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1\">Pengfei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Romila Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Abhinaba Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhaya_N/0/1/0/all/0/1\">Niyati Chhaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>",
          "description": "We address the problem of recognizing emotion cause in conversations, define\ntwo novel sub-tasks of this problem, and provide a corresponding dialogue-level\ndataset, along with strong Transformer-based baselines. The dataset is\navailable at https://github.com/declare-lab/RECCON.\n\nIntroduction: Recognizing the cause behind emotions in text is a fundamental\nyet under-explored area of research in NLP. Advances in this area hold the\npotential to improve interpretability and performance in affect-based models.\nIdentifying emotion causes at the utterance level in conversations is\nparticularly challenging due to the intermingling dynamics among the\ninterlocutors.\n\nMethod: We introduce the task of Recognizing Emotion Cause in CONversations\nwith an accompanying dataset named RECCON, containing over 1,000 dialogues and\n10,000 utterance cause-effect pairs. Furthermore, we define different cause\ntypes based on the source of the causes, and establish strong Transformer-based\nbaselines to address two different sub-tasks on this dataset: causal span\nextraction and causal emotion entailment.\n\nResult: Our Transformer-based baselines, which leverage contextual\npre-trained embeddings, such as RoBERTa, outperform the state-of-the-art\nemotion cause extraction approaches\n\nConclusion: We introduce a new task highly relevant for (explainable)\nemotion-aware artificial intelligence: recognizing emotion cause in\nconversations, provide a new highly challenging publicly available\ndialogue-level dataset for this task, and give strong baseline results on this\ndataset.",
          "link": "http://arxiv.org/abs/2012.11820",
          "publishedOn": "2021-07-30T02:13:27.819Z",
          "wordCount": 705,
          "title": "Recognizing Emotion Cause in Conversations. (arXiv:2012.11820v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grunewald_S/0/1/0/all/0/1\">Stefan Gr&#xfc;newald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_A/0/1/0/all/0/1\">Annemarie Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhn_J/0/1/0/all/0/1\">Jonas Kuhn</a>",
          "description": "The introduction of pre-trained transformer-based contextualized word\nembeddings has led to considerable improvements in the accuracy of graph-based\nparsers for frameworks such as Universal Dependencies (UD). However, previous\nworks differ in various dimensions, including their choice of pre-trained\nlanguage models and whether they use LSTM layers. With the aims of\ndisentangling the effects of these choices and identifying a simple yet widely\napplicable architecture, we introduce STEPS, a new modular graph-based\ndependency parser. Using STEPS, we perform a series of analyses on the UD\ncorpora of a diverse set of languages. We find that the choice of pre-trained\nembeddings has by far the greatest impact on parser performance and identify\nXLM-R as a robust choice across the languages in our study. Adding LSTM layers\nprovides no benefits when using transformer-based embeddings. A multi-task\ntraining setup outputting additional UD features may contort results. Taking\nthese insights together, we propose a simple but widely applicable parser\narchitecture and configuration, achieving new state-of-the-art results (in\nterms of LAS) for 10 out of 12 diverse languages.",
          "link": "http://arxiv.org/abs/2010.12699",
          "publishedOn": "2021-07-30T02:13:27.771Z",
          "wordCount": 665,
          "title": "Applying Occam's Razor to Transformer-Based Dependency Parsing: What Works, What Doesn't, and What is Really Necessary. (arXiv:2010.12699v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01995",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Paniagua_V/0/1/0/all/0/1\">V&#xed;ctor Su&#xe1;rez-Paniagua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huayu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitfield_E/0/1/0/all/0/1\">Emma Whitfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghan Wu</a>",
          "description": "The identification of rare diseases from clinical notes with Natural Language\nProcessing (NLP) is challenging due to the few cases available for machine\nlearning and the need of data annotation from clinical experts. We propose a\nmethod using ontologies and weak supervision. The approach includes two steps:\n(i) Text-to-UMLS, linking text mentions to concepts in Unified Medical Language\nSystem (UMLS), with a named entity linking tool (e.g. SemEHR) and weak\nsupervision based on customised rules and Bidirectional Encoder Representations\nfrom Transformers (BERT) based contextual representations, and (ii)\nUMLS-to-ORDO, matching UMLS concepts to rare diseases in Orphanet Rare Disease\nOntology (ORDO). Using MIMIC-III US intensive care discharge summaries as a\ncase study, we show that the Text-to-UMLS process can be greatly improved with\nweak supervision, without any annotated data from domain experts. Our analysis\nshows that the overall pipeline processing discharge summaries can surface rare\ndisease cases, which are mostly uncaptured in manual ICD codes of the hospital\nadmissions.",
          "link": "http://arxiv.org/abs/2105.01995",
          "publishedOn": "2021-07-30T02:13:27.757Z",
          "wordCount": 656,
          "title": "Rare Disease Identification from Clinical Notes with Ontologies and Weak Supervision. (arXiv:2105.01995v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomerantz_R/0/1/0/all/0/1\">Roi Pomerantz</a>",
          "description": "We present Translatotron 2, a neural direct speech-to-speech translation\nmodel that can be trained end-to-end. Translatotron 2 consists of a speech\nencoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention\nmodule that connects all the previous three components. Experimental results\nsuggest that Translatotron 2 outperforms the original Translatotron by a large\nmargin in terms of translation quality and predicted speech naturalness, and\ndrastically improves the robustness of the predicted speech by mitigating\nover-generation, such as babbling or long pause. We also propose a new method\nfor retaining the source speaker's voice in the translated speech. The trained\nmodel is restricted to retain the source speaker's voice, and unlike the\noriginal Translatotron, it is not able to generate speech in a different\nspeaker's voice, making the model more robust for production deployment, by\nmitigating potential misuse for creating spoofing audio artifacts. When the new\nmethod is used together with a simple concatenation-based data augmentation,\nthe trained Translatotron 2 model is able to retain each speaker's voice for\ninput with speaker turns.",
          "link": "http://arxiv.org/abs/2107.08661",
          "publishedOn": "2021-07-30T02:13:27.734Z",
          "wordCount": 631,
          "title": "Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Clouatre_L/0/1/0/all/0/1\">Louis Clouatre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1\">Prasanna Parthasarathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zouaq_A/0/1/0/all/0/1\">Amal Zouaq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>",
          "description": "Recent research analyzing the sensitivity of natural language understanding\nmodels to word-order perturbations have shown that the state-of-the-art models\nin several language tasks may have a unique way to understand the text that\ncould seldom be explained with conventional syntax and semantics. In this\npaper, we investigate the insensitivity of natural language models to\nword-order by quantifying perturbations and analysing their effect on neural\nmodels' performance on language understanding tasks in GLUE benchmark. Towards\nthat end, we propose two metrics - the Direct Neighbour Displacement (DND) and\nthe Index Displacement Count (IDC) - that score the local and global ordering\nof tokens in the perturbed texts and observe that perturbation functions found\nin prior literature affect only the global ordering while the local ordering\nremains relatively unperturbed. We propose perturbations at the granularity of\nsub-words and characters to study the correlation between DND, IDC and the\nperformance of neural language models on natural language tasks. We find that\nneural language models - pretrained and non-pretrained Transformers, LSTMs, and\nConvolutional architectures - require local ordering more so than the global\nordering of tokens. The proposed metrics and the suite of perturbations allow a\nsystematic way to study the (in)sensitivity of neural language understanding\nmodels to varying degree of perturbations.",
          "link": "http://arxiv.org/abs/2107.13955",
          "publishedOn": "2021-07-30T02:13:27.560Z",
          "wordCount": 648,
          "title": "Demystifying Neural Language Models' Insensitivity to Word-Order. (arXiv:2107.13955v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1\">Ankush Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sohom Ghosh</a>",
          "description": "Hypernym and synonym matching are one of the mainstream Natural Language\nProcessing (NLP) tasks. In this paper, we present systems that attempt to solve\nthis problem. We designed these systems to participate in the FinSim-3, a\nshared task of FinNLP workshop at IJCAI-2021. The shared task is focused on\nsolving this problem for the financial domain. We experimented with various\ntransformer based pre-trained embeddings by fine-tuning these for either\nclassification or phrase similarity tasks. We also augmented the provided\ndataset with abbreviations derived from prospectus provided by the organizers\nand definitions of the financial terms from DBpedia [Auer et al., 2007],\nInvestopedia, and the Financial Industry Business Ontology (FIBO). Our best\nperforming system uses both FinBERT [Araci, 2019] and data augmentation from\nthe afore-mentioned sources. We observed that term expansion using data\naugmentation in conjunction with semantic similarity is beneficial for this\ntask and could be useful for the other tasks that deal with short phrases. Our\nbest performing model (Accuracy: 0.917, Rank: 1.156) was developed by\nfine-tuning SentenceBERT [Reimers et al., 2019] (with FinBERT at the backend)\nover an extended labelled set created using the hierarchy of labels present in\nFIBO.",
          "link": "http://arxiv.org/abs/2107.13764",
          "publishedOn": "2021-07-30T02:13:27.536Z",
          "wordCount": 654,
          "title": "Term Expansion and FinBERT fine-tuning for Hypernym and Synonym Ranking of Financial Terms. (arXiv:2107.13764v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.05680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nortje_L/0/1/0/all/0/1\">Leanne Nortje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>",
          "description": "We propose direct multimodal few-shot models that learn a shared embedding\nspace of spoken words and images from only a few paired examples. Imagine an\nagent is shown an image along with a spoken word describing the object in the\npicture, e.g. pen, book and eraser. After observing a few paired examples of\neach class, the model is asked to identify the \"book\" in a set of unseen\npictures. Previous work used a two-step indirect approach relying on learned\nunimodal representations: speech-speech and image-image comparisons are\nperformed across the support set of given speech-image pairs. We propose two\ndirect models which instead learn a single multimodal space where inputs from\ndifferent modalities are directly comparable: a multimodal triplet network\n(MTriplet) and a multimodal correspondence autoencoder (MCAE). To train these\ndirect models, we mine speech-image pairs: the support set is used to pair up\nunlabelled in-domain speech and images. In a speech-to-image digit matching\ntask, direct models outperform indirect models, with the MTriplet achieving the\nbest multimodal five-shot accuracy. We show that the improvements are due to\nthe combination of unsupervised and transfer learning in the direct models, and\nthe absence of two-step compounding errors.",
          "link": "http://arxiv.org/abs/2012.05680",
          "publishedOn": "2021-07-30T02:13:27.511Z",
          "wordCount": 664,
          "title": "Direct multimodal few-shot learning of speech and images. (arXiv:2012.05680v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.11485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zehong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Son Tran</a>",
          "description": "Word embeddings can reflect the semantic representations, and the embedding\nqualities can be comprehensively evaluated with human natural reading-related\ncognitive data sources. In this paper, we proposed the CogniFNN framework,\nwhich is the first attempt at using fuzzy neural networks to extract non-linear\nand non-stationary characteristics for evaluations of English word embeddings\nagainst the corresponding cognitive datasets. In our experiment, we used 15\nhuman cognitive datasets across three modalities: EEG, fMRI, and eye-tracking,\nand selected the mean square error and multiple hypotheses testing as metrics\nto evaluate our proposed CogniFNN framework. Compared to the recent pioneer\nframework, our proposed CogniFNN showed smaller prediction errors of both\ncontext-independent (GloVe) and context-sensitive (BERT) word embeddings, and\nachieved higher significant ratios with randomly generated word embeddings. Our\nfindings suggested that the CogniFNN framework could provide a more accurate\nand comprehensive evaluation of cognitive word embeddings. It will potentially\nbe beneficial to the further word embeddings evaluation on extrinsic natural\nlanguage processing tasks.",
          "link": "http://arxiv.org/abs/2009.11485",
          "publishedOn": "2021-07-30T02:13:27.470Z",
          "wordCount": 641,
          "title": "CogniFNN: A Fuzzy Neural Network Framework for Cognitive Word Embedding Evaluation. (arXiv:2009.11485v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13586",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weizhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengbao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1\">Hiroaki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "This paper surveys and organizes research works in a new paradigm in natural\nlanguage processing, which we dub \"prompt-based learning\". Unlike traditional\nsupervised learning, which trains a model to take in an input x and predict an\noutput y as P(y|x), prompt-based learning is based on language models that\nmodel the probability of text directly. To use these models to perform\nprediction tasks, the original input x is modified using a template into a\ntextual string prompt x' that has some unfilled slots, and then the language\nmodel is used to probabilistically fill the unfilled information to obtain a\nfinal string x, from which the final output y can be derived. This framework is\npowerful and attractive for a number of reasons: it allows the language model\nto be pre-trained on massive amounts of raw text, and by defining a new\nprompting function the model is able to perform few-shot or even zero-shot\nlearning, adapting to new scenarios with few or no labeled data. In this paper\nwe introduce the basics of this promising paradigm, describe a unified set of\nmathematical notations that can cover a wide variety of existing work, and\norganize existing work along several dimensions, e.g.the choice of pre-trained\nmodels, prompts, and tuning strategies. To make the field more accessible to\ninterested beginners, we not only make a systematic review of existing works\nand a highly structured typology of prompt-based concepts, but also release\nother resources, e.g., a website this http URL including\nconstantly-updated survey, and paperlist.",
          "link": "http://arxiv.org/abs/2107.13586",
          "publishedOn": "2021-07-30T02:13:27.208Z",
          "wordCount": 710,
          "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. (arXiv:2107.13586v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yichun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>",
          "description": "Pre-trained language models (PLMs) have achieved great success in natural\nlanguage processing. Most of PLMs follow the default setting of architecture\nhyper-parameters (e.g., the hidden dimension is a quarter of the intermediate\ndimension in feed-forward sub-networks) in BERT (Devlin et al., 2019). Few\nstudies have been conducted to explore the design of architecture\nhyper-parameters in BERT, especially for the more efficient PLMs with tiny\nsizes, which are essential for practical deployment on resource-constrained\ndevices. In this paper, we adopt the one-shot Neural Architecture Search (NAS)\nto automatically search architecture hyper-parameters. Specifically, we\ncarefully design the techniques of one-shot learning and the search space to\nprovide an adaptive and efficient development way of tiny PLMs for various\nlatency constraints. We name our method AutoTinyBERT and evaluate its\neffectiveness on the GLUE and SQuAD benchmarks. The extensive experiments show\nthat our method outperforms both the SOTA search-based baseline (NAS-BERT) and\nthe SOTA distillation-based methods (such as DistilBERT, TinyBERT, MiniLM and\nMobileBERT). In addition, based on the obtained architectures, we propose a\nmore efficient development method that is even faster than the development of a\nsingle PLM.",
          "link": "http://arxiv.org/abs/2107.13686",
          "publishedOn": "2021-07-30T02:13:27.181Z",
          "wordCount": 639,
          "title": "AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models. (arXiv:2107.13686v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anchit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1\">Aleksandra Piktus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sonal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>",
          "description": "Pre-training on larger datasets with ever increasing model size is now a\nproven recipe for increased performance across almost all NLP tasks. A notable\nexception is information retrieval, where additional pre-training has so far\nfailed to produce convincing results. We show that, with the right pre-training\nsetup, this barrier can be overcome. We demonstrate this by pre-training large\nbi-encoder models on 1) a recently released set of 65 million synthetically\ngenerated questions, and 2) 200 million post-comment pairs from a preexisting\ndataset of Reddit conversations made available by pushshift.io. We evaluate on\na set of information retrieval and dialogue retrieval benchmarks, showing\nsubstantial improvements over supervised baselines.",
          "link": "http://arxiv.org/abs/2107.13602",
          "publishedOn": "2021-07-30T02:13:27.152Z",
          "wordCount": 554,
          "title": "Domain-matched Pre-training Tasks for Dense Retrieval. (arXiv:2107.13602v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vasquez_Rodriguez_L/0/1/0/all/0/1\">Laura V&#xe1;squez-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shardlow_M/0/1/0/all/0/1\">Matthew Shardlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Przybyla_P/0/1/0/all/0/1\">Piotr Przyby&#x142;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>",
          "description": "Modern text simplification (TS) heavily relies on the availability of gold\nstandard data to build machine learning models. However, existing studies show\nthat parallel TS corpora contain inaccurate simplifications and incorrect\nalignments. Additionally, evaluation is usually performed by using metrics such\nas BLEU or SARI to compare system output to the gold standard. A major\nlimitation is that these metrics do not match human judgements and the\nperformance on different datasets and linguistic phenomena vary greatly.\nFurthermore, our research shows that the test and training subsets of parallel\ndatasets differ significantly. In this work, we investigate existing TS\ncorpora, providing new insights that will motivate the improvement of existing\nstate-of-the-art TS evaluation methods. Our contributions include the analysis\nof TS corpora based on existing modifications used for simplification and an\nempirical study on TS models performance by using better-distributed datasets.\nWe demonstrate that by improving the distribution of TS datasets, we can build\nmore robust TS models.",
          "link": "http://arxiv.org/abs/2107.13662",
          "publishedOn": "2021-07-30T02:13:27.101Z",
          "wordCount": 601,
          "title": "Investigating Text Simplification Evaluation. (arXiv:2107.13662v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nurce_E/0/1/0/all/0/1\">Erida Nurce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keci_J/0/1/0/all/0/1\">Jorgel Keci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>",
          "description": "The ever growing usage of social media in the recent years has had a direct\nimpact on the increased presence of hate speech and offensive speech in online\nplatforms. Research on effective detection of such content has mainly focused\non English and a few other widespread languages, while the leftover majority\nfail to have the same work put into them and thus cannot benefit from the\nsteady advancements made in the field. In this paper we present \\textsc{Shaj},\nan annotated Albanian dataset for hate speech and offensive speech that has\nbeen constructed from user-generated content on various social media platforms.\nIts annotation follows the hierarchical schema introduced in OffensEval. The\ndataset is tested using three different classification models, the best of\nwhich achieves an F1 score of 0.77 for the identification of offensive\nlanguage, 0.64 F1 score for the automatic categorization of offensive types and\nlastly, 0.52 F1 score for the offensive language target identification.",
          "link": "http://arxiv.org/abs/2107.13592",
          "publishedOn": "2021-07-30T02:13:27.042Z",
          "wordCount": 573,
          "title": "Detecting Abusive Albanian. (arXiv:2107.13592v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-29T02:00:07.674Z",
          "wordCount": 700,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundgaard_K/0/1/0/all/0/1\">Keld T. Lundgaard</a>",
          "description": "By leveraging large amounts of product data collected across hundreds of live\ne-commerce websites, we construct 1000 unique classification tasks that share\nsimilarly-structured input data, comprised of both text and images. These\nclassification tasks focus on learning the product hierarchy of different\ne-commerce websites, causing many of them to be correlated. Adopting a\nmulti-modal transformer model, we solve these tasks in unison using multi-task\nlearning (MTL). Extensive experiments are presented over an initial 100-task\ndataset to reveal best practices for \"large-scale MTL\" (i.e., MTL with more\nthan 100 tasks). From these experiments, a final, unified methodology is\nderived, which is composed of both best practices and new proposals such as\nDyPa, a simple heuristic for automatically allocating task-specific parameters\nto tasks that could benefit from extra capacity. Using our large-scale MTL\nmethodology, we successfully train a single model across all 1000 tasks in our\ndataset while using minimal task specific parameters, thereby showing that it\nis possible to extend several orders of magnitude beyond current efforts in\nMTL.",
          "link": "http://arxiv.org/abs/2107.13054",
          "publishedOn": "2021-07-29T02:00:07.655Z",
          "wordCount": 623,
          "title": "Exceeding the Limits of Visual-Linguistic Multi-Task Learning. (arXiv:2107.13054v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2104.04039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>",
          "description": "Large pre-trained neural language models (LM) have very powerful text\ngeneration capabilities. However, in practice, they are hard to control for\ncreative purposes. We describe a Plug-and-Play controllable language generation\nframework, Plug-and-Blend, that allows a human user to input multiple control\ncodes (topics). In the context of automated story generation, this allows a\nhuman user loose or fine-grained control of the topics and transitions between\nthem that will appear in the generated story, and can even allow for\noverlapping, blended topics. Automated evaluations show our framework, working\nwith different generative LMs, controls the generation towards given\ncontinuous-weighted control codes while keeping the generated sentences fluent,\ndemonstrating strong blending capability. A human participant evaluation shows\nthat the generated stories are observably transitioning between two topics.",
          "link": "http://arxiv.org/abs/2104.04039",
          "publishedOn": "2021-07-29T02:00:07.636Z",
          "wordCount": 601,
          "title": "Plug-and-Blend: A Framework for Controllable Story Generation with Blended Control Codes. (arXiv:2104.04039v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>",
          "description": "This paper argues that training GANs on local and non-local dependencies in\nspeech data offers insights into how deep neural networks discretize continuous\ndata and how symbolic-like rule-based morphophonological processes emerge in a\ndeep convolutional architecture. Acquisition of speech has recently been\nmodeled as a dependency between latent space and data generated by GANs in\nBegu\\v{s} (2020b; arXiv:2006.03965), who models learning of a simple local\nallophonic distribution. We extend this approach to test learning of local and\nnon-local phonological processes that include approximations of morphological\nprocesses. We further parallel outputs of the model to results of a behavioral\nexperiment where human subjects are trained on the data used for training the\nGAN network. Four main conclusions emerge: (i) the networks provide useful\ninformation for computational models of speech acquisition even if trained on a\ncomparatively small dataset of an artificial grammar learning experiment; (ii)\nlocal processes are easier to learn than non-local processes, which matches\nboth behavioral data in human subjects and typology in the world's languages.\nThis paper also proposes (iii) how we can actively observe the network's\nprogress in learning and explore the effect of training steps on learning\nrepresentations by keeping latent space constant across different training\nsteps. Finally, this paper shows that (iv) the network learns to encode the\npresence of a prefix with a single latent variable; by interpolating this\nvariable, we can actively observe the operation of a non-local phonological\nprocess. The proposed technique for retrieving learning representations has\ngeneral implications for our understanding of how GANs discretize continuous\nspeech data and suggests that rule-like generalizations in the training data\nare represented as an interaction between variables in the network's latent\nspace.",
          "link": "http://arxiv.org/abs/2009.12711",
          "publishedOn": "2021-07-29T02:00:07.628Z",
          "wordCount": 761,
          "title": "Local and non-local dependency learning and emergence of rule-like representations in speech data by Deep Convolutional Generative Adversarial Networks. (arXiv:2009.12711v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1\">Charaf Eddine Benarab</a>",
          "description": "Knowledge is acquired by humans through experience, and no boundary is set\nbetween the kinds of knowledge or skill levels we can achieve on different\ntasks at the same time. When it comes to Neural Networks, that is not the case,\nthe major breakthroughs in the field are extremely task and domain specific.\nVision and language are dealt with in separate manners, using separate methods\nand different datasets. In this work, we propose to use knowledge acquired by\nbenchmark Vision Models which are trained on ImageNet to help a much smaller\narchitecture learn to classify text. After transforming the textual data\ncontained in the IMDB dataset to gray scale images. An analysis of different\ndomains and the Transfer Learning method is carried out. Despite the challenge\nposed by the very different datasets, promising results are achieved. The main\ncontribution of this work is a novel approach which links large pretrained\nmodels on both language and vision to achieve state-of-the-art results in\ndifferent sub-fields from the original task. Without needing high compute\ncapacity resources. Specifically, Sentiment Analysis is achieved after\ntransferring knowledge between vision and language models. BERT embeddings are\ntransformed into grayscale images, these images are then used as training\nexamples for pre-trained vision models such as VGG16 and ResNet\n\nIndex Terms: BERT, Convolutional Neural Networks, Domain Adaptation, image\nclassification, Natural Language Processing, t-SNE, text classification,\nTransfer Learning",
          "link": "http://arxiv.org/abs/2106.12479",
          "publishedOn": "2021-07-29T02:00:07.616Z",
          "wordCount": 706,
          "title": "Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heeringa_W/0/1/0/all/0/1\">Wilbert Heeringa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouma_G/0/1/0/all/0/1\">Gosse Bouma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofman_M/0/1/0/all/0/1\">Martha Hofman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drenth_E/0/1/0/all/0/1\">Eduard Drenth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijffels_J/0/1/0/all/0/1\">Jan Wijffels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velde_H/0/1/0/all/0/1\">Hans Van de Velde</a>",
          "description": "We present a lemmatizer/POS-tagger/dependency parser for West Frisian using a\ncorpus of 44,714 words in 3,126 sentences that were annotated according to the\nguidelines of Universal Dependency version 2. POS tags were assigned to words\nby using a Dutch POS tagger that was applied to a literal word-by-word\ntranslation, or to sentences of a Dutch parallel text. Best results were\nobtained when using literal translations that were created by using the Frisian\ntranslation program Oersetter. Morphologic and syntactic annotations were\ngenerated on the basis of a literal Dutch translation as well. The performance\nof the lemmatizer/tagger/annotator when it was trained using default parameters\nwas compared to the performance that was obtained when using the parameter\nvalues that were used for training the LassySmall UD 2.5 corpus. A significant\nimprovement was found for `lemma'. The Frisian lemmatizer/PoS tagger/dependency\nparser is released as a web app and as a web service.",
          "link": "http://arxiv.org/abs/2107.07974",
          "publishedOn": "2021-07-29T02:00:07.608Z",
          "wordCount": 623,
          "title": "POS tagging, lemmatization and dependency parsing of West Frisian. (arXiv:2107.07974v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1\">Keunwoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>",
          "description": "We propose a multimodal singing language classification model that uses both\naudio content and textual metadata. LRID-Net, the proposed model, takes an\naudio signal and a language probability vector estimated from the metadata and\noutputs the probabilities of the target languages. Optionally, LRID-Net is\nfacilitated with modality dropouts to handle a missing modality. In the\nexperiment, we trained several LRID-Nets with varying modality dropout\nconfiguration and tested them with various combinations of input modalities.\nThe experiment results demonstrate that using multimodal input improves\nperformance. The results also suggest that adopting modality dropout does not\ndegrade the performance of the model when there are full modality inputs while\nenabling the model to handle missing modality cases to some extent.",
          "link": "http://arxiv.org/abs/2103.01893",
          "publishedOn": "2021-07-29T02:00:07.589Z",
          "wordCount": 607,
          "title": "Listen, Read, and Identify: Multimodal Singing Language Identification of Music. (arXiv:2103.01893v4 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdelgwad_M/0/1/0/all/0/1\">Mohammed M.Abdelgwad</a>",
          "description": "Aspect-based sentiment analysis(ABSA) is a textual analysis methodology that\ndefines the polarity of opinions on certain aspects related to specific\ntargets. The majority of research on ABSA is in English, with a small amount of\nwork available in Arabic. Most previous Arabic research has relied on deep\nlearning models that depend primarily on context-independent word embeddings\n(e.g.word2vec), where each word has a fixed representation independent of its\ncontext. This article explores the modeling capabilities of contextual\nembeddings from pre-trained language models, such as BERT, and making use of\nsentence pair input on Arabic ABSA tasks. In particular, we are building a\nsimple but effective BERT-based neural baseline to handle this task. Our BERT\narchitecture with a simple linear classification layer surpassed the\nstate-of-the-art works, according to the experimental results on the\nbenchmarked Arabic hotel reviews dataset.",
          "link": "http://arxiv.org/abs/2107.13290",
          "publishedOn": "2021-07-29T02:00:07.582Z",
          "wordCount": 561,
          "title": "Arabic aspect based sentiment analysis using BERT. (arXiv:2107.13290v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chawla_K/0/1/0/all/0/1\">Kushal Chawla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clever_R/0/1/0/all/0/1\">Rene Clever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_J/0/1/0/all/0/1\">Jaysa Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_G/0/1/0/all/0/1\">Gale Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gratch_J/0/1/0/all/0/1\">Jonathan Gratch</a>",
          "description": "Negotiation is a complex social interaction that encapsulates emotional\nencounters in human decision-making. Virtual agents that can negotiate with\nhumans are useful in pedagogy and conversational AI. To advance the development\nof such agents, we explore the prediction of two important subjective goals in\na negotiation - outcome satisfaction and partner perception. Specifically, we\nanalyze the extent to which emotion attributes extracted from the negotiation\nhelp in the prediction, above and beyond the individual difference variables.\nWe focus on a recent dataset in chat-based negotiations, grounded in a\nrealistic camping scenario. We study three degrees of emotion dimensions -\nemoticons, lexical, and contextual by leveraging affective lexicons and a\nstate-of-the-art deep learning architecture. Our insights will be helpful in\ndesigning adaptive negotiation agents that interact through realistic\ncommunication interfaces.",
          "link": "http://arxiv.org/abs/2107.13165",
          "publishedOn": "2021-07-29T02:00:07.574Z",
          "wordCount": 579,
          "title": "Towards Emotion-Aware Agents For Negotiation Dialogues. (arXiv:2107.13165v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogozhnikov_A/0/1/0/all/0/1\">Alex Rogozhnikov</a>",
          "description": "Without positional information, attention-based transformer neural networks\nare permutation-invariant. Absolute or relative positional embeddings are the\nmost popular ways to feed transformer models positional information. Absolute\npositional embeddings are simple to implement, but suffer from generalization\nissues when evaluating on sequences of different length than those seen at\ntraining time. Relative positions are more robust to length change, but are\nmore complex to implement and yield inferior model throughput. In this paper,\nwe propose an augmentation-based approach (CAPE) for absolute positional\nembeddings, which keeps the advantages of both absolute (simplicity and speed)\nand relative position embeddings (better generalization). In addition, our\nempirical evaluation on state-of-the-art models in machine translation, image\nand speech recognition demonstrates that CAPE leads to better generalization\nperformance as well as increased stability with respect to training\nhyper-parameters.",
          "link": "http://arxiv.org/abs/2106.03143",
          "publishedOn": "2021-07-29T02:00:07.459Z",
          "wordCount": 607,
          "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings. (arXiv:2106.03143v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xinshuai Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1\">Anh Tuan Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>",
          "description": "Robustness against word substitutions has a well-defined and widely\nacceptable form, i.e., using semantically similar words as substitutions, and\nthus it is considered as a fundamental stepping-stone towards broader\nrobustness in natural language processing. Previous defense methods capture\nword substitutions in vector space by using either $l_2$-ball or\nhyper-rectangle, which results in perturbation sets that are not inclusive\nenough or unnecessarily large, and thus impedes mimicry of worst cases for\nrobust training. In this paper, we introduce a novel \\textit{Adversarial Sparse\nConvex Combination} (ASCC) method. We model the word substitution attack space\nas a convex hull and leverages a regularization term to enforce perturbation\ntowards an actual substitution, thus aligning our modeling better with the\ndiscrete textual space. Based on the ASCC method, we further propose\nASCC-defense, which leverages ASCC to generate worst-case perturbations and\nincorporates adversarial training towards robustness. Experiments show that\nASCC-defense outperforms the current state-of-the-arts in terms of robustness\non two prevailing NLP tasks, \\emph{i.e.}, sentiment analysis and natural\nlanguage inference, concerning several attacks across multiple model\narchitectures. Besides, we also envision a new class of defense towards\nrobustness in NLP, where our robustly trained word vectors can be plugged into\na normally trained model and enforce its robustness without applying any other\ndefense techniques.",
          "link": "http://arxiv.org/abs/2107.13541",
          "publishedOn": "2021-07-29T02:00:07.429Z",
          "wordCount": 644,
          "title": "Towards Robustness Against Natural Language Word Substitutions. (arXiv:2107.13541v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1\">Huaixiao Tou</a>",
          "description": "Recent years have witnessed the prosperity of legal artificial intelligence\nwith the development of technologies. In this paper, we propose a novel legal\napplication of legal provision prediction (LPP), which aims to predict the\nrelated legal provisions of affairs. We formulate this task as a challenging\nknowledge graph completion problem, which requires not only text understanding\nbut also graph reasoning. To this end, we propose a novel text-guided graph\nreasoning approach. We collect amounts of real-world legal provision data from\nthe Guangdong government service website and construct a legal dataset called\nLegalLPP. Extensive experimental results on the dataset show that our approach\nachieves better performance compared with baselines. The code and dataset are\navailable in \\url{https://github.com/zjunlp/LegalPP} for reproducibility.",
          "link": "http://arxiv.org/abs/2104.02284",
          "publishedOn": "2021-07-29T02:00:07.417Z",
          "wordCount": 591,
          "title": "Text-guided Legal Knowledge Graph Reasoning. (arXiv:2104.02284v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13530",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kessler_S/0/1/0/all/0/1\">Samuel Kessler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thomas_B/0/1/0/all/0/1\">Bethan Thomas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karout_S/0/1/0/all/0/1\">Salah Karout</a>",
          "description": "We present a method for continual learning of speech representations for\nmultiple languages using self-supervised learning (SSL) and applying these for\nautomatic speech recognition. There is an abundance of unannotated speech, so\ncreating self-supervised representations from raw audio and finetuning on a\nsmall annotated datasets is a promising direction to build speech recognition\nsystems. Wav2vec models perform SSL on raw audio in a pretraining phase and\nthen finetune on a small fraction of annotated data. SSL models have produced\nstate of the art results for ASR. However, these models are very expensive to\npretrain with self-supervision. We tackle the problem of learning new language\nrepresentations continually from audio without forgetting a previous language\nrepresentation. We use ideas from continual learning to transfer knowledge from\na previous task to speed up pretraining a new language task. Our\ncontinual-wav2vec2 model can decrease pretraining times by 32% when learning a\nnew language task, and learn this new audio-language representation without\nforgetting previous language representation.",
          "link": "http://arxiv.org/abs/2107.13530",
          "publishedOn": "2021-07-29T02:00:07.406Z",
          "wordCount": 634,
          "title": "Continual-wav2vec2: an Application of Continual Learning for Self-Supervised Automatic Speech Recognition. (arXiv:2107.13530v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_S/0/1/0/all/0/1\">Stephen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dras_M/0/1/0/all/0/1\">Mark Dras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Mark Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>",
          "description": "Sequence-to-Sequence (S2S) neural text generation models, especially the\npre-trained ones (e.g., BART and T5), have exhibited compelling performance on\nvarious natural language generation tasks. However, the black-box nature of\nthese models limits their application in tasks where specific rules (e.g.,\ncontrollable constraints, prior knowledge) need to be executed. Previous works\neither design specific model structure (e.g., Copy Mechanism corresponding to\nthe rule \"the generated output should include certain words in the source\ninput\") or implement specialized inference algorithm (e.g., Constrained Beam\nSearch) to execute particular rules through the text generation. These methods\nrequire careful design case-by-case and are difficult to support multiple rules\nconcurrently. In this paper, we propose a novel module named Neural\nRule-Execution Tracking Machine that can be equipped into various\ntransformer-based generators to leverage multiple rules simultaneously to guide\nthe neural generation model for superior generation performance in a unified\nand scalable way. Extensive experimental results on several benchmarks verify\nthe effectiveness of our proposed model in both controllable and general text\ngeneration.",
          "link": "http://arxiv.org/abs/2107.13077",
          "publishedOn": "2021-07-29T02:00:07.398Z",
          "wordCount": 605,
          "title": "Neural Rule-Execution Tracking Machine For Transformer-Based Text Generation. (arXiv:2107.13077v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tessler_M/0/1/0/all/0/1\">Michael Henry Tessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsividis_P/0/1/0/all/0/1\">Pedro A. Tsividis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madeano_J/0/1/0/all/0/1\">Jason Madeano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harper_B/0/1/0/all/0/1\">Brin Harper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>",
          "description": "Knowledge built culturally across generations allows humans to learn far more\nthan an individual could glean from their own experience in a lifetime.\nCultural knowledge in turn rests on language: language is the richest record of\nwhat previous generations believed, valued, and practiced. The power and\nmechanisms of language as a means of cultural learning, however, are not well\nunderstood. We take a first step towards reverse-engineering cultural learning\nthrough language. We developed a suite of complex high-stakes tasks in the form\nof minimalist-style video games, which we deployed in an iterated learning\nparadigm. Game participants were limited to only two attempts (two lives) to\nbeat each game and were allowed to write a message to a future participant who\nread the message before playing. Knowledge accumulated gradually across\ngenerations, allowing later generations to advance further in the games and\nperform more efficient actions. Multigenerational learning followed a\nstrikingly similar trajectory to individuals learning alone with an unlimited\nnumber of lives. These results suggest that language provides a sufficient\nmedium to express and accumulate the knowledge people acquire in these diverse\ntasks: the dynamics of the environment, valuable goals, dangerous risks, and\nstrategies for success. The video game paradigm we pioneer here is thus a rich\ntest bed for theories of cultural transmission and learning from language.",
          "link": "http://arxiv.org/abs/2107.13377",
          "publishedOn": "2021-07-29T02:00:07.388Z",
          "wordCount": 675,
          "title": "Growing knowledge culturally across generations to solve novel, complex tasks. (arXiv:2107.13377v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.06467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Andrew Yates</a>",
          "description": "The goal of text ranking is to generate an ordered list of texts retrieved\nfrom a corpus in response to a query. Although the most common formulation of\ntext ranking is search, instances of the task can also be found in many natural\nlanguage processing applications. This survey provides an overview of text\nranking with neural network architectures known as transformers, of which BERT\nis the best-known example. The combination of transformers and self-supervised\npretraining has been responsible for a paradigm shift in natural language\nprocessing (NLP), information retrieval (IR), and beyond. In this survey, we\nprovide a synthesis of existing work as a single point of entry for\npractitioners who wish to gain a better understanding of how to apply\ntransformers to text ranking problems and researchers who wish to pursue work\nin this area. We cover a wide range of modern techniques, grouped into two\nhigh-level categories: transformer models that perform reranking in multi-stage\narchitectures and dense retrieval techniques that perform ranking directly.\nThere are two themes that pervade our survey: techniques for handling long\ndocuments, beyond typical sentence-by-sentence processing in NLP, and\ntechniques for addressing the tradeoff between effectiveness (i.e., result\nquality) and efficiency (e.g., query latency, model and index size). Although\ntransformer architectures and pretraining techniques are recent innovations,\nmany aspects of how they are applied to text ranking are relatively well\nunderstood and represent mature techniques. However, there remain many open\nresearch questions, and thus in addition to laying out the foundations of\npretrained transformers for text ranking, this survey also attempts to\nprognosticate where the field is heading.",
          "link": "http://arxiv.org/abs/2010.06467",
          "publishedOn": "2021-07-29T02:00:07.368Z",
          "wordCount": 725,
          "title": "Pretrained Transformers for Text Ranking: BERT and Beyond. (arXiv:2010.06467v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>",
          "description": "In task-oriented conversation systems, natural language generation systems\nthat generate sentences with specific information related to conversation flow\nare useful. Our study focuses on language generation by considering various\ninformation representing the meaning of utterances as multiple conditions of\ngeneration. NLG from meaning representations, the conditions for sentence\nmeaning, generally goes through two steps: sentence planning and surface\nrealization. However, we propose a simple one-stage framework to generate\nutterances directly from MR (Meaning Representation). Our model is based on\nGPT2 and generates utterances with flat conditions on slot and value pairs,\nwhich does not need to determine the structure of the sentence. We evaluate\nseveral systems in the E2E dataset with 6 automatic metrics. Our system is a\nsimple method, but it demonstrates comparable performance to previous systems\nin automated metrics. In addition, using only 10\\% of the data set without any\nother techniques, our model achieves comparable performance, and shows the\npossibility of performing zero-shot generation and expanding to other datasets.",
          "link": "http://arxiv.org/abs/2101.04257",
          "publishedOn": "2021-07-29T02:00:07.354Z",
          "wordCount": 616,
          "title": "Transforming Multi-Conditioned Generation from Meaning Representation. (arXiv:2101.04257v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>",
          "description": "The knowledge of scripts, common chains of events in stereotypical scenarios,\nis a valuable asset for task-oriented natural language understanding systems.\nWe propose the Goal-Oriented Script Construction task, where a model produces a\nsequence of steps to accomplish a given goal. We pilot our task on the first\nmultilingual script learning dataset supporting 18 languages collected from\nwikiHow, a website containing half a million how-to articles. For baselines, we\nconsider both a generation-based approach using a language model and a\nretrieval-based approach by first retrieving the relevant steps from a large\ncandidate pool and then ordering them. We show that our task is practical,\nfeasible but challenging for state-of-the-art Transformer models, and that our\nmethods can be readily deployed for various other datasets and domains with\ndecent zero-shot performance.",
          "link": "http://arxiv.org/abs/2107.13189",
          "publishedOn": "2021-07-29T02:00:07.330Z",
          "wordCount": 562,
          "title": "Goal-Oriented Script Construction. (arXiv:2107.13189v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13425",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_T/0/1/0/all/0/1\">Tieyun Qian</a>",
          "description": "Existing methods in relation extraction have leveraged the lexical features\nin the word sequence and the syntactic features in the parse tree. Though\neffective, the lexical features extracted from the successive word sequence may\nintroduce some noise that has little or no meaningful content. Meanwhile, the\nsyntactic features are usually encoded via graph convolutional networks which\nhave restricted receptive field. To address the above limitations, we propose a\nmulti-scale feature and metric learning framework for relation extraction.\nSpecifically, we first develop a multi-scale convolutional neural network to\naggregate the non-successive mainstays in the lexical sequence. We also design\na multi-scale graph convolutional network which can increase the receptive\nfield towards specific syntactic roles. Moreover, we present a multi-scale\nmetric learning paradigm to exploit both the feature-level relation between\nlexical and syntactic features and the sample-level relation between instances\nwith the same or different classes. We conduct extensive experiments on three\nreal world datasets for various types of relation extraction tasks. The results\ndemonstrate that our model significantly outperforms the state-of-the-art\napproaches.",
          "link": "http://arxiv.org/abs/2107.13425",
          "publishedOn": "2021-07-29T02:00:07.322Z",
          "wordCount": 600,
          "title": "Multi-Scale Feature and Metric Learning for Relation Extraction. (arXiv:2107.13425v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_V/0/1/0/all/0/1\">Vivek Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witteveen_S/0/1/0/all/0/1\">Sam Witteveen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrews_M/0/1/0/all/0/1\">Martin Andrews</a>",
          "description": "Creating explanations for answers to science questions is a challenging task\nthat requires multi-hop inference over a large set of fact sentences. This\nyear, to refocus the Textgraphs Shared Task on the problem of gathering\nrelevant statements (rather than solely finding a single 'correct path'), the\nWorldTree dataset was augmented with expert ratings of 'relevance' of\nstatements to each overall explanation. Our system, which achieved second place\non the Shared Task leaderboard, combines initial statement retrieval; language\nmodels trained to predict the relevance scores; and ensembling of a number of\nthe resulting rankings. Our code implementation is made available at\nhttps://github.com/mdda/worldtree_corpus/tree/textgraphs_2021",
          "link": "http://arxiv.org/abs/2107.13031",
          "publishedOn": "2021-07-29T02:00:07.305Z",
          "wordCount": 570,
          "title": "Red Dragon AI at TextGraphs 2021 Shared Task: Multi-Hop Inference Explanation Regeneration by Matching Expert Ratings. (arXiv:2107.13031v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.02951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>",
          "description": "How can deep neural networks encode information that corresponds to words in\nhuman speech into raw acoustic data? This paper proposes two neural network\narchitectures for modeling unsupervised lexical learning from raw acoustic\ninputs, ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN),\nthat combine a Deep Convolutional GAN architecture for audio data (WaveGAN;\narXiv:1705.07904) with an information theoretic extension of GAN -- InfoGAN\n(arXiv:1606.03657), and propose a new latent space structure that can model\nfeatural learning simultaneously with a higher level classification and allows\nfor a very low-dimension vector representation of lexical items. Lexical\nlearning is modeled as emergent from an architecture that forces a deep neural\nnetwork to output data such that unique information is retrievable from its\nacoustic outputs. The networks trained on lexical items from TIMIT learn to\nencode unique information corresponding to lexical items in the form of\ncategorical variables in their latent space. By manipulating these variables,\nthe network outputs specific lexical items. The network occasionally outputs\ninnovative lexical items that violate training data, but are linguistically\ninterpretable and highly informative for cognitive modeling and neural network\ninterpretability. Innovative outputs suggest that phonetic and phonological\nrepresentations learned by the network can be productively recombined and\ndirectly paralleled to productivity in human speech: a fiwGAN network trained\non `suit' and `dark' outputs innovative `start', even though it never saw\n`start' or even a [st] sequence in the training data. We also argue that\nsetting latent featural codes to values well beyond training range results in\nalmost categorical generation of prototypical lexical items and reveals\nunderlying values of each latent code.",
          "link": "http://arxiv.org/abs/2006.02951",
          "publishedOn": "2021-07-29T02:00:07.279Z",
          "wordCount": 756,
          "title": "CiwGAN and fiwGAN: Encoding information in acoustic data to model lexical learning with Generative Adversarial Networks. (arXiv:2006.02951v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.12875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arrabales_R/0/1/0/all/0/1\">Ra&#xfa;l Arrabales</a>",
          "description": "Most depression assessment tools are based on self-report questionnaires,\nsuch as the Patient Health Questionnaire (PHQ-9). These psychometric\ninstruments can be easily adapted to an online setting by means of electronic\nforms. However, this approach lacks the interacting and engaging features of\nmodern digital environments. With the aim of making depression screening more\navailable, attractive and effective, we developed Perla, a conversational agent\nable to perform an interview based on the PHQ-9. We also conducted a validation\nstudy in which we compared the results obtained by the traditional self-report\nquestionnaire with Perla's automated interview. Analyzing the results from this\nstudy we draw two significant conclusions: firstly, Perla is much preferred by\nInternet users, achieving more than 2.5 times more reach than a traditional\nform-based questionnaire; secondly, her psychometric properties (Cronbach's\nalpha of 0.81, sensitivity of 96% and specificity of 90%) are excellent and\ncomparable to the traditional well-established depression screening\nquestionnaires.",
          "link": "http://arxiv.org/abs/2008.12875",
          "publishedOn": "2021-07-29T02:00:07.256Z",
          "wordCount": 634,
          "title": "Perla: A Conversational Agent for Depression Screening in Digital Ecosystems. Design, Implementation and Validation. (arXiv:2008.12875v2 [cs.CY] UPDATED)"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2108.00735",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Swijsen_L/0/1/0/all/0/1\">Lars Swijsen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Veken_J/0/1/0/all/0/1\">Joeri Van der Veken</a>, <a href=\"http://arxiv.org/find/math/1/au:+Vannieuwenhoven_N/0/1/0/all/0/1\">Nick Vannieuwenhoven</a>",
          "description": "We propose a Riemannian conjugate gradient (CG) optimization method for\nfinding low rank approximations of incomplete tensors. Our main contribution\nconsists of an explicit expression of the geodesics on the Segre manifold.\nThese are exploited in our algorithm to perform the retractions. We apply our\nmethod to movie rating predictions in a recommender system for the MovieLens\ndataset, and identification of pure fluorophores via fluorescent spectroscopy\nwith missing data. In this last application, we recover the tensor\ndecomposition from less than $10\\%$ of the data.",
          "link": "http://arxiv.org/abs/2108.00735",
          "publishedOn": "2021-08-03T02:06:29.491Z",
          "wordCount": 530,
          "title": "Tensor completion using geodesics on Segre manifolds. (arXiv:2108.00735v1 [math.DG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.09060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noufi_C/0/1/0/all/0/1\">Camille Noufi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Prateek Verma</a>",
          "description": "In music and speech, meaning is derived at multiple levels of context.\nAffect, for example, can be inferred both by a short sound token and by sonic\npatterns over a longer temporal window such as an entire recording. In this\nletter, we focus on inferring meaning from this dichotomy of contexts. We show\nhow contextual representations of short sung vocal lines can be implicitly\nlearned from fundamental frequency ($F_0$) and thus be used as a meaningful\nfeature space for downstream Music Information Retrieval (MIR) tasks. We\npropose three self-supervised deep learning paradigms which leverage pseudotask\nlearning of these two levels of context to produce latent representation\nspaces. We evaluate the usefulness of these representations by embedding unseen\npitch contours into each space and conducting downstream classification tasks.\nOur results show that contextual representation can enhance downstream\nclassification by as much as 15\\% as compared to using traditional statistical\ncontour features.",
          "link": "http://arxiv.org/abs/2007.09060",
          "publishedOn": "2021-08-03T02:06:29.478Z",
          "wordCount": 632,
          "title": "Self-Supervised Learning of Context-Aware Pitch Prosody Representations. (arXiv:2007.09060v4 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Harin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoeger_F/0/1/0/all/0/1\">Frank Hoeger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoenwiesner_M/0/1/0/all/0/1\">Marc Schoenwiesner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1\">Minsu Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1\">Nori Jacoby</a>",
          "description": "Do people from different cultural backgrounds perceive the mood in music the\nsame way? How closely do human ratings across different cultures approximate\nautomatic mood detection algorithms that are often trained on corpora of\npredominantly Western popular music? Analyzing 166 participants responses from\nBrazil, South Korea, and the US, we examined the similarity between the ratings\nof nine categories of perceived moods in music and estimated their alignment\nwith four popular mood detection algorithms. We created a dataset of 360 recent\npop songs drawn from major music charts of the countries and constructed\nsemantically identical mood descriptors across English, Korean, and Portuguese\nlanguages. Multiple participants from the three countries rated their\nfamiliarity, preference, and perceived moods for a given song. Ratings were\nhighly similar within and across cultures for basic mood attributes such as\nsad, cheerful, and energetic. However, we found significant cross-cultural\ndifferences for more complex characteristics such as dreamy and love. To our\nsurprise, the results of mood detection algorithms were uniformly correlated\nacross human ratings from all three countries and did not show a detectable\nbias towards any particular culture. Our study thus suggests that the mood\ndetection algorithms can be considered as an objective measure at least within\nthe popular music context.",
          "link": "http://arxiv.org/abs/2108.00768",
          "publishedOn": "2021-08-03T02:06:29.432Z",
          "wordCount": 676,
          "title": "Cross-cultural Mood Perception in Pop Songs and its Alignment with Mood Detection Algorithms. (arXiv:2108.00768v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2007.00814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1\">Omar Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>",
          "description": "Systems for Open-Domain Question Answering (OpenQA) generally depend on a\nretriever for finding candidate passages in a large corpus and a reader for\nextracting answers from those passages. In much recent work, the retriever is a\nlearned component that uses coarse-grained vector representations of questions\nand passages. We argue that this modeling choice is insufficiently expressive\nfor dealing with the complexity of natural language questions. To address this,\nwe define ColBERT-QA, which adapts the scalable neural retrieval model ColBERT\nto OpenQA. ColBERT creates fine-grained interactions between questions and\npassages. We propose an efficient weak supervision strategy that iteratively\nuses ColBERT to create its own training data. This greatly improves OpenQA\nretrieval on Natural Questions, SQuAD, and TriviaQA, and the resulting system\nattains state-of-the-art extractive OpenQA performance on all three datasets.",
          "link": "http://arxiv.org/abs/2007.00814",
          "publishedOn": "2021-08-03T02:06:29.392Z",
          "wordCount": 609,
          "title": "Relevance-guided Supervision for OpenQA with ColBERT. (arXiv:2007.00814v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>",
          "description": "It is widely acknowledged that learning joint embeddings of recipes with\nimages is challenging due to the diverse composition and deformation of\ningredients in cooking procedures. We present a Multi-modal Semantics enhanced\nJoint Embedding approach (MSJE) for learning a common feature space between the\ntwo modalities (text and image), with the ultimate goal of providing\nhigh-performance cross-modal retrieval services. Our MSJE approach has three\nunique features. First, we extract the TFIDF feature from the title,\ningredients and cooking instructions of recipes. By determining the\nsignificance of word sequences through combining LSTM learned features with\ntheir TFIDF features, we encode a recipe into a TFIDF weighted vector for\ncapturing significant key terms and how such key terms are used in the\ncorresponding cooking instructions. Second, we combine the recipe TFIDF feature\nwith the recipe sequence feature extracted through two-stage LSTM networks,\nwhich is effective in capturing the unique relationship between a recipe and\nits associated image(s). Third, we further incorporate TFIDF enhanced category\nsemantics to improve the mapping of image modality and to regulate the\nsimilarity loss function during the iterative learning of cross-modal joint\nembedding. Experiments on the benchmark dataset Recipe1M show the proposed\napproach outperforms the state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.00724",
          "publishedOn": "2021-08-03T02:06:29.377Z",
          "wordCount": 655,
          "title": "Learning TFIDF Enhanced Joint Embedding for Recipe-Image Cross-Modal Retrieval Service. (arXiv:2108.00724v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hansi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhichao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingyao Ai</a>",
          "description": "User and item reviews are valuable for the construction of recommender\nsystems. In general, existing review-based methods for recommendation can be\nbroadly categorized into two groups: the siamese models that build static user\nand item representations from their reviews respectively, and the\ninteraction-based models that encode user and item dynamically according to the\nsimilarity or relationships of their reviews. Although the interaction-based\nmodels have more model capacity and fit human purchasing behavior better,\nseveral problematic model designs and assumptions of the existing\ninteraction-based models lead to its suboptimal performance compared to\nexisting siamese models. In this paper, we identify three problems of the\nexisting interaction-based recommendation models and propose a couple of\nsolutions as well as a new interaction-based model to incorporate review data\nfor rating prediction. Our model implements a relevance matching model with\nregularized training losses to discover user relevant information from long\nitem reviews, and it also adapts a zero attention strategy to dynamically\nbalance the item-dependent and item-independent information extracted from user\nreviews. Empirical experiments and case studies on Amazon Product Benchmark\ndatasets show that our model can extract effective and interpretable user/item\nrepresentations from their reviews and outperforms multiple types of\nstate-of-the-art review-based recommendation models.",
          "link": "http://arxiv.org/abs/2101.06387",
          "publishedOn": "2021-08-03T02:06:29.266Z",
          "wordCount": 660,
          "title": "A Zero Attentive Relevance Matching Networkfor Review Modeling in Recommendation System. (arXiv:2101.06387v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.04695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitrevski_B/0/1/0/all/0/1\">Blagoj Mitrevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filipovic_M/0/1/0/all/0/1\">Milena Filipovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1\">Emma Lejal Glaude</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>",
          "description": "Multi-objective gradient methods are becoming the standard for solving\nmulti-objective problems. Among others, they show promising results in\ndeveloping multi-objective recommender systems with both correlated and\nconflicting objectives. Classic multi-gradient descent usually relies on the\ncombination of the gradients, not including the computation of first and second\nmoments of the gradients. This leads to a brittle behavior and misses important\nareas in the solution space. In this work, we create a multi-objective\nmodel-agnostic Adamize method that leverages the benefits of the Adam optimizer\nin single-objective problems. This corrects and stabilizes the gradients of\nevery objective before calculating a common gradient descent vector that\noptimizes all the objectives simultaneously. We evaluate the benefits of\nmulti-objective Adamize on two multi-objective recommender systems and for\nthree different objective combinations, both correlated or conflicting. We\nreport significant improvements, measured with three different Pareto front\nmetrics: hypervolume, coverage, and spacing. Finally, we show that the Adamized\nPareto front strictly dominates the previous one on multiple objective pairs.",
          "link": "http://arxiv.org/abs/2009.04695",
          "publishedOn": "2021-08-03T02:06:29.253Z",
          "wordCount": 643,
          "title": "Momentum-based Gradient Methods in Multi-Objective Recommendation. (arXiv:2009.04695v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grundmann_P/0/1/0/all/0/1\">Paul Grundmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_S/0/1/0/all/0/1\">Sebastian Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loser_A/0/1/0/all/0/1\">Alexander L&#xf6;ser</a>",
          "description": "Retrieving answer passages from long documents is a complex task requiring\nsemantic understanding of both discourse and document context. We approach this\nchallenge specifically in a clinical scenario, where doctors retrieve cohorts\nof patients based on diagnoses and other latent medical aspects. We introduce\nCAPR, a rule-based self-supervision objective for training Transformer language\nmodels for domain-specific passage matching. In addition, we contribute a novel\nretrieval dataset based on clinical notes to simulate this scenario on a large\ncorpus of clinical notes. We apply our objective in four Transformer-based\narchitectures: Contextual Document Vectors, Bi-, Poly- and Cross-encoders. From\nour extensive evaluation on MIMIC-III and three other healthcare datasets, we\nreport that CAPR outperforms strong baselines in the retrieval of\ndomain-specific passages and effectively generalizes across rule-based and\nhuman-labeled passages. This makes the model powerful especially in zero-shot\nscenarios where only limited training data is available.",
          "link": "http://arxiv.org/abs/2108.00775",
          "publishedOn": "2021-08-03T02:06:29.231Z",
          "wordCount": 572,
          "title": "Self-supervised Answer Retrieval on Clinical Notes. (arXiv:2108.00775v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1\">Jingtao Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiaxin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiqun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shaoping Ma</a>",
          "description": "Recently, Information Retrieval community has witnessed fast-paced advances\nin Dense Retrieval (DR), which performs first-stage retrieval by encoding\ndocuments in a low-dimensional embedding space and querying them with\nembedding-based search. Despite the impressive ranking performance, previous\nstudies usually adopt brute-force search to acquire candidates, which is\nprohibitive in practical Web search scenarios due to its tremendous memory\nusage and time cost. To overcome these problems, vector compression methods, a\nbranch of Approximate Nearest Neighbor Search (ANNS), have been adopted in many\npractical embedding-based retrieval applications. One of the most popular\nmethods is Product Quantization (PQ). However, although existing vector\ncompression methods including PQ can help improve the efficiency of DR, they\nincur severely decayed retrieval performance due to the separation between\nencoding and compression. To tackle this problem, we present JPQ, which stands\nfor Joint optimization of query encoding and Product Quantization. It trains\nthe query encoder and PQ index jointly in an end-to-end manner based on three\noptimization strategies, namely ranking-oriented loss, PQ centroid\noptimization, and end-to-end negative sampling. We evaluate JPQ on two publicly\navailable retrieval benchmarks. Experimental results show that JPQ\nsignificantly outperforms existing popular vector compression methods in terms\nof different trade-off settings. Compared with previous DR models that use\nbrute-force search, JPQ almost matches the best retrieval performance with 30x\ncompression on index size. The compressed index further brings 10x speedup on\nCPU and 2x speedup on GPU in query latency.",
          "link": "http://arxiv.org/abs/2108.00644",
          "publishedOn": "2021-08-03T02:06:29.167Z",
          "wordCount": 681,
          "title": "Jointly Optimizing Query Encoder and Product Quantization to Improve Retrieval Performance. (arXiv:2108.00644v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2011.07734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1\">Qihao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun Chen</a>",
          "description": "Recommendation from implicit feedback is a highly challenging task due to the\nlack of reliable negative feedback data. Existing methods address this\nchallenge by treating all the un-observed data as negative (dislike) but\ndownweight the confidence of these data. However, this treatment causes two\nproblems: (1) Confidence weights of the unobserved data are usually assigned\nmanually, which lack flexibility and may create empirical bias on evaluating\nuser's preference. (2) To handle massive volume of the unobserved feedback\ndata, most of the existing methods rely on stochastic inference and data\nsampling strategies. However, since a user is only aware of a very small\nfraction of items in a large dataset, it is difficult for existing samplers to\nselect informative training instances in which the user really dislikes the\nitem rather than does not know it.\n\nTo address the above two problems, we propose two novel recommendation\nmethods SamWalker and SamWalker++ that support both adaptive confidence\nassignment and efficient model learning. SamWalker models data confidence with\na social network-aware function, which can adaptively specify different weights\nto different data according to users' social contexts. However, the social\nnetwork information may not be available in many recommender systems, which\nhinders application of SamWalker. Thus, we further propose SamWalker++, which\ndoes not require any side information and models data confidence with a\nconstructed pseudo-social network. We also develop fast random-walk-based\nsampling strategies for our SamWalker and SamWalker++ to adaptively draw\ninformative training instances, which can speed up gradient estimation and\nreduce sampling variance. Extensive experiments on five real-world datasets\ndemonstrate the superiority of the proposed SamWalker and SamWalker++.",
          "link": "http://arxiv.org/abs/2011.07734",
          "publishedOn": "2021-08-03T02:06:29.135Z",
          "wordCount": 728,
          "title": "SamWalker++: recommendation with informative sampling strategy. (arXiv:2011.07734v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kihwan Kim</a>",
          "description": "All learning algorithms for recommendations face inevitable and critical\ntrade-off between exploiting partial knowledge of a user's preferences for\nshort-term satisfaction and exploring additional user preferences for long-term\ncoverage. Although exploration is indispensable for long success of a\nrecommender system, the exploration has been considered as the risk to decrease\nuser satisfaction. The reason for the risk is that items chosen for exploration\nfrequently mismatch with the user's interests. To mitigate this risk,\nrecommender systems have mixed items chosen for exploration into a\nrecommendation list, disguising the items as recommendations to elicit feedback\non the items to discover the user's additional tastes. This mix-in approach has\nbeen widely used in many recommenders, but there is rare research, evaluating\nthe effectiveness of the mix-in approach or proposing a new approach for\neliciting user feedback without deceiving users. In this work, we aim to\npropose a new approach for feedback elicitation without any deception and\ncompare our approach to the conventional mix-in approach for evaluation. To\nthis end, we designed a recommender interface that reveals which items are for\nexploration and conducted a within-subject study with 94 MTurk workers. Our\nresults indicated that users left significantly more feedback on items chosen\nfor exploration with our interface. Besides, users evaluated that our new\ninterface is better than the conventional mix-in interface in terms of novelty,\ndiversity, transparency, trust, and satisfaction. Finally, path analysis show\nthat, in only our new interface, exploration caused to increase user-centric\nevaluation metrics. Our work paves the way for how to design an interface,\nwhich utilizes learning algorithm based on users' feedback signals, giving\nbetter user experience and gathering more feedback data.",
          "link": "http://arxiv.org/abs/2108.00151",
          "publishedOn": "2021-08-03T02:06:28.832Z",
          "wordCount": 708,
          "title": "An Empirical analysis on Transparent Algorithmic Exploration in Recommender Systems. (arXiv:2108.00151v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moradi_E/0/1/0/all/0/1\">Ehsan Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_D/0/1/0/all/0/1\">Debajyoti Mondal</a>",
          "description": "Graph layouts are key to exploring massive graphs. An enormous number of\nnodes and edges do not allow network analysis software to produce meaningful\nvisualization of the pervasive networks. Long computation time, memory and\ndisplay limitations encircle the software's ability to explore massive graphs.\nThis paper introduces BigGraphVis, a new parallel graph visualization method\nthat uses GPU parallel processing and community detection algorithm to\nvisualize graph communities. We combine parallelized streaming community\ndetection algorithm and probabilistic data structure to leverage parallel\nprocessing of Graphics Processing Unit (GPU). To the best of our knowledge,\nthis is the first attempt to combine the power of streaming algorithms coupled\nwith GPU computing to tackle big graph visualization challenges. Our method\nextracts community information in a few passes on the edge list, and renders\nthe community structures using the ForceAtlas2 algorithm. Our experiment with\nmassive real-life graphs indicates that about 70 to 95 percent speedup can be\nachieved by visualizing graph communities, and the visualization appears to be\nmeaningful and reliable. The biggest graph that we examined contains above 3\nmillion nodes and 34 million edges, and the layout computation took about five\nminutes. We also observed that the BigGraphVis coloring strategy can be\nsuccessfully applied to produce a more informative ForceAtlas2 layout.",
          "link": "http://arxiv.org/abs/2108.00529",
          "publishedOn": "2021-08-03T02:06:28.800Z",
          "wordCount": 667,
          "title": "BigGraphVis: Leveraging Streaming Algorithms and GPU Acceleration for Visualizing Big Graphs. (arXiv:2108.00529v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Anshul Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_N/0/1/0/all/0/1\">Noveen Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sheshansh Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sumeet Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1\">Purushottam Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1\">Manik Varma</a>",
          "description": "Deep extreme classification (XC) seeks to train deep architectures that can\ntag a data point with its most relevant subset of labels from an extremely\nlarge label set. The core utility of XC comes from predicting labels that are\nrarely seen during training. Such rare labels hold the key to personalized\nrecommendations that can delight and surprise a user. However, the large number\nof rare labels and small amount of training data per rare label offer\nsignificant statistical and computational challenges. State-of-the-art deep XC\nmethods attempt to remedy this by incorporating textual descriptions of labels\nbut do not adequately address the problem. This paper presents ECLARE, a\nscalable deep learning architecture that incorporates not only label text, but\nalso label correlations, to offer accurate real-time predictions within a few\nmilliseconds. Core contributions of ECLARE include a frugal architecture and\nscalable techniques to train deep models along with label correlation graphs at\nthe scale of millions of labels. In particular, ECLARE offers predictions that\nare 2 to 14% more accurate on both publicly available benchmark datasets as\nwell as proprietary datasets for a related products recommendation task sourced\nfrom the Bing search engine. Code for ECLARE is available at\nhttps://github.com/Extreme-classification/ECLARE.",
          "link": "http://arxiv.org/abs/2108.00261",
          "publishedOn": "2021-08-03T02:06:28.775Z",
          "wordCount": 655,
          "title": "ECLARE: Extreme Classification with Label Graph Correlations. (arXiv:2108.00261v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Veretennikov_A/0/1/0/all/0/1\">Alexander B. Veretennikov</a>",
          "description": "The problem of proximity full-text search is considered. If a search query\ncontains high-frequently occurring words, then multi-component key indexes\ndeliver an improvement in the search speed compared with ordinary inverted\nindexes. It was shown that we can increase the search speed by up to 130 times\nin cases when queries consist of high-frequently occurring words. In this\npaper, we investigate how the multi-component key index architecture affects\nthe quality of the search. We consider several well-known methods of relevance\nranking, where these methods are of different authors. Using these methods, we\nperform the search in the ordinary inverted index and then in an index enhanced\nwith multi-component key indexes. The results show that with multi-component\nkey indexes we obtain search results that are very close, in terms of relevance\nranking, to the search results that are obtained by means of ordinary inverted\nindexes.",
          "link": "http://arxiv.org/abs/2108.00410",
          "publishedOn": "2021-08-03T02:06:28.765Z",
          "wordCount": 608,
          "title": "Relevance ranking for proximity full-text search based on additional indexes with multi-component keys. (arXiv:2108.00410v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Anshul Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahiya_K/0/1/0/all/0/1\">Kunal Dahiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sheshansh Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saini_D/0/1/0/all/0/1\">Deepak Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sumeet Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1\">Purushottam Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1\">Manik Varma</a>",
          "description": "Extreme multi-label classification (XML) involves tagging a data point with\nits most relevant subset of labels from an extremely large label set, with\nseveral applications such as product-to-product recommendation with millions of\nproducts. Although leading XML algorithms scale to millions of labels, they\nlargely ignore label meta-data such as textual descriptions of the labels. On\nthe other hand, classical techniques that can utilize label metadata via\nrepresentation learning using deep networks struggle in extreme settings. This\npaper develops the DECAF algorithm that addresses these challenges by learning\nmodels enriched by label metadata that jointly learn model parameters and\nfeature representations using deep networks and offer accurate classification\nat the scale of millions of labels. DECAF makes specific contributions to model\narchitecture design, initialization, and training, enabling it to offer up to\n2-6% more accurate prediction than leading extreme classifiers on publicly\navailable benchmark product-to-product recommendation datasets, such as\nLF-AmazonTitles-1.3M. At the same time, DECAF was found to be up to 22x faster\nat inference than leading deep extreme classifiers, which makes it suitable for\nreal-time applications that require predictions within a few milliseconds. The\ncode for DECAF is available at the following URL\nhttps://github.com/Extreme-classification/DECAF.",
          "link": "http://arxiv.org/abs/2108.00368",
          "publishedOn": "2021-08-03T02:06:28.738Z",
          "wordCount": 654,
          "title": "DECAF: Deep Extreme Classification with Label Features. (arXiv:2108.00368v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ulukus_S/0/1/0/all/0/1\">Sennur Ulukus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gastpar_M/0/1/0/all/0/1\">Michael Gastpar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafar_S/0/1/0/all/0/1\">Syed Jafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_R/0/1/0/all/0/1\">Ravi Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1\">Chao Tian</a>",
          "description": "Most of our lives are conducted in the cyberspace. The human notion of\nprivacy translates into a cyber notion of privacy on many functions that take\nplace in the cyberspace. This article focuses on three such functions: how to\nprivately retrieve information from cyberspace (privacy in information\nretrieval), how to privately leverage large-scale distributed/parallel\nprocessing (privacy in distributed computing), and how to learn/train machine\nlearning models from private data spread across multiple users (privacy in\ndistributed (federated) learning). The article motivates each privacy setting,\ndescribes the problem formulation, summarizes breakthrough results in the\nhistory of each problem, and gives recent results and discusses some of the\nmajor ideas that emerged in each field. In addition, the cross-cutting\ntechniques and interconnections between the three topics are discussed along\nwith a set of open problems and challenges.",
          "link": "http://arxiv.org/abs/2108.00026",
          "publishedOn": "2021-08-03T02:06:28.542Z",
          "wordCount": 591,
          "title": "Private Retrieval, Computing and Learning: Recent Progress and Future Challenges. (arXiv:2108.00026v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhishan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guohui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Dawei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>",
          "description": "As a critical component for online advertising and marking, click-through\nrate (CTR) prediction has draw lots of attentions from both industry and\nacademia field. Recently, the deep learning has become the mainstream\nmethodological choice for CTR. Despite of sustainable efforts have been made,\nexisting approaches still pose several challenges. On the one hand, high-order\ninteraction between the features is under-explored. On the other hand,\nhigh-order interactions may neglect the semantic information from the low-order\nfields. In this paper, we proposed a novel prediction method, named FINT, that\nemploys the Field-aware INTeraction layer which captures high-order feature\ninteractions while retaining the low-order field information. To empirically\ninvestigate the effectiveness and robustness of the FINT, we perform extensive\nexperiments on the three realistic databases: KDD2012, Criteo and Avazu. The\nobtained results demonstrate that the FINT can significantly improve the\nperformance compared to the existing methods, without increasing the amount of\ncomputation required. Moreover, the proposed method brought about 2.72\\%\nincrease to the advertising revenue of a big online video app through A/B\ntesting. To better promote the research in CTR field, we released our code as\nwell as reference implementation at: https://github.com/zhishan01/FINT.",
          "link": "http://arxiv.org/abs/2107.01999",
          "publishedOn": "2021-08-02T01:58:23.232Z",
          "wordCount": 654,
          "title": "FINT: Field-aware INTeraction Neural Network For CTR Prediction. (arXiv:2107.01999v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.08903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1\">Nan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sichen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_K/0/1/0/all/0/1\">Kyle Kai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabowo_A/0/1/0/all/0/1\">Arian Prabowo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Mohammad Saiedur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1\">Flora D. Salim</a>",
          "description": "Generative Adversarial Networks (GANs) have shown remarkable success in\nproducing realistic-looking images in the computer vision area. Recently,\nGAN-based techniques are shown to be promising for spatio-temporal-based\napplications such as trajectory prediction, events generation and time-series\ndata imputation. While several reviews for GANs in computer vision have been\npresented, no one has considered addressing the practical applications and\nchallenges relevant to spatio-temporal data. In this paper, we have conducted a\ncomprehensive review of the recent developments of GANs for spatio-temporal\ndata. We summarise the application of popular GAN architectures for\nspatio-temporal data and the common practices for evaluating the performance of\nspatio-temporal applications with GANs. Finally, we point out future research\ndirections to benefit researchers in this area.",
          "link": "http://arxiv.org/abs/2008.08903",
          "publishedOn": "2021-08-02T01:58:23.075Z",
          "wordCount": 636,
          "title": "Generative Adversarial Networks for Spatio-temporal Data: A Survey. (arXiv:2008.08903v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Damak_K/0/1/0/all/0/1\">Khalil Damak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khenissi_S/0/1/0/all/0/1\">Sami Khenissi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasraoui_O/0/1/0/all/0/1\">Olfa Nasraoui</a>",
          "description": "Recent work in recommender systems has emphasized the importance of fairness,\nwith a particular interest in bias and transparency, in addition to predictive\naccuracy. In this paper, we focus on the state of the art pairwise ranking\nmodel, Bayesian Personalized Ranking (BPR), which has previously been found to\noutperform pointwise models in predictive accuracy, while also being able to\nhandle implicit feedback. Specifically, we address two limitations of BPR: (1)\nBPR is a black box model that does not explain its outputs, thus limiting the\nuser's trust in the recommendations, and the analyst's ability to scrutinize a\nmodel's outputs; and (2) BPR is vulnerable to exposure bias due to the data\nbeing Missing Not At Random (MNAR). This exposure bias usually translates into\nan unfairness against the least popular items because they risk being\nunder-exposed by the recommender system. In this work, we first propose a novel\nexplainable loss function and a corresponding Matrix Factorization-based model\ncalled Explainable Bayesian Personalized Ranking (EBPR) that generates\nrecommendations along with item-based explanations. Then, we theoretically\nquantify additional exposure bias resulting from the explainability, and use it\nas a basis to propose an unbiased estimator for the ideal EBPR loss. The result\nis a ranking model that aptly captures both debiased and explainable user\npreferences. Finally, we perform an empirical study on three real-world\ndatasets that demonstrate the advantages of our proposed models.",
          "link": "http://arxiv.org/abs/2107.14768",
          "publishedOn": "2021-08-02T01:58:23.001Z",
          "wordCount": 695,
          "title": "Debiased Explainable Pairwise Ranking from Implicit Feedback. (arXiv:2107.14768v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carreon_E/0/1/0/all/0/1\">Elisa Claire Alem&#xe1;n Carre&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espana_H/0/1/0/all/0/1\">Hugo Alberto Mendoza Espa&#xf1;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nonaka_H/0/1/0/all/0/1\">Hirofumi Nonaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hiraoka_T/0/1/0/all/0/1\">Toru Hiraoka</a>",
          "description": "Since culture influences expectations, perceptions, and satisfaction, a\ncross-culture study is necessary to understand the differences between Japan's\nbiggest tourist populations, Chinese and Western tourists. However, with\never-increasing customer populations, this is hard to accomplish without\nextensive customer base studies. There is a need for an automated method for\nidentifying these expectations at a large scale. For this, we used a\ndata-driven approach to our analysis. Our study analyzed their satisfaction\nfactors comparing soft attributes, such as service, with hard attributes, such\nas location and facilities, and studied different price ranges. We collected\nhotel reviews and extracted keywords to classify the sentiment of sentences\nwith an SVC. We then used dependency parsing and part-of-speech tagging to\nextract nouns tied to positive adjectives. We found that Chinese tourists\nconsider room quality more than hospitality, whereas Westerners are delighted\nmore by staff behavior. Furthermore, the lack of a Chinese-friendly environment\nfor Chinese customers and cigarette smell for Western ones can be disappointing\nfactors of their stay. As one of the first studies in the tourism field to use\nthe high-standard Japanese hospitality environment for this analysis, our\ncross-cultural study contributes to both the theoretical understanding of\nsatisfaction and suggests practical applications and strategies for hotel\nmanagers.",
          "link": "http://arxiv.org/abs/2107.14681",
          "publishedOn": "2021-08-02T01:58:22.752Z",
          "wordCount": 662,
          "title": "Differences in Chinese and Western tourists faced with Japanese hospitality: A natural language processing approach. (arXiv:2107.14681v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korzeniowski_F/0/1/0/all/0/1\">Filip Korzeniowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oramas_S/0/1/0/all/0/1\">Sergio Oramas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouyon_F/0/1/0/all/0/1\">Fabien Gouyon</a>",
          "description": "Artist similarity plays an important role in organizing, understanding, and\nsubsequently, facilitating discovery in large collections of music. In this\npaper, we present a hybrid approach to computing similarity between artists\nusing graph neural networks trained with triplet loss. The novelty of using a\ngraph neural network architecture is to combine the topology of a graph of\nartist connections with content features to embed artists into a vector space\nthat encodes similarity. To evaluate the proposed method, we compile the new\nOLGA dataset, which contains artist similarities from AllMusic, together with\ncontent features from AcousticBrainz. With 17,673 artists, this is the largest\nacademic artist similarity dataset that includes content-based features to\ndate. Moreover, we also showcase the scalability of our approach by\nexperimenting with a much larger proprietary dataset. Results show the\nsuperiority of the proposed approach over current state-of-the-art methods for\nmusic similarity. Finally, we hope that the OLGA dataset will facilitate\nresearch on data-driven models for artist similarity.",
          "link": "http://arxiv.org/abs/2107.14541",
          "publishedOn": "2021-08-02T01:58:22.703Z",
          "wordCount": 614,
          "title": "Artist Similarity with Graph Neural Networks. (arXiv:2107.14541v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haokui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenze Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Buzhou Tang</a>",
          "description": "Approximate Nearest neighbor search (ANNS) plays a crucial role in\ninformation retrieval, which has a wide range of application scenarios.\nTherefore, during past several years, a lot of fast ANNS approaches have been\nproposed. Among these approaches, graph-based methods are one of the most\npopular type, as they have shown attractive theoretical guarantees and low\nquery latency. In this paper, we propose a learnable compression network with\ntransformer (LCNT), which projects feature vectors from high dimensional space\nonto low dimensional space, while preserving neighbor relationship. The\nproposed model can be generalized to existing graph-based methods to accelerate\nthe process of building indexing graph and further reduce query latency.\nSpecifically, the proposed LCNT contains two major parts, projection part and\nharmonizing part. In the projection part, input vectors are projected into a\nsequence of subspaces via multi channel sparse projection network. In the\nharmonizing part, a modified Transformer network is employed to harmonize\nfeatures in subspaces and combine them to get a new feature. To evaluate the\neffectiveness of the proposed model, we conduct experiments on two\nmillion-scale databases, GIST1M and Deep1M. Experimental results show that the\nproposed model can improve the speed of building indexing graph to 2-3 times\nits original speed without sacrificing accuracy significantly. The query\nlatency is reduced by a factor of 1.3 to 2.0. In addition, the proposed model\ncan also be combined with other popular quantization methods.",
          "link": "http://arxiv.org/abs/2107.14415",
          "publishedOn": "2021-08-02T01:58:22.680Z",
          "wordCount": 671,
          "title": "Learnable Compression Network with Transformer for Approximate Nearest Neighbor Search. (arXiv:2107.14415v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sciascio_E/0/1/0/all/0/1\">Eugenio Di Sciascio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_A/0/1/0/all/0/1\">Antonio Ferrara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancino_A/0/1/0/all/0/1\">Alberto Carlo Maria Mancino</a>",
          "description": "Deep Learning and factorization-based collaborative filtering recommendation\nmodels have undoubtedly dominated the scene of recommender systems in recent\nyears. However, despite their outstanding performance, these methods require a\ntraining time proportional to the size of the embeddings and it further\nincreases when also side information is considered for the computation of the\nrecommendation list. In fact, in these cases we have that with a large number\nof high-quality features, the resulting models are more complex and difficult\nto train. This paper addresses this problem by presenting KGFlex: a sparse\nfactorization approach that grants an even greater degree of expressiveness. To\nachieve this result, KGFlex analyzes the historical data to understand the\ndimensions the user decisions depend on (e.g., movie direction, musical genre,\nnationality of book writer). KGFlex represents each item feature as an\nembedding and it models user-item interactions as a factorized entropy-driven\ncombination of the item attributes relevant to the user. KGFlex facilitates the\ntraining process by letting users update only those relevant features on which\nthey base their decisions. In other words, the user-item prediction is mediated\nby the user's personal view that considers only relevant features. An extensive\nexperimental evaluation shows the approach's effectiveness, considering the\nrecommendation results' accuracy, diversity, and induced bias. The public\nimplementation of KGFlex is available at https://split.to/kgflex.",
          "link": "http://arxiv.org/abs/2107.14290",
          "publishedOn": "2021-08-02T01:58:22.651Z",
          "wordCount": 657,
          "title": "Sparse Feature Factorization for Recommender Systems with Knowledge Graphs. (arXiv:2107.14290v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lordelo_C/0/1/0/all/0/1\">Carlos Lordelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1\">Emmanouil Benetos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixon_S/0/1/0/all/0/1\">Simon Dixon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahlback_S/0/1/0/all/0/1\">Sven Ahlb&#xe4;ck</a>",
          "description": "This paper proposes a deep convolutional neural network for performing\nnote-level instrument assignment. Given a polyphonic multi-instrumental music\nsignal along with its ground truth or predicted notes, the objective is to\nassign an instrumental source for each note. This problem is addressed as a\npitch-informed classification task where each note is analysed individually. We\nalso propose to utilise several kernel shapes in the convolutional layers in\norder to facilitate learning of efficient timbre-discriminative feature maps.\nExperiments on the MusicNet dataset using 7 instrument classes show that our\napproach is able to achieve an average F-score of 0.904 when the original\nmulti-pitch annotations are used as the pitch information for the system, and\nthat it also excels if the note information is provided using third-party\nmulti-pitch estimation algorithms. We also include ablation studies\ninvestigating the effects of the use of multiple kernel shapes and comparing\ndifferent input representations for the audio and the note-related information.",
          "link": "http://arxiv.org/abs/2107.13617",
          "publishedOn": "2021-07-30T02:13:27.230Z",
          "wordCount": 625,
          "title": "Pitch-Informed Instrument Assignment Using a Deep Convolutional Network with Multiple Kernel Shapes. (arXiv:2107.13617v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elwood_A/0/1/0/all/0/1\">Adam Elwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasparin_A/0/1/0/all/0/1\">Alberto Gasparin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozza_A/0/1/0/all/0/1\">Alessandro Rozza</a>",
          "description": "With the rise in use of social media to promote branded products, the demand\nfor effective influencer marketing has increased. Brands are looking for\nimproved ways to identify valuable influencers among a vast catalogue; this is\neven more challenging with \"micro-influencers\", which are more affordable than\nmainstream ones but difficult to discover. In this paper, we propose a novel\nmulti-task learning framework to improve the state of the art in\nmicro-influencer ranking based on multimedia content. Moreover, since the\nvisual congruence between a brand and influencer has been shown to be good\nmeasure of compatibility, we provide an effective visual method for\ninterpreting our models' decisions, which can also be used to inform brands'\nmedia strategies. We compare with the current state-of-the-art on a recently\nconstructed public dataset and we show significant improvement both in terms of\naccuracy and model complexity. The techniques for ranking and interpretation\npresented in this work can be generalised to arbitrary multimedia ranking tasks\nthat have datasets with a similar structure.",
          "link": "http://arxiv.org/abs/2107.13943",
          "publishedOn": "2021-07-30T02:13:27.216Z",
          "wordCount": 599,
          "title": "Ranking Micro-Influencers: a Novel Multi-Task Learning and Interpretable Framework. (arXiv:2107.13943v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deldjoo_Y/0/1/0/all/0/1\">Yashar Deldjoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merra_F/0/1/0/all/0/1\">Felice Antonio Merra</a>",
          "description": "Recommender systems (RSs) employ user-item feedback, e.g., ratings, to match\ncustomers to personalized lists of products. Approaches to top-k recommendation\nmainly rely on Learning-To-Rank algorithms and, among them, the most widely\nadopted is Bayesian Personalized Ranking (BPR), which bases on a pair-wise\noptimization approach. Recently, BPR has been found vulnerable against\nadversarial perturbations of its model parameters. Adversarial Personalized\nRanking (APR) mitigates this issue by robustifying BPR via an adversarial\ntraining procedure. The empirical improvements of APR's accuracy performance on\nBPR have led to its wide use in several recommender models. However, a key\noverlooked aspect has been the beyond-accuracy performance of APR, i.e.,\nnovelty, coverage, and amplification of popularity bias, considering that\nrecent results suggest that BPR, the building block of APR, is sensitive to the\nintensification of biases and reduction of recommendation novelty. In this\nwork, we model the learning characteristics of the BPR and APR optimization\nframeworks to give mathematical evidence that, when the feedback data have a\ntailed distribution, APR amplifies the popularity bias more than BPR due to an\nunbalanced number of received positive updates from short-head items. Using\nmatrix factorization (MF), we empirically validate the theoretical results by\nperforming preliminary experiments on two public datasets to compare BPR-MF and\nAPR-MF performance on accuracy and beyond-accuracy metrics. The experimental\nresults consistently show the degradation of novelty and coverage measures and\na worrying amplification of bias.",
          "link": "http://arxiv.org/abs/2107.13876",
          "publishedOn": "2021-07-30T02:13:27.172Z",
          "wordCount": 687,
          "title": "Understanding the Effects of Adversarial Personalized Ranking Optimization Method on Recommendation Quality. (arXiv:2107.13876v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Depasquale_E/0/1/0/all/0/1\">Etienne-Victor Depasquale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salam_H/0/1/0/all/0/1\">Humaira Abdul Salam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davoli_F/0/1/0/all/0/1\">Franco Davoli</a>",
          "description": "We suggest an enhancement to structural coding through the use of (a)\ncausally bound codes, (b) basic constructs of graph theory and (c) statistics.\nAs is the norm with structural coding, the codes are collected into categories.\nThe categories are represented by nodes (graph theory). The causality is\nillustrated through links (graph theory) between the nodes and the entire set\nof linked nodes is collected into a single directed acyclic graph. The number\nof occurrences of the nodes and the links provide the input required to analyze\nrelative frequency of occurrence, as well as opening a scope for further\nstatistical analysis. While our raw data was a corpus of literature from a\nspecific discipline, this enhancement is accessible to any qualitative analysis\nthat recognizes causality in its structural codes.",
          "link": "http://arxiv.org/abs/2107.13983",
          "publishedOn": "2021-07-30T02:13:27.143Z",
          "wordCount": 579,
          "title": "PAD: a graphical and numerical enhancement of structural coding to facilitate thematic analysis of a literature corpus. (arXiv:2107.13983v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhizhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>",
          "description": "Despite advances in neural machine translation, cross-lingual retrieval tasks\nin which queries and documents live in different natural language spaces remain\nchallenging. Although neural translation models may provide an intuitive\napproach to tackle the cross-lingual problem, their resource-consuming training\nand advanced model structures may complicate the overall retrieval pipeline and\nreduce users engagement. In this paper, we build our end-to-end Cross-Lingual\nArabic Information REtrieval (CLAIRE) system based on the cross-lingual word\nembedding where searchers are assumed to have a passable passive understanding\nof Arabic and various supporting information in English is provided to aid\nretrieval experience. The proposed system has three major advantages: (1) The\nusage of English-Arabic word embedding simplifies the overall pipeline and\navoids the potential mistakes caused by machine translation. (2) Our CLAIRE\nsystem can incorporate arbitrary word embedding-based neural retrieval models\nwithout structural modification. (3) Early empirical results on an Arabic news\ncollection show promising performance.",
          "link": "http://arxiv.org/abs/2107.13751",
          "publishedOn": "2021-07-30T02:13:27.126Z",
          "wordCount": 572,
          "title": "The Cross-Lingual Arabic Information REtrieval (CLAIRE) System. (arXiv:2107.13751v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/1908.00814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wan-Lei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Peng-Cheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>",
          "description": "k-nearest neighbor graph is a fundamental data structure in many disciplines\nsuch as information retrieval, data-mining, pattern recognition, and machine\nlearning, etc. In the literature, considerable research has been focusing on\nhow to efficiently build an approximate k-nearest neighbor graph (k-NN graph)\nfor a fixed dataset. Unfortunately, a closely related issue of how to merge two\nexisting k-NN graphs has been overlooked. In this paper, we address the issue\nof k-NN graph merging in two different scenarios. In the first scenario, a\nsymmetric merge algorithm is proposed to combine two approximate k-NN graphs.\nThe algorithm facilitates large-scale processing by the efficient merging of\nk-NN graphs that are produced in parallel. In the second scenario, a joint\nmerge algorithm is proposed to expand an existing k-NN graph with a raw\ndataset. The algorithm enables the incremental construction of a hierarchical\napproximate k-NN graph. Superior performance is attained when leveraging the\nhierarchy for NN search of various data types, dimensionality, and distance\nmeasures.",
          "link": "http://arxiv.org/abs/1908.00814",
          "publishedOn": "2021-07-30T02:13:27.117Z",
          "wordCount": 660,
          "title": "On the Merge of k-NN Graph. (arXiv:1908.00814v6 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anchit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1\">Aleksandra Piktus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sonal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>",
          "description": "Pre-training on larger datasets with ever increasing model size is now a\nproven recipe for increased performance across almost all NLP tasks. A notable\nexception is information retrieval, where additional pre-training has so far\nfailed to produce convincing results. We show that, with the right pre-training\nsetup, this barrier can be overcome. We demonstrate this by pre-training large\nbi-encoder models on 1) a recently released set of 65 million synthetically\ngenerated questions, and 2) 200 million post-comment pairs from a preexisting\ndataset of Reddit conversations made available by pushshift.io. We evaluate on\na set of information retrieval and dialogue retrieval benchmarks, showing\nsubstantial improvements over supervised baselines.",
          "link": "http://arxiv.org/abs/2107.13602",
          "publishedOn": "2021-07-30T02:13:27.087Z",
          "wordCount": 554,
          "title": "Domain-matched Pre-training Tasks for Dense Retrieval. (arXiv:2107.13602v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.02590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellogin_A/0/1/0/all/0/1\">Alejandro Bellog&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_A/0/1/0/all/0/1\">Antonio Ferrara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malitesta_D/0/1/0/all/0/1\">Daniele Malitesta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merra_F/0/1/0/all/0/1\">Felice Antonio Merra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomo_C/0/1/0/all/0/1\">Claudio Pomo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donini_F/0/1/0/all/0/1\">Francesco Maria Donini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>",
          "description": "Recommender Systems have shown to be an effective way to alleviate the\nover-choice problem and provide accurate and tailored recommendations. However,\nthe impressive number of proposed recommendation algorithms, splitting\nstrategies, evaluation protocols, metrics, and tasks, has made rigorous\nexperimental evaluation particularly challenging. Puzzled and frustrated by the\ncontinuous recreation of appropriate evaluation benchmarks, experimental\npipelines, hyperparameter optimization, and evaluation procedures, we have\ndeveloped an exhaustive framework to address such needs. Elliot is a\ncomprehensive recommendation framework that aims to run and reproduce an entire\nexperimental pipeline by processing a simple configuration file. The framework\nloads, filters, and splits the data considering a vast set of strategies (13\nsplitting methods and 8 filtering approaches, from temporal training-test\nsplitting to nested K-folds Cross-Validation). Elliot optimizes hyperparameters\n(51 strategies) for several recommendation algorithms (50), selects the best\nmodels, compares them with the baselines providing intra-model statistics,\ncomputes metrics (36) spanning from accuracy to beyond-accuracy, bias, and\nfairness, and conducts statistical analysis (Wilcoxon and Paired t-test). The\naim is to provide the researchers with a tool to ease (and make them\nreproducible) all the experimental evaluation phases, from data reading to\nresults collection. Elliot is available on GitHub\n(https://github.com/sisinflab/elliot).",
          "link": "http://arxiv.org/abs/2103.02590",
          "publishedOn": "2021-07-30T02:13:27.069Z",
          "wordCount": 708,
          "title": "Elliot: a Comprehensive and Rigorous Framework for Reproducible Recommender Systems Evaluation. (arXiv:2103.02590v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daghaghi_S/0/1/0/all/0/1\">Shabnam Daghaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medini_T/0/1/0/all/0/1\">Tharun Medini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meisburger_N/0/1/0/all/0/1\">Nicholas Meisburger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Beidi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengnan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Anshumali Shrivastava</a>",
          "description": "Softmax classifiers with a very large number of classes naturally occur in\nmany applications such as natural language processing and information\nretrieval. The calculation of full softmax is costly from the computational and\nenergy perspective. There have been various sampling approaches to overcome\nthis challenge, popularly known as negative sampling (NS). Ideally, NS should\nsample negative classes from a distribution that is dependent on the input\ndata, the current parameters, and the correct positive class. Unfortunately,\ndue to the dynamically updated parameters and data samples, there is no\nsampling scheme that is provably adaptive and samples the negative classes\nefficiently. Therefore, alternative heuristics like random sampling, static\nfrequency-based sampling, or learning-based biased sampling, which primarily\ntrade either the sampling cost or the adaptivity of samples per iteration are\nadopted. In this paper, we show two classes of distributions where the sampling\nscheme is truly adaptive and provably generates negative samples in\nnear-constant time. Our implementation in C++ on CPU is significantly superior,\nboth in terms of wall-clock time and accuracy, compared to the most optimized\nTensorFlow implementations of other popular negative sampling approaches on\npowerful NVIDIA V100 GPU.",
          "link": "http://arxiv.org/abs/2012.15843",
          "publishedOn": "2021-07-30T02:13:27.055Z",
          "wordCount": 674,
          "title": "A Tale of Two Efficient and Informative Negative Sampling Distributions. (arXiv:2012.15843v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lartigou_F/0/1/0/all/0/1\">Fabrice Lartigou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Govorov_M/0/1/0/all/0/1\">Michael Govorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aisake_T/0/1/0/all/0/1\">Tofiga Aisake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Pankajeshwara N. Sharma</a>",
          "description": "This article deals with the development of an interactive up-to-date Pacific\nIslands Web GIS Atlas. It focuses on the compilation of spatial data from the\ntwelve member countries of the University of the South Pacific (Cook Islands,\nFiji Islands, Kiribati Islands, Marshall Islands, Nauru, Niue, Tonga, Tuvalu,\nTokelau, Solomon Islands, Vanuatu, and Western Samoa). A previous bitmap web\nAtlas was created in 1996, and was a pilot activity investigating the potential\nfor using Geographical Information Systems (GIS) in the South Pacific. The\nobjective of the new atlas is to provide sets of spatial and attributive data\nand maps for use of educators, students, researchers, policy makers and other\nrelevant user groups and the public. GIS is a highly flexible and dynamic\ntechnology that allows the construction and analysis of maps and data sets from\na variety of sources and formats. Nowadays, GIS application has moved from\nlocal and client-server applications to a three-tier architecture: Client (Web\nBrowser) -- Application Web Map Server -- Spatial Data Warehouses. The\nobjective of this project is to produce an Atlas that will include interactive\nmaps and data on an Application Web Map Server. Intergraph products such as\nGeoMedia Professional, Web Map and Web Publisher have been selected for the web\natlas production and design. In an interactive environment, an atlas will be\ncomposed from a series of maps and data profiles, which will be based on legend\nentries, queries, hot spots and cartographic tools. Only the first stage of\ndevelopment of the atlas and related technological solutions are outlined in\nthis article.",
          "link": "http://arxiv.org/abs/2107.14041",
          "publishedOn": "2021-07-30T02:13:27.025Z",
          "wordCount": 706,
          "title": "Interactive GIS Web-Atlas for Twelve Pacific Islands Countries. (arXiv:2107.14041v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhizhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>",
          "description": "The goal of information retrieval is to recommend a list of document\ncandidates that are most relevant to a given query. Listwise learning trains\nneural retrieval models by comparing various candidates simultaneously on a\nlarge scale, offering much more competitive performance than pairwise and\npointwise schemes. Existing listwise ranking losses treat the candidate\ndocument list as a whole unit without further inspection. Some candidates with\nmoderate semantic prominence may be ignored by the noisy similarity signals or\novershadowed by a few especially pronounced candidates. As a result, existing\nranking losses fail to exploit the full potential of neural retrieval models.\nTo address these concerns, we apply the classic pooling technique to conduct\nmulti-level coarse graining and propose ExpertRank, a novel expert-based\nlistwise ranking loss. The proposed scheme has three major advantages: (1)\nExpertRank introduces the profound physics concept of coarse graining to\ninformation retrieval by selecting prominent candidates at various local levels\nbased on model prediction and inter-document comparison. (2) ExpertRank applies\nthe mixture of experts (MoE) technique to combine different experts effectively\nby extending the traditional ListNet. (3) Compared to other existing listwise\nlearning approaches, ExpertRank produces much more reliable and competitive\nperformance for various neural retrieval models with different complexities,\nfrom traditional models, such as KNRM, ConvKNRM, MatchPyramid, to sophisticated\nBERT/ALBERT-based retrieval models.",
          "link": "http://arxiv.org/abs/2107.13752",
          "publishedOn": "2021-07-30T02:13:27.009Z",
          "wordCount": 639,
          "title": "ExpertRank: A Multi-level Coarse-grained Expert-based Listwise Ranking Loss. (arXiv:2107.13752v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadakis_M/0/1/0/all/0/1\">Manolis Fragkiadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putten_P/0/1/0/all/0/1\">Peter van der Putten</a>",
          "description": "Sign language lexica are a useful resource for researchers and people\nlearning sign languages. Current implementations allow a user to search a sign\neither by its gloss or by selecting its primary features such as handshape and\nlocation. This study focuses on exploring a reverse search functionality where\na user can sign a query sign in front of a webcam and retrieve a set of\nmatching signs. By extracting different body joints combinations (upper body,\ndominant hand's arm and wrist) using the pose estimation framework OpenPose, we\ncompare four techniques (PCA, UMAP, DTW and Euclidean distance) as distance\nmetrics between 20 query signs, each performed by eight participants on a 1200\nsign lexicon. The results show that UMAP and DTW can predict a matching sign\nwith an 80\\% and 71\\% accuracy respectively at the top-20 retrieved signs using\nthe movement of the dominant hand arm. Using DTW and adding more sign instances\nfrom other participants in the lexicon, the accuracy can be raised to 90\\% at\nthe top-10 ranking. Our results suggest that our methodology can be used with\nno training in any sign language lexicon regardless of its size.",
          "link": "http://arxiv.org/abs/2107.13637",
          "publishedOn": "2021-07-30T02:13:26.946Z",
          "wordCount": 660,
          "title": "Sign and Search: Sign Search Functionality for Sign Language Lexica. (arXiv:2107.13637v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mayor_O/0/1/0/all/0/1\">Oriol Barbany Mayor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellini_V/0/1/0/all/0/1\">Vito Bellini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buchholz_A/0/1/0/all/0/1\">Alexander Buchholz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benedetto_G/0/1/0/all/0/1\">Giuseppe Di Benedetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granziol_D/0/1/0/all/0/1\">Diego Marco Granziol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruffini_M/0/1/0/all/0/1\">Matteo Ruffini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_Y/0/1/0/all/0/1\">Yannik Stein</a>",
          "description": "Learning-to-rank (LTR) algorithms are ubiquitous and necessary to explore the\nextensive catalogs of media providers. To avoid the user examining all the\nresults, its preferences are used to provide a subset of relatively small size.\nThe user preferences can be inferred from the interactions with the presented\ncontent if explicit ratings are unavailable. However, directly using implicit\nfeedback can lead to learning wrong relevance models and is known as biased\nLTR. The mismatch between implicit feedback and true relevances is due to\nvarious nuisances, with position bias one of the most relevant. Position bias\nmodels consider that the lack of interaction with a presented item is not only\nattributed to the item being irrelevant but because the item was not examined.\nThis paper introduces a method for modeling the probability of an item being\nseen in different contexts, e.g., for different users, with a single estimator.\nOur suggested method, denoted as contextual (EM)-based regression, is\nranker-agnostic and able to correctly learn the latent examination\nprobabilities while only using implicit feedback. Our empirical results\nindicate that the method introduced in this paper outperforms other existing\nposition bias estimators in terms of relative error when the examination\nprobability varies across queries. Moreover, the estimated values provide a\nranking performance boost when used to debias the implicit ranking data even if\nthere is no context dependency on the examination probabilities.",
          "link": "http://arxiv.org/abs/2107.13327",
          "publishedOn": "2021-07-29T02:00:06.739Z",
          "wordCount": 661,
          "title": "Ranker-agnostic Contextual Position Bias Estimation. (arXiv:2107.13327v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Farwa K. Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flanagan_A/0/1/0/all/0/1\">Adrian Flanagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kuan E. Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alamgir_Z/0/1/0/all/0/1\">Zareen Alamgir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammad_Ud_Din_M/0/1/0/all/0/1\">Muhammad Ammad-Ud-Din</a>",
          "description": "We introduce the payload optimization method for federated recommender\nsystems (FRS). In federated learning (FL), the global model payload that is\nmoved between the server and users depends on the number of items to recommend.\nThe model payload grows when there is an increasing number of items. This\nbecomes challenging for an FRS if it is running in production mode. To tackle\nthe payload challenge, we formulated a multi-arm bandit solution that selected\npart of the global model and transmitted it to all users. The selection process\nwas guided by a novel reward function suitable for FL systems. So far as we are\naware, this is the first optimization method that seeks to address item\ndependent payloads. The method was evaluated using three benchmark\nrecommendation datasets. The empirical validation confirmed that the proposed\nmethod outperforms the simpler methods that do not benefit from the bandits for\nthe purpose of item selection. In addition, we have demonstrated the usefulness\nof our proposed method by rigorously evaluating the effects of a payload\nreduction on the recommendation performance degradation. Our method achieved up\nto a 90\\% reduction in model payload, yielding only a $\\sim$4\\% - 8\\% loss in\nthe recommendation performance for highly sparse datasets",
          "link": "http://arxiv.org/abs/2107.13078",
          "publishedOn": "2021-07-29T02:00:06.672Z",
          "wordCount": 674,
          "title": "A Payload Optimization Method for Federated Recommender Systems. (arXiv:2107.13078v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.06467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Andrew Yates</a>",
          "description": "The goal of text ranking is to generate an ordered list of texts retrieved\nfrom a corpus in response to a query. Although the most common formulation of\ntext ranking is search, instances of the task can also be found in many natural\nlanguage processing applications. This survey provides an overview of text\nranking with neural network architectures known as transformers, of which BERT\nis the best-known example. The combination of transformers and self-supervised\npretraining has been responsible for a paradigm shift in natural language\nprocessing (NLP), information retrieval (IR), and beyond. In this survey, we\nprovide a synthesis of existing work as a single point of entry for\npractitioners who wish to gain a better understanding of how to apply\ntransformers to text ranking problems and researchers who wish to pursue work\nin this area. We cover a wide range of modern techniques, grouped into two\nhigh-level categories: transformer models that perform reranking in multi-stage\narchitectures and dense retrieval techniques that perform ranking directly.\nThere are two themes that pervade our survey: techniques for handling long\ndocuments, beyond typical sentence-by-sentence processing in NLP, and\ntechniques for addressing the tradeoff between effectiveness (i.e., result\nquality) and efficiency (e.g., query latency, model and index size). Although\ntransformer architectures and pretraining techniques are recent innovations,\nmany aspects of how they are applied to text ranking are relatively well\nunderstood and represent mature techniques. However, there remain many open\nresearch questions, and thus in addition to laying out the foundations of\npretrained transformers for text ranking, this survey also attempts to\nprognosticate where the field is heading.",
          "link": "http://arxiv.org/abs/2010.06467",
          "publishedOn": "2021-07-29T02:00:06.660Z",
          "wordCount": 725,
          "title": "Pretrained Transformers for Text Ranking: BERT and Beyond. (arXiv:2010.06467v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellogin_A/0/1/0/all/0/1\">Alejandro Bellog&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomo_C/0/1/0/all/0/1\">Claudio Pomo</a>",
          "description": "Collaborative filtering models based on matrix factorization and learned\nsimilarities using Artificial Neural Networks (ANNs) have gained significant\nattention in recent years. This is, in part, because ANNs have demonstrated\ngood results in a wide variety of recommendation tasks. The introduction of\nANNs within the recommendation ecosystem has been recently questioned, raising\nseveral comparisons in terms of efficiency and effectiveness. One aspect most\nof these comparisons have in common is their focus on accuracy, neglecting\nother evaluation dimensions important for the recommendation, such as novelty,\ndiversity, or accounting for biases. We replicate experiments from three papers\nthat compare Neural Collaborative Filtering (NCF) and Matrix Factorization\n(MF), to extend the analysis to other evaluation dimensions. Our contribution\nshows that the experiments are entirely reproducible, and we extend the study\nincluding other accuracy metrics and two statistical hypothesis tests. We\ninvestigated the Diversity and Novelty of the recommendations, showing that MF\nprovides a better accuracy also on the long tail, although NCF provides a\nbetter item coverage and more diversified recommendations. We discuss the bias\neffect generated by the tested methods. They show a relatively small bias, but\nother recommendation baselines, with competitive accuracy performance,\nconsistently show to be less affected by this issue. This is the first work, to\nthe best of our knowledge, where several evaluation dimensions have been\nexplored for an array of SOTA algorithms covering recent adaptations of ANNs\nand MF. Hence, we show the potential these techniques may have on\nbeyond-accuracy evaluation while analyzing the effect on reproducibility these\ncomplementary dimensions may spark. Available at\ngithub.com/sisinflab/Reenvisioning-the-comparison-between-Neural-Collaborative-Filtering-and-Matrix-Factorization",
          "link": "http://arxiv.org/abs/2107.13472",
          "publishedOn": "2021-07-29T02:00:06.641Z",
          "wordCount": 710,
          "title": "Reenvisioning Collaborative Filtering vs Matrix Factorization. (arXiv:2107.13472v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dantong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minjia Zhang</a>",
          "description": "Graph-based algorithms have shown great empirical potential for the\napproximate nearest neighbor (ANN) search problem. Currently, graph-based ANN\nsearch algorithms are designed mainly using heuristics, whereas theoretical\nanalysis of such algorithms is quite lacking. In this paper, we study a\nfundamental model of proximity graphs used in graph-based ANN search, called\nMonotonic Relative Neighborhood Graph (MRNG), from a theoretical perspective.\nWe use mathematical proofs to explain why proximity graphs that are built based\non MRNG tend to have good searching performance. We also run experiments on\nMRNG and graphs generalizing MRNG to obtain a deeper understanding of the\nmodel. Our experiments give guidance on how to approximate and generalize MRNG\nto build proximity graphs on a large scale. In addition, we discover and study\na hidden structure of MRNG called conflicting nodes, and we give theoretical\nevidence how conflicting nodes could be used to improve ANN search methods that\nare based on MRNG.",
          "link": "http://arxiv.org/abs/2107.13052",
          "publishedOn": "2021-07-29T02:00:06.609Z",
          "wordCount": 584,
          "title": "Understanding and Generalizing Monotonic Proximity Graphs for Approximate Nearest Neighbor Search. (arXiv:2107.13052v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_V/0/1/0/all/0/1\">Vivek Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witteveen_S/0/1/0/all/0/1\">Sam Witteveen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrews_M/0/1/0/all/0/1\">Martin Andrews</a>",
          "description": "Creating explanations for answers to science questions is a challenging task\nthat requires multi-hop inference over a large set of fact sentences. This\nyear, to refocus the Textgraphs Shared Task on the problem of gathering\nrelevant statements (rather than solely finding a single 'correct path'), the\nWorldTree dataset was augmented with expert ratings of 'relevance' of\nstatements to each overall explanation. Our system, which achieved second place\non the Shared Task leaderboard, combines initial statement retrieval; language\nmodels trained to predict the relevance scores; and ensembling of a number of\nthe resulting rankings. Our code implementation is made available at\nhttps://github.com/mdda/worldtree_corpus/tree/textgraphs_2021",
          "link": "http://arxiv.org/abs/2107.13031",
          "publishedOn": "2021-07-29T02:00:06.591Z",
          "wordCount": 570,
          "title": "Red Dragon AI at TextGraphs 2021 Shared Task: Multi-Hop Inference Explanation Regeneration by Matching Expert Ratings. (arXiv:2107.13031v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dallmann_A/0/1/0/all/0/1\">Alexander Dallmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoller_D/0/1/0/all/0/1\">Daniel Zoller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hotho_A/0/1/0/all/0/1\">Andreas Hotho</a>",
          "description": "At the present time, sequential item recommendation models are compared by\ncalculating metrics on a small item subset (target set) to speed up\ncomputation. The target set contains the relevant item and a set of negative\nitems that are sampled from the full item set. Two well-known strategies to\nsample negative items are uniform random sampling and sampling by popularity to\nbetter approximate the item frequency distribution in the dataset. Most\nrecently published papers on sequential item recommendation rely on sampling by\npopularity to compare the evaluated models. However, recent work has already\nshown that an evaluation with uniform random sampling may not be consistent\nwith the full ranking, that is, the model ranking obtained by evaluating a\nmetric using the full item set as target set, which raises the question whether\nthe ranking obtained by sampling by popularity is equal to the full ranking. In\nthis work, we re-evaluate current state-of-the-art sequential recommender\nmodels from the point of view, whether these sampling strategies have an impact\non the final ranking of the models. We therefore train four recently proposed\nsequential recommendation models on five widely known datasets. For each\ndataset and model, we employ three evaluation strategies. First, we compute the\nfull model ranking. Then we evaluate all models on a target set sampled by the\ntwo different sampling strategies, uniform random sampling and sampling by\npopularity with the commonly used target set size of 100, compute the model\nranking for each strategy and compare them with each other. Additionally, we\nvary the size of the sampled target set. Overall, we find that both sampling\nstrategies can produce inconsistent rankings compared with the full ranking of\nthe models. Furthermore, both sampling by popularity and uniform random\nsampling do not consistently produce the same ranking ...",
          "link": "http://arxiv.org/abs/2107.13045",
          "publishedOn": "2021-07-29T02:00:06.544Z",
          "wordCount": 740,
          "title": "A Case Study on Sampling Strategies for Evaluating Neural Sequential Item Recommendation Models. (arXiv:2107.13045v1 [cs.IR])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.08264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.",
          "link": "http://arxiv.org/abs/2107.08264",
          "publishedOn": "2021-08-03T02:06:28.916Z",
          "wordCount": 730,
          "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Hsing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>",
          "description": "The surprisingness of a song is an essential and seemingly subjective factor\nin determining whether the listener likes it. With the help of information\ntheory, it can be described as the transition probability of a music sequence\nmodeled as a Markov chain. In this study, we introduce the concept of deriving\nentropy variations over time, so that the surprise contour of each chord\nsequence can be extracted. Based on this, we propose a user-controllable\nframework that uses a conditional variational autoencoder (CVAE) to harmonize\nthe melody based on the given chord surprise indication. Through explicit\nconditions, the model can randomly generate various and harmonic chord\nprogressions for a melody, and the Spearman's correlation and p-value\nsignificance show that the resulting chord progressions match the given\nsurprise contour quite well. The vanilla CVAE model was evaluated in a basic\nmelody harmonization task (no surprise control) in terms of six objective\nmetrics. The results of experiments on the Hooktheory Lead Sheet Dataset show\nthat our model achieves performance comparable to the state-of-the-art melody\nharmonization model.",
          "link": "http://arxiv.org/abs/2108.00378",
          "publishedOn": "2021-08-03T02:06:28.809Z",
          "wordCount": 610,
          "title": "SurpriseNet: Melody Harmonization Conditioning on User-controlled Surprise Contours. (arXiv:2108.00378v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1\">Uttaran Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Childs_E/0/1/0/all/0/1\">Elizabeth Childs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rewkowski_N/0/1/0/all/0/1\">Nicholas Rewkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>",
          "description": "We present a generative adversarial network to synthesize 3D pose sequences\nof co-speech upper-body gestures with appropriate affective expressions. Our\nnetwork consists of two components: a generator to synthesize gestures from a\njoint embedding space of features encoded from the input speech and the seed\nposes, and a discriminator to distinguish between the synthesized pose\nsequences and real 3D pose sequences. We leverage the Mel-frequency cepstral\ncoefficients and the text transcript computed from the input speech in separate\nencoders in our generator to learn the desired sentiments and the associated\naffective cues. We design an affective encoder using multi-scale\nspatial-temporal graph convolutions to transform 3D pose sequences into latent,\npose-based affective features. We use our affective encoder in both our\ngenerator, where it learns affective features from the seed poses to guide the\ngesture synthesis, and our discriminator, where it enforces the synthesized\ngestures to contain the appropriate affective expressions. We perform extensive\nevaluations on two benchmark datasets for gesture synthesis from the speech,\nthe TED Gesture Dataset and the GENEA Challenge 2020 Dataset. Compared to the\nbest baselines, we improve the mean absolute joint error by 10--33%, the mean\nacceleration difference by 8--58%, and the Fr\\'echet Gesture Distance by\n21--34%. We also conduct a user study and observe that compared to the best\ncurrent baselines, around 15.28% of participants indicated our synthesized\ngestures appear more plausible, and around 16.32% of participants felt the\ngestures had more appropriate affective expressions aligned with the speech.",
          "link": "http://arxiv.org/abs/2108.00262",
          "publishedOn": "2021-08-03T02:06:28.790Z",
          "wordCount": 703,
          "title": "Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective Expression Learning. (arXiv:2108.00262v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>",
          "description": "Occluded person re-identification (ReID) aims to match person images with\nocclusion. It is fundamentally challenging because of the serious occlusion\nwhich aggravates the misalignment problem between images. At the cost of\nincorporating a pose estimator, many works introduce pose information to\nalleviate the misalignment in both training and testing. To achieve high\naccuracy while preserving low inference complexity, we propose a network named\nPose-Guided Feature Learning with Knowledge Distillation (PGFL-KD), where the\npose information is exploited to regularize the learning of semantics aligned\nfeatures but is discarded in testing. PGFL-KD consists of a main branch (MB),\nand two pose-guided branches, \\ieno, a foreground-enhanced branch (FEB), and a\nbody part semantics aligned branch (SAB). The FEB intends to emphasise the\nfeatures of visible body parts while excluding the interference of obstructions\nand background (\\ieno, foreground feature alignment). The SAB encourages\ndifferent channel groups to focus on different body parts to have body part\nsemantics aligned representation. To get rid of the dependency on pose\ninformation when testing, we regularize the MB to learn the merits of the FEB\nand SAB through knowledge distillation and interaction-based training.\nExtensive experiments on occluded, partial, and holistic ReID tasks show the\neffectiveness of our proposed network.",
          "link": "http://arxiv.org/abs/2108.00139",
          "publishedOn": "2021-08-03T02:06:28.717Z",
          "wordCount": 652,
          "title": "Pose-Guided Feature Learning with Knowledge Distillation for Occluded Person Re-Identification. (arXiv:2108.00139v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>",
          "description": "This paper introduces a two-phase deep feature calibration framework for\nefficient learning of semantics enhanced text-image cross-modal joint\nembedding, which clearly separates the deep feature calibration in data\npreprocessing from training the joint embedding model. We use the Recipe1M\ndataset for the technical description and empirical validation. In\npreprocessing, we perform deep feature calibration by combining deep feature\nengineering with semantic context features derived from raw text-image input\ndata. We leverage LSTM to identify key terms, NLP methods to produce ranking\nscores for key terms before generating the key term feature. We leverage\nwideResNet50 to extract and encode the image category semantics to help\nsemantic alignment of the learned recipe and image embeddings in the joint\nlatent space. In joint embedding learning, we perform deep feature calibration\nby optimizing the batch-hard triplet loss function with soft-margin and double\nnegative sampling, also utilizing the category-based alignment loss and\ndiscriminator-based alignment loss. Extensive experiments demonstrate that our\nSEJE approach with the deep feature calibration significantly outperforms the\nstate-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.00705",
          "publishedOn": "2021-08-03T02:06:28.695Z",
          "wordCount": 615,
          "title": "Efficient Deep Feature Calibration for Cross-Modal Joint Embedding Learning. (arXiv:2108.00705v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Javaheri_A/0/1/0/all/0/1\">Alireza Javaheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brites_C/0/1/0/all/0/1\">Catarina Brites</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_F/0/1/0/all/0/1\">Fernando Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ascenso_J/0/1/0/all/0/1\">Jo&#xe3;o Ascenso</a>",
          "description": "Point clouds (PCs) are a powerful 3D visual representation paradigm for many\nemerging application domains, especially virtual and augmented reality, and\nautonomous vehicles. However, the large amount of PC data required for highly\nimmersive and realistic experiences requires the availability of efficient,\nlossy PC coding solutions are critical. Recently, two MPEG PC coding standards\nhave been developed to address the relevant application requirements and\nfurther developments are expected in the future. In this context, the\nassessment of PC quality, notably for decoded PCs, is critical and asks for the\ndesign of efficient objective PC quality metrics. In this paper, a novel\npoint-to-distribution metric is proposed for PC quality assessment considering\nboth the geometry and texture. This new quality metric exploits the\nscale-invariance property of the Mahalanobis distance to assess first the\ngeometry and color point-to-distribution distortions, which are after fused to\nobtain a joint geometry and color quality metric. The proposed quality metric\nsignificantly outperforms the best PC quality assessment metrics in the\nliterature.",
          "link": "http://arxiv.org/abs/2108.00054",
          "publishedOn": "2021-08-03T02:06:28.684Z",
          "wordCount": 620,
          "title": "A Point-to-Distribution Joint Geometry and Color Metric for Point Cloud Quality Assessment. (arXiv:2108.00054v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_P/0/1/0/all/0/1\">Prithwiraj Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raju_R/0/1/0/all/0/1\">Rajan Saha Raju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1\">Arif Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">M. Shahidur Rahman</a>",
          "description": "Text-to-Speech (TTS) system is a system where speech is synthesized from a\ngiven text following any particular approach. Concatenative synthesis, Hidden\nMarkov Model (HMM) based synthesis, Deep Learning (DL) based synthesis with\nmultiple building blocks, etc. are the main approaches for implementing a TTS\nsystem. Here, we are presenting our deep learning-based end-to-end Bangla\nspeech synthesis system. It has been implemented with minimal human annotation\nusing only 3 major components (Encoder, Decoder, Post-processing net including\nwaveform synthesis). It does not require any frontend preprocessor and\nGrapheme-to-Phoneme (G2P) converter. Our model has been trained with\nphonetically balanced 20 hours of single speaker speech data. It has obtained a\n3.79 Mean Opinion Score (MOS) on a scale of 5.0 as subjective evaluation and a\n0.77 Perceptual Evaluation of Speech Quality(PESQ) score on a scale of [-0.5,\n4.5] as objective evaluation. It is outperforming all existing non-commercial\nstate-of-the-art Bangla TTS systems based on naturalness.",
          "link": "http://arxiv.org/abs/2108.00500",
          "publishedOn": "2021-08-03T02:06:28.669Z",
          "wordCount": 587,
          "title": "End to End Bangla Speech Synthesis. (arXiv:2108.00500v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+K_P/0/1/0/all/0/1\">Priyadarshini K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Siddhartha Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borkar_V/0/1/0/all/0/1\">Vivek Borkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Subhasis Chaudhuri</a>",
          "description": "Active metric learning is the problem of incrementally selecting high-utility\nbatches of training data (typically, ordered triplets) to annotate, in order to\nprogressively improve a learned model of a metric over some input domain as\nrapidly as possible. Standard approaches, which independently assess the\ninformativeness of each triplet in a batch, are susceptible to highly\ncorrelated batches with many redundant triplets and hence low overall utility.\nWhile a recent work \\cite{kumari2020batch} proposes batch-decorrelation\nstrategies for metric learning, they rely on ad hoc heuristics to estimate the\ncorrelation between two triplets at a time. We present a novel batch active\nmetric learning method that leverages the Maximum Entropy Principle to learn\nthe least biased estimate of triplet distribution for a given set of prior\nconstraints. To avoid redundancy between triplets, our method collectively\nselects batches with maximum joint entropy, which simultaneously captures both\ninformativeness and diversity. We take advantage of the submodularity of the\njoint entropy function to construct a tractable solution using an efficient\ngreedy algorithm based on Gram-Schmidt orthogonalization that is provably\n$\\left( 1 - \\frac{1}{e} \\right)$-optimal. Our approach is the first batch\nactive metric learning method to define a unified score that balances\ninformativeness and diversity for an entire batch of triplets. Experiments with\nseveral real-world datasets demonstrate that our algorithm is robust,\ngeneralizes well to different applications and input modalities, and\nconsistently outperforms the state-of-the-art.",
          "link": "http://arxiv.org/abs/2102.07365",
          "publishedOn": "2021-08-03T02:06:28.648Z",
          "wordCount": 736,
          "title": "A Unified Batch Selection Policy for Active Metric Learning. (arXiv:2102.07365v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruilong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1\">David A. Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1\">Angjoo Kanazawa</a>",
          "description": "We present AIST++, a new multi-modal dataset of 3D dance motion and music,\nalong with FACT, a Full-Attention Cross-modal Transformer network for\ngenerating 3D dance motion conditioned on music. The proposed AIST++ dataset\ncontains 5.2 hours of 3D dance motion in 1408 sequences, covering 10 dance\ngenres with multi-view videos with known camera poses -- the largest dataset of\nthis kind to our knowledge. We show that naively applying sequence models such\nas transformers to this dataset for the task of music conditioned 3D motion\ngeneration does not produce satisfactory 3D motion that is well correlated with\nthe input music. We overcome these shortcomings by introducing key changes in\nits architecture design and supervision: FACT model involves a deep cross-modal\ntransformer block with full-attention that is trained to predict $N$ future\nmotions. We empirically show that these changes are key factors in generating\nlong sequences of realistic dance motion that are well-attuned to the input\nmusic. We conduct extensive experiments on AIST++ with user studies, where our\nmethod outperforms recent state-of-the-art methods both qualitatively and\nquantitatively.",
          "link": "http://arxiv.org/abs/2101.08779",
          "publishedOn": "2021-08-03T02:06:28.610Z",
          "wordCount": 670,
          "title": "AI Choreographer: Music Conditioned 3D Dance Generation with AIST++. (arXiv:2101.08779v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingsong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hai Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhimin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>",
          "description": "Automated tagging of video advertisements has been a critical yet challenging\nproblem, and it has drawn increasing interests in last years as its\napplications seem to be evident in many fields. Despite sustainable efforts\nhave been made, the tagging task is still suffered from several challenges,\nsuch as, efficiently feature fusion approach is desirable, but under-explored\nin previous studies. In this paper, we present our approach for Multimodal\nVideo Ads Tagging in the 2021 Tencent Advertising Algorithm Competition.\nSpecifically, we propose a novel multi-modal feature fusion framework, with the\ngoal to combine complementary information from multiple modalities. This\nframework introduces stacking-based ensembling approach to reduce the influence\nof varying levels of noise and conflicts between different modalities. Thus,\nour framework can boost the performance of the tagging task, compared to\nprevious methods. To empirically investigate the effectiveness and robustness\nof the proposed framework, we conduct extensive experiments on the challenge\ndatasets. The obtained results suggest that our framework can significantly\noutperform related approaches and our method ranks as the 1st place on the\nfinal leaderboard, with a Global Average Precision (GAP) of 82.63%. To better\npromote the research in this field, we will release our code in the final\nversion.",
          "link": "http://arxiv.org/abs/2108.00679",
          "publishedOn": "2021-08-03T02:06:28.590Z",
          "wordCount": 658,
          "title": "Multimodal Feature Fusion for Video Advertisements Tagging Via Stacking Ensemble. (arXiv:2108.00679v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03496",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mimnaugh_K/0/1/0/all/0/1\">Katherine J. Mimnaugh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suomalainen_M/0/1/0/all/0/1\">Markku Suomalainen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becerra_I/0/1/0/all/0/1\">Israel Becerra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_E/0/1/0/all/0/1\">Eliezer Lozano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murrieta_Cid_R/0/1/0/all/0/1\">Rafael Murrieta-Cid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LaValle_S/0/1/0/all/0/1\">Steven M. LaValle</a>",
          "description": "This paper considers how the motions of a telepresence robot moving\nautonomously affect a person immersed in the robot through a head-mounted\ndisplay. In particular, we explore the preference, comfort, and naturalness of\nelements of piecewise linear paths compared to the same elements on a smooth\npath. In a user study, thirty-six subjects watched panoramic videos of three\ndifferent paths through a simulated museum in virtual reality and responded to\nquestionnaires regarding each path. Preference for a particular path was\ninfluenced the most by comfort, forward speed, and characteristics of the\nturns. Preference was also strongly associated with the users' perceived\nnaturalness, which was primarily determined by the ability to see salient\nobjects, the distance to the walls and objects, as well as the turns.\nParticipants favored the paths that had a one meter per second forward speed\nand rated the path with the least amount of turns as the most comfortable",
          "link": "http://arxiv.org/abs/2103.03496",
          "publishedOn": "2021-07-30T02:13:27.261Z",
          "wordCount": 628,
          "title": "Analysis of User Preferences for Robot Motions in Immersive Telepresence. (arXiv:2103.03496v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xinyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dongjin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuncong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengzhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingchao Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>",
          "description": "Detecting abnormal activities in real-world surveillance videos is an\nimportant yet challenging task as the prior knowledge about video anomalies is\nusually limited or unavailable. Despite that many approaches have been\ndeveloped to resolve this problem, few of them can capture the normal\nspatio-temporal patterns effectively and efficiently. Moreover, existing works\nseldom explicitly consider the local consistency at frame level and global\ncoherence of temporal dynamics in video sequences. To this end, we propose\nConvolutional Transformer based Dual Discriminator Generative Adversarial\nNetworks (CT-D2GAN) to perform unsupervised video anomaly detection.\nSpecifically, we first present a convolutional transformer to perform future\nframe prediction. It contains three key components, i.e., a convolutional\nencoder to capture the spatial information of the input video clips, a temporal\nself-attention module to encode the temporal dynamics, and a convolutional\ndecoder to integrate spatio-temporal features and predict the future frame.\nNext, a dual discriminator based adversarial training procedure, which jointly\nconsiders an image discriminator that can maintain the local consistency at\nframe-level and a video discriminator that can enforce the global coherence of\ntemporal dynamics, is employed to enhance the future frame prediction. Finally,\nthe prediction error is used to identify abnormal video frames. Thoroughly\nempirical studies on three public video anomaly detection datasets, i.e., UCSD\nPed2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the effectiveness of\nthe proposed adversarial spatio-temporal modeling framework.",
          "link": "http://arxiv.org/abs/2107.13720",
          "publishedOn": "2021-07-30T02:13:27.224Z",
          "wordCount": 708,
          "title": "Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for Video Anomaly Detection. (arXiv:2107.13720v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheuk_K/0/1/0/all/0/1\">Kin Wai Cheuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1\">Dorien Herremans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Li Su</a>",
          "description": "Most of the current supervised automatic music transcription (AMT) models\nlack the ability to generalize. This means that they have trouble transcribing\nreal-world music recordings from diverse musical genres that are not presented\nin the labelled training data. In this paper, we propose a semi-supervised\nframework, ReconVAT, which solves this issue by leveraging the huge amount of\navailable unlabelled music recordings. The proposed ReconVAT uses\nreconstruction loss and virtual adversarial training. When combined with\nexisting U-net models for AMT, ReconVAT achieves competitive results on common\nbenchmark datasets such as MAPS and MusicNet. For example, in the few-shot\nsetting for the string part version of MusicNet, ReconVAT achieves F1-scores of\n61.0% and 41.6% for the note-wise and note-with-offset-wise metrics\nrespectively, which translates into an improvement of 22.2% and 62.5% compared\nto the supervised baseline model. Our proposed framework also demonstrates the\npotential of continual learning on new data, which could be useful in\nreal-world applications whereby new data is constantly available.",
          "link": "http://arxiv.org/abs/2107.04954",
          "publishedOn": "2021-07-30T02:13:27.197Z",
          "wordCount": 632,
          "title": "ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data. (arXiv:2107.04954v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14179",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_A/0/1/0/all/0/1\">Anique Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shan Liu</a>",
          "description": "Photo-realistic point cloud capture and transmission are the fundamental\nenablers for immersive visual communication. The coding process of dynamic\npoint clouds, especially video-based point cloud compression (V-PCC) developed\nby the MPEG standardization group, is now delivering state-of-the-art\nperformance in compression efficiency. V-PCC is based on the projection of the\npoint cloud patches to 2D planes and encoding the sequence as 2D texture and\ngeometry patch sequences. However, the resulting quantization errors from\ncoding can introduce compression artifacts, which can be very unpleasant for\nthe quality of experience (QoE). In this work, we developed a novel\nout-of-the-loop point cloud geometry artifact removal solution that can\nsignificantly improve reconstruction quality without additional bandwidth cost.\nOur novel framework consists of a point cloud sampling scheme, an artifact\nremoval network, and an aggregation scheme. The point cloud sampling scheme\nemploys a cube-based neighborhood patch extraction to divide the point cloud\ninto patches. The geometry artifact removal network then processes these\npatches to obtain artifact-removed patches. The artifact-removed patches are\nthen merged together using an aggregation scheme to obtain the final\nartifact-removed point cloud. We employ 3D deep convolutional feature learning\nfor geometry artifact removal that jointly recovers both the quantization\ndirection and the quantization noise level by exploiting projection and\nquantization prior. The simulation results demonstrate that the proposed method\nis highly effective and can considerably improve the quality of the\nreconstructed point cloud.",
          "link": "http://arxiv.org/abs/2107.14179",
          "publishedOn": "2021-07-30T02:13:27.160Z",
          "wordCount": 673,
          "title": "Video-based Point Cloud Compression Artifact Removal. (arXiv:2107.14179v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadakis_M/0/1/0/all/0/1\">Manolis Fragkiadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putten_P/0/1/0/all/0/1\">Peter van der Putten</a>",
          "description": "Sign language lexica are a useful resource for researchers and people\nlearning sign languages. Current implementations allow a user to search a sign\neither by its gloss or by selecting its primary features such as handshape and\nlocation. This study focuses on exploring a reverse search functionality where\na user can sign a query sign in front of a webcam and retrieve a set of\nmatching signs. By extracting different body joints combinations (upper body,\ndominant hand's arm and wrist) using the pose estimation framework OpenPose, we\ncompare four techniques (PCA, UMAP, DTW and Euclidean distance) as distance\nmetrics between 20 query signs, each performed by eight participants on a 1200\nsign lexicon. The results show that UMAP and DTW can predict a matching sign\nwith an 80\\% and 71\\% accuracy respectively at the top-20 retrieved signs using\nthe movement of the dominant hand arm. Using DTW and adding more sign instances\nfrom other participants in the lexicon, the accuracy can be raised to 90\\% at\nthe top-10 ranking. Our results suggest that our methodology can be used with\nno training in any sign language lexicon regardless of its size.",
          "link": "http://arxiv.org/abs/2107.13637",
          "publishedOn": "2021-07-30T02:13:26.993Z",
          "wordCount": 660,
          "title": "Sign and Search: Sign Search Functionality for Sign Language Lexica. (arXiv:2107.13637v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-29T02:00:07.070Z",
          "wordCount": 700,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1\">Javier Naranjo-Alcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1\">Sergi Perez-Castanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Garcia_A/0/1/0/all/0/1\">Aaron Lopez-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1\">Pedro Zuccarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1\">Maximo Cobos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1\">Francesc J. Ferri</a>",
          "description": "The use of multiple and semantically correlated sources can provide\ncomplementary information to each other that may not be evident when working\nwith individual modalities on their own. In this context, multi-modal models\ncan help producing more accurate and robust predictions in machine learning\ntasks where audio-visual data is available. This paper presents a multi-modal\nmodel for automatic scene classification that exploits simultaneously auditory\nand visual information. The proposed approach makes use of two separate\nnetworks which are respectively trained in isolation on audio and visual data,\nso that each network specializes in a given modality. The visual subnetwork is\na pre-trained VGG16 model followed by a bidiretional recurrent layer, while the\nresidual audio subnetwork is based on stacked squeeze-excitation convolutional\nblocks trained from scratch. After training each subnetwork, the fusion of\ninformation from the audio and visual streams is performed at two different\nstages. The early fusion stage combines features resulting from the last\nconvolutional block of the respective subnetworks at different time steps to\nfeed a bidirectional recurrent structure. The late fusion stage combines the\noutput of the early fusion stage with the independent predictions provided by\nthe two subnetworks, resulting in the final prediction. We evaluate the method\nusing the recently published TAU Audio-Visual Urban Scenes 2021, which contains\nsynchronized audio and video recordings from 12 European cities in 10 different\nscene classes. The proposed model has been shown to provide an excellent\ntrade-off between prediction performance (86.5%) and system complexity (15M\nparameters) in the evaluation results of the DCASE 2021 Challenge.",
          "link": "http://arxiv.org/abs/2107.13180",
          "publishedOn": "2021-07-29T02:00:06.906Z",
          "wordCount": 712,
          "title": "Squeeze-Excitation Convolutional Recurrent Neural Networks for Audio-Visual Scene Classification. (arXiv:2107.13180v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenhua Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "In this work, we propose an adversarial unsupervised domain adaptation (UDA)\napproach with the inherent conditional and label shifts, in which we aim to\nalign the distributions w.r.t. both $p(x|y)$ and $p(y)$. Since the label is\ninaccessible in the target domain, the conventional adversarial UDA assumes\n$p(y)$ is invariant across domains, and relies on aligning $p(x)$ as an\nalternative to the $p(x|y)$ alignment. To address this, we provide a thorough\ntheoretical and empirical analysis of the conventional adversarial UDA methods\nunder both conditional and label shifts, and propose a novel and practical\nalternative optimization scheme for adversarial UDA. Specifically, we infer the\nmarginal $p(y)$ and align $p(x|y)$ iteratively in the training, and precisely\nalign the posterior $p(y|x)$ in testing. Our experimental results demonstrate\nits effectiveness on both classification and segmentation UDA, and partial UDA.",
          "link": "http://arxiv.org/abs/2107.13469",
          "publishedOn": "2021-07-29T02:00:06.786Z",
          "wordCount": 605,
          "title": "Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate. (arXiv:2107.13469v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13385",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wieckowski_A/0/1/0/all/0/1\">Adam Wieckowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lehmann_C/0/1/0/all/0/1\">Christian Lehmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bross_B/0/1/0/all/0/1\">Benjamin Bross</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marpe_D/0/1/0/all/0/1\">Detlev Marpe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biatek_T/0/1/0/all/0/1\">Thibaud Biatek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raulet_M/0/1/0/all/0/1\">Mickael Raulet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feuvre_J/0/1/0/all/0/1\">Jean Le Feuvre</a>",
          "description": "Versatile Video Coding (VVC) is the most recent international video coding\nstandard jointly developed by ITU-T and ISO/IEC, which has been finalized in\nJuly 2020. VVC allows for significant bit-rate reductions around 50% for the\nsame subjective video quality compared to its predecessor, High Efficiency\nVideo Coding (HEVC). One year after finalization, VVC support in devices and\nchipsets is still under development, which is aligned with the typical\ndevelopment cycles of new video coding standards. This paper presents\nopen-source software packages that allow building a complete VVC end-to-end\ntoolchain already one year after its finalization. This includes the Fraunhofer\nHHI VVenC library for fast and efficient VVC encoding as well as HHI's VVdeC\nlibrary for live decoding. An experimental integration of VVC in the GPAC\nsoftware tools and FFmpeg media framework allows packaging VVC bitstreams, e.g.\nencoded with VVenC, in MP4 file format and using DASH for content creation and\nstreaming. The integration of VVdeC allows playback on the receiver. Given\nthese packages, step-by-step tutorials are provided for two possible\napplication scenarios: VVC file encoding plus playback and adaptive streaming\nwith DASH.",
          "link": "http://arxiv.org/abs/2107.13385",
          "publishedOn": "2021-07-29T02:00:06.776Z",
          "wordCount": 652,
          "title": "A Complete End-To-End Open Source Toolchain for the Versatile Video Coding (VVC) Standard. (arXiv:2107.13385v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianhua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">Fei Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xiangui Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yun-Qing Shi</a>",
          "description": "A great challenge to steganography has arisen with the wide application of\nsteganalysis methods based on convolutional neural networks (CNNs). To this\nend, embedding cost learning frameworks based on generative adversarial\nnetworks (GANs) have been proposed and achieved success for spatial\nsteganography. However, the application of GAN to JPEG steganography is still\nin the prototype stage; its anti-detectability and training efficiency should\nbe improved. In conventional steganography, research has shown that the\nside-information calculated from the precover can be used to enhance security.\nHowever, it is hard to calculate the side-information without the spatial\ndomain image. In this work, an embedding cost learning framework for JPEG\nSteganography via a Generative Adversarial Network (JS-GAN) has been proposed,\nthe learned embedding cost can be further adjusted asymmetrically according to\nthe estimated side-information. Experimental results have demonstrated that the\nproposed method can automatically learn a content-adaptive embedding cost\nfunction, and use the estimated side-information properly can effectively\nimprove the security performance. For example, under the attack of a classic\nsteganalyzer GFR with quality factor 75 and 0.4 bpnzAC, the proposed JS-GAN can\nincrease the detection error 2.58% over J-UNIWARD, and the estimated\nside-information aided version JS-GAN(ESI) can further increase the security\nperformance by 11.25% over JS-GAN.",
          "link": "http://arxiv.org/abs/2107.13151",
          "publishedOn": "2021-07-29T02:00:06.723Z",
          "wordCount": 638,
          "title": "JPEG Steganography with Embedding Cost Learning and Side-Information Estimation. (arXiv:2107.13151v1 [cs.MM])"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2108.00177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chuanjian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">An Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yiping Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>",
          "description": "Recent studies on deep convolutional neural networks present a simple\nparadigm of architecture design, i.e., models with more MACs typically achieve\nbetter accuracy, such as EfficientNet and RegNet. These works try to enlarge\nall the stages in the model with one unified rule by sampling and statistical\nmethods. However, we observe that some network architectures have similar MACs\nand accuracies, but their allocations on computations for different stages are\nquite different. In this paper, we propose to enlarge the capacity of CNN\nmodels by improving their width, depth and resolution on stage level. Under the\nassumption that the top-performing smaller CNNs are a proper subcomponent of\nthe top-performing larger CNNs, we propose an greedy network enlarging method\nbased on the reallocation of computations. With step-by-step modifying the\ncomputations on different stages, the enlarged network will be equipped with\noptimal allocation and utilization of MACs. On EfficientNet, our method\nconsistently outperforms the performance of the original scaling method. In\nparticular, with application of our method on GhostNet, we achieve\nstate-of-the-art 80.9% and 84.3% ImageNet top-1 accuracies under the setting of\n600M and 4.4B MACs, respectively.",
          "link": "http://arxiv.org/abs/2108.00177",
          "publishedOn": "2021-08-03T02:06:33.600Z",
          "wordCount": 620,
          "title": "Greedy Network Enlarging. (arXiv:2108.00177v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08624",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weidong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>",
          "description": "With the complexity of the network structure, uncertainty inference has\nbecome an important task to improve the classification accuracy for artificial\nintelligence systems. For image classification tasks, we propose a structured\nDropConnect (SDC) framework to model the output of a deep neural network by a\nDirichlet distribution. We introduce a DropConnect strategy on weights in the\nfully connected layers during training. In test, we split the network into\nseveral sub-networks, and then model the Dirichlet distribution by match its\nmoments with the mean and variance of the outputs of these sub-networks. The\nentropy of the estimated Dirichlet distribution is finally utilized for\nuncertainty inference. In this paper, this framework is implemented on LeNet$5$\nand VGG$16$ models for misclassification detection and out-of-distribution\ndetection on MNIST and CIFAR-$10$ datasets. Experimental results show that the\nperformance of the proposed SDC can be comparable to other uncertainty\ninference methods. Furthermore, the SDC is adapted well to different network\nstructures with certain generalization capabilities and research prospects.",
          "link": "http://arxiv.org/abs/2106.08624",
          "publishedOn": "2021-08-03T02:06:33.463Z",
          "wordCount": 645,
          "title": "Structured DropConnect for Uncertainty Inference in Image Classification. (arXiv:2106.08624v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>",
          "description": "Scene text detection and recognition have been well explored in the past few\nyears. Despite the progress, efficient and accurate end-to-end spotting of\narbitrarily-shaped text remains challenging. In this work, we propose an\nend-to-end text spotting framework, termed PAN++, which can efficiently detect\nand recognize text of arbitrary shapes in natural scenes. PAN++ is based on the\nkernel representation that reformulates a text line as a text kernel (central\nregion) surrounded by peripheral pixels. By systematically comparing with\nexisting scene text representations, we show that our kernel representation can\nnot only describe arbitrarily-shaped text but also well distinguish adjacent\ntext. Moreover, as a pixel-based representation, the kernel representation can\nbe predicted by a single fully convolutional network, which is very friendly to\nreal-time applications. Taking the advantages of the kernel representation, we\ndesign a series of components as follows: 1) a computationally efficient\nfeature enhancement network composed of stacked Feature Pyramid Enhancement\nModules (FPEMs); 2) a lightweight detection head cooperating with Pixel\nAggregation (PA); and 3) an efficient attention-based recognition head with\nMasked RoI. Benefiting from the kernel representation and the tailored\ncomponents, our method achieves high inference speed while maintaining\ncompetitive accuracy. Extensive experiments show the superiority of our method.\nFor example, the proposed PAN++ achieves an end-to-end text spotting F-measure\nof 64.9 at 29.2 FPS on the Total-Text dataset, which significantly outperforms\nthe previous best method. Code will be available at: https://git.io/PAN.",
          "link": "http://arxiv.org/abs/2105.00405",
          "publishedOn": "2021-08-03T02:06:33.422Z",
          "wordCount": 737,
          "title": "PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped Text. (arXiv:2105.00405v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Assran_M/0/1/0/all/0/1\">Mahmoud Assran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1\">Mathilde Caron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1\">Piotr Bojanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1\">Nicolas Ballas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1\">Michael Rabbat</a>",
          "description": "This paper proposes a novel method of learning by predicting view assignments\nwith support samples (PAWS). The method trains a model to minimize a\nconsistency loss, which ensures that different views of the same unlabeled\ninstance are assigned similar pseudo-labels. The pseudo-labels are generated\nnon-parametrically, by comparing the representations of the image views to\nthose of a set of randomly sampled labeled images. The distance between the\nview representations and labeled representations is used to provide a weighting\nover class labels, which we interpret as a soft pseudo-label. By\nnon-parametrically incorporating labeled samples in this way, PAWS extends the\ndistance-metric loss used in self-supervised methods such as BYOL and SwAV to\nthe semi-supervised setting. Despite the simplicity of the approach, PAWS\noutperforms other semi-supervised methods across architectures, setting a new\nstate-of-the-art for a ResNet-50 on ImageNet trained with either 10% or 1% of\nthe labels, reaching 75.5% and 66.5% top-1 respectively. PAWS requires 4x to\n12x less training than the previous best methods.",
          "link": "http://arxiv.org/abs/2104.13963",
          "publishedOn": "2021-08-03T02:06:33.337Z",
          "wordCount": 674,
          "title": "Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples. (arXiv:2104.13963v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1\">Siyuan Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Ailing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>",
          "description": "Heatmap-based methods dominate in the field of human pose estimation by\nmodelling the output distribution through likelihood heatmaps. In contrast,\nregression-based methods are more efficient but suffer from inferior\nperformance. In this work, we explore maximum likelihood estimation (MLE) to\ndevelop an efficient and effective regression-based methods. From the\nperspective of MLE, adopting different regression losses is making different\nassumptions about the output density function. A density function closer to the\ntrue distribution leads to a better regression performance. In light of this,\nwe propose a novel regression paradigm with Residual Log-likelihood Estimation\n(RLE) to capture the underlying output distribution. Concretely, RLE learns the\nchange of the distribution instead of the unreferenced underlying distribution\nto facilitate the training process. With the proposed reparameterization\ndesign, our method is compatible with off-the-shelf flow models. The proposed\nmethod is effective, efficient and flexible. We show its potential in various\nhuman pose estimation tasks with comprehensive experiments. Compared to the\nconventional regression paradigm, regression with RLE bring 12.4 mAP\nimprovement on MSCOCO without any test-time overhead. Moreover, for the first\ntime, especially on multi-person pose estimation, our regression method is\nsuperior to the heatmap-based methods. Our code is available at\nhttps://github.com/Jeff-sjtu/res-loglikelihood-regression",
          "link": "http://arxiv.org/abs/2107.11291",
          "publishedOn": "2021-08-03T02:06:33.329Z",
          "wordCount": 687,
          "title": "Human Pose Regression with Residual Log-likelihood Estimation. (arXiv:2107.11291v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chugg_B/0/1/0/all/0/1\">Ben Chugg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1\">Brandon Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eicher_S/0/1/0/all/0/1\">Seiji Eicher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sandy Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1\">Daniel E. Ho</a>",
          "description": "Much environmental enforcement in the United States has historically relied\non either self-reported data or physical, resource-intensive, infrequent\ninspections. Advances in remote sensing and computer vision, however, have the\npotential to augment compliance monitoring by detecting early warning signs of\nnoncompliance. We demonstrate a process for rapid identification of significant\nstructural expansion using Planet's 3m/pixel satellite imagery products and\nfocusing on Concentrated Animal Feeding Operations (CAFOs) in the US as a test\ncase. Unpermitted building expansion has been a particular challenge with\nCAFOs, which pose significant health and environmental risks. Using new\nhand-labeled dataset of 145,053 images of 1,513 CAFOs, we combine\nstate-of-the-art building segmentation with a likelihood-based change-point\ndetection model to provide a robust signal of building expansion (AUC = 0.86).\nA major advantage of this approach is that it can work with higher cadence\n(daily to weekly), but lower resolution (3m/pixel), satellite imagery than\npreviously used in similar environmental settings. It is also highly\ngeneralizable and thus provides a near real-time monitoring tool to prioritize\nenforcement resources in other settings where unpermitted construction poses\nenvironmental risk, e.g. zoning, habitat modification, or wetland protection.",
          "link": "http://arxiv.org/abs/2105.14159",
          "publishedOn": "2021-08-03T02:06:32.905Z",
          "wordCount": 683,
          "title": "Enhancing Environmental Enforcement with Near Real-Time Monitoring: Likelihood-Based Detection of Structural Expansion of Intensive Livestock Farms. (arXiv:2105.14159v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_G/0/1/0/all/0/1\">Guojia Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhenkuan Pan</a>",
          "description": "Images captured underwater are often characterized by low contrast, color\ndistortion, and noise. To address these visual degradations, we propose a novel\nscheme by constructing an adaptive color and contrast enhancement, and\ndenoising (ACCE-D) framework for underwater image enhancement. In the proposed\nframework, Difference of Gaussian (DoG) filter and bilateral filter are\nrespectively employed to decompose the high-frequency and low-frequency\ncomponents. Benefited from this separation, we utilize soft-thresholding\noperation to suppress the noise in the high-frequency component. Specially, the\nlow-frequency component is enhanced by using an adaptive color and contrast\nenhancement (ACCE) strategy. The proposed ACCE is an adaptive variational\nframework implemented in the HSI color space, which integrates data term and\nregularized term, as well as introduces Gaussian weight and Heaviside function\nto avoid over-enhancement and oversaturation. Moreover, we derive a numerical\nsolution for ACCE, and adopt a pyramid-based strategy to accelerate the solving\nprocedure. Experimental results demonstrate that our strategy is effective in\ncolor correction, visibility improvement, and detail revealing. Comparison with\nstate-of-the-art techniques also validate the superiority of proposed method.\nFurthermore, we have verified the utility of our proposed ACCE-D for enhancing\nother types of degraded scenes, including foggy scene, sandstorm scene and\nlow-light scene.",
          "link": "http://arxiv.org/abs/2104.01073",
          "publishedOn": "2021-08-03T02:06:32.870Z",
          "wordCount": 664,
          "title": "Enhancing Underwater Image via Adaptive Color and Contrast Enhancement, and Denoising. (arXiv:2104.01073v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.12906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1\">Uttaran Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_T/0/1/0/all/0/1\">Trisha Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohan Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Randhavane_T/0/1/0/all/0/1\">Tanmay Randhavane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1\">Aniket Bera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>",
          "description": "We present a novel classifier network called STEP, to classify perceived\nhuman emotion from gaits, based on a Spatial Temporal Graph Convolutional\nNetwork (ST-GCN) architecture. Given an RGB video of an individual walking, our\nformulation implicitly exploits the gait features to classify the emotional\nstate of the human into one of four emotions: happy, sad, angry, or neutral. We\nuse hundreds of annotated real-world gait videos and augment them with\nthousands of annotated synthetic gaits generated using a novel generative\nnetwork called STEP-Gen, built on an ST-GCN based Conditional Variational\nAutoencoder (CVAE). We incorporate a novel push-pull regularization loss in the\nCVAE formulation of STEP-Gen to generate realistic gaits and improve the\nclassification accuracy of STEP. We also release a novel dataset (E-Gait),\nwhich consists of $2,177$ human gaits annotated with perceived emotions along\nwith thousands of synthetic gaits. In practice, STEP can learn the affective\nfeatures and exhibits classification accuracy of 89% on E-Gait, which is 14 -\n30% more accurate over prior methods.",
          "link": "http://arxiv.org/abs/1910.12906",
          "publishedOn": "2021-08-03T02:06:32.791Z",
          "wordCount": 659,
          "title": "STEP: Spatial Temporal Graph Convolutional Networks for Emotion Perception from Gaits. (arXiv:1910.12906v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Joy T. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agu_N/0/1/0/all/0/1\">Nkechinyere N. Agu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1\">Ismini Lourentzou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Arjun Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paguio_J/0/1/0/all/0/1\">Joseph A. Paguio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jasper S. Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dee_E/0/1/0/all/0/1\">Edward C. Dee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_W/0/1/0/all/0/1\">William Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashyap_S/0/1/0/all/0/1\">Satyananda Kashyap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giovannini_A/0/1/0/all/0/1\">Andrea Giovannini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1\">Leo A. Celi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Mehdi Moradi</a>",
          "description": "Despite the progress in automatic detection of radiologic findings from chest\nX-ray (CXR) images in recent years, a quantitative evaluation of the\nexplainability of these models is hampered by the lack of locally labeled\ndatasets for different findings. With the exception of a few expert-labeled\nsmall-scale datasets for specific findings, such as pneumonia and pneumothorax,\nmost of the CXR deep learning models to date are trained on global \"weak\"\nlabels extracted from text reports, or trained via a joint image and\nunstructured text learning strategy. Inspired by the Visual Genome effort in\nthe computer vision community, we constructed the first Chest ImaGenome dataset\nwith a scene graph data structure to describe $242,072$ images. Local\nannotations are automatically produced using a joint rule-based natural\nlanguage processing (NLP) and atlas-based bounding box detection pipeline.\nThrough a radiologist constructed CXR ontology, the annotations for each CXR\nare connected as an anatomy-centered scene graph, useful for image-level\nreasoning and multimodal fusion applications. Overall, we provide: i) $1,256$\ncombinations of relation annotations between $29$ CXR anatomical locations\n(objects with bounding box coordinates) and their attributes, structured as a\nscene graph per image, ii) over $670,000$ localized comparison relations (for\nimproved, worsened, or no change) between the anatomical locations across\nsequential exams, as well as ii) a manually annotated gold standard scene graph\ndataset from $500$ unique patients.",
          "link": "http://arxiv.org/abs/2108.00316",
          "publishedOn": "2021-08-03T02:06:32.765Z",
          "wordCount": 697,
          "title": "Chest ImaGenome Dataset for Clinical Reasoning. (arXiv:2108.00316v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiageng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_M/0/1/0/all/0/1\">Minzhe Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chenhan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hanxue Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yamin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chaoqiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>",
          "description": "Current perception models in autonomous driving have become notorious for\ngreatly relying on a mass of annotated data to cover unseen cases and address\nthe long-tail problem. On the other hand, learning from unlabeled large-scale\ncollected data and incrementally self-training powerful recognition models have\nreceived increasing attention and may become the solutions of next-generation\nindustry-level powerful and robust perception models in autonomous driving.\nHowever, the research community generally suffered from data inadequacy of\nthose essential real-world scene data, which hampers the future exploration of\nfully/semi/self-supervised methods for 3D perception. In this paper, we\nintroduce the ONCE (One millioN sCenEs) dataset for 3D object detection in the\nautonomous driving scenario. The ONCE dataset consists of 1 million LiDAR\nscenes and 7 million corresponding camera images. The data is selected from 144\ndriving hours, which is 20x longer than the largest 3D autonomous driving\ndataset available (e.g. nuScenes and Waymo), and it is collected across a range\nof different areas, periods and weather conditions. To facilitate future\nresearch on exploiting unlabeled data for 3D detection, we additionally provide\na benchmark in which we reproduce and evaluate a variety of self-supervised and\nsemi-supervised methods on the ONCE dataset. We conduct extensive analyses on\nthose methods and provide valuable observations on their performance related to\nthe scale of used data. Data, code, and more information are available at\nhttps://once-for-auto-driving.github.io/index.html.",
          "link": "http://arxiv.org/abs/2106.11037",
          "publishedOn": "2021-08-03T02:06:32.661Z",
          "wordCount": 720,
          "title": "One Million Scenes for Autonomous Driving: ONCE Dataset. (arXiv:2106.11037v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.08708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1\">Uttaran Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roncal_C/0/1/0/all/0/1\">Christian Roncal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_T/0/1/0/all/0/1\">Trisha Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohan Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapsaskis_K/0/1/0/all/0/1\">Kyra Kapsaskis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_K/0/1/0/all/0/1\">Kurt Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1\">Aniket Bera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>",
          "description": "We present an autoencoder-based semi-supervised approach to classify\nperceived human emotions from walking styles obtained from videos or\nmotion-captured data and represented as sequences of 3D poses. Given the motion\non each joint in the pose at each time step extracted from 3D pose sequences,\nwe hierarchically pool these joint motions in a bottom-up manner in the\nencoder, following the kinematic chains in the human body. We also constrain\nthe latent embeddings of the encoder to contain the space of\npsychologically-motivated affective features underlying the gaits. We train the\ndecoder to reconstruct the motions per joint per time step in a top-down manner\nfrom the latent embeddings. For the annotated data, we also train a classifier\nto map the latent embeddings to emotion labels. Our semi-supervised approach\nachieves a mean average precision of 0.84 on the Emotion-Gait benchmark\ndataset, which contains both labeled and unlabeled gaits collected from\nmultiple sources. We outperform current state-of-art algorithms for both\nemotion recognition and action recognition from 3D gaits by 7%--23% on the\nabsolute. More importantly, we improve the average precision by 10%--50% on the\nabsolute on classes that each makes up less than 25% of the labeled part of the\nEmotion-Gait benchmark dataset.",
          "link": "http://arxiv.org/abs/1911.08708",
          "publishedOn": "2021-08-03T02:06:32.638Z",
          "wordCount": 732,
          "title": "Take an Emotion Walk: Perceiving Emotions from Gaits Using Hierarchical Attention Pooling and Affective Mapping. (arXiv:1911.08708v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zihan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "Binary neural networks (BNNs) have received increasing attention due to their\nsuperior reductions of computation and memory. Most existing works focus on\neither lessening the quantization error by minimizing the gap between the\nfull-precision weights and their binarization or designing a gradient\napproximation to mitigate the gradient mismatch, while leaving the \"dead\nweights\" untouched. This leads to slow convergence when training BNNs. In this\npaper, for the first time, we explore the influence of \"dead weights\" which\nrefer to a group of weights that are barely updated during the training of\nBNNs, and then introduce rectified clamp unit (ReCU) to revive the \"dead\nweights\" for updating. We prove that reviving the \"dead weights\" by ReCU can\nresult in a smaller quantization error. Besides, we also take into account the\ninformation entropy of the weights, and then mathematically analyze why the\nweight standardization can benefit BNNs. We demonstrate the inherent\ncontradiction between minimizing the quantization error and maximizing the\ninformation entropy, and then propose an adaptive exponential scheduler to\nidentify the range of the \"dead weights\". By considering the \"dead weights\",\nour method offers not only faster BNN training, but also state-of-the-art\nperformance on CIFAR-10 and ImageNet, compared with recent methods. Code can be\navailable at https://github.com/z-hXu/ReCU.",
          "link": "http://arxiv.org/abs/2103.12369",
          "publishedOn": "2021-08-03T02:06:32.606Z",
          "wordCount": 696,
          "title": "ReCU: Reviving the Dead Weights in Binary Neural Networks. (arXiv:2103.12369v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14256",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_B/0/1/0/all/0/1\">Boing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yinxi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weitz_P/0/1/0/all/0/1\">Philippe Weitz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lindberg_J/0/1/0/all/0/1\">Johan Lindberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hartman_J/0/1/0/all/0/1\">Johan Hartman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Egevad_L/0/1/0/all/0/1\">Lars Egevad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gronberg_H/0/1/0/all/0/1\">Henrik Gr&#xf6;nberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eklund_M/0/1/0/all/0/1\">Martin Eklund</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rantalainen_M/0/1/0/all/0/1\">Mattias Rantalainen</a>",
          "description": "Background: Transrectal ultrasound guided systematic biopsies of the prostate\nis a routine procedure to establish a prostate cancer diagnosis. However, the\n10-12 prostate core biopsies only sample a relatively small volume of the\nprostate, and tumour lesions in regions between biopsy cores can be missed,\nleading to a well-known low sensitivity to detect clinically relevant cancer.\nAs a proof-of-principle, we developed and validated a deep convolutional neural\nnetwork model to distinguish between morphological patterns in benign prostate\nbiopsy whole slide images from men with and without established cancer.\nMethods: This study included 14,354 hematoxylin and eosin stained whole slide\nimages from benign prostate biopsies from 1,508 men in two groups: men without\nan established prostate cancer (PCa) diagnosis and men with at least one core\nbiopsy diagnosed with PCa. 80% of the participants were assigned as training\ndata and used for model optimization (1,211 men), and the remaining 20% (297\nmen) as a held-out test set used to evaluate model performance. An ensemble of\n10 deep convolutional neural network models was optimized for classification of\nbiopsies from men with and without established cancer. Hyperparameter\noptimization and model selection was performed by cross-validation in the\ntraining data . Results: Area under the receiver operating characteristic curve\n(ROC-AUC) was estimated as 0.727 (bootstrap 95% CI: 0.708-0.745) on biopsy\nlevel and 0.738 (bootstrap 95% CI: 0.682 - 0.796) on man level. At a\nspecificity of 0.9 the model had an estimated sensitivity of 0.348. Conclusion:\nThe developed model has the ability to detect men with risk of missed PCa due\nto under-sampling of the prostate. The proposed model has the potential to\nreduce the number of false negative cases in routine systematic prostate\nbiopsies and to indicate men who could benefit from MRI-guided re-biopsy.",
          "link": "http://arxiv.org/abs/2106.14256",
          "publishedOn": "2021-08-03T02:06:32.587Z",
          "wordCount": 783,
          "title": "Using deep learning to detect patients at risk for prostate cancer despite benign biopsies. (arXiv:2106.14256v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Heng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miththanthaya_H/0/1/0/all/0/1\">Halady Akhilesha Miththanthaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harshit/0/1/0/all/0/1\">Harshit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_S/0/1/0/all/0/1\">Siranjiv Ramana Rajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoqiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhilin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuewei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>",
          "description": "Visual tracking has achieved considerable progress in recent years. However,\ncurrent research in the field mainly focuses on tracking of opaque objects,\nwhile little attention is paid to transparent object tracking. In this paper,\nwe make the first attempt in exploring this problem by proposing a Transparent\nObject Tracking Benchmark (TOTB). Specifically, TOTB consists of 225 videos\n(86K frames) from 15 diverse transparent object categories. Each sequence is\nmanually labeled with axis-aligned bounding boxes. To the best of our\nknowledge, TOTB is the first benchmark dedicated to transparent object\ntracking. In order to understand how existing trackers perform and to provide\ncomparison for future research on TOTB, we extensively evaluate 25\nstate-of-the-art tracking algorithms. The evaluation results exhibit that more\nefforts are needed to improve transparent object tracking. Besides, we observe\nsome nontrivial findings from the evaluation that are discrepant with some\ncommon beliefs in opaque object tracking. For example, we find that deeper\nfeatures are not always good for improvements. Moreover, to encourage future\nresearch, we introduce a novel tracker, named TransATOM, which leverages\ntransparency features for tracking and surpasses all 25 evaluated approaches by\na large margin. By releasing TOTB, we expect to facilitate future research and\napplication of transparent object tracking in both the academia and industry.\nThe TOTB and evaluation results as well as TransATOM are available at\nhttps://hengfan2010.github.io/projects/TOTB.",
          "link": "http://arxiv.org/abs/2011.10875",
          "publishedOn": "2021-08-03T02:06:32.568Z",
          "wordCount": 693,
          "title": "Transparent Object Tracking Benchmark. (arXiv:2011.10875v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1903.06519",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Platonova_G/0/1/0/all/0/1\">Ganna Platonova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stys_D/0/1/0/all/0/1\">Dalibor Stys</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soucek_P/0/1/0/all/0/1\">Pavel Soucek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lonhus_K/0/1/0/all/0/1\">Kirill Lonhus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valenta_J/0/1/0/all/0/1\">Jan Valenta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rychtarikova_R/0/1/0/all/0/1\">Renata Rychtarikova</a>",
          "description": "The most realistic information about the transparent sample such as a live\ncell can be obtained only using bright-field light microscopy. At\nhigh-intensity pulsing LED illumination, we captured a primary\n12-bit-per-channel (bpc) response from an observed sample using a bright-field\nmicroscope equipped with a high-resolution (4872x3248) image sensor. In order\nto suppress data distortions originating from the light interactions with\nelements in the optical path, poor sensor reproduction (geometrical defects of\nthe camera sensor and some peculiarities of sensor sensitivity), we propose a\nspectroscopic approach for the correction of this uncompressed 12-bpc data by\nsimultaneous calibration of all parts of the experimental arrangement.\nMoreover, the final intensities of the corrected images are proportional to the\nphoton fluxes detected by a camera sensor. It can be visualized in 8-bpc\nintensity depth after the Least Information Loss compression [Lect. Notes\nBioinform. 9656, 527 (2016)].",
          "link": "http://arxiv.org/abs/1903.06519",
          "publishedOn": "2021-08-03T02:06:32.554Z",
          "wordCount": 641,
          "title": "Spectroscopic Approach to Correction and Visualisation of Bright-Field Light Transmission Microscopy Biological Data. (arXiv:1903.06519v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1\">Zhanghui Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aojun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jing-Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinjiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qingmin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wayne Zhang</a>",
          "description": "Network compression has been widely studied since it is able to reduce the\nmemory and computation cost during inference. However, previous methods seldom\ndeal with complicated structures like residual connections, group/depth-wise\nconvolution and feature pyramid network, where channels of multiple layers are\ncoupled and need to be pruned simultaneously. In this paper, we present a\ngeneral channel pruning approach that can be applied to various complicated\nstructures. Particularly, we propose a layer grouping algorithm to find coupled\nchannels automatically. Then we derive a unified metric based on Fisher\ninformation to evaluate the importance of a single channel and coupled\nchannels. Moreover, we find that inference speedup on GPUs is more correlated\nwith the reduction of memory rather than FLOPs, and thus we employ the memory\nreduction of each channel to normalize the importance. Our method can be used\nto prune any structures including those with coupled channels. We conduct\nextensive experiments on various backbones, including the classic ResNet and\nResNeXt, mobile-friendly MobileNetV2, and the NAS-based RegNet, both on image\nclassification and object detection which is under-explored. Experimental\nresults validate that our method can effectively prune sophisticated networks,\nboosting inference speed without sacrificing accuracy.",
          "link": "http://arxiv.org/abs/2108.00708",
          "publishedOn": "2021-08-03T02:06:32.548Z",
          "wordCount": 649,
          "title": "Group Fisher Pruning for Practical Network Compression. (arXiv:2108.00708v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1\">Daniel Berleant</a>",
          "description": "The accuracy of DL classifiers is unstable in that it often changes\nsignificantly when retested on adversarial images, imperfect images, or\nperturbed images. This paper adds to the small but fundamental body of work on\nbenchmarking the robustness of DL classifiers on defective images. Unlike\nexisted single-factor digital perturbation work, we provide state-of-the-art\ntwo-factor perturbation that provides two natural perturbations on images\napplied in different sequences. The two-factor perturbation includes (1) two\ndigital perturbations (Salt & pepper noise and Gaussian noise) applied in both\nsequences. (2) one digital perturbation (salt & pepper noise) and a geometric\nperturbation (rotation) applied in different sequences. To measure robust DL\nclassifiers, previous scientists provided 15 types of single-factor corruption.\nWe created 69 benchmarking image sets, including a clean set, sets with single\nfactor perturbations, and sets with two-factor perturbation conditions. To be\nbest of our knowledge, this is the first report that two-factor perturbed\nimages improves both robustness and accuracy of DL classifiers. Previous\nresearch evaluating deep learning (DL) classifiers has often used top-1/top-5\naccuracy, so researchers have usually offered tables, line diagrams, and bar\ncharts to display accuracy of DL classifiers. But these existed approaches\ncannot quantitively evaluate robustness of DL classifiers. We innovate a new\ntwo-dimensional, statistical visualization tool, including mean accuracy and\ncoefficient of variation (CV), to benchmark the robustness of DL classifiers.\nAll source codes and related image sets are shared on websites\n(this http URL or\nhttps://github.com/daiweiworking/RobustDeepLearningUsingPerturbations ) to\nsupport future academic research and industry projects.",
          "link": "http://arxiv.org/abs/2103.03102",
          "publishedOn": "2021-08-03T02:06:32.535Z",
          "wordCount": 722,
          "title": "Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00639",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+White_J/0/1/0/all/0/1\">Jacob M. White</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Crozier_S/0/1/0/all/0/1\">Stuart Crozier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chandra_S/0/1/0/all/0/1\">Shekhar S. Chandra</a>",
          "description": "Sampling strategies are important for sparse imaging methodologies,\nespecially those employing the discrete Fourier transform (DFT). Chaotic\nsensing is one such methodology that employs deterministic, fractal sampling in\nconjunction with finite, iterative reconstruction schemes to form an image from\nlimited samples. Using a sampling pattern constructed entirely from periodic\nlines in DFT space, chaotic sensing was found to outperform traditional\ncompressed sensing for magnetic resonance imaging; however, only one such\nsampling pattern was presented and the reason for its fractal nature was not\nproven. Through the introduction of a novel image transform known as the\nkaleidoscope transform, which formalises and extends upon the concept of\ndownsampling and concatenating an image with itself, this paper: (1)\ndemonstrates a fundamental relationship between multiplication in modular\narithmetic and downsampling; (2) provides a rigorous mathematical explanation\nfor the fractal nature of the sampling pattern in the DFT; and (3) leverages\nthis understanding to develop a collection of novel fractal sampling patterns\nfor the 2D DFT with customisable properties. The ability to design tailor-made\nfractal sampling patterns expands the utility of the DFT in chaotic imaging and\nmay form the basis for a bespoke chaotic sensing methodology, in which the\nfractal sampling matches the imaging task for improved reconstruction.",
          "link": "http://arxiv.org/abs/2108.00639",
          "publishedOn": "2021-08-03T02:06:32.529Z",
          "wordCount": 662,
          "title": "Bespoke Fractal Sampling Patterns for Discrete Fourier Space via the Kaleidoscope Transform. (arXiv:2108.00639v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Y/0/1/0/all/0/1\">Yen Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_B/0/1/0/all/0/1\">Bao Le</a>",
          "description": "Recent breakthroughs in the field of semi-supervised learning have achieved\nresults that match state-of-the-art traditional supervised learning methods.\nMost successful semi-supervised learning approaches in computer vision focus on\nleveraging huge amount of unlabeled data, learning the general representation\nvia data augmentation and transformation, creating pseudo labels, implementing\ndifferent loss functions, and eventually transferring this knowledge to more\ntask-specific smaller models. In this paper, we aim to conduct our analyses on\nthree different aspects of SimCLR, the current state-of-the-art semi-supervised\nlearning framework for computer vision. First, we analyze properties of\ncontrast learning on fine-tuning, as we understand that contrast learning is\nwhat makes this method so successful. Second, we research knowledge\ndistillation through teacher-forcing paradigm. We observe that when the teacher\nand the student share the same base model, knowledge distillation will achieve\nbetter result. Finally, we study how transfer learning works and its\nrelationship with the number of classes on different data sets. Our results\nindicate that transfer learning performs better when number of classes are\nsmaller.",
          "link": "http://arxiv.org/abs/2108.00587",
          "publishedOn": "2021-08-03T02:06:32.456Z",
          "wordCount": 606,
          "title": "Semi-Supervising Learning, Transfer Learning, and Knowledge Distillation with SimCLR. (arXiv:2108.00587v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jingxian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lichao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Y/0/1/0/all/0/1\">Yufei Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Garcia_A/0/1/0/all/0/1\">Abel Gonzalez-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanning Zhang</a>",
          "description": "The target representation learned by convolutional neural networks plays an\nimportant role in Thermal Infrared (TIR) tracking. Currently, most of the\ntop-performing TIR trackers are still employing representations learned by the\nmodel trained on the RGB data. However, this representation does not take into\naccount the information in the TIR modality itself, limiting the performance of\nTIR tracking. To solve this problem, we propose to distill representations of\nthe TIR modality from the RGB modality with Cross-Modal Distillation (CMD) on a\nlarge amount of unlabeled paired RGB-TIR data. We take advantage of the\ntwo-branch architecture of the baseline tracker, i.e. DiMP, for cross-modal\ndistillation working on two components of the tracker. Specifically, we use one\nbranch as a teacher module to distill the representation learned by the model\ninto the other branch. Benefiting from the powerful model in the RGB modality,\nthe cross-modal distillation can learn the TIR-specific representation for\npromoting TIR tracking. The proposed approach can be incorporated into\ndifferent baseline trackers conveniently as a generic and independent\ncomponent. Furthermore, the semantic coherence of paired RGB and TIR images is\nutilized as a supervised signal in the distillation loss for cross-modal\nknowledge transfer. In practice, three different approaches are explored to\ngenerate paired RGB-TIR patches with the same semantics for training in an\nunsupervised way. It is easy to extend to an even larger scale of unlabeled\ntraining data. Extensive experiments on the LSOTB-TIR dataset and PTB-TIR\ndataset demonstrate that our proposed cross-modal distillation method\neffectively learns TIR-specific target representations transferred from the RGB\nmodality. Our tracker outperforms the baseline tracker by achieving absolute\ngains of 2.3% Success, 2.7% Precision, and 2.5% Normalized Precision\nrespectively.",
          "link": "http://arxiv.org/abs/2108.00187",
          "publishedOn": "2021-08-03T02:06:32.441Z",
          "wordCount": 733,
          "title": "Unsupervised Cross-Modal Distillation for Thermal Infrared Tracking. (arXiv:2108.00187v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaobo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>",
          "description": "In this paper, we study the task of hallucinating an authentic\nhigh-resolution (HR) face from an occluded thumbnail. We propose a multi-stage\nProgressive Upsampling and Inpainting Generative Adversarial Network, dubbed\nPro-UIGAN, which exploits facial geometry priors to replenish and upsample (8*)\nthe occluded and tiny faces (16*16 pixels). Pro-UIGAN iteratively (1) estimates\nfacial geometry priors for low-resolution (LR) faces and (2) acquires\nnon-occluded HR face images under the guidance of the estimated priors. Our\nmulti-stage hallucination network super-resolves and inpaints occluded LR faces\nin a coarse-to-fine manner, thus reducing unwanted blurriness and artifacts\nsignificantly. Specifically, we design a novel cross-modal transformer module\nfor facial priors estimation, in which an input face and its landmark features\nare formulated as queries and keys, respectively. Such a design encourages\njoint feature learning across the input facial and landmark features, and deep\nfeature correspondences will be discovered by attention. Thus, facial\nappearance features and facial geometry priors are learned in a mutual\npromotion manner. Extensive experiments demonstrate that our Pro-UIGAN achieves\nvisually pleasing HR faces, reaching superior performance in downstream tasks,\ni.e., face alignment, face parsing, face recognition and expression\nclassification, compared with other state-of-the-art (SotA) methods.",
          "link": "http://arxiv.org/abs/2108.00602",
          "publishedOn": "2021-08-03T02:06:32.419Z",
          "wordCount": 627,
          "title": "Pro-UIGAN: Progressive Face Hallucination from Occluded Thumbnails. (arXiv:2108.00602v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.07935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Libo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shenghui Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chenxi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiaoliang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkinson_P/0/1/0/all/0/1\">Peter M. Atkinson</a>",
          "description": "Assigning geospatial objects with specific categories at the pixel level is a\nfundamental task in remote sensing image analysis. Along with rapid development\nin sensor technologies, remotely sensed images can be captured at multiple\nspatial resolutions (MSR) with information content manifested at different\nscales. Extracting information from these MSR images represents huge\nopportunities for enhanced feature representation and characterisation.\nHowever, MSR images suffer from two critical issues: 1) increased scale\nvariation of geo-objects and 2) loss of detailed information at coarse spatial\nresolutions. To bridge these gaps, in this paper, we propose a novel\nscale-aware neural network (SaNet) for semantic segmentation of MSR remotely\nsensed imagery. SaNet deploys a densely connected feature network (DCFPN)\nmodule to capture high-quality multi-scale context, such that the scale\nvariation is handled properly and the quality of segmentation is increased for\nboth large and small objects. A spatial feature recalibration (SFR) module is\nfurther incorporated into the network to learn intact semantic content with\nenhanced spatial relationships, where the negative effects of information loss\nare removed. The combination of DCFPN and SFR allows SaNet to learn scale-aware\nfeature representation, which outperforms the existing multi-scale feature\nrepresentation. Extensive experiments on three semantic segmentation datasets\ndemonstrated the effectiveness of the proposed SaNet in cross-resolution\nsegmentation.",
          "link": "http://arxiv.org/abs/2103.07935",
          "publishedOn": "2021-08-03T02:06:32.393Z",
          "wordCount": 698,
          "title": "Scale-aware Neural Network for Semantic Segmentation of Multi-resolution Remotely Sensed Images. (arXiv:2103.07935v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sim_I/0/1/0/all/0/1\">Issac Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Ju-Hyung Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Young-Wan Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">JiHwan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">SeonTaek Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Keun Kim</a>",
          "description": "Latest CNN-based object detection models are quite accurate but require a\nhigh-performance GPU to run in real-time. They still are heavy in terms of\nmemory size and speed for an embedded system with limited memory space. Since\nthe object detection for autonomous system is run on an embedded processor, it\nis preferable to compress the detection network as light as possible while\npreserving the detection accuracy. There are several popular lightweight\ndetection models but their accuracy is too low for safe driving applications.\nTherefore, this paper proposes a new object detection model, referred as\nYOffleNet, which is compressed at a high ratio while minimizing the accuracy\nloss for real-time and safe driving application on an autonomous system. The\nbackbone network architecture is based on YOLOv4, but we could compress the\nnetwork greatly by replacing the high-calculation-load CSP DenseNet with the\nlighter modules of ShuffleNet. Experiments with KITTI dataset showed that the\nproposed YOffleNet is compressed by 4.7 times than the YOLOv4-s that could\nachieve as fast as 46 FPS on an embedded GPU system(NVIDIA Jetson AGX Xavier).\nCompared to the high compression ratio, the accuracy is reduced slightly to\n85.8% mAP, that is only 2.6% lower than YOLOv4-s. Thus, the proposed network\nshowed a high potential to be deployed on the embedded system of the autonomous\nsystem for the real-time and accurate object detection applications.",
          "link": "http://arxiv.org/abs/2108.00392",
          "publishedOn": "2021-08-03T02:06:32.352Z",
          "wordCount": 688,
          "title": "Developing a Compressed Object Detection Model based on YOLOv4 for Deployment on Embedded GPU Platform of Autonomous System. (arXiv:2108.00392v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00454",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_L/0/1/0/all/0/1\">Le Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>",
          "description": "Point clouds obtained from 3D sensors are usually sparse. Existing methods\nmainly focus on upsampling sparse point clouds in a supervised manner by using\ndense ground truth point clouds. In this paper, we propose a self-supervised\npoint cloud upsampling network (SSPU-Net) to generate dense point clouds\nwithout using ground truth. To achieve this, we exploit the consistency between\nthe input sparse point cloud and generated dense point cloud for the shapes and\nrendered images. Specifically, we first propose a neighbor expansion unit (NEU)\nto upsample the sparse point clouds, where the local geometric structures of\nthe sparse point clouds are exploited to learn weights for point interpolation.\nThen, we develop a differentiable point cloud rendering unit (DRU) as an\nend-to-end module in our network to render the point cloud into multi-view\nimages. Finally, we formulate a shape-consistent loss and an image-consistent\nloss to train the network so that the shapes of the sparse and dense point\nclouds are as consistent as possible. Extensive results on the CAD and scanned\ndatasets demonstrate that our method can achieve impressive results in a\nself-supervised manner. Code is available at https://github.com/Avlon/SSPU-Net.",
          "link": "http://arxiv.org/abs/2108.00454",
          "publishedOn": "2021-08-03T02:06:32.193Z",
          "wordCount": 629,
          "title": "SSPU-Net: Self-Supervised Point Cloud Upsampling via Differentiable Rendering. (arXiv:2108.00454v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00406",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>",
          "description": "Super-resolution (SR) is a fundamental and representative task of low-level\nvision area. It is generally thought that the features extracted from the SR\nnetwork have no specific semantic information, and the network simply learns\ncomplex non-linear mappings from input to output. Can we find any \"semantics\"\nin SR networks? In this paper, we give affirmative answers to this question. By\nanalyzing the feature representations with dimensionality reduction and\nvisualization, we successfully discover the deep semantic representations in SR\nnetworks, \\textit{i.e.}, deep degradation representations (DDR), which relate\nto the image degradation types and degrees. We also reveal the differences in\nrepresentation semantics between classification and SR networks. Through\nextensive experiments and analysis, we draw a series of observations and\nconclusions, which are of great significance for future work, such as\ninterpreting the intrinsic mechanisms of low-level CNN networks and developing\nnew evaluation approaches for blind SR.",
          "link": "http://arxiv.org/abs/2108.00406",
          "publishedOn": "2021-08-03T02:06:32.154Z",
          "wordCount": 592,
          "title": "Discovering \"Semantics\" in Super-Resolution Networks. (arXiv:2108.00406v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Men_H/0/1/0/all/0/1\">Hui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hanhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenadeleh_M/0/1/0/all/0/1\">Mohsen Jenadeleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saupe_D/0/1/0/all/0/1\">Dietmar Saupe</a>",
          "description": "In subjective full-reference image quality assessment, differences between\nperceptual image qualities of the reference image and its distorted versions\nare evaluated, often using degradation category ratings (DCR). However, the DCR\nhas been criticized since differences between rating categories on this ordinal\nscale might not be perceptually equidistant, and observers may have different\nunderstandings of the categories. Pair comparisons (PC) of distorted images,\nfollowed by Thurstonian reconstruction of scale values, overcome these\nproblems. In addition, PC is more sensitive than DCR, and it can provide scale\nvalues in fractional, just noticeable difference (JND) units that express a\nprecise perceptional interpretation. Still, the comparison of images of nearly\nthe same quality can be difficult. We introduce boosting techniques embedded in\nmore general triplet comparisons (TC) that increase the sensitivity even more.\nBoosting amplifies the artefacts of distorted images, enlarges their visual\nrepresentation by zooming, increases the visibility of the distortions by a\nflickering effect, or combines some of the above. Experimental results show the\neffectiveness of boosted TC for seven types of distortion. We crowdsourced over\n1.7 million responses to triplet questions. A detailed analysis shows that\nboosting increases the discriminatory power and allows to reduce the number of\nsubjective ratings without sacrificing the accuracy of the resulting relative\nimage quality values. Our technique paves the way to fine-grained image quality\ndatasets, allowing for more distortion levels, yet with high-quality subjective\nannotations. We also provide the details for Thurstonian scale reconstruction\nfrom TC and our annotated dataset, KonFiG-IQA, containing 10 source images,\nprocessed using 7 distortion types at 12 or even 30 levels, uniformly spaced\nover a span of 3 JND units.",
          "link": "http://arxiv.org/abs/2108.00201",
          "publishedOn": "2021-08-03T02:06:32.132Z",
          "wordCount": 705,
          "title": "Subjective Image Quality Assessment with Boosted Triplet Comparisons. (arXiv:2108.00201v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.16507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yating Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinchi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>",
          "description": "Regression-based methods have recently shown promising results in\nreconstructing human meshes from monocular images. By directly mapping raw\npixels to model parameters, these methods can produce parametric models in a\nfeed-forward manner via neural networks. However, minor deviation in parameters\nmay lead to noticeable misalignment between the estimated meshes and image\nevidences. To address this issue, we propose a Pyramidal Mesh Alignment\nFeedback (PyMAF) loop to leverage a feature pyramid and rectify the predicted\nparameters explicitly based on the mesh-image alignment status in our deep\nregressor. In PyMAF, given the currently predicted parameters, mesh-aligned\nevidences will be extracted from finer-resolution features accordingly and fed\nback for parameter rectification. To reduce noise and enhance the reliability\nof these evidences, an auxiliary pixel-wise supervision is imposed on the\nfeature encoder, which provides mesh-image correspondence guidance for our\nnetwork to preserve the most related information in spatial features. The\nefficacy of our approach is validated on several benchmarks, including\nHuman3.6M, 3DPW, LSP, and COCO, where experimental results show that our\napproach consistently improves the mesh-image alignment of the reconstruction.\nThe project page with code and video results can be found at\nhttps://hongwenzhang.github.io/pymaf.",
          "link": "http://arxiv.org/abs/2103.16507",
          "publishedOn": "2021-08-03T02:06:32.085Z",
          "wordCount": 697,
          "title": "PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop. (arXiv:2103.16507v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nasibullah/0/1/0/all/0/1\">Nasibullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanta_P/0/1/0/all/0/1\">Partha Pratim Mohanta</a>",
          "description": "Video captioning is one of the challenging problems at the intersection of\nvision and language, having many real-life applications in video retrieval,\nvideo surveillance, assisting visually challenged people, Human-machine\ninterface, and many more. Recent deep learning-based methods have shown\npromising results but are still on the lower side than other vision tasks (such\nas image classification, object detection). A significant drawback with\nexisting video captioning methods is that they are optimized over cross-entropy\nloss function, which is uncorrelated to the de facto evaluation metrics (BLEU,\nMETEOR, CIDER, ROUGE).In other words, cross-entropy is not a proper surrogate\nof the true loss function for video captioning. This paper addresses the\ndrawback by introducing a dynamic loss network (DLN), which provides an\nadditional feedback signal that directly reflects the evaluation metrics. Our\nresults on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to\nText (MSRVTT) datasets outperform previous methods.",
          "link": "http://arxiv.org/abs/2107.11707",
          "publishedOn": "2021-08-03T02:06:32.079Z",
          "wordCount": 609,
          "title": "Boosting Video Captioning with Dynamic Loss Network. (arXiv:2107.11707v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.02284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenmann_M/0/1/0/all/0/1\">Matthias Eisenmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarikaya_D/0/1/0/all/0/1\">Duygu Sarikaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marz_K/0/1/0/all/0/1\">Keno M&#xe4;rz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_T/0/1/0/all/0/1\">Toby Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malpani_A/0/1/0/all/0/1\">Anand Malpani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallert_J/0/1/0/all/0/1\">Johannes Fallert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feussner_H/0/1/0/all/0/1\">Hubertus Feussner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannarou_S/0/1/0/all/0/1\">Stamatia Giannarou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mascagni_P/0/1/0/all/0/1\">Pietro Mascagni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakawala_H/0/1/0/all/0/1\">Hirenkumar Nakawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_A/0/1/0/all/0/1\">Adrian Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pugh_C/0/1/0/all/0/1\">Carla Pugh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedula_S/0/1/0/all/0/1\">Swaroop S. Vedula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cleary_K/0/1/0/all/0/1\">Kevin Cleary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fichtinger_G/0/1/0/all/0/1\">Gabor Fichtinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forestier_G/0/1/0/all/0/1\">Germain Forestier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibaud_B/0/1/0/all/0/1\">Bernard Gibaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grantcharov_T/0/1/0/all/0/1\">Teodor Grantcharov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashizume_M/0/1/0/all/0/1\">Makoto Hashizume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heckmann_Notzel_D/0/1/0/all/0/1\">Doreen Heckmann-N&#xf6;tzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenngott_H/0/1/0/all/0/1\">Hannes G. Kenngott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kikinis_R/0/1/0/all/0/1\">Ron Kikinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mundermann_L/0/1/0/all/0/1\">Lars M&#xfc;ndermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onogur_S/0/1/0/all/0/1\">Sinan Onogur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1\">Raphael Sznitman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1\">Russell H. Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu D. Tizabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1\">Martin Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1\">Gregory D. Hager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumuth_T/0/1/0/all/0/1\">Thomas Neumuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_J/0/1/0/all/0/1\">Justin Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gockel_I/0/1/0/all/0/1\">Ines Gockel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goedeke_J/0/1/0/all/0/1\">Jan Goedeke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_D/0/1/0/all/0/1\">Daniel A. Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joyeux_L/0/1/0/all/0/1\">Luc Joyeux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1\">Kyle Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leff_D/0/1/0/all/0/1\">Daniel R. Leff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madani_A/0/1/0/all/0/1\">Amin Madani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcus_H/0/1/0/all/0/1\">Hani J. Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meireles_O/0/1/0/all/0/1\">Ozanan Meireles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seitel_A/0/1/0/all/0/1\">Alexander Seitel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teber_D/0/1/0/all/0/1\">Dogu Teber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uckert_F/0/1/0/all/0/1\">Frank &#xdc;ckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Stich_B/0/1/0/all/0/1\">Beat P. M&#xfc;ller-Stich</a>, et al. (2 additional authors not shown)",
          "description": "Recent developments in data science in general and machine learning in\nparticular have transformed the way experts envision the future of surgery.\nSurgical Data Science (SDS) is a new research field that aims to improve the\nquality of interventional healthcare through the capture, organization,\nanalysis and modeling of data. While an increasing number of data-driven\napproaches and clinical applications have been studied in the fields of\nradiological and clinical data science, translational success stories are still\nlacking in surgery. In this publication, we shed light on the underlying\nreasons and provide a roadmap for future advances in the field. Based on an\ninternational workshop involving leading researchers in the field of SDS, we\nreview current practice, key achievements and initiatives as well as available\nstandards and tools for a number of topics relevant to the field, namely (1)\ninfrastructure for data acquisition, storage and access in the presence of\nregulatory constraints, (2) data annotation and sharing and (3) data analytics.\nWe further complement this technical perspective with (4) a review of currently\navailable SDS products and the translational progress from academia and (5) a\nroadmap for faster clinical translation and exploitation of the full potential\nof SDS, based on an international multi-round Delphi process.",
          "link": "http://arxiv.org/abs/2011.02284",
          "publishedOn": "2021-08-03T02:06:32.050Z",
          "wordCount": 787,
          "title": "Surgical Data Science -- from Concepts toward Clinical Translation. (arXiv:2011.02284v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09543",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1\">Richard Osuala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1\">Kaisar Kushibar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garrucho_L/0/1/0/all/0/1\">Lidia Garrucho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Linardos_A/0/1/0/all/0/1\">Akis Linardos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szafranowska_Z/0/1/0/all/0/1\">Zuzanna Szafranowska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1\">Stefan Klein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1\">Oliver Diaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>",
          "description": "Despite technological and medical advances, the detection, interpretation,\nand treatment of cancer based on imaging data continue to pose significant\nchallenges. These include high inter-observer variability, difficulty of\nsmall-sized lesion detection, nodule interpretation and malignancy\ndetermination, inter- and intra-tumour heterogeneity, class imbalance,\nsegmentation inaccuracies, and treatment effect uncertainty. The recent\nadvancements in Generative Adversarial Networks (GANs) in computer vision as\nwell as in medical imaging may provide a basis for enhanced capabilities in\ncancer detection and analysis. In this review, we assess the potential of GANs\nto address a number of key challenges of cancer imaging, including data\nscarcity and imbalance, domain and dataset shifts, data access and privacy,\ndata annotation and quantification, as well as cancer detection, tumour\nprofiling and treatment planning. We provide a critical appraisal of the\nexisting literature of GANs applied to cancer imagery, together with\nsuggestions on future research directions to address these challenges. We\nanalyse and discuss 163 papers that apply adversarial training techniques in\nthe context of cancer imaging and elaborate their methodologies, advantages and\nlimitations. With this work, we strive to bridge the gap between the needs of\nthe clinical cancer imaging community and the current and prospective research\non GANs in the artificial intelligence community.",
          "link": "http://arxiv.org/abs/2107.09543",
          "publishedOn": "2021-08-03T02:06:32.034Z",
          "wordCount": 695,
          "title": "A Review of Generative Adversarial Networks in Cancer Imaging: New Applications, New Solutions. (arXiv:2107.09543v1 [eess.IV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Dachuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabanovic_E/0/1/0/all/0/1\">Eldar Sabanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizzetto_L/0/1/0/all/0/1\">Luca Rizzetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrickij_V/0/1/0/all/0/1\">Viktor Skrickij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliverio_R/0/1/0/all/0/1\">Roberto Oliverio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaviani_N/0/1/0/all/0/1\">Nadia Kaviani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yunguang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bureika_G/0/1/0/all/0/1\">Gintautas Bureika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_S/0/1/0/all/0/1\">Stefano Ricci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hecht_M/0/1/0/all/0/1\">Markus Hecht</a>",
          "description": "In the application of computer-vision based displacement measurement, an\noptical target is usually required to prove the reference. In the case that the\noptical target cannot be attached to the measuring objective, edge detection,\nfeature matching and template matching are the most common approaches in\ntarget-less photogrammetry. However, their performance significantly relies on\nparameter settings. This becomes problematic in dynamic scenes where\ncomplicated background texture exists and varies over time. To tackle this\nissue, we propose virtual point tracking for real-time target-less dynamic\ndisplacement measurement, incorporating deep learning techniques and domain\nknowledge. Our approach consists of three steps: 1) automatic calibration for\ndetection of region of interest; 2) virtual point detection for each video\nframe using deep convolutional neural network; 3) domain-knowledge based rule\nengine for point tracking in adjacent frames. The proposed approach can be\nexecuted on an edge computer in a real-time manner (i.e. over 30 frames per\nsecond). We demonstrate our approach for a railway application, where the\nlateral displacement of the wheel on the rail is measured during operation. We\nalso implement an algorithm using template matching and line detection as the\nbaseline for comparison. The numerical experiments have been performed to\nevaluate the performance and the latency of our approach in the harsh railway\nenvironment with noisy and varying backgrounds.",
          "link": "http://arxiv.org/abs/2101.06702",
          "publishedOn": "2021-08-03T02:06:32.028Z",
          "wordCount": 708,
          "title": "Deep Learning based Virtual Point Tracking for Real-Time Target-less Dynamic Displacement Measurement in Railway Applications. (arXiv:2101.06702v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaxiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xueming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Li Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "Recently, some approaches are proposed to harness deep convolutional networks\nto facilitate superpixel segmentation. The common practice is to first evenly\ndivide the image into a pre-defined number of grids and then learn to associate\neach pixel with its surrounding grids. However, simply applying a series of\nconvolution operations with limited receptive fields can only implicitly\nperceive the relations between the pixel and its surrounding grids.\nConsequently, existing methods often fail to provide an effective context when\ninferring the association map. To remedy this issue, we propose a novel\n\\textbf{A}ssociation \\textbf{I}mplantation (AI) module to enable the network to\nexplicitly capture the relations between the pixel and its surrounding grids.\nThe proposed AI module directly implants the features of grid cells to the\nsurrounding of its corresponding central pixel, and conducts convolution on the\npadded window to adaptively transfer knowledge between them. With such an\nimplantation operation, the network could explicitly harvest the pixel-grid\nlevel context, which is more in line with the target of superpixel segmentation\ncomparing to the pixel-wise relation. Furthermore, to pursue better boundary\nprecision, we design a boundary-perceiving loss to help the network\ndiscriminate the pixels around boundaries in hidden feature level, which could\nbenefit the subsequent inferring modules to accurately identify more boundary\npixels. Extensive experiments on BSDS500 and NYUv2 datasets show that our\nmethod could not only achieve state-of-the-art performance but maintain\nsatisfactory inference efficiency.",
          "link": "http://arxiv.org/abs/2101.10696",
          "publishedOn": "2021-08-03T02:06:31.986Z",
          "wordCount": 714,
          "title": "AINet: Association Implantation for Superpixel Segmentation. (arXiv:2101.10696v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_P/0/1/0/all/0/1\">Pranav Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaucorps_P/0/1/0/all/0/1\">Pierre de Beaucorps</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>",
          "description": "Deep reinforcement Learning for end-to-end driving is limited by the need of\ncomplex reward engineering. Sparse rewards can circumvent this challenge but\nsuffers from long training time and leads to sub-optimal policy. In this work,\nwe explore full-control driving with only goal-constrained sparse reward and\npropose a curriculum learning approach for end-to-end driving using only\nnavigation view maps that benefit from small virtual-to-real domain gap. To\naddress the complexity of multiple driving policies, we learn concurrent\nindividual policies selected at inference by a navigation system. We\ndemonstrate the ability of our proposal to generalize on unseen road layout,\nand to drive significantly longer than in the training.",
          "link": "http://arxiv.org/abs/2103.09189",
          "publishedOn": "2021-08-03T02:06:31.976Z",
          "wordCount": 578,
          "title": "Goal-constrained Sparse Reinforcement Learning for End-to-End Driving. (arXiv:2103.09189v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Rui Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Ming Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1\">Raquel Urtasun</a>",
          "description": "3D object detection is a key component of many robotic applications such as\nself-driving vehicles. While many approaches rely on expensive 3D sensors such\nas LiDAR to produce accurate 3D estimates, methods that exploit stereo cameras\nhave recently shown promising results at a lower cost. Existing approaches\ntackle this problem in two steps: first depth estimation from stereo images is\nperformed to produce a pseudo LiDAR point cloud, which is then used as input to\na 3D object detector. However, this approach is suboptimal due to the\nrepresentation mismatch, as the two tasks are optimized in two different metric\nspaces. In this paper we propose a model that unifies these two tasks and\nperforms them in the same metric space. Specifically, we directly construct a\npseudo LiDAR feature volume (PLUME) in 3D space, which is then used to solve\nboth depth estimation and object detection tasks. Our approach achieves\nstate-of-the-art performance with much faster inference times when compared to\nexisting methods on the challenging KITTI benchmark.",
          "link": "http://arxiv.org/abs/2101.06594",
          "publishedOn": "2021-08-03T02:06:31.966Z",
          "wordCount": 645,
          "title": "PLUMENet: Efficient 3D Object Detection from Stereo Images. (arXiv:2101.06594v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nenggan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "People live in a 3D world. However, existing works on person\nre-identification (re-id) mostly consider the semantic representation learning\nin a 2D space, intrinsically limiting the understanding of people. In this\nwork, we address this limitation by exploring the prior knowledge of the 3D\nbody structure. Specifically, we project 2D images to a 3D space and introduce\na novel parameter-efficient Omni-scale Graph Network (OG-Net) to learn the\npedestrian representation directly from 3D point clouds. OG-Net effectively\nexploits the local information provided by sparse 3D points and takes advantage\nof the structure and appearance information in a coherent manner. With the help\nof 3D geometry information, we can learn a new type of deep re-id feature free\nfrom noisy variants, such as scale and viewpoint. To our knowledge, we are\namong the first attempts to conduct person re-identification in the 3D space.\nWe demonstrate through extensive experiments that the proposed method (1) eases\nthe matching difficulty in the traditional 2D space, (2) exploits the\ncomplementary information of 2D appearance and 3D structure, (3) achieves\ncompetitive results with limited parameters on four large-scale person re-id\ndatasets, and (4) has good scalability to unseen datasets. Our code, models and\ngenerated 3D human data are publicly available at\nhttps://github.com/layumi/person-reid-3d .",
          "link": "http://arxiv.org/abs/2006.04569",
          "publishedOn": "2021-08-03T02:06:31.958Z",
          "wordCount": 685,
          "title": "Parameter-Efficient Person Re-identification in the 3D Space. (arXiv:2006.04569v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12086",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yubao Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Ying Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qingshan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>",
          "description": "Hyperspectral compressive imaging takes advantage of compressive sensing\ntheory to achieve coded aperture snapshot measurement without temporal\nscanning, and the entire three-dimensional spatial-spectral data is captured by\na two-dimensional projection during a single integration period. Its core issue\nis how to reconstruct the underlying hyperspectral image using compressive\nsensing reconstruction algorithms. Due to the diversity in the spectral\nresponse characteristics and wavelength range of different spectral imaging\ndevices, previous works are often inadequate to capture complex spectral\nvariations or lack the adaptive capacity to new hyperspectral imagers. In order\nto address these issues, we propose an unsupervised spatial-spectral network to\nreconstruct hyperspectral images only from the compressive snapshot\nmeasurement. The proposed network acts as a conditional generative model\nconditioned on the snapshot measurement, and it exploits the spatial-spectral\nattention module to capture the joint spatial-spectral correlation of\nhyperspectral images. The network parameters are optimized to make sure that\nthe network output can closely match the given snapshot measurement according\nto the imaging model, thus the proposed network can adapt to different imaging\nsettings, which can inherently enhance the applicability of the network.\nExtensive experiments upon multiple datasets demonstrate that our network can\nachieve better reconstruction results than the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2012.12086",
          "publishedOn": "2021-08-03T02:06:31.942Z",
          "wordCount": 664,
          "title": "Unsupervised Spatial-spectral Network Learning for Hyperspectral Compressive Snapshot Reconstruction. (arXiv:2012.12086v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zunlei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhonghua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiuming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lechao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuexuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>",
          "description": "The microvascular invasion (MVI) is a major prognostic factor in\nhepatocellular carcinoma, which is one of the malignant tumors with the highest\nmortality rate. The diagnosis of MVI needs discovering the vessels that contain\nhepatocellular carcinoma cells and counting their number in each vessel, which\ndepends heavily on experiences of the doctor, is largely subjective and\ntime-consuming. However, there is no algorithm as yet tailored for the MVI\ndetection from pathological images. This paper collects the first pathological\nliver image dataset containing 522 whole slide images with labels of vessels,\nMVI, and hepatocellular carcinoma grades. The first and essential step for the\nautomatic diagnosis of MVI is the accurate segmentation of vessels. The unique\ncharacteristics of pathological liver images, such as super-large size,\nmulti-scale vessel, and blurred vessel edges, make the accurate vessel\nsegmentation challenging. Based on the collected dataset, we propose an\nEdge-competing Vessel Segmentation Network (EVS-Net), which contains a\nsegmentation network and two edge segmentation discriminators. The segmentation\nnetwork, combined with an edge-aware self-supervision mechanism, is devised to\nconduct vessel segmentation with limited labeled patches. Meanwhile, two\ndiscriminators are introduced to distinguish whether the segmented vessel and\nbackground contain residual features in an adversarial manner. In the training\nstage, two discriminators are devised tocompete for the predicted position of\nedges. Exhaustive experiments demonstrate that, with only limited labeled\npatches, EVS-Net achieves a close performance of fully supervised methods,\nwhich provides a convenient tool for the pathological liver vessel\nsegmentation. Code is publicly available at\nhttps://github.com/zju-vipa/EVS-Net.",
          "link": "http://arxiv.org/abs/2108.00384",
          "publishedOn": "2021-08-03T02:06:31.925Z",
          "wordCount": 697,
          "title": "Edge-competing Pathological Liver Vessel Segmentation with Limited Labels. (arXiv:2108.00384v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wentao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Zhiyu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_C/0/1/0/all/0/1\">Chengyu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiman Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Tingming Bai</a>",
          "description": "Although instance segmentation has made considerable advancement over recent\nyears, it's still a challenge to design high accuracy algorithms with real-time\nperformance. In this paper, we propose a real-time instance segmentation\nframework termed OrienMask. Upon the one-stage object detector YOLOv3, a mask\nhead is added to predict some discriminative orientation maps, which are\nexplicitly defined as spatial offset vectors for both foreground and background\npixels. Thanks to the discrimination ability of orientation maps, masks can be\nrecovered without the need for extra foreground segmentation. All instances\nthat match with the same anchor size share a common orientation map. This\nspecial sharing strategy reduces the amortized memory utilization for mask\npredictions but without loss of mask granularity. Given the surviving box\npredictions after NMS, instance masks can be concurrently constructed from the\ncorresponding orientation maps with low complexity. Owing to the concise design\nfor mask representation and its effective integration with the anchor-based\nobject detector, our method is qualified under real-time conditions while\nmaintaining competitive accuracy. Experiments on COCO benchmark show that\nOrienMask achieves 34.8 mask AP at the speed of 42.7 fps evaluated with a\nsingle RTX 2080 Ti. The code is available at https://github.com/duwt/OrienMask.",
          "link": "http://arxiv.org/abs/2106.12204",
          "publishedOn": "2021-08-03T02:06:31.879Z",
          "wordCount": 659,
          "title": "Real-time Instance Segmentation with Discriminative Orientation Maps. (arXiv:2106.12204v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+K_P/0/1/0/all/0/1\">Priyadarshini K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Siddhartha Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borkar_V/0/1/0/all/0/1\">Vivek Borkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Subhasis Chaudhuri</a>",
          "description": "Active metric learning is the problem of incrementally selecting high-utility\nbatches of training data (typically, ordered triplets) to annotate, in order to\nprogressively improve a learned model of a metric over some input domain as\nrapidly as possible. Standard approaches, which independently assess the\ninformativeness of each triplet in a batch, are susceptible to highly\ncorrelated batches with many redundant triplets and hence low overall utility.\nWhile a recent work \\cite{kumari2020batch} proposes batch-decorrelation\nstrategies for metric learning, they rely on ad hoc heuristics to estimate the\ncorrelation between two triplets at a time. We present a novel batch active\nmetric learning method that leverages the Maximum Entropy Principle to learn\nthe least biased estimate of triplet distribution for a given set of prior\nconstraints. To avoid redundancy between triplets, our method collectively\nselects batches with maximum joint entropy, which simultaneously captures both\ninformativeness and diversity. We take advantage of the submodularity of the\njoint entropy function to construct a tractable solution using an efficient\ngreedy algorithm based on Gram-Schmidt orthogonalization that is provably\n$\\left( 1 - \\frac{1}{e} \\right)$-optimal. Our approach is the first batch\nactive metric learning method to define a unified score that balances\ninformativeness and diversity for an entire batch of triplets. Experiments with\nseveral real-world datasets demonstrate that our algorithm is robust,\ngeneralizes well to different applications and input modalities, and\nconsistently outperforms the state-of-the-art.",
          "link": "http://arxiv.org/abs/2102.07365",
          "publishedOn": "2021-08-03T02:06:31.872Z",
          "wordCount": 736,
          "title": "A Unified Batch Selection Policy for Active Metric Learning. (arXiv:2102.07365v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feixiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zheheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhihua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_L/0/1/0/all/0/1\">Lei Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhile Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haikuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_M/0/1/0/all/0/1\">Minrui Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Ling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>",
          "description": "Automated analysis of mouse behaviours is crucial for many applications in\nneuroscience. However, quantifying mouse behaviours from videos or images\nremains a challenging problem, where pose estimation plays an important role in\ndescribing mouse behaviours. Although deep learning based methods have made\npromising advances in human pose estimation, they cannot be directly applied to\npose estimation of mice due to different physiological natures. Particularly,\nsince mouse body is highly deformable, it is a challenge to accurately locate\ndifferent keypoints on the mouse body. In this paper, we propose a novel\nHourglass network based model, namely Graphical Model based Structured Context\nEnhancement Network (GM-SCENet) where two effective modules, i.e., Structured\nContext Mixer (SCM) and Cascaded Multi-Level Supervision (CMLS) are\nsubsequently implemented. SCM can adaptively learn and enhance the proposed\nstructured context information of each mouse part by a novel graphical model\nthat takes into account the motion difference between body parts. Then, the\nCMLS module is designed to jointly train the proposed SCM and the Hourglass\nnetwork by generating multi-level information, increasing the robustness of the\nwhole network.Using the multi-level prediction information from SCM and CMLS,\nwe develop an inference method to ensure the accuracy of the localisation\nresults. Finally, we evaluate our proposed approach against several\nbaselines...",
          "link": "http://arxiv.org/abs/2012.00630",
          "publishedOn": "2021-08-03T02:06:31.865Z",
          "wordCount": 707,
          "title": "Structured Context Enhancement Network for Mouse Pose Estimation. (arXiv:2012.00630v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nakashima_K/0/1/0/all/0/1\">Kazuto Nakashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurazume_R/0/1/0/all/0/1\">Ryo Kurazume</a>",
          "description": "3D laser scanning by LiDAR sensors plays an important role for mobile robots\nto understand their surroundings. Nevertheless, not all systems have high\nresolution and accuracy due to hardware limitations, weather conditions, and so\non. Generative modeling of LiDAR data as scene priors is one of the promising\nsolutions to compensate for unreliable or incomplete observations. In this\npaper, we propose a novel generative model for learning LiDAR data based on\ngenerative adversarial networks. As in the related studies, we process LiDAR\ndata as a compact yet lossless representation, a cylindrical depth map.\nHowever, despite the smoothness of real-world objects, many points on the depth\nmap are dropped out through the laser measurement, which causes learning\ndifficulty on generative models. To circumvent this issue, we introduce\nmeasurement uncertainty into the generation process, which allows the model to\nlearn a disentangled representation of the underlying shape and the dropout\nnoises from a collection of real LiDAR data. To simulate the lossy measurement,\nwe adopt a differentiable sampling framework to drop points based on the\nlearned uncertainty. We demonstrate the effectiveness of our method on\nsynthesis and reconstruction tasks using two datasets. We further showcase\npotential applications by restoring LiDAR data with various types of\ncorruption.",
          "link": "http://arxiv.org/abs/2102.11952",
          "publishedOn": "2021-08-03T02:06:31.828Z",
          "wordCount": 675,
          "title": "Learning to Drop Points for LiDAR Scan Synthesis. (arXiv:2102.11952v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1\">Mustafa Hajij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batayneh_F/0/1/0/all/0/1\">Fawwaz Batayneh</a>",
          "description": "Topological Data Analysis (TDA) has emerged recently as a robust tool to\nextract and compare the structure of datasets. TDA identifies features in data\nsuch as connected components and holes and assigns a quantitative measure to\nthese features. Several studies reported that topological features extracted by\nTDA tools provide unique information about the data, discover new insights, and\ndetermine which feature is more related to the outcome. On the other hand, the\noverwhelming success of deep neural networks in learning patterns and\nrelationships has been proven on a vast array of data applications, images in\nparticular. To capture the characteristics of both powerful tools, we propose\n\\textit{TDA-Net}, a novel ensemble network that fuses topological and deep\nfeatures for the purpose of enhancing model generalizability and accuracy. We\napply the proposed \\textit{TDA-Net} to a critical application, which is the\nautomated detection of COVID-19 from CXR images. The experimental results\nshowed that the proposed network achieved excellent performance and suggests\nthe applicability of our method in practice.",
          "link": "http://arxiv.org/abs/2101.08398",
          "publishedOn": "2021-08-03T02:06:31.821Z",
          "wordCount": 718,
          "title": "TDA-Net: Fusion of Persistent Homology and Deep Learning Features for COVID-19 Detection in Chest X-Ray Images. (arXiv:2101.08398v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Je Hyeong Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hanjo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1\">Gi Pyo Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Junghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hyeong-Seok Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1\">Ig-Jae Kim</a>",
          "description": "Face recognition now requires a large number of labelled masked face images\nin the era of this unprecedented COVID-19 pandemic. Unfortunately, the rapid\nspread of the virus has left us little time to prepare for such dataset in the\nwild. To circumvent this issue, we present a 3D model-based approach called\nWearMask3D for augmenting face images of various poses to the masked face\ncounterparts. Our method proceeds by first fitting a 3D morphable model on the\ninput image, second overlaying the mask surface onto the face model and warping\nthe respective mask texture, and last projecting the 3D mask back to 2D. The\nmask texture is adapted based on the brightness and resolution of the input\nimage. By working in 3D, our method can produce more natural masked faces of\ndiverse poses from a single mask texture. To compare precisely between\ndifferent augmentation approaches, we have constructed a dataset comprising\nmasked and unmasked faces with labels called MFW-mini. Experimental results\ndemonstrate WearMask3D produces more realistic masked faces, and utilizing\nthese images for training leads to state-of-the-art recognition accuracy for\nmasked faces.",
          "link": "http://arxiv.org/abs/2103.00803",
          "publishedOn": "2021-08-03T02:06:31.794Z",
          "wordCount": 721,
          "title": "A 3D model-based approach for fitting masks to faces in the wild. (arXiv:2103.00803v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.10420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1\">Gustavo Henrique de Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcao_A/0/1/0/all/0/1\">Alexandre Xavier Falc&#xe3;o</a>",
          "description": "Machine learning techniques have been paramount throughout the last years,\nbeing applied in a wide range of tasks, such as classification, object\nrecognition, person identification, and image segmentation. Nevertheless,\nconventional classification algorithms, e.g., Logistic Regression, Decision\nTrees, and Bayesian classifiers, might lack complexity and diversity, not\nsuitable when dealing with real-world data. A recent graph-inspired classifier,\nknown as the Optimum-Path Forest, has proven to be a state-of-the-art\ntechnique, comparable to Support Vector Machines and even surpassing it in some\ntasks. This paper proposes a Python-based Optimum-Path Forest framework,\ndenoted as OPFython, where all of its functions and classes are based upon the\noriginal C language implementation. Additionally, as OPFython is a Python-based\nlibrary, it provides a more friendly environment and a faster prototyping\nworkspace than the C language.",
          "link": "http://arxiv.org/abs/2001.10420",
          "publishedOn": "2021-08-03T02:06:31.787Z",
          "wordCount": 628,
          "title": "OPFython: A Python-Inspired Optimum-Path Forest Classifier. (arXiv:2001.10420v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruilong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1\">David A. Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1\">Angjoo Kanazawa</a>",
          "description": "We present AIST++, a new multi-modal dataset of 3D dance motion and music,\nalong with FACT, a Full-Attention Cross-modal Transformer network for\ngenerating 3D dance motion conditioned on music. The proposed AIST++ dataset\ncontains 5.2 hours of 3D dance motion in 1408 sequences, covering 10 dance\ngenres with multi-view videos with known camera poses -- the largest dataset of\nthis kind to our knowledge. We show that naively applying sequence models such\nas transformers to this dataset for the task of music conditioned 3D motion\ngeneration does not produce satisfactory 3D motion that is well correlated with\nthe input music. We overcome these shortcomings by introducing key changes in\nits architecture design and supervision: FACT model involves a deep cross-modal\ntransformer block with full-attention that is trained to predict $N$ future\nmotions. We empirically show that these changes are key factors in generating\nlong sequences of realistic dance motion that are well-attuned to the input\nmusic. We conduct extensive experiments on AIST++ with user studies, where our\nmethod outperforms recent state-of-the-art methods both qualitatively and\nquantitatively.",
          "link": "http://arxiv.org/abs/2101.08779",
          "publishedOn": "2021-08-03T02:06:31.779Z",
          "wordCount": 670,
          "title": "AI Choreographer: Music Conditioned 3D Dance Generation with AIST++. (arXiv:2101.08779v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.02620",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xia_T/0/1/0/all/0/1\">Tian Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chartsias_A/0/1/0/all/0/1\">Agisilaos Chartsias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chengjia Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>",
          "description": "How will my face look when I get older? Or, for a more challenging question:\nHow will my brain look when I get older? To answer this question one must\ndevise (and learn from data) a multivariate auto-regressive function which\ngiven an image and a desired target age generates an output image. While\ncollecting data for faces may be easier, collecting longitudinal brain data is\nnot trivial. We propose a deep learning-based method that learns to simulate\nsubject-specific brain ageing trajectories without relying on longitudinal\ndata. Our method synthesises images conditioned on two factors: age (a\ncontinuous variable), and status of Alzheimer's Disease (AD, an ordinal\nvariable). With an adversarial formulation we learn the joint distribution of\nbrain appearance, age and AD status, and define reconstruction losses to\naddress the challenging problem of preserving subject identity. We compare with\nseveral benchmarks using two widely used datasets. We evaluate the quality and\nrealism of synthesised images using ground-truth longitudinal data and a\npre-trained age predictor. We show that, despite the use of cross-sectional\ndata, our model learns patterns of gray matter atrophy in the middle temporal\ngyrus in patients with AD. To demonstrate generalisation ability, we train on\none dataset and evaluate predictions on the other. In conclusion, our model\nshows an ability to separate age, disease influence and anatomy using only 2D\ncross-sectional data that should should be useful in large studies into\nneurodegenerative disease, that aim to combine several data sources. To\nfacilitate such future studies by the community at large our code is made\navailable at https://github.com/xiat0616/BrainAgeing.",
          "link": "http://arxiv.org/abs/1912.02620",
          "publishedOn": "2021-08-03T02:06:31.769Z",
          "wordCount": 762,
          "title": "Learning to synthesise the ageing brain without longitudinal data. (arXiv:1912.02620v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeevan_P/0/1/0/all/0/1\">Pranav Jeevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1\">Amit Sethi</a> (Indian Institute of Technology Bombay)",
          "description": "We propose three improvements to vision transformers (ViT) to reduce the\nnumber of trainable parameters without compromising classification accuracy. We\naddress two shortcomings of the early ViT architectures -- quadratic bottleneck\nof the attention mechanism and the lack of an inductive bias in their\narchitectures that rely on unrolling the two-dimensional image structure.\nLinear attention mechanisms overcome the bottleneck of quadratic complexity,\nwhich restricts application of transformer models in vision tasks. We modify\nthe ViT architecture to work on longer sequence data by replacing the quadratic\nattention with efficient transformers, such as Performer, Linformer and\nNystr\\\"omformer of linear complexity creating Vision X-formers (ViX). We show\nthat all three versions of ViX may be more accurate than ViT for image\nclassification while using far fewer parameters and computational resources. We\nalso compare their performance with FNet and multi-layer perceptron (MLP)\nmixer. We further show that replacing the initial linear embedding layer by\nconvolutional layers in ViX further increases their performance. Furthermore,\nour tests on recent vision transformer models, such as LeViT, Convolutional\nvision Transformer (CvT), Compact Convolutional Transformer (CCT) and\nPooling-based Vision Transformer (PiT) show that replacing the attention with\nNystr\\\"omformer or Performer saves GPU usage and memory without deteriorating\nthe classification accuracy. We also show that replacing the standard learnable\n1D position embeddings in ViT with Rotary Position Embedding (RoPE) give\nfurther improvements in accuracy. Incorporating these changes can democratize\ntransformers by making them accessible to those with limited data and computing\nresources.",
          "link": "http://arxiv.org/abs/2107.02239",
          "publishedOn": "2021-08-03T02:06:31.748Z",
          "wordCount": 733,
          "title": "Vision Xformers: Efficient Attention for Image Classification. (arXiv:2107.02239v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>",
          "description": "Siamese tracking has achieved groundbreaking performance in recent years,\nwhere the essence is the efficient matching operator cross-correlation and its\nvariants. Besides the remarkable success, it is important to note that the\nheuristic matching network design relies heavily on expert experience.\nMoreover, we experimentally find that one sole matching operator is difficult\nto guarantee stable tracking in all challenging environments. Thus, in this\nwork, we introduce six novel matching operators from the perspective of feature\nfusion instead of explicit similarity learning, namely Concatenation,\nPointwise-Addition, Pairwise-Relation, FiLM, Simple-Transformer and\nTransductive-Guidance, to explore more feasibility on matching operator\nselection. The analyses reveal these operators' selective adaptability on\ndifferent environment degradation types, which inspires us to combine them to\nexplore complementary features. To this end, we propose binary channel\nmanipulation (BCM) to search for the optimal combination of these operators.\nBCM determines to retrain or discard one operator by learning its contribution\nto other tracking steps. By inserting the learned matching networks to a strong\nbaseline tracker Ocean, our model achieves favorable gains by $67.2 \\rightarrow\n71.4$, $52.6 \\rightarrow 58.3$, $70.3 \\rightarrow 76.0$ success on OTB100,\nLaSOT, and TrackingNet, respectively. Notably, Our tracker, dubbed AutoMatch,\nuses less than half of training data/time than the baseline tracker, and runs\nat 50 FPS using PyTorch. Code and model will be released at\nhttps://github.com/JudasDie/SOTS.",
          "link": "http://arxiv.org/abs/2108.00803",
          "publishedOn": "2021-08-03T02:06:31.725Z",
          "wordCount": 666,
          "title": "Learn to Match: Automatic Matching Network Design for Visual Tracking. (arXiv:2108.00803v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1\">Olivier Deforges</a>",
          "description": "Salient human detection (SHD) in dynamic 360{\\deg} immersive videos is of\ngreat importance for various applications such as robotics, inter-human and\nhuman-object interaction in augmented reality. However, 360{\\deg} video SHD has\nbeen seldom discussed in the computer vision community due to a lack of\ndatasets with large-scale omnidirectional videos and rich annotations. To this\nend, we propose SHD360, the first 360{\\deg} video SHD dataset which contains\nvarious real-life daily scenes. Our SHD360 provides six-level hierarchical\nannotations for 6,268 key frames uniformly sampled from 37,403 omnidirectional\nvideo frames at 4K resolution. Specifically, each collected frame is labeled\nwith a super-class, a sub-class, associated attributes (e.g., geometrical\ndistortion), bounding boxes and per-pixel object-/instance-level masks. As a\nresult, our SHD360 contains totally 16,238 salient human instances with\nmanually annotated pixel-wise ground truth. Since so far there is no method\nproposed for 360{\\deg} image/video SHD, we systematically benchmark 11\nrepresentative state-of-the-art salient object detection (SOD) approaches on\nour SHD360, and explore key issues derived from extensive experimenting\nresults. We hope our proposed dataset and benchmark could serve as a good\nstarting point for advancing human-centric researches towards 360{\\deg}\npanoramic data. Our dataset and benchmark will be publicly available at\nhttps://github.com/PanoAsh/SHD360.",
          "link": "http://arxiv.org/abs/2105.11578",
          "publishedOn": "2021-08-03T02:06:31.712Z",
          "wordCount": 691,
          "title": "SHD360: A Benchmark Dataset for Salient Human Detection in 360{\\deg} Videos. (arXiv:2105.11578v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sarah Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_E/0/1/0/all/0/1\">Esther Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anirudh Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Praveen_S/0/1/0/all/0/1\">Satyarth Praveen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1\">Meher Anand Kasam</a>",
          "description": "Due to the nature of their pathways, NASA Terra and NASA Aqua satellites\ncapture imagery containing swath gaps, which are areas of no data. Swath gaps\ncan overlap the region of interest (ROI) completely, often rendering the entire\nimagery unusable by Machine Learning (ML) models. This problem is further\nexacerbated when the ROI rarely occurs (e.g. a hurricane) and, on occurrence,\nis partially overlapped with a swath gap. With annotated data as supervision, a\nmodel can learn to differentiate between the area of focus and the swath gap.\nHowever, annotation is expensive and currently the vast majority of existing\ndata is unannotated. Hence, we propose an augmentation technique that\nconsiderably removes the existence of swath gaps in order to allow CNNs to\nfocus on the ROI, and thus successfully use data with swath gaps for training.\nWe experiment on the UC Merced Land Use Dataset, where we add swath gaps\nthrough empty polygons (up to 20 percent areas) and then apply augmentation\ntechniques to fill the swath gaps. We compare the model trained with our\naugmentation techniques on the swath gap-filled data with the model trained on\nthe original swath gap-less data and note highly augmented performance.\nAdditionally, we perform a qualitative analysis using activation maps that\nvisualizes the effectiveness of our trained network in not paying attention to\nthe swath gaps. We also evaluate our results with a human baseline and show\nthat, in certain cases, the filled swath gaps look so realistic that even a\nhuman evaluator did not distinguish between original satellite images and swath\ngap-filled images. Since this method is aimed at unlabeled data, it is widely\ngeneralizable and impactful for large scale unannotated datasets from various\nspace data domains.",
          "link": "http://arxiv.org/abs/2106.07113",
          "publishedOn": "2021-08-03T02:06:31.702Z",
          "wordCount": 790,
          "title": "Reducing Effects of Swath Gaps on Unsupervised Machine Learning Models for NASA MODIS Instruments. (arXiv:2106.07113v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.01037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiamei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1\">Sebastian Lapuschkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1\">Wojciech Samek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1\">Alexander Binder</a>",
          "description": "This paper analyzes the predictions of image captioning models with attention\nmechanisms beyond visualizing the attention itself. We develop variants of\nlayer-wise relevance propagation (LRP) and gradient-based explanation methods,\ntailored to image captioning models with attention mechanisms. We compare the\ninterpretability of attention heatmaps systematically against the explanations\nprovided by explanation methods such as LRP, Grad-CAM, and Guided Grad-CAM. We\nshow that explanation methods provide simultaneously pixel-wise image\nexplanations (supporting and opposing pixels of the input image) and linguistic\nexplanations (supporting and opposing words of the preceding sequence) for each\nword in the predicted captions. We demonstrate with extensive experiments that\nexplanation methods 1) can reveal additional evidence used by the model to make\ndecisions compared to attention; 2) correlate to object locations with high\nprecision; 3) are helpful to \"debug\" the model, e.g. by analyzing the reasons\nfor hallucinated object words. With the observed properties of explanations, we\nfurther design an LRP-inference fine-tuning strategy that reduces the issue of\nobject hallucination in image captioning models, and meanwhile, maintains the\nsentence fluency. We conduct experiments with two widely used attention\nmechanisms: the adaptive attention mechanism calculated with the additive\nattention and the multi-head attention mechanism calculated with the scaled dot\nproduct.",
          "link": "http://arxiv.org/abs/2001.01037",
          "publishedOn": "2021-08-03T02:06:31.685Z",
          "wordCount": 703,
          "title": "Explain and Improve: LRP-Inference Fine-Tuning for Image Captioning Models. (arXiv:2001.01037v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1\">Changyong Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>",
          "description": "Knowledge distillation (KD) has been proven to be a simple and effective tool\nfor training compact models. Almost all KD variants for dense prediction tasks\nalign the student and teacher networks' feature maps in the spatial domain,\ntypically by minimizing point-wise and/or pair-wise discrepancy. Observing that\nin semantic segmentation, some layers' feature activations of each channel tend\nto encode saliency of scene categories (analogue to class activation mapping),\nwe propose to align features channel-wise between the student and teacher\nnetworks. To this end, we first transform the feature map of each channel into\na probabilty map using softmax normalization, and then minimize the\nKullback-Leibler (KL) divergence of the corresponding channels of the two\nnetworks. By doing so, our method focuses on mimicking the soft distributions\nof channels between networks. In particular, the KL divergence enables learning\nto pay more attention to the most salient regions of the channel-wise maps,\npresumably corresponding to the most useful signals for semantic segmentation.\nExperiments demonstrate that our channel-wise distillation outperforms almost\nall existing spatial distillation methods for semantic segmentation\nconsiderably, and requires less computational cost during training. We\nconsistently achieve superior performance on three benchmarks with various\nnetwork structures. Code is available at: https://git.io/ChannelDis",
          "link": "http://arxiv.org/abs/2011.13256",
          "publishedOn": "2021-08-03T02:06:31.670Z",
          "wordCount": 690,
          "title": "Channel-wise Knowledge Distillation for Dense Prediction. (arXiv:2011.13256v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04007",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Legarreta_J/0/1/0/all/0/1\">Jon Haitz Legarreta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petit_L/0/1/0/all/0/1\">Laurent Petit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rheault_F/0/1/0/all/0/1\">Fran&#xe7;ois Rheault</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Theaud_G/0/1/0/all/0/1\">Guillaume Theaud</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lemaire_C/0/1/0/all/0/1\">Carl Lemaire</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Descoteaux_M/0/1/0/all/0/1\">Maxime Descoteaux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jodoin_P/0/1/0/all/0/1\">Pierre-Marc Jodoin</a>",
          "description": "Current brain white matter fiber tracking techniques show a number of\nproblems, including: generating large proportions of streamlines that do not\naccurately describe the underlying anatomy; extracting streamlines that are not\nsupported by the underlying diffusion signal; and under-representing some fiber\npopulations, among others. In this paper, we describe a novel autoencoder-based\nlearning method to filter streamlines from diffusion MRI tractography, and\nhence, to obtain more reliable tractograms. Our method, dubbed FINTA (Filtering\nin Tractography using Autoencoders) uses raw, unlabeled tractograms to train\nthe autoencoder, and to learn a robust representation of brain streamlines.\nSuch an embedding is then used to filter undesired streamline samples using a\nnearest neighbor algorithm. Our experiments on both synthetic and in vivo human\nbrain diffusion MRI tractography data obtain accuracy scores exceeding the 90\\%\nthreshold on the test set. Results reveal that FINTA has a superior filtering\nperformance compared to conventional, anatomy-based methods, and the\nRecoBundles state-of-the-art method. Additionally, we demonstrate that FINTA\ncan be applied to partial tractograms without requiring changes to the\nframework. We also show that the proposed method generalizes well across\ndifferent tracking methods and datasets, and shortens significantly the\ncomputation time for large (>1 M streamlines) tractograms. Together, this work\nbrings forward a new deep learning framework in tractography based on\nautoencoders, which offers a flexible and powerful method for white matter\nfiltering and bundling that could enhance tractometry and connectivity\nanalyses.",
          "link": "http://arxiv.org/abs/2010.04007",
          "publishedOn": "2021-08-03T02:06:31.662Z",
          "wordCount": 748,
          "title": "Filtering in tractography using autoencoders (FINTA). (arXiv:2010.04007v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1808.08186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Misra_R/0/1/0/all/0/1\">Rajesh Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_K/0/1/0/all/0/1\">Kumar S. Ray</a>",
          "description": "In Computer Vision,object tracking is a very old and complex problem.Though\nthere are several existing algorithms for object tracking, still there are\nseveral challenges remain to be solved. For instance, variation of illumination\nof light, noise, occlusion, sudden start and stop of moving object, shading\netc,make the object tracking a complex problem not only for dynamic background\nbut also for static background. In this paper we propose a dual approach for\nobject tracking based on optical flow and swarm Intelligence.The optical flow\nbased KLT(Kanade-Lucas-Tomasi) tracker, tracks the dominant points of the\ntarget object from first frame to last frame of a video sequence;whereas swarm\nIntelligence based PSO (Particle Swarm Optimization) tracker simultaneously\ntracks the boundary information of the target object from second frame to last\nframe of the same video sequence.This dual function of tracking makes the\ntrackers very much robust with respect to the above stated problems. The\nflexibility of our approach is that it can be successfully applicable in\nvariable background as well as static background.We compare the performance of\nthe proposed dual tracking algorithm with several benchmark datasets and obtain\nvery competitive results in general and in most of the cases we obtained\nsuperior results using dual tracking algorithm. We also compare the performance\nof the proposed dual tracker with some existing PSO based algorithms for\ntracking and achieved better results.",
          "link": "http://arxiv.org/abs/1808.08186",
          "publishedOn": "2021-08-03T02:06:31.636Z",
          "wordCount": 700,
          "title": "Dual approach for object tracking based on optical flow and swarm intelligence. (arXiv:1808.08186v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conway_D/0/1/0/all/0/1\">Dennis Conway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_L/0/1/0/all/0/1\">Loic Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lechervy_A/0/1/0/all/0/1\">Alexis Lechervy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurie_F/0/1/0/all/0/1\">Frederic Jurie</a>",
          "description": "Machine learning tools are becoming increasingly powerful and widely used.\nUnfortunately membership attacks, which seek to uncover information from data\nsets used in machine learning, have the potential to limit data sharing. In\nthis paper we consider an approach to increase the privacy protection of data\nsets, as applied to face recognition. Using an auxiliary face recognition\nmodel, we build on the StyleGAN generative adversarial network and feed it with\nlatent codes combining two distinct sub-codes, one encoding visual identity\nfactors, and, the other, non-identity factors. By independently varying these\nvectors during image generation, we create a synthetic data set of fictitious\nface identities. We use this data set to train a face recognition model. The\nmodel performance degrades in comparison to the state-of-the-art of face\nverification. When tested with a simple membership attack our model provides\ngood privacy protection, however the model performance degrades in comparison\nto the state-of-the-art of face verification. We find that the addition of a\nsmall amount of private data greatly improves the performance of our model,\nwhich highlights the limitations of using synthetic data to train machine\nlearning models.",
          "link": "http://arxiv.org/abs/2108.00800",
          "publishedOn": "2021-08-03T02:06:31.629Z",
          "wordCount": 622,
          "title": "Training face verification models from generated face identity data. (arXiv:2108.00800v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1904.05519",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1\">Uttaran Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Govindu_V/0/1/0/all/0/1\">Venu Madhav Govindu</a>",
          "description": "We present an accurate, robust and fast method for registration of 3D scans.\nOur motion estimation optimizes a robust cost function on the intrinsic\nrepresentation of rigid motions, i.e., the Special Euclidean group\n$\\mathbb{SE}(3)$. We exploit the geometric properties of Lie groups as well as\nthe robustness afforded by an iteratively reweighted least squares\noptimization. We also generalize our approach to a joint multiview method that\nsimultaneously solves for the registration of a set of scans. We demonstrate\nthe efficacy of our approach by thorough experimental validation. Our approach\nsignificantly outperforms the state-of-the-art robust 3D registration method\nbased on a line process in terms of both speed and accuracy. We also show that\nthis line process method is a special case of our principled geometric\nsolution. Finally, we also present scenarios where global registration based on\nfeature correspondences fails but multiview ICP based on our robust motion\nestimation is successful.",
          "link": "http://arxiv.org/abs/1904.05519",
          "publishedOn": "2021-08-03T02:06:31.613Z",
          "wordCount": 635,
          "title": "Efficient and Robust Registration on the 3D Special Euclidean Group. (arXiv:1904.05519v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.03727",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Andreeva_M/0/1/0/all/0/1\">M.V. Andreeva</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kalyuzhnyuk_A/0/1/0/all/0/1\">A.V. Kalyuzhnyuk</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Krutko_V/0/1/0/all/0/1\">V.V. Krutko</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Russkikh_N/0/1/0/all/0/1\">N.E. Russkikh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Taimanov_I/0/1/0/all/0/1\">I.A. Taimanov</a>",
          "description": "Representative Elementary Volume (REV) at which the material properties do\nnot vary with change in volume is an important quantity for making measurements\nor simulations which represent the whole. We discuss the geometrical method to\nevaluation of REV based on the quantities coming in the Steiner formula from\nconvex geometry. For bodies in the three-space this formula gives us four\nscalar functionals known as scalar Minkowski functionals. We demonstrate on\ncertain samples that the values of such averaged functionals almost stabilize\nfor cells for which the length of edges are greater than certain threshold\nvalue R. Therefore, from this point of view, it is reasonable to consider cubes\nof volume R^3 as representative elementary volumes.",
          "link": "http://arxiv.org/abs/2008.03727",
          "publishedOn": "2021-08-03T02:06:31.602Z",
          "wordCount": 576,
          "title": "Representative elementary volume via averaged scalar Minkowski functionals. (arXiv:2008.03727v2 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haichou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yishu Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeqin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haohua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1\">Bingzhong Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaofeng Li</a>",
          "description": "Delineating the lesion area is an important task in image-based diagnosis.\nPixel-wise classification is a popular approach to segmenting the region of\ninterest. However, at fuzzy boundaries such methods usually result in glitches,\ndiscontinuity, or disconnection, inconsistent with the fact that lesions are\nsolid and smooth. To overcome these undesirable artifacts, we propose the\nBezierSeg model which outputs bezier curves encompassing the region of\ninterest. Directly modelling the contour with analytic equations ensures that\nthe segmentation is connected, continuous, and the boundary is smooth. In\naddition, it offers sub-pixel accuracy. Without loss of accuracy, the bezier\ncontour can be resampled and overlaid with images of any resolution. Moreover,\na doctor can conveniently adjust the curve's control points to refine the\nresult. Our experiments show that the proposed method runs in real time and\nachieves accuracy competitive with pixel-wise segmentation models.",
          "link": "http://arxiv.org/abs/2108.00760",
          "publishedOn": "2021-08-03T02:06:31.596Z",
          "wordCount": 592,
          "title": "BezierSeg: Parametric Shape Representation for Fast Object Segmentation in Medical Images. (arXiv:2108.00760v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07200",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sun_K/0/1/0/all/0/1\">Kai Sun</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yanhua Gao</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Xie_T/0/1/0/all/0/1\">Ting Xie</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1\">Qingqing Yang</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Le Chen</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1\">Kuansong Wang</a> (3), <a href=\"http://arxiv.org/find/eess/1/au:+Yu_G/0/1/0/all/0/1\">Gang Yu</a> (1) ((1) Department of Biomedical Engineering, School of Basic Medical Sciences, Central South University, 172 Tongzipo Road, Changsha, 410013, China. (2) Department of Ultrasound, Shaanxi Provincial People&#x27;s Hospital,256 Youyixi Road, Xi&#x27;an, 710068, China. (3) Department of Pathology, School of Basic Medical Sciences, Central South University, 172 Tongzipo Road, Changsha, 410013, China.)",
          "description": "Background. Digital pathology has aroused widespread interest in modern\npathology. The key of digitalization is to scan the whole slide image (WSI) at\nhigh magnification. The lager the magnification is, the richer details WSI will\nprovide, but the scanning time is longer and the file size of obtained is\nlarger. Methods. We design a strategy to scan slides with low resolution (5X)\nand a super-resolution method is proposed to restore the image details when in\ndiagnosis. The method is based on a multi-scale generative adversarial network,\nwhich sequentially generates three high-resolution images such as 10X, 20X and\n40X. Results. The peak-signal-to-noise-ratio of 10X to 40X generated images are\n24.16, 22.27 and 20.44, and the structural-similarity-index are 0.845, 0.680\nand 0.512, which are better than other super-resolution networks. Visual\nscoring average and standard deviation from three pathologists is 3.63\nplus-minus 0.52, 3.70 plus-minus 0.57 and 3.74 plus-minus 0.56 and the p value\nof analysis of variance is 0.37, indicating that generated images include\nsufficient information for diagnosis. The average value of Kappa test is 0.99,\nmeaning the diagnosis of generated images is highly consistent with that of the\nreal images. Conclusion. This proposed method can generate high-quality 10X,\n20X, 40X images from 5X images at the same time, in which the time and storage\ncosts of digitalization can be effectively reduced up to 1/64 of the previous\ncosts. The proposed method provides a better alternative for low-cost storage,\nfaster image share of digital pathology. Keywords. Digital pathology;\nSuper-resolution; Low resolution scanning; Low cost",
          "link": "http://arxiv.org/abs/2105.07200",
          "publishedOn": "2021-08-03T02:06:31.477Z",
          "wordCount": 783,
          "title": "Multi-scale super-resolution generation of low-resolution scanned pathological images. (arXiv:2105.07200v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1\">Anusua Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1\">Mohit Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1\">Nikhil Kumar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinsche_M/0/1/0/all/0/1\">Markus Hinsche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Prashant Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matiaschek_M/0/1/0/all/0/1\">Markus Matiaschek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behrens_T/0/1/0/all/0/1\">Tristan Behrens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Militeri_M/0/1/0/all/0/1\">Mirco Militeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birge_C/0/1/0/all/0/1\">Cameron Birge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaushik_S/0/1/0/all/0/1\">Shivangi Kaushik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_A/0/1/0/all/0/1\">Archisman Mohapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_R/0/1/0/all/0/1\">Rita Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodhia_R/0/1/0/all/0/1\">Rahul Dodhia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1\">Juan Lavista Ferres</a>",
          "description": "Malnutrition is a global health crisis and is the leading cause of death\namong children under five. Detecting malnutrition requires anthropometric\nmeasurements of weight, height, and middle-upper arm circumference. However,\nmeasuring them accurately is a challenge, especially in the global south, due\nto limited resources. In this work, we propose a CNN-based approach to estimate\nthe height of standing children under five years from depth images collected\nusing a smart-phone. According to the SMART Methodology Manual [5], the\nacceptable accuracy for height is less than 1.4 cm. On training our deep\nlearning model on 87131 depth images, our model achieved an average mean\nabsolute error of 1.64% on 57064 test images. For 70.3% test images, we\nestimated height accurately within the acceptable 1.4 cm range. Thus, our\nproposed solution can accurately detect stunting (low height-for-age) in\nstanding children below five years of age.",
          "link": "http://arxiv.org/abs/2105.01688",
          "publishedOn": "2021-08-03T02:06:31.441Z",
          "wordCount": 641,
          "title": "Height Estimation of Children under Five Years using Depth Images. (arXiv:2105.01688v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14255",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kang_M/0/1/0/all/0/1\">Myeongkyun Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chikontwe_P/0/1/0/all/0/1\">Philip Chikontwe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luna_M/0/1/0/all/0/1\">Miguel Luna</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hong_K/0/1/0/all/0/1\">Kyung Soo Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahn_J/0/1/0/all/0/1\">June Hong Ahn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1\">Sang Hyun Park</a>",
          "description": "Following the pandemic outbreak, several works have proposed to diagnose\nCOVID-19 with deep learning in computed tomography (CT); reporting performance\non-par with experts. However, models trained/tested on the same in-distribution\ndata may rely on the inherent data biases for successful prediction, failing to\ngeneralize on out-of-distribution samples or CT with different scanning\nprotocols. Early attempts have partly addressed bias-mitigation and\ngeneralization through augmentation or re-sampling, but are still limited by\ncollection costs and the difficulty of quantifying bias in medical images. In\nthis work, we propose Mixing-AdaSIN; a bias mitigation method that uses a\ngenerative model to generate de-biased images by mixing texture information\nbetween different labeled CT scans with semantically similar features. Here, we\nuse Adaptive Structural Instance Normalization (AdaSIN) to enhance de-biasing\ngeneration quality and guarantee structural consistency. Following, a\nclassifier trained with the generated images learns to correctly predict the\nlabel without bias and generalizes better. To demonstrate the efficacy of our\nmethod, we construct a biased COVID-19 vs. bacterial pneumonia dataset based on\nCT protocols and compare with existing state-of-the-art de-biasing methods. Our\nexperiments show that classifiers trained with de-biased generated images\nreport improved in-distribution performance and generalization on an external\nCOVID-19 dataset.",
          "link": "http://arxiv.org/abs/2103.14255",
          "publishedOn": "2021-08-03T02:06:31.434Z",
          "wordCount": 721,
          "title": "Mixing-AdaSIN: Constructing a De-biased Dataset using Adaptive Structural Instance Normalization and Texture Mixing. (arXiv:2103.14255v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_P/0/1/0/all/0/1\">Pablo G&#xf3;mez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meoni_G/0/1/0/all/0/1\">Gabriele Meoni</a>",
          "description": "Supervised learning techniques are at the center of many tasks in remote\nsensing. Unfortunately, these methods, especially recent deep learning methods,\noften require large amounts of labeled data for training. Even though\nsatellites acquire large amounts of data, labeling the data is often tedious,\nexpensive and requires expert knowledge. Hence, improved methods that require\nfewer labeled samples are needed. We present MSMatch, the first semi-supervised\nlearning approach competitive with supervised methods on scene classification\non the EuroSAT and UC Merced Land Use benchmark datasets. We test both RGB and\nmultispectral images of EuroSAT and perform various ablation studies to\nidentify the critical parts of the model. The trained neural network achieves\nstate-of-the-art results on EuroSAT with an accuracy that is up to 19.76%\nbetter than previous methods depending on the number of labeled training\nexamples. With just five labeled examples per class, we reach 94.53% and 95.86%\naccuracy on the EuroSAT RGB and multispectral datasets, respectively. On the UC\nMerced Land Use dataset, we outperform previous works by up to 5.59% and reach\n90.71% with five labeled examples. Our results show that MSMatch is capable of\ngreatly reducing the requirements for labeled data. It translates well to\nmultispectral data and should enable various applications that are currently\ninfeasible due to a lack of labeled data. We provide the source code of MSMatch\nonline to enable easy reproduction and quick adoption.",
          "link": "http://arxiv.org/abs/2103.10368",
          "publishedOn": "2021-08-03T02:06:31.416Z",
          "wordCount": 713,
          "title": "MSMatch: Semi-Supervised Multispectral Scene Classification with Few Labels. (arXiv:2103.10368v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonard_J/0/1/0/all/0/1\">John J. Leonard</a>",
          "description": "For a robot deployed in the world, it is desirable to have the ability of\nautonomous learning to improve its initial pre-set knowledge. We formalize this\nas a bootstrapped self-supervised learning problem where a system is initially\nbootstrapped with supervised training on a labeled dataset and we look for a\nself-supervised training method that can subsequently improve the system over\nthe supervised training baseline using only unlabeled data. In this work, we\nleverage temporal consistency between frames in monocular video to perform this\nbootstrapped self-supervised training. We show that a well-trained\nstate-of-the-art semantic segmentation network can be further improved through\nour method. In addition, we show that the bootstrapped self-supervised training\nframework can help a network learn depth estimation better than pure supervised\ntraining or self-supervised training.",
          "link": "http://arxiv.org/abs/2103.11031",
          "publishedOn": "2021-08-03T02:06:31.403Z",
          "wordCount": 602,
          "title": "Bootstrapped Self-Supervised Training with Monocular Video for Semantic Segmentation and Depth Estimation. (arXiv:2103.11031v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiangpeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengqing Zhu</a>",
          "description": "Continual learning aims to learn new tasks incrementally using less\ncomputation and memory resources instead of retraining the model from scratch\nwhenever new task arrives. However, existing approaches are designed in\nsupervised fashion assuming all data from new tasks have been manually\nannotated, which are not practical for many real-life applications. In this\nwork, we propose to use pseudo label instead of the ground truth to make\ncontinual learning feasible in unsupervised mode. The pseudo labels of new data\nare obtained by applying global clustering algorithm and we propose to use the\nmodel updated from last incremental step as the feature extractor. Due to the\nscarcity of existing work, we introduce a new benchmark experimental protocol\nfor unsupervised continual learning of image classification task under\nclass-incremental setting where no class label is provided for each incremental\nlearning step. Our method is evaluated on the CIFAR-100 and ImageNet (ILSVRC)\ndatasets by incorporating the pseudo label with various existing supervised\napproaches and show promising results in unsupervised scenario.",
          "link": "http://arxiv.org/abs/2104.07164",
          "publishedOn": "2021-08-03T02:06:31.397Z",
          "wordCount": 636,
          "title": "Unsupervised Continual Learning Via Pseudo Labels. (arXiv:2104.07164v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_F/0/1/0/all/0/1\">Feifei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yawei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1\">Lu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>",
          "description": "The recent emerged weakly supervised object localization (WSOL) methods can\nlearn to localize an object in the image only using image-level labels.\nPrevious works endeavor to perceive the interval objects from the small and\nsparse discriminative attention map, yet ignoring the co-occurrence confounder\n(e.g., bird and sky), which makes the model inspection (e.g., CAM) hard to\ndistinguish between the object and context. In this paper, we make an early\nattempt to tackle this challenge via causal intervention (CI). Our proposed\nmethod, dubbed CI-CAM, explores the causalities among images, contexts, and\ncategories to eliminate the biased co-occurrence in the class activation maps\nthus improving the accuracy of object localization. Extensive experiments on\nseveral benchmarks demonstrate the effectiveness of CI-CAM in learning the\nclear object boundaries from confounding contexts. Particularly, in\nCUB-200-2011 which severely suffers from the co-occurrence confounder, CI-CAM\nsignificantly outperforms the traditional CAM-based baseline (58.39% vs 52.4%\nin top-1 localization accuracy). While in more general scenarios such as\nImageNet, CI-CAM can also perform on par with the state of the arts.",
          "link": "http://arxiv.org/abs/2104.10351",
          "publishedOn": "2021-08-03T02:06:31.375Z",
          "wordCount": 663,
          "title": "Improving Weakly-supervised Object Localization via Causal Intervention. (arXiv:2104.10351v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Stan Weixian Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadiyaram_D/0/1/0/all/0/1\">Deepti Ghadiyaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feiszli_M/0/1/0/all/0/1\">Matt Feiszli</a>",
          "description": "This paper presents a novel task together with a new benchmark for detecting\ngeneric, taxonomy-free event boundaries that segment a whole video into chunks.\nConventional work in temporal video segmentation and action detection focuses\non localizing pre-defined action categories and thus does not scale to generic\nvideos. Cognitive Science has known since last century that humans consistently\nsegment videos into meaningful temporal chunks. This segmentation happens\nnaturally, without pre-defined event categories and without being explicitly\nasked to do so. Here, we repeat these cognitive experiments on mainstream CV\ndatasets; with our novel annotation guideline which addresses the complexities\nof taxonomy-free event boundary annotation, we introduce the task of Generic\nEvent Boundary Detection (GEBD) and the new benchmark Kinetics-GEBD. Our\nKinetics-GEBD has the largest number of boundaries (e.g. 32 of ActivityNet, 8\nof EPIC-Kitchens-100) which are in-the-wild, taxonomy-free, cover generic event\nchange, and respect human perception diversity. We view GEBD as an important\nstepping stone towards understanding the video as a whole, and believe it has\nbeen previously neglected due to a lack of proper task definition and\nannotations. Through experiment and human study we demonstrate the value of the\nannotations. Further, we benchmark supervised and un-supervised GEBD approaches\non the TAPOS dataset and our Kinetics-GEBD, together with method design\nexplorations that suggest future directions. We release our annotations and\nbaseline codes at CVPR'21 LOVEU Challenge:\nhttps://sites.google.com/view/loveucvpr21.",
          "link": "http://arxiv.org/abs/2101.10511",
          "publishedOn": "2021-08-03T02:06:31.367Z",
          "wordCount": 719,
          "title": "Generic Event Boundary Detection: A Benchmark for Event Segmentation. (arXiv:2101.10511v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lantao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dehong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_H/0/1/0/all/0/1\">Hassan Mansour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boufounos_P/0/1/0/all/0/1\">Petros T. Boufounos</a>",
          "description": "Blind pansharpening addresses the problem of generating a high\nspatial-resolution multi-spectral (HRMS) image given a low spatial-resolution\nmulti-spectral (LRMS) image with the guidance of its associated spatially\nmisaligned high spatial-resolution panchromatic (PAN) image without parametric\nside information. In this paper, we propose a fast approach to blind\npansharpening and achieve state-of-the-art image reconstruction quality.\nTypical blind pansharpening algorithms are often computationally intensive\nsince the blur kernel and the target HRMS image are often computed using\niterative solvers and in an alternating fashion. To achieve fast blind\npansharpening, we decouple the solution of the blur kernel and of the HRMS\nimage. First, we estimate the blur kernel by computing the kernel coefficients\nwith minimum total generalized variation that blur a downsampled version of the\nPAN image to approximate a linear combination of the LRMS image channels. Then,\nwe estimate each channel of the HRMS image using local Laplacian prior to\nregularize the relationship between each HRMS channel and the PAN image.\nSolving the HRMS image is accelerated by both parallelizing across the channels\nand by fast numerical algorithms for each channel. Due to the fast scheme and\nthe powerful priors we used on the blur kernel coefficients (total generalized\nvariation) and on the cross-channel relationship (local Laplacian prior),\nnumerical experiments demonstrate that our algorithm outperforms\nstate-of-the-art model-based counterparts in terms of both computational time\nand reconstruction quality of the HRMS images.",
          "link": "http://arxiv.org/abs/2103.09943",
          "publishedOn": "2021-08-03T02:06:31.361Z",
          "wordCount": 730,
          "title": "Fast and High-Quality Blind Multi-Spectral Image Pansharpening. (arXiv:2103.09943v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Anindita Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheema_N/0/1/0/all/0/1\">Noshaba Cheema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_C/0/1/0/all/0/1\">Cennet Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slusallek_P/0/1/0/all/0/1\">Philipp Slusallek</a>",
          "description": "\"How can we animate 3D-characters from a movie script or move robots by\nsimply telling them what we would like them to do?\" \"How unstructured and\ncomplex can we make a sentence and still generate plausible movements from it?\"\nThese are questions that need to be answered in the long-run, as the field is\nstill in its infancy. Inspired by these problems, we present a new technique\nfor generating compositional actions, which handles complex input sentences.\nOur output is a 3D pose sequence depicting the actions in the input sentence.\nWe propose a hierarchical two-stream sequential model to explore a finer\njoint-level mapping between natural language sentences and 3D pose sequences\ncorresponding to the given motion. We learn two manifold representations of the\nmotion -- one each for the upper body and the lower body movements. Our model\ncan generate plausible pose sequences for short sentences describing single\nactions as well as long compositional sentences describing multiple sequential\nand superimposed actions. We evaluate our proposed model on the publicly\navailable KIT Motion-Language Dataset containing 3D pose data with\nhuman-annotated sentences. Experimental results show that our model advances\nthe state-of-the-art on text-based motion synthesis in objective evaluations by\na margin of 50%. Qualitative evaluations based on a user study indicate that\nour synthesized motions are perceived to be the closest to the ground-truth\nmotion captures for both short and compositional sentences.",
          "link": "http://arxiv.org/abs/2103.14675",
          "publishedOn": "2021-08-03T02:06:31.350Z",
          "wordCount": 723,
          "title": "Synthesis of Compositional Animations from Textual Descriptions. (arXiv:2103.14675v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13833",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Calli_E/0/1/0/all/0/1\">Erdi &#xc7;all&#x131;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Murphy_K/0/1/0/all/0/1\">Keelin Murphy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurstjens_S/0/1/0/all/0/1\">Steef Kurstjens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samson_T/0/1/0/all/0/1\">Tijs Samson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herpers_R/0/1/0/all/0/1\">Robert Herpers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smits_H/0/1/0/all/0/1\">Henk Smits</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rutten_M/0/1/0/all/0/1\">Matthieu Rutten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginneken_B/0/1/0/all/0/1\">Bram van Ginneken</a>",
          "description": "In the context of the current global pandemic and the limitations of the\nRT-PCR test, we propose a novel deep learning architecture, DFCN (Denoising\nFully Connected Network). Since medical facilities around the world differ\nenormously in what laboratory tests or chest imaging may be available, DFCN is\ndesigned to be robust to missing input data. An ablation study extensively\nevaluates the performance benefits of the DFCN as well as its robustness to\nmissing inputs. Data from 1088 patients with confirmed RT-PCR results are\nobtained from two independent medical facilities. The data includes results\nfrom 27 laboratory tests and a chest x-ray scored by a deep learning model.\nTraining and test datasets are taken from different medical facilities. Data is\nmade publicly available. The performance of DFCN in predicting the RT-PCR\nresult is compared with 3 related architectures as well as a Random Forest\nbaseline. All models are trained with varying levels of masked input data to\nencourage robustness to missing inputs. Missing data is simulated at test time\nby masking inputs randomly. DFCN outperforms all other models with statistical\nsignificance using random subsets of input data with 2-27 available inputs.\nWhen all 28 inputs are available DFCN obtains an AUC of 0.924, higher than any\nother model. Furthermore, with clinically meaningful subsets of parameters\nconsisting of just 6 and 7 inputs respectively, DFCN achieves higher AUCs than\nany other model, with values of 0.909 and 0.919.",
          "link": "http://arxiv.org/abs/2103.13833",
          "publishedOn": "2021-08-03T02:06:31.341Z",
          "wordCount": 766,
          "title": "Deep Learning with robustness to missing data: A novel approach to the detection of COVID-19. (arXiv:2103.13833v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jianlong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhibin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>",
          "description": "Recently, significant progress has been made on semantic segmentation.\nHowever, the success of supervised semantic segmentation typically relies on a\nlarge amount of labelled data, which is time-consuming and costly to obtain.\nInspired by the success of semi-supervised learning methods in image\nclassification, here we propose a simple yet effective semi-supervised learning\nframework for semantic segmentation. We demonstrate that the devil is in the\ndetails: a set of simple design and training techniques can collectively\nimprove the performance of semi-supervised semantic segmentation significantly.\nPrevious works [3, 27] fail to employ strong augmentation in pseudo label\nlearning efficiently, as the large distribution change caused by strong\naugmentation harms the batch normalisation statistics. We design a new batch\nnormalisation, namely distribution-specific batch normalisation (DSBN) to\naddress this problem and demonstrate the importance of strong augmentation for\nsemantic segmentation. Moreover, we design a self correction loss which is\neffective in noise resistance. We conduct a series of ablation studies to show\nthe effectiveness of each component. Our method achieves state-of-the-art\nresults in the semi-supervised settings on the Cityscapes and Pascal VOC\ndatasets.",
          "link": "http://arxiv.org/abs/2104.07256",
          "publishedOn": "2021-08-03T02:06:31.305Z",
          "wordCount": 670,
          "title": "A Simple Baseline for Semi-supervised Semantic Segmentation with Strong Data Augmentation. (arXiv:2104.07256v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.12175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Ying Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hairong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwan_C/0/1/0/all/0/1\">Chiman Kwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoya_N/0/1/0/all/0/1\">Naoto Yokoya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>",
          "description": "Hyperspectral images (HSI) provide rich spectral information that contributed\nto the successful performance improvement of numerous computer vision tasks.\nHowever, it can only be achieved at the expense of images' spatial resolution.\nHyperspectral image super-resolution (HSI-SR) addresses this problem by fusing\nlow resolution (LR) HSI with multispectral image (MSI) carrying much higher\nspatial resolution (HR). All existing HSI-SR approaches require the LR HSI and\nHR MSI to be well registered and the reconstruction accuracy of the HR HSI\nrelies heavily on the registration accuracy of different modalities. This paper\nexploits the uncharted problem domain of HSI-SR without the requirement of\nmulti-modality registration. Given the unregistered LR HSI and HR MSI with\noverlapped regions, we design a unique unsupervised learning structure linking\nthe two unregistered modalities by projecting them into the same statistical\nspace through the same encoder. The mutual information (MI) is further adopted\nto capture the non-linear statistical dependencies between the representations\nfrom two modalities (carrying spatial information) and their raw inputs. By\nmaximizing the MI, spatial correlations between different modalities can be\nwell characterized to further reduce the spectral distortion. A collaborative\n$l_{2,1}$ norm is employed as the reconstruction error instead of the more\ncommon $l_2$ norm, so that individual pixels can be recovered as accurately as\npossible. With this design, the network allows to extract correlated spectral\nand spatial information from unregistered images that better preserves the\nspectral information. The proposed method is referred to as unregistered and\nunsupervised mutual Dirichlet Net ($u^2$-MDN). Extensive experimental results\nusing benchmark HSI datasets demonstrate the superior performance of $u^2$-MDN\nas compared to the state-of-the-art.",
          "link": "http://arxiv.org/abs/1904.12175",
          "publishedOn": "2021-08-03T02:06:31.292Z",
          "wordCount": 776,
          "title": "Unsupervised and Unregistered Hyperspectral Image Super-Resolution with Mutual Dirichlet-Net. (arXiv:1904.12175v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1\">Md Afzal Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meraz_M/0/1/0/all/0/1\">Md Meraz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Pavan Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1\">Mohammed Javed</a>",
          "description": "In this paper, we present new feature encoding methods for Detection of 3D\nobjects in point clouds. We used a graph neural network (GNN) for Detection of\n3D objects namely cars, pedestrians, and cyclists. Feature encoding is one of\nthe important steps in Detection of 3D objects. The dataset used is point cloud\ndata which is irregular and unstructured and it needs to be encoded in such a\nway that ensures better feature encapsulation. Earlier works have used relative\ndistance as one of the methods to encode the features. These methods are not\nresistant to rotation variance problems in Graph Neural Networks. We have\nincluded angular-based measures while performing feature encoding in graph\nneural networks. Along with that, we have performed a comparison between other\nmethods like Absolute, Relative, Euclidean distances, and a combination of the\nAngle and Relative methods. The model is trained and evaluated on the subset of\nthe KITTI object detection benchmark dataset under resource constraints. Our\nresults demonstrate that a combination of angle measures and relative distance\nhas performed better than other methods. In comparison to the baseline\nmethod(relative), it achieved better performance. We also performed time\nanalysis of various feature encoding methods.",
          "link": "http://arxiv.org/abs/2108.00780",
          "publishedOn": "2021-08-03T02:06:31.285Z",
          "wordCount": 652,
          "title": "Angle Based Feature Learning in GNN for 3D Object Detection using Point Cloud. (arXiv:2108.00780v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuxin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_H/0/1/0/all/0/1\">Haoran Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F Frangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jianqiao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>",
          "description": "Nodule segmentation from breast ultrasound images is challenging yet\nessential for the diagnosis. Weakly-supervised segmentation (WSS) can help\nreduce time-consuming and cumbersome manual annotation. Unlike existing\nweakly-supervised approaches, in this study, we propose a novel and general WSS\nframework called Flip Learning, which only needs the box annotation.\nSpecifically, the target in the label box will be erased gradually to flip the\nclassification tag, and the erased region will be considered as the\nsegmentation result finally. Our contribution is three-fold. First, our\nproposed approach erases on superpixel level using a Multi-agent Reinforcement\nLearning framework to exploit the prior boundary knowledge and accelerate the\nlearning process. Second, we design two rewards: classification score and\nintensity distribution reward, to avoid under- and over-segmentation,\nrespectively. Third, we adopt a coarse-to-fine learning strategy to reduce the\nresidual errors and improve the segmentation performance. Extensively validated\non a large dataset, our proposed approach achieves competitive performance and\nshows great potential to narrow the gap between fully-supervised and\nweakly-supervised learning.",
          "link": "http://arxiv.org/abs/2108.00752",
          "publishedOn": "2021-08-03T02:06:31.274Z",
          "wordCount": 624,
          "title": "Flip Learning: Erase to Segment. (arXiv:2108.00752v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.13744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reiser_C/0/1/0/all/0/1\">Christian Reiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Songyou Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yiyi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>",
          "description": "NeRF synthesizes novel views of a scene with unprecedented quality by fitting\na neural radiance field to RGB images. However, NeRF requires querying a deep\nMulti-Layer Perceptron (MLP) millions of times, leading to slow rendering\ntimes, even on modern GPUs. In this paper, we demonstrate that real-time\nrendering is possible by utilizing thousands of tiny MLPs instead of one single\nlarge MLP. In our setting, each individual MLP only needs to represent parts of\nthe scene, thus smaller and faster-to-evaluate MLPs can be used. By combining\nthis divide-and-conquer strategy with further optimizations, rendering is\naccelerated by three orders of magnitude compared to the original NeRF model\nwithout incurring high storage costs. Further, using teacher-student\ndistillation for training, we show that this speed-up can be achieved without\nsacrificing visual quality.",
          "link": "http://arxiv.org/abs/2103.13744",
          "publishedOn": "2021-08-03T02:06:31.265Z",
          "wordCount": 614,
          "title": "KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs. (arXiv:2103.13744v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanzhina_N/0/1/0/all/0/1\">Natalia Khanzhina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapenok_A/0/1/0/all/0/1\">Alexey Lapenok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1\">Andrey Filchenkov</a>",
          "description": "According to recent studies, commonly used computer vision datasets contain\nabout 4% of label errors. For example, the COCO dataset is known for its high\nlevel of noise in data labels, which limits its use for training robust neural\ndeep architectures in a real-world scenario. To model such a noise, in this\npaper we have proposed the homoscedastic aleatoric uncertainty estimation, and\npresent a series of novel loss functions to address the problem of image object\ndetection at scale. Specifically, the proposed functions are based on Bayesian\ninference and we have incorporated them into the common community-adopted\nobject detection deep learning architecture RetinaNet. We have also shown that\nmodeling of homoscedastic aleatoric uncertainty using our novel functions\nallows to increase the model interpretability and to improve the object\ndetection performance being evaluated on the COCO dataset.",
          "link": "http://arxiv.org/abs/2108.00784",
          "publishedOn": "2021-08-03T02:06:31.244Z",
          "wordCount": 587,
          "title": "Towards Robust Object Detection: Bayesian RetinaNet for Homoscedastic Aleatoric Uncertainty Modeling. (arXiv:2108.00784v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parchami_M/0/1/0/all/0/1\">Mostafa Parchami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayed_S/0/1/0/all/0/1\">Saif Iftekar Sayed</a>",
          "description": "Feature tracking is the building block of many applications such as visual\nodometry, augmented reality, and target tracking. Unfortunately, the\nstate-of-the-art vision-based tracking algorithms fail in surgical images due\nto the challenges imposed by the nature of such environments. In this paper, we\nproposed a novel and unified deep learning-based approach that can learn how to\ntrack features reliably as well as learn how to detect such reliable features\nfor tracking purposes. The proposed network dubbed as Deep-PT, consists of a\ntracker network which is a convolutional neural network simulating\ncross-correlation in terms of deep learning and two fully connected networks\nthat operate on the output of intermediate layers of the tracker to detect\nfeatures and predict trackability of the detected points. The ability to detect\nfeatures based on the capabilities of the tracker distinguishes the proposed\nmethod from previous algorithms used in this area and improves the robustness\nof the algorithms against dynamics of the scene. The network is trained using\nmultiple datasets due to the lack of specialized dataset for feature tracking\ndatasets and extensive comparisons are conducted to compare the accuracy of\nDeep-PT against recent pixel tracking algorithms. As the experiments suggest,\nthe proposed deep architecture deliberately learns what to track and how to\ntrack and outperforms the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.00105",
          "publishedOn": "2021-08-03T02:06:31.238Z",
          "wordCount": 658,
          "title": "Deep Feature Tracker: A Novel Application for Deep Convolutional Neural Networks. (arXiv:2108.00105v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00690",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huilin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1\">Junyan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pujin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>",
          "description": "We innovatively propose a flexible and consistent face alignment framework,\nLDDMM-Face, the key contribution of which is a deformation layer that naturally\nembeds facial geometry in a diffeomorphic way. Instead of predicting facial\nlandmarks via heatmap or coordinate regression, we formulate this task in a\ndiffeomorphic registration manner and predict momenta that uniquely\nparameterize the deformation between initial boundary and true boundary, and\nthen perform large deformation diffeomorphic metric mapping (LDDMM)\nsimultaneously for curve and landmark to localize the facial landmarks. Due to\nthe embedding of LDDMM into a deep network, LDDMM-Face can consistently\nannotate facial landmarks without ambiguity and flexibly handle various\nannotation schemes, and can even predict dense annotations from sparse ones.\nOur method can be easily integrated into various face alignment networks. We\nextensively evaluate LDDMM-Face on four benchmark datasets: 300W, WFLW, HELEN\nand COFW-68. LDDMM-Face is comparable or superior to state-of-the-art methods\nfor traditional within-dataset and same-annotation settings, but truly\ndistinguishes itself with outstanding performance when dealing with\nweakly-supervised learning (partial-to-full), challenging cases (e.g., occluded\nfaces), and different training and prediction datasets. In addition, LDDMM-Face\nshows promising results on the most challenging task of predicting across\ndatasets with different annotation schemes.",
          "link": "http://arxiv.org/abs/2108.00690",
          "publishedOn": "2021-08-03T02:06:31.233Z",
          "wordCount": 644,
          "title": "LDDMM-Face: Large Deformation Diffeomorphic Metric Learning for Flexible and Consistent Face Alignment. (arXiv:2108.00690v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huayan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1\">Tin Lun Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijayakumar_S/0/1/0/all/0/1\">Sethu Vijayakumar</a>",
          "description": "Dynamic environments that include unstructured moving objects pose a hard\nproblem for Simultaneous Localization and Mapping (SLAM) performance. The\nmotion of rigid objects can be typically tracked by exploiting their texture\nand geometric features. However, humans moving in the scene are often one of\nthe most important, interactive targets - they are very hard to track and\nreconstruct robustly due to non-rigid shapes. In this work, we present a fast,\nlearning-based human object detector to isolate the dynamic human objects and\nrealise a real-time dense background reconstruction framework. We go further by\nestimating and reconstructing the human pose and shape. The final output\nenvironment maps not only provide the dense static backgrounds but also contain\nthe dynamic human meshes and their trajectories. Our Dynamic SLAM system runs\nat around 26 frames per second (fps) on GPUs, while additionally turning on\naccurate human pose estimation can be executed at up to 10 fps.",
          "link": "http://arxiv.org/abs/2108.00695",
          "publishedOn": "2021-08-03T02:06:31.221Z",
          "wordCount": 602,
          "title": "PoseFusion2: Simultaneous Background Reconstruction and Human Shape Recovery in Real-time. (arXiv:2108.00695v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingsong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hai Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhimin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>",
          "description": "Automated tagging of video advertisements has been a critical yet challenging\nproblem, and it has drawn increasing interests in last years as its\napplications seem to be evident in many fields. Despite sustainable efforts\nhave been made, the tagging task is still suffered from several challenges,\nsuch as, efficiently feature fusion approach is desirable, but under-explored\nin previous studies. In this paper, we present our approach for Multimodal\nVideo Ads Tagging in the 2021 Tencent Advertising Algorithm Competition.\nSpecifically, we propose a novel multi-modal feature fusion framework, with the\ngoal to combine complementary information from multiple modalities. This\nframework introduces stacking-based ensembling approach to reduce the influence\nof varying levels of noise and conflicts between different modalities. Thus,\nour framework can boost the performance of the tagging task, compared to\nprevious methods. To empirically investigate the effectiveness and robustness\nof the proposed framework, we conduct extensive experiments on the challenge\ndatasets. The obtained results suggest that our framework can significantly\noutperform related approaches and our method ranks as the 1st place on the\nfinal leaderboard, with a Global Average Precision (GAP) of 82.63%. To better\npromote the research in this field, we will release our code in the final\nversion.",
          "link": "http://arxiv.org/abs/2108.00679",
          "publishedOn": "2021-08-03T02:06:31.213Z",
          "wordCount": 658,
          "title": "Multimodal Feature Fusion for Video Advertisements Tagging Via Stacking Ensemble. (arXiv:2108.00679v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>",
          "description": "It is widely acknowledged that learning joint embeddings of recipes with\nimages is challenging due to the diverse composition and deformation of\ningredients in cooking procedures. We present a Multi-modal Semantics enhanced\nJoint Embedding approach (MSJE) for learning a common feature space between the\ntwo modalities (text and image), with the ultimate goal of providing\nhigh-performance cross-modal retrieval services. Our MSJE approach has three\nunique features. First, we extract the TFIDF feature from the title,\ningredients and cooking instructions of recipes. By determining the\nsignificance of word sequences through combining LSTM learned features with\ntheir TFIDF features, we encode a recipe into a TFIDF weighted vector for\ncapturing significant key terms and how such key terms are used in the\ncorresponding cooking instructions. Second, we combine the recipe TFIDF feature\nwith the recipe sequence feature extracted through two-stage LSTM networks,\nwhich is effective in capturing the unique relationship between a recipe and\nits associated image(s). Third, we further incorporate TFIDF enhanced category\nsemantics to improve the mapping of image modality and to regulate the\nsimilarity loss function during the iterative learning of cross-modal joint\nembedding. Experiments on the benchmark dataset Recipe1M show the proposed\napproach outperforms the state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.00724",
          "publishedOn": "2021-08-03T02:06:31.207Z",
          "wordCount": 655,
          "title": "Learning TFIDF Enhanced Joint Embedding for Recipe-Image Cross-Modal Retrieval Service. (arXiv:2108.00724v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.01329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rossi_L/0/1/0/all/0/1\">Leonardo Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimi_A/0/1/0/all/0/1\">Akbar Karimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prati_A/0/1/0/all/0/1\">Andrea Prati</a>",
          "description": "Within the field of instance segmentation, most of the state-of-the-art deep\nlearning networks rely nowadays on cascade architectures, where multiple object\ndetectors are trained sequentially, re-sampling the ground truth at each step.\nThis offers a solution to the problem of exponentially vanishing positive\nsamples. However, it also translates into an increase in network complexity in\nterms of the number of parameters. To address this issue, we propose\nRecursively Refined R-CNN (R^3-CNN) which avoids duplicates by introducing a\nloop mechanism instead. At the same time, it achieves a quality boost using a\nrecursive re-sampling technique, where a specific IoU quality is utilized in\neach recursion to eventually equally cover the positive spectrum. Our\nexperiments highlight the specific encoding of the loop mechanism in the\nweights, requiring its usage at inference time. The R^3-CNN architecture is\nable to surpass the recently proposed HTC model, while reducing the number of\nparameters significantly. Experiments on COCO minival 2017 dataset show\nperformance boost independently from the utilized baseline model. The code is\navailable online at https://github.com/IMPLabUniPr/mmdetection/tree/r3_cnn.",
          "link": "http://arxiv.org/abs/2104.01329",
          "publishedOn": "2021-08-03T02:06:31.190Z",
          "wordCount": 638,
          "title": "Recursively Refined R-CNN: Instance Segmentation with Self-RoI Rebalancing. (arXiv:2104.01329v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>",
          "description": "This paper introduces a two-phase deep feature calibration framework for\nefficient learning of semantics enhanced text-image cross-modal joint\nembedding, which clearly separates the deep feature calibration in data\npreprocessing from training the joint embedding model. We use the Recipe1M\ndataset for the technical description and empirical validation. In\npreprocessing, we perform deep feature calibration by combining deep feature\nengineering with semantic context features derived from raw text-image input\ndata. We leverage LSTM to identify key terms, NLP methods to produce ranking\nscores for key terms before generating the key term feature. We leverage\nwideResNet50 to extract and encode the image category semantics to help\nsemantic alignment of the learned recipe and image embeddings in the joint\nlatent space. In joint embedding learning, we perform deep feature calibration\nby optimizing the batch-hard triplet loss function with soft-margin and double\nnegative sampling, also utilizing the category-based alignment loss and\ndiscriminator-based alignment loss. Extensive experiments demonstrate that our\nSEJE approach with the deep feature calibration significantly outperforms the\nstate-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.00705",
          "publishedOn": "2021-08-03T02:06:31.184Z",
          "wordCount": 615,
          "title": "Efficient Deep Feature Calibration for Cross-Modal Joint Embedding Learning. (arXiv:2108.00705v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shanlin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xiangyi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaohui Xie</a>",
          "description": "Although having achieved great success in medical image segmentation, deep\nconvolutional neural networks usually require a large dataset with manual\nannotations for training and are difficult to generalize to unseen classes.\nFew-shot learning has the potential to address these challenges by learning new\nclasses from only a few labeled examples. In this work, we propose a new\nframework for few-shot medical image segmentation based on prototypical\nnetworks. Our innovation lies in the design of two key modules: 1) a context\nrelation encoder (CRE) that uses correlation to capture local relation features\nbetween foreground and background regions; and 2) a recurrent mask refinement\nmodule that repeatedly uses the CRE and a prototypical network to recapture the\nchange of context relationship and refine the segmentation mask iteratively.\nExperiments on two abdomen CT datasets and an abdomen MRI dataset show the\nproposed method obtains substantial improvement over the state-of-the-art\nmethods by an average of 16.32%, 8.45% and 6.24% in terms of DSC, respectively.\nCode is publicly available.",
          "link": "http://arxiv.org/abs/2108.00622",
          "publishedOn": "2021-08-03T02:06:31.177Z",
          "wordCount": 605,
          "title": "Recurrent Mask Refinement for Few-Shot Medical Image Segmentation. (arXiv:2108.00622v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pu_M/0/1/0/all/0/1\">Mengyang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yaping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Q/0/1/0/all/0/1\">Qingji Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>",
          "description": "As a fundamental building block in computer vision, edges can be categorised\ninto four types according to the discontinuity in surface-Reflectance,\nIllumination, surface-Normal or Depth. While great progress has been made in\ndetecting generic or individual types of edges, it remains under-explored to\ncomprehensively study all four edge types together. In this paper, we propose a\nnovel neural network solution, RINDNet, to jointly detect all four types of\nedges. Taking into consideration the distinct attributes of each type of edges\nand the relationship between them, RINDNet learns effective representations for\neach of them and works in three stages. In stage I, RINDNet uses a common\nbackbone to extract features shared by all edges. Then in stage II it branches\nto prepare discriminative features for each edge type by the corresponding\ndecoder. In stage III, an independent decision head for each type aggregates\nthe features from previous stages to predict the initial results. Additionally,\nan attention module learns attention maps for all types to capture the\nunderlying relations between them, and these maps are combined with initial\nresults to generate the final edge detection results. For training and\nevaluation, we construct the first public benchmark, BSDS-RIND, with all four\ntypes of edges carefully annotated. In our experiments, RINDNet yields\npromising results in comparison with state-of-the-art methods. Additional\nanalysis is presented in supplementary material.",
          "link": "http://arxiv.org/abs/2108.00616",
          "publishedOn": "2021-08-03T02:06:31.171Z",
          "wordCount": 668,
          "title": "RINDNet: Edge Detection for Discontinuity in Reflectance, Illumination, Normal and Depth. (arXiv:2108.00616v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chongyi Li</a>",
          "description": "Object detection in three-dimensional (3D) space attracts much interest from\nacademia and industry since it is an essential task in AI-driven applications\nsuch as robotics, autonomous driving, and augmented reality. As the basic\nformat of 3D data, the point cloud can provide detailed geometric information\nabout the objects in the original 3D space. However, due to 3D data's sparsity\nand unorderedness, specially designed networks and modules are needed to\nprocess this type of data. Attention mechanism has achieved impressive\nperformance in diverse computer vision tasks; however, it is unclear how\nattention modules would affect the performance of 3D point cloud object\ndetection and what sort of attention modules could fit with the inherent\nproperties of 3D data. This work investigates the role of the attention\nmechanism in 3D point cloud object detection and provides insights into the\npotential of different attention modules. To achieve that, we comprehensively\ninvestigate classical 2D attentions, novel 3D attentions, including the latest\npoint cloud transformers on SUN RGB-D and ScanNetV2 datasets. Based on the\ndetailed experiments and analysis, we conclude the effects of different\nattention modules. This paper is expected to serve as a reference source for\nbenefiting attention-embedded 3D point cloud object detection. The code and\ntrained models are available at:\nhttps://github.com/ShiQiu0419/attentions_in_3D_detection.",
          "link": "http://arxiv.org/abs/2108.00620",
          "publishedOn": "2021-08-03T02:06:31.164Z",
          "wordCount": 658,
          "title": "Investigating Attention Mechanism in 3D Point Cloud Object Detection. (arXiv:2108.00620v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Safronov_E/0/1/0/all/0/1\">Evgenii Safronov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piga_N/0/1/0/all/0/1\">Nicola Piga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colledanchise_M/0/1/0/all/0/1\">Michele Colledanchise</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natale_L/0/1/0/all/0/1\">Lorenzo Natale</a>",
          "description": "Recent visual pose estimation and tracking solutions provide notable results\non popular datasets such as T-LESS and YCB. However, in the real world, we can\nfind ambiguous objects that do not allow exact classification and detection\nfrom a single view. In this work, we propose a framework that, given a single\nview of an object, provides the coordinates of a next viewpoint to discriminate\nthe object against similar ones, if any, and eliminates ambiguities. We also\ndescribe a complete pipeline from a real object's scans to the viewpoint\nselection and classification. We validate our approach with a Franka Emika\nPanda robot and common household objects featured with ambiguities. We released\nthe source code to reproduce our experiments.",
          "link": "http://arxiv.org/abs/2108.00737",
          "publishedOn": "2021-08-03T02:06:31.148Z",
          "wordCount": 567,
          "title": "Active Perception for Ambiguous Objects Classification. (arXiv:2108.00737v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Trung X. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mina_R/0/1/0/all/0/1\">Rusty John Lloyd Mina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Issa_D/0/1/0/all/0/1\">Dias Issa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1\">Chang D. Yoo</a>",
          "description": "In this work, we propose a novel methodology for self-supervised learning for\ngenerating global and local attention-aware visual features. Our approach is\nbased on training a model to differentiate between specific image\ntransformations of an input sample and the patched images. Utilizing this\napproach, the proposed method is able to outperform the previous best\ncompetitor by 1.03% on the Tiny-ImageNet dataset and by 2.32% on the STL-10\ndataset. Furthermore, our approach outperforms the fully-supervised learning\nmethod on the STL-10 dataset. Experimental results and visualizations show the\ncapability of successfully learning global and local attention-aware visual\nrepresentations.",
          "link": "http://arxiv.org/abs/2108.00475",
          "publishedOn": "2021-08-03T02:06:31.143Z",
          "wordCount": 544,
          "title": "Self-supervised Learning with Local Attention-Aware Feature. (arXiv:2108.00475v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heidler_K/0/1/0/all/0/1\">Konrad Heidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lichao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Di Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Pu Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guangyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>",
          "description": "Many current deep learning approaches make extensive use of backbone networks\npre-trained on large datasets like ImageNet, which are then fine-tuned to\nperform a certain task. In remote sensing, the lack of comparable large\nannotated datasets and the wide diversity of sensing platforms impedes similar\ndevelopments. In order to contribute towards the availability of pre-trained\nbackbone networks in remote sensing, we devise a self-supervised approach for\npre-training deep neural networks. By exploiting the correspondence between\ngeo-tagged audio recordings and remote sensing imagery, this is done in a\ncompletely label-free manner, eliminating the need for laborious manual\nannotation. For this purpose, we introduce the SoundingEarth dataset, which\nconsists of co-located aerial imagery and audio samples all around the world.\nUsing this dataset, we then pre-train ResNet models to map samples from both\nmodalities into a common embedding space, which encourages the models to\nunderstand key properties of a scene that influence both visual and auditory\nappearance. To validate the usefulness of the proposed approach, we evaluate\nthe transfer learning performance of pre-trained weights obtained against\nweights obtained through other means. By fine-tuning the models on a number of\ncommonly used remote sensing datasets, we show that our approach outperforms\nexisting pre-training strategies for remote sensing imagery. The dataset, code\nand pre-trained model weights will be available at\nhttps://github.com/khdlr/SoundingEarth.",
          "link": "http://arxiv.org/abs/2108.00688",
          "publishedOn": "2021-08-03T02:06:31.137Z",
          "wordCount": 690,
          "title": "Self-supervised Audiovisual Representation Learning for Remote Sensing Data. (arXiv:2108.00688v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00713",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1\">Brennan Nichyporuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardinell_J/0/1/0/all/0/1\">Jillian Cardinell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1\">Justin Szeto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mehta_R/0/1/0/all/0/1\">Raghav Mehta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios Tsaftaris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1\">Douglas L. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>",
          "description": "Many automatic machine learning models developed for focal pathology (e.g.\nlesions, tumours) detection and segmentation perform well, but do not\ngeneralize as well to new patient cohorts, impeding their widespread adoption\ninto real clinical contexts. One strategy to create a more diverse,\ngeneralizable training set is to naively pool datasets from different cohorts.\nSurprisingly, training on this \\it{big data} does not necessarily increase, and\nmay even reduce, overall performance and model generalizability, due to the\nexistence of cohort biases that affect label distributions. In this paper, we\npropose a generalized affine conditioning framework to learn and account for\ncohort biases across multi-source datasets, which we call Source-Conditioned\nInstance Normalization (SCIN). Through extensive experimentation on three\ndifferent, large scale, multi-scanner, multi-centre Multiple Sclerosis (MS)\nclinical trial MRI datasets, we show that our cohort bias adaptation method (1)\nimproves performance of the network on pooled datasets relative to naively\npooling datasets and (2) can quickly adapt to a new cohort by fine-tuning the\ninstance normalization parameters, thus learning the new cohort bias with only\n10 labelled samples.",
          "link": "http://arxiv.org/abs/2108.00713",
          "publishedOn": "2021-08-03T02:06:31.120Z",
          "wordCount": 638,
          "title": "Cohort Bias Adaptation in Aggregated Datasets for Lesion Segmentation. (arXiv:2108.00713v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00584",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Maoguo Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>",
          "description": "Crowd localization is a new computer vision task, evolved from crowd\ncounting. Different from the latter, it provides more precise location\ninformation for each instance, not just counting numbers for the whole crowd\nscene, which brings greater challenges, especially in extremely congested crowd\nscenes. In this paper, we focus on how to achieve precise instance localization\nin high-density crowd scenes, and to alleviate the problem that the feature\nextraction ability of the traditional model is reduced due to the target\nocclusion, the image blur, etc. To this end, we propose a Dilated Convolutional\nSwin Transformer (DCST) for congested crowd scenes. Specifically, a\nwindow-based vision transformer is introduced into the crowd localization task,\nwhich effectively improves the capacity of representation learning. Then, the\nwell-designed dilated convolutional module is inserted into some different\nstages of the transformer to enhance the large-range contextual information.\nExtensive experiments evidence the effectiveness of the proposed methods and\nachieve state-of-the-art performance on five popular datasets. Especially, the\nproposed model achieves F1-measure of 77.5\\% and MAE of 84.2 in terms of\nlocalization and counting performance, respectively.",
          "link": "http://arxiv.org/abs/2108.00584",
          "publishedOn": "2021-08-03T02:06:31.112Z",
          "wordCount": 618,
          "title": "Congested Crowd Instance Localization with Dilated Convolutional Swin Transformer. (arXiv:2108.00584v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00552",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Peng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lingyun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egorov_A/0/1/0/all/0/1\">Anton Egorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>",
          "description": "Accurate localization on autonomous driving cars is essential for autonomy\nand driving safety, especially for complex urban streets and search-and-rescue\nsubterranean environments where high-accurate GPS is not available. However\ncurrent odometry estimation may introduce the drifting problems in long-term\nnavigation without robust global localization. The main challenges involve\nscene divergence under the interference of dynamic environments and effective\nperception of observation and object layout variance from different viewpoints.\nTo tackle these challenges, we present PSE-Match, a viewpoint-free place\nrecognition method based on parallel semantic analysis of isolated semantic\nattributes from 3D point-cloud models. Compared with the original point cloud,\nthe observed variance of semantic attributes is smaller. PSE-Match incorporates\na divergence place learning network to capture different semantic attributes\nparallelly through the spherical harmonics domain. Using both existing\nbenchmark datasets and two in-field collected datasets, our experiments show\nthat the proposed method achieves above 70% average recall with top one\nretrieval and above 95% average recall with top ten retrieval cases. And\nPSE-Match has also demonstrated an obvious generalization ability with a\nlimited training dataset.",
          "link": "http://arxiv.org/abs/2108.00552",
          "publishedOn": "2021-08-03T02:06:31.078Z",
          "wordCount": 630,
          "title": "PSE-Match: A Viewpoint-free Place Recognition Method with Parallel Semantic Embedding. (arXiv:2108.00552v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bowen Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekris_K/0/1/0/all/0/1\">Kostas Bekris</a>",
          "description": "Tracking the 6D pose of objects in video sequences is important for robot\nmanipulation. Most prior efforts, however, often assume that the target\nobject's CAD model, at least at a category-level, is available for offline\ntraining or during online template matching. This work proposes BundleTrack, a\ngeneral framework for 6D pose tracking of novel objects, which does not depend\nupon 3D models, either at the instance or category-level. It leverages the\ncomplementary attributes of recent advances in deep learning for segmentation\nand robust feature extraction, as well as memory-augmented pose graph\noptimization for spatiotemporal consistency. This enables long-term, low-drift\ntracking under various challenging scenarios, including significant occlusions\nand object motions. Comprehensive experiments given two public benchmarks\ndemonstrate that the proposed approach significantly outperforms state-of-art,\ncategory-level 6D tracking or dynamic SLAM methods. When compared against\nstate-of-art methods that rely on an object instance CAD model, comparable\nperformance is achieved, despite the proposed method's reduced information\nrequirements. An efficient implementation in CUDA provides a real-time\nperformance of 10Hz for the entire framework. Code is available at:\nhttps://github.com/wenbowen123/BundleTrack",
          "link": "http://arxiv.org/abs/2108.00516",
          "publishedOn": "2021-08-03T02:06:31.072Z",
          "wordCount": 628,
          "title": "BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models. (arXiv:2108.00516v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miao_B/0/1/0/all/0/1\">Bo Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Liguang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1\">Tin Lun Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangsheng Xu</a>",
          "description": "Accurate perception of the surrounding scene is helpful for robots to make\nreasonable judgments and behaviours. Therefore, developing effective scene\nrepresentation and recognition methods are of significant importance in\nrobotics. Currently, a large body of research focuses on developing novel\nauxiliary features and networks to improve indoor scene recognition ability.\nHowever, few of them focus on directly constructing object features and\nrelations for indoor scene recognition. In this paper, we analyze the\nweaknesses of current methods and propose an Object-to-Scene (OTS) method,\nwhich extracts object features and learns object relations to recognize indoor\nscenes. The proposed OTS first extracts object features based on the\nsegmentation network and the proposed object feature aggregation module (OFAM).\nAfterwards, the object relations are calculated and the scene representation is\nconstructed based on the proposed object attention module (OAM) and global\nrelation aggregation module (GRAM). The final results in this work show that\nOTS successfully extracts object features and learns object relations from the\nsegmentation network. Moreover, OTS outperforms the state-of-the-art methods by\nmore than 2\\% on indoor scene recognition without using any additional streams.\nCode is publicly available at: https://github.com/FreeformRobotics/OTS.",
          "link": "http://arxiv.org/abs/2108.00399",
          "publishedOn": "2021-08-03T02:06:31.053Z",
          "wordCount": 634,
          "title": "Object-to-Scene: Learning to Transfer Object Knowledge to Indoor Scene Recognition. (arXiv:2108.00399v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Albanese_A/0/1/0/all/0/1\">Andrea Albanese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nardello_M/0/1/0/all/0/1\">Matteo Nardello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunelli_D/0/1/0/all/0/1\">Davide Brunelli</a>",
          "description": "Artificial intelligence has smoothly penetrated several economic activities,\nespecially monitoring and control applications, including the agriculture\nsector. However, research efforts toward low-power sensing devices with fully\nfunctional machine learning (ML) on-board are still fragmented and limited in\nsmart farming. Biotic stress is one of the primary causes of crop yield\nreduction. With the development of deep learning in computer vision technology,\nautonomous detection of pest infestation through images has become an important\nresearch direction for timely crop disease diagnosis. This paper presents an\nembedded system enhanced with ML functionalities, ensuring continuous detection\nof pest infestation inside fruit orchards. The embedded solution is based on a\nlow-power embedded sensing system along with a Neural Accelerator able to\ncapture and process images inside common pheromone-based traps. Three different\nML algorithms have been trained and deployed, highlighting the capabilities of\nthe platform. Moreover, the proposed approach guarantees an extended battery\nlife thanks to the integration of energy harvesting functionalities. Results\nshow how it is possible to automate the task of pest infestation for unlimited\ntime without the farmer's intervention.",
          "link": "http://arxiv.org/abs/2108.00421",
          "publishedOn": "2021-08-03T02:06:31.047Z",
          "wordCount": 626,
          "title": "Automated Pest Detection with DNN on the Edge for Precision Agriculture. (arXiv:2108.00421v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00568",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guihong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_S/0/1/0/all/0/1\">Sumit K. Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogras_U/0/1/0/all/0/1\">Umit Y. Ogras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marculescu_R/0/1/0/all/0/1\">Radu Marculescu</a>",
          "description": "Neural architecture search (NAS) is a promising technique to design efficient\nand high-performance deep neural networks (DNNs). As the performance\nrequirements of ML applications grow continuously, the hardware accelerators\nstart playing a central role in DNN design. This trend makes NAS even more\ncomplicated and time-consuming for most real applications. This paper proposes\nFLASH, a very fast NAS methodology that co-optimizes the DNN accuracy and\nperformance on a real hardware platform. As the main theoretical contribution,\nwe first propose the NN-Degree, an analytical metric to quantify the\ntopological characteristics of DNNs with skip connections (e.g., DenseNets,\nResNets, Wide-ResNets, and MobileNets). The newly proposed NN-Degree allows us\nto do training-free NAS within one second and build an accuracy predictor by\ntraining as few as 25 samples out of a vast search space with more than 63\nbillion configurations. Second, by performing inference on the target hardware,\nwe fine-tune and validate our analytical models to estimate the latency, area,\nand energy consumption of various DNN architectures while executing standard ML\ndatasets. Third, we construct a hierarchical algorithm based on simplicial\nhomology global optimization (SHGO) to optimize the model-architecture\nco-design process, while considering the area, latency, and energy consumption\nof the target hardware. We demonstrate that, compared to the state-of-the-art\nNAS approaches, our proposed hierarchical SHGO-based algorithm enables more\nthan four orders of magnitude speedup (specifically, the execution time of the\nproposed algorithm is about 0.1 seconds). Finally, our experimental evaluations\nshow that FLASH is easily transferable to different hardware architectures,\nthus enabling us to do NAS on a Raspberry Pi-3B processor in less than 3\nseconds.",
          "link": "http://arxiv.org/abs/2108.00568",
          "publishedOn": "2021-08-03T02:06:31.033Z",
          "wordCount": 711,
          "title": "FLASH: Fast Neural Architecture Search with Hardware Optimization. (arXiv:2108.00568v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaojun Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Huanqian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonglin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xingxing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>",
          "description": "In recent years, intellectual property (IP), which represents literary,\ninventions, artistic works, etc, gradually attract more and more people's\nattention. Particularly, with the rise of e-commerce, the IP not only\nrepresents the product design and brands, but also represents the images/videos\ndisplayed on e-commerce platforms. Unfortunately, some attackers adopt some\nadversarial methods to fool the well-trained logo detection model for\ninfringement. To overcome this problem, a novel logo detector based on the\nmechanism of looking and thinking twice is proposed in this paper for robust\nlogo detection. The proposed detector is different from other mainstream\ndetectors, which can effectively detect small objects, long-tail objects, and\nis robust to adversarial images. In detail, we extend detectoRS algorithm to a\ncascade schema with an equalization loss function, multi-scale transformations,\nand adversarial data augmentation. A series of experimental results have shown\nthat the proposed method can effectively improve the robustness of the\ndetection model. Moreover, we have applied the proposed methods to competition\nACM MM2021 Robust Logo Detection that is organized by Alibaba on the Tianchi\nplatform and won top 2 in 36489 teams. Code is available at\nhttps://github.com/jiaxiaojunQAQ/Robust-Logo-Detection.",
          "link": "http://arxiv.org/abs/2108.00422",
          "publishedOn": "2021-08-03T02:06:31.026Z",
          "wordCount": 631,
          "title": "An Effective and Robust Detector for Logo Detection. (arXiv:2108.00422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00596",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iftekhar_A/0/1/0/all/0/1\">A S M Iftekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Satish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McEver_R/0/1/0/all/0/1\">R. Austin McEver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Suya You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunath_B/0/1/0/all/0/1\">B.S. Manjunath</a>",
          "description": "The human-object interaction (HOI) detection task refers to localizing\nhumans, localizing objects, and predicting the interactions between each\nhuman-object pair. HOI is considered one of the fundamental steps in truly\nunderstanding complex visual scenes. For detecting HOI, it is important to\nutilize relative spatial configurations and object semantics to find salient\nspatial regions of images that highlight the interactions between human object\npairs. This issue is addressed by the proposed self-attention based guided\ntransformer network, GTNet. GTNet encodes this spatial contextual information\nin human and object visual features via self-attention while achieving a 4%-6%\nimprovement over previous state of the art results on both the V-COCO and\nHICO-DET datasets. Code will be made available online.",
          "link": "http://arxiv.org/abs/2108.00596",
          "publishedOn": "2021-08-03T02:06:31.008Z",
          "wordCount": 562,
          "title": "GTNet:Guided Transformer Network for Detecting Human-Object Interactions. (arXiv:2108.00596v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiju Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taejoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>",
          "description": "Adversarial training based on the maximum classifier discrepancy between the\ntwo classifier structures has achieved great success in unsupervised domain\nadaptation tasks for image classification. The approach adopts the structure of\ntwo classifiers, though simple and intuitive, the learned classification\nboundary may not well represent the data property in the new domain. In this\npaper, we propose to extend the structure to multiple classifiers to further\nboost its performance. To this end, we propose a very straightforward approach\nto adding more classifiers. We employ the principle that the classifiers are\ndifferent from each other to construct a discrepancy loss function for multiple\nclassifiers. Through the loss function construction method, we make it possible\nto add any number of classifiers to the original framework. The proposed\napproach is validated through extensive experimental evaluations. We\ndemonstrate that, on average, adopting the structure of three classifiers\nnormally yields the best performance as a trade-off between the accuracy and\nefficiency. With minimum extra computational costs, the proposed approach can\nsignificantly improve the original algorithm.",
          "link": "http://arxiv.org/abs/2108.00610",
          "publishedOn": "2021-08-03T02:06:30.996Z",
          "wordCount": 607,
          "title": "Multiple Classifiers Based Maximum Classifier Discrepancy for Unsupervised Domain Adaptation. (arXiv:2108.00610v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Gangming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Weifeng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>",
          "description": "Feature pyramids have been proven powerful in image understanding tasks that\nrequire multi-scale features. State-of-the-art methods for multi-scale feature\nlearning focus on performing feature interactions across space and scales using\nneural networks with a fixed topology. In this paper, we propose graph feature\npyramid networks that are capable of adapting their topological structures to\nvarying intrinsic image structures and supporting simultaneous feature\ninteractions across all scales. We first define an image-specific superpixel\nhierarchy for each input image to represent its intrinsic image structures. The\ngraph feature pyramid network inherits its structure from this superpixel\nhierarchy. Contextual and hierarchical layers are designed to achieve feature\ninteractions within the same scale and across different scales. To make these\nlayers more powerful, we introduce two types of local channel attention for\ngraph neural networks by generalizing global channel attention for\nconvolutional neural networks. The proposed graph feature pyramid network can\nenhance the multiscale features from a convolutional feature pyramid network.\nWe evaluate our graph feature pyramid network in the object detection task by\nintegrating it into the Faster R-CNN algorithm. The modified algorithm\noutperforms not only previous state-of-the-art feature pyramid-based methods\nwith a clear margin but also other popular detection methods on both MS-COCO\n2017 validation and test datasets.",
          "link": "http://arxiv.org/abs/2108.00580",
          "publishedOn": "2021-08-03T02:06:30.988Z",
          "wordCount": 646,
          "title": "GraphFPN: Graph Feature Pyramid Network for Object Detection. (arXiv:2108.00580v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haitong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shuang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xia Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kaiyue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hongjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nizhuan Wang</a>",
          "description": "It is a challenging task to accurately perform semantic segmentation due to\nthe complexity of real picture scenes. Many semantic segmentation methods based\non traditional deep learning insufficiently captured the semantic and\nappearance information of images, which put limit on their generality and\nrobustness for various application scenes. In this paper, we proposed a novel\nstrategy that reformulated the popularly-used convolution operation to\nmulti-layer convolutional sparse coding block to ease the aforementioned\ndeficiency. This strategy can be possibly used to significantly improve the\nsegmentation performance of any semantic segmentation model that involves\nconvolutional operations. To prove the effectiveness of our idea, we chose the\nwidely-used U-Net model for the demonstration purpose, and we designed CSC-Unet\nmodel series based on U-Net. Through extensive analysis and experiments, we\nprovided credible evidence showing that the multi-layer convolutional sparse\ncoding block enables semantic segmentation model to converge faster, can\nextract finer semantic and appearance information of images, and improve the\nability to recover spatial detail information. The best CSC-Unet model\nsignificantly outperforms the results of the original U-Net on three public\ndatasets with different scenarios, i.e., 87.14% vs. 84.71% on DeepCrack\ndataset, 68.91% vs. 67.09% on Nuclei dataset, and 53.68% vs. 48.82% on CamVid\ndataset, respectively.",
          "link": "http://arxiv.org/abs/2108.00408",
          "publishedOn": "2021-08-03T02:06:30.938Z",
          "wordCount": 661,
          "title": "CSC-Unet: A Novel Convolutional Sparse Coding Strategy based Neural Network for Semantic Segmentation. (arXiv:2108.00408v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1\">Naveed Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1\">Navid Kardan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>",
          "description": "Deep Learning (DL) is the most widely used tool in the contemporary field of\ncomputer vision. Its ability to accurately solve complex problems is employed\nin vision research to learn deep neural models for a variety of tasks,\nincluding security critical applications. However, it is now known that DL is\nvulnerable to adversarial attacks that can manipulate its predictions by\nintroducing visually imperceptible perturbations in images and videos. Since\nthe discovery of this phenomenon in 2013~[1], it has attracted significant\nattention of researchers from multiple sub-fields of machine intelligence. In\n[2], we reviewed the contributions made by the computer vision community in\nadversarial attacks on deep learning (and their defenses) until the advent of\nyear 2018. Many of those contributions have inspired new directions in this\narea, which has matured significantly since witnessing the first generation\nmethods. Hence, as a legacy sequel of [2], this literature review focuses on\nthe advances in this area since 2018. To ensure authenticity, we mainly\nconsider peer-reviewed contributions published in the prestigious sources of\ncomputer vision and machine learning research. Besides a comprehensive\nliterature review, the article also provides concise definitions of technical\nterminologies for non-experts in this domain. Finally, this article discusses\nchallenges and future outlook of this direction based on the literature\nreviewed herein and [2].",
          "link": "http://arxiv.org/abs/2108.00401",
          "publishedOn": "2021-08-03T02:06:30.932Z",
          "wordCount": 673,
          "title": "Threat of Adversarial Attacks on Deep Learning in Computer Vision: Survey II. (arXiv:2108.00401v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00462",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Choubo Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>",
          "description": "Existing anomaly detection paradigms overwhelmingly focus on training\ndetection models using exclusively normal data or unlabeled data (mostly normal\nsamples). One notorious issue with these approaches is that they are weak in\ndiscriminating anomalies from normal samples due to the lack of the knowledge\nabout the anomalies. Here, we study the problem of few-shot anomaly detection,\nin which we aim at using a few labeled anomaly examples to train\nsample-efficient discriminative detection models. To address this problem, we\nintroduce a novel weakly-supervised anomaly detection framework to train\ndetection models without assuming the examples illustrating all possible\nclasses of anomaly.\n\nSpecifically, the proposed approach learns discriminative normality\n(regularity) by leveraging the labeled anomalies and a prior probability to\nenforce expressive representations of normality and unbounded deviated\nrepresentations of abnormality. This is achieved by an end-to-end optimization\nof anomaly scores with a neural deviation learning, in which the anomaly scores\nof normal samples are imposed to approximate scalar scores drawn from the prior\nwhile that of anomaly examples is enforced to have statistically significant\ndeviations from these sampled scores in the upper tail. Furthermore, our model\nis optimized to learn fine-grained normality and abnormality by top-K\nmultiple-instance-learning-based feature subspace deviation learning, allowing\nmore generalized representations. Comprehensive experiments on nine real-world\nimage anomaly detection benchmarks show that our model is substantially more\nsample-efficient and robust, and performs significantly better than\nstate-of-the-art competing methods in both closed-set and open-set settings.\nOur model can also offer explanation capability as a result of its prior-driven\nanomaly score learning. Code and datasets are available at:\nhttps://git.io/DevNet.",
          "link": "http://arxiv.org/abs/2108.00462",
          "publishedOn": "2021-08-03T02:06:30.926Z",
          "wordCount": 714,
          "title": "Explainable Deep Few-shot Anomaly Detection with Deviation Networks. (arXiv:2108.00462v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>",
          "description": "Low-light images captured in the real world are inevitably corrupted by\nsensor noise. Such noise is spatially variant and highly dependent on the\nunderlying pixel intensity, deviating from the oversimplified assumptions in\nconventional denoising. Existing light enhancement methods either overlook the\nimportant impact of real-world noise during enhancement, or treat noise removal\nas a separate pre- or post-processing step. We present Coordinated Enhancement\nfor Real-world Low-light Noisy Images (CERL), that seamlessly integrates light\nenhancement and noise suppression parts into a unified and physics-grounded\noptimization framework. For the real low-light noise removal part, we customize\na self-supervised denoising model that can easily be adapted without referring\nto clean ground-truth images. For the light enhancement part, we also improve\nthe design of a state-of-the-art backbone. The two parts are then joint\nformulated into one principled plug-and-play optimization. Our approach is\ncompared against state-of-the-art low-light enhancement methods both\nqualitatively and quantitatively. Besides standard benchmarks, we further\ncollect and test on a new realistic low-light mobile photography dataset\n(RLMP), whose mobile-captured photos display heavier realistic noise than those\ntaken by high-quality cameras. CERL consistently produces the most visually\npleasing and artifact-free results across all experiments. Our RLMP dataset and\ncodes are available at: https://github.com/VITA-Group/CERL.",
          "link": "http://arxiv.org/abs/2108.00478",
          "publishedOn": "2021-08-03T02:06:30.919Z",
          "wordCount": 651,
          "title": "CERL: A Unified Optimization Framework for Light Enhancement with Realistic Noise. (arXiv:2108.00478v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Liguang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cen_J/0/1/0/all/0/1\">Jun Cen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenglong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1\">Tin Lun Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangsheng Xu</a>",
          "description": "Scene recognition is a fundamental task in robotic perception. For human\nbeings, scene recognition is reasonable because they have abundant object\nknowledge of the real world. The idea of transferring prior object knowledge\nfrom humans to scene recognition is significant but still less exploited. In\nthis paper, we propose to utilize meaningful object representations for indoor\nscene representation. First, we utilize an improved object model (IOM) as a\nbaseline that enriches the object knowledge by introducing a scene parsing\nalgorithm pretrained on the ADE20K dataset with rich object categories related\nto the indoor scene. To analyze the object co-occurrences and pairwise object\nrelations, we formulate the IOM from a Bayesian perspective as the Bayesian\nobject relation model (BORM). Meanwhile, we incorporate the proposed BORM with\nthe PlacesCNN model as the combined Bayesian object relation model (CBORM) for\nscene recognition and significantly outperforms the state-of-the-art methods on\nthe reduced Places365 dataset, and SUN RGB-D dataset without retraining,\nshowing the excellent generalization ability of the proposed method. Code can\nbe found at https://github.com/hszhoushen/borm.",
          "link": "http://arxiv.org/abs/2108.00397",
          "publishedOn": "2021-08-03T02:06:30.913Z",
          "wordCount": 630,
          "title": "BORM: Bayesian Object Relation Model for Indoor Scene Recognition. (arXiv:2108.00397v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zust_L/0/1/0/all/0/1\">Lojze &#x17d;ust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kristan_M/0/1/0/all/0/1\">Matej Kristan</a>",
          "description": "Coastal water autonomous boats rely on robust perception methods for obstacle\ndetection and timely collision avoidance. The current state-of-the-art is based\non deep segmentation networks trained on large datasets. Per-pixel ground truth\nlabeling of such datasets, however, is labor-intensive and expensive. We\nobserve that far less information is required for practical obstacle avoidance\n- the location of water edge on static obstacles like shore and approximate\nlocation and bounds of dynamic obstacles in the water is sufficient to plan a\nreaction. We propose a new scaffolding learning regime (SLR) that allows\ntraining obstacle detection segmentation networks only from such weak\nannotations, thus significantly reducing the cost of ground-truth labeling.\nExperiments show that maritime obstacle segmentation networks trained using SLR\nsubstantially outperform the same networks trained with dense ground truth\nlabels. Thus accuracy is not sacrificed for labelling simplicity but is in fact\nimproved, which is a remarkable result.",
          "link": "http://arxiv.org/abs/2108.00564",
          "publishedOn": "2021-08-03T02:06:30.906Z",
          "wordCount": 591,
          "title": "Learning Maritime Obstacle Detection from Weak Annotations by Scaffolding. (arXiv:2108.00564v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fassold_H/0/1/0/all/0/1\">Hannes Fassold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karakottas_A/0/1/0/all/0/1\">Antonis Karakottas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsatsou_D/0/1/0/all/0/1\">Dorothea Tsatsou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarpalas_D/0/1/0/all/0/1\">Dimitrios Zarpalas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takacs_B/0/1/0/all/0/1\">Barnabas Takacs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuhrhop_C/0/1/0/all/0/1\">Christian Fuhrhop</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manfredi_A/0/1/0/all/0/1\">Angelo Manfredi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patz_N/0/1/0/all/0/1\">Nicolas Patz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonoli_S/0/1/0/all/0/1\">Simona Tonoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dulskaia_I/0/1/0/all/0/1\">Iana Dulskaia</a>",
          "description": "Spherical 360{\\deg} video is a novel media format, rapidly becoming adopted\nin media production and consumption of immersive media. Due to its novelty,\nthere is a lack of tools for producing highly engaging interactive 360{\\deg}\nvideo for consumption on a multitude of platforms. In this work, we describe\nthe work done so far in the Hyper360 project on tools for mixed 360{\\deg} video\nand 3D content. Furthermore, the first pilots which have been produced with the\nHyper360 tools and results of the audience assessment of the produced pilots\nare presented.",
          "link": "http://arxiv.org/abs/2108.00430",
          "publishedOn": "2021-08-03T02:06:30.845Z",
          "wordCount": 543,
          "title": "Hyper360 -- a Next Generation Toolset for Immersive Media. (arXiv:2108.00430v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhoubo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Puqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raveaux_R/0/1/0/all/0/1\">Romain Raveaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huadong Liu</a>",
          "description": "Graph matching is an important problem that has received widespread\nattention, especially in the field of computer vision. Recently,\nstate-of-the-art methods seek to incorporate graph matching with deep learning.\nHowever, there is no research to explain what role the graph matching algorithm\nplays in the model. Therefore, we propose an approach integrating a MILP\nformulation of the graph matching problem. This formulation is solved to\noptimal and it provides inherent baseline. Meanwhile, similar approaches are\nderived by releasing the optimal guarantee of the graph matching solver and by\nintroducing a quality level. This quality level controls the quality of the\nsolutions provided by the graph matching solver. In addition, several\nrelaxations of the graph matching problem are put to the test. Our experimental\nevaluation gives several theoretical insights and guides the direction of deep\ngraph matching methods.",
          "link": "http://arxiv.org/abs/2108.00394",
          "publishedOn": "2021-08-03T02:06:30.834Z",
          "wordCount": 604,
          "title": "Deep graph matching meets mixed-integer linear programming: Relax at your own risk ?. (arXiv:2108.00394v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xujie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fuwei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoye Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1\">Michael C. Kampffmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Haonan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>",
          "description": "Despite recent progress on image-based virtual try-on, current methods are\nconstraint by shared warping networks and thus fail to synthesize natural\ntry-on results when faced with clothing categories that require different\nwarping operations. In this paper, we address this problem by finding clothing\ncategory-specific warping networks for the virtual try-on task via Neural\nArchitecture Search (NAS). We introduce a NAS-Warping Module and elaborately\ndesign a bilevel hierarchical search space to identify the optimal\nnetwork-level and operation-level flow estimation architecture. Given the\nnetwork-level search space, containing different numbers of warping blocks, and\nthe operation-level search space with different convolution operations, we\njointly learn a combination of repeatable warping cells and convolution\noperations specifically for the clothing-person alignment. Moreover, a\nNAS-Fusion Module is proposed to synthesize more natural final try-on results,\nwhich is realized by leveraging particular skip connections to produce\nbetter-fused features that are required for seamlessly fusing the warped\nclothing and the unchanged person part. We adopt an efficient and stable\none-shot searching strategy to search the above two modules. Extensive\nexperiments demonstrate that our WAS-VTON significantly outperforms the\nprevious fixed-architecture try-on methods with more natural warping results\nand virtual try-on results.",
          "link": "http://arxiv.org/abs/2108.00386",
          "publishedOn": "2021-08-03T02:06:30.817Z",
          "wordCount": 635,
          "title": "WAS-VTON: Warping Architecture Search for Virtual Try-on Network. (arXiv:2108.00386v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guoxing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yizhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_A/0/1/0/all/0/1\">Anqi Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Pei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>",
          "description": "4D reconstruction of human-object interaction is critical for immersive VR/AR\nexperience and human activity understanding. Recent advances still fail to\nrecover fine geometry and texture results from sparse RGB inputs, especially\nunder challenging human-object interactions scenarios. In this paper, we\npropose a neural human performance capture and rendering system to generate\nboth high-quality geometry and photo-realistic texture of both human and\nobjects under challenging interaction scenarios in arbitrary novel views, from\nonly sparse RGB streams. To deal with complex occlusions raised by human-object\ninteractions, we adopt a layer-wise scene decoupling strategy and perform\nvolumetric reconstruction and neural rendering of the human and object.\nSpecifically, for geometry reconstruction, we propose an interaction-aware\nhuman-object capture scheme that jointly considers the human reconstruction and\nobject reconstruction with their correlations. Occlusion-aware human\nreconstruction and robust human-aware object tracking are proposed for\nconsistent 4D human-object dynamic reconstruction. For neural texture\nrendering, we propose a layer-wise human-object rendering scheme, which\ncombines direction-aware neural blending weight learning and spatial-temporal\ntexture completion to provide high-resolution and photo-realistic texture\nresults in the occluded scenarios. Extensive experiments demonstrate the\neffectiveness of our approach to achieve high-quality geometry and texture\nreconstruction in free viewpoints for challenging human-object interactions.",
          "link": "http://arxiv.org/abs/2108.00362",
          "publishedOn": "2021-08-03T02:06:30.800Z",
          "wordCount": 656,
          "title": "Neural Free-Viewpoint Performance Rendering under ComplexHuman-object Interactions. (arXiv:2108.00362v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zunlei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lechao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yajie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiangtong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>",
          "description": "When confronted with objects of unknown types in an image, humans can\neffortlessly and precisely tell their visual boundaries. This recognition\nmechanism and underlying generalization capability seem to contrast to\nstate-of-the-art image segmentation networks that rely on large-scale\ncategory-aware annotated training samples. In this paper, we make an attempt\ntowards building models that explicitly account for visual boundary knowledge,\nin hope to reduce the training effort on segmenting unseen categories.\nSpecifically, we investigate a new task termed as Boundary Knowledge\nTranslation (BKT). Given a set of fully labeled categories, BKT aims to\ntranslate the visual boundary knowledge learned from the labeled categories, to\na set of novel categories, each of which is provided only a few labeled\nsamples. To this end, we propose a Translation Segmentation Network\n(Trans-Net), which comprises a segmentation network and two boundary\ndiscriminators. The segmentation network, combined with a boundary-aware\nself-supervised mechanism, is devised to conduct foreground segmentation, while\nthe two discriminators work together in an adversarial manner to ensure an\naccurate segmentation of the novel categories under light supervision.\nExhaustive experiments demonstrate that, with only tens of labeled samples as\nguidance, Trans-Net achieves close results on par with fully supervised\nmethods.",
          "link": "http://arxiv.org/abs/2108.00379",
          "publishedOn": "2021-08-03T02:06:30.733Z",
          "wordCount": 640,
          "title": "Visual Boundary Knowledge Translation for Foreground Segmentation. (arXiv:2108.00379v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00402",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhendong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manh_V/0/1/0/all/0/1\">Van Manh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoqiong Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campello_V/0/1/0/all/0/1\">V&#xed;ctor Campello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F Frangi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>",
          "description": "The performance of deep segmentation models often degrades due to\ndistribution shifts in image intensities between the training and test data\nsets. This is particularly pronounced in multi-centre studies involving data\nacquired using multi-vendor scanners, with variations in acquisition protocols.\nIt is challenging to address this degradation because the shift is often not\nknown \\textit{a priori} and hence difficult to model. We propose a novel\nframework to ensure robust segmentation in the presence of such distribution\nshifts. Our contribution is three-fold. First, inspired by the spirit of\ncurriculum learning, we design a novel style curriculum to train the\nsegmentation models using an easy-to-hard mode. A style transfer model with\nstyle fusion is employed to generate the curriculum samples. Gradually focusing\non complex and adversarial style samples can significantly boost the robustness\nof the models. Second, instead of subjectively defining the curriculum\ncomplexity, we adopt an automated gradient manipulation method to control the\nhard and adversarial sample generation process. Third, we propose the Local\nGradient Sign strategy to aggregate the gradient locally and stabilise training\nduring gradient manipulation. The proposed framework can generalise to unknown\ndistribution without using any target data. Extensive experiments on the public\nM\\&Ms Challenge dataset demonstrate that our proposed framework can generalise\ndeep models well to unknown distributions and achieve significant improvements\nin segmentation accuracy.",
          "link": "http://arxiv.org/abs/2108.00402",
          "publishedOn": "2021-08-03T02:06:30.727Z",
          "wordCount": 678,
          "title": "Style Curriculum Learning for Robust Medical Image Segmentation. (arXiv:2108.00402v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Min Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lingxiao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1\">Xingyu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>",
          "description": "Person re-identification (Re-ID) aims to match pedestrians under dis-joint\ncameras. Most Re-ID methods formulate it as visual representation learning and\nimage search, and its accuracy is consequently affected greatly by the search\nspace. Spatial-temporal information has been proven to be efficient to filter\nirrelevant negative samples and significantly improve Re-ID accuracy. However,\nexisting spatial-temporal person Re-ID methods are still rough and do not\nexploit spatial-temporal information sufficiently. In this paper, we propose a\nnovel Instance-level and Spatial-Temporal Disentangled Re-ID method (InSTD), to\nimprove Re-ID accuracy. In our proposed framework, personalized information\nsuch as moving direction is explicitly considered to further narrow down the\nsearch space. Besides, the spatial-temporal transferring probability is\ndisentangled from joint distribution to marginal distribution, so that outliers\ncan also be well modeled. Abundant experimental analyses are presented, which\ndemonstrates the superiority and provides more insights into our method. The\nproposed method achieves mAP of 90.8% on Market-1501 and 89.1% on\nDukeMTMC-reID, improving from the baseline 82.2% and 72.7%, respectively.\nBesides, in order to provide a better benchmark for person re-identification,\nwe release a cleaned data list of DukeMTMC-reID with this paper:\nhttps://github.com/RenMin1991/cleaned-DukeMTMC-reID/",
          "link": "http://arxiv.org/abs/2108.00171",
          "publishedOn": "2021-08-03T02:06:30.720Z",
          "wordCount": 631,
          "title": "Learning Instance-level Spatial-Temporal Patterns for Person Re-identification. (arXiv:2108.00171v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinghua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>",
          "description": "Microorganisms are widely distributed in the human daily living environment.\nThey play an essential role in environmental pollution control, disease\nprevention and treatment, and food and drug production. The identification,\ncounting, and detection are the basic steps for making full use of different\nmicroorganisms. However, the conventional analysis methods are expensive,\nlaborious, and time-consuming. To overcome these limitations, artificial neural\nnetworks are applied for microorganism image analysis. We conduct this review\nto understand the development process of microorganism image analysis based on\nartificial neural networks. In this review, the background and motivation are\nintroduced first. Then, the development of artificial neural networks and\nrepresentative networks are introduced. After that, the papers related to\nmicroorganism image analysis based on classical and deep neural networks are\nreviewed from the perspectives of different tasks. In the end, the methodology\nanalysis and potential direction are discussed.",
          "link": "http://arxiv.org/abs/2108.00358",
          "publishedOn": "2021-08-03T02:06:30.707Z",
          "wordCount": 614,
          "title": "Applications of Artificial Neural Networks in Microorganism Image Analysis: A Comprehensive Review from Conventional Multilayer Perceptron to Popular Convolutional Neural Network and Potential Visual Transformer. (arXiv:2108.00358v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1\">Sayak Nag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raychaudhuri_D/0/1/0/all/0/1\">Dripta S. Raychaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sujoy Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>",
          "description": "We study the problem of how to identify samples from unseen categories\n(open-set classification) when there are only a few samples given from the seen\ncategories (few-shot setting). The challenge of learning a good abstraction for\na class with very few samples makes it extremely difficult to detect samples\nfrom the unseen categories; consequently, open-set recognition has received\nminimal attention in the few-shot setting. Most open-set few-shot\nclassification methods regularize the softmax score to indicate uniform\nprobability for open class samples but we argue that this approach is often\ninaccurate, especially at a fine-grained level. Instead, we propose a novel\nexemplar reconstruction-based meta-learning strategy for jointly detecting open\nclass samples, as well as, categorizing samples from seen classes via\nmetric-based classification. The exemplars, which act as representatives of a\nclass, can either be provided in the training dataset or estimated in the\nfeature domain. Our framework, named Reconstructing Exemplar based Few-shot\nOpen-set ClaSsifier (ReFOCS), is tested on a wide variety of datasets and the\nexperimental results clearly highlight our method as the new state of the art.",
          "link": "http://arxiv.org/abs/2108.00340",
          "publishedOn": "2021-08-03T02:06:30.700Z",
          "wordCount": 613,
          "title": "Learning Few-shot Open-set Classifiers using Exemplar Reconstruction. (arXiv:2108.00340v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shooter_M/0/1/0/all/0/1\">Moira Shooter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malleson_C/0/1/0/all/0/1\">Charles Malleson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilton_A/0/1/0/all/0/1\">Adrian Hilton</a> (University of Surrey)",
          "description": "Estimating the pose of animals can facilitate the understanding of animal\nmotion which is fundamental in disciplines such as biomechanics, neuroscience,\nethology, robotics and the entertainment industry. Human pose estimation models\nhave achieved high performance due to the huge amount of training data\navailable. Achieving the same results for animal pose estimation is challenging\ndue to the lack of animal pose datasets. To address this problem we introduce\nSyDog: a synthetic dataset of dogs containing ground truth pose and bounding\nbox coordinates which was generated using the game engine, Unity. We\ndemonstrate that pose estimation models trained on SyDog achieve better\nperformance than models trained purely on real data and significantly reduce\nthe need for the labour intensive labelling of images. We release the SyDog\ndataset as a training and evaluation benchmark for research in animal motion.",
          "link": "http://arxiv.org/abs/2108.00249",
          "publishedOn": "2021-08-03T02:06:30.683Z",
          "wordCount": 608,
          "title": "SyDog: A Synthetic Dog Dataset for Improved 2D Pose Estimation. (arXiv:2108.00249v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Heng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1\">Yew-Soon Ong</a>",
          "description": "Current one-stage methods for visual grounding encode the language query as\none holistic sentence embedding before fusion with visual feature. Such a\nformulation does not treat each word of a query sentence on par when modeling\nlanguage to visual attention, therefore prone to neglect words which are less\nimportant for sentence embedding but critical for visual grounding. In this\npaper we propose Word2Pix: a one-stage visual grounding network based on\nencoder-decoder transformer architecture that enables learning for textual to\nvisual feature correspondence via word to pixel attention. The embedding of\neach word from the query sentence is treated alike by attending to visual\npixels individually instead of single holistic sentence embedding. In this way,\neach word is given equivalent opportunity to adjust the language to vision\nattention towards the referent target through multiple stacks of transformer\ndecoder layers. We conduct the experiments on RefCOCO, RefCOCO+ and RefCOCOg\ndatasets and the proposed Word2Pix outperforms existing one-stage methods by a\nnotable margin. The results obtained also show that Word2Pix surpasses\ntwo-stage visual grounding models, while at the same time keeping the merits of\none-stage paradigm namely end-to-end training and real-time inference speed\nintact.",
          "link": "http://arxiv.org/abs/2108.00205",
          "publishedOn": "2021-08-03T02:06:30.675Z",
          "wordCount": 639,
          "title": "Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding. (arXiv:2108.00205v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shapira_G/0/1/0/all/0/1\">Gil Shapira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_N/0/1/0/all/0/1\">Noga Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldin_I/0/1/0/all/0/1\">Ishay Goldin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jevnisek_R/0/1/0/all/0/1\">Roy J. Jevnisek</a>",
          "description": "Facial landmarks (FLM) estimation is a critical component in many\nface-related applications. In this work, we aim to optimize for both accuracy\nand speed and explore the trade-off between them. Our key observation is that\nnot all faces are created equal. Frontal faces with neutral expressions\nconverge faster than faces with extreme poses or expressions. To differentiate\namong samples, we train our model to predict the regression error after each\niteration. If the current iteration is accurate enough, we stop iterating,\nsaving redundant iterations while keeping the accuracy in check. We also\nobserve that as neighboring patches overlap, we can infer all facial landmarks\n(FLMs) with only a small number of patches without a major accuracy sacrifice.\nArchitecturally, we offer a multi-scale, patch-based, lightweight feature\nextractor with a fine-grained local patch attention module, which computes a\npatch weighting according to the information in the patch itself and enhances\nthe expressive power of the patch features. We analyze the patch attention data\nto infer where the model is attending when regressing facial landmarks and\ncompare it to face attention in humans. Our model runs in real-time on a mobile\ndevice GPU, with 95 Mega Multiply-Add (MMA) operations, outperforming all\nstate-of-the-art methods under 1000 MMA, with a normalized mean error of 8.16\non the 300W challenging dataset.",
          "link": "http://arxiv.org/abs/2108.00377",
          "publishedOn": "2021-08-03T02:06:30.642Z",
          "wordCount": 663,
          "title": "Knowing When to Quit: Selective Cascaded Regression with Patch Attention for Real-Time Face Alignment. (arXiv:2108.00377v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jinyuan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yupei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>",
          "description": "Self-supervised learning in computer vision aims to pre-train an image\nencoder using a large amount of unlabeled images or (image, text) pairs. The\npre-trained image encoder can then be used as a feature extractor to build\ndownstream classifiers for many downstream tasks with a small amount of or no\nlabeled training data. In this work, we propose BadEncoder, the first backdoor\nattack to self-supervised learning. In particular, our BadEncoder injects\nbackdoors into a pre-trained image encoder such that the downstream classifiers\nbuilt based on the backdoored image encoder for different downstream tasks\nsimultaneously inherit the backdoor behavior. We formulate our BadEncoder as an\noptimization problem and we propose a gradient descent based method to solve\nit, which produces a backdoored image encoder from a clean one. Our extensive\nempirical evaluation results on multiple datasets show that our BadEncoder\nachieves high attack success rates while preserving the accuracy of the\ndownstream classifiers. We also show the effectiveness of BadEncoder using two\npublicly available, real-world image encoders, i.e., Google's image encoder\npre-trained on ImageNet and OpenAI's Contrastive Language-Image Pre-training\n(CLIP) image encoder pre-trained on 400 million (image, text) pairs collected\nfrom the Internet. Moreover, we consider defenses including Neural Cleanse and\nMNTD (empirical defenses) as well as PatchGuard (a provable defense). Our\nresults show that these defenses are insufficient to defend against BadEncoder,\nhighlighting the needs for new defenses against our BadEncoder. Our code is\npublicly available at: https://github.com/jjy1994/BadEncoder.",
          "link": "http://arxiv.org/abs/2108.00352",
          "publishedOn": "2021-08-03T02:06:30.633Z",
          "wordCount": 693,
          "title": "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning. (arXiv:2108.00352v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>",
          "description": "Most existing Siamese-based tracking methods execute the classification and\nregression of the target object based on the similarity maps. However, they\neither employ a single map from the last convolutional layer which degrades the\nlocalization accuracy in complex scenarios or separately use multiple maps for\ndecision making, introducing intractable computations for aerial mobile\nplatforms. Thus, in this work, we propose an efficient and effective\nhierarchical feature transformer (HiFT) for aerial tracking. Hierarchical\nsimilarity maps generated by multi-level convolutional layers are fed into the\nfeature transformer to achieve the interactive fusion of spatial (shallow\nlayers) and semantics cues (deep layers). Consequently, not only the global\ncontextual information can be raised, facilitating the target search, but also\nour end-to-end architecture with the transformer can efficiently learn the\ninterdependencies among multi-level features, thereby discovering a\ntracking-tailored feature space with strong discriminability. Comprehensive\nevaluations on four aerial benchmarks have proven the effectiveness of HiFT.\nReal-world tests on the aerial platform have strongly validated its\npracticability with a real-time speed. Our code is available at\nhttps://github.com/vision4robotics/HiFT.",
          "link": "http://arxiv.org/abs/2108.00202",
          "publishedOn": "2021-08-03T02:06:30.623Z",
          "wordCount": 621,
          "title": "HiFT: Hierarchical Feature Transformer for Aerial Tracking. (arXiv:2108.00202v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Fang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sanping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wei Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>",
          "description": "Understanding complex social interactions among agents is a key challenge for\ntrajectory prediction. Most existing methods consider the interactions between\npairwise traffic agents or in a local area, while the nature of interactions is\nunlimited, involving an uncertain number of agents and non-local areas\nsimultaneously. Besides, they only focus on homogeneous trajectory prediction,\nnamely those among agents of the same category, while neglecting people's\ndiverse reaction patterns toward traffic agents in different categories. To\naddress these problems, we propose a simple yet effective Unlimited\nNeighborhood Interaction Network (UNIN), which predicts trajectories of\nheterogeneous agents in multiply categories. Specifically, the proposed\nunlimited neighborhood interaction module generates the fused-features of all\nagents involved in an interaction simultaneously, which is adaptive to any\nnumber of agents and any range of interaction area. Meanwhile, a hierarchical\ngraph attention module is proposed to obtain category-tocategory interaction\nand agent-to-agent interaction. Finally, parameters of a Gaussian Mixture Model\nare estimated for generating the future trajectories. Extensive experimental\nresults on benchmark datasets demonstrate a significant performance improvement\nof our method over the state-ofthe-art methods.",
          "link": "http://arxiv.org/abs/2108.00238",
          "publishedOn": "2021-08-03T02:06:30.605Z",
          "wordCount": 618,
          "title": "Unlimited Neighborhood Interaction for Heterogeneous Trajectory Prediction. (arXiv:2108.00238v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shan_M/0/1/0/all/0/1\">Mo Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qiaojun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jau_Y/0/1/0/all/0/1\">You-Yi Jau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanasov_N/0/1/0/all/0/1\">Nikolay Atanasov</a>",
          "description": "Autonomous systems need to understand the semantics and geometry of their\nsurroundings in order to comprehend and safely execute object-level task\nspecifications. This paper proposes an expressive yet compact model for joint\nobject pose and shape optimization, and an associated optimization algorithm to\ninfer an object-level map from multi-view RGB-D camera observations. The model\nis expressive because it captures the identities, positions, orientations, and\nshapes of objects in the environment. It is compact because it relies on a\nlow-dimensional latent representation of implicit object shape, allowing\nonboard storage of large multi-category object maps. Different from other works\nthat rely on a single object representation format, our approach has a bi-level\nobject model that captures both the coarse level scale as well as the fine\nlevel shape details. Our approach is evaluated on the large-scale real-world\nScanNet dataset and compared against state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.00355",
          "publishedOn": "2021-08-03T02:06:30.599Z",
          "wordCount": 598,
          "title": "ELLIPSDF: Joint Object Pose and Shape Optimization with a Bi-level Ellipsoid and Signed Distance Function Description. (arXiv:2108.00355v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Mingyuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoqiong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuxin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xindi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F Frangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>",
          "description": "3D ultrasound (US) is widely used for its rich diagnostic information.\nHowever, it is criticized for its limited field of view. 3D freehand US\nreconstruction is promising in addressing the problem by providing broad range\nand freeform scan. The existing deep learning based methods only focus on the\nbasic cases of skill sequences, and the model relies on the training data\nheavily. The sequences in real clinical practice are a mix of diverse skills\nand have complex scanning paths. Besides, deep models should adapt themselves\nto the testing cases with prior knowledge for better robustness, rather than\nonly fit to the training cases. In this paper, we propose a novel approach to\nsensorless freehand 3D US reconstruction considering the complex skill\nsequences. Our contribution is three-fold. First, we advance a novel online\nlearning framework by designing a differentiable reconstruction algorithm. It\nrealizes an end-to-end optimization from section sequences to the reconstructed\nvolume. Second, a self-supervised learning method is developed to explore the\ncontext information that reconstructed by the testing data itself, promoting\nthe perception of the model. Third, inspired by the effectiveness of shape\nprior, we also introduce adversarial training to strengthen the learning of\nanatomical shape prior in the reconstructed volume. By mining the context and\nstructural cues of the testing data, our online learning methods can drive the\nmodel to handle complex skill sequences. Experimental results on developmental\ndysplasia of the hip US and fetal US datasets show that, our proposed method\ncan outperform the start-of-the-art methods regarding the shift errors and path\nsimilarities.",
          "link": "http://arxiv.org/abs/2108.00274",
          "publishedOn": "2021-08-03T02:06:30.590Z",
          "wordCount": 720,
          "title": "Self Context and Shape Prior for Sensorless Freehand 3D Ultrasound Reconstruction. (arXiv:2108.00274v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Li Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kaiwen Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Z. Jane Wang</a>",
          "description": "Deep learning based image classification models are shown vulnerable to\nadversarial attacks by injecting deliberately crafted noises to clean images.\nTo defend against adversarial attacks in a training-free and attack-agnostic\nmanner, this work proposes a novel and effective reconstruction-based defense\nframework by delving into deep image prior (DIP). Fundamentally different from\nexisting reconstruction-based defenses, the proposed method analyzes and\nexplicitly incorporates the model decision process into our defense. Given an\nadversarial image, firstly we map its reconstructed images during DIP\noptimization to the model decision space, where cross-boundary images can be\ndetected and on-boundary images can be further localized. Then, adversarial\nnoise is purified by perturbing on-boundary images along the reverse direction\nto the adversarial image. Finally, on-manifold images are stitched to construct\nan image that can be correctly predicted by the victim classifier. Extensive\nexperiments demonstrate that the proposed method outperforms existing\nstate-of-the-art reconstruction-based methods both in defending white-box\nattacks and defense-aware attacks. Moreover, the proposed method can maintain a\nhigh visual quality during adversarial image reconstruction.",
          "link": "http://arxiv.org/abs/2108.00180",
          "publishedOn": "2021-08-03T02:06:30.581Z",
          "wordCount": 629,
          "title": "Delving into Deep Image Prior for Adversarial Defense: A Novel Reconstruction-based Defense Framework. (arXiv:2108.00180v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaibing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1\">Renshu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toyoura_M/0/1/0/all/0/1\">Masahiro Toyoura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Gang Xu</a>",
          "description": "A key challenge in the task of human pose and shape estimation is occlusion,\nincluding self-occlusions, object-human occlusions, and inter-person\nocclusions. The lack of diverse and accurate pose and shape training data\nbecomes a major bottleneck, especially for scenes with occlusions in the wild.\nIn this paper, we focus on the estimation of human pose and shape in the case\nof inter-person occlusions, while also handling object-human occlusions and\nself-occlusion. We propose a framework that synthesizes occlusion-aware\nsilhouette and 2D keypoints data and directly regress to the SMPL pose and\nshape parameters. A neural 3D mesh renderer is exploited to enable silhouette\nsupervision on the fly, which contributes to great improvements in shape\nestimation. In addition, keypoints-and-silhouette-driven training data in\npanoramic viewpoints are synthesized to compensate for the lack of viewpoint\ndiversity in any existing dataset. Experimental results show that we are among\nstate-of-the-art on the 3DPW dataset in terms of pose accuracy and evidently\noutperform the rank-1 method in terms of shape accuracy. Top performance is\nalso achieved on SSP-3D in terms of shape prediction accuracy.",
          "link": "http://arxiv.org/abs/2108.00351",
          "publishedOn": "2021-08-03T02:06:30.573Z",
          "wordCount": 628,
          "title": "LASOR: Learning Accurate 3D Human Pose and Shape Via Synthetic Occlusion-Aware Data and Neural Mesh Rendering. (arXiv:2108.00351v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_D/0/1/0/all/0/1\">Deming Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>",
          "description": "Learning with noisy labels is an important and challenging task for training\naccurate deep neural networks. Some commonly-used loss functions, such as Cross\nEntropy (CE), suffer from severe overfitting to noisy labels. Robust loss\nfunctions that satisfy the symmetric condition were tailored to remedy this\nproblem, which however encounter the underfitting effect. In this paper, we\ntheoretically prove that \\textbf{any loss can be made robust to noisy labels}\nby restricting the network output to the set of permutations over a fixed\nvector. When the fixed vector is one-hot, we only need to constrain the output\nto be one-hot, which however produces zero gradients almost everywhere and thus\nmakes gradient-based optimization difficult. In this work, we introduce the\nsparse regularization strategy to approximate the one-hot constraint, which is\ncomposed of network output sharpening operation that enforces the output\ndistribution of a network to be sharp and the $\\ell_p$-norm ($p\\le 1$)\nregularization that promotes the network output to be sparse. This simple\napproach guarantees the robustness of arbitrary loss functions while not\nhindering the fitting ability. Experimental results demonstrate that our method\ncan significantly improve the performance of commonly-used loss functions in\nthe presence of noisy labels and class imbalance, and outperform the\nstate-of-the-art methods. The code is available at\nhttps://github.com/hitcszx/lnl_sr.",
          "link": "http://arxiv.org/abs/2108.00192",
          "publishedOn": "2021-08-03T02:06:30.550Z",
          "wordCount": 657,
          "title": "Learning with Noisy Labels via Sparse Regularization. (arXiv:2108.00192v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kelvin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Healey_C/0/1/0/all/0/1\">Christopher Healey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianfu Wu</a>",
          "description": "Stereo matching has recently witnessed remarkable progress using Deep Neural\nNetworks (DNNs). But, how robust are they? Although it has been well-known that\nDNNs often suffer from adversarial vulnerability with a catastrophic drop in\nperformance, the situation is even worse in stereo matching. This paper first\nshows that a type of weak white-box attacks can fail state-of-the-art methods.\nThe attack is learned by a proposed stereo-constrained projected gradient\ndescent (PGD) method in stereo matching. This observation raises serious\nconcerns for the deployment of DNN-based stereo matching. Parallel to the\nadversarial vulnerability, DNN-based stereo matching is typically trained under\nthe so-called simulation to reality pipeline, and thus domain generalizability\nis an important problem. This paper proposes to rethink the learnable DNN-based\nfeature backbone towards adversarially-robust and domain generalizable stereo\nmatching, either by completely removing it or by applying it only to the left\nreference image. It computes the matching cost volume using the classic\nmulti-scale census transform (i.e., local binary pattern) of the raw input\nstereo images, followed by a stacked Hourglass head sub-network solving the\nmatching problem. In experiments, the proposed method is tested in the\nSceneFlow dataset and the KITTI2015 benchmark. It significantly improves the\nadversarial robustness, while retaining accuracy performance comparable to\nstate-of-the-art methods. It also shows better generalizability from simulation\n(SceneFlow) to real (KITTI) datasets when no fine-tuning is used.",
          "link": "http://arxiv.org/abs/2108.00335",
          "publishedOn": "2021-08-03T02:06:30.542Z",
          "wordCount": 670,
          "title": "Towards Adversarially Robust and Domain Generalizable Stereo Matching by Rethinking DNN Feature Backbones. (arXiv:2108.00335v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boekhoudt_K/0/1/0/all/0/1\">Kayleigh Boekhoudt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matei_A/0/1/0/all/0/1\">Alina Matei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghaei_M/0/1/0/all/0/1\">Maya Aghaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talavera_E/0/1/0/all/0/1\">Estefan&#xed;a Talavera</a>",
          "description": "The automatic detection of anomalies captured by surveillance settings is\nessential for speeding the otherwise laborious approach. To date, UCF-Crime is\nthe largest available dataset for automatic visual analysis of anomalies and\nconsists of real-world crime scenes of various categories. In this paper, we\nintroduce HR-Crime, a subset of the UCF-Crime dataset suitable for\nhuman-related anomaly detection tasks. We rely on state-of-the-art techniques\nto build the feature extraction pipeline for human-related anomaly detection.\nFurthermore, we present the baseline anomaly detection analysis on the\nHR-Crime. HR-Crime as well as the developed feature extraction pipeline and the\nextracted features will be publicly available for further research in the\nfield.",
          "link": "http://arxiv.org/abs/2108.00246",
          "publishedOn": "2021-08-03T02:06:30.533Z",
          "wordCount": 548,
          "title": "HR-Crime: Human-Related Anomaly Detection in Surveillance Videos. (arXiv:2108.00246v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00273",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Muhammad Monjurul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ruwen Qin</a>",
          "description": "Traffic accident anticipation is a vital function of Automated Driving\nSystems (ADSs) for providing a safety-guaranteed driving experience. An\naccident anticipation model aims to predict accidents promptly and accurately\nbefore they occur. Existing Artificial Intelligence (AI) models of accident\nanticipation lack a human-interpretable explanation of their decision-making.\nAlthough these models perform well, they remain a black-box to the ADS users,\nthus difficult to get their trust. To this end, this paper presents a Gated\nRecurrent Unit (GRU) network that learns spatio-temporal relational features\nfor the early anticipation of traffic accidents from dashcam video data. A\npost-hoc attention mechanism named Grad-CAM is integrated into the network to\ngenerate saliency maps as the visual explanation of the accident anticipation\ndecision. An eye tracker captures human eye fixation points for generating\nhuman attention maps. The explainability of network-generated saliency maps is\nevaluated in comparison to human attention maps. Qualitative and quantitative\nresults on a public crash dataset confirm that the proposed explainable network\ncan anticipate an accident on average 4.57 seconds before it occurs, with\n94.02% average precision. In further, various post-hoc attention-based XAI\nmethods are evaluated and compared. It confirms that the Grad-CAM chosen by\nthis study can generate high-quality, human-interpretable saliency maps (with\n1.42 Normalized Scanpath Saliency) for explaining the crash anticipation\ndecision. Importantly, results confirm that the proposed AI model, with a\nhuman-inspired design, can outperform humans in the accident anticipation.",
          "link": "http://arxiv.org/abs/2108.00273",
          "publishedOn": "2021-08-03T02:06:30.522Z",
          "wordCount": 677,
          "title": "Towards explainable artificial intelligence (XAI) for early anticipation of traffic accidents. (arXiv:2108.00273v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Ziyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhenghao Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Gangming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Weifeng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>",
          "description": "Deep features have been proven powerful in building accurate dense semantic\ncorrespondences in various previous works. However, the multi-scale and\npyramidal hierarchy of convolutional neural networks has not been well studied\nto learn discriminative pixel-level features for semantic correspondence. In\nthis paper, we propose a multi-scale matching network that is sensitive to tiny\nsemantic differences between neighboring pixels. We follow the coarse-to-fine\nmatching strategy and build a top-down feature and matching enhancement scheme\nthat is coupled with the multi-scale hierarchy of deep convolutional neural\nnetworks. During feature enhancement, intra-scale enhancement fuses\nsame-resolution feature maps from multiple layers together via local\nself-attention and cross-scale enhancement hallucinates higher-resolution\nfeature maps along the top-down hierarchy. Besides, we learn complementary\nmatching details at different scales thus the overall matching score is refined\nby features of different semantic levels gradually. Our multi-scale matching\nnetwork can be trained end-to-end easily with few additional learnable\nparameters. Experimental results demonstrate that the proposed method achieves\nstate-of-the-art performance on three popular benchmarks with high\ncomputational efficiency.",
          "link": "http://arxiv.org/abs/2108.00211",
          "publishedOn": "2021-08-03T02:06:30.514Z",
          "wordCount": 609,
          "title": "Multi-scale Matching Networks for Semantic Correspondence. (arXiv:2108.00211v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>",
          "description": "Transformers have made much progress in dealing with visual tasks. However,\nexisting vision transformers still do not possess an ability that is important\nto visual input: building the attention among features of different scales. The\nreasons for this problem are two-fold: (1) Input embeddings of each layer are\nequal-scale without cross-scale features; (2) Some vision transformers\nsacrifice the small-scale features of embeddings to lower the cost of the\nself-attention module. To make up this defect, we propose Cross-scale Embedding\nLayer (CEL) and Long Short Distance Attention (LSDA). In particular, CEL blends\neach embedding with multiple patches of different scales, providing the model\nwith cross-scale embeddings. LSDA splits the self-attention module into a\nshort-distance and long-distance one, also lowering the cost but keeping both\nsmall-scale and large-scale features in embeddings. Through these two designs,\nwe achieve cross-scale attention. Besides, we propose dynamic position bias for\nvision transformers to make the popular relative position bias apply to\nvariable-sized images. Based on these proposed modules, we construct our vision\narchitecture called CrossFormer. Experiments show that CrossFormer outperforms\nother transformers on several representative visual tasks, especially object\ndetection and segmentation. The code has been released:\nhttps://github.com/cheerss/CrossFormer.",
          "link": "http://arxiv.org/abs/2108.00154",
          "publishedOn": "2021-08-03T02:06:30.495Z",
          "wordCount": 648,
          "title": "CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention. (arXiv:2108.00154v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Emam_Z/0/1/0/all/0/1\">Zeyad Emam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondrich_A/0/1/0/all/0/1\">Andrew Kondrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_S/0/1/0/all/0/1\">Sasha Harrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_F/0/1/0/all/0/1\">Felix Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yushi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1\">Aerin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Branson_E/0/1/0/all/0/1\">Elliot Branson</a>",
          "description": "High-quality labeled datasets play a crucial role in fueling the development\nof machine learning (ML), and in particular the development of deep learning\n(DL). However, since the emergence of the ImageNet dataset and the AlexNet\nmodel in 2012, the size of new open-source labeled vision datasets has remained\nroughly constant. Consequently, only a minority of publications in the computer\nvision community tackle supervised learning on datasets that are orders of\nmagnitude larger than Imagenet. In this paper, we survey computer vision\nresearch domains that study the effects of such large datasets on model\nperformance across different vision tasks. We summarize the community's current\nunderstanding of those effects, and highlight some open questions related to\ntraining with massive datasets. In particular, we tackle: (a) The largest\ndatasets currently used in computer vision research and the interesting\ntakeaways from training on such datasets; (b) The effectiveness of pre-training\non large datasets; (c) Recent advancements and hurdles facing synthetic\ndatasets; (d) An overview of double descent and sample non-monotonicity\nphenomena; and finally, (e) A brief discussion of lifelong/continual learning\nand how it fares compared to learning from huge labeled datasets in an offline\nsetting. Overall, our findings are that research on optimization for deep\nlearning focuses on perfecting the training routine and thus making DL models\nless data hungry, while research on synthetic datasets aims to offset the cost\nof data labeling. However, for the time being, acquiring non-synthetic labeled\ndata remains indispensable to boost performance.",
          "link": "http://arxiv.org/abs/2108.00114",
          "publishedOn": "2021-08-03T02:06:30.486Z",
          "wordCount": 707,
          "title": "On The State of Data In Computer Vision: Human Annotations Remain Indispensable for Developing Deep Learning Models. (arXiv:2108.00114v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gichoya_J/0/1/0/all/0/1\">Judy Wawira Gichoya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purkayastha_S/0/1/0/all/0/1\">Saptarshi Purkayastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_I/0/1/0/all/0/1\">Imon Banerjee</a>",
          "description": "Traditional anomaly detection methods focus on detecting inter-class\nvariations while medical image novelty identification is inherently an\nintra-class detection problem. For example, a machine learning model trained\nwith normal chest X-ray and common lung abnormalities, is expected to discover\nand flag idiopathic pulmonary fibrosis which a rare lung disease and unseen by\nthe model during training. The nuances from intra-class variations and lack of\nrelevant training data in medical image analysis pose great challenges for\nexisting anomaly detection methods. To tackle the challenges, we propose a\nhybrid model - Transformation-based Embedding learning for Novelty Detection\n(TEND) which without any out-of-distribution training data, performs novelty\nidentification by combining both autoencoder-based and classifier-based method.\nWith a pre-trained autoencoder as image feature extractor, TEND learns to\ndiscriminate the feature embeddings of in-distribution data from the\ntransformed counterparts as fake out-of-distribution inputs. To enhance the\nseparation, a distance objective is optimized to enforce a margin between the\ntwo classes. Extensive experimental results on both natural image datasets and\nmedical image datasets are presented and our method out-performs\nstate-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.00117",
          "publishedOn": "2021-08-03T02:06:30.477Z",
          "wordCount": 614,
          "title": "Margin-Aware Intra-Class Novelty Identification for Medical Images. (arXiv:2108.00117v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>",
          "description": "Occluded person re-identification (ReID) aims to match person images with\nocclusion. It is fundamentally challenging because of the serious occlusion\nwhich aggravates the misalignment problem between images. At the cost of\nincorporating a pose estimator, many works introduce pose information to\nalleviate the misalignment in both training and testing. To achieve high\naccuracy while preserving low inference complexity, we propose a network named\nPose-Guided Feature Learning with Knowledge Distillation (PGFL-KD), where the\npose information is exploited to regularize the learning of semantics aligned\nfeatures but is discarded in testing. PGFL-KD consists of a main branch (MB),\nand two pose-guided branches, \\ieno, a foreground-enhanced branch (FEB), and a\nbody part semantics aligned branch (SAB). The FEB intends to emphasise the\nfeatures of visible body parts while excluding the interference of obstructions\nand background (\\ieno, foreground feature alignment). The SAB encourages\ndifferent channel groups to focus on different body parts to have body part\nsemantics aligned representation. To get rid of the dependency on pose\ninformation when testing, we regularize the MB to learn the merits of the FEB\nand SAB through knowledge distillation and interaction-based training.\nExtensive experiments on occluded, partial, and holistic ReID tasks show the\neffectiveness of our proposed network.",
          "link": "http://arxiv.org/abs/2108.00139",
          "publishedOn": "2021-08-03T02:06:30.442Z",
          "wordCount": 652,
          "title": "Pose-Guided Feature Learning with Knowledge Distillation for Occluded Person Re-Identification. (arXiv:2108.00139v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">You-Wei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1\">Chuan-Xian Ren</a>",
          "description": "As a vital problem in classification-oriented transfer, unsupervised domain\nadaptation (UDA) has attracted widespread attention in recent years. Previous\nUDA methods assume the marginal distributions of different domains are shifted\nwhile ignoring the discriminant information in the label distributions. This\nleads to classification performance degeneration in real applications. In this\nwork, we focus on the conditional distribution shift problem which is of great\nconcern to current conditional invariant models. We aim to seek a kernel\ncovariance embedding for conditional distribution which remains yet unexplored.\nTheoretically, we propose the Conditional Kernel Bures (CKB) metric for\ncharacterizing conditional distribution discrepancy, and derive an empirical\nestimation for the CKB metric without introducing the implicit kernel feature\nmap. It provides an interpretable approach to understand the knowledge transfer\nmechanism. The established consistency theory of the empirical estimation\nprovides a theoretical guarantee for convergence. A conditional distribution\nmatching network is proposed to learn the conditional invariant and\ndiscriminative features for UDA. Extensive experiments and analysis show the\nsuperiority of our proposed model.",
          "link": "http://arxiv.org/abs/2108.00302",
          "publishedOn": "2021-08-03T02:06:30.434Z",
          "wordCount": 599,
          "title": "Conditional Bures Metric for Domain Adaptation. (arXiv:2108.00302v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fengping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Chun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_D/0/1/0/all/0/1\">Danmin Miao</a>",
          "description": "Micro-expressions are spontaneous, unconscious facial movements that show\npeople's true inner emotions and have great potential in related fields of\npsychological testing. Since the face is a 3D deformation object, the\noccurrence of an expression can arouse spatial deformation of the face, but\nlimited by the available databases are 2D videos, which lack the description of\n3D spatial information of micro-expressions. Therefore, we proposed a new\nmicro-expression database containing 2D video sequences and 3D point clouds\nsequences. The database includes 259 micro-expressions sequences, and these\nsamples were classified using the objective method based on facial action\ncoding system, as well as the non-objective method that combines video contents\nand participants' self-reports. We extracted facial 2D and 3D features using\nlocal binary patterns on three orthogonal planes and curvature descriptors,\nrespectively, and performed baseline evaluations of the two features and their\nfusion results with leave-one-subject-out(LOSO) and 10-fold cross-validation\nmethods. The best fusion performances were 58.84% and 73.03% for non-objective\nclassification and 66.36% and 77.42% for objective classification, both of\nwhich have improved performance compared to using LBP-TOP features only.The\ndatabase offers original and cropped micro-expression samples, which will\nfacilitate the exploration and research on 3D Spatio-temporal features of\nmicro-expressions.",
          "link": "http://arxiv.org/abs/2108.00166",
          "publishedOn": "2021-08-03T02:06:30.427Z",
          "wordCount": 637,
          "title": "A Dynamic 3D Spontaneous Micro-expression Database: Establishment and Evaluation. (arXiv:2108.00166v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00094",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gutierrez_N/0/1/0/all/0/1\">Nolan B. Gutierrez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beksi_W/0/1/0/all/0/1\">William J. Beksi</a>",
          "description": "Thermal images model the long-infrared range of the electromagnetic spectrum\nand provide meaningful information even when there is no visible illumination.\nYet, unlike imagery that represents radiation from the visible continuum,\ninfrared images are inherently low-resolution due to hardware constraints. The\nrestoration of thermal images is critical for applications that involve safety,\nsearch and rescue, and military operations. In this paper, we introduce a\nsystem to efficiently reconstruct thermal images. Specifically, we explore how\nto effectively attend to contrasting receptive fields (RFs) where increasing\nthe RFs of a network can be computationally expensive. For this purpose, we\nintroduce a deep attention to varying receptive fields network (AVRFN). We\nsupply a gated convolutional layer with higher-order information extracted from\ndisparate RFs, whereby an RF is parameterized by a dilation rate. In this way,\nthe dilation rate can be tuned to use fewer parameters thus increasing the\nefficacy of AVRFN. Our experimental results show an improvement over the state\nof the art when compared against competing thermal image super-resolution\nmethods.",
          "link": "http://arxiv.org/abs/2108.00094",
          "publishedOn": "2021-08-03T02:06:30.395Z",
          "wordCount": 631,
          "title": "Thermal Image Super-Resolution Using Second-Order Channel Attention with Varying Receptive Fields. (arXiv:2108.00094v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00146",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1\">Lipeng Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>",
          "description": "Top-$k$ multi-label learning, which returns the top-$k$ predicted labels from\nan input, has many practical applications such as image annotation, document\nanalysis, and web search engine. However, the vulnerabilities of such\nalgorithms with regards to dedicated adversarial perturbation attacks have not\nbeen extensively studied previously. In this work, we develop methods to create\nadversarial perturbations that can be used to attack top-$k$ multi-label\nlearning-based image annotation systems (TkML-AP). Our methods explicitly\nconsider the top-$k$ ranking relation and are based on novel loss functions.\nExperimental evaluations on large-scale benchmark datasets including PASCAL VOC\nand MS COCO demonstrate the effectiveness of our methods in reducing the\nperformance of state-of-the-art top-$k$ multi-label learning methods, under\nboth untargeted and targeted attacks.",
          "link": "http://arxiv.org/abs/2108.00146",
          "publishedOn": "2021-08-03T02:06:30.388Z",
          "wordCount": 563,
          "title": "T$_k$ML-AP: Adversarial Attacks to Top-$k$ Multi-Label Learning. (arXiv:2108.00146v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhaoming Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1\">Liang Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>",
          "description": "In this paper, we propose MGNet, a simple and effective multiplex graph\nconvolutional network (GCN) model for multimodal brain network analysis. The\nproposed method integrates tensor representation into the multiplex GCN model\nto extract the latent structures of a set of multimodal brain networks, which\nallows an intuitive 'grasping' of the common space for multimodal data.\nMultimodal representations are then generated with multiplex GCNs to capture\nspecific graph structures. We conduct classification task on two challenging\nreal-world datasets (HIV and Bipolar disorder), and the proposed MGNet\ndemonstrates state-of-the-art performance compared to competitive benchmark\nmethods. Apart from objective evaluations, this study may bear special\nsignificance upon network theory to the understanding of human connectome in\ndifferent modalities. The code is available at\nhttps://github.com/ZhaomingKong/MGNets.",
          "link": "http://arxiv.org/abs/2108.00158",
          "publishedOn": "2021-08-03T02:06:30.381Z",
          "wordCount": 563,
          "title": "Multiplex Graph Networks for Multimodal Brain Network Analysis. (arXiv:2108.00158v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1\">Zhongyun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Gang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chunxia Xiao</a>",
          "description": "The seamless illumination integration between a foreground object and a\nbackground scene is an important but challenging task in computer vision and\naugmented reality community. However, to our knowledge, there is no publicly\navailable high-quality dataset that meets the illumination seamless integration\ntask, which greatly hinders the development of this research direction. To this\nend, we apply a physically-based rendering method to create a large-scale,\nhigh-quality dataset, named IH dataset, which provides rich illumination\ninformation for seamless illumination integration task. In addition, we propose\na deep learning-based SI-GAN method, a multi-task collaborative network, which\nmakes full use of the multi-scale attention mechanism and adversarial learning\nstrategy to directly infer mapping relationship between the inserted foreground\nobject and corresponding background environment, and edit object illumination\naccording to the proposed illumination exchange mechanism in parallel network.\nBy this means, we can achieve the seamless illumination integration without\nexplicit estimation of 3D geometric information. Comprehensive experiments on\nboth our dataset and real-world images collected from the Internet show that\nour proposed SI-GAN provides a practical and effective solution for image-based\nobject illumination editing, and validate the superiority of our method against\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.00150",
          "publishedOn": "2021-08-03T02:06:30.360Z",
          "wordCount": 633,
          "title": "Scene Inference for Object Illumination Editing. (arXiv:2108.00150v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara L. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "We introduce mTVR, a large-scale multilingual video moment retrieval dataset,\ncontaining 218K English and Chinese queries from 21.8K TV show video clips. The\ndataset is collected by extending the popular TVR dataset (in English) with\npaired Chinese queries and subtitles. Compared to existing moment retrieval\ndatasets, mTVR is multilingual, larger, and comes with diverse annotations. We\nfurther propose mXML, a multilingual moment retrieval model that learns and\noperates on data from both languages, via encoder parameter sharing and\nlanguage neighborhood constraints. We demonstrate the effectiveness of mXML on\nthe newly collected MTVR dataset, where mXML outperforms strong monolingual\nbaselines while using fewer parameters. In addition, we also provide detailed\ndataset analyses and model ablations. Data and code are publicly available at\nhttps://github.com/jayleicn/mTVRetrieval",
          "link": "http://arxiv.org/abs/2108.00061",
          "publishedOn": "2021-08-03T02:06:30.354Z",
          "wordCount": 569,
          "title": "MTVR: Multilingual Moment Retrieval in Videos. (arXiv:2108.00061v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Sangwoo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hyunwoo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kihyuk Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Contrastive self-supervised learning has shown impressive results in learning\nvisual representations from unlabeled images by enforcing invariance against\ndifferent data augmentations. However, the learned representations are often\ncontextually biased to the spurious scene correlations of different objects or\nobject and background, which may harm their generalization on the downstream\ntasks. To tackle the issue, we develop a novel object-aware contrastive\nlearning framework that first (a) localizes objects in a self-supervised manner\nand then (b) debias scene correlations via appropriate data augmentations\nconsidering the inferred object locations. For (a), we propose the contrastive\nclass activation map (ContraCAM), which finds the most discriminative regions\n(e.g., objects) in the image compared to the other images using the\ncontrastively trained models. We further improve the ContraCAM to detect\nmultiple objects and entire shapes via an iterative refinement procedure. For\n(b), we introduce two data augmentations based on ContraCAM, object-aware\nrandom crop and background mixup, which reduce contextual and background biases\nduring contrastive self-supervised learning, respectively. Our experiments\ndemonstrate the effectiveness of our representation learning framework,\nparticularly when trained under multi-object images or evaluated under the\nbackground (and distribution) shifted images.",
          "link": "http://arxiv.org/abs/2108.00049",
          "publishedOn": "2021-08-03T02:06:30.347Z",
          "wordCount": 632,
          "title": "Object-aware Contrastive Learning for Debiased Scene Representation. (arXiv:2108.00049v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Gary Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wessler_B/0/1/0/all/0/1\">Benjamin Wessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1\">Michael C. Hughes</a>",
          "description": "Semi-supervised image classification has shown substantial progress in\nlearning from limited labeled data, but recent advances remain largely untested\nfor clinical applications. Motivated by the urgent need to improve timely\ndiagnosis of life-threatening heart conditions, especially aortic stenosis, we\ndevelop a benchmark dataset to assess semi-supervised approaches to two tasks\nrelevant to cardiac ultrasound (echocardiogram) interpretation: view\nclassification and disease severity classification. We find that a\nstate-of-the-art method called MixMatch achieves promising gains in heldout\naccuracy on both tasks, learning from a large volume of truly unlabeled images\nas well as a labeled set collected at great expense to achieve better\nperformance than is possible with the labeled set alone. We further pursue\npatient-level diagnosis prediction, which requires aggregating across hundreds\nof images of diverse view types, most of which are irrelevant, to make a\ncoherent prediction. The best patient-level performance is achieved by new\nmethods that prioritize diagnosis predictions from images that are predicted to\nbe clinically-relevant views and transfer knowledge from the view task to the\ndiagnosis task. We hope our released Tufts Medical Echocardiogram Dataset and\nevaluation framework inspire further improvements in multi-task semi-supervised\nlearning for clinical applications.",
          "link": "http://arxiv.org/abs/2108.00080",
          "publishedOn": "2021-08-03T02:06:30.267Z",
          "wordCount": 674,
          "title": "A New Semi-supervised Learning Benchmark for Classifying View and Diagnosing Aortic Stenosis from Echocardiograms. (arXiv:2108.00080v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00145",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lantao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kuida Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orchard_M/0/1/0/all/0/1\">Michael T. Orchard</a>",
          "description": "Manifold models consider natural-image patches to be on a low-dimensional\nmanifold embedded in a high dimensional state space and each patch and its\nsimilar patches to approximately lie on a linear affine subspace. Manifold\nmodels are closely related to semi-local similarity, a well-known property of\nnatural images, referring to that for most natural-image patches, several\nsimilar patches can be found in its spatial neighborhood. Many approaches to\nsingle image interpolation use manifold models to exploit semi-local similarity\nby two mutually exclusive parts: i) searching each target patch's similar\npatches and ii) operating on the searched similar patches, the target patch and\nthe measured input pixels to estimate the target patch. Unfortunately, aliasing\nin the input image makes it challenging for both parts. A very few works\nexplicitly deal with those challenges and only ad-hoc solutions are proposed.\n\nTo overcome the challenge in the first part, we propose a carefully-designed\nadaptive technique to remove aliasing in severely aliased regions, which cannot\nbe removed from traditional techniques. This technique enables reliable\nidentification of similar patches even in the presence of strong aliasing. To\novercome the challenge in the second part, we propose to use the\naliasing-removed image to guide the initialization of the interpolated image\nand develop a progressive scheme to refine the interpolated image based on\nmanifold models. Experimental results demonstrate that our approach\nreconstructs edges with both smoothness along contours and sharpness across\nprofiles, and achieves an average Peak Signal-to-Noise Ratio (PSNR)\nsignificantly higher than existing model-based approaches.",
          "link": "http://arxiv.org/abs/2108.00145",
          "publishedOn": "2021-08-03T02:06:30.254Z",
          "wordCount": 676,
          "title": "Manifold-Inspired Single Image Interpolation. (arXiv:2108.00145v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dyck_L/0/1/0/all/0/1\">Leonard E. van Dyck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwitt_R/0/1/0/all/0/1\">Roland Kwitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_S/0/1/0/all/0/1\">Sebastian J. Denzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruber_W/0/1/0/all/0/1\">Walter R. Gruber</a>",
          "description": "Deep convolutional neural networks (DCNNs) and the ventral visual pathway\nshare vast architectural and functional similarities in visual challenges such\nas object recognition. Recent insights have demonstrated that both hierarchical\ncascades can be compared in terms of both exerted behavior and underlying\nactivation. However, these approaches ignore key differences in spatial\npriorities of information processing. In this proof-of-concept study, we\ndemonstrate a comparison of human observers (N = 45) and three feedforward\nDCNNs through eye tracking and saliency maps. The results reveal fundamentally\ndifferent resolutions in both visualization methods that need to be considered\nfor an insightful comparison. Moreover, we provide evidence that a DCNN with\nbiologically plausible receptive field sizes called vNet reveals higher\nagreement with human viewing behavior as contrasted with a standard ResNet\narchitecture. We find that image-specific factors such as category, animacy,\narousal, and valence have a direct link to the agreement of spatial object\nrecognition priorities in humans and DCNNs, while other measures such as\ndifficulty and general image properties do not. With this approach, we try to\nopen up new perspectives at the intersection of biological and computer vision\nresearch.",
          "link": "http://arxiv.org/abs/2108.00107",
          "publishedOn": "2021-08-03T02:06:30.233Z",
          "wordCount": 648,
          "title": "Comparing object recognition in humans and deep convolutional neural networks -- An eye tracking study. (arXiv:2108.00107v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alamri_F/0/1/0/all/0/1\">Faisal Alamri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1\">Anjan Dutta</a>",
          "description": "Zero-Shot Learning (ZSL) aims to recognise unseen object classes, which are\nnot observed during the training phase. The existing body of works on ZSL\nmostly relies on pretrained visual features and lacks the explicit attribute\nlocalisation mechanism on images. In this work, we propose an attention-based\nmodel in the problem settings of ZSL to learn attributes useful for unseen\nclass recognition. Our method uses an attention mechanism adapted from Vision\nTransformer to capture and learn discriminative attributes by splitting images\ninto small patches. We conduct experiments on three popular ZSL benchmarks\n(i.e., AWA2, CUB and SUN) and set new state-of-the-art harmonic mean results\n{on all the three datasets}, which illustrate the effectiveness of our proposed\nmethod.",
          "link": "http://arxiv.org/abs/2108.00045",
          "publishedOn": "2021-08-03T02:06:30.226Z",
          "wordCount": 562,
          "title": "Multi-Head Self-Attention via Vision Transformer for Zero-Shot Learning. (arXiv:2108.00045v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1\">Dario Augusto Borges Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_J/0/1/0/all/0/1\">Jorge Guevara Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadrozny_B/0/1/0/all/0/1\">Bianca Zadrozny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watson_C/0/1/0/all/0/1\">Campbell Watson</a>",
          "description": "One of the consequences of climate change is anobserved increase in the\nfrequency of extreme cli-mate events. That poses a challenge for\nweatherforecast and generation algorithms, which learnfrom historical data but\nshould embed an often un-certain bias to create correct scenarios. This\npaperinvestigates how mapping climate data to a knowndistribution using\nvariational autoencoders mighthelp explore such biases and control the\nsynthesisof weather fields towards more extreme climatescenarios. We\nexperimented using a monsoon-affected precipitation dataset from southwest\nIn-dia, which should give a roughly stable pattern ofrainy days and ease our\ninvestigation. We reportcompelling results showing that mapping complexweather\ndata to a known distribution implementsan efficient control for weather field\nsynthesis to-wards more (or less) extreme scenarios.",
          "link": "http://arxiv.org/abs/2108.00048",
          "publishedOn": "2021-08-03T02:06:30.145Z",
          "wordCount": 558,
          "title": "Controlling Weather Field Synthesis Using Variational Autoencoders. (arXiv:2108.00048v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gefei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yuling Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Can Yang</a>",
          "description": "We propose to learn a generative model via entropy interpolation with a\nSchr\\\"{o}dinger Bridge. The generative learning task can be formulated as\ninterpolating between a reference distribution and a target distribution based\non the Kullback-Leibler divergence. At the population level, this entropy\ninterpolation is characterized via an SDE on $[0,1]$ with a time-varying drift\nterm. At the sample level, we derive our Schr\\\"{o}dinger Bridge algorithm by\nplugging the drift term estimated by a deep score estimator and a deep density\nratio estimator into the Euler-Maruyama method. Under some mild smoothness\nassumptions of the target distribution, we prove the consistency of both the\nscore estimator and the density ratio estimator, and then establish the\nconsistency of the proposed Schr\\\"{o}dinger Bridge approach. Our theoretical\nresults guarantee that the distribution learned by our approach converges to\nthe target distribution. Experimental results on multimodal synthetic data and\nbenchmark data support our theoretical findings and indicate that the\ngenerative model via Schr\\\"{o}dinger Bridge is comparable with state-of-the-art\nGANs, suggesting a new formulation of generative learning. We demonstrate its\nusefulness in image interpolation and image inpainting.",
          "link": "http://arxiv.org/abs/2106.10410",
          "publishedOn": "2021-08-02T01:58:24.455Z",
          "wordCount": 648,
          "title": "Deep Generative Learning via Schr\\\"{o}dinger Bridge. (arXiv:2106.10410v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hepburn_A/0/1/0/all/0/1\">Alexander Hepburn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laparra_V/0/1/0/all/0/1\">Valero Laparra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_Rodriguez_R/0/1/0/all/0/1\">Raul Santos-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balle_J/0/1/0/all/0/1\">Johannes Ball&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malo_J/0/1/0/all/0/1\">Jes&#xfa;s Malo</a>",
          "description": "It has been demonstrated many times that the behavior of the human visual\nsystem is connected to the statistics of natural images. Since machine learning\nrelies on the statistics of training data as well, the above connection has\ninteresting implications when using perceptual distances (which mimic the\nbehavior of the human visual system) as a loss function. In this paper, we aim\nto unravel the non-trivial relationship between the probability distribution of\nthe data, perceptual distances, and unsupervised machine learning. To this end,\nwe show that perceptual sensitivity is correlated with the probability of an\nimage in its close neighborhood. We also explore the relation between distances\ninduced by autoencoders and the probability distribution of the data used for\ntraining them, as well as how these induced distances are correlated with human\nperception. Finally, we discuss why perceptual distances might not lead to\nnoticeable gains in performance over standard Euclidean distances in common\nimage processing tasks except when data is scarce and the perceptual distance\nprovides regularization.",
          "link": "http://arxiv.org/abs/2106.04427",
          "publishedOn": "2021-08-02T01:58:24.435Z",
          "wordCount": 649,
          "title": "On the relation between statistical learning and perceptual distances. (arXiv:2106.04427v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sirko_W/0/1/0/all/0/1\">Wojciech Sirko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashubin_S/0/1/0/all/0/1\">Sergii Kashubin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_M/0/1/0/all/0/1\">Marvin Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Annkah_A/0/1/0/all/0/1\">Abigail Annkah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouchareb_Y/0/1/0/all/0/1\">Yasser Salah Eddine Bouchareb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dauphin_Y/0/1/0/all/0/1\">Yann Dauphin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1\">Daniel Keysers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_M/0/1/0/all/0/1\">Maxim Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cisse_M/0/1/0/all/0/1\">Moustapha Cisse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quinn_J/0/1/0/all/0/1\">John Quinn</a>",
          "description": "Identifying the locations and footprints of buildings is vital for many\npractical and scientific purposes. Such information can be particularly useful\nin developing regions where alternative data sources may be scarce. In this\nwork, we describe a model training pipeline for detecting buildings across the\nentire continent of Africa, using 50 cm satellite imagery. Starting with the\nU-Net model, widely used in satellite image analysis, we study variations in\narchitecture, loss functions, regularization, pre-training, self-training and\npost-processing that increase instance segmentation performance. Experiments\nwere carried out using a dataset of 100k satellite images across Africa\ncontaining 1.75M manually labelled building instances, and further datasets for\npre-training and self-training. We report novel methods for improving\nperformance of building detection with this type of model, including the use of\nmixup (mAP +0.12) and self-training with soft KL loss (mAP +0.06). The\nresulting pipeline obtains good results even on a wide variety of challenging\nrural and urban contexts, and was used to create the Open Buildings dataset of\n516M Africa-wide detected footprints.",
          "link": "http://arxiv.org/abs/2107.12283",
          "publishedOn": "2021-08-02T01:58:24.389Z",
          "wordCount": 635,
          "title": "Continental-Scale Building Detection from High Resolution Satellite Imagery. (arXiv:2107.12283v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scholler_C/0/1/0/all/0/1\">Christoph Sch&#xf6;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a>",
          "description": "The future motion of traffic participants is inherently uncertain. To plan\nsafely, therefore, an autonomous agent must take into account multiple possible\ntrajectory outcomes and prioritize them. Recently, this problem has been\naddressed with generative neural networks. However, most generative models\neither do not learn the true underlying trajectory distribution reliably, or do\nnot allow predictions to be associated with likelihoods. In our work, we model\nmotion prediction directly as a density estimation problem with a normalizing\nflow between a noise distribution and the future motion distribution. Our\nmodel, named FloMo, allows likelihoods to be computed in a single network pass\nand can be trained directly with maximum likelihood estimation. Furthermore, we\npropose a method to stabilize training flows on trajectory datasets and a new\ndata augmentation transformation that improves the performance and\ngeneralization of our model. Our method achieves state-of-the-art performance\non three popular prediction datasets, with a significant gap to most competing\nmodels.",
          "link": "http://arxiv.org/abs/2103.03614",
          "publishedOn": "2021-08-02T01:58:24.339Z",
          "wordCount": 634,
          "title": "FloMo: Tractable Motion Prediction with Normalizing Flows. (arXiv:2103.03614v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14803",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Herbreteau_S/0/1/0/all/0/1\">S&#xe9;bastien Herbreteau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kervrann_C/0/1/0/all/0/1\">Charles Kervrann</a>",
          "description": "This work tackles the issue of noise removal from images, focusing on the\nwell-known DCT image denoising algorithm. The latter, stemming from signal\nprocessing, has been well studied over the years. Though very simple, it is\nstill used in crucial parts of state-of-the-art \"traditional\" denoising\nalgorithms such as BM3D. Since a few years however, deep convolutional neural\nnetworks (CNN) have outperformed their traditional counterparts, making signal\nprocessing methods less attractive. In this paper, we demonstrate that a DCT\ndenoiser can be seen as a shallow CNN and thereby its original linear transform\ncan be tuned through gradient descent in a supervised manner, improving\nconsiderably its performance. This gives birth to a fully interpretable CNN\ncalled DCT2net. To deal with remaining artifacts induced by DCT2net, an\noriginal hybrid solution between DCT and DCT2net is proposed combining the best\nthat these two methods can offer; DCT2net is selected to process non-stationary\nimage patches while DCT is optimal for piecewise smooth patches. Experiments on\nartificially noisy images demonstrate that two-layer DCT2net provides\ncomparable results to BM3D and is as fast as DnCNN algorithm composed of more\nthan a dozen of layers.",
          "link": "http://arxiv.org/abs/2107.14803",
          "publishedOn": "2021-08-02T01:58:24.331Z",
          "wordCount": 632,
          "title": "DCT2net: an interpretable shallow CNN for image denoising. (arXiv:2107.14803v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>",
          "description": "Recently, the Siamese-based method has stood out from multitudinous tracking\nmethods owing to its state-of-the-art (SOTA) performance. Nevertheless, due to\nvarious special challenges in UAV tracking, \\textit{e.g.}, severe occlusion and\nfast motion, most existing Siamese-based trackers hardly combine superior\nperformance with high efficiency. To this concern, in this paper, a novel\nattentional Siamese tracker (SiamAPN++) is proposed for real-time UAV tracking.\nBy virtue of the attention mechanism, we conduct a special attentional\naggregation network (AAN) consisting of self-AAN and cross-AAN for raising the\nrepresentation ability of features eventually. The former AAN aggregates and\nmodels the self-semantic interdependencies of the single feature map via\nspatial and channel dimensions. The latter aims to aggregate the\ncross-interdependencies of two different semantic features including the\nlocation information of anchors. In addition, the anchor proposal network based\non dual features is proposed to raise its robustness of tracking objects with\nvarious scales. Experiments on two well-known authoritative benchmarks are\nconducted, where SiamAPN++ outperforms its baseline SiamAPN and other SOTA\ntrackers. Besides, real-world tests onboard a typical embedded platform\ndemonstrate that SiamAPN++ achieves promising tracking results with real-time\nspeed.",
          "link": "http://arxiv.org/abs/2106.08816",
          "publishedOn": "2021-08-02T01:58:24.317Z",
          "wordCount": 662,
          "title": "SiamAPN++: Siamese Attentional Aggregation Network for Real-Time UAV Tracking. (arXiv:2106.08816v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Linxi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_DArpino_C/0/1/0/all/0/1\">Claudia P&#xe9;rez-D&#x27;Arpino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1\">Shyamal Buch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_L/0/1/0/all/0/1\">Lyne P. Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1\">Micael E. Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1\">Kent Vainio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Josiah Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>",
          "description": "We present iGibson 1.0, a novel simulation environment to develop robotic\nsolutions for interactive tasks in large-scale realistic scenes. Our\nenvironment contains 15 fully interactive home-sized scenes with 108 rooms\npopulated with rigid and articulated objects. The scenes are replicas of\nreal-world homes, with distribution and the layout of objects aligned to those\nof the real world. iGibson 1.0 integrates several key features to facilitate\nthe study of interactive tasks: i) generation of high-quality virtual sensor\nsignals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain\nrandomization to change the materials of the objects (both visual and physical)\nand/or their shapes, iii) integrated sampling-based motion planners to generate\ncollision-free trajectories for robot bases and arms, and iv) intuitive\nhuman-iGibson interface that enables efficient collection of human\ndemonstrations. Through experiments, we show that the full interactivity of the\nscenes enables agents to learn useful visual representations that accelerate\nthe training of downstream manipulation tasks. We also show that iGibson 1.0\nfeatures enable the generalization of navigation agents, and that the\nhuman-iGibson interface and integrated motion planners facilitate efficient\nimitation learning of human demonstrated (mobile) manipulation behaviors.\niGibson 1.0 is open-source, equipped with comprehensive examples and\ndocumentation. For more information, visit our project website:\nthis http URL",
          "link": "http://arxiv.org/abs/2012.02924",
          "publishedOn": "2021-08-02T01:58:24.292Z",
          "wordCount": 748,
          "title": "IGibson 1.0: a Simulation Environment for Interactive Tasks in Large Realistic Scenes. (arXiv:2012.02924v5 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.10706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chen Feng</a>",
          "description": "In the domain of visual tracking, most deep learning-based trackers highlight\nthe accuracy but casting aside efficiency. Therefore, their real-world\ndeployment on mobile platforms like the unmanned aerial vehicle (UAV) is\nimpeded. In this work, a novel two-stage Siamese network-based method is\nproposed for aerial tracking, \\textit{i.e.}, stage-1 for high-quality anchor\nproposal generation, stage-2 for refining the anchor proposal. Different from\nanchor-based methods with numerous pre-defined fixed-sized anchors, our\nno-prior method can 1) increase the robustness and generalization to different\nobjects with various sizes, especially to small, occluded, and fast-moving\nobjects, under complex scenarios in light of the adaptive anchor generation, 2)\nmake calculation feasible due to the substantial decrease of anchor numbers. In\naddition, compared to anchor-free methods, our framework has better performance\nowing to refinement at stage-2. Comprehensive experiments on three benchmarks\nhave proven the superior performance of our approach, with a speed of around\n200 frames/s.",
          "link": "http://arxiv.org/abs/2012.10706",
          "publishedOn": "2021-08-02T01:58:24.273Z",
          "wordCount": 643,
          "title": "Siamese Anchor Proposal Network for High-Speed Aerial Tracking. (arXiv:2012.10706v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1\">Congcong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chen Feng</a>",
          "description": "LiDAR point clouds collected from a moving vehicle are functions of its\ntrajectories, because the sensor motion needs to be compensated to avoid\ndistortions. When autonomous vehicles are sending LiDAR point clouds to deep\nnetworks for perception and planning, could the motion compensation\nconsequently become a wide-open backdoor in those networks, due to both the\nadversarial vulnerability of deep learning and GPS-based vehicle trajectory\nestimation that is susceptible to wireless spoofing? We demonstrate such\npossibilities for the first time: instead of directly attacking point cloud\ncoordinates which requires tampering with the raw LiDAR readings, only\nadversarial spoofing of a self-driving car's trajectory with small\nperturbations is enough to make safety-critical objects undetectable or\ndetected with incorrect positions. Moreover, polynomial trajectory perturbation\nis developed to achieve a temporally-smooth and highly-imperceptible attack.\nExtensive experiments on 3D object detection have shown that such attacks not\nonly lower the performance of the state-of-the-art detectors effectively, but\nalso transfer to other detectors, raising a red flag for the community. The\ncode is available on https://ai4ce.github.io/FLAT/.",
          "link": "http://arxiv.org/abs/2103.15326",
          "publishedOn": "2021-08-02T01:58:24.248Z",
          "wordCount": 645,
          "title": "Fooling LiDAR Perception via Adversarial Trajectory Perturbation. (arXiv:2103.15326v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zijian Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>",
          "description": "In this paper, we propose MINE to perform novel view synthesis and depth\nestimation via dense 3D reconstruction from a single image. Our approach is a\ncontinuous depth generalization of the Multiplane Images (MPI) by introducing\nthe NEural radiance fields (NeRF). Given a single image as input, MINE predicts\na 4-channel image (RGB and volume density) at arbitrary depth values to jointly\nreconstruct the camera frustum and fill in occluded contents. The reconstructed\nand inpainted frustum can then be easily rendered into novel RGB or depth views\nusing differentiable rendering. Extensive experiments on RealEstate10K, KITTI\nand Flowers Light Fields show that our MINE outperforms state-of-the-art by a\nlarge margin in novel view synthesis. We also achieve competitive results in\ndepth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our\nsource code is available at https://github.com/vincentfung13/MINE",
          "link": "http://arxiv.org/abs/2103.14910",
          "publishedOn": "2021-08-02T01:58:24.240Z",
          "wordCount": 639,
          "title": "MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis. (arXiv:2103.14910v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15687",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gerg_I/0/1/0/all/0/1\">Isaac Gerg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Monga_V/0/1/0/all/0/1\">Vishal Monga</a>",
          "description": "Synthetic aperture sonar (SAS) requires precise positional and environmental\ninformation to produce well-focused output during the image reconstruction\nstep. However, errors in these measurements are commonly present resulting in\ndefocused imagery. To overcome these issues, an \\emph{autofocus} algorithm is\nemployed as a post-processing step after image reconstruction for the purpose\nof improving image quality using the image content itself. These algorithms are\nusually iterative and metric-based in that they seek to optimize an image\nsharpness metric. In this letter, we demonstrate the potential of machine\nlearning, specifically deep learning, to address the autofocus problem. We\nformulate the problem as a self-supervised, phase error estimation task using a\ndeep network we call Deep Autofocus. Our formulation has the advantages of\nbeing non-iterative (and thus fast) and not requiring ground truth\nfocused-defocused images pairs as often required by other deblurring deep\nlearning methods. We compare our technique against a set of common sharpness\nmetrics optimized using gradient descent over a real-world dataset. Our results\ndemonstrate Deep Autofocus can produce imagery that is perceptually as good as\nbenchmark iterative techniques but at a substantially lower computational cost.\nWe conclude that our proposed Deep Autofocus can provide a more favorable\ncost-quality trade-off than state-of-the-art alternatives with significant\npotential of future research.",
          "link": "http://arxiv.org/abs/2010.15687",
          "publishedOn": "2021-08-02T01:58:24.219Z",
          "wordCount": 670,
          "title": "Deep Autofocus for Synthetic Aperture Sonar. (arXiv:2010.15687v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haizhou Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youcai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenjie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>",
          "description": "It is a consensus that small models perform quite poorly under the paradigm\nof self-supervised contrastive learning. Existing methods usually adopt a large\noff-the-shelf model to transfer knowledge to the small one via knowledge\ndistillation. Despite their effectiveness, distillation-based methods may not\nbe suitable for some resource-restricted scenarios due to the huge\ncomputational expenses of deploying a large model. In this paper, we study the\nissue of training self-supervised small models without distillation signals. We\nfirst evaluate the representation spaces of the small models and make two\nnon-negligible observations: (i) small models can complete the pretext task\nwithout overfitting despite its limited capacity; (ii) small models universally\nsuffer the problem of over-clustering. Then we verify multiple assumptions that\nare considered to alleviate the over-clustering phenomenon. Finally, we combine\nthe validated techniques and improve the baseline of five small architectures\nwith considerable margins, which indicates that training small self-supervised\ncontrastive models is feasible even without distillation signals.",
          "link": "http://arxiv.org/abs/2107.14762",
          "publishedOn": "2021-08-02T01:58:24.206Z",
          "wordCount": 610,
          "title": "On the Efficacy of Small Self-Supervised Contrastive Models without Distillation Signals. (arXiv:2107.14762v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1\">Jean-Baptiste Alayrac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doersch_C/0/1/0/all/0/1\">Carl Doersch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_C/0/1/0/all/0/1\">Catalin Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">David Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1\">Skanda Koppula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andrew Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1\">Evan Shelhamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1\">Olivier H&#xe9;naff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1\">Matthew M. Botvinick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Jo&#xe3;o Carreira</a>",
          "description": "The recently-proposed Perceiver model obtains good results on several domains\n(images, audio, multimodal, point clouds) while scaling linearly in compute and\nmemory with the input size. While the Perceiver supports many kinds of inputs,\nit can only produce very simple outputs such as class scores. Perceiver IO\novercomes this limitation without sacrificing the original's appealing\nproperties by learning to flexibly query the model's latent space to produce\noutputs of arbitrary size and semantics. Perceiver IO still decouples model\ndepth from data size and still scales linearly with data size, but now with\nrespect to both input and output sizes. The full Perceiver IO model achieves\nstrong results on tasks with highly structured output spaces, such as natural\nlanguage and visual understanding, StarCraft II, and multi-task and multi-modal\ndomains. As highlights, Perceiver IO matches a Transformer-based BERT baseline\non the GLUE language benchmark without the need for input tokenization and\nachieves state-of-the-art performance on Sintel optical flow estimation.",
          "link": "http://arxiv.org/abs/2107.14795",
          "publishedOn": "2021-08-02T01:58:24.128Z",
          "wordCount": 639,
          "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs. (arXiv:2107.14795v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kamnitsas_K/0/1/0/all/0/1\">Konstantinos Kamnitsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winzeck_S/0/1/0/all/0/1\">Stefan Winzeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornaropoulos_E/0/1/0/all/0/1\">Evgenios N. Kornaropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitehouse_D/0/1/0/all/0/1\">Daniel Whitehouse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Englman_C/0/1/0/all/0/1\">Cameron Englman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phyu_P/0/1/0/all/0/1\">Poe Phyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pao_N/0/1/0/all/0/1\">Norman Pao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_D/0/1/0/all/0/1\">David K. Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_T/0/1/0/all/0/1\">Tilak Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newcombe_V/0/1/0/all/0/1\">Virginia F.J. Newcombe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>",
          "description": "Semi-supervised learning (SSL) uses unlabeled data during training to learn\nbetter models. Previous studies on SSL for medical image segmentation focused\nmostly on improving model generalization to unseen data. In some applications,\nhowever, our primary interest is not generalization but to obtain optimal\npredictions on a specific unlabeled database that is fully available during\nmodel development. Examples include population studies for extracting imaging\nphenotypes. This work investigates an often overlooked aspect of SSL,\ntransduction. It focuses on the quality of predictions made on the unlabeled\ndata of interest when they are included for optimization during training,\nrather than improving generalization. We focus on the self-training framework\nand explore its potential for transduction. We analyze it through the lens of\nInformation Gain and reveal that learning benefits from the use of calibrated\nor under-confident models. Our extensive experiments on a large MRI database\nfor multi-class segmentation of traumatic brain lesions shows promising results\nwhen comparing transductive with inductive predictions. We believe this study\nwill inspire further research on transductive learning, a well-suited paradigm\nfor medical image analysis.",
          "link": "http://arxiv.org/abs/2107.08964",
          "publishedOn": "2021-08-02T01:58:24.048Z",
          "wordCount": 674,
          "title": "Transductive image segmentation: Self-training and effect of uncertainty estimation. (arXiv:2107.08964v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>",
          "description": "To improve the viewer's Quality of Experience (QoE) and optimize computer\ngraphics applications, 3D model quality assessment (3D-QA) has become an\nimportant task in the multimedia area. Point cloud and mesh are the two most\nwidely used digital representation formats of 3D models, the visual quality of\nwhich is quite sensitive to lossy operations like simplification and\ncompression. Therefore, many related studies such as point cloud quality\nassessment (PCQA) and mesh quality assessment (MQA) have been carried out to\nmeasure the caused visual quality degradations. However, a large part of\nprevious studies utilizes full-reference (FR) metrics, which means they may\nfail to predict the quality level with the absence of the reference 3D model.\nFurthermore, few 3D-QA metrics are carried out to consider color information,\nwhich significantly restricts the effectiveness and scope of application. In\nthis paper, we propose a no-reference (NR) quality assessment metric for\ncolored 3D models represented by both point cloud and mesh. First, we project\nthe 3D models from 3D space into quality-related geometry and color feature\ndomains. Then, the natural scene statistics (NSS) and entropy are utilized to\nextract quality-aware features. Finally, the Support Vector Regressor (SVR) is\nemployed to regress the quality-aware features into quality scores. Our method\nis mainly validated on the colored point cloud quality assessment database\n(SJTU-PCQA) and the colored mesh quality assessment database (CMDM). The\nexperimental results show that the proposed method outperforms all the\nstate-of-art NR 3D-QA metrics and obtains an acceptable gap with the\nstate-of-art FR 3D-QA metrics.",
          "link": "http://arxiv.org/abs/2107.02041",
          "publishedOn": "2021-08-02T01:58:24.001Z",
          "wordCount": 741,
          "title": "No-Reference Quality Assessment for 3D Colored Point Cloud and Mesh Models. (arXiv:2107.02041v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Liangjian Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lili Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>",
          "description": "The goal of few-shot classification is to classify new categories with few\nlabeled examples within each class. Nowadays, the excellent performance in\nhandling few-shot classification problems is shown by metric-based\nmeta-learning methods. However, it is very hard for previous methods to\ndiscriminate the fine-grained sub-categories in the embedding space without\nfine-grained labels. This may lead to unsatisfactory generalization to\nfine-grained subcategories, and thus affects model interpretation. To tackle\nthis problem, we introduce the contrastive loss into few-shot classification\nfor learning latent fine-grained structure in the embedding space. Furthermore,\nto overcome the drawbacks of random image transformation used in current\ncontrastive learning in producing noisy and inaccurate image pairs (i.e.,\nviews), we develop a learning-to-learn algorithm to automatically generate\ndifferent views of the same image. Extensive experiments on standard few-shot\nlearning benchmarks demonstrate the superiority of our method.",
          "link": "http://arxiv.org/abs/2107.09242",
          "publishedOn": "2021-08-02T01:58:23.993Z",
          "wordCount": 595,
          "title": "Boosting Few-Shot Classification with View-Learnable Contrastive Learning. (arXiv:2107.09242v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1\">Matteo Stefanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1\">Silvia Cascianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Connecting Vision and Language plays an essential role in Generative\nIntelligence. For this reason, large research efforts have been devoted to\nimage captioning, i.e. describing images with syntactically and semantically\nmeaningful sentences. Starting from 2015 the task has generally been addressed\nwith pipelines composed of a visual encoder and a language model for text\ngeneration. During these years, both components have evolved considerably\nthrough the exploitation of object regions, attributes, the introduction of\nmulti-modal connections, fully-attentive approaches, and BERT-like early-fusion\nstrategies. However, regardless of the impressive results, research in image\ncaptioning has not reached a conclusive answer yet. This work aims at providing\na comprehensive overview of image captioning approaches, from visual encoding\nand text generation to training strategies, datasets, and evaluation metrics.\nIn this respect, we quantitatively compare many relevant state-of-the-art\napproaches to identify the most impactful technical innovations in\narchitectures and training strategies. Moreover, many variants of the problem\nand its open challenges are discussed. The final goal of this work is to serve\nas a tool for understanding the existing literature and highlighting the future\ndirections for a research area where Computer Vision and Natural Language\nProcessing can find an optimal synergy.",
          "link": "http://arxiv.org/abs/2107.06912",
          "publishedOn": "2021-08-02T01:58:23.964Z",
          "wordCount": 659,
          "title": "From Show to Tell: A Survey on Image Captioning. (arXiv:2107.06912v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guowen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Han Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shangwei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Run Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>",
          "description": "This paper presents a novel fingerprinting scheme for the Intellectual\nProperty (IP) protection of Generative Adversarial Networks (GANs). Prior\nsolutions for classification models adopt adversarial examples as the\nfingerprints, which can raise stealthiness and robustness problems when they\nare applied to the GAN models. Our scheme constructs a composite deep learning\nmodel from the target GAN and a classifier. Then we generate stealthy\nfingerprint samples from this composite model, and register them to the\nclassifier for effective ownership verification. This scheme inspires three\nconcrete methodologies to practically protect the modern GAN models.\nTheoretical analysis proves that these methods can satisfy different security\nrequirements necessary for IP protection. We also conduct extensive experiments\nto show that our solutions outperform existing strategies in terms of\nstealthiness, functionality-preserving and unremovability.",
          "link": "http://arxiv.org/abs/2106.11760",
          "publishedOn": "2021-08-02T01:58:23.958Z",
          "wordCount": 609,
          "title": "A Novel Verifiable Fingerprinting Scheme for Generative Adversarial Networks. (arXiv:2106.11760v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.09193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sekuboyina_A/0/1/0/all/0/1\">Anjany Sekuboyina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Husseini_M/0/1/0/all/0/1\">Malek E. Husseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayat_A/0/1/0/all/0/1\">Amirhossein Bayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loffler_M/0/1/0/all/0/1\">Maximilian L&#xf6;ffler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebl_H/0/1/0/all/0/1\">Hans Liebl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tetteh_G/0/1/0/all/0/1\">Giles Tetteh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukacka_J/0/1/0/all/0/1\">Jan Kuka&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Payer_C/0/1/0/all/0/1\">Christian Payer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stern_D/0/1/0/all/0/1\">Darko &#x160;tern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urschler_M/0/1/0/all/0/1\">Martin Urschler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maodong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">Dalong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lessmann_N/0/1/0/all/0/1\">Nikolas Lessmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yujin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianfu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambellan_F/0/1/0/all/0/1\">Felix Ambellan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiranashvili_T/0/1/0/all/0/1\">Tamaz Amiranashvili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehlke_M/0/1/0/all/0/1\">Moritz Ehlke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamecker_H/0/1/0/all/0/1\">Hans Lamecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehnert_S/0/1/0/all/0/1\">Sebastian Lehnert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lirio_M/0/1/0/all/0/1\">Marilia Lirio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olaguer_N/0/1/0/all/0/1\">Nicol&#xe1;s P&#xe9;rez de Olaguer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramm_H/0/1/0/all/0/1\">Heiko Ramm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_M/0/1/0/all/0/1\">Manish Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tack_A/0/1/0/all/0/1\">Alexander Tack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zachow_S/0/1/0/all/0/1\">Stefan Zachow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinjun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angerman_C/0/1/0/all/0/1\">Christoph Angerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_K/0/1/0/all/0/1\">Kevin Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirszenberg_A/0/1/0/all/0/1\">Alexandre Kirszenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puybareau_E/0/1/0/all/0/1\">&#xc9;lodie Puybareau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Di Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yiwei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rapazzo_B/0/1/0/all/0/1\">Brandon H. Rapazzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeah_T/0/1/0/all/0/1\">Timyoas Yeah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Amber Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shangliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1\">Feng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiqiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiangshang_Z/0/1/0/all/0/1\">Zheng Xiangshang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liming_X/0/1/0/all/0/1\">Xu Liming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netherton_T/0/1/0/all/0/1\">Tucker J. Netherton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mumme_R/0/1/0/all/0/1\">Raymond P. Mumme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Court_L/0/1/0/all/0/1\">Laurence E. Court</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixun Huang</a>, et al. (18 additional authors not shown)",
          "description": "Vertebral labelling and segmentation are two fundamental tasks in an\nautomated spine processing pipeline. Reliable and accurate processing of spine\nimages is expected to benefit clinical decision-support systems for diagnosis,\nsurgery planning, and population-based analysis on spine and bone health.\nHowever, designing automated algorithms for spine processing is challenging\npredominantly due to considerable variations in anatomy and acquisition\nprotocols and due to a severe shortage of publicly available data. Addressing\nthese limitations, the Large Scale Vertebrae Segmentation Challenge (VerSe) was\norganised in conjunction with the International Conference on Medical Image\nComputing and Computer Assisted Intervention (MICCAI) in 2019 and 2020, with a\ncall for algorithms towards labelling and segmentation of vertebrae. Two\ndatasets containing a total of 374 multi-detector CT scans from 355 patients\nwere prepared and 4505 vertebrae have individually been annotated at\nvoxel-level by a human-machine hybrid algorithm (https://osf.io/nqjyw/,\nhttps://osf.io/t98fz/). A total of 25 algorithms were benchmarked on these\ndatasets. In this work, we present the the results of this evaluation and\nfurther investigate the performance-variation at vertebra-level, scan-level,\nand at different fields-of-view. We also evaluate the generalisability of the\napproaches to an implicit domain shift in data by evaluating the top performing\nalgorithms of one challenge iteration on data from the other iteration. The\nprincipal takeaway from VerSe: the performance of an algorithm in labelling and\nsegmenting a spine scan hinges on its ability to correctly identify vertebrae\nin cases of rare anatomical variations. The content and code concerning VerSe\ncan be accessed at: https://github.com/anjany/verse.",
          "link": "http://arxiv.org/abs/2001.09193",
          "publishedOn": "2021-08-02T01:58:23.945Z",
          "wordCount": 933,
          "title": "VerSe: A Vertebrae Labelling and Segmentation Benchmark for Multi-detector CT Images. (arXiv:2001.09193v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhishan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guohui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Dawei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>",
          "description": "As a critical component for online advertising and marking, click-through\nrate (CTR) prediction has draw lots of attentions from both industry and\nacademia field. Recently, the deep learning has become the mainstream\nmethodological choice for CTR. Despite of sustainable efforts have been made,\nexisting approaches still pose several challenges. On the one hand, high-order\ninteraction between the features is under-explored. On the other hand,\nhigh-order interactions may neglect the semantic information from the low-order\nfields. In this paper, we proposed a novel prediction method, named FINT, that\nemploys the Field-aware INTeraction layer which captures high-order feature\ninteractions while retaining the low-order field information. To empirically\ninvestigate the effectiveness and robustness of the FINT, we perform extensive\nexperiments on the three realistic databases: KDD2012, Criteo and Avazu. The\nobtained results demonstrate that the FINT can significantly improve the\nperformance compared to the existing methods, without increasing the amount of\ncomputation required. Moreover, the proposed method brought about 2.72\\%\nincrease to the advertising revenue of a big online video app through A/B\ntesting. To better promote the research in CTR field, we released our code as\nwell as reference implementation at: https://github.com/zhishan01/FINT.",
          "link": "http://arxiv.org/abs/2107.01999",
          "publishedOn": "2021-08-02T01:58:23.925Z",
          "wordCount": 654,
          "title": "FINT: Field-aware INTeraction Neural Network For CTR Prediction. (arXiv:2107.01999v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bengar_J/0/1/0/all/0/1\">Javad Zolfaghari Bengar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raducanu_B/0/1/0/all/0/1\">Bogdan Raducanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>",
          "description": "Active learning aims to select samples to be annotated that yield the largest\nperformance improvement for the learning algorithm. Many methods approach this\nproblem by measuring the informativeness of samples and do this based on the\ncertainty of the network predictions for samples. However, it is well-known\nthat neural networks are overly confident about their prediction and are\ntherefore an untrustworthy source to assess sample informativeness. In this\npaper, we propose a new informativeness-based active learning method. Our\nmeasure is derived from the learning dynamics of a neural network. More\nprecisely we track the label assignment of the unlabeled data pool during the\ntraining of the algorithm. We capture the learning dynamics with a metric\ncalled label-dispersion, which is low when the network consistently assigns the\nsame label to the sample during the training of the network and high when the\nassigned label changes frequently. We show that label-dispersion is a promising\npredictor of the uncertainty of the network, and show on two benchmark datasets\nthat an active learning algorithm based on label-dispersion obtains excellent\nresults.",
          "link": "http://arxiv.org/abs/2107.14707",
          "publishedOn": "2021-08-02T01:58:23.917Z",
          "wordCount": 635,
          "title": "When Deep Learners Change Their Mind: Learning Dynamics for Active Learning. (arXiv:2107.14707v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Duo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>",
          "description": "Domain adaptation is critical for success when confronting with the lack of\nannotations in a new domain. As the huge time consumption of labeling process\non 3D point cloud, domain adaptation for 3D semantic segmentation is of great\nexpectation. With the rise of multi-modal datasets, large amount of 2D images\nare accessible besides 3D point clouds. In light of this, we propose to further\nleverage 2D data for 3D domain adaptation by intra and inter domain cross modal\nlearning. As for intra-domain cross modal learning, most existing works sample\nthe dense 2D pixel-wise features into the same size with sparse 3D point-wise\nfeatures, resulting in the abandon of numerous useful 2D features. To address\nthis problem, we propose Dynamic sparse-to-dense Cross Modal Learning (DsCML)\nto increase the sufficiency of multi-modality information interaction for\ndomain adaptation. For inter-domain cross modal learning, we further advance\nCross Modal Adversarial Learning (CMAL) on 2D and 3D data which contains\ndifferent semantic content aiming to promote high-level modal complementarity.\nWe evaluate our model under various multi-modality domain adaptation settings\nincluding day-to-night, country-to-country and dataset-to-dataset, brings large\nimprovements over both uni-modal and multi-modal domain adaptation methods on\nall settings.",
          "link": "http://arxiv.org/abs/2107.14724",
          "publishedOn": "2021-08-02T01:58:23.910Z",
          "wordCount": 652,
          "title": "Sparse-to-dense Feature Matching: Intra and Inter domain Cross-modal Learning in Domain Adaptation for 3D Semantic Segmentation. (arXiv:2107.14724v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sako_T/0/1/0/all/0/1\">Tomas Sako</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_A/0/1/0/all/0/1\">Arturo Jr M. Martinez</a>",
          "description": "Since the United Nations launched the Sustainable Development Goals (SDG) in\n2015, numerous universities, NGOs and other organizations have attempted to\ndevelop tools for monitoring worldwide progress in achieving them. Led by\nadvancements in the fields of earth observation techniques, data sciences and\nthe emergence of artificial intelligence, a number of research teams have\ndeveloped innovative tools for highlighting areas of vulnerability and tracking\nthe implementation of SDG targets. In this paper we demonstrate that\nindividuals with no organizational affiliation and equipped only with common\nhardware, publicly available datasets and cloud-based computing services can\nparticipate in the improvement of predicting machine-learning-based approaches\nto predicting local poverty levels in a given agro-ecological environment. The\napproach builds upon several pioneering efforts over the last five years\nrelated to mapping poverty by deep learning to process satellite imagery and\n\"ground-truth\" data from the field to link features with incidence of poverty\nin a particular context. The approach employs new methods for object\nidentification in order to optimize the modeled results and achieve\nsignificantly high accuracy. A key goal of the project was to intentionally\nkeep costs as low as possible - by using freely available resources - so that\ncitizen scientists, students and organizations could replicate the method in\nother areas of interest. Moreover, for simplicity, the input data used were\nderived from just a handful of sources (involving only earth observation and\npopulation headcounts). The results of the project could therefore certainly be\nstrengthened further through the integration of proprietary data from social\nnetworks, mobile phone providers, and other sources.",
          "link": "http://arxiv.org/abs/2107.14700",
          "publishedOn": "2021-08-02T01:58:23.903Z",
          "wordCount": 697,
          "title": "Seeing poverty from space, how much can it be tuned?. (arXiv:2107.14700v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_S/0/1/0/all/0/1\">Sakshi Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1\">Vinay Kumar Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1\">Srijith P K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1\">Piyush Rai</a>",
          "description": "We present a continual learning approach for generative adversarial networks\n(GANs), by designing and leveraging parameter-efficient feature map\ntransformations. Our approach is based on learning a set of global and\ntask-specific parameters. The global parameters are fixed across tasks whereas\nthe task-specific parameters act as local adapters for each task, and help in\nefficiently obtaining task-specific feature maps. Moreover, we propose an\nelement-wise addition of residual bias in the transformed feature space, which\nfurther helps stabilize GAN training in such settings. Our approach also\nleverages task similarity information based on the Fisher information matrix.\nLeveraging this knowledge from previous tasks significantly improves the model\nperformance. In addition, the similarity measure also helps reduce the\nparameter growth in continual adaptation and helps to learn a compact model. In\ncontrast to the recent approaches for continually-learned GANs, the proposed\napproach provides a memory-efficient way to perform effective continual data\ngeneration. Through extensive experiments on challenging and diverse datasets,\nwe show that the feature-map-transformation approach outperforms\nstate-of-the-art methods for continually-learned GANs, with substantially fewer\nparameters. The proposed method generates high-quality samples that can also\nimprove the generative-replay-based continual learning for discriminative\ntasks.",
          "link": "http://arxiv.org/abs/2103.04032",
          "publishedOn": "2021-08-02T01:58:23.897Z",
          "wordCount": 667,
          "title": "CAM-GAN: Continual Adaptation Modules for Generative Adversarial Networks. (arXiv:2103.04032v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Budd_S/0/1/0/all/0/1\">Samuel Budd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Day_T/0/1/0/all/0/1\">Thomas Day</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simpson_J/0/1/0/all/0/1\">John Simpson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lloyd_K/0/1/0/all/0/1\">Karen Lloyd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthew_J/0/1/0/all/0/1\">Jacqueline Matthew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skelton_E/0/1/0/all/0/1\">Emily Skelton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_R/0/1/0/all/0/1\">Reza Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>",
          "description": "Probably yes. -- Supervised Deep Learning dominates performance scores for\nmany computer vision tasks and defines the state-of-the-art. However, medical\nimage analysis lags behind natural image applications. One of the many reasons\nis the lack of well annotated medical image data available to researchers. One\nof the first things researchers are told is that we require significant\nexpertise to reliably and accurately interpret and label such data. We see\nsignificant inter- and intra-observer variability between expert annotations of\nmedical images. Still, it is a widely held assumption that novice annotators\nare unable to provide useful annotations for use by clinical Deep Learning\nmodels. In this work we challenge this assumption and examine the implications\nof using a minimally trained novice labelling workforce to acquire annotations\nfor a complex medical image dataset. We study the time and cost implications of\nusing novice annotators, the raw performance of novice annotators compared to\ngold-standard expert annotators, and the downstream effects on a trained Deep\nLearning segmentation model's performance for detecting a specific congenital\nheart disease (hypoplastic left heart syndrome) in fetal ultrasound imaging.",
          "link": "http://arxiv.org/abs/2107.14682",
          "publishedOn": "2021-08-02T01:58:23.880Z",
          "wordCount": 647,
          "title": "Can non-specialists provide high quality gold standard labels in challenging modalities?. (arXiv:2107.14682v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gnecco_L/0/1/0/all/0/1\">Lucas Gnecco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boria_N/0/1/0/all/0/1\">Nicolas Boria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bougleux_S/0/1/0/all/0/1\">S&#xe9;bastien Bougleux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yger_F/0/1/0/all/0/1\">Florian Yger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blumenthal_D/0/1/0/all/0/1\">David B. Blumenthal</a>",
          "description": "The inference of minimum spanning arborescences within a set of objects is a\ngeneral problem which translates into numerous application-specific\nunsupervised learning tasks. We introduce a unified and generic structure\ncalled edit arborescence that relies on edit paths between data in a\ncollection, as well as the Min Edit Arborescence Problem, which asks for an\nedit arborescence that minimizes the sum of costs of its inner edit paths.\nThrough the use of suitable cost functions, this generic framework allows to\nmodel a variety of problems. In particular, we show that by introducing\nencoding size preserving edit costs, it can be used as an efficient method for\ncompressing collections of labeled graphs. Experiments on various graph\ndatasets, with comparisons to standard compression tools, show the potential of\nour method.",
          "link": "http://arxiv.org/abs/2107.14525",
          "publishedOn": "2021-08-02T01:58:23.872Z",
          "wordCount": 585,
          "title": "The Minimum Edit Arborescence Problem and Its Use in Compressing Graph Collections [Extended Version]. (arXiv:2107.14525v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poliarnyi_N/0/1/0/all/0/1\">Nikolai Poliarnyi</a>",
          "description": "We present an out-of-core variational approach for surface reconstruction\nfrom a set of aligned depth maps. Input depth maps are supposed to be\nreconstructed from regular photos or/and can be a representation of terrestrial\nLIDAR point clouds. Our approach is based on surface reconstruction via total\ngeneralized variation minimization ($TGV$) because of its strong\nvisibility-based noise-filtering properties and GPU-friendliness. Our main\ncontribution is an out-of-core OpenCL-accelerated adaptation of this numerical\nalgorithm which can handle arbitrarily large real-world scenes with scale\ndiversity.",
          "link": "http://arxiv.org/abs/2107.14790",
          "publishedOn": "2021-08-02T01:58:23.865Z",
          "wordCount": 535,
          "title": "Out-of-Core Surface Reconstruction via Global $TGV$ Minimization. (arXiv:2107.14790v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soutif__Cormerais_A/0/1/0/all/0/1\">Albin Soutif--Cormerais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masana_M/0/1/0/all/0/1\">Marc Masana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost Van de Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1\">Bart&#x142;omiej Twardowski</a>",
          "description": "In class-incremental learning, an agent with limited resources needs to learn\na sequence of classification tasks, forming an ever growing classification\nproblem, with the constraint of not being able to access data from previous\ntasks. The main difference with task-incremental learning, where a task-ID is\navailable at inference time, is that the learner also needs to perform\ncross-task discrimination, i.e. distinguish between classes that have not been\nseen together. Approaches to tackle this problem are numerous and mostly make\nuse of an external memory (buffer) of non-negligible size. In this paper, we\nablate the learning of cross-task features and study its influence on the\nperformance of basic replay strategies used for class-IL. We also define a new\nforgetting measure for class-incremental learning, and see that forgetting is\nnot the principal cause of low performance. Our experimental results show that\nfuture algorithms for class-incremental learning should not only prevent\nforgetting, but also aim to improve the quality of the cross-task features, and\nthe knowledge transfer between tasks. This is especially important when tasks\ncontain limited amount of data.",
          "link": "http://arxiv.org/abs/2106.11930",
          "publishedOn": "2021-08-02T01:58:23.859Z",
          "wordCount": 651,
          "title": "On the importance of cross-task features for class-incremental learning. (arXiv:2106.11930v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_V/0/1/0/all/0/1\">Vineet Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cech_J/0/1/0/all/0/1\">Jan Cech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horaud_R/0/1/0/all/0/1\">Radu Horaud</a>",
          "description": "The combination of range sensors with color cameras can be very useful for\nrobot navigation, semantic perception, manipulation, and telepresence. Several\nmethods of combining range- and color-data have been investigated and\nsuccessfully used in various robotic applications. Most of these systems suffer\nfrom the problems of noise in the range-data and resolution mismatch between\nthe range sensor and the color cameras, since the resolution of current range\nsensors is much less than the resolution of color cameras. High-resolution\ndepth maps can be obtained using stereo matching, but this often fails to\nconstruct accurate depth maps of weakly/repetitively textured scenes, or if the\nscene exhibits complex self-occlusions. Range sensors provide coarse depth\ninformation regardless of presence/absence of texture. The use of a calibrated\nsystem, composed of a time-of-flight (TOF) camera and of a stereoscopic camera\npair, allows data fusion thus overcoming the weaknesses of both individual\nsensors. We propose a novel TOF-stereo fusion method based on an efficient\nseed-growing algorithm which uses the TOF data projected onto the stereo image\npair as an initial set of correspondences. These initial \"seeds\" are then\npropagated based on a Bayesian model which combines an image similarity score\nwith rough depth priors computed from the low-resolution range data. The\noverall result is a dense and accurate depth map at the resolution of the color\ncameras at hand. We show that the proposed algorithm outperforms 2D image-based\nstereo algorithms and that the results are of higher resolution than\noff-the-shelf color-range sensors, e.g., Kinect. Moreover, the algorithm\npotentially exhibits real-time performance on a single CPU.",
          "link": "http://arxiv.org/abs/2107.14688",
          "publishedOn": "2021-08-02T01:58:23.851Z",
          "wordCount": 701,
          "title": "High-Resolution Depth Maps Based on TOF-Stereo Fusion. (arXiv:2107.14688v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1911.12377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Landi_F/0/1/0/all/0/1\">Federico Landi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corsini_M/0/1/0/all/0/1\">Massimiliano Corsini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Vision-and-Language Navigation (VLN) is a challenging task in which an agent\nneeds to follow a language-specified path to reach a target destination. The\ngoal gets even harder as the actions available to the agent get simpler and\nmove towards low-level, atomic interactions with the environment. This setting\ntakes the name of low-level VLN. In this paper, we strive for the creation of\nan agent able to tackle three key issues: multi-modality, long-term\ndependencies, and adaptability towards different locomotive settings. To that\nend, we devise \"Perceive, Transform, and Act\" (PTA): a fully-attentive VLN\narchitecture that leaves the recurrent approach behind and the first\nTransformer-like architecture incorporating three different modalities -\nnatural language, images, and low-level actions for the agent control. In\nparticular, we adopt an early fusion strategy to merge lingual and visual\ninformation efficiently in our encoder. We then propose to refine the decoding\nphase with a late fusion extension between the agent's history of actions and\nthe perceptual modalities. We experimentally validate our model on two\ndatasets: PTA achieves promising results in low-level VLN on R2R and achieves\ngood performance in the recently proposed R4R benchmark. Our code is publicly\navailable at https://github.com/aimagelab/perceive-transform-and-act.",
          "link": "http://arxiv.org/abs/1911.12377",
          "publishedOn": "2021-08-02T01:58:23.845Z",
          "wordCount": 687,
          "title": "Multimodal Attention Networks for Low-Level Vision-and-Language Navigation. (arXiv:1911.12377v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14599",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hengli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1\">Peide Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "Freespace detection is a fundamental component of autonomous driving\nperception. Recently, deep convolutional neural networks (DCNNs) have achieved\nimpressive performance for this task. In particular, SNE-RoadSeg, our\npreviously proposed method based on a surface normal estimator (SNE) and a\ndata-fusion DCNN (RoadSeg), has achieved impressive performance in freespace\ndetection. However, SNE-RoadSeg is computationally intensive, and it is\ndifficult to execute in real time. To address this problem, we introduce\nSNE-RoadSeg+, an upgraded version of SNE-RoadSeg. SNE-RoadSeg+ consists of 1)\nSNE+, a module for more accurate surface normal estimation, and 2) RoadSeg+, a\ndata-fusion DCNN that can greatly minimize the trade-off between accuracy and\nefficiency with the use of deep supervision. Extensive experimental results\nhave demonstrated the effectiveness of our SNE+ for surface normal estimation\nand the superior performance of our SNE-RoadSeg+ over all other freespace\ndetection approaches. Specifically, our SNE-RoadSeg+ runs in real time, and\nmeanwhile, achieves the state-of-the-art performance on the KITTI road\nbenchmark. Our project page is at\nhttps://www.sne-roadseg.site/sne-roadseg-plus.",
          "link": "http://arxiv.org/abs/2107.14599",
          "publishedOn": "2021-08-02T01:58:23.822Z",
          "wordCount": 619,
          "title": "SNE-RoadSeg+: Rethinking Depth-Normal Translation and Deep Supervision for Freespace Detection. (arXiv:2107.14599v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Youjia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Taotao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minzhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Teng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minye Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>",
          "description": "Photo-realistic facial video portrait reenactment benefits virtual production\nand numerous VR/AR experiences. The task remains challenging as the portrait\nshould maintain high realism and consistency with the target environment. In\nthis paper, we present a relightable neural video portrait, a simultaneous\nrelighting and reenactment scheme that transfers the head pose and facial\nexpressions from a source actor to a portrait video of a target actor with\narbitrary new backgrounds and lighting conditions. Our approach combines 4D\nreflectance field learning, model-based facial performance capture and\ntarget-aware neural rendering. Specifically, we adopt a rendering-to-video\ntranslation network to first synthesize high-quality OLAT imagesets and alpha\nmattes from hybrid facial performance capture results. We then design a\nsemantic-aware facial normalization scheme to enable reliable explicit control\nas well as a multi-frame multi-task learning strategy to encode content,\nsegmentation and temporal information simultaneously for high-quality\nreflectance field inference. After training, our approach further enables\nphoto-realistic and controllable video portrait editing of the target\nperformer. Reliable face poses and expression editing is obtained by applying\nthe same hybrid facial capture and normalization scheme to the source video\ninput, while our explicit alpha and OLAT output enable high-quality relit and\nbackground editing. With the ability to achieve simultaneous relighting and\nreenactment, we are able to improve the realism in a variety of virtual\nproduction and video rewrite applications.",
          "link": "http://arxiv.org/abs/2107.14735",
          "publishedOn": "2021-08-02T01:58:23.815Z",
          "wordCount": 658,
          "title": "Relightable Neural Video Portrait. (arXiv:2107.14735v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiye Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wankou Yang</a>",
          "description": "Video based fall detection accuracy has been largely improved due to the\nrecent progress on deep convolutional neural networks. However, there still\nexists some challenges, such as lighting variation, complex background, which\ndegrade the accuracy and generalization ability of these approaches. Meanwhile,\nlarge computation cost limits the application of existing fall detection\napproaches. To alleviate these problems, a video based fall detection approach\nusing human poses is proposed in this paper. First, a lightweight pose\nestimator extracts 2D poses from video sequences and then 2D poses are lifted\nto 3D poses. Second, we introduce a robust fall detection network to recognize\nfall events using estimated 3D poses, which increases respective filed and\nmaintains low computation cost by dilated convolutions. The experimental\nresults show that the proposed fall detection approach achieves a high accuracy\nof 99.83% on large benchmark action recognition dataset NTU RGB+D and real-time\nperformance of 18 FPS on a non-GPU platform and 63 FPS on a GPU platform.",
          "link": "http://arxiv.org/abs/2107.14633",
          "publishedOn": "2021-08-02T01:58:23.807Z",
          "wordCount": 592,
          "title": "Video Based Fall Detection Using Human Poses. (arXiv:2107.14633v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yung-Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerg_I/0/1/0/all/0/1\">Isaac D. Gerg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monga_V/0/1/0/all/0/1\">Vishal Monga</a>",
          "description": "Deep learning has not been routinely employed for semantic segmentation of\nseabed environment for synthetic aperture sonar (SAS) imagery due to the\nimplicit need of abundant training data such methods necessitate. Abundant\ntraining data, specifically pixel-level labels for all images, is usually not\navailable for SAS imagery due to the complex logistics (e.g., diver survey,\nchase boat, precision position information) needed for obtaining accurate\nground-truth. Many hand-crafted feature based algorithms have been proposed to\nsegment SAS in an unsupervised fashion. However, there is still room for\nimprovement as the feature extraction step of these methods is fixed. In this\nwork, we present a new iterative unsupervised algorithm for learning deep\nfeatures for SAS image segmentation. Our proposed algorithm alternates between\nclustering superpixels and updating the parameters of a convolutional neural\nnetwork (CNN) so that the feature extraction for image segmentation can be\noptimized. We demonstrate the efficacy of our method on a realistic benchmark\ndataset. Our results show that the performance of our proposed method is\nconsiderably better than current state-of-the-art methods in SAS image\nsegmentation.",
          "link": "http://arxiv.org/abs/2107.14563",
          "publishedOn": "2021-08-02T01:58:23.801Z",
          "wordCount": 616,
          "title": "Iterative, Deep, and Unsupervised Synthetic Aperture Sonar Image Segmentation. (arXiv:2107.14563v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hayat_H/0/1/0/all/0/1\">Hassan Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ventura_C/0/1/0/all/0/1\">Carles Ventura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapedriza_A/0/1/0/all/0/1\">Agata Lapedriza</a>",
          "description": "Understanding the emotional impact of movies has become important for\naffective movie analysis, ranking, and indexing. Methods for recognizing evoked\nemotions are usually trained on human annotated data. Concretely, viewers watch\nvideo clips and have to manually annotate the emotions they experienced while\nwatching the videos. Then, the common practice is to aggregate the different\nannotations, by computing average scores or majority voting, and train and test\nmodels on these aggregated annotations. With this procedure a single aggregated\nevoked emotion annotation is obtained per each video. However, emotions\nexperienced while watching a video are subjective: different individuals might\nexperience different emotions. In this paper, we model the emotions evoked by\nvideos in a different manner: instead of modeling the aggregated value we\njointly model the emotions experienced by each viewer and the aggregated value\nusing a multi-task learning approach. Concretely, we propose two deep learning\narchitectures: a Single-Task (ST) architecture and a Multi-Task (MT)\narchitecture. Our results show that the MT approach can more accurately model\neach viewer and the aggregated annotation when compared to methods that are\ndirectly trained on the aggregated annotations. Furthermore, our approach\noutperforms the current state-of-the-art results on the COGNIMUSE benchmark.",
          "link": "http://arxiv.org/abs/2107.14529",
          "publishedOn": "2021-08-02T01:58:23.794Z",
          "wordCount": 643,
          "title": "Recognizing Emotions evoked by Movies using Multitask Learning. (arXiv:2107.14529v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadekar_K/0/1/0/all/0/1\">Kaustubh Sadekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1\">Ashish Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1\">Shanmuganathan Raman</a>",
          "description": "While recent learning based methods have been observed to be superior for\nseveral vision-related applications, their potential in generating artistic\neffects has not been explored much. One such interesting application is Shadow\nArt - a unique form of sculptural art where 2D shadows cast by a 3D sculpture\nproduce artistic effects. In this work, we revisit shadow art using\ndifferentiable rendering based optimization frameworks to obtain the 3D\nsculpture from a set of shadow (binary) images and their corresponding\nprojection information. Specifically, we discuss shape optimization through\nvoxel as well as mesh-based differentiable renderers. Our choice of using\ndifferentiable rendering for generating shadow art sculptures can be attributed\nto its ability to learn the underlying 3D geometry solely from image data, thus\nreducing the dependence on 3D ground truth. The qualitative and quantitative\nresults demonstrate the potential of the proposed framework in generating\ncomplex 3D sculptures that go beyond those seen in contemporary art pieces\nusing just a set of shadow images as input. Further, we demonstrate the\ngeneration of 3D sculptures to cast shadows of faces, animated movie\ncharacters, and applicability of the framework to sketch-based 3D\nreconstruction of underlying shapes.",
          "link": "http://arxiv.org/abs/2107.14539",
          "publishedOn": "2021-08-02T01:58:23.778Z",
          "wordCount": 630,
          "title": "Shadow Art Revisited: A Differentiable Rendering Based Approach. (arXiv:2107.14539v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14476",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1\">Caifeng Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouwman_R/0/1/0/all/0/1\">R. Arthur Bouwman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dekker_L/0/1/0/all/0/1\">Lukas R. C. Dekker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolen_A/0/1/0/all/0/1\">Alexander F. Kolen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+With_P/0/1/0/all/0/1\">Peter H. N. de With</a>",
          "description": "Medical instrument segmentation in 3D ultrasound is essential for\nimage-guided intervention. However, to train a successful deep neural network\nfor instrument segmentation, a large number of labeled images are required,\nwhich is expensive and time-consuming to obtain. In this article, we propose a\nsemi-supervised learning (SSL) framework for instrument segmentation in 3D US,\nwhich requires much less annotation effort than the existing methods. To\nachieve the SSL learning, a Dual-UNet is proposed to segment the instrument.\nThe Dual-UNet leverages unlabeled data using a novel hybrid loss function,\nconsisting of uncertainty and contextual constraints. Specifically, the\nuncertainty constraints leverage the uncertainty estimation of the predictions\nof the UNet, and therefore improve the unlabeled information for SSL training.\nIn addition, contextual constraints exploit the contextual information of the\ntraining images, which are used as the complementary information for voxel-wise\nuncertainty estimation. Extensive experiments on multiple ex-vivo and in-vivo\ndatasets show that our proposed method achieves Dice score of about 68.6%-69.1%\nand the inference time of about 1 sec. per volume. These results are better\nthan the state-of-the-art SSL methods and the inference time is comparable to\nthe supervised approaches.",
          "link": "http://arxiv.org/abs/2107.14476",
          "publishedOn": "2021-08-02T01:58:23.770Z",
          "wordCount": 645,
          "title": "Medical Instrument Segmentation in 3D US by Hybrid Constrained Semi-Supervised Learning. (arXiv:2107.14476v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Casamitjana_A/0/1/0/all/0/1\">Adri&#xe0; Casamitjana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1\">Matteo Mancini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iglesias_J/0/1/0/all/0/1\">Juan Eugenio Iglesias</a>",
          "description": "Nonlinear inter-modality registration is often challenging due to the lack of\nobjective functions that are good proxies for alignment. Here we propose a\nsynthesis-by-registration method to convert this problem into an easier\nintra-modality task. We introduce a registration loss for weakly supervised\nimage translation between domains that does not require perfectly aligned\ntraining data. This loss capitalises on a registration U-Net with frozen\nweights, to drive a synthesis CNN towards the desired translation. We\ncomplement this loss with a structure preserving constraint based on\ncontrastive learning, which prevents blurring and content shifts due to\noverfitting. We apply this method to the registration of histological sections\nto MRI slices, a key step in 3D histology reconstruction. Results on two\ndifferent public datasets show improvements over registration based on mutual\ninformation (13% reduction in landmark error) and synthesis-based algorithms\nsuch as CycleGAN (11% reduction), and are comparable to a registration CNN with\nlabel supervision.",
          "link": "http://arxiv.org/abs/2107.14449",
          "publishedOn": "2021-08-02T01:58:23.762Z",
          "wordCount": 610,
          "title": "Synth-by-Reg (SbR): Contrastive learning for synthesis-based registration of paired images. (arXiv:2107.14449v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14659",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Concha_A/0/1/0/all/0/1\">Alejo Concha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burri_M/0/1/0/all/0/1\">Michael Burri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briales_J/0/1/0/all/0/1\">Jes&#xfa;s Briales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forster_C/0/1/0/all/0/1\">Christian Forster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oth_L/0/1/0/all/0/1\">Luc Oth</a>",
          "description": "Mobile AR applications benefit from fast initialization to display\nworld-locked effects instantly. However, standard visual odometry or SLAM\nalgorithms require motion parallax to initialize (see Figure 1) and, therefore,\nsuffer from delayed initialization. In this paper, we present a 6-DoF monocular\nvisual odometry that initializes instantly and without motion parallax. Our\nmain contribution is a pose estimator that decouples estimating the 5-DoF\nrelative rotation and translation direction from the 1-DoF translation\nmagnitude. While scale is not observable in a monocular vision-only setting, it\nis still paramount to estimate a consistent scale over the whole trajectory\n(even if not physically accurate) to avoid AR effects moving erroneously along\ndepth. In our approach, we leverage the fact that depth errors are not\nperceivable to the user during rotation-only motion. However, as the user\nstarts translating the device, depth becomes perceivable and so does the\ncapability to estimate consistent scale. Our proposed algorithm naturally\ntransitions between these two modes. We perform extensive validations of our\ncontributions with both a publicly available dataset and synthetic data. We\nshow that the proposed pose estimator outperforms the classical approaches for\n6-DoF pose estimation used in the literature in low-parallax configurations. We\nrelease a dataset for the relative pose problem using real data to facilitate\nthe comparison with future solutions for the relative pose problem. Our\nsolution is either used as a full odometry or as a preSLAM component of any\nsupported SLAM system (ARKit, ARCore) in world-locked AR effects on platforms\nsuch as Instagram and Facebook.",
          "link": "http://arxiv.org/abs/2107.14659",
          "publishedOn": "2021-08-02T01:58:23.752Z",
          "wordCount": 692,
          "title": "Instant Visual Odometry Initialization for Mobile AR. (arXiv:2107.14659v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14611",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_H/0/1/0/all/0/1\">Haosong Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1\">Jinyu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1\">Fanghong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengguo Li</a>",
          "description": "Localizing pre-visited places during long-term simultaneous localization and\nmapping, i.e. loop closure detection (LCD), is a crucial technique to correct\naccumulated inconsistencies. As one of the most effective and efficient\nsolutions, Bag-of-Words (BoW) builds a visual vocabulary to associate features\nand then detect loops. Most existing approaches that build vocabularies\noff-line determine scales of the vocabulary by trial-and-error, which often\nresults in unreasonable feature association. Moreover, the accuracy of the\nalgorithm usually declines due to perceptual aliasing, as the BoW-based method\nignores the positions of visual features. To overcome these disadvantages, we\npropose a natural convergence criterion based on the comparison between the\nradii of nodes and the drifts of feature descriptors, which is then utilized to\nbuild the optimal vocabulary automatically. Furthermore, we present a novel\ntopological graph verification method for validating candidate loops so that\ngeometrical positions of the words can be involved with a negligible increase\nin complexity, which can significantly improve the accuracy of LCD. Experiments\non various public datasets and comparisons against several state-of-the-art\nalgorithms verify the performance of our proposed approach.",
          "link": "http://arxiv.org/abs/2107.14611",
          "publishedOn": "2021-08-02T01:58:23.746Z",
          "wordCount": 629,
          "title": "Automatic Vocabulary and Graph Verification for Accurate Loop Closure Detection. (arXiv:2107.14611v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xunlin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yangxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Minlong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>",
          "description": "Nowadays, customer's demands for E-commerce are more diversified, which\nintroduces more complications to the product retrieval industry. Previous\nmethods are either subject to single-modal input or perform supervised\nimage-level product retrieval, thus fail to accommodate real-life scenarios\nwhere enormous weakly annotated multi-modal data are present. In this paper, we\ninvestigate a more realistic setting that aims to perform weakly-supervised\nmulti-modal instance-level product retrieval among fine-grained product\ncategories. To promote the study of this challenging task, we contribute\nProduct1M, one of the largest multi-modal cosmetic datasets for real-world\ninstance-level retrieval. Notably, Product1M contains over 1 million\nimage-caption pairs and consists of two sample types, i.e., single-product and\nmulti-product samples, which encompass a wide variety of cosmetics brands. In\naddition to the great diversity, Product1M enjoys several appealing\ncharacteristics including fine-grained categories, complex combinations, and\nfuzzy correspondence that well mimic the real-world scenes. Moreover, we\npropose a novel model named Cross-modal contrAstive Product Transformer for\ninstance-level prodUct REtrieval (CAPTURE), that excels in capturing the\npotential synergy between multi-modal inputs via a hybrid-stream transformer in\na self-supervised manner.CAPTURE generates discriminative instance features via\nmasked multi-modal learning as well as cross-modal contrastive pretraining and\nit outperforms several SOTA cross-modal baselines. Extensive ablation studies\nwell demonstrate the effectiveness and the generalization capacity of our\nmodel.",
          "link": "http://arxiv.org/abs/2107.14572",
          "publishedOn": "2021-08-02T01:58:23.738Z",
          "wordCount": 657,
          "title": "Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-modal Pretraining. (arXiv:2107.14572v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yousong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chaoyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1\">Guosheng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wei Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Ming Tang</a>",
          "description": "Transformer has achieved great success in computer vision, while how to split\npatches in an image remains a problem. Existing methods usually use a\nfixed-size patch embedding which might destroy the semantics of objects. To\naddress this problem, we propose a new Deformable Patch (DePatch) module which\nlearns to adaptively split the images into patches with different positions and\nscales in a data-driven way rather than using predefined fixed patches. In this\nway, our method can well preserve the semantics in patches. The DePatch module\ncan work as a plug-and-play module, which can easily be incorporated into\ndifferent transformers to achieve an end-to-end training. We term this\nDePatch-embedded transformer as Deformable Patch-based Transformer (DPT) and\nconduct extensive evaluations of DPT on image classification and object\ndetection. Results show DPT can achieve 81.9% top-1 accuracy on ImageNet\nclassification, and 43.7% box mAP with RetinaNet, 44.3% with Mask R-CNN on\nMSCOCO object detection. Code has been made available at:\nhttps://github.com/CASIA-IVA-Lab/DPT .",
          "link": "http://arxiv.org/abs/2107.14467",
          "publishedOn": "2021-08-02T01:58:23.720Z",
          "wordCount": 616,
          "title": "DPT: Deformable Patch-based Transformer for Visual Recognition. (arXiv:2107.14467v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14531",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Araujo_R/0/1/0/all/0/1\">R. J. Ara&#xfa;jo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardoso_J/0/1/0/all/0/1\">J. S. Cardoso</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_H/0/1/0/all/0/1\">H. P. Oliveira</a>",
          "description": "Blood vessel segmentation is one of the most studied topics in computer\nvision, due to its relevance in daily clinical practice. Despite the evolution\nthe field has been facing, especially after the dawn of deep learning,\nimportant challenges are still not solved. One of them concerns the consistency\nof the topological properties of the vascular trees, given that the best\nperforming methodologies do not directly penalize mistakes such as broken\nsegments and end up producing predictions with disconnected trees. This is\nparticularly relevant in graph-like structures, such as blood vessel trees,\ngiven that it puts at risk the characterization steps that follow the\nsegmentation task. In this paper, we propose a similarity index which captures\nthe topological consistency of the predicted segmentations having as reference\nthe ground truth. We also design a novel loss function based on the\nmorphological closing operator and show how it allows to learn deep neural\nnetwork models which produce more topologically coherent masks. Our experiments\ntarget well known retinal benchmarks and a coronary angiogram database.",
          "link": "http://arxiv.org/abs/2107.14531",
          "publishedOn": "2021-08-02T01:58:23.713Z",
          "wordCount": 635,
          "title": "Topological Similarity Index and Loss Function for Blood Vessel Segmentation. (arXiv:2107.14531v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14443",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Galetto_F/0/1/0/all/0/1\">Fernando J. Galetto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_G/0/1/0/all/0/1\">Guang Deng</a>",
          "description": "The depth information is useful in many image processing applications.\nHowever, since taking a picture is a process of projection of a 3D scene onto a\n2D imaging sensor, the depth information is embedded in the image. Extracting\nthe depth information from the image is a challenging task. A guiding principle\nis that the level of blurriness due to defocus is related to the distance\nbetween the object and the focal plane. Based on this principle and the widely\nused assumption that Gaussian blur is a good model for defocus blur, we\nformulate the problem of estimating the spatially varying defocus blurriness as\na Gaussian blur classification problem. We solved the problem by training a\ndeep neural network to classify image patches into one of the 20 levels of\nblurriness. We have created a dataset of more than 500000 image patches of size\n32x32 which are used to train and test several well-known network models. We\nfind that MobileNetV2 is suitable for this application due to its low memory\nrequirement and high accuracy. The trained model is used to determine the patch\nblurriness which is then refined by applying an iterative weighted guided\nfilter. The result is a defocus map that carries the information of the degree\nof blurriness for each pixel. We compare the proposed method with\nstate-of-the-art techniques and we demonstrate its successful applications in\nadaptive image enhancement, defocus magnification, and multi-focus image\nfusion.",
          "link": "http://arxiv.org/abs/2107.14443",
          "publishedOn": "2021-08-02T01:58:23.692Z",
          "wordCount": 689,
          "title": "Single image deep defocus estimation and its applications. (arXiv:2107.14443v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung-Nghia Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huy H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1\">Isao Echizen</a>",
          "description": "The proliferation of deepfake media is raising concerns among the public and\nrelevant authorities. It has become essential to develop countermeasures\nagainst forged faces in social media. This paper presents a comprehensive study\non two new countermeasure tasks: multi-face forgery detection and segmentation\nin-the-wild. Localizing forged faces among multiple human faces in unrestricted\nnatural scenes is far more challenging than the traditional deepfake\nrecognition task. To promote these new tasks, we have created the first\nlarge-scale dataset posing a high level of challenges that is designed with\nface-wise rich annotations explicitly for face forgery detection and\nsegmentation, namely OpenForensics. With its rich annotations, our\nOpenForensics dataset has great potentials for research in both deepfake\nprevention and general human face detection. We have also developed a suite of\nbenchmarks for these tasks by conducting an extensive evaluation of\nstate-of-the-art instance detection and segmentation methods on our newly\nconstructed dataset in various scenarios. The dataset, benchmark results,\ncodes, and supplementary materials will be publicly available on our project\npage: https://sites.google.com/view/ltnghia/research/openforensics",
          "link": "http://arxiv.org/abs/2107.14480",
          "publishedOn": "2021-08-02T01:58:23.652Z",
          "wordCount": 622,
          "title": "OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery Detection And Segmentation In-The-Wild. (arXiv:2107.14480v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yongxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianlei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiaolin Qin</a>",
          "description": "Nowadays, plenty of deep learning technologies are being applied to all\naspects of autonomous driving with promising results. Among them, object\ndetection is the key to improve the ability of an autonomous agent to perceive\nits environment so that it can (re)act. However, previous vision-based object\ndetectors cannot achieve satisfactory performance under real-time driving\nscenarios. To remedy this, we present the real-time steaming perception system\nin this paper, which is also the 2nd Place solution of Streaming Perception\nChallenge (Workshop on Autonomous Driving at CVPR 2021) for the detection-only\ntrack. Unlike traditional object detection challenges, which focus mainly on\nthe absolute performance, streaming perception task requires achieving a\nbalance of accuracy and latency, which is crucial for real-time autonomous\ndriving. We adopt YOLOv5 as our basic framework, data augmentation,\nBag-of-Freebies, and Transformer are adopted to improve streaming object\ndetection performance with negligible extra inference cost. On the Argoverse-HD\ntest set, our method achieves 33.2 streaming AP (34.6 streaming AP verified by\nthe organizer) under the required hardware. Its performance significantly\nsurpasses the fixed baseline of 13.6 (host team), demonstrating the\npotentiality of application.",
          "link": "http://arxiv.org/abs/2107.14388",
          "publishedOn": "2021-08-02T01:58:23.644Z",
          "wordCount": 617,
          "title": "Real-time Streaming Perception System for Autonomous Driving. (arXiv:2107.14388v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14425",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaotian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_H/0/1/0/all/0/1\">Hanling Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Ling Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>",
          "description": "There has been a recent surge of research interest in attacking the problem\nof social relation inference based on images. Existing works classify social\nrelations mainly by creating complicated graphs of human interactions, or\nlearning the foreground and/or background information of persons and objects,\nbut ignore holistic scene context. The holistic scene refers to the\nfunctionality of a place in images, such as dinning room, playground and\noffice. In this paper, by mimicking human understanding on images, we propose\nan approach of \\textbf{PR}actical \\textbf{I}nference in \\textbf{S}ocial\nr\\textbf{E}lation (PRISE), which concisely learns interactive features of\npersons and discriminative features of holistic scenes. Technically, we develop\na simple and fast relational graph convolutional network to capture interactive\nfeatures of all persons in one image. To learn the holistic scene feature, we\nelaborately design a contrastive learning task based on image scene\nclassification. To further boost the performance in social relation inference,\nwe collect and distribute a new large-scale dataset, which consists of about\n240 thousand unlabeled images. The extensive experimental results show that our\nnovel learning framework significantly beats the state-of-the-art methods,\ne.g., PRISE achieves 6.8$\\%$ improvement for domain classification in PIPA\ndataset.",
          "link": "http://arxiv.org/abs/2107.14425",
          "publishedOn": "2021-08-02T01:58:23.636Z",
          "wordCount": 642,
          "title": "Enhancing Social Relation Inference with Concise Interaction Graph and Discriminative Scene Representation. (arXiv:2107.14425v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14519",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zongben Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>",
          "description": "It has been shown that equivariant convolution is very helpful for many types\nof computer vision tasks. Recently, the 2D filter parametrization technique\nplays an important role when designing equivariant convolutions. However, the\ncurrent filter parametrization method still has its evident drawbacks, where\nthe most critical one lies in the accuracy problem of filter representation.\nAgainst this issue, in this paper we modify the classical Fourier series\nexpansion for 2D filters, and propose a new set of atomic basis functions for\nfilter parametrization. The proposed filter parametrization method not only\nfinely represents 2D filters with zero error when the filter is not rotated,\nbut also substantially alleviates the fence-effect-caused quality degradation\nwhen the filter is rotated. Accordingly, we construct a new equivariant\nconvolution method based on the proposed filter parametrization method, named\nF-Conv. We prove that the equivariance of the proposed F-Conv is exact in the\ncontinuous domain, which becomes approximate only after discretization.\nExtensive experiments show the superiority of the proposed method.\nParticularly, we adopt rotation equivariant convolution methods to image\nsuper-resolution task, and F-Conv evidently outperforms previous filter\nparametrization based method in this task, reflecting its intrinsic capability\nof faithfully preserving rotation symmetries in local image features.",
          "link": "http://arxiv.org/abs/2107.14519",
          "publishedOn": "2021-08-02T01:58:23.629Z",
          "wordCount": 642,
          "title": "Fourier Series Expansion Based Filter Parametrization for Equivariant Convolutions. (arXiv:2107.14519v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leroy_R/0/1/0/all/0/1\">R&#xe9;my Leroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trouve_Peloux_P/0/1/0/all/0/1\">Pauline Trouv&#xe9;-Peloux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Champagnat_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Champagnat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saux_B/0/1/0/all/0/1\">Bertrand Le Saux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_M/0/1/0/all/0/1\">Marcela Carvalho</a>",
          "description": "Good quality reconstruction and comprehension of a scene rely on 3D\nestimation methods. The 3D information was usually obtained from images by\nstereo-photogrammetry, but deep learning has recently provided us with\nexcellent results for monocular depth estimation. Building up a sufficiently\nlarge and rich training dataset to achieve these results requires onerous\nprocessing. In this paper, we address the problem of learning outdoor 3D point\ncloud from monocular data using a sparse ground-truth dataset. We propose\nPix2Point, a deep learning-based approach for monocular 3D point cloud\nprediction, able to deal with complete and challenging outdoor scenes. Our\nmethod relies on a 2D-3D hybrid neural network architecture, and a supervised\nend-to-end minimisation of an optimal transport divergence between point\nclouds. We show that, when trained on sparse point clouds, our simple promising\napproach achieves a better coverage of 3D outdoor scenes than efficient\nmonocular depth methods.",
          "link": "http://arxiv.org/abs/2107.14498",
          "publishedOn": "2021-08-02T01:58:23.621Z",
          "wordCount": 610,
          "title": "Pix2Point: Learning Outdoor 3D Using Sparse Point Clouds and Optimal Transport. (arXiv:2107.14498v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiajun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanyong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>",
          "description": "As an emerging data modal with precise distance sensing, LiDAR point clouds\nhave been placed great expectations on 3D scene understanding. However, point\nclouds are always sparsely distributed in the 3D space, and with unstructured\nstorage, which makes it difficult to represent them for effective 3D object\ndetection. To this end, in this work, we regard point clouds as hollow-3D data\nand propose a new architecture, namely Hallucinated Hollow-3D R-CNN\n($\\text{H}^2$3D R-CNN), to address the problem of 3D object detection. In our\napproach, we first extract the multi-view features by sequentially projecting\nthe point clouds into the perspective view and the bird-eye view. Then, we\nhallucinate the 3D representation by a novel bilaterally guided multi-view\nfusion block. Finally, the 3D objects are detected via a box refinement module\nwith a novel Hierarchical Voxel RoI Pooling operation. The proposed\n$\\text{H}^2$3D R-CNN provides a new angle to take full advantage of\ncomplementary information in the perspective view and the bird-eye view with an\nefficient framework. We evaluate our approach on the public KITTI Dataset and\nWaymo Open Dataset. Extensive experiments demonstrate the superiority of our\nmethod over the state-of-the-art algorithms with respect to both effectiveness\nand efficiency. The code will be made available at\n\\url{https://github.com/djiajunustc/H-23D_R-CNN}.",
          "link": "http://arxiv.org/abs/2107.14391",
          "publishedOn": "2021-08-02T01:58:23.612Z",
          "wordCount": 656,
          "title": "From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection. (arXiv:2107.14391v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>",
          "description": "Semantic segmentation requires per-pixel prediction for a given image.\nTypically, the output resolution of a segmentation network is severely reduced\ndue to the downsampling operations in the CNN backbone. Most previous methods\nemploy upsampling decoders to recover the spatial resolution. Various decoders\nwere designed in the literature. Here, we propose a novel decoder, termed\ndynamic neural representational decoder (NRD), which is simple yet\nsignificantly more efficient. As each location on the encoder's output\ncorresponds to a local patch of the semantic labels, in this work, we represent\nthese local patches of labels with compact neural networks. This neural\nrepresentation enables our decoder to leverage the smoothness prior in the\nsemantic label space, and thus makes our decoder more efficient. Furthermore,\nthese neural representations are dynamically generated and conditioned on the\noutputs of the encoder networks. The desired semantic labels can be efficiently\ndecoded from the neural representations, resulting in high-resolution semantic\nsegmentation predictions. We empirically show that our proposed decoder can\noutperform the decoder in DeeplabV3+ with only 30% computational complexity,\nand achieve competitive performance with the methods using dilated encoders\nwith only 15% computation. Experiments on the Cityscapes, ADE20K, and PASCAL\nContext datasets demonstrate the effectiveness and efficiency of our proposed\nmethod.",
          "link": "http://arxiv.org/abs/2107.14428",
          "publishedOn": "2021-08-02T01:58:23.605Z",
          "wordCount": 638,
          "title": "Dynamic Neural Representational Decoders for High-Resolution Semantic Segmentation. (arXiv:2107.14428v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guangze Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>",
          "description": "Recent years have witnessed the fast evolution and promising performance of\nthe convolutional neural network (CNN)-based trackers, which aim at imitating\nbiological visual systems. However, current CNN-based trackers can hardly\ngeneralize well to low-light scenes that are commonly lacked in the existing\ntraining set. In indistinguishable night scenarios frequently encountered in\nunmanned aerial vehicle (UAV) tracking-based applications, the robustness of\nthe state-of-the-art (SOTA) trackers drops significantly. To facilitate aerial\ntracking in the dark through a general fashion, this work proposes a low-light\nimage enhancer namely DarkLighter, which dedicates to alleviate the impact of\npoor illumination and noise iteratively. A lightweight map estimation network,\ni.e., ME-Net, is trained to efficiently estimate illumination maps and noise\nmaps jointly. Experiments are conducted with several SOTA trackers on numerous\nUAV dark tracking scenes. Exhaustive evaluations demonstrate the reliability\nand universality of DarkLighter, with high efficiency. Moreover, DarkLighter\nhas further been implemented on a typical UAV system. Real-world tests at night\nscenes have verified its practicability and dependability.",
          "link": "http://arxiv.org/abs/2107.14389",
          "publishedOn": "2021-08-02T01:58:23.597Z",
          "wordCount": 609,
          "title": "DarkLighter: Light Up the Darkness for UAV Tracking. (arXiv:2107.14389v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14292",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shafique_A/0/1/0/all/0/1\">Abubakr Shafique</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Babaie_M/0/1/0/all/0/1\">Morteza Babaie</a> (1 and 3), <a href=\"http://arxiv.org/find/eess/1/au:+Sajadi_M/0/1/0/all/0/1\">Mahjabin Sajadi</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Batten_A/0/1/0/all/0/1\">Adrian Batten</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Skdar_S/0/1/0/all/0/1\">Soma Skdar</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Tizhoosh_H/0/1/0/all/0/1\">H.R. Tizhoosh</a> (1 and 3) ((1) Kimia Lab, University of Waterloo, Waterloo, ON, Canada., (2) Department of Pathology, Grand River Hospital, Kitchener, ON, Canada., and (3) Vector Institute, MaRS Centre, Toronto, Canada.)",
          "description": "Joint analysis of multiple biomarker images and tissue morphology is\nimportant for disease diagnosis, treatment planning and drug development. It\nrequires cross-staining comparison among Whole Slide Images (WSIs) of\nimmuno-histochemical and hematoxylin and eosin (H&E) microscopic slides.\nHowever, automatic, and fast cross-staining alignment of enormous gigapixel\nWSIs at single-cell precision is challenging. In addition to morphological\ndeformations introduced during slide preparation, there are large variations in\ncell appearance and tissue morphology across different staining. In this paper,\nwe propose a two-step automatic feature-based cross-staining WSI alignment to\nassist localization of even tiny metastatic foci in the assessment of lymph\nnode. Image pairs were aligned allowing for translation, rotation, and scaling.\nThe registration was performed automatically by first detecting landmarks in\nboth images, using the scale-invariant image transform (SIFT), followed by the\nfast sample consensus (FSC) protocol for finding point correspondences and\nfinally aligned the images. The Registration results were evaluated using both\nvisual and quantitative criteria using the Jaccard index. The average Jaccard\nsimilarity index of the results produced by the proposed system is 0.942 when\ncompared with the manual registration.",
          "link": "http://arxiv.org/abs/2107.14292",
          "publishedOn": "2021-08-02T01:58:23.581Z",
          "wordCount": 686,
          "title": "Automatic Multi-Stain Registration of Whole Slide Images in Histopathology. (arXiv:2107.14292v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tongzhou Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1\">Fanbo Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Derek Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Stone Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhiwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>",
          "description": "Learning generalizable manipulation skills is central for robots to achieve\ntask automation in environments with endless scene and object variations.\nHowever, existing robot learning environments are limited in both scale and\ndiversity of 3D assets (especially of articulated objects), making it difficult\nto train and evaluate the generalization ability of agents over novel objects.\nIn this work, we focus on object-level generalization and propose SAPIEN\nManipulation Skill Benchmark (abbreviated as ManiSkill), a large-scale\nlearning-from-demonstrations benchmark for articulated object manipulation with\nvisual input (point cloud and image). ManiSkill supports object-level\nvariations by utilizing a rich and diverse set of articulated objects, and each\ntask is carefully designed for learning manipulations on a single category of\nobjects. We equip ManiSkill with high-quality demonstrations to facilitate\nlearning-from-demonstrations approaches and perform evaluations on common\nbaseline algorithms. We believe ManiSkill can encourage the robot learning\ncommunity to explore more on learning generalizable object manipulation skills.",
          "link": "http://arxiv.org/abs/2107.14483",
          "publishedOn": "2021-08-02T01:58:23.571Z",
          "wordCount": 606,
          "title": "ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills. (arXiv:2107.14483v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruihuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianzhong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuaijun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>",
          "description": "Most existing domain adaptation methods focus on adaptation from only one\nsource domain, however, in practice there are a number of relevant sources that\ncould be leveraged to help improve performance on target domain. We propose a\nnovel approach named T-SVDNet to address the task of Multi-source Domain\nAdaptation (MDA), which is featured by incorporating Tensor Singular Value\nDecomposition (T-SVD) into a neural network's training pipeline. Overall,\nhigh-order correlations among multiple domains and categories are fully\nexplored so as to better bridge the domain gap. Specifically, we impose\nTensor-Low-Rank (TLR) constraint on a tensor obtained by stacking up a group of\nprototypical similarity matrices, aiming at capturing consistent data structure\nacross different domains. Furthermore, to avoid negative transfer brought by\nnoisy source data, we propose a novel uncertainty-aware weighting strategy to\nadaptively assign weights to different source domains and samples based on the\nresult of uncertainty estimation. Extensive experiments conducted on public\nbenchmarks demonstrate the superiority of our model in addressing the task of\nMDA compared to state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.14447",
          "publishedOn": "2021-08-02T01:58:23.561Z",
          "wordCount": 623,
          "title": "T-SVDNet: Exploring High-Order Prototypical Correlations for Multi-Source Domain Adaptation. (arXiv:2107.14447v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hanxiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qingnan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Youyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">C. Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>",
          "description": "We describe an unsupervised domain adaptation method for image content shift\ncaused by viewpoint changes for a semantic segmentation task. Most existing\nmethods perform domain alignment in a shared space and assume that the mapping\nfrom the aligned space to the output is transferable. However, the novel\ncontent induced by viewpoint changes may nullify such a space for effective\nalignments, thus resulting in negative adaptation. Our method works without\naligning any statistics of the images between the two domains. Instead, it\nutilizes a view transformation network trained only on color images to\nhallucinate the semantic images for the target. Despite the lack of\nsupervision, the view transformation network can still generalize to semantic\nimages thanks to the inductive bias introduced by the attention mechanism.\nFurthermore, to resolve ambiguities in converting the semantic images to\nsemantic labels, we treat the view transformation network as a functional\nrepresentation of an unknown mapping implied by the color images and propose\nfunctional label hallucination to generate pseudo-labels in the target domain.\nOur method surpasses baselines built on state-of-the-art correspondence\nestimation and view synthesis methods. Moreover, it outperforms the\nstate-of-the-art unsupervised domain adaptation methods that utilize\nself-training and adversarial domain alignment. Our code and dataset will be\nmade publicly available.",
          "link": "http://arxiv.org/abs/2107.14285",
          "publishedOn": "2021-08-02T01:58:23.521Z",
          "wordCount": 658,
          "title": "ADeLA: Automatic Dense Labeling with Attention for Viewpoint Adaptation in Semantic Segmentation. (arXiv:2107.14285v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Margapuri_V/0/1/0/all/0/1\">Venkat Margapuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Penumajji_N/0/1/0/all/0/1\">Niketa Penumajji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neilsen_M/0/1/0/all/0/1\">Mitchell Neilsen</a>",
          "description": "Smart environments are environments where digital devices are connected to\neach other over the Internet and operate in sync. Security is of paramount\nimportance in such environments. This paper addresses aspects of authorized\naccess and intruder detection for smart environments. Proposed is PiBase, an\nInternet of Things (IoT)-based app that aids in detecting intruders and\nproviding security. The hardware for the application consists of a Raspberry\nPi, a PIR motion sensor to detect motion from infrared radiation in the\nenvironment, an Android mobile phone and a camera. The software for the\napplication is written in Java, Python and NodeJS. The PIR sensor and Pi camera\nmodule connected to the Raspberry Pi aid in detecting human intrusion. Machine\nlearning algorithms, namely Haar-feature based cascade classifiers and Linear\nBinary Pattern Histograms (LBPH), are used for face detection and face\nrecognition, respectively. The app lets the user create a list of non-intruders\nand anyone that is not on the list is identified as an intruder. The app alerts\nthe user only in the event of an intrusion by using the Google Firebase Cloud\nMessaging service to trigger a notification to the app. The user may choose to\nadd the detected intruder to the list of non-intruders through the app to avoid\nfurther detections as intruder. Face detection by the Haar Cascade algorithm\nyields a recall of 94.6%. Thus, the system is both highly effective and\nrelatively low cost.",
          "link": "http://arxiv.org/abs/2107.14325",
          "publishedOn": "2021-08-02T01:58:23.489Z",
          "wordCount": 682,
          "title": "PiBase: An IoT-based Security System using Raspberry Pi and Google Firebase. (arXiv:2107.14325v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jingwei Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingjing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunmao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>",
          "description": "Automatic facial action unit (AU) recognition is a challenging task due to\nthe scarcity of manual annotations. To alleviate this problem, a large amount\nof efforts has been dedicated to exploiting various methods which leverage\nnumerous unlabeled data. However, many aspects with regard to some unique\nproperties of AUs, such as the regional and relational characteristics, are not\nsufficiently explored in previous works. Motivated by this, we take the AU\nproperties into consideration and propose two auxiliary AU related tasks to\nbridge the gap between limited annotations and the model performance in a\nself-supervised manner via the unlabeled data. Specifically, to enhance the\ndiscrimination of regional features with AU relation embedding, we design a\ntask of RoI inpainting to recover the randomly cropped AU patches. Meanwhile, a\nsingle image based optical flow estimation task is proposed to leverage the\ndynamic change of facial muscles and encode the motion information into the\nglobal feature representation. Based on these two self-supervised auxiliary\ntasks, local features, mutual relation and motion cues of AUs are better\ncaptured in the backbone network with the proposed regional and temporal based\nauxiliary task learning (RTATL) framework. Extensive experiments on BP4D and\nDISFA demonstrate the superiority of our method and new state-of-the-art\nperformances are achieved.",
          "link": "http://arxiv.org/abs/2107.14399",
          "publishedOn": "2021-08-02T01:58:23.463Z",
          "wordCount": 674,
          "title": "Self-Supervised Regional and Temporal Auxiliary Tasks for Facial Action Unit Recognition. (arXiv:2107.14399v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1\">Runzhou Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuangzhuang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yihan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wenxin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Li Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>",
          "description": "In this report, we introduce our winning solution to the Real-time 3D\nDetection and also the \"Most Efficient Model\" in the Waymo Open Dataset\nChallenges at CVPR 2021. Extended from our last year's award-winning model\nAFDet, we have made a handful of modifications to the base model, to improve\nthe accuracy and at the same time to greatly reduce the latency. The modified\nmodel, named as AFDetV2, is featured with a lite 3D Feature Extractor, an\nimproved RPN with extended receptive field and an added sub-head that produces\nan IoU-aware confidence score. These model enhancements, together with enriched\ndata augmentation, stochastic weights averaging, and a GPU-based implementation\nof voxelization, lead to a winning accuracy of 73.12 mAPH/L2 for our AFDetV2\nwith a latency of 60.06 ms, and an accuracy of 72.57 mAPH/L2 for our\nAFDetV2-base, entitled as the \"Most Efficient Model\" by the challenge sponsor,\nwith a winning latency of 55.86 ms.",
          "link": "http://arxiv.org/abs/2107.14342",
          "publishedOn": "2021-08-02T01:58:23.457Z",
          "wordCount": 592,
          "title": "Real-Time Anchor-Free Single-Stage 3D Detection with IoU-Awareness. (arXiv:2107.14342v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Safarani_S/0/1/0/all/0/1\">Shahd Safarani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nix_A/0/1/0/all/0/1\">Arne Nix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willeke_K/0/1/0/all/0/1\">Konstantin Willeke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadena_S/0/1/0/all/0/1\">Santiago A. Cadena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Restivo_K/0/1/0/all/0/1\">Kelli Restivo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denfield_G/0/1/0/all/0/1\">George Denfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_A/0/1/0/all/0/1\">Andreas S. Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinz_F/0/1/0/all/0/1\">Fabian H. Sinz</a>",
          "description": "Deep neural networks set the state-of-the-art across many tasks in computer\nvision, but their generalization ability to image distortions is surprisingly\nfragile. In contrast, the mammalian visual system is robust to a wide range of\nperturbations. Recent work suggests that this generalization ability can be\nexplained by useful inductive biases encoded in the representations of visual\nstimuli throughout the visual cortex. Here, we successfully leveraged these\ninductive biases with a multi-task learning approach: we jointly trained a deep\nnetwork to perform image classification and to predict neural activity in\nmacaque primary visual cortex (V1). We measured the out-of-distribution\ngeneralization abilities of our network by testing its robustness to image\ndistortions. We found that co-training on monkey V1 data leads to increased\nrobustness despite the absence of those distortions during training.\nAdditionally, we showed that our network's robustness is very close to that of\nan Oracle network where parts of the architecture are directly trained on noisy\nimages. Our results also demonstrated that the network's representations become\nmore brain-like as their robustness improves. Using a novel constrained\nreconstruction analysis, we investigated what makes our brain-regularized\nnetwork more robust. We found that our co-trained network is more sensitive to\ncontent than noise when compared to a Baseline network that we trained for\nimage classification alone. Using DeepGaze-predicted saliency maps for ImageNet\nimages, we found that our monkey co-trained network tends to be more sensitive\nto salient regions in a scene, reminiscent of existing theories on the role of\nV1 in the detection of object borders and bottom-up saliency. Overall, our work\nexpands the promising research avenue of transferring inductive biases from the\nbrain, and provides a novel analysis of the effects of our transfer.",
          "link": "http://arxiv.org/abs/2107.14344",
          "publishedOn": "2021-08-02T01:58:23.450Z",
          "wordCount": 745,
          "title": "Towards robust vision by multi-task learning on monkey visual cortex. (arXiv:2107.14344v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1\">Tianxiang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuchen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>",
          "description": "The existence of redundancy in Convolutional Neural Networks (CNNs) enables\nus to remove some filters/channels with acceptable performance drops. However,\nthe training objective of CNNs usually tends to minimize an accuracy-related\nloss function without any attention paid to the redundancy, making the\nredundancy distribute randomly on all the filters, such that removing any of\nthem may trigger information loss and accuracy drop, necessitating a following\nfinetuning step for recovery. In this paper, we propose to manipulate the\nredundancy during training to facilitate network pruning. To this end, we\npropose a novel Centripetal SGD (C-SGD) to make some filters identical,\nresulting in ideal redundancy patterns, as such filters become purely redundant\ndue to their duplicates; hence removing them does not harm the network. As\nshown on CIFAR and ImageNet, C-SGD delivers better performance because the\nredundancy is better organized, compared to the existing methods. The\nefficiency also characterizes C-SGD because it is as fast as regular SGD,\nrequires no finetuning, and can be conducted simultaneously on all the layers\neven in very deep CNNs. Besides, C-SGD can improve the accuracy of CNNs by\nfirst training a model with the same architecture but wider layers then\nsqueezing it into the original width.",
          "link": "http://arxiv.org/abs/2107.14444",
          "publishedOn": "2021-08-02T01:58:23.439Z",
          "wordCount": 666,
          "title": "Manipulating Identical Filter Redundancy for Efficient Pruning on Deep and Complicated CNN. (arXiv:2107.14444v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Winston Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_T/0/1/0/all/0/1\">Tejas Shah</a>",
          "description": "Images acquired by computer vision systems under low light conditions have\nmultiple characteristics like high noise, lousy illumination, reflectance, and\nbad contrast, which make object detection tasks difficult. Much work has been\ndone to enhance images using various pixel manipulation techniques, as well as\ndeep neural networks - some focused on improving the illumination, while some\non reducing the noise. Similarly, considerable research has been done in object\ndetection neural network models. In our work, we break down the problem into\ntwo phases: 1)First, we explore which image enhancement algorithm is more\nsuited for object detection tasks, where accurate feature retrieval is more\nimportant than good image quality. Specifically, we look at basic histogram\nequalization techniques and unpaired image translation techniques. 2)In the\nsecond phase, we explore different object detection models that can be applied\nto the enhanced image. We conclude by comparing all results, calculating mean\naverage precisions (mAP), and giving some directions for future work.",
          "link": "http://arxiv.org/abs/2107.14382",
          "publishedOn": "2021-08-02T01:58:23.426Z",
          "wordCount": 587,
          "title": "Exploring Low-light Object Detection Techniques. (arXiv:2107.14382v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14287",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shilin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hieu Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>",
          "description": "While single image shadow detection has been improving rapidly in recent\nyears, video shadow detection remains a challenging task due to data scarcity\nand the difficulty in modelling temporal consistency. The current video shadow\ndetection method achieves this goal via co-attention, which mostly exploits\ninformation that is temporally coherent but is not robust in detecting moving\nshadows and small shadow regions. In this paper, we propose a simple but\npowerful method to better aggregate information temporally. We use an optical\nflow based warping module to align and then combine features between frames. We\napply this warping module across multiple deep-network layers to retrieve\ninformation from neighboring frames including both local details and high-level\nsemantic information. We train and test our framework on the ViSha dataset.\nExperimental results show that our model outperforms the state-of-the-art video\nshadow detection method by 28%, reducing BER from 16.7 to 12.0.",
          "link": "http://arxiv.org/abs/2107.14287",
          "publishedOn": "2021-08-02T01:58:23.418Z",
          "wordCount": 577,
          "title": "Temporal Feature Warping for Video Shadow Detection. (arXiv:2107.14287v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>",
          "description": "Correspondence selection aims to correctly select the consistent matches\n(inliers) from an initial set of putative correspondences. The selection is\nchallenging since putative matches are typically extremely unbalanced, largely\ndominated by outliers, and the random distribution of such outliers further\ncomplicates the learning process for learning-based methods. To address this\nissue, we propose to progressively prune the correspondences via a\nlocal-to-global consensus learning procedure. We introduce a ``pruning'' block\nthat lets us identify reliable candidates among the initial matches according\nto consensus scores estimated using local-to-global dynamic graphs. We then\nachieve progressive pruning by stacking multiple pruning blocks sequentially.\nOur method outperforms state-of-the-arts on robust line fitting, camera pose\nestimation and retrieval-based image localization benchmarks by significant\nmargins and shows promising generalization ability to different datasets and\ndetector/descriptor combinations.",
          "link": "http://arxiv.org/abs/2101.00591",
          "publishedOn": "2021-07-30T02:13:30.129Z",
          "wordCount": 607,
          "title": "Progressive Correspondence Pruning by Consensus Learning. (arXiv:2101.00591v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valassakis_E/0/1/0/all/0/1\">Eugene Valassakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palo_N/0/1/0/all/0/1\">Norman Di Palo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1\">Edward Johns</a>",
          "description": "In this paper, we study the problem of zero-shot sim-to-real when the task\nrequires both highly precise control with sub-millimetre error tolerance, and\nwide task space generalisation. Our framework involves a coarse-to-fine\ncontroller, where trajectories begin with classical motion planning using\nICP-based pose estimation, and transition to a learned end-to-end controller\nwhich maps images to actions and is trained in simulation with domain\nrandomisation. In this way, we achieve precise control whilst also generalising\nthe controller across wide task spaces, and keeping the robustness of\nvision-based, end-to-end control. Real-world experiments on a range of\ndifferent tasks show that, by exploiting the best of both worlds, our framework\nsignificantly outperforms purely motion planning methods, and purely\nlearning-based methods. Furthermore, we answer a range of questions on best\npractices for precise sim-to-real transfer, such as how different image sensor\nmodalities and image feature representations perform.",
          "link": "http://arxiv.org/abs/2105.11283",
          "publishedOn": "2021-07-30T02:13:29.558Z",
          "wordCount": 626,
          "title": "Coarse-to-Fine for Sim-to-Real: Sub-Millimetre Precision Across Wide Task Spaces. (arXiv:2105.11283v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Shuquan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Songfang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>",
          "description": "Point cloud segmentation is a fundamental task in 3D. Despite recent progress\non point cloud segmentation with the power of deep networks, current deep\nlearning methods based on the clean label assumptions may fail with noisy\nlabels. Yet, object class labels are often mislabeled in real-world point cloud\ndatasets. In this work, we take the lead in solving this issue by proposing a\nnovel Point Noise-Adaptive Learning (PNAL) framework. Compared to existing\nnoise-robust methods on image tasks, our PNAL is noise-rate blind, to cope with\nthe spatially variant noise rate problem specific to point clouds.\nSpecifically, we propose a novel point-wise confidence selection to obtain\nreliable labels based on the historical predictions of each point. A novel\ncluster-wise label correction is proposed with a voting strategy to generate\nthe best possible label taking the neighbor point correlations into\nconsideration. We conduct extensive experiments to demonstrate the\neffectiveness of PNAL on both synthetic and real-world noisy datasets. In\nparticular, even with $60\\%$ symmetric noisy labels, our proposed method\nproduces much better results than its baseline counterpart without PNAL and is\ncomparable to the ideal upper bound trained on a completely clean dataset.\nMoreover, we fully re-labeled the test set of a popular but noisy real-world\nscene dataset ScanNetV2 to make it clean, for rigorous experiment and future\nresearch. Our code and data will be available at\n\\url{https://shuquanye.com/PNAL_website/}.",
          "link": "http://arxiv.org/abs/2107.14230",
          "publishedOn": "2021-07-30T02:13:29.197Z",
          "wordCount": 683,
          "title": "Learning with Noisy Labels for Robust Point Cloud Segmentation. (arXiv:2107.14230v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luczynski_T/0/1/0/all/0/1\">Tomasz Luczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willners_J/0/1/0/all/0/1\">Jonatan Scharff Willners</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vargas_E/0/1/0/all/0/1\">Elizabeth Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roe_J/0/1/0/all/0/1\">Joshua Roe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shida Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petillot_Y/0/1/0/all/0/1\">Yvan Petillot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>",
          "description": "This paper presents a novel dataset for the development of visual navigation\nand simultaneous localisation and mapping (SLAM) algorithms as well as for\nunderwater intervention tasks. It differs from existing datasets as it contains\nground truth for the vehicle's position captured by an underwater motion\ntracking system. The dataset contains distortion-free and rectified stereo\nimages along with the calibration parameters of the stereo camera setup.\nFurthermore, the experiments were performed and recorded in a controlled\nenvironment, where current and waves could be generated allowing the dataset to\ncover a wide range of conditions - from calm water to waves and currents of\nsignificant strength.",
          "link": "http://arxiv.org/abs/2107.13628",
          "publishedOn": "2021-07-30T02:13:29.140Z",
          "wordCount": 542,
          "title": "Underwater inspection and intervention dataset. (arXiv:2107.13628v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shkodrani_S/0/1/0/all/0/1\">Sindi Shkodrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manfredi_M/0/1/0/all/0/1\">Marco Manfredi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baka_N/0/1/0/all/0/1\">N&#xf3;ra Baka</a>",
          "description": "Attempts of learning from hierarchical taxonomies in computer vision have\nbeen mostly focusing on image classification. Though ways of best harvesting\nlearning improvements from hierarchies in classification are far from being\nsolved, there is a need to target these problems in other vision tasks such as\nobject detection. As progress on the classification side is often dependent on\nhierarchical cross-entropy losses, novel detection architectures using sigmoid\nas an output function instead of softmax cannot easily apply these advances,\nrequiring novel methods in detection. In this work we establish a theoretical\nframework based on probability and set theory for extracting parent predictions\nand a hierarchical loss that can be used across tasks, showing results across\nclassification and detection benchmarks and opening up the possibility of\nhierarchical learning for sigmoid-based detection architectures.",
          "link": "http://arxiv.org/abs/2107.13627",
          "publishedOn": "2021-07-30T02:13:29.104Z",
          "wordCount": 574,
          "title": "United We Learn Better: Harvesting Learning Improvements From Class Hierarchies Across Tasks. (arXiv:2107.13627v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiangmiao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>",
          "description": "3D object detection is an important capability needed in various practical\napplications such as driver assistance systems. Monocular 3D detection, as an\neconomical solution compared to conventional settings relying on binocular\nvision or LiDAR, has drawn increasing attention recently but still yields\nunsatisfactory results. This paper first presents a systematic study on this\nproblem and observes that the current monocular 3D detection problem can be\nsimplified as an instance depth estimation problem: The inaccurate instance\ndepth blocks all the other 3D attribute predictions from improving the overall\ndetection performance. However, recent methods directly estimate the depth\nbased on isolated instances or pixels while ignoring the geometric relations\nacross different objects, which can be valuable constraints as the key\ninformation about depth is not directly manifest in the monocular image.\nTherefore, we construct geometric relation graphs across predicted objects and\nuse the graph to facilitate depth estimation. As the preliminary depth\nestimation of each instance is usually inaccurate in this ill-posed setting, we\nincorporate a probabilistic representation to capture the uncertainty. It\nprovides an important indicator to identify confident predictions and further\nguide the depth propagation. Despite the simplicity of the basic idea, our\nmethod obtains significant improvements on KITTI and nuScenes benchmarks,\nachieving the 1st place out of all monocular vision-only methods while still\nmaintaining real-time efficiency. Code and models will be released at\nhttps://github.com/open-mmlab/mmdetection3d.",
          "link": "http://arxiv.org/abs/2107.14160",
          "publishedOn": "2021-07-30T02:13:29.036Z",
          "wordCount": 662,
          "title": "Probabilistic and Geometric Depth: Detecting Objects in Perspective. (arXiv:2107.14160v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sabater_A/0/1/0/all/0/1\">Alberto Sabater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_L/0/1/0/all/0/1\">Laura Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_Victor_J/0/1/0/all/0/1\">Jose Santos-Victor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardino_A/0/1/0/all/0/1\">Alexandre Bernardino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1\">Luis Montesano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1\">Ana C. Murillo</a>",
          "description": "One-shot action recognition aims to recognize new action categories from a\nsingle reference example, typically referred to as the anchor example. This\nwork presents a novel approach for one-shot action recognition in the wild that\ncomputes motion representations robust to variable kinematic conditions.\nOne-shot action recognition is then performed by evaluating anchor and target\nmotion representations. We also develop a set of complementary steps that boost\nthe action recognition performance in the most challenging scenarios. Our\napproach is evaluated on the public NTU-120 one-shot action recognition\nbenchmark, outperforming previous action recognition models. Besides, we\nevaluate our framework on a real use-case of therapy with autistic people.\nThese recordings are particularly challenging due to high-level artifacts from\nthe patient motion. Our results provide not only quantitative but also online\nqualitative measures, essential for the patient evaluation and monitoring\nduring the actual therapy.",
          "link": "http://arxiv.org/abs/2102.08997",
          "publishedOn": "2021-07-30T02:13:28.989Z",
          "wordCount": 627,
          "title": "One-shot action recognition in challenging therapy scenarios. (arXiv:2102.08997v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Baobei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>",
          "description": "Depth completion deals with the problem of recovering dense depth maps from\nsparse ones, where color images are often used to facilitate this completion.\nRecent approaches mainly focus on image guided learning to predict dense\nresults. However, blurry image guidance and object structures in depth still\nimpede the performance of image guided frameworks. To tackle these problems, we\nexplore a repetitive design in our image guided network to sufficiently and\ngradually recover depth values. Specifically, the repetition is embodied in a\ncolor image guidance branch and a depth generation branch. In the former\nbranch, we design a repetitive hourglass network to extract higher-level image\nfeatures of complex environments, which can provide powerful context guidance\nfor depth prediction. In the latter branch, we design a repetitive guidance\nmodule based on dynamic convolution where the convolution factorization is\napplied to simultaneously reduce its complexity and progressively model\nhigh-frequency structures, e.g., boundaries. Further, in this module, we\npropose an adaptive fusion mechanism to effectively aggregate multi-step depth\nfeatures. Extensive experiments show that our method achieves state-of-the-art\nresult on the NYUv2 dataset and ranks 1st on the KITTI benchmark at the time of\nsubmission.",
          "link": "http://arxiv.org/abs/2107.13802",
          "publishedOn": "2021-07-30T02:13:28.982Z",
          "wordCount": 639,
          "title": "RigNet: Repetitive Image Guided Network for Depth Completion. (arXiv:2107.13802v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zeyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xuyang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jiaxiang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Runze Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jiayu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangyuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongbo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_C/0/1/0/all/0/1\">Chiew-Lan Tai</a>",
          "description": "In recent years, sparse voxel-based methods have become the state-of-the-arts\nfor 3D semantic segmentation of indoor scenes, thanks to the powerful 3D CNNs.\nNevertheless, being oblivious to the underlying geometry, voxel-based methods\nsuffer from ambiguous features on spatially close objects and struggle with\nhandling complex and irregular geometries due to the lack of geodesic\ninformation. In view of this, we present Voxel-Mesh Network (VMNet), a novel 3D\ndeep architecture that operates on the voxel and mesh representations\nleveraging both the Euclidean and geodesic information. Intuitively, the\nEuclidean information extracted from voxels can offer contextual cues\nrepresenting interactions between nearby objects, while the geodesic\ninformation extracted from meshes can help separate objects that are spatially\nclose but have disconnected surfaces. To incorporate such information from the\ntwo domains, we design an intra-domain attentive module for effective feature\naggregation and an inter-domain attentive module for adaptive feature fusion.\nExperimental results validate the effectiveness of VMNet: specifically, on the\nchallenging ScanNet dataset for large-scale segmentation of indoor scenes, it\noutperforms the state-of-the-art SparseConvNet and MinkowskiNet (74.6% vs 72.5%\nand 73.6% in mIoU) with a simpler network structure (17M vs 30M and 38M\nparameters). Code release: https://github.com/hzykent/VMNet",
          "link": "http://arxiv.org/abs/2107.13824",
          "publishedOn": "2021-07-30T02:13:28.957Z",
          "wordCount": 646,
          "title": "VMNet: Voxel-Mesh Network for Geodesic-Aware 3D Semantic Segmentation. (arXiv:2107.13824v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.05101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>",
          "description": "Ternary Neural Networks (TNNs) have received much attention due to being\npotentially orders of magnitude faster in inference, as well as more power\nefficient, than full-precision counterparts. However, 2 bits are required to\nencode the ternary representation with only 3 quantization levels leveraged. As\na result, conventional TNNs have similar memory consumption and speed compared\nwith the standard 2-bit models, but have worse representational capability.\nMoreover, there is still a significant gap in accuracy between TNNs and\nfull-precision networks, hampering their deployment to real applications. To\ntackle these two challenges, in this work, we first show that, under some mild\nconstraints, computational complexity of the ternary inner product can be\nreduced by a factor of 2. Second, to mitigate the performance gap, we\nelaborately design an implementation-dependent ternary quantization algorithm.\nThe proposed framework is termed Fast and Accurate Ternary Neural Networks\n(FATNN). Experiments on image classification demonstrate that our FATNN\nsurpasses the state-of-the-arts by a significant margin in accuracy. More\nimportantly, speedup evaluation compared with various precisions is analyzed on\nseveral platforms, which serves as a strong benchmark for further research.",
          "link": "http://arxiv.org/abs/2008.05101",
          "publishedOn": "2021-07-30T02:13:28.948Z",
          "wordCount": 669,
          "title": "FATNN: Fast and Accurate Ternary Neural Networks. (arXiv:2008.05101v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samani_E/0/1/0/all/0/1\">Ekta U. Samani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xingjian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Ashis G. Banerjee</a>",
          "description": "Object recognition in unseen indoor environments remains a challenging\nproblem for visual perception of mobile robots. In this letter, we propose the\nuse of topologically persistent features, which rely on the objects' shape\ninformation, to address this challenge. In particular, we extract two kinds of\nfeatures, namely, sparse persistence image (PI) and amplitude, by applying\npersistent homology to multi-directional height function-based filtrations of\nthe cubical complexes representing the object segmentation maps. The features\nare then used to train a fully connected network for recognition. For\nperformance evaluation, in addition to a widely used shape dataset and a\nbenchmark indoor scenes dataset, we collect a new dataset, comprising scene\nimages from two different environments, namely, a living room and a mock\nwarehouse. The scenes are captured using varying camera poses under different\nillumination conditions and include up to five different objects from a given\nset of fourteen objects. On the benchmark indoor scenes dataset, sparse PI\nfeatures show better recognition performance in unseen environments than the\nfeatures learned using the widely used ResNetV2-56 and EfficientNet-B4 models.\nFurther, they provide slightly higher recall and accuracy values than Faster\nR-CNN, an end-to-end object detection method, and its state-of-the-art variant,\nDomain Adaptive Faster R-CNN. The performance of our methods also remains\nrelatively unchanged from the training environment (living room) to the unseen\nenvironment (mock warehouse) in the new dataset. In contrast, the performance\nof the object detection methods drops substantially. We also implement the\nproposed method on a real-world robot to demonstrate its usefulness.",
          "link": "http://arxiv.org/abs/2010.03196",
          "publishedOn": "2021-07-30T02:13:28.943Z",
          "wordCount": 766,
          "title": "Visual Object Recognition in Indoor Environments Using Topologically Persistent Features. (arXiv:2010.03196v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaopeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chris Xiaoxuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengdi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peijun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1\">Niki Trigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1\">Andrew Markham</a>",
          "description": "Accurately describing and detecting 2D and 3D keypoints is crucial to\nestablishing correspondences across images and point clouds. Despite a plethora\nof learning-based 2D or 3D local feature descriptors and detectors having been\nproposed, the derivation of a shared descriptor and joint keypoint detector\nthat directly matches pixels and points remains under-explored by the\ncommunity. This work takes the initiative to establish fine-grained\ncorrespondences between 2D images and 3D point clouds. In order to directly\nmatch pixels and points, a dual fully convolutional framework is presented that\nmaps 2D and 3D inputs into a shared latent representation space to\nsimultaneously describe and detect keypoints. Furthermore, an ultra-wide\nreception mechanism in combination with a novel loss function are designed to\nmitigate the intrinsic information variations between pixel and point local\nregions. Extensive experimental results demonstrate that our framework shows\ncompetitive performance in fine-grained matching between images and point\nclouds and achieves state-of-the-art results for the task of indoor visual\nlocalization. Our source code will be available at [no-name-for-blind-review].",
          "link": "http://arxiv.org/abs/2103.01055",
          "publishedOn": "2021-07-30T02:13:28.934Z",
          "wordCount": 663,
          "title": "P2-Net: Joint Description and Detection of Local Features for Pixel and Point Matching. (arXiv:2103.01055v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01288",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tiange Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianhui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>",
          "description": "Discrete point cloud objects lack sufficient shape descriptors of 3D\ngeometries. In this paper, we present a novel method for aggregating\nhypothetical curves in point clouds. Sequences of connected points (curves) are\ninitially grouped by taking guided walks in the point clouds, and then\nsubsequently aggregated back to augment their point-wise features. We provide\nan effective implementation of the proposed aggregation strategy including a\nnovel curve grouping operator followed by a curve aggregation operator. Our\nmethod was benchmarked on several point cloud analysis tasks where we achieved\nthe state-of-the-art classification accuracy of 94.2% on the ModelNet40\nclassification task, instance IoU of 86.8 on the ShapeNetPart segmentation\ntask, and cosine error of 0.11 on the ModelNet40 normal estimation task.",
          "link": "http://arxiv.org/abs/2105.01288",
          "publishedOn": "2021-07-30T02:13:28.914Z",
          "wordCount": 592,
          "title": "Walk in the Cloud: Learning Curves for Point Clouds Shape Analysis. (arXiv:2105.01288v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wehrbein_T/0/1/0/all/0/1\">Tom Wehrbein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1\">Marco Rudolph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wandt_B/0/1/0/all/0/1\">Bastian Wandt</a>",
          "description": "3D human pose estimation from monocular images is a highly ill-posed problem\ndue to depth ambiguities and occlusions. Nonetheless, most existing works\nignore these ambiguities and only estimate a single solution. In contrast, we\ngenerate a diverse set of hypotheses that represents the full posterior\ndistribution of feasible 3D poses. To this end, we propose a normalizing flow\nbased method that exploits the deterministic 3D-to-2D mapping to solve the\nambiguous inverse 2D-to-3D problem. Additionally, uncertain detections and\nocclusions are effectively modeled by incorporating uncertainty information of\nthe 2D detector as condition. Further keys to success are a learned 3D pose\nprior and a generalization of the best-of-M loss. We evaluate our approach on\nthe two benchmark datasets Human3.6M and MPI-INF-3DHP, outperforming all\ncomparable methods in most metrics. The implementation is available on GitHub.",
          "link": "http://arxiv.org/abs/2107.13788",
          "publishedOn": "2021-07-30T02:13:28.908Z",
          "wordCount": 575,
          "title": "Probabilistic Monocular 3D Human Pose Estimation with Normalizing Flows. (arXiv:2107.13788v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianxiao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongbin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shane Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinmei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qiuda Yu</a>",
          "description": "Monocular depth estimation and semantic segmentation are two fundamental\ngoals of scene understanding. Due to the advantages of task interaction, many\nworks study the joint task learning algorithm. However, most existing methods\nfail to fully leverage the semantic labels, ignoring the provided context\nstructures and only using them to supervise the prediction of segmentation\nsplit. In this paper, we propose a network injected with contextual information\n(CI-Net) to solve the problem. Specifically, we introduce self-attention block\nin the encoder to generate attention map. With supervision from the ground\ntruth created by semantic labels, the network is embedded with contextual\ninformation so that it could understand the scene better, utilizing dependent\nfeatures to make accurate prediction. Besides, a feature sharing module is\nconstructed to make the task-specific features deeply fused and a consistency\nloss is devised to make the features mutually guided. We evaluate the proposed\nCI-Net on the NYU-Depth-v2 and SUN-RGBD datasets. The experimental results\nvalidate that our proposed CI-Net is competitive with the state-of-the-arts.",
          "link": "http://arxiv.org/abs/2107.13800",
          "publishedOn": "2021-07-30T02:13:28.902Z",
          "wordCount": 615,
          "title": "CI-Net: Contextual Information for Joint Semantic Segmentation and Depth Estimation. (arXiv:2107.13800v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.10860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muyao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yehui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>",
          "description": "With the rapid development of measurement technology, LiDAR and depth cameras\nare widely used in the perception of the 3D environment. Recent learning based\nmethods for robot perception most focus on the image or video, but deep\nlearning methods for dynamic 3D point cloud sequences are underexplored.\nTherefore, developing efficient and accurate perception method compatible with\nthese advanced instruments is pivotal to autonomous driving and service robots.\nAn Anchor-based Spatio-Temporal Attention 3D Convolution operation (ASTA3DConv)\nis proposed in this paper to process dynamic 3D point cloud sequences. The\nproposed convolution operation builds a regular receptive field around each\npoint by setting several virtual anchors around each point. The features of\nneighborhood points are firstly aggregated to each anchor based on the\nspatio-temporal attention mechanism. Then, anchor-based 3D convolution is\nadopted to aggregate these anchors' features to the core points. The proposed\nmethod makes better use of the structured information within the local region\nand learns spatio-temporal embedding features from dynamic 3D point cloud\nsequences. Anchor-based Spatio-Temporal Attention 3D Convolutional Neural\nNetworks (ASTA3DCNNs) are built for classification and segmentation tasks based\non the proposed ASTA3DConv and evaluated on action recognition and semantic\nsegmentation tasks. The experiments and ablation studies on MSRAction3D and\nSynthia datasets demonstrate the superior performance and effectiveness of our\nmethod for dynamic 3D point cloud sequences. Our method achieves the\nstate-of-the-art performance among the methods with dynamic 3D point cloud\nsequences as input on MSRAction3D and Synthia datasets.",
          "link": "http://arxiv.org/abs/2012.10860",
          "publishedOn": "2021-07-30T02:13:28.895Z",
          "wordCount": 724,
          "title": "Anchor-Based Spatio-Temporal Attention 3D Convolutional Networks for Dynamic 3D Point Cloud Sequences. (arXiv:2012.10860v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhihao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yinghong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruimao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>",
          "description": "Compared with the visual grounding on 2D images, the natural-language-guided\n3D object localization on point clouds is more challenging. In this paper, we\npropose a new model, named InstanceRefer, to achieve a superior 3D visual\ngrounding through the grounding-by-matching strategy. In practice, our model\nfirst predicts the target category from the language descriptions using a\nsimple language classification model. Then, based on the category, our model\nsifts out a small number of instance candidates (usually less than 20) from the\npanoptic segmentation of point clouds. Thus, the non-trivial 3D visual\ngrounding task has been effectively re-formulated as a simplified\ninstance-matching problem, considering that instance-level candidates are more\nrational than the redundant 3D object proposals. Subsequently, for each\ncandidate, we perform the multi-level contextual inference, i.e., referring\nfrom instance attribute perception, instance-to-instance relation perception,\nand instance-to-background global localization perception, respectively.\nEventually, the most relevant candidate is selected and localized by ranking\nconfidence scores, which are obtained by the cooperative holistic\nvisual-language feature matching. Experiments confirm that our method\noutperforms previous state-of-the-arts on ScanRefer online benchmark and\nNr3D/Sr3D datasets.",
          "link": "http://arxiv.org/abs/2103.01128",
          "publishedOn": "2021-07-30T02:13:28.879Z",
          "wordCount": 675,
          "title": "InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring. (arXiv:2103.01128v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiongchao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatemi_A/0/1/0/all/0/1\">Arezou Fatemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lira_W/0/1/0/all/0/1\">Wallace Lira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fenggen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_B/0/1/0/all/0/1\">Biao Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1\">Ali Mahdavi-Amiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "We introduce RaidaR, a rich annotated image dataset of rainy street scenes,\nto support autonomous driving research. The new dataset contains the largest\nnumber of rainy images (58,542) to date, 5,000 of which provide semantic\nsegmentations and 3,658 provide object instance segmentations. The RaidaR\nimages cover a wide range of realistic rain-induced artifacts, including fog,\ndroplets, and road reflections, which can effectively augment existing street\nscene datasets to improve data-driven machine perception during rainy weather.\nTo facilitate efficient annotation of a large volume of images, we develop a\nsemi-automatic scheme combining manual segmentation and an automated processing\nakin to cross validation, resulting in 10-20 fold reduction on annotation time.\nWe demonstrate the utility of our new dataset by showing how data augmentation\nwith RaidaR can elevate the accuracy of existing segmentation algorithms. We\nalso present a novel unpaired image-to-image translation algorithm for\nadding/removing rain artifacts, which directly benefits from RaidaR.",
          "link": "http://arxiv.org/abs/2104.04606",
          "publishedOn": "2021-07-30T02:13:28.873Z",
          "wordCount": 635,
          "title": "RaidaR: A Rich Annotated Image Dataset of Rainy Street Scenes. (arXiv:2104.04606v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Willes_J/0/1/0/all/0/1\">John Willes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_J/0/1/0/all/0/1\">James Harrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harakeh_A/0/1/0/all/0/1\">Ali Harakeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1\">Steven Waslander</a>",
          "description": "As autonomous decision-making agents move from narrow operating environments\nto unstructured worlds, learning systems must move from a closed-world\nformulation to an open-world and few-shot setting in which agents continuously\nlearn new classes from small amounts of information. This stands in stark\ncontrast to modern machine learning systems that are typically designed with a\nknown set of classes and a large number of examples for each class. In this\nwork we extend embedding-based few-shot learning algorithms to the open-world\nrecognition setting. We combine Bayesian non-parametric class priors with an\nembedding-based pre-training scheme to yield a highly flexible framework which\nwe refer to as few-shot learning for open world recognition (FLOWR). We\nbenchmark our framework on open-world extensions of the common MiniImageNet and\nTieredImageNet few-shot learning datasets. Our results show, compared to prior\nmethods, strong classification accuracy performance and up to a 12% improvement\nin H-measure (a measure of novel class detection) from our non-parametric\nopen-world few-shot learning scheme.",
          "link": "http://arxiv.org/abs/2107.13682",
          "publishedOn": "2021-07-30T02:13:28.867Z",
          "wordCount": 596,
          "title": "Bayesian Embeddings for Few-Shot Open World Recognition. (arXiv:2107.13682v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wenpeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuemiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>",
          "description": "Existing GAN inversion methods are stuck in a paradox that the inverted codes\ncan either achieve high-fidelity reconstruction, or retain the editing\ncapability. Having only one of them clearly cannot realize real image editing.\nIn this paper, we resolve this paradox by introducing consecutive images (\\eg,\nvideo frames or the same person with different poses) into the inversion\nprocess. The rationale behind our solution is that the continuity of\nconsecutive images leads to inherent editable directions. This inborn property\nis used for two unique purposes: 1) regularizing the joint inversion process,\nsuch that each of the inverted code is semantically accessible from one of the\nother and fastened in a editable domain; 2) enforcing inter-image coherence,\nsuch that the fidelity of each inverted code can be maximized with the\ncomplement of other images. Extensive experiments demonstrate that our\nalternative significantly outperforms state-of-the-art methods in terms of\nreconstruction fidelity and editability on both the real image dataset and\nsynthesis dataset. Furthermore, our method provides the first support of\nvideo-based GAN inversion, and an interesting application of unsupervised\nsemantic transfer from consecutive images. Source code can be found at:\n\\url{https://github.com/Qingyang-Xu/InvertingGANs_with_ConsecutiveImgs}.",
          "link": "http://arxiv.org/abs/2107.13812",
          "publishedOn": "2021-07-30T02:13:28.852Z",
          "wordCount": 636,
          "title": "From Continuity to Editability: Inverting GANs with Consecutive Images. (arXiv:2107.13812v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raman_C/0/1/0/all/0/1\">Chirag Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_H/0/1/0/all/0/1\">Hayley Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loog_M/0/1/0/all/0/1\">Marco Loog</a>",
          "description": "The default paradigm for the forecasting of human behavior in social\nconversations is characterized by top-down approaches. These involve\nidentifying predictive relationships between low level nonverbal cues and\nfuture semantic events of interest (e.g. turn changes, group leaving). A common\nhurdle however, is the limited availability of labeled data for supervised\nlearning. In this work, we take the first step in the direction of a bottom-up\nself-supervised approach in the domain. We formulate the task of Social Cue\nForecasting to leverage the larger amount of unlabeled low-level behavior cues,\nand characterize the modeling challenges involved. To address these, we take a\nmeta-learning approach and propose the Social Process (SP) models--socially\naware sequence-to-sequence (Seq2Seq) models within the Neural Process (NP)\nfamily. SP models learn extractable representations of non-semantic future cues\nfor each participant, while capturing global uncertainty by jointly reasoning\nabout the future for all members of the group. Evaluation on synthesized and\nreal-world behavior data shows that our SP models achieve higher log-likelihood\nthan the NP baselines, and also highlights important considerations for\napplying such techniques within the domain of social human interactions.",
          "link": "http://arxiv.org/abs/2107.13576",
          "publishedOn": "2021-07-30T02:13:28.836Z",
          "wordCount": 628,
          "title": "Social Processes: Self-Supervised Forecasting of Nonverbal Cues in Social Conversations. (arXiv:2107.13576v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03725",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Ping Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yujin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangyin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1\">Qingpei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yong Tan</a>",
          "description": "Reconstructing a high-precision and high-fidelity 3D human hand from a color\nimage plays a central role in replicating a realistic virtual hand in\nhuman-computer interaction and virtual reality applications. The results of\ncurrent methods are lacking in accuracy and fidelity due to various hand poses\nand severe occlusions. In this study, we propose an I2UV-HandNet model for\naccurate hand pose and shape estimation as well as 3D hand super-resolution\nreconstruction. Specifically, we present the first UV-based 3D hand shape\nrepresentation. To recover a 3D hand mesh from an RGB image, we design an\nAffineNet to predict a UV position map from the input in an image-to-image\ntranslation fashion. To obtain a higher fidelity shape, we exploit an\nadditional SRNet to transform the low-resolution UV map outputted by AffineNet\ninto a high-resolution one. For the first time, we demonstrate the\ncharacterization capability of the UV-based hand shape representation. Our\nexperiments show that the proposed method achieves state-of-the-art performance\non several challenging benchmarks.",
          "link": "http://arxiv.org/abs/2102.03725",
          "publishedOn": "2021-07-30T02:13:28.830Z",
          "wordCount": 648,
          "title": "I2UV-HandNet: Image-to-UV Prediction Network for Accurate and High-fidelity 3D Hand Mesh Modeling. (arXiv:2102.03725v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.08218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xianyu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1\">Ziwei Liao</a>",
          "description": "Plane feature is a kind of stable landmark to reduce drift error in SLAM\nsystem. It is easy and fast to extract planes from dense point cloud, which is\ncommonly acquired from RGB-D camera or lidar. But for stereo camera, it is hard\nto compute dense point cloud accurately and efficiently. In this paper, we\npropose a novel method to compute plane parameters using intersecting lines\nwhich are extracted from the stereo image. The plane features commonly exist on\nthe surface of man-made objects and structure, which have regular shape and\nstraight edge lines. In 3D space, two intersecting lines can determine such a\nplane. Thus we extract line segments from both stereo left and right image. By\nstereo matching, we compute the endpoints and line directions in 3D space, and\nthen the planes from two intersecting lines. We discard those inaccurate plane\nfeatures in the frame tracking. Adding such plane features in stereo SLAM\nsystem reduces the drift error and refines the performance. We test our\nproposed system on public datasets and demonstrate its robust and accurate\nestimation results, compared with state-of-the-art SLAM systems. To benefit the\nresearch of plane-based SLAM, we release our codes at\nhttps://github.com/fishmarch/Stereo-Plane-SLAM.",
          "link": "http://arxiv.org/abs/2008.08218",
          "publishedOn": "2021-07-30T02:13:28.808Z",
          "wordCount": 675,
          "title": "Stereo Plane SLAM Based on Intersecting Lines. (arXiv:2008.08218v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qingtian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_C/0/1/0/all/0/1\">Chen Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zizhuang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yisong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoping Wang</a>",
          "description": "3D reconstruction has lately attracted increasing attention due to its wide\napplication in many areas, such as autonomous driving, robotics and virtual\nreality. As a dominant technique in artificial intelligence, deep learning has\nbeen successfully adopted to solve various computer vision problems. However,\ndeep learning for 3D reconstruction is still at its infancy due to its unique\nchallenges and varying pipelines. To stimulate future research, this paper\npresents a review of recent progress in deep learning methods for Multi-view\nStereo (MVS), which is considered as a crucial task of image-based 3D\nreconstruction. It also presents comparative results on several publicly\navailable datasets, with insightful observations and inspiring future research\ndirections.",
          "link": "http://arxiv.org/abs/2106.15328",
          "publishedOn": "2021-07-30T02:13:28.801Z",
          "wordCount": 568,
          "title": "Deep Learning for Multi-View Stereo via Plane Sweep: A Survey. (arXiv:2106.15328v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuen_J/0/1/0/all/0/1\">Jason Kuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "We introduce a new image segmentation task, termed Entity Segmentation (ES)\nwith the aim to segment all visual entities in an image without considering\nsemantic category labels. It has many practical applications in image\nmanipulation/editing where the segmentation mask quality is typically crucial\nbut category labels are less important. In this setting, all\nsemantically-meaningful segments are equally treated as categoryless entities\nand there is no thing-stuff distinction. Based on our unified entity\nrepresentation, we propose a center-based entity segmentation framework with\ntwo novel modules to improve mask quality. Experimentally, both our new task\nand framework demonstrate superior advantages as against existing work. In\nparticular, ES enables the following: (1) merging multiple datasets to form a\nlarge training set without the need to resolve label conflicts; (2) any model\ntrained on one dataset can generalize exceptionally well to other datasets with\nunseen domains. Our code is made publicly available at\nhttps://github.com/dvlab-research/Entity.",
          "link": "http://arxiv.org/abs/2107.14228",
          "publishedOn": "2021-07-30T02:13:28.778Z",
          "wordCount": 595,
          "title": "Open-World Entity Segmentation. (arXiv:2107.14228v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.14119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1\">Emanuel Ben-Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1\">Tal Ridnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_N/0/1/0/all/0/1\">Nadav Zamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_I/0/1/0/all/0/1\">Itamar Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protter_M/0/1/0/all/0/1\">Matan Protter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1\">Lihi Zelnik-Manor</a>",
          "description": "In a typical multi-label setting, a picture contains on average few positive\nlabels, and many negative ones. This positive-negative imbalance dominates the\noptimization process, and can lead to under-emphasizing gradients from positive\nlabels during training, resulting in poor accuracy. In this paper, we introduce\na novel asymmetric loss (\"ASL\"), which operates differently on positive and\nnegative samples. The loss enables to dynamically down-weights and\nhard-thresholds easy negative samples, while also discarding possibly\nmislabeled samples. We demonstrate how ASL can balance the probabilities of\ndifferent samples, and how this balancing is translated to better mAP scores.\nWith ASL, we reach state-of-the-art results on multiple popular multi-label\ndatasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate\nASL applicability for other tasks, such as single-label classification and\nobject detection. ASL is effective, easy to implement, and does not increase\nthe training time or complexity.\n\nImplementation is available at: https://github.com/Alibaba-MIIL/ASL.",
          "link": "http://arxiv.org/abs/2009.14119",
          "publishedOn": "2021-07-30T02:13:28.754Z",
          "wordCount": 650,
          "title": "Asymmetric Loss For Multi-Label Classification. (arXiv:2009.14119v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pizzati_F/0/1/0/all/0/1\">Fabio Pizzati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerri_P/0/1/0/all/0/1\">Pietro Cerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>",
          "description": "Image-to-image translation (i2i) networks suffer from entanglement effects in\npresence of physics-related phenomena in target domain (such as occlusions,\nfog, etc), thus lowering the translation quality and variability. In this\npaper, we present a comprehensive method for disentangling physics-based traits\nin the translation, guiding the learning process with neural or physical\nmodels. For the latter, we integrate adversarial estimation and genetic\nalgorithms to correctly achieve disentanglement. The results show our approach\ndramatically increase performances in many challenging scenarios for image\ntranslation.",
          "link": "http://arxiv.org/abs/2107.14229",
          "publishedOn": "2021-07-30T02:13:28.747Z",
          "wordCount": 524,
          "title": "Guided Disentanglement in Generative Networks. (arXiv:2107.14229v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1\">Nuoxing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liangliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>",
          "description": "Trajectory prediction is confronted with the dilemma to capture the\nmulti-modal nature of future dynamics with both diversity and accuracy. In this\npaper, we present a distribution discrimination (DisDis) method to predict\npersonalized motion patterns by distinguishing the potential distributions.\nMotivated by that the motion pattern of each person is personalized due to\nhis/her habit, our DisDis learns the latent distribution to represent different\nmotion patterns and optimize it by the contrastive discrimination. This\ndistribution discrimination encourages latent distributions to be more\ndiscriminative. Our method can be integrated with existing multi-modal\nstochastic predictive models as a plug-and-play module to learn the more\ndiscriminative latent distribution. To evaluate the latent distribution, we\nfurther propose a new metric, probability cumulative minimum distance (PCMD)\ncurve, which cumulatively calculates the minimum distance on the sorted\nprobabilities. Experimental results on the ETH and UCY datasets show the\neffectiveness of our method.",
          "link": "http://arxiv.org/abs/2107.14204",
          "publishedOn": "2021-07-30T02:13:28.729Z",
          "wordCount": 589,
          "title": "Personalized Trajectory Prediction via Distribution Discrimination. (arXiv:2107.14204v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.08773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_E/0/1/0/all/0/1\">Eddie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>",
          "description": "Data augmentations are important ingredients in the recipe for training\nrobust neural networks, especially in computer vision. A fundamental question\nis whether neural network features encode data augmentation transformations. To\nanswer this question, we introduce a systematic approach to investigate which\nlayers of neural networks are the most predictive of augmentation\ntransformations. Our approach uses features in pre-trained vision models with\nminimal additional processing to predict common properties transformed by\naugmentation (scale, aspect ratio, hue, saturation, contrast, and brightness).\nSurprisingly, neural network features not only predict data augmentation\ntransformations, but they predict many transformations with high accuracy.\nAfter validating that neural networks encode features corresponding to\naugmentation transformations, we show that these features are encoded in the\nearly layers of modern CNNs, though the augmentation signal fades in deeper\nlayers.",
          "link": "http://arxiv.org/abs/2003.08773",
          "publishedOn": "2021-07-30T02:13:28.721Z",
          "wordCount": 601,
          "title": "Do CNNs Encode Data Augmentations?. (arXiv:2003.08773v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thoduka_S/0/1/0/all/0/1\">Santosh Thoduka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ploger_P/0/1/0/all/0/1\">Paul G. Pl&#xf6;ger</a>",
          "description": "Execution monitoring is essential for robots to detect and respond to\nfailures. Since it is impossible to enumerate all failures for a given task, we\nlearn from successful executions of the task to detect visual anomalies during\nruntime. Our method learns to predict the motions that occur during the nominal\nexecution of a task, including camera and robot body motion. A probabilistic\nU-Net architecture is used to learn to predict optical flow, and the robot's\nkinematics and 3D model are used to model camera and body motion. The errors\nbetween the observed and predicted motion are used to calculate an anomaly\nscore. We evaluate our method on a dataset of a robot placing a book on a\nshelf, which includes anomalies such as falling books, camera occlusions, and\nrobot disturbances. We find that modeling camera and body motion, in addition\nto the learning-based optical flow prediction, results in an improvement of the\narea under the receiver operating characteristic curve from 0.752 to 0.804, and\nthe area under the precision-recall curve from 0.467 to 0.549.",
          "link": "http://arxiv.org/abs/2107.14206",
          "publishedOn": "2021-07-30T02:13:28.698Z",
          "wordCount": 624,
          "title": "Using Visual Anomaly Detection for Task Execution Monitoring. (arXiv:2107.14206v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00179",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunhui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1\">Davide Modolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>",
          "description": "Most action recognition solutions rely on dense sampling to precisely cover\nthe informative temporal clip. Extensively searching temporal region is\nexpensive for a real-world application. In this work, we focus on improving the\ninference efficiency of current action recognition backbones on trimmed videos,\nand illustrate that one action model can also cover then informative region by\ndropping non-informative features. We present Selective Feature Compression\n(SFC), an action recognition inference strategy that greatly increase model\ninference efficiency without any accuracy compromise. Differently from previous\nworks that compress kernel sizes and decrease the channel dimension, we propose\nto compress feature flow at spatio-temporal dimension without changing any\nbackbone parameters. Our experiments on Kinetics-400, UCF101 and ActivityNet\nshow that SFC is able to reduce inference speed by 6-7x and memory usage by\n5-6x compared with the commonly used 30 crops dense sampling procedure, while\nalso slightly improving Top1 Accuracy. We thoroughly quantitatively and\nqualitatively evaluate SFC and all its components and show how does SFC learn\nto attend to important video regions and to drop temporal features that are\nuninformative for the task of action recognition.",
          "link": "http://arxiv.org/abs/2104.00179",
          "publishedOn": "2021-07-30T02:13:28.681Z",
          "wordCount": 654,
          "title": "Selective Feature Compression for Efficient Activity Recognition Inference. (arXiv:2104.00179v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02303",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sabater_A/0/1/0/all/0/1\">Alberto Sabater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_I/0/1/0/all/0/1\">I&#xf1;igo Alonso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1\">Luis Montesano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1\">Ana C. Murillo</a>",
          "description": "Hand action recognition is a special case of action recognition with\napplications in human-robot interaction, virtual reality or life-logging\nsystems. Building action classifiers able to work for such heterogeneous action\ndomains is very challenging. There are very subtle changes across different\nactions from a given application but also large variations across domains (e.g.\nvirtual reality vs life-logging). This work introduces a novel skeleton-based\nhand motion representation model that tackles this problem. The framework we\npropose is agnostic to the application domain or camera recording view-point.\nWhen working on a single domain (intra-domain action classification) our\napproach performs better or similar to current state-of-the-art methods on\nwell-known hand action recognition benchmarks. And, more importantly, when\nperforming hand action recognition for action domains and camera perspectives\nwhich our approach has not been trained for (cross-domain action\nclassification), our proposed framework achieves comparable performance to\nintra-domain state-of-the-art methods. These experiments show the robustness\nand generalization capabilities of our framework.",
          "link": "http://arxiv.org/abs/2103.02303",
          "publishedOn": "2021-07-30T02:13:28.658Z",
          "wordCount": 619,
          "title": "Domain and View-point Agnostic Hand Action Recognition. (arXiv:2103.02303v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1\">Aditya Jyoti Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Smaranjit Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1\">Kanishka Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nethaji_N/0/1/0/all/0/1\">Niketha Nethaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Shivam Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purkayastha_A/0/1/0/all/0/1\">Arnab Dutta Purkayastha</a>",
          "description": "Tourism in India plays a quintessential role in the country's economy with an\nestimated 9.2% GDP share for the year 2018. With a yearly growth rate of 6.2%,\nthe industry holds a huge potential for being the primary driver of the economy\nas observed in the nations of the Middle East like the United Arab Emirates.\nThe historical and cultural diversity exhibited throughout the geography of the\nnation is a unique spectacle for people around the world and therefore serves\nto attract tourists in tens of millions in number every year. Traditionally,\ntour guides or academic professionals who study these heritage monuments were\nresponsible for providing information to the visitors regarding their\narchitectural and historical significance. However, unfortunately this system\nhas several caveats when considered on a large scale such as unavailability of\nsufficient trained people, lack of accurate information, failure to convey the\nrichness of details in an attractive format etc. Recently, machine learning\napproaches revolving around the usage of monument pictures have been shown to\nbe useful for rudimentary analysis of heritage sights. This paper serves as a\nsurvey of the research endeavors undertaken in this direction which would\neventually provide insights for building an automated decision system that\ncould be utilized to make the experience of tourism in India more modernized\nfor visitors.",
          "link": "http://arxiv.org/abs/2107.14070",
          "publishedOn": "2021-07-30T02:13:28.638Z",
          "wordCount": 690,
          "title": "Machine Learning Advances aiding Recognition and Classification of Indian Monuments and Landmarks. (arXiv:2107.14070v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuewen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingru Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>",
          "description": "Image captioning is shown to be able to achieve a better performance by using\nscene graphs to represent the relations of objects in the image. The current\ncaptioning encoders generally use a Graph Convolutional Net (GCN) to represent\nthe relation information and merge it with the object region features via\nconcatenation or convolution to get the final input for sentence decoding.\nHowever, the GCN-based encoders in the existing methods are less effective for\ncaptioning due to two reasons. First, using the image captioning as the\nobjective (i.e., Maximum Likelihood Estimation) rather than a relation-centric\nloss cannot fully explore the potential of the encoder. Second, using a\npre-trained model instead of the encoder itself to extract the relationships is\nnot flexible and cannot contribute to the explainability of the model. To\nimprove the quality of image captioning, we propose a novel architecture\nReFormer -- a RElational transFORMER to generate features with relation\ninformation embedded and to explicitly express the pair-wise relationships\nbetween objects in the image. ReFormer incorporates the objective of scene\ngraph generation with that of image captioning using one modified Transformer\nmodel. This design allows ReFormer to generate not only better image captions\nwith the bene-fit of extracting strong relational image features, but also\nscene graphs to explicitly describe the pair-wise relation-ships. Experiments\non publicly available datasets show that our model significantly outperforms\nstate-of-the-art methods on image captioning and scene graph generation",
          "link": "http://arxiv.org/abs/2107.14178",
          "publishedOn": "2021-07-30T02:13:28.600Z",
          "wordCount": 664,
          "title": "ReFormer: The Relational Transformer for Image Captioning. (arXiv:2107.14178v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">Eugene Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Cheng-Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yi Lee</a>",
          "description": "Deep neural networks (DNNs) are known to perform well when deployed to test\ndistributions that shares high similarity with the training distribution.\nFeeding DNNs with new data sequentially that were unseen in the training\ndistribution has two major challenges -- fast adaptation to new tasks and\ncatastrophic forgetting of old tasks. Such difficulties paved way for the\non-going research on few-shot learning and continual learning. To tackle these\nproblems, we introduce Attentive Independent Mechanisms (AIM). We incorporate\nthe idea of learning using fast and slow weights in conjunction with the\ndecoupling of the feature extraction and higher-order conceptual learning of a\nDNN. AIM is designed for higher-order conceptual learning, modeled by a mixture\nof experts that compete to learn independent concepts to solve a new task. AIM\nis a modular component that can be inserted into existing deep learning\nframeworks. We demonstrate its capability for few-shot learning by adding it to\nSIB and trained on MiniImageNet and CIFAR-FS, showing significant improvement.\nAIM is also applied to ANML and OML trained on Omniglot, CIFAR-100 and\nMiniImageNet to demonstrate its capability in continual learning. Code made\npublicly available at https://github.com/huang50213/AIM-Fewshot-Continual.",
          "link": "http://arxiv.org/abs/2107.14053",
          "publishedOn": "2021-07-30T02:13:28.594Z",
          "wordCount": 643,
          "title": "Few-Shot and Continual Learning with Attentive Independent Mechanisms. (arXiv:2107.14053v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.10143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tomen_N/0/1/0/all/0/1\">Nergis Tomen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan van Gemert</a>",
          "description": "Convolutional layers in CNNs implement linear filters which decompose the\ninput into different frequency bands. However, most modern architectures\nneglect standard principles of filter design when optimizing their model\nchoices regarding the size and shape of the convolutional kernel. In this work,\nwe consider the well-known problem of spectral leakage caused by windowing\nartifacts in filtering operations in the context of CNNs. We show that the\nsmall size of CNN kernels make them susceptible to spectral leakage, which may\ninduce performance-degrading artifacts. To address this issue, we propose the\nuse of larger kernel sizes along with the Hamming window function to alleviate\nleakage in CNN architectures. We demonstrate improved classification accuracy\non multiple benchmark datasets including Fashion-MNIST, CIFAR-10, CIFAR-100 and\nImageNet with the simple use of a standard window function in convolutional\nlayers. Finally, we show that CNNs employing the Hamming window display\nincreased robustness against various adversarial attacks.",
          "link": "http://arxiv.org/abs/2101.10143",
          "publishedOn": "2021-07-30T02:13:28.535Z",
          "wordCount": 615,
          "title": "Spectral Leakage and Rethinking the Kernel Size in CNNs. (arXiv:2101.10143v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1\">Juan C. P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeanneret_G/0/1/0/all/0/1\">Guillaume Jeanneret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueda_L/0/1/0/all/0/1\">Laura Rueda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1\">Pablo Arbel&#xe1;ez</a>",
          "description": "Deep learning models are prone to being fooled by imperceptible perturbations\nknown as adversarial attacks. In this work, we study how equipping models with\nTest-time Transformation Ensembling (TTE) can work as a reliable defense\nagainst such attacks. While transforming the input data, both at train and test\ntimes, is known to enhance model performance, its effects on adversarial\nrobustness have not been studied. Here, we present a comprehensive empirical\nstudy of the impact of TTE, in the form of widely-used image transforms, on\nadversarial robustness. We show that TTE consistently improves model robustness\nagainst a variety of powerful attacks without any need for re-training, and\nthat this improvement comes at virtually no trade-off with accuracy on clean\nsamples. Finally, we show that the benefits of TTE transfer even to the\ncertified robustness domain, in which TTE provides sizable and consistent\nimprovements.",
          "link": "http://arxiv.org/abs/2107.14110",
          "publishedOn": "2021-07-30T02:13:28.493Z",
          "wordCount": 588,
          "title": "Enhancing Adversarial Robustness via Test-time Transformation Ensembling. (arXiv:2107.14110v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Houwen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Minghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1\">Hongyang Chao</a>",
          "description": "Relative position encoding (RPE) is important for transformer to capture\nsequence ordering of input tokens. General efficacy has been proven in natural\nlanguage processing. However, in computer vision, its efficacy is not well\nstudied and even remains controversial, e.g., whether relative position\nencoding can work equally well as absolute position? In order to clarify this,\nwe first review existing relative position encoding methods and analyze their\npros and cons when applied in vision transformers. We then propose new relative\nposition encoding methods dedicated to 2D images, called image RPE (iRPE). Our\nmethods consider directional relative distance modeling as well as the\ninteractions between queries and relative position embeddings in self-attention\nmechanism. The proposed iRPE methods are simple and lightweight. They can be\neasily plugged into transformer blocks. Experiments demonstrate that solely due\nto the proposed encoding methods, DeiT and DETR obtain up to 1.5% (top-1 Acc)\nand 1.3% (mAP) stable improvements over their original versions on ImageNet and\nCOCO respectively, without tuning any extra hyperparameters such as learning\nrate and weight decay. Our ablation and analysis also yield interesting\nfindings, some of which run counter to previous understanding. Code and models\nare open-sourced at https://github.com/microsoft/Cream/tree/main/iRPE.",
          "link": "http://arxiv.org/abs/2107.14222",
          "publishedOn": "2021-07-30T02:13:28.486Z",
          "wordCount": 641,
          "title": "Rethinking and Improving Relative Position Encoding for Vision Transformer. (arXiv:2107.14222v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_N/0/1/0/all/0/1\">Nikhil Woodruff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Enshaei_A/0/1/0/all/0/1\">Amir Enshaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_B/0/1/0/all/0/1\">Bashar Awwad Shiekh Hasan</a>",
          "description": "Signatures present on corporate documents are often used in investigations of\nrelationships between persons of interest, and prior research into the task of\noffline signature verification has evaluated a wide range of methods on\nstandard signature datasets. However, such tasks often benefit from prior human\nsupervision in the collection, adjustment and labelling of isolated signature\nimages from which all real-world context has been removed. Signatures found in\nonline document repositories such as the United Kingdom Companies House\nregularly contain high variation in location, size, quality and degrees of\nobfuscation under stamps. We propose an integrated pipeline of signature\nextraction and curation, with no human assistance from the obtaining of company\ndocuments to the clustering of individual signatures. We use a sequence of\nheuristic methods, convolutional neural networks, generative adversarial\nnetworks and convolutional Siamese networks for signature extraction,\nfiltering, cleaning and embedding respectively. We evaluate both the\neffectiveness of the pipeline at matching obscured same-author signature pairs\nand the effectiveness of the entire pipeline against a human baseline for\ndocument signature analysis, as well as presenting uses for such a pipeline in\nthe field of real-world anti-money laundering investigation.",
          "link": "http://arxiv.org/abs/2107.14091",
          "publishedOn": "2021-07-30T02:13:28.479Z",
          "wordCount": 629,
          "title": "Fully-Automatic Pipeline for Document Signature Analysis to Detect Money Laundering Activities. (arXiv:2107.14091v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fangrui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chongruo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>",
          "description": "Semantic segmentation is a challenging problem due to difficulties in\nmodeling context in complex scenes and class confusions along boundaries. Most\nliterature either focuses on context modeling or boundary refinement, which is\nless generalizable in open-world scenarios. In this work, we advocate a unified\nframework(UN-EPT) to segment objects by considering both context information\nand boundary artifacts. We first adapt a sparse sampling strategy to\nincorporate the transformer-based attention mechanism for efficient context\nmodeling. In addition, a separate spatial branch is introduced to capture image\ndetails for boundary refinement. The whole model can be trained in an\nend-to-end manner. We demonstrate promising performance on three popular\nbenchmarks for semantic segmentation with low memory footprint. Code will be\nreleased soon.",
          "link": "http://arxiv.org/abs/2107.14209",
          "publishedOn": "2021-07-30T02:13:28.472Z",
          "wordCount": 558,
          "title": "A Unified Efficient Pyramid Transformer for Semantic Segmentation. (arXiv:2107.14209v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.03972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_H/0/1/0/all/0/1\">Haizhou Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Rui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1\">Zijie Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuang Liu</a>",
          "description": "Estimating 3D poses of multiple humans in real-time is a classic but still\nchallenging task in computer vision. Its major difficulty lies in the ambiguity\nin cross-view association of 2D poses and the huge state space when there are\nmultiple people in multiple views. In this paper, we present a novel solution\nfor multi-human 3D pose estimation from multiple calibrated camera views. It\ntakes 2D poses in different camera coordinates as inputs and aims for the\naccurate 3D poses in the global coordinate. Unlike previous methods that\nassociate 2D poses among all pairs of views from scratch at every frame, we\nexploit the temporal consistency in videos to match the 2D inputs with 3D poses\ndirectly in 3-space. More specifically, we propose to retain the 3D pose for\neach person and update them iteratively via the cross-view multi-human\ntracking. This novel formulation improves both accuracy and efficiency, as we\ndemonstrated on widely-used public datasets. To further verify the scalability\nof our method, we propose a new large-scale multi-human dataset with 12 to 28\ncamera views. Without bells and whistles, our solution achieves 154 FPS on 12\ncameras and 34 FPS on 28 cameras, indicating its ability to handle large-scale\nreal-world applications. The proposed dataset is released at\nhttps://github.com/longcw/crossview_3d_pose_tracking.",
          "link": "http://arxiv.org/abs/2003.03972",
          "publishedOn": "2021-07-30T02:13:28.456Z",
          "wordCount": 703,
          "title": "Cross-View Tracking for Multi-Human 3D Pose Estimation at over 100 FPS. (arXiv:2003.03972v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14175",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Basty_N/0/1/0/all/0/1\">Nicolas Basty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thanaj_M/0/1/0/all/0/1\">Marjola Thanaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cule_M/0/1/0/all/0/1\">Madeleine Cule</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sorokin_E/0/1/0/all/0/1\">Elena P. Sorokin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bell_J/0/1/0/all/0/1\">Jimmy D. Bell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thomas_E/0/1/0/all/0/1\">E. Louise Thomas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Whitcher_B/0/1/0/all/0/1\">Brandon Whitcher</a>",
          "description": "Dixon MRI is widely used for body composition studies. Current processing\nmethods associated with large whole-body volumes are time intensive and prone\nto artifacts during fat-water separation performed on the scanner, making the\ndata difficult to analyse. The most common artifact are fat-water swaps, where\nthe labels are inverted at the voxel level. It is common for researchers to\ndiscard swapped data (generally around 10%), which can be wasteful and lead to\nunintended biases. The UK Biobank is acquiring Dixon MRI for over 100,000\nparticipants, and thousands of swaps will occur. If those go undetected, errors\nwill propagate into processes such as abdominal organ segmentation and dilute\nthe results in population-based analyses. There is a clear need for a fast and\nrobust method to accurately separate fat and water channels. In this work we\npropose such a method based on style transfer using a conditional generative\nadversarial network. We also introduce a new Dixon loss function for the\ngenerator model. Using data from the UK Biobank Dixon MRI, our model is able to\npredict highly accurate fat and water channels that are free from artifacts. We\nshow that the model separates fat and water channels using either single input\n(in-phase) or dual input (in-phase and opposed-phase), with the latter\nproducing improved results. Our proposed method enables faster and more\naccurate downstream analysis of body composition from Dixon MRI in population\nstudies by eliminating the need for visual inspection or discarding data due to\nfat-water swaps.",
          "link": "http://arxiv.org/abs/2107.14175",
          "publishedOn": "2021-07-30T02:13:28.449Z",
          "wordCount": 707,
          "title": "Swap-Free Fat-Water Separation in Dixon MRI using Conditional Generative Adversarial Networks. (arXiv:2107.14175v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1\">Chun-Han Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_W/0/1/0/all/0/1\">Wei-Chih Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>",
          "description": "Reasoning 3D shapes from 2D images is an essential yet challenging task,\nespecially when only single-view images are at our disposal. While an object\ncan have a complicated shape, individual parts are usually close to geometric\nprimitives and thus are easier to model. Furthermore, parts provide a mid-level\nrepresentation that is robust to appearance variations across objects in a\nparticular category. In this work, we tackle the problem of 3D part discovery\nfrom only 2D image collections. Instead of relying on manually annotated parts\nfor supervision, we propose a self-supervised approach, latent part discovery\n(LPD). Our key insight is to learn a novel part shape prior that allows each\npart to fit an object shape faithfully while constrained to have simple\ngeometry. Extensive experiments on the synthetic ShapeNet, PartNet, and\nreal-world Pascal 3D+ datasets show that our method discovers consistent object\nparts and achieves favorable reconstruction accuracy compared to the existing\nmethods with the same level of supervision.",
          "link": "http://arxiv.org/abs/2107.13629",
          "publishedOn": "2021-07-30T02:13:28.442Z",
          "wordCount": 599,
          "title": "Discovering 3D Parts from Image Collections. (arXiv:2107.13629v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Beihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qinmu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Generalized zero-shot learning (GZSL) has achieved significant progress, with\nmany efforts dedicated to overcoming the problems of visual-semantic domain gap\nand seen-unseen bias. However, most existing methods directly use feature\nextraction models trained on ImageNet alone, ignoring the cross-dataset bias\nbetween ImageNet and GZSL benchmarks. Such a bias inevitably results in\npoor-quality visual features for GZSL tasks, which potentially limits the\nrecognition performance on both seen and unseen classes. In this paper, we\npropose a simple yet effective GZSL method, termed feature refinement for\ngeneralized zero-shot learning (FREE), to tackle the above problem. FREE\nemploys a feature refinement (FR) module that incorporates\n\\textit{semantic$\\rightarrow$visual} mapping into a unified generative model to\nrefine the visual features of seen and unseen class samples. Furthermore, we\npropose a self-adaptive margin center loss (SAMC-loss) that cooperates with a\nsemantic cycle-consistency loss to guide FR to learn class- and\nsemantically-relevant representations, and concatenate the features in FR to\nextract the fully refined features. Extensive experiments on five benchmark\ndatasets demonstrate the significant performance gain of FREE over its baseline\nand current state-of-the-art methods. Our codes are available at\nhttps://github.com/shiming-chen/FREE .",
          "link": "http://arxiv.org/abs/2107.13807",
          "publishedOn": "2021-07-30T02:13:28.436Z",
          "wordCount": 633,
          "title": "FREE: Feature Refinement for Generalized Zero-Shot Learning. (arXiv:2107.13807v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+LeBauer_D/0/1/0/all/0/1\">David LeBauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnette_M/0/1/0/all/0/1\">Max Burnette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahlgren_N/0/1/0/all/0/1\">Noah Fahlgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kooper_R/0/1/0/all/0/1\">Rob Kooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McHenry_K/0/1/0/all/0/1\">Kenton McHenry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1\">Abby Stylianou</a>",
          "description": "A core objective of the TERRA-REF project was to generate an open-access\nreference dataset for the study of evaluation of sensing technology to study\nplants under field conditions. The TERRA-REF program deployed a suite of high\nresolution, cutting edge technology sensors on a gantry system with the aim of\nscanning 1 hectare (~$10^4$ m) at around $1 mm^2$ spatial resolution multiple\ntimes per week. The system contains co-located sensors including a stereo-pair\nRGB camera, a thermal imager, a laser scanner to capture 3D structure, and two\nhyperspectral cameras covering wavelengths of 300-2500nm. This sensor data is\nprovided alongside over sixty types of traditional plant measurements that can\nbe used to train new machine learning models. Associated weather and\nenvironmental measurements, information about agronomic management and\nexperimental design, and the genomic sequences of hundreds of plant varieties\nhave been collected and are available alongside the sensor and plant trait\n(phenotype) data.\n\nOver the course of four years and ten growing seasons, the TERRA-REF system\ngenerated over 1 PB of sensor data and almost 45 million files. The subset that\nhas been released to the public domain accounts for two seasons and about half\nof the total data volume. This provides an unprecedented opportunity for\ninvestigations far beyond the core biological scope of the project.\n\nThis focus of this paper is to provide the Computer Vision and Machine\nLearning communities an overview of the available data and some potential\napplications of this one of a kind data.",
          "link": "http://arxiv.org/abs/2107.14072",
          "publishedOn": "2021-07-30T02:13:28.427Z",
          "wordCount": 713,
          "title": "What Does TERRA-REF's High Resolution, Multi Sensor Plant Sensing Public Domain Data Offer the Computer Vision Community?. (arXiv:2107.14072v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jizhizi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Sihan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Recently, there has been an increasing concern about the privacy issue raised\nby using personally identifiable information in machine learning. However,\nprevious portrait matting methods were all based on identifiable portrait\nimages. To fill the gap, we present P3M-10k in this paper, which is the first\nlarge-scale anonymized benchmark for Privacy-Preserving Portrait Matting.\nP3M-10k consists of 10,000 high-resolution face-blurred portrait images along\nwith high-quality alpha mattes. We systematically evaluate both trimap-free and\ntrimap-based matting methods on P3M-10k and find that existing matting methods\nshow different generalization capabilities when following the\nPrivacy-Preserving Training (PPT) setting, i.e., training on face-blurred\nimages and testing on arbitrary images. To devise a better trimap-free portrait\nmatting model, we propose P3M-Net, which leverages the power of a unified\nframework for both semantic perception and detail matting, and specifically\nemphasizes the interaction between them and the encoder to facilitate the\nmatting process. Extensive experiments on P3M-10k demonstrate that P3M-Net\noutperforms the state-of-the-art methods in terms of both objective metrics and\nsubjective visual quality. Besides, it shows good generalization capacity under\nthe PPT setting, confirming the value of P3M-10k for facilitating future\nresearch and enabling potential real-world applications. The source code and\ndataset are available at https://github.com/JizhiziLi/P3M",
          "link": "http://arxiv.org/abs/2104.14222",
          "publishedOn": "2021-07-30T02:13:28.420Z",
          "wordCount": 672,
          "title": "Privacy-Preserving Portrait Matting. (arXiv:2104.14222v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">TianYang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">XiaoJun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1\">Josef Kittler</a>",
          "description": "The Transformer architecture has achieved rapiddevelopment in recent years,\noutperforming the CNN archi-tectures in many computer vision tasks, such as the\nVisionTransformers (ViT) for image classification. However, existingvisual\ntransformer models aim to extract semantic informationfor high-level tasks such\nas classification and detection, distortingthe spatial resolution of the input\nimage, thus sacrificing thecapacity in reconstructing the input or generating\nhigh-resolutionimages. In this paper, therefore, we propose a Patch\nPyramidTransformer(PPT) to effectively address the above issues. Specif-ically,\nwe first design a Patch Transformer to transform theimage into a sequence of\npatches, where transformer encodingis performed for each patch to extract local\nrepresentations.In addition, we construct a Pyramid Transformer to\neffectivelyextract the non-local information from the entire image.\nAfterobtaining a set of multi-scale, multi-dimensional, and multi-anglefeatures\nof the original image, we design the image reconstructionnetwork to ensure that\nthe features can be reconstructed intothe original input. To validate the\neffectiveness, we apply theproposed Patch Pyramid Transformer to the image\nfusion task.The experimental results demonstrate its superior\nperformanceagainst the state-of-the-art fusion approaches, achieving the\nbestresults on several evaluation indicators. The underlying capacityof the PPT\nnetwork is reflected by its universal power in featureextraction and image\nreconstruction, which can be directlyapplied to different image fusion tasks\nwithout redesigning orretraining the network.",
          "link": "http://arxiv.org/abs/2107.13967",
          "publishedOn": "2021-07-30T02:13:28.404Z",
          "wordCount": 671,
          "title": "PPT Fusion: Pyramid Patch Transformerfor a Case Study in Image Fusion. (arXiv:2107.13967v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13718",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Luchuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>",
          "description": "Crowd counting is a challenging task due to the issues such as scale\nvariation and perspective variation in real crowd scenes. In this paper, we\npropose a novel Cascaded Residual Density Network (CRDNet) in a coarse-to-fine\napproach to generate the high-quality density map for crowd counting more\naccurately. (1) We estimate the residual density maps by multi-scale pyramidal\nfeatures through cascaded residual density modules. It can improve the quality\nof density map layer by layer effectively. (2) A novel additional local count\nloss is presented to refine the accuracy of crowd counting, which reduces the\nerrors of pixel-wise Euclidean loss by restricting the number of people in the\nlocal crowd areas. Experiments on two public benchmark datasets show that the\nproposed method achieves effective improvement compared with the\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.13718",
          "publishedOn": "2021-07-30T02:13:28.394Z",
          "wordCount": 566,
          "title": "Cascaded Residual Density Network for Crowd Counting. (arXiv:2107.13718v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chang-Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng-Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_F/0/1/0/all/0/1\">Feng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>",
          "description": "Semantic segmentation models trained on public datasets have achieved great\nsuccess in recent years. However, these models didn't consider the\npersonalization issue of segmentation though it is important in practice. In\nthis paper, we address the problem of personalized image segmentation. The\nobjective is to generate more accurate segmentation results on unlabeled\npersonalized images by investigating the data's personalized traits. To open up\nfuture research in this area, we collect a large dataset containing various\nusers' personalized images called PIS (Personalized Image Semantic\nSegmentation). We also survey some recent researches related to this problem\nand report their performance on our dataset. Furthermore, by observing the\ncorrelation among a user's personalized images, we propose a baseline method\nthat incorporates the inter-image context when segmenting certain images.\nExtensive experiments show that our method outperforms the existing methods on\nthe proposed dataset. The code and the PIS dataset will be made publicly\navailable.",
          "link": "http://arxiv.org/abs/2107.13978",
          "publishedOn": "2021-07-30T02:13:28.375Z",
          "wordCount": 582,
          "title": "Personalized Image Semantic Segmentation. (arXiv:2107.13978v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scabini_L/0/1/0/all/0/1\">Leonardo F. S. Scabini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruno_O/0/1/0/all/0/1\">Odemir M. Bruno</a>",
          "description": "Understanding the behavior of Artificial Neural Networks is one of the main\ntopics in the field recently, as black-box approaches have become usual since\nthe widespread of deep learning. Such high-dimensional models may manifest\ninstabilities and weird properties that resemble complex systems. Therefore, we\npropose Complex Network (CN) techniques to analyze the structure and\nperformance of fully connected neural networks. For that, we build a dataset\nwith 4 thousand models and their respective CN properties. They are employed in\na supervised classification setup considering four vision benchmarks. Each\nneural network is approached as a weighted and undirected graph of neurons and\nsynapses, and centrality measures are computed after training. Results show\nthat these measures are highly related to the network classification\nperformance. We also propose the concept of Bag-Of-Neurons (BoN), a CN-based\napproach for finding topological signatures linking similar neurons. Results\nsuggest that six neuronal types emerge in such networks, independently of the\ntarget domain, and are distributed differently according to classification\naccuracy. We also tackle specific CN properties related to performance, such as\nhigher subgraph centrality on lower-performing models. Our findings suggest\nthat CN properties play a critical role in the performance of fully connected\nneural networks, with topological patterns emerging independently on a wide\nrange of models.",
          "link": "http://arxiv.org/abs/2107.14062",
          "publishedOn": "2021-07-30T02:13:28.368Z",
          "wordCount": 686,
          "title": "Structure and Performance of Fully Connected Neural Networks: Emerging Complex Network Properties. (arXiv:2107.14062v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14185",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hengchang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhifei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhan Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1\">Kui Ren</a>",
          "description": "Transferability of adversarial examples is of central importance for\nattacking an unknown model, which facilitates adversarial attacks in more\npractical scenarios, e.g., blackbox attacks. Existing transferable attacks tend\nto craft adversarial examples by indiscriminately distorting features to\ndegrade prediction accuracy in a source model without aware of intrinsic\nfeatures of objects in the images. We argue that such brute-force degradation\nwould introduce model-specific local optimum into adversarial examples, thus\nlimiting the transferability. By contrast, we propose the Feature\nImportance-aware Attack (FIA), which disrupts important object-aware features\nthat dominate model decisions consistently. More specifically, we obtain\nfeature importance by introducing the aggregate gradient, which averages the\ngradients with respect to feature maps of the source model, computed on a batch\nof random transforms of the original clean image. The gradients will be highly\ncorrelated to objects of interest, and such correlation presents invariance\nacross different models. Besides, the random transforms will preserve intrinsic\nfeatures of objects and suppress model-specific information. Finally, the\nfeature importance guides to search for adversarial examples towards disrupting\ncritical features, achieving stronger transferability. Extensive experimental\nevaluation demonstrates the effectiveness and superior performance of the\nproposed FIA, i.e., improving the success rate by 8.4% against normally trained\nmodels and 11.7% against defense models as compared to the state-of-the-art\ntransferable attacks. Code is available at: https://github.com/hcguoO0/FIA",
          "link": "http://arxiv.org/abs/2107.14185",
          "publishedOn": "2021-07-30T02:13:28.353Z",
          "wordCount": 658,
          "title": "Feature Importance-aware Transferable Adversarial Attacks. (arXiv:2107.14185v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazaheri_A/0/1/0/all/0/1\">Amir Mazaheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>",
          "description": "Video generation is one of the most challenging tasks in Machine Learning and\nComputer Vision fields of study. In this paper, we tackle the text to video\ngeneration problem, which is a conditional form of video generation. Humans can\nlisten/read natural language sentences, and can imagine or visualize what is\nbeing described; therefore, we believe that video generation from natural\nlanguage sentences will have an important impact on Artificial Intelligence.\nVideo generation is relatively a new field of study in Computer Vision, which\nis far from being solved. The majority of recent works deal with synthetic\ndatasets or real datasets with very limited types of objects, scenes, and\nemotions. To the best of our knowledge, this is the very first work on the text\n(free-form sentences) to video generation on more realistic video datasets like\nActor and Action Dataset (A2D) or UCF101. We tackle the complicated problem of\nvideo generation by regressing the latent representations of the first and last\nframes and employing a context-aware interpolation method to build the latent\nrepresentations of in-between frames. We propose a stacking ``upPooling'' block\nto sequentially generate RGB frames out of each latent representation and\nprogressively increase the resolution. Moreover, our proposed Discriminator\nencodes videos based on single and multiple frames. We provide quantitative and\nqualitative results to support our arguments and show the superiority of our\nmethod over well-known baselines like Recurrent Neural Network (RNN) and\nDeconvolution (as known as Convolutional Transpose) based video generation\nmethods.",
          "link": "http://arxiv.org/abs/2107.13766",
          "publishedOn": "2021-07-30T02:13:28.342Z",
          "wordCount": 682,
          "title": "Video Generation from Text Employing Latent Path Construction for Temporal Modeling. (arXiv:2107.13766v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kellenberger_B/0/1/0/all/0/1\">Benjamin Kellenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vargas_Munoz_J/0/1/0/all/0/1\">John E. Vargas-Mu&#xf1;oz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuia_D/0/1/0/all/0/1\">Devis Tuia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daudt_R/0/1/0/all/0/1\">Rodrigo C. Daudt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whelan_T/0/1/0/all/0/1\">Thao T-T Whelan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayo_B/0/1/0/all/0/1\">Brenda Ayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1\">Ferda Ofli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1\">Muhammad Imran</a>",
          "description": "Humanitarian actions require accurate information to efficiently delegate\nsupport operations. Such information can be maps of building footprints,\nbuilding functions, and population densities. While the access to this\ninformation is comparably easy in industrialized countries thanks to reliable\ncensus data and national geo-data infrastructures, this is not the case for\ndeveloping countries, where that data is often incomplete or outdated. Building\nmaps derived from remote sensing images may partially remedy this challenge in\nsuch countries, but are not always accurate due to different landscape\nconfigurations and lack of validation data. Even when they exist, building\nfootprint layers usually do not reveal more fine-grained building properties,\nsuch as the number of stories or the building's function (e.g., office,\nresidential, school, etc.). In this project we aim to automate building\nfootprint and function mapping using heterogeneous data sources. In a first\nstep, we intend to delineate buildings from satellite data, using deep learning\nmodels for semantic image segmentation. Building functions shall be retrieved\nby parsing social media data like for instance tweets, as well as ground-based\nimagery, to automatically identify different buildings functions and retrieve\nfurther information such as the number of building stories. Building maps\naugmented with those additional attributes make it possible to derive more\naccurate population density maps, needed to support the targeted provision of\nhumanitarian aid.",
          "link": "http://arxiv.org/abs/2107.14123",
          "publishedOn": "2021-07-30T02:13:28.335Z",
          "wordCount": 667,
          "title": "Mapping Vulnerable Populations with AI. (arXiv:2107.14123v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wenhang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chunyan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Ancong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hongwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-Shi Zheng</a>",
          "description": "Person re-identification (Re-ID) aims to match person images across\nnon-overlapping camera views. The majority of Re-ID methods focus on\nsmall-scale surveillance systems in which each pedestrian is captured in\ndifferent camera views of adjacent scenes. However, in large-scale surveillance\nsystems that cover larger areas, it is required to track a pedestrian of\ninterest across distant scenes (e.g., a criminal suspect escapes from one city\nto another). Since most pedestrians appear in limited local areas, it is\ndifficult to collect training data with cross-camera pairs of the same person.\nIn this work, we study intra-camera supervised person re-identification across\ndistant scenes (ICS-DS Re-ID), which uses cross-camera unpaired data with\nintra-camera identity labels for training. It is challenging as cross-camera\npaired data plays a crucial role for learning camera-invariant features in most\nexisting Re-ID methods. To learn camera-invariant representation from\ncross-camera unpaired training data, we propose a cross-camera feature\nprediction method to mine cross-camera self supervision information from\ncamera-specific feature distribution by transforming fake cross-camera positive\nfeature pairs and minimize the distances of the fake pairs. Furthermore, we\nautomatically localize and extract local-level feature by a transformer. Joint\nlearning of global-level and local-level features forms a global-local\ncross-camera feature prediction scheme for mining fine-grained cross-camera\nself supervision information. Finally, cross-camera self supervision and\nintra-camera supervision are aggregated in a framework. The experiments are\nconducted in the ICS-DS setting on Market-SCT, Duke-SCT and MSMT17-SCT\ndatasets. The evaluation results demonstrate the superiority of our method,\nwhich gains significant improvements of 15.4 Rank-1 and 22.3 mAP on Market-SCT\nas compared to the second best method.",
          "link": "http://arxiv.org/abs/2107.13904",
          "publishedOn": "2021-07-30T02:13:28.325Z",
          "wordCount": 722,
          "title": "Cross-Camera Feature Prediction for Intra-Camera Supervised Person Re-identification across Distant Scenes. (arXiv:2107.13904v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13833",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Noort_F/0/1/0/all/0/1\">Frieda van den Noort</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sirmacek_B/0/1/0/all/0/1\">Beril Sirmacek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Slump_C/0/1/0/all/0/1\">Cornelis H. Slump</a>",
          "description": "The prevalance of pelvic floor problems is high within the female population.\nTransperineal ultrasound (TPUS) is the main imaging modality used to\ninvestigate these problems. Automating the analysis of TPUS data will help in\ngrowing our understanding of pelvic floor related problems. In this study we\npresent a U-net like neural network with some convolutional long short term\nmemory (CLSTM) layers to automate the 3D segmentation of the levator ani muscle\n(LAM) in TPUS volumes. The CLSTM layers are added to preserve the inter-slice\n3D information. We reach human level performance on this segmentation task.\nTherefore, we conclude that we successfully automated the segmentation of the\nLAM on 3D TPUS data. This paves the way towards automatic in-vivo analysis of\nthe LAM mechanics in the context of large study populations.",
          "link": "http://arxiv.org/abs/2107.13833",
          "publishedOn": "2021-07-30T02:13:28.318Z",
          "wordCount": 587,
          "title": "Recurrent U-net for automatic pelvic floor muscle segmentation on 3D ultrasound. (arXiv:2107.13833v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Luchuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Huihui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>",
          "description": "Abnormal behavior detection in surveillance video is a pivotal part of the\nintelligent city. Most existing methods only consider how to detect anomalies,\nwith less considering to explain the reason of the anomalies. We investigate an\northogonal perspective based on the reason of these abnormal behaviors. To this\nend, we propose a multivariate fusion method that analyzes each target through\nthree branches: object, action and motion. The object branch focuses on the\nappearance information, the motion branch focuses on the distribution of the\nmotion features, and the action branch focuses on the action category of the\ntarget. The information that these branches focus on is different, and they can\ncomplement each other and jointly detect abnormal behavior. The final abnormal\nscore can then be obtained by combining the abnormal scores of the three\nbranches.",
          "link": "http://arxiv.org/abs/2107.13706",
          "publishedOn": "2021-07-30T02:13:28.312Z",
          "wordCount": 569,
          "title": "Abnormal Behavior Detection Based on Target Analysis. (arXiv:2107.13706v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyons_M/0/1/0/all/0/1\">Michael J. Lyons</a>",
          "description": "Twenty-five years ago, my colleagues Miyuki Kamachi and Jiro Gyoba and I\ndesigned and photographed JAFFE, a set of facial expression images intended for\nuse in a study of face perception. In 2019, without seeking permission or\ninforming us, Kate Crawford and Trevor Paglen exhibited JAFFE in two widely\npublicized art shows. In addition, they published a nonfactual account of the\nimages in the essay \"Excavating AI: The Politics of Images in Machine Learning\nTraining Sets.\" The present article recounts the creation of the JAFFE dataset\nand unravels each of Crawford and Paglen's fallacious statements. I also\ndiscuss JAFFE more broadly in connection with research on facial expression,\naffective computing, and human-computer interaction.",
          "link": "http://arxiv.org/abs/2107.13998",
          "publishedOn": "2021-07-30T02:13:28.297Z",
          "wordCount": 572,
          "title": "\"Excavating AI\" Re-excavated: Debunking a Fallacious Account of the JAFFE Dataset. (arXiv:2107.13998v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Loginov_V/0/1/0/all/0/1\">Vladimir Loginov</a>",
          "description": "Recent works in the text recognition area have pushed forward the recognition\nresults to the new horizons. But for a long time a lack of large human-labeled\nnatural text recognition datasets has been forcing researchers to use synthetic\ndata for training text recognition models. Even though synthetic datasets are\nvery large (MJSynth and SynthTest, two most famous synthetic datasets, have\nseveral million images each), their diversity could be insufficient, compared\nto natural datasets like ICDAR and others. Fortunately, the recently released\ntext-recognition annotation for OpenImages V5 dataset has comparable with\nsynthetic dataset number of instances and more diverse examples. We have used\nthis annotation with a Text Recognition head architecture from the Yet Another\nMask Text Spotter and got comparable to the SOTA results. On some datasets we\nhave even outperformed previous SOTA models. In this paper we also introduce a\ntext recognition model. The model's code is available.",
          "link": "http://arxiv.org/abs/2107.13938",
          "publishedOn": "2021-07-30T02:13:28.291Z",
          "wordCount": 586,
          "title": "Why You Should Try the Real Data for the Scene Text Recognition. (arXiv:2107.13938v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1\">Aditya Jyoti Paul</a>",
          "description": "For over hundreds of millions of years, sea turtles and their ancestors have\nswum in the vast expanses of the ocean. They have undergone a number of\nevolutionary changes, leading to speciation and sub-speciation. However, in the\npast few decades, some of the most notable forces driving the genetic variance\nand population decline have been global warming and anthropogenic impact\nranging from large-scale poaching, collecting turtle eggs for food, besides\ndumping trash including plastic waste into the ocean. This leads to severe\ndetrimental effects in the sea turtle population, driving them to extinction.\nThis research focusses on the forces causing the decline in sea turtle\npopulation, the necessity for the global conservation efforts along with its\nsuccesses and failures, followed by an in-depth analysis of the modern advances\nin detection and recognition of sea turtles, involving Machine Learning and\nComputer Vision systems, aiding the conservation efforts.",
          "link": "http://arxiv.org/abs/2107.14061",
          "publishedOn": "2021-07-30T02:13:28.233Z",
          "wordCount": 610,
          "title": "The Need and Status of Sea Turtle Conservation and Survey of Associated Computer Vision Advances. (arXiv:2107.14061v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huan_J/0/1/0/all/0/1\">Jun Huan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>",
          "description": "While deep learning succeeds in a wide range of tasks, it highly depends on\nthe massive collection of annotated data which is expensive and time-consuming.\nTo lower the cost of data annotation, active learning has been proposed to\ninteractively query an oracle to annotate a small proportion of informative\nsamples in an unlabeled dataset. Inspired by the fact that the samples with\nhigher loss are usually more informative to the model than the samples with\nlower loss, in this paper we present a novel deep active learning approach that\nqueries the oracle for data annotation when the unlabeled sample is believed to\nincorporate high loss. The core of our approach is a measurement Temporal\nOutput Discrepancy (TOD) that estimates the sample loss by evaluating the\ndiscrepancy of outputs given by models at different optimization steps. Our\ntheoretical investigation shows that TOD lower-bounds the accumulated sample\nloss thus it can be used to select informative unlabeled samples. On basis of\nTOD, we further develop an effective unlabeled data sampling strategy as well\nas an unsupervised learning criterion that enhances model performance by\nincorporating the unlabeled data. Due to the simplicity of TOD, our active\nlearning approach is efficient, flexible, and task-agnostic. Extensive\nexperimental results demonstrate that our approach achieves superior\nperformances than the state-of-the-art active learning methods on image\nclassification and semantic segmentation tasks.",
          "link": "http://arxiv.org/abs/2107.14153",
          "publishedOn": "2021-07-30T02:13:28.218Z",
          "wordCount": 673,
          "title": "Semi-Supervised Active Learning with Temporal Output Discrepancy. (arXiv:2107.14153v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jizong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1\">Chrisitian Desrosiers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>",
          "description": "Pre-training a recognition model with contrastive learning on a large dataset\nof unlabeled data has shown great potential to boost the performance of a\ndownstream task, e.g., image classification. However, in domains such as\nmedical imaging, collecting unlabeled data can be challenging and expensive. In\nthis work, we propose to adapt contrastive learning to work with meta-label\nannotations, for improving the model's performance in medical image\nsegmentation even when no additional unlabeled data is available. Meta-labels\nsuch as the location of a 2D slice in a 3D MRI scan or the type of device used,\noften come for free during the acquisition process. We use the meta-labels for\npre-training the image encoder as well as to regularize a semi-supervised\ntraining, in which a reduced set of annotated data is used for training.\nFinally, to fully exploit the weak annotations, a self-paced learning approach\nis used to help the learning and discriminate useful labels from noise. Results\non three different medical image segmentation datasets show that our approach:\ni) highly boosts the performance of a model trained on a few scans, ii)\noutperforms previous contrastive and semi-supervised approaches, and iii)\nreaches close to the performance of a model trained on the full data.",
          "link": "http://arxiv.org/abs/2107.13741",
          "publishedOn": "2021-07-30T02:13:28.205Z",
          "wordCount": 639,
          "title": "Self-Paced Contrastive Learning for Semi-supervisedMedical Image Segmentation with Meta-labels. (arXiv:2107.13741v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Forecasting human trajectories in complex dynamic environments plays a\ncritical role in autonomous vehicles and intelligent robots. Most existing\nmethods learn to predict future trajectories by behavior clues from history\ntrajectories and interaction clues from environments. However, the inherent\nbias between training and deployment environments is ignored. Hence, we propose\na counterfactual analysis method for human trajectory prediction to investigate\nthe causality between the predicted trajectories and input clues and alleviate\nthe negative effects brought by environment bias. We first build a causal graph\nfor trajectory forecasting with history trajectory, future trajectory, and the\nenvironment interactions. Then, we cut off the inference from environment to\ntrajectory by constructing the counterfactual intervention on the trajectory\nitself. Finally, we compare the factual and counterfactual trajectory clues to\nalleviate the effects of environment bias and highlight the trajectory clues.\nOur counterfactual analysis is a plug-and-play module that can be applied to\nany baseline prediction methods including RNN- and CNN-based ones. We show that\nour method achieves consistent improvement for different baselines and obtains\nthe state-of-the-art results on public pedestrian trajectory forecasting\nbenchmarks.",
          "link": "http://arxiv.org/abs/2107.14202",
          "publishedOn": "2021-07-30T02:13:28.151Z",
          "wordCount": 620,
          "title": "Human Trajectory Prediction via Counterfactual Analysis. (arXiv:2107.14202v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13820",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ching/0/1/0/all/0/1\">Ching</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_K/0/1/0/all/0/1\">Kai Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao/0/1/0/all/0/1\">Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_J/0/1/0/all/0/1\">Jerry Chang</a>, Yun, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_C/0/1/0/all/0/1\">Chien Cheng</a>",
          "description": "The purpose of this study is to differentiate malignant and benign\nmediastinal lesions by using the three-dimensional convolutional neural network\nthrough the endobronchial ultrasound (EBUS) image. Compared with previous\nstudy, our proposed model is robust to noise and able to fuse various imaging\nfeatures and spatiotemporal features of EBUS videos. Endobronchial\nultrasound-guided transbronchial needle aspiration (EBUS-TBNA) is a diagnostic\ntool for intrathoracic lymph nodes. Physician can observe the characteristics\nof the lesion using grayscale mode, doppler mode, and elastography during the\nprocedure. To process the EBUS data in the form of a video and appropriately\nintegrate the features of multiple imaging modes, we used a time-series\nthree-dimensional convolutional neural network (3D CNN) to learn the\nspatiotemporal features and design a variety of architectures to fuse each\nimaging mode. Our model (Res3D_UDE) took grayscale mode, Doppler mode, and\nelastography as training data and achieved an accuracy of 82.00% and area under\nthe curve (AUC) of 0.83 on the validation set. Compared with previous study, we\ndirectly used videos recorded during procedure as training and validation data,\nwithout additional manual selection, which might be easier for clinical\napplication. In addition, model designed with 3D CNN can also effectively learn\nspatiotemporal features and improve accuracy. In the future, our model may be\nused to guide physicians to quickly and correctly find the target lesions for\nslice sampling during the inspection process, reduce the number of slices of\nbenign lesions, and shorten the inspection time.",
          "link": "http://arxiv.org/abs/2107.13820",
          "publishedOn": "2021-07-30T02:13:28.133Z",
          "wordCount": 707,
          "title": "The interpretation of endobronchial ultrasound image using 3D convolutional neural network for differentiating malignant and benign mediastinal lesions. (arXiv:2107.13820v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yanqing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_B/0/1/0/all/0/1\">Bangning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Miaogen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yanlong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chunliu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Juan Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Changyu Shen</a>",
          "description": "Multiple optical scattering occurs when light propagates in a non-uniform\nmedium. During the multiple scattering, images were distorted and the spatial\ninformation they carried became scrambled. However, the image information is\nnot lost but presents in the form of speckle patterns (SPs). In this study, we\nbuilt up an optical random scattering system based on an LCD and an RGB laser\nsource. We found that the image classification can be improved by the help of\nrandom scattering which is considered as a feedforward neural network to\nextracts features from image. Along with the ridge classification deployed on\ncomputer, we achieved excellent classification accuracy higher than 94%, for a\nvariety of data sets covering medical, agricultural, environmental protection\nand other fields. In addition, the proposed optical scattering system has the\nadvantages of high speed, low power consumption, and miniaturization, which is\nsuitable for deploying in edge computing applications.",
          "link": "http://arxiv.org/abs/2107.14051",
          "publishedOn": "2021-07-30T02:13:27.993Z",
          "wordCount": 600,
          "title": "Improvement of image classification by multiple optical scattering. (arXiv:2107.14051v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinzhu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianzhu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yating Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>",
          "description": "Geometry Projection is a powerful depth estimation method in monocular 3D\nobject detection. It estimates depth dependent on heights, which introduces\nmathematical priors into the deep model. But projection process also introduces\nthe error amplification problem, in which the error of the estimated height\nwill be amplified and reflected greatly at the output depth. This property\nleads to uncontrollable depth inferences and also damages the training\nefficiency. In this paper, we propose a Geometry Uncertainty Projection Network\n(GUP Net) to tackle the error amplification problem at both inference and\ntraining stages. Specifically, a GUP module is proposed to obtains the\ngeometry-guided uncertainty of the inferred depth, which not only provides high\nreliable confidence for each depth but also benefits depth learning.\nFurthermore, at the training stage, we propose a Hierarchical Task Learning\nstrategy to reduce the instability caused by error amplification. This learning\nalgorithm monitors the learning situation of each task by a proposed indicator\nand adaptively assigns the proper loss weights for different tasks according to\ntheir pre-tasks situation. Based on that, each task starts learning only when\nits pre-tasks are learned well, which can significantly improve the stability\nand efficiency of the training process. Extensive experiments demonstrate the\neffectiveness of the proposed method. The overall model can infer more reliable\nobject depth than existing methods and outperforms the state-of-the-art\nimage-based monocular 3D detectors by 3.74% and 4.7% AP40 of the car and\npedestrian categories on the KITTI benchmark.",
          "link": "http://arxiv.org/abs/2107.13774",
          "publishedOn": "2021-07-30T02:13:27.987Z",
          "wordCount": 691,
          "title": "Geometry Uncertainty Projection Network for Monocular 3D Object Detection. (arXiv:2107.13774v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elhagry_A/0/1/0/all/0/1\">Ahmed Elhagry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gla_R/0/1/0/all/0/1\">Rawan Gla</a>",
          "description": "Sign language is a set of gestures that deaf people use to communicate.\nUnfortunately, normal people don't understand it, which creates a communication\ngap that needs to be filled. Because of the variations in (Egyptian Sign\nLanguage) ESL from one region to another, ESL provides a challenging research\nproblem. In this work, we are providing applied research with its video-based\nEgyptian sign language recognition system that serves the local community of\ndeaf people in Egypt, with a moderate and reasonable accuracy. We present a\ncomputer vision system with two different neural networks architectures. The\nfirst is a Convolutional Neural Network (CNN) for extracting spatial features.\nThe CNN model was retrained on the inception mod. The second architecture is a\nCNN followed by a Long Short-Term Memory (LSTM) for extracting both spatial and\ntemporal features. The two models achieved an accuracy of 90% and 72%,\nrespectively. We examined the power of these two architectures to distinguish\nbetween 9 common words (with similar signs) among some deaf people community in\nEgypt.",
          "link": "http://arxiv.org/abs/2107.13647",
          "publishedOn": "2021-07-30T02:13:27.981Z",
          "wordCount": 599,
          "title": "Egyptian Sign Language Recognition Using CNN and LSTM. (arXiv:2107.13647v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13703",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Afshari_M/0/1/0/all/0/1\">Mehdi Afshari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tizhoosh_H/0/1/0/all/0/1\">H.R. Tizhoosh</a>",
          "description": "Histopathology digital scans are large-size images that contain valuable\ninformation at the pixel level. Content-based comparison of these images is a\nchallenging task. This study proposes a content-based similarity measure for\nhigh-resolution gigapixel histopathology images. The proposed similarity\nmeasure is an expansion of cosine vector similarity to a matrix. Each image is\ndivided into same-size patches with a meaningful amount of information (i.e.,\ncontained enough tissue). The similarity is measured by the extraction of\npatch-level deep embeddings of the last pooling layer of a pre-trained deep\nmodel at four different magnification levels, namely, 1x, 2.5x, 5x, and 10x\nmagnifications. In addition, for faster measurement, embedding reduction is\ninvestigated. Finally, to assess the proposed method, an image search method is\nimplemented. Results show that the similarity measure represents the slide\nlabels with a maximum accuracy of 93.18\\% for top-5 search at 5x magnification.",
          "link": "http://arxiv.org/abs/2107.13703",
          "publishedOn": "2021-07-30T02:13:27.974Z",
          "wordCount": 589,
          "title": "A Similarity Measure of Histopathology Images by Deep Embeddings. (arXiv:2107.13703v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shan_W/0/1/0/all/0/1\">Wenkang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>",
          "description": "Most of the existing 3D human pose estimation approaches mainly focus on\npredicting 3D positional relationships between the root joint and other human\njoints (local motion) instead of the overall trajectory of the human body\n(global motion). Despite the great progress achieved by these approaches, they\nare not robust to global motion, and lack the ability to accurately predict\nlocal motion with a small movement range. To alleviate these two problems, we\npropose a relative information encoding method that yields positional and\ntemporal enhanced representations. Firstly, we encode positional information by\nutilizing relative coordinates of 2D poses to enhance the consistency between\nthe input and output distribution. The same posture with different absolute 2D\npositions can be mapped to a common representation. It is beneficial to resist\nthe interference of global motion on the prediction results. Second, we encode\ntemporal information by establishing the connection between the current pose\nand other poses of the same person within a period of time. More attention will\nbe paid to the movement changes before and after the current pose, resulting in\nbetter prediction performance on local motion with a small movement range. The\nablation studies validate the effectiveness of the proposed relative\ninformation encoding method. Besides, we introduce a multi-stage optimization\nmethod to the whole framework to further exploit the positional and temporal\nenhanced representations. Our method outperforms state-of-the-art methods on\ntwo public datasets. Code is available at\nhttps://github.com/paTRICK-swk/Pose3D-RIE.",
          "link": "http://arxiv.org/abs/2107.13994",
          "publishedOn": "2021-07-30T02:13:27.957Z",
          "wordCount": 705,
          "title": "Improving Robustness and Accuracy via Relative Information Encoding in 3D Human Pose Estimation. (arXiv:2107.13994v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruicong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Feng Lu</a>",
          "description": "Deep neural networks have significantly improved appearance-based gaze\nestimation accuracy. However, it still suffers from unsatisfactory performance\nwhen generalizing the trained model to new domains, e.g., unseen environments\nor persons. In this paper, we propose a plug-and-play gaze adaptation framework\n(PnP-GA), which is an ensemble of networks that learn collaboratively with the\nguidance of outliers. Since our proposed framework does not require\nground-truth labels in the target domain, the existing gaze estimation networks\ncan be directly plugged into PnP-GA and generalize the algorithms to new\ndomains. We test PnP-GA on four gaze domain adaptation tasks, ETH-to-MPII,\nETH-to-EyeDiap, Gaze360-to-MPII, and Gaze360-to-EyeDiap. The experimental\nresults demonstrate that the PnP-GA framework achieves considerable performance\nimprovements of 36.9%, 31.6%, 19.4%, and 11.8% over the baseline system. The\nproposed framework also outperforms the state-of-the-art domain adaptation\napproaches on gaze domain adaptation tasks. Code has been released at\nhttps://github.com/DreamtaleCore/PnP-GA.",
          "link": "http://arxiv.org/abs/2107.13780",
          "publishedOn": "2021-07-30T02:13:27.942Z",
          "wordCount": 586,
          "title": "Generalizing Gaze Estimation with Outlier-guided Collaborative Adaptation. (arXiv:2107.13780v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chuanguang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1\">Zhulin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Linhang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongjun Xu</a>",
          "description": "Knowledge distillation often involves how to define and transfer knowledge\nfrom teacher to student effectively. Although recent self-supervised\ncontrastive knowledge achieves the best performance, forcing the network to\nlearn such knowledge may damage the representation learning of the original\nclass recognition task. We therefore adopt an alternative self-supervised\naugmented task to guide the network to learn the joint distribution of the\noriginal recognition task and self-supervised auxiliary task. It is\ndemonstrated as a richer knowledge to improve the representation power without\nlosing the normal classification capability. Moreover, it is incomplete that\nprevious methods only transfer the probabilistic knowledge between the final\nlayers. We propose to append several auxiliary classifiers to hierarchical\nintermediate feature maps to generate diverse self-supervised knowledge and\nperform the one-to-one transfer to teach the student network thoroughly. Our\nmethod significantly surpasses the previous SOTA SSKD with an average\nimprovement of 2.56\\% on CIFAR-100 and an improvement of 0.77\\% on ImageNet\nacross widely used network pairs. Codes are available at\nhttps://github.com/winycg/HSAKD.",
          "link": "http://arxiv.org/abs/2107.13715",
          "publishedOn": "2021-07-30T02:13:27.915Z",
          "wordCount": 599,
          "title": "Hierarchical Self-supervised Augmented Knowledge Distillation. (arXiv:2107.13715v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iwanowski_M/0/1/0/all/0/1\">Marcin Iwanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzabka_M/0/1/0/all/0/1\">Marcin Grzabka</a>",
          "description": "The paper describes a method for measuring the similarity and symmetry of an\nimage annotated with bounding boxes indicating image objects. The latter\nrepresentation became popular recently due to the rapid development of fast and\nefficient deep-learning-based object-detection methods. The proposed approach\nallows for comparing sets of bounding boxes to estimate the degree of\nsimilarity of their underlying images. It is based on the fuzzy approach that\nuses the fuzzy mutual position (FMP) matrix to describe spatial composition and\nrelations between bounding boxes within an image. A method of computing the\nsimilarity of two images described by their FMP matrices is proposed and the\nalgorithm of its computation. It outputs the single scalar value describing the\ndegree of content-based image similarity. By modifying the method`s parameters,\ninstead of similarity, the reflectional symmetry of object composition may also\nbe measured. The proposed approach allows for measuring differences in objects`\ncomposition of various intensities. It is also invariant to translation and\nscaling and - in case of symmetry detection - position and orientation of the\nsymmetry axis. A couple of examples illustrate the method.",
          "link": "http://arxiv.org/abs/2107.13651",
          "publishedOn": "2021-07-30T02:13:27.909Z",
          "wordCount": 649,
          "title": "Similarity and symmetry measures based on fuzzy descriptors of image objects` composition. (arXiv:2107.13651v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1\">Chongyang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1\">Xiaoxue Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkara_S/0/1/0/all/0/1\">Srinivas Sunkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Abhinav Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jindong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arcas_B/0/1/0/all/0/1\">Blaise Aguera y Arcas</a>",
          "description": "To improve the accessibility of smart devices and to simplify their usage,\nbuilding models which understand user interfaces (UIs) and assist users to\ncomplete their tasks is critical. However, unique challenges are proposed by\nUI-specific characteristics, such as how to effectively leverage multimodal UI\nfeatures that involve image, text, and structural metadata and how to achieve\ngood performance when high-quality labeled data is unavailable. To address such\nchallenges we introduce UIBert, a transformer-based joint image-text model\ntrained through novel pre-training tasks on large-scale unlabeled UI data to\nlearn generic feature representations for a UI and its components. Our key\nintuition is that the heterogeneous features in a UI are self-aligned, i.e.,\nthe image and text features of UI components, are predictive of each other. We\npropose five pretraining tasks utilizing this self-alignment among different\nfeatures of a UI component and across various components in the same UI. We\nevaluate our method on nine real-world downstream UI tasks where UIBert\noutperforms strong multimodal baselines by up to 9.26% accuracy.",
          "link": "http://arxiv.org/abs/2107.13731",
          "publishedOn": "2021-07-30T02:13:27.894Z",
          "wordCount": 620,
          "title": "UIBert: Learning Generic Multimodal Representations for UI Understanding. (arXiv:2107.13731v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13742",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taherkhani_F/0/1/0/all/0/1\">Fariborz Taherkhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talreja_V/0/1/0/all/0/1\">Veeru Talreja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valenti_M/0/1/0/all/0/1\">Matthew C. Valenti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "In recent years, with the advent of deep-learning, face recognition has\nachieved exceptional success. However, many of these deep face recognition\nmodels perform much better in handling frontal faces compared to profile faces.\nThe major reason for poor performance in handling of profile faces is that it\nis inherently difficult to learn pose-invariant deep representations that are\nuseful for profile face recognition. In this paper, we hypothesize that the\nprofile face domain possesses a latent connection with the frontal face domain\nin a latent feature subspace. We look to exploit this latent connection by\nprojecting the profile faces and frontal faces into a common latent subspace\nand perform verification or retrieval in the latent domain. We leverage a\ncoupled conditional generative adversarial network (cpGAN) structure to find\nthe hidden relationship between the profile and frontal images in a latent\ncommon embedding subspace. Specifically, the cpGAN framework consists of two\nconditional GAN-based sub-networks, one dedicated to the frontal domain and the\nother dedicated to the profile domain. Each sub-network tends to find a\nprojection that maximizes the pair-wise correlation between the two feature\ndomains in a common embedding feature subspace. The efficacy of our approach\ncompared with the state-of-the-art is demonstrated using the CFP, CMU\nMulti-PIE, IJB-A, and IJB-C datasets. Additionally, we have also implemented a\ncoupled convolutional neural network (cpCNN) and an adversarial discriminative\ndomain adaptation network (ADDA) for profile to frontal face recognition. We\nhave evaluated the performance of cpCNN and ADDA and compared it with the\nproposed cpGAN. Finally, we have also evaluated our cpGAN for reconstruction of\nfrontal faces from input profile faces contained in the VGGFace2 dataset.",
          "link": "http://arxiv.org/abs/2107.13742",
          "publishedOn": "2021-07-30T02:13:27.888Z",
          "wordCount": 731,
          "title": "Profile to Frontal Face Recognition in the Wild Using Coupled Conditional GAN. (arXiv:2107.13742v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kloeker_L/0/1/0/all/0/1\">Laurent Kloeker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kloeker_A/0/1/0/all/0/1\">Amarin Kloeker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomsen_F/0/1/0/all/0/1\">Fabian Thomsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erraji_A/0/1/0/all/0/1\">Armin Erraji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_L/0/1/0/all/0/1\">Lutz Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamberty_S/0/1/0/all/0/1\">Serge Lamberty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_A/0/1/0/all/0/1\">Adrian Fazekas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kallo_E/0/1/0/all/0/1\">Eszter Kall&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oeser_M/0/1/0/all/0/1\">Markus Oeser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flechon_C/0/1/0/all/0/1\">Charlotte Fl&#xe9;chon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lohmiller_J/0/1/0/all/0/1\">Jochen Lohmiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_P/0/1/0/all/0/1\">Pascal Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sommer_M/0/1/0/all/0/1\">Martin Sommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winter_H/0/1/0/all/0/1\">Helen Winter</a>",
          "description": "With the Corridor for New Mobility Aachen - D\\\"usseldorf, an integrated\ndevelopment environment is created, incorporating existing test capabilities,\nto systematically test and validate automated vehicles in interaction with\nconnected Intelligent Transport Systems Stations (ITS-Ss). This is achieved\nthrough a time- and cost-efficient toolchain and methodology, in which\nsimulation, closed test sites as well as test fields in public transport are\nlinked in the best possible way. By implementing a digital twin, the recorded\ntraffic events can be visualized in real-time and driving functions can be\ntested in the simulation based on real data. In order to represent diverse\ntraffic scenarios, the corridor contains a highway section, a rural area, and\nurban areas. First, this paper outlines the project goals before describing the\nindividual project contents in more detail. These include the concepts of\ntraffic detection, driving function development, digital twin development, and\npublic involvement.",
          "link": "http://arxiv.org/abs/2107.14048",
          "publishedOn": "2021-07-30T02:13:27.880Z",
          "wordCount": 613,
          "title": "Corridor for new mobility Aachen-D\\\"usseldorf: Methods and concepts of the research project ACCorD. (arXiv:2107.14048v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhiyuan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yaohai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ruisong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yayu Gao</a>",
          "description": "In this paper, we propose an efficient human pose estimation network -- SFM\n(slender fusion model) by fusing multi-level features and adding lightweight\nattention blocks -- HSA (High-Level Spatial Attention). Many existing methods\non efficient network have already taken feature fusion into consideration,\nwhich largely boosts the performance. However, its performance is far inferior\nto large network such as ResNet and HRNet due to its limited fusion operation\nin the network. Specifically, we expand the number of fusion operation by\nbuilding bridges between two pyramid frameworks without adding layers.\nMeanwhile, to capture long-range dependency, we propose a lightweight attention\nblock -- HSA, which computes second-order attention map. In summary, SFM\nmaximizes the number of feature fusion in a limited number of layers. HSA\nlearns high precise spatial information by computing the attention of spatial\nattention map. With the help of SFM and HSA, our network is able to generate\nmulti-level feature and extract precise global spatial information with little\ncomputing resource. Thus, our method achieve comparable or even better accuracy\nwith less parameters and computational cost. Our SFM achieve 89.0 in PCKh@0.5,\n42.0 in PCKh@0.1 on MPII validation set and 71.7 in AP, 90.7 in AP@0.5 on COCO\nvalidation with only 1.7G FLOPs and 1.5M parameters. The source code will be\npublic soon.",
          "link": "http://arxiv.org/abs/2107.13693",
          "publishedOn": "2021-07-30T02:13:27.858Z",
          "wordCount": 657,
          "title": "Efficient Human Pose Estimation by Maximizing Fusion and High-Level Spatial Attention. (arXiv:2107.13693v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yu Cheng Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsougenis_E/0/1/0/all/0/1\">Efstratios Tsougenis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsui_K/0/1/0/all/0/1\">Kwok-Leung Tsui</a>",
          "description": "Counting the repetition of human exercise and physical rehabilitation is a\ncommon task in rehabilitation and exercise training. The existing vision-based\nrepetition counting methods less emphasize the concurrent motions in the same\nvideo. This work presents a vision-based human motion repetition counting\napplicable to counting concurrent motions through the skeleton location\nextracted from various pose estimation methods. The presented method was\nvalidated on the University of Idaho Physical Rehabilitation Movements Data Set\n(UI-PRMD), and MM-fit dataset. The overall mean absolute error (MAE) for mm-fit\nwas 0.06 with off-by-one Accuracy (OBOA) 0.94. Overall MAE for UI-PRMD dataset\nwas 0.06 with OBOA 0.95. We have also tested the performance in a variety of\ncamera locations and concurrent motions with conveniently collected video with\noverall MAE 0.06 and OBOA 0.88. The proposed method provides a view-angle and\nmotion agnostic concurrent motion counting. This method can potentially use in\nlarge-scale remote rehabilitation and exercise training with only one camera.",
          "link": "http://arxiv.org/abs/2107.13760",
          "publishedOn": "2021-07-30T02:13:27.846Z",
          "wordCount": 589,
          "title": "Viewpoint-Invariant Exercise Repetition Counting. (arXiv:2107.13760v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13643",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elhagry_A/0/1/0/all/0/1\">Ahmed Elhagry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeed_M/0/1/0/all/0/1\">Mohamed Saeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araia_M/0/1/0/all/0/1\">Musie Araia</a>",
          "description": "Human pose estimation (HPE) is one of the most challenging tasks in computer\nvision as humans are deformable by nature and thus their pose has so much\nvariance. HPE aims to correctly identify the main joint locations of a single\nperson or multiple people in a given image or video. Locating joints of a\nperson in images or videos is an important task that can be applied in action\nrecognition and object tracking. As have many computer vision tasks, HPE has\nadvanced massively with the introduction of deep learning to the field. In this\npaper, we focus on one of the deep learning-based approaches of HPE proposed by\nNewell et al., which they named the stacked hourglass network. Their approach\nis widely used in many applications and is regarded as one of the best works in\nthis area. The main focus of their approach is to capture as much information\nas it can at all possible scales so that a coherent understanding of the local\nfeatures and full-body location is achieved. Their findings demonstrate that\nimportant cues such as orientation of a person, arrangement of limbs, and\nadjacent joints' relative location can be identified from multiple scales at\ndifferent resolutions. To do so, they makes use of a single pipeline to process\nimages in multiple resolutions, which comprises a skip layer to not lose\nspatial information at each resolution. The resolution of the images stretches\nas lower as 4x4 to make sure that a smaller spatial feature is included. In\nthis study, we study the effect of architectural modifications on the\ncomputational speed and accuracy of the network.",
          "link": "http://arxiv.org/abs/2107.13643",
          "publishedOn": "2021-07-30T02:13:27.840Z",
          "wordCount": 703,
          "title": "Lighter Stacked Hourglass Human Pose Estimation. (arXiv:2107.13643v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xinyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dongjin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuncong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengzhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingchao Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>",
          "description": "Detecting abnormal activities in real-world surveillance videos is an\nimportant yet challenging task as the prior knowledge about video anomalies is\nusually limited or unavailable. Despite that many approaches have been\ndeveloped to resolve this problem, few of them can capture the normal\nspatio-temporal patterns effectively and efficiently. Moreover, existing works\nseldom explicitly consider the local consistency at frame level and global\ncoherence of temporal dynamics in video sequences. To this end, we propose\nConvolutional Transformer based Dual Discriminator Generative Adversarial\nNetworks (CT-D2GAN) to perform unsupervised video anomaly detection.\nSpecifically, we first present a convolutional transformer to perform future\nframe prediction. It contains three key components, i.e., a convolutional\nencoder to capture the spatial information of the input video clips, a temporal\nself-attention module to encode the temporal dynamics, and a convolutional\ndecoder to integrate spatio-temporal features and predict the future frame.\nNext, a dual discriminator based adversarial training procedure, which jointly\nconsiders an image discriminator that can maintain the local consistency at\nframe-level and a video discriminator that can enforce the global coherence of\ntemporal dynamics, is employed to enhance the future frame prediction. Finally,\nthe prediction error is used to identify abnormal video frames. Thoroughly\nempirical studies on three public video anomaly detection datasets, i.e., UCSD\nPed2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the effectiveness of\nthe proposed adversarial spatio-temporal modeling framework.",
          "link": "http://arxiv.org/abs/2107.13720",
          "publishedOn": "2021-07-30T02:13:27.812Z",
          "wordCount": 708,
          "title": "Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for Video Anomaly Detection. (arXiv:2107.13720v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chengkuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Ming Y. Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_D/0/1/0/all/0/1\">Drew F. K. Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tiffany Y. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaumberg_A/0/1/0/all/0/1\">Andrew J. Schaumberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1\">Faisal Mahmood</a>",
          "description": "The expanding adoption of digital pathology has enabled the curation of large\nrepositories of histology whole slide images (WSIs), which contain a wealth of\ninformation. Similar pathology image search offers the opportunity to comb\nthrough large historical repositories of gigapixel WSIs to identify cases with\nsimilar morphological features and can be particularly useful for diagnosing\nrare diseases, identifying similar cases for predicting prognosis, treatment\noutcomes, and potential clinical trial success. A critical challenge in\ndeveloping a WSI search and retrieval system is scalability, which is uniquely\nchallenging given the need to search a growing number of slides that each can\nconsist of billions of pixels and are several gigabytes in size. Such systems\nare typically slow and retrieval speed often scales with the size of the\nrepository they search through, making their clinical adoption tedious and are\nnot feasible for repositories that are constantly growing. Here we present Fast\nImage Search for Histopathology (FISH), a histology image search pipeline that\nis infinitely scalable and achieves constant search speed that is independent\nof the image database size while being interpretable and without requiring\ndetailed annotations. FISH uses self-supervised deep learning to encode\nmeaningful representations from WSIs and a Van Emde Boas tree for fast search,\nfollowed by an uncertainty-based ranking algorithm to retrieve similar WSIs. We\nevaluated FISH on multiple tasks and datasets with over 22,000 patient cases\nspanning 56 disease subtypes. We additionally demonstrate that FISH can be used\nto assist with the diagnosis of rare cancer types where sufficient cases may\nnot be available to train traditional supervised deep models. FISH is available\nas an easy-to-use, open-source software package\n(https://github.com/mahmoodlab/FISH).",
          "link": "http://arxiv.org/abs/2107.13587",
          "publishedOn": "2021-07-30T02:13:27.802Z",
          "wordCount": 725,
          "title": "Fast and Scalable Image Search For Histology. (arXiv:2107.13587v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadakis_M/0/1/0/all/0/1\">Manolis Fragkiadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putten_P/0/1/0/all/0/1\">Peter van der Putten</a>",
          "description": "Sign language lexica are a useful resource for researchers and people\nlearning sign languages. Current implementations allow a user to search a sign\neither by its gloss or by selecting its primary features such as handshape and\nlocation. This study focuses on exploring a reverse search functionality where\na user can sign a query sign in front of a webcam and retrieve a set of\nmatching signs. By extracting different body joints combinations (upper body,\ndominant hand's arm and wrist) using the pose estimation framework OpenPose, we\ncompare four techniques (PCA, UMAP, DTW and Euclidean distance) as distance\nmetrics between 20 query signs, each performed by eight participants on a 1200\nsign lexicon. The results show that UMAP and DTW can predict a matching sign\nwith an 80\\% and 71\\% accuracy respectively at the top-20 retrieved signs using\nthe movement of the dominant hand arm. Using DTW and adding more sign instances\nfrom other participants in the lexicon, the accuracy can be raised to 90\\% at\nthe top-10 ranking. Our results suggest that our methodology can be used with\nno training in any sign language lexicon regardless of its size.",
          "link": "http://arxiv.org/abs/2107.13637",
          "publishedOn": "2021-07-30T02:13:27.788Z",
          "wordCount": 660,
          "title": "Sign and Search: Sign Search Functionality for Sign Language Lexica. (arXiv:2107.13637v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiali Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>",
          "description": "The fact that there exists a gap between low-level features and semantic\nmeanings of images, called the semantic gap, is known for decades. Resolution\nof the semantic gap is a long standing problem. The semantic gap problem is\nreviewed and a survey on recent efforts in bridging the gap is made in this\nwork. Most importantly, we claim that the semantic gap is primarily bridged\nthrough supervised learning today. Experiences are drawn from two application\ndomains to illustrate this point: 1) object detection and 2) metric learning\nfor content-based image retrieval (CBIR). To begin with, this paper offers a\nhistorical retrospective on supervision, makes a gradual transition to the\nmodern data-driven methodology and introduces commonly used datasets. Then, it\nsummarizes various supervision methods to bridge the semantic gap in the\ncontext of object detection and metric learning.",
          "link": "http://arxiv.org/abs/2107.13757",
          "publishedOn": "2021-07-30T02:13:27.763Z",
          "wordCount": 576,
          "title": "Bridging Gap between Image Pixels and Semantics via Supervision: A Survey. (arXiv:2107.13757v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Breiki_F/0/1/0/all/0/1\">Farha Al Breiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridzuan_M/0/1/0/all/0/1\">Muhammad Ridzuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandhe_R/0/1/0/all/0/1\">Rushali Grandhe</a>",
          "description": "Fine-grained image classification involves identifying different\nsubcategories of a class which possess very subtle discriminatory features.\nFine-grained datasets usually provide bounding box annotations along with class\nlabels to aid the process of classification. However, building large scale\ndatasets with such annotations is a mammoth task. Moreover, this extensive\nannotation is time-consuming and often requires expertise, which is a huge\nbottleneck in building large datasets. On the other hand, self-supervised\nlearning (SSL) exploits the freely available data to generate supervisory\nsignals which act as labels. The features learnt by performing some pretext\ntasks on huge unlabelled data proves to be very helpful for multiple downstream\ntasks.\n\nOur idea is to leverage self-supervision such that the model learns useful\nrepresentations of fine-grained image classes. We experimented with 3 kinds of\nmodels: Jigsaw solving as pretext task, adversarial learning (SRGAN) and\ncontrastive learning based (SimCLR) model. The learned features are used for\ndownstream tasks such as fine-grained image classification. Our code is\navailable at\nthis http URL",
          "link": "http://arxiv.org/abs/2107.13973",
          "publishedOn": "2021-07-30T02:13:27.750Z",
          "wordCount": 608,
          "title": "Self-Supervised Learning for Fine-Grained Image Classification. (arXiv:2107.13973v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1\">William Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1\">Philippe Burlina</a>",
          "description": "When deploying artificial intelligence (AI) in the real world, being able to\ntrust the operation of the AI by characterizing how it performs is an\never-present and important topic. An important and still largely unexplored\ntask in this characterization is determining major factors within the real\nworld that affect the AI's behavior, such as weather conditions or lighting,\nand either a) being able to give justification for why it may have failed or b)\neliminating the influence the factor has. Determining these sensitive factors\nheavily relies on collected data that is diverse enough to cover numerous\ncombinations of these factors, which becomes more onerous when having many\npotential sensitive factors or operating in complex environments. This paper\ninvestigates methods that discover and separate out individual semantic\nsensitive factors from a given dataset to conduct this characterization as well\nas addressing mitigation of these factors' sensitivity. We also broaden\nremediation of fairness, which normally only addresses socially relevant\nfactors, and widen it to deal with the desensitization of AI with regard to all\npossible aspects of variation in the domain. The proposed methods which\ndiscover these major factors reduce the potentially onerous demands of\ncollecting a sufficiently diverse dataset. In experiments using the road sign\n(GTSRB) and facial imagery (CelebA) datasets, we show the promise of using this\nscheme to perform this characterization and remediation and demonstrate that\nour approach outperforms state of the art approaches.",
          "link": "http://arxiv.org/abs/2107.13625",
          "publishedOn": "2021-07-30T02:13:27.728Z",
          "wordCount": 674,
          "title": "Generalizing Fairness: Discovery and Mitigation of Unknown Sensitive Attributes. (arXiv:2107.13625v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinmin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinzhu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jun Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>",
          "description": "As a crucial task of autonomous driving, 3D object detection has made great\nprogress in recent years. However, monocular 3D object detection remains a\nchallenging problem due to the unsatisfactory performance in depth estimation.\nMost existing monocular methods typically directly regress the scene depth\nwhile ignoring important relationships between the depth and various geometric\nelements (e.g. bounding box sizes, 3D object dimensions, and object poses). In\nthis paper, we propose to learn geometry-guided depth estimation with\nprojective modeling to advance monocular 3D object detection. Specifically, a\nprincipled geometry formula with projective modeling of 2D and 3D depth\npredictions in the monocular 3D object detection network is devised. We further\nimplement and embed the proposed formula to enable geometry-aware deep\nrepresentation learning, allowing effective 2D and 3D interactions for boosting\nthe depth estimation. Moreover, we provide a strong baseline through addressing\nsubstantial misalignment between 2D annotation and projected boxes to ensure\nrobust learning with the proposed geometric formula. Experiments on the KITTI\ndataset show that our method remarkably improves the detection performance of\nthe state-of-the-art monocular-based method without extra data by 2.80% on the\nmoderate test setting. The model and code will be released at\nhttps://github.com/YinminZhang/MonoGeo.",
          "link": "http://arxiv.org/abs/2107.13931",
          "publishedOn": "2021-07-30T02:13:27.720Z",
          "wordCount": 650,
          "title": "Learning Geometry-Guided Depth via Projective Modeling for Monocular 3D Object Detection. (arXiv:2107.13931v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korschens_M/0/1/0/all/0/1\">Matthias K&#xf6;rschens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodesheim_P/0/1/0/all/0/1\">Paul Bodesheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romermann_C/0/1/0/all/0/1\">Christine R&#xf6;mermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucher_S/0/1/0/all/0/1\">Solveig Franziska Bucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Migliavacca_M/0/1/0/all/0/1\">Mirco Migliavacca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulrich_J/0/1/0/all/0/1\">Josephine Ulrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>",
          "description": "Monitoring the responses of plants to environmental changes is essential for\nplant biodiversity research. This, however, is currently still being done\nmanually by botanists in the field. This work is very laborious, and the data\nobtained is, though following a standardized method to estimate plant coverage,\nusually subjective and has a coarse temporal resolution. To remedy these\ncaveats, we investigate approaches using convolutional neural networks (CNNs)\nto automatically extract the relevant data from images, focusing on plant\ncommunity composition and species coverages of 9 herbaceous plant species. To\nthis end, we investigate several standard CNN architectures and different\npretraining methods. We find that we outperform our previous approach at higher\nimage resolutions using a custom CNN with a mean absolute error of 5.16%. In\naddition to these investigations, we also conduct an error analysis based on\nthe temporal aspect of the plant cover images. This analysis gives insight into\nwhere problems for automatic approaches lie, like occlusion and likely\nmisclassifications caused by temporal changes.",
          "link": "http://arxiv.org/abs/2106.11154",
          "publishedOn": "2021-07-29T02:00:11.252Z",
          "wordCount": 644,
          "title": "Automatic Plant Cover Estimation with Convolutional Neural Networks. (arXiv:2106.11154v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.03860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pellikka_M/0/1/0/all/0/1\">Matti Pellikka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahtinen_V/0/1/0/all/0/1\">Valtteri Lahtinen</a>",
          "description": "We propose a novel method for large-scale image stitching that is robust\nagainst repetitive patterns and featureless regions in the imagery. In such\ncases, state-of-the-art image stitching methods easily produce image alignment\nartifacts, since they may produce false pairwise image registrations that are\nin conflict within the global connectivity graph. Our method augments the\ncurrent methods by collecting all the plausible pairwise image registration\ncandidates, among which globally consistent candidates are chosen. This enables\nthe stitching process to determine the correct pairwise registrations by\nutilizing all the available information from the whole imagery, such as\nunambiguous registrations outside the repeating pattern and featureless\nregions. We formalize the method as a weighted multigraph whose nodes represent\nthe individual image transformations from the composite image, and whose sets\nof multiple edges between two nodes represent all the plausible transformations\nbetween the pixel coordinates of the two images. The edge weights represent the\nplausibility of the transformations. The image transformations and the edge\nweights are solved from a non-linear minimization problem with linear\nconstraints, for which a projection method is used. As an example, we apply the\nmethod in a large-scale scanning application where the transformations are\nprimarily translations with only slight rotation and scaling component. Despite\nthese simplifications, the state-of-the-art methods do not produce adequate\nresults in such applications, since the image overlap is small, which can be\nfeatureless or repetitive, and misalignment artifacts and their concealment are\nunacceptable.",
          "link": "http://arxiv.org/abs/2004.03860",
          "publishedOn": "2021-07-29T02:00:11.244Z",
          "wordCount": 717,
          "title": "A Robust Method for Image Stitching. (arXiv:2004.03860v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shepley_A/0/1/0/all/0/1\">Andrew Shepley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falzon_G/0/1/0/all/0/1\">Greg Falzon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwan_P/0/1/0/all/0/1\">Paul Kwan</a>",
          "description": "Confluence is a novel non-Intersection over Union (IoU) alternative to\nNon-Maxima Suppression (NMS) in bounding box post-processing in object\ndetection. It overcomes the inherent limitations of IoU-based NMS variants to\nprovide a more stable, consistent predictor of bounding box clustering by using\na normalized Manhattan Distance inspired proximity metric to represent bounding\nbox clustering. Unlike Greedy and Soft NMS, it does not rely solely on\nclassification confidence scores to select optimal bounding boxes, instead\nselecting the box which is closest to every other box within a given cluster\nand removing highly confluent neighboring boxes. Confluence is experimentally\nvalidated on the MS COCO and CrowdHuman benchmarks, improving Average Precision\nby up to 2.3-3.8% and Average Recall by up to 5.3-7.2% when compared against\nde-facto standard and state of the art NMS variants. Quantitative results are\nsupported by extensive qualitative analysis and threshold sensitivity analysis\nexperiments support the conclusion that Confluence is more robust than NMS\nvariants. Confluence represents a paradigm shift in bounding box processing,\nwith potential to replace IoU in bounding box regression processes.",
          "link": "http://arxiv.org/abs/2012.00257",
          "publishedOn": "2021-07-29T02:00:11.228Z",
          "wordCount": 645,
          "title": "Confluence: A Robust Non-IoU Alternative to Non-Maxima Suppression in Object Detection. (arXiv:2012.00257v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hagemann_A/0/1/0/all/0/1\">Annika Hagemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knorr_M/0/1/0/all/0/1\">Moritz Knorr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janssen_H/0/1/0/all/0/1\">Holger Janssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1\">Christoph Stiller</a>",
          "description": "Accurate camera calibration is a precondition for many computer vision\napplications. Calibration errors, such as wrong model assumptions or imprecise\nparameter estimation, can deteriorate a system's overall performance, making\nthe reliable detection and quantification of these errors critical. In this\nwork, we introduce an evaluation scheme to capture the fundamental error\nsources in camera calibration: systematic errors (biases) and uncertainty\n(variance). The proposed bias detection method uncovers smallest systematic\nerrors and thereby reveals imperfections of the calibration setup and provides\nthe basis for camera model selection. A novel resampling-based uncertainty\nestimator enables uncertainty estimation under non-ideal conditions and thereby\nextends the classical covariance estimator. Furthermore, we derive a simple\nuncertainty metric that is independent of the camera model. In combination, the\nproposed methods can be used to assess the accuracy of individual calibrations,\nbut also to benchmark new calibration algorithms, camera models, or calibration\nsetups. We evaluate the proposed methods with simulations and real cameras.",
          "link": "http://arxiv.org/abs/2107.13484",
          "publishedOn": "2021-07-29T02:00:11.192Z",
          "wordCount": 589,
          "title": "Inferring bias and uncertainty in camera calibration. (arXiv:2107.13484v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenhua Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "In this work, we propose an adversarial unsupervised domain adaptation (UDA)\napproach with the inherent conditional and label shifts, in which we aim to\nalign the distributions w.r.t. both $p(x|y)$ and $p(y)$. Since the label is\ninaccessible in the target domain, the conventional adversarial UDA assumes\n$p(y)$ is invariant across domains, and relies on aligning $p(x)$ as an\nalternative to the $p(x|y)$ alignment. To address this, we provide a thorough\ntheoretical and empirical analysis of the conventional adversarial UDA methods\nunder both conditional and label shifts, and propose a novel and practical\nalternative optimization scheme for adversarial UDA. Specifically, we infer the\nmarginal $p(y)$ and align $p(x|y)$ iteratively in the training, and precisely\nalign the posterior $p(y|x)$ in testing. Our experimental results demonstrate\nits effectiveness on both classification and segmentation UDA, and partial UDA.",
          "link": "http://arxiv.org/abs/2107.13469",
          "publishedOn": "2021-07-29T02:00:11.185Z",
          "wordCount": 605,
          "title": "Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate. (arXiv:2107.13469v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03577",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Franco_Barranco_D/0/1/0/all/0/1\">Daniel Franco-Barranco</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munoz_Barrutia_A/0/1/0/all/0/1\">Arrate Mu&#xf1;oz-Barrutia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arganda_Carreras_I/0/1/0/all/0/1\">Ignacio Arganda-Carreras</a>",
          "description": "Electron microscopy (EM) allows the identification of intracellular\norganelles such as mitochondria, providing insights for clinical and scientific\nstudies. In recent years, a number of novel deep learning architectures have\nbeen published reporting superior performance, or even human-level accuracy,\ncompared to previous approaches on public mitochondria segmentation datasets.\nUnfortunately, many of these publications do not make neither the code nor the\nfull training details public to support the results obtained, leading to\nreproducibility issues and dubious model comparisons. For that reason, and\nfollowing a recent code of best practices for reporting experimental results,\nwe present an extensive study of the state-of-the-art deep learning\narchitectures for the segmentation of mitochondria on EM volumes, and evaluate\nthe impact in performance of different variations of 2D and 3D U-Net-like\nmodels for this task. To better understand the contribution of each component,\na common set of pre- and post-processing operations has been implemented and\ntested with each approach. Moreover, an exhaustive sweep of hyperparameters\nvalues for all architectures have been performed and each configuration has\nbeen run multiple times to report the mean and standard deviation values of the\nevaluation metrics. Using this methodology, we found very stable architectures\nand hyperparameter configurations that consistently obtain state-of-the-art\nresults in the well-known EPFL Hippocampus mitochondria segmentation dataset.\nFurthermore, we have benchmarked our proposed models on two other available\ndatasets, Lucchi++ and Kasthuri++, where they outperform all previous works.\nThe code derived from this research and its documentation are publicly\navailable.",
          "link": "http://arxiv.org/abs/2104.03577",
          "publishedOn": "2021-07-29T02:00:11.149Z",
          "wordCount": 711,
          "title": "Stable deep neural network architectures for mitochondria segmentation on electron microscopy volumes. (arXiv:2104.03577v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1\">Maria Pia Del Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiller_D/0/1/0/all/0/1\">Dario Spiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1\">Pierre Philippe Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "In recent years, the growth of Machine Learning (ML) algorithms has raised\nthe number of studies including their applicability in a variety of different\nscenarios. Among all, one of the hardest ones is the aerospace, due to its\npeculiar physical requirements. In this context, a feasibility study and a\nfirst prototype for an Artificial Intelligence (AI) model to be deployed on\nboard satellites are presented in this work. As a case study, the detection of\nvolcanic eruptions has been investigated as a method to swiftly produce alerts\nand allow immediate interventions. Two Convolutional Neural Networks (CNNs)\nhave been proposed and designed, showing how to efficiently implement them for\nidentifying the eruptions and at the same time adapting their complexity in\norder to fit on board requirements.",
          "link": "http://arxiv.org/abs/2106.15281",
          "publishedOn": "2021-07-29T02:00:10.339Z",
          "wordCount": 602,
          "title": "On Board Volcanic Eruption Detection through CNNs and Satellite Multispectral Imagery. (arXiv:2106.15281v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yue Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Liangxiu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleerekoper_A/0/1/0/all/0/1\">Anthony Kleerekoper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Sheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tongle Hu</a>",
          "description": "Late blight disease is one of the most destructive diseases in potato crop,\nleading to serious yield losses globally. Accurate diagnosis of the disease at\nearly stage is critical for precision disease control and management. Current\nfarm practices in crop disease diagnosis are based on manual visual inspection,\nwhich is costly, time consuming, subject to individual bias. Recent advances in\nimaging sensors (e.g. RGB, multiple spectral and hyperspectral cameras), remote\nsensing and machine learning offer the opportunity to address this challenge.\nParticularly, hyperspectral imagery (HSI) combining with machine learning/deep\nlearning approaches is preferable for accurately identifying specific plant\ndiseases because the HSI consists of a wide range of high-quality reflectance\ninformation beyond human vision, capable of capturing both spectral-spatial\ninformation. The proposed method considers the potential disease specific\nreflectance radiation variance caused by the canopy structural diversity,\nintroduces the multiple capsule layers to model the hierarchical structure of\nthe spectral-spatial disease attributes with the encapsulated features to\nrepresent the various classes and the rotation invariance of the disease\nattributes in the feature space. We have evaluated the proposed method with the\nreal UAV-based HSI data under the controlled field conditions. The\neffectiveness of the hierarchical features has been quantitatively assessed and\ncompared with the existing representative machine learning/deep learning\nmethods. The experiment results show that the proposed model significantly\nimproves the accuracy performance when considering hierarchical-structure of\nspectral-spatial features, comparing to the existing methods only using\nspectral, or spatial or spectral-spatial features without consider\nhierarchical-structure of spectral-spatial features.",
          "link": "http://arxiv.org/abs/2107.13277",
          "publishedOn": "2021-07-29T02:00:10.166Z",
          "wordCount": 707,
          "title": "A Novel CropdocNet for Automated Potato Late Blight Disease Detection from the Unmanned Aerial Vehicle-based Hyperspectral Imagery. (arXiv:2107.13277v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feeney_P/0/1/0/all/0/1\">Patrick Feeney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1\">Michael C. Hughes</a>",
          "description": "The pixelwise reconstruction error of deep autoencoders is often utilized for\nimage novelty detection and localization under the assumption that pixels with\nhigh error indicate which parts of the input image are unfamiliar and therefore\nlikely to be novel. This assumed correlation between pixels with high\nreconstruction error and novel regions of input images has not been verified\nand may limit the accuracy of these methods. In this paper we utilize saliency\nmaps to evaluate whether this correlation exists. Saliency maps reveal directly\nhow much a change in each input pixel would affect reconstruction loss, while\neach pixel's reconstruction error may be attributed to many input pixels when\nlayers are fully connected. We compare saliency maps to reconstruction error\nmaps via qualitative visualizations as well as quantitative correspondence\nbetween the top K elements of the maps for both novel and normal images. Our\nresults indicate that reconstruction error maps do not closely correlate with\nthe importance of pixels in the input images, making them insufficient for\nnovelty localization.",
          "link": "http://arxiv.org/abs/2107.13379",
          "publishedOn": "2021-07-29T02:00:10.151Z",
          "wordCount": 606,
          "title": "Evaluating the Use of Reconstruction Error for Novelty Localization. (arXiv:2107.13379v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiufu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Sheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1\">Zhihui Lai</a>",
          "description": "Though widely used in image classification, convolutional neural networks\n(CNNs) are prone to noise interruptions, i.e. the CNN output can be drastically\nchanged by small image noise. To improve the noise robustness, we try to\nintegrate CNNs with wavelet by replacing the common down-sampling (max-pooling,\nstrided-convolution, and average pooling) with discrete wavelet transform\n(DWT). We firstly propose general DWT and inverse DWT (IDWT) layers applicable\nto various orthogonal and biorthogonal discrete wavelets like Haar, Daubechies,\nand Cohen, etc., and then design wavelet integrated CNNs (WaveCNets) by\nintegrating DWT into the commonly used CNNs (VGG, ResNets, and DenseNet).\nDuring the down-sampling, WaveCNets apply DWT to decompose the feature maps\ninto the low-frequency and high-frequency components. Containing the main\ninformation including the basic object structures, the low-frequency component\nis transmitted into the following layers to generate robust high-level\nfeatures. The high-frequency components are dropped to remove most of the data\nnoises. The experimental results show that %wavelet accelerates the CNN\ntraining, and WaveCNets achieve higher accuracy on ImageNet than various\nvanilla CNNs. We have also tested the performance of WaveCNets on the noisy\nversion of ImageNet, ImageNet-C and six adversarial attacks, the results\nsuggest that the proposed DWT/IDWT layers could provide better noise-robustness\nand adversarial robustness. When applying WaveCNets as backbones, the\nperformance of object detectors (i.e., faster R-CNN and RetinaNet) on COCO\ndetection dataset are consistently improved. We believe that suppression of\naliasing effect, i.e. separation of low frequency and high frequency\ninformation, is the main advantages of our approach. The code of our DWT/IDWT\nlayer and different WaveCNets are available at\nhttps://github.com/CVI-SZU/WaveCNet.",
          "link": "http://arxiv.org/abs/2107.13335",
          "publishedOn": "2021-07-29T02:00:10.141Z",
          "wordCount": 720,
          "title": "WaveCNet: Wavelet Integrated CNNs to Suppress Aliasing Effect for Noise-Robust Image Classification. (arXiv:2107.13335v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhigao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yin Zhao</a>",
          "description": "We conduct a subjective experiment to compare the performance of traditional\nimage coding methods and learning-based image coding methods. HEVC and VVC, the\nstate-of-the-art traditional coding methods, are used as the representative\ntraditional methods. The learning-based methods used contain not only CNN-based\nmethods, but also a GAN-based method, all of which are advanced or typical.\nSingle Stimuli (SS), which is also called Absolute Category Rating (ACR), is\nadopted as the methodology of the experiment to obtain perceptual quality of\nimages. Additionally, we utilize some typical and frequently used objective\nquality metrics to evaluate the coding methods in the experiment as comparison.\nThe experiment shows that CNN-based and GAN-based methods can perform better\nthan traditional methods in low bit-rates. In high bit-rates, however, it is\nhard to verify whether CNN-based methods are superior to traditional methods.\nBecause the GAN method does not provide models with high target bit-rates, we\ncannot exactly tell the performance of the GAN method in high bit-rates.\nFurthermore, some popular objective quality metrics have not shown the ability\nwell to measure quality of images generated by learning-based coding methods,\nespecially the GAN-based one.",
          "link": "http://arxiv.org/abs/2107.13122",
          "publishedOn": "2021-07-29T02:00:10.118Z",
          "wordCount": 631,
          "title": "Subjective evaluation of traditional and learning-based image coding methods. (arXiv:2107.13122v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenjiang Liu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zunlei Feng</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chengji Shen</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Ou_K/0/1/0/all/0/1\">Kairi Ou</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haihong Tang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a> (2) ((1) Alibaba Group, (2) Zhejiang University)",
          "description": "Image virtual try-on task has abundant applications and has become a hot\nresearch topic recently. Existing 2D image-based virtual try-on methods aim to\ntransfer a target clothing image onto a reference person, which has two main\ndisadvantages: cannot control the size and length precisely; unable to\naccurately estimate the user's figure in the case of users wearing thick\nclothes, resulting in inaccurate dressing effect. In this paper, we put forward\nan akin task that aims to dress clothing for underwear models. %, which is also\nan urgent need in e-commerce scenarios. To solve the above drawbacks, we\npropose a Shape Controllable Virtual Try-On Network (SC-VTON), where a graph\nattention network integrates the information of model and clothing to generate\nthe warped clothing image. In addition, the control points are incorporated\ninto SC-VTON for the desired clothing shape. Furthermore, by adding a Splitting\nNetwork and a Synthesis Network, we can use clothing/model pair data to help\noptimize the deformation module and generalize the task to the typical virtual\ntry-on task. Extensive experiments show that the proposed method can achieve\naccurate shape control. Meanwhile, compared with other methods, our method can\ngenerate high-resolution results with detailed textures.",
          "link": "http://arxiv.org/abs/2107.13156",
          "publishedOn": "2021-07-29T02:00:10.105Z",
          "wordCount": 658,
          "title": "Shape Controllable Virtual Try-on for Underwear Models. (arXiv:2107.13156v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yanda Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongrun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yitian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xuesheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaowei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yalin Zheng</a>",
          "description": "Semi-supervised approaches for crowd counting attract attention, as the fully\nsupervised paradigm is expensive and laborious due to its request for a large\nnumber of images of dense crowd scenarios and their annotations. This paper\nproposes a spatial uncertainty-aware semi-supervised approach via regularized\nsurrogate task (binary segmentation) for crowd counting problems. Different\nfrom existing semi-supervised learning-based crowd counting methods, to exploit\nthe unlabeled data, our proposed spatial uncertainty-aware teacher-student\nframework focuses on high confident regions' information while addressing the\nnoisy supervision from the unlabeled data in an end-to-end manner.\nSpecifically, we estimate the spatial uncertainty maps from the teacher model's\nsurrogate task to guide the feature learning of the main task (density\nregression) and the surrogate task of the student model at the same time.\nBesides, we introduce a simple yet effective differential transformation layer\nto enforce the inherent spatial consistency regularization between the main\ntask and the surrogate task in the student model, which helps the surrogate\ntask to yield more reliable predictions and generates high-quality uncertainty\nmaps. Thus, our model can also address the task-level perturbation problems\nthat occur spatial inconsistency between the primary and surrogate tasks in the\nstudent model. Experimental results on four challenging crowd counting datasets\ndemonstrate that our method achieves superior performance to the\nstate-of-the-art semi-supervised methods.",
          "link": "http://arxiv.org/abs/2107.13271",
          "publishedOn": "2021-07-29T02:00:10.004Z",
          "wordCount": 657,
          "title": "Spatial Uncertainty-Aware Semi-Supervised Crowd Counting. (arXiv:2107.13271v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1\">Laurent Kneip</a>",
          "description": "Camera calibration is an important prerequisite towards the solution of 3D\ncomputer vision problems. Traditional methods rely on static images of a\ncalibration pattern. This raises interesting challenges towards the practical\nusage of event cameras, which notably require image change to produce\nsufficient measurements. The current standard for event camera calibration\ntherefore consists of using flashing patterns. They have the advantage of\nsimultaneously triggering events in all reprojected pattern feature locations,\nbut it is difficult to construct or use such patterns in the field. We present\nthe first dynamic event camera calibration algorithm. It calibrates directly\nfrom events captured during relative motion between camera and calibration\npattern. The method is propelled by a novel feature extraction mechanism for\ncalibration patterns, and leverages existing calibration tools before\noptimizing all parameters through a multi-segment continuous-time formulation.\nAs demonstrated through our results on real data, the obtained calibration\nmethod is highly convenient and reliably calibrates from data sequences\nspanning less than 10 seconds.",
          "link": "http://arxiv.org/abs/2107.06749",
          "publishedOn": "2021-07-29T02:00:08.913Z",
          "wordCount": 629,
          "title": "Dynamic Event Camera Calibration. (arXiv:2107.06749v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sayak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>",
          "description": "Floods wreak havoc throughout the world, causing billions of dollars in\ndamages, and uprooting communities, ecosystems and economies. Accurate and\nrobust flood detection including delineating open water flood areas and\nidentifying flood levels can aid in disaster response and mitigation. However,\nestimating flood levels remotely is of essence as physical access to flooded\nareas is limited and the ability to deploy instruments in potential flood zones\ncan be dangerous. Aligning flood extent mapping with local topography can\nprovide a plan-of-action that the disaster response team can consider. Thus,\nremote flood level estimation via satellites like Sentinel-1 can prove to be\nremedial. The Emerging Techniques in Computational Intelligence (ETCI)\ncompetition on Flood Detection tasked participants with predicting flooded\npixels after training with synthetic aperture radar (SAR) images in a\nsupervised setting. We use a cyclical approach involving two stages (1)\ntraining an ensemble model of multiple UNet architectures with available high\nand low confidence labeled data and, (2) generating pseudo labels or low\nconfidence labels on the unlabeled test dataset, and then, combining the\ngenerated labels with the previously available high confidence labeled dataset.\nThis assimilated dataset is used for the next round of training ensemble\nmodels. This cyclical process is repeated until the performance improvement\nplateaus. Additionally, we post process our results with Conditional Random\nFields. Our approach sets a high score on the public leaderboard for the ETCI\ncompetition with 0.7654 IoU. Our method, which we release with all the code\nincluding trained models, can also be used as an open science benchmark for the\nSentinel-1 released dataset on GitHub. To the best of our knowledge we believe\nthis the first works to try out semi-supervised learning to improve flood\nsegmentation models.",
          "link": "http://arxiv.org/abs/2107.08369",
          "publishedOn": "2021-07-29T02:00:08.850Z",
          "wordCount": 774,
          "title": "Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-29T02:00:08.842Z",
          "wordCount": 700,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogozhnikov_A/0/1/0/all/0/1\">Alex Rogozhnikov</a>",
          "description": "Without positional information, attention-based transformer neural networks\nare permutation-invariant. Absolute or relative positional embeddings are the\nmost popular ways to feed transformer models positional information. Absolute\npositional embeddings are simple to implement, but suffer from generalization\nissues when evaluating on sequences of different length than those seen at\ntraining time. Relative positions are more robust to length change, but are\nmore complex to implement and yield inferior model throughput. In this paper,\nwe propose an augmentation-based approach (CAPE) for absolute positional\nembeddings, which keeps the advantages of both absolute (simplicity and speed)\nand relative position embeddings (better generalization). In addition, our\nempirical evaluation on state-of-the-art models in machine translation, image\nand speech recognition demonstrates that CAPE leads to better generalization\nperformance as well as increased stability with respect to training\nhyper-parameters.",
          "link": "http://arxiv.org/abs/2106.03143",
          "publishedOn": "2021-07-29T02:00:08.818Z",
          "wordCount": 607,
          "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings. (arXiv:2106.03143v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Di Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "3D point cloud completion is very challenging because it heavily relies on\nthe accurate understanding of the complex 3D shapes (e.g., high-curvature,\nconcave/convex, and hollowed-out 3D shapes) and the unknown & diverse patterns\nof the partially available point clouds. In this paper, we propose a novel\nsolution,i.e., Point-block Carving (PC), for completing the complex 3D point\ncloud completion. Given the partial point cloud as the guidance, we carve a3D\nblock that contains the uniformly distributed 3D points, yielding the entire\npoint cloud. To achieve PC, we propose a new network architecture, i.e.,\nCarveNet. This network conducts the exclusive convolution on each point of the\nblock, where the convolutional kernels are trained on the 3D shape data.\nCarveNet determines which point should be carved, for effectively recovering\nthe details of the complete shapes. Furthermore, we propose a sensor-aware\nmethod for data augmentation,i.e., SensorAug, for training CarveNet on richer\npatterns of partial point clouds, thus enhancing the completion power of the\nnetwork. The extensive evaluations on the ShapeNet and KITTI datasets\ndemonstrate the generality of our approach on the partial point clouds with\ndiverse patterns. On these datasets, CarveNet successfully outperforms the\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.13452",
          "publishedOn": "2021-07-29T02:00:08.799Z",
          "wordCount": 643,
          "title": "CarveNet: Carving Point-Block for Complex 3D Shape Completion. (arXiv:2107.13452v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09405",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Schirris_Y/0/1/0/all/0/1\">Yoni Schirris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gavves_E/0/1/0/all/0/1\">Efstratios Gavves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nederlof_I/0/1/0/all/0/1\">Iris Nederlof</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Horlings_H/0/1/0/all/0/1\">Hugo Mark Horlings</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1\">Jonas Teuwen</a>",
          "description": "We propose a Deep learning-based weak label learning method for analysing\nwhole slide images (WSIs) of Hematoxylin and Eosin (H&E) stained tumorcells not\nrequiring pixel-level or tile-level annotations using Self-supervised\npre-training and heterogeneity-aware deep Multiple Instance LEarning\n(DeepSMILE). We apply DeepSMILE to the task of Homologous recombination\ndeficiency (HRD) and microsatellite instability (MSI) prediction. We utilize\ncontrastive self-supervised learning to pre-train a feature extractor on\nhistopathology tiles of cancer tissue. Additionally, we use variability-aware\ndeep multiple instance learning to learn the tile feature aggregation function\nwhile modeling tumor heterogeneity. Compared to state-of-the-art genomic label\nclassification methods, DeepSMILE improves classification performance for HRD\nfrom $70.43\\pm4.10\\%$ to $83.79\\pm1.25\\%$ AUC and MSI from $78.56\\pm6.24\\%$ to\n$90.32\\pm3.58\\%$ AUC in a multi-center breast and colorectal cancer dataset,\nrespectively. These improvements suggest we can improve genomic label\nclassification performance without collecting larger datasets. In the future,\nthis may reduce the need for expensive genome sequencing techniques, provide\npersonalized therapy recommendations based on widely available WSIs of cancer\ntissue, and improve patient care with quicker treatment decisions - also in\nmedical centers without access to genome sequencing resources.",
          "link": "http://arxiv.org/abs/2107.09405",
          "publishedOn": "2021-07-29T02:00:08.791Z",
          "wordCount": 673,
          "title": "DeepSMILE: Self-supervised heterogeneity-aware multiple instance learning for DNA damage response defect classification directly from H&E whole-slide images. (arXiv:2107.09405v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">Edward S. Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1\">Oleh Rybkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1\">Dinesh Jayaraman</a>",
          "description": "Training visuomotor robot controllers from scratch on a new robot typically\nrequires generating large amounts of robot-specific data. Could we leverage\ndata previously collected on another robot to reduce or even completely remove\nthis need for robot-specific data? We propose a \"robot-aware\" solution paradigm\nthat exploits readily available robot \"self-knowledge\" such as proprioception,\nkinematics, and camera calibration to achieve this. First, we learn modular\ndynamics models that pair a transferable, robot-agnostic world dynamics module\nwith a robot-specific, analytical robot dynamics module. Next, we set up visual\nplanning costs that draw a distinction between the robot self and the world.\nOur experiments on tabletop manipulation tasks in simulation and on real robots\ndemonstrate that these plug-in improvements dramatically boost the\ntransferability of visuomotor controllers, even permitting zero-shot transfer\nonto new robots for the very first time. Project website:\nhttps://hueds.github.io/rac/",
          "link": "http://arxiv.org/abs/2107.09047",
          "publishedOn": "2021-07-29T02:00:08.783Z",
          "wordCount": 603,
          "title": "Know Thyself: Transferable Visuomotor Control Through Robot-Awareness. (arXiv:2107.09047v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "Domain adaptation is to transfer the shared knowledge learned from the source\ndomain to a new environment, i.e., target domain. One common practice is to\ntrain the model on both labeled source-domain data and unlabeled target-domain\ndata. Yet the learned models are usually biased due to the strong supervision\nof the source domain. Most researchers adopt the early-stopping strategy to\nprevent over-fitting, but when to stop training remains a challenging problem\nsince the lack of the target-domain validation set. In this paper, we propose\none efficient bootstrapping method, called Adaboost Student, explicitly\nlearning complementary models during training and liberating users from\nempirical early stopping. Adaboost Student combines the deep model learning\nwith the conventional training strategy, i.e., adaptive boosting, and enables\ninteractions between learned models and the data sampler. We adopt one adaptive\ndata sampler to progressively facilitate learning on hard samples and aggregate\n\"weak\" models to prevent over-fitting. Extensive experiments show that (1)\nWithout the need to worry about the stopping time, AdaBoost Student provides\none robust solution by efficient complementary model learning during training.\n(2) AdaBoost Student is orthogonal to most domain adaptation methods, which can\nbe combined with existing approaches to further improve the state-of-the-art\nperformance. We have achieved competitive results on three widely-used scene\nsegmentation domain adaptation benchmarks.",
          "link": "http://arxiv.org/abs/2103.15685",
          "publishedOn": "2021-07-29T02:00:08.776Z",
          "wordCount": 684,
          "title": "Adaptive Boosting for Domain Adaptation: Towards Robust Predictions in Scene Segmentation. (arXiv:2103.15685v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowakowski_A/0/1/0/all/0/1\">Artur Nowakowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puglisi_E/0/1/0/all/0/1\">Erika Puglisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1\">Maria Pia Del Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mifdal_J/0/1/0/all/0/1\">Jamila Mifdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirri_F/0/1/0/all/0/1\">Fiora Pirri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1\">Pierre Philippe Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "The abundance of clouds, located both spatially and temporally, often makes\nremote sensing (RS) applications with optical images difficult or even\nimpossible to perform. Traditional cloud removing techniques have been studied\nfor years, and recently, Machine Learning (ML)-based approaches have also been\nconsidered. In this manuscript, a novel method for the restoration of\nclouds-corrupted optical images is presented, able to generate the whole\noptical scene of interest, not only the cloudy pixels, and based on a Joint\nData Fusion paradigm, where three deep neural networks are hierarchically\ncombined. Spatio-temporal features are separately extracted by a conditional\nGenerative Adversarial Network (cGAN) and by a Convolutional Long Short-Term\nMemory (ConvLSTM), from Synthetic Aperture Radar (SAR) data and optical\ntime-series of data respectively, and then combined with a U-shaped network.\nThe use of time-series of data has been rarely explored in the state of the art\nfor this peculiar objective, and moreover existing models do not combine both\nspatio-temporal domains and SAR-optical imagery. Quantitative and qualitative\nresults have shown a good ability of the proposed method in producing\ncloud-free images, by also preserving the details and outperforming the cGAN\nand the ConvLSTM when individually used. Both the code and the dataset have\nbeen implemented from scratch and made available to interested researchers for\nfurther analysis and investigation.",
          "link": "http://arxiv.org/abs/2106.12226",
          "publishedOn": "2021-07-29T02:00:08.768Z",
          "wordCount": 710,
          "title": "Spatio-Temporal SAR-Optical Data Fusion for Cloud Removal via a Deep Hierarchical Model. (arXiv:2106.12226v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhengbo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zitang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weilian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamata_S/0/1/0/all/0/1\">Sei-ichiro Kamata</a>",
          "description": "Various deep neural network architectures (DNNs) maintain massive vital\nrecords in computer vision. While drawing attention worldwide, the design of\nthe overall structure lacks general guidance. Based on the relationship between\nDNN design and numerical differential equations, we performed a fair comparison\nof the residual design with higher-order perspectives. We show that the widely\nused DNN design strategy, constantly stacking a small design (usually 2-3\nlayers), could be easily improved, supported by solid theoretical knowledge and\nwith no extra parameters needed. We reorganise the residual design in\nhigher-order ways, which is inspired by the observation that many effective\nnetworks can be interpreted as different numerical discretisations of\ndifferential equations. The design of ResNet follows a relatively simple\nscheme, which is Euler forward; however, the situation becomes complicated\nrapidly while stacking. We suppose that stacked ResNet is somehow equalled to a\nhigher-order scheme; then, the current method of forwarding propagation might\nbe relatively weak compared with a typical high-order method such as\nRunge-Kutta. We propose HO-ResNet to verify the hypothesis of widely used CV\nbenchmarks with sufficient experiments. Stable and noticeable increases in\nperformance are observed, and convergence and robustness are also improved. Our\nstacking strategy improved ResNet-30 by 2.15 per cent and ResNet-58 by 2.35 per\ncent on CIFAR-10, with the same settings and parameters. The proposed strategy\nis fundamental and theoretical and can therefore be applied to any network as a\ngeneral guideline.",
          "link": "http://arxiv.org/abs/2103.15244",
          "publishedOn": "2021-07-29T02:00:08.760Z",
          "wordCount": 727,
          "title": "Rethinking ResNets: Improved Stacking Strategies With High Order Schemes. (arXiv:2103.15244v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Terry_J/0/1/0/all/0/1\">J. K. Terry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayakumar_M/0/1/0/all/0/1\">Mario Jayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alwis_K/0/1/0/all/0/1\">Kusal De Alwis</a>",
          "description": "The general approach taken when training deep learning classifiers is to save\nthe parameters after every few iterations, train until either a human observer\nor a simple metric-based heuristic decides the network isn't learning anymore,\nand then backtrack and pick the saved parameters with the best validation\naccuracy. Simple methods are used to determine if a neural network isn't\nlearning anymore because, as long as it's well after the optimal values are\nfound, the condition doesn't impact the final accuracy of the model. However\nfrom a runtime perspective, this is of great significance to the many cases\nwhere numerous neural networks are trained simultaneously (e.g. hyper-parameter\ntuning). Motivated by this, we introduce a statistical significance test to\ndetermine if a neural network has stopped learning. This stopping criterion\nappears to represent a happy medium compared to other popular stopping\ncriterions, achieving comparable accuracy to the criterions that achieve the\nhighest final accuracies in 77% or fewer epochs, while the criterions which\nstop sooner do so with an appreciable loss to final accuracy. Additionally, we\nuse this as the basis of a new learning rate scheduler, removing the need to\nmanually choose learning rate schedules and acting as a quasi-line search,\nachieving superior or comparable empirical performance to existing methods.",
          "link": "http://arxiv.org/abs/2103.01205",
          "publishedOn": "2021-07-29T02:00:08.752Z",
          "wordCount": 691,
          "title": "Statistically Significant Stopping of Neural Network Training. (arXiv:2103.01205v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dongming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Weakly-supervised temporal action localization (WS-TAL) aims to localize\nactions in untrimmed videos with only video-level labels. Most existing models\nfollow the \"localization by classification\" procedure: locate temporal regions\ncontributing most to the video-level classification. Generally, they process\neach snippet (or frame) individually and thus overlook the fruitful temporal\ncontext relation. Here arises the single snippet cheating issue: \"hard\"\nsnippets are too vague to be classified. In this paper, we argue that learning\nby comparing helps identify these hard snippets and we propose to utilize\nsnippet Contrastive learning to Localize Actions, CoLA for short. Specifically,\nwe propose a Snippet Contrast (SniCo) Loss to refine the hard snippet\nrepresentation in feature space, which guides the network to perceive precise\ntemporal boundaries and avoid the temporal interval interruption. Besides,\nsince it is infeasible to access frame-level annotations, we introduce a Hard\nSnippet Mining algorithm to locate the potential hard snippets. Substantial\nanalyses verify that this mining strategy efficaciously captures the hard\nsnippets and SniCo Loss leads to more informative feature representation.\nExtensive experiments show that CoLA achieves state-of-the-art results on\nTHUMOS'14 and ActivityNet v1.2 datasets. CoLA code is publicly available at\nhttps://github.com/zhang-can/CoLA.",
          "link": "http://arxiv.org/abs/2103.16392",
          "publishedOn": "2021-07-29T02:00:08.728Z",
          "wordCount": 669,
          "title": "CoLA: Weakly-Supervised Temporal Action Localization with Snippet Contrastive Learning. (arXiv:2103.16392v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morales_D/0/1/0/all/0/1\">David Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talavera_E/0/1/0/all/0/1\">Estefania Talavera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remeseiro_B/0/1/0/all/0/1\">Beatriz Remeseiro</a>",
          "description": "The field of deep learning is evolving in different directions, with still\nthe need for more efficient training strategies. In this work, we present a\nnovel and robust training scheme that integrates visual explanation techniques\nin the learning process. Unlike the attention mechanisms that focus on the\nrelevant parts of images, we aim to improve the robustness of the model by\nmaking it pay attention to other regions as well. Broadly speaking, the idea is\nto distract the classifier in the learning process to force it to focus not\nonly on relevant regions but also on those that, a priori, are not so\ninformative for the discrimination of the class. We tested the proposed\napproach by embedding it into the learning process of a convolutional neural\nnetwork for the analysis and classification of two well-known datasets, namely\nStanford cars and FGVC-Aircraft. Furthermore, we evaluated our model on a\nreal-case scenario for the classification of egocentric images, allowing us to\nobtain relevant information about peoples' lifestyles. In particular, we work\non the challenging EgoFoodPlaces dataset, achieving state-of-the-art results\nwith a lower level of complexity. The obtained results indicate the suitability\nof our proposed training scheme for image classification, improving the\nrobustness of the final model.",
          "link": "http://arxiv.org/abs/2012.14173",
          "publishedOn": "2021-07-29T02:00:08.714Z",
          "wordCount": 696,
          "title": "Playing to distraction: towards a robust training of CNN classifiers through visual explanation techniques. (arXiv:2012.14173v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaoqing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>",
          "description": "Crowd counting has drawn much attention due to its importance in\nsafety-critical surveillance systems. Especially, deep neural network (DNN)\nmethods have significantly reduced estimation errors for crowd counting\nmissions. Recent studies have demonstrated that DNNs are vulnerable to\nadversarial attacks, i.e., normal images with human-imperceptible perturbations\ncould mislead DNNs to make false predictions. In this work, we propose a robust\nattack strategy called Adversarial Patch Attack with Momentum (APAM) to\nsystematically evaluate the robustness of crowd counting models, where the\nattacker's goal is to create an adversarial perturbation that severely degrades\ntheir performances, thus leading to public safety accidents (e.g., stampede\naccidents). Especially, the proposed attack leverages the extreme-density\nbackground information of input images to generate robust adversarial patches\nvia a series of transformations (e.g., interpolation, rotation, etc.). We\nobserve that by perturbing less than 6\\% of image pixels, our attacks severely\ndegrade the performance of crowd counting systems, both digitally and\nphysically. To better enhance the adversarial robustness of crowd counting\nmodels, we propose the first regression model-based Randomized Ablation (RA),\nwhich is more sufficient than Adversarial Training (ADT) (Mean Absolute Error\nof RA is 5 lower than ADT on clean samples and 30 lower than ADT on adversarial\nexamples). Extensive experiments on five crowd counting models demonstrate the\neffectiveness and generality of the proposed method. The supplementary\nmaterials and certificate retrained models are available at\n\\url{https://www.dropbox.com/s/hc4fdx133vht0qb/ACM_MM2021_Supp.pdf?dl=0}",
          "link": "http://arxiv.org/abs/2104.10868",
          "publishedOn": "2021-07-29T02:00:08.692Z",
          "wordCount": 719,
          "title": "Towards Adversarial Patch Analysis and Certified Defense against Crowd Counting. (arXiv:2104.10868v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Somanath_G/0/1/0/all/0/1\">Gowri Somanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1\">Daniel Kurz</a>",
          "description": "We present a method to estimate an HDR environment map from a narrow\nfield-of-view LDR camera image in real-time. This enables perceptually\nappealing reflections and shading on virtual objects of any material finish,\nfrom mirror to diffuse, rendered into a real physical environment using\naugmented reality. Our method is based on our efficient convolutional neural\nnetwork architecture, EnvMapNet, trained end-to-end with two novel losses,\nProjectionLoss for the generated image, and ClusterLoss for adversarial\ntraining. Through qualitative and quantitative comparison to state-of-the-art\nmethods, we demonstrate that our algorithm reduces the directional error of\nestimated light sources by more than 50%, and achieves 3.7 times lower Frechet\nInception Distance (FID). We further showcase a mobile application that is able\nto run our neural network model in under 9 ms on an iPhone XS, and render in\nreal-time, visually coherent virtual objects in previously unseen real-world\nenvironments.",
          "link": "http://arxiv.org/abs/2011.10687",
          "publishedOn": "2021-07-29T02:00:08.676Z",
          "wordCount": 658,
          "title": "HDR Environment Map Estimation for Real-Time Augmented Reality. (arXiv:2011.10687v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.03064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1\">Xuefei Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Changcheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenshuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zixuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shuang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huazhong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>",
          "description": "Conducting efficient performance estimations of neural architectures is a\nmajor challenge in neural architecture search (NAS). To reduce the architecture\ntraining costs in NAS, one-shot estimators (OSEs) amortize the architecture\ntraining costs by sharing the parameters of one supernet between all\narchitectures. Recently, zero-shot estimators (ZSEs) that involve no training\nare proposed to further reduce the architecture evaluation cost. Despite the\nhigh efficiency of these estimators, the quality of such estimations has not\nbeen thoroughly studied. In this paper, we conduct an extensive and organized\nassessment of OSEs and ZSEs on three NAS benchmarks: NAS-Bench-101/201/301.\nSpecifically, we employ a set of NAS-oriented criteria to study the behavior of\nOSEs and ZSEs and reveal that they have certain biases and variances. After\nanalyzing how and why the OSE estimations are unsatisfying, we explore how to\nmitigate the correlation gap of OSEs from several perspectives. For ZSEs, we\nfind that current ZSEs are not satisfying enough in these benchmark search\nspaces, and analyze their biases. Through our analysis, we give out suggestions\nfor future application and development of efficient architecture performance\nestimators. Furthermore, the analysis framework proposed in our work could be\nutilized in future research to give a more comprehensive understanding of newly\ndesigned architecture performance estimators. All codes and analysis scripts\nare available at https://github.com/walkerning/aw_nas.",
          "link": "http://arxiv.org/abs/2008.03064",
          "publishedOn": "2021-07-29T02:00:08.658Z",
          "wordCount": 706,
          "title": "Evaluating Efficient Performance Estimators of Neural Architectures. (arXiv:2008.03064v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiunn-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chenxi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achar_M/0/1/0/all/0/1\">Madhav Achar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1\">Maani Ghaffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grizzle_J/0/1/0/all/0/1\">Jessy W. Grizzle</a>",
          "description": "Sensor calibration, which can be intrinsic or extrinsic, is an essential step\nto achieve the measurement accuracy required for modern perception and\nnavigation systems deployed on autonomous robots. To date, intrinsic\ncalibration models for spinning LiDARs have been based on hypothesized based on\ntheir physical mechanisms, resulting in anywhere from three to ten parameters\nto be estimated from data, while no phenomenological models have yet been\nproposed for solid-state LiDARs. Instead of going down that road, we propose to\nabstract away from the physics of a LiDAR type (spinning vs solid-state, for\nexample), and focus on the spatial geometry of the point cloud generated by the\nsensor. By modeling the calibration parameters as an element of a special\nmatrix Lie Group, we achieve a unifying view of calibration for different types\nof LiDARs. We further prove mathematically that the proposed model is\nwell-constrained (has a unique answer) given four appropriately orientated\ntargets. The proof provides a guideline for target positioning in the form of a\ntetrahedron. Moreover, an existing Semidefinite programming global solver for\nSE(3) can be modified to compute efficiently the optimal calibration\nparameters. For solid state LiDARs, we illustrate how the method works in\nsimulation. For spinning LiDARs, we show with experimental data that the\nproposed matrix Lie Group model performs equally well as physics-based models\nin terms of reducing the P2P distance, while being more robust to noise.",
          "link": "http://arxiv.org/abs/2012.03321",
          "publishedOn": "2021-07-29T02:00:08.644Z",
          "wordCount": 701,
          "title": "Global Unifying Intrinsic Calibration for Spinning and Solid-State LiDARs. (arXiv:2012.03321v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13542",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wyburd_M/0/1/0/all/0/1\">Madeleine K. Wyburd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dinsdale_N/0/1/0/all/0/1\">Nicola K. Dinsdale</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Namburete_A/0/1/0/all/0/1\">Ana I.L. Namburete</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jenkinson_M/0/1/0/all/0/1\">Mark Jenkinson</a>",
          "description": "Accurate topology is key when performing meaningful anatomical segmentations,\nhowever, it is often overlooked in traditional deep learning methods. In this\nwork we propose TEDS-Net: a novel segmentation method that guarantees accurate\ntopology. Our method is built upon a continuous diffeomorphic framework, which\nenforces topology preservation. However, in practice, diffeomorphic fields are\nrepresented using a finite number of parameters and sampled using methods such\nas linear interpolation, violating the theoretical guarantees. We therefore\nintroduce additional modifications to more strictly enforce it. Our network\nlearns how to warp a binary prior, with the desired topological\ncharacteristics, to complete the segmentation task. We tested our method on\nmyocardium segmentation from an open-source 2D heart dataset. TEDS-Net\npreserved topology in 100% of the cases, compared to 90% from the U-Net,\nwithout sacrificing on Hausdorff Distance or Dice performance. Code will be\nmade available at: www.github.com/mwyburd/TEDS-Net",
          "link": "http://arxiv.org/abs/2107.13542",
          "publishedOn": "2021-07-29T02:00:08.636Z",
          "wordCount": 613,
          "title": "TEDS-Net: Enforcing Diffeomorphisms in Spatial Transformers to Guarantee Topology Preservation in Segmentations. (arXiv:2107.13542v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.08032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1\">Spandan Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1\">Timothy Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dozier_J/0/1/0/all/0/1\">Jamell Dozier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_H/0/1/0/all/0/1\">Helen Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandari_N/0/1/0/all/0/1\">Nishchal Bhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1\">Fr&#xe9;do Durand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>",
          "description": "Object recognition and viewpoint estimation lie at the heart of visual\nunderstanding. Recent works suggest that convolutional neural networks (CNNs)\nfail to generalize to out-of-distribution (OOD) category-viewpoint\ncombinations, ie. combinations not seen during training. In this paper, we\ninvestigate when and how such OOD generalization may be possible by evaluating\nCNNs trained to classify both object category and 3D viewpoint on OOD\ncombinations, and identifying the neural mechanisms that facilitate such OOD\ngeneralization. We show that increasing the number of in-distribution\ncombinations (ie. data diversity) substantially improves generalization to OOD\ncombinations, even with the same amount of training data. We compare learning\ncategory and viewpoint in separate and shared network architectures, and\nobserve starkly different trends on in-distribution and OOD combinations, ie.\nwhile shared networks are helpful in-distribution, separate networks\nsignificantly outperform shared ones at OOD combinations. Finally, we\ndemonstrate that such OOD generalization is facilitated by the neural mechanism\nof specialization, ie. the emergence of two types of neurons -- neurons\nselective to category and invariant to viewpoint, and vice versa.",
          "link": "http://arxiv.org/abs/2007.08032",
          "publishedOn": "2021-07-29T02:00:08.629Z",
          "wordCount": 654,
          "title": "When and how do CNNs generalize to out-of-distribution category-viewpoint combinations?. (arXiv:2007.08032v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yubin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Pengyi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>",
          "description": "There has been a growing interest in unsupervised domain adaptation (UDA) to\nalleviate the data scalability issue, while the existing works usually focus on\nclassifying independently discrete labels. However, in many tasks (e.g.,\nmedical diagnosis), the labels are discrete and successively distributed. The\nUDA for ordinal classification requires inducing non-trivial ordinal\ndistribution prior to the latent space. Target for this, the partially ordered\nset (poset) is defined for constraining the latent vector. Instead of the\ntypically i.i.d. Gaussian latent prior, in this work, a recursively conditional\nGaussian (RCG) set is proposed for ordered constraint modeling, which admits a\ntractable joint distribution prior. Furthermore, we are able to control the\ndensity of content vectors that violate the poset constraint by a simple\n\"three-sigma rule\". We explicitly disentangle the cross-domain images into a\nshared ordinal prior induced ordinal content space and two separate\nsource/target ordinal-unrelated spaces, and the self-training is worked on the\nshared space exclusively for ordinal-aware domain alignment. Extensive\nexperiments on UDA medical diagnoses and facial age estimation demonstrate its\neffectiveness.",
          "link": "http://arxiv.org/abs/2107.13467",
          "publishedOn": "2021-07-29T02:00:08.588Z",
          "wordCount": 624,
          "title": "Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation. (arXiv:2107.13467v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenhao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eun_K/0/1/0/all/0/1\">Kim Ji Eun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>",
          "description": "Deep Generative Models (DGMs) are known for their superior capability in\ngenerating realistic data. Extending purely data-driven approaches, recent\nspecialized DGMs may satisfy additional controllable requirements such as\nembedding a traffic sign in a driving scene, by manipulating patterns\n\\textit{implicitly} in the neuron or feature level. In this paper, we introduce\na novel method to incorporate domain knowledge \\textit{explicitly} in the\ngeneration process to achieve semantically controllable scene generation. We\ncategorize our knowledge into two types to be consistent with the composition\nof natural scenes, where the first type represents the property of objects and\nthe second type represents the relationship among objects. We then propose a\ntree-structured generative model to learn complex scene representation, whose\nnodes and edges are naturally corresponding to the two types of knowledge\nrespectively. Knowledge can be explicitly integrated to enable semantically\ncontrollable scene generation by imposing semantic rules on properties of nodes\nand edges in the tree structure. We construct a synthetic example to illustrate\nthe controllability and explainability of our method in a clean setting. We\nfurther extend the synthetic example to realistic autonomous vehicle driving\nenvironments and conduct extensive experiments to show that our method\nefficiently identifies adversarial traffic scenes against different\nstate-of-the-art 3D point cloud segmentation models satisfying the traffic\nrules specified as the explicit knowledge.",
          "link": "http://arxiv.org/abs/2106.04066",
          "publishedOn": "2021-07-29T02:00:08.567Z",
          "wordCount": 703,
          "title": "Semantically Controllable Scene Generation with Guidance of Explicit Knowledge. (arXiv:2106.04066v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13407",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mora_Martin_G/0/1/0/all/0/1\">Germ&#xe1;n Mora-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turpin_A/0/1/0/all/0/1\">Alex Turpin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ruget_A/0/1/0/all/0/1\">Alice Ruget</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halimi_A/0/1/0/all/0/1\">Abderrahim Halimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Henderson_R/0/1/0/all/0/1\">Robert Henderson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leach_J/0/1/0/all/0/1\">Jonathan Leach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gyongy_I/0/1/0/all/0/1\">Istvan Gyongy</a>",
          "description": "3D time-of-flight (ToF) imaging is used in a variety of applications such as\naugmented reality (AR), computer interfaces, robotics and autonomous systems.\nSingle-photon avalanche diodes (SPADs) are one of the enabling technologies\nproviding accurate depth data even over long ranges. By developing SPADs in\narray format with integrated processing combined with pulsed, flood-type\nillumination, high-speed 3D capture is possible. However, array sizes tend to\nbe relatively small, limiting the lateral resolution of the resulting depth\nmaps, and, consequently, the information that can be extracted from the image\nfor applications such as object detection. In this paper, we demonstrate that\nthese limitations can be overcome through the use of convolutional neural\nnetworks (CNNs) for high-performance object detection. We present outdoor\nresults from a portable SPAD camera system that outputs 16-bin photon timing\nhistograms with 64x32 spatial resolution. The results, obtained with exposure\ntimes down to 2 ms (equivalent to 500 FPS) and in signal-to-background (SBR)\nratios as low as 0.05, point to the advantages of providing the CNN with full\nhistogram data rather than point clouds alone. Alternatively, a combination of\npoint cloud and active intensity data may be used as input, for a similar level\nof performance. In either case, the GPU-accelerated processing time is less\nthan 1 ms per frame, leading to an overall latency (image acquisition plus\nprocessing) in the millisecond range, making the results relevant for\nsafety-critical computer vision applications which would benefit from faster\nthan human reaction times.",
          "link": "http://arxiv.org/abs/2107.13407",
          "publishedOn": "2021-07-29T02:00:08.547Z",
          "wordCount": 705,
          "title": "High-speed object detection with a single-photon time-of-flight image sensor. (arXiv:2107.13407v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15564",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_Q/0/1/0/all/0/1\">Qingcheng Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_L/0/1/0/all/0/1\">Lin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1\">He Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xing_J/0/1/0/all/0/1\">Jiezhen Xing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jicong Zhang</a>",
          "description": "The novel Coronavirus disease (COVID-19) is a highly contagious virus and has\nspread all over the world, posing an extremely serious threat to all countries.\nAutomatic lung infection segmentation from computed tomography (CT) plays an\nimportant role in the quantitative analysis of COVID-19. However, the major\nchallenge lies in the inadequacy of annotated COVID-19 datasets. Currently,\nthere are several public non-COVID lung lesion segmentation datasets, providing\nthe potential for generalizing useful information to the related COVID-19\nsegmentation task. In this paper, we propose a novel relation-driven\ncollaborative learning model to exploit shared knowledge from non-COVID lesions\nfor annotation-efficient COVID-19 CT lung infection segmentation. The model\nconsists of a general encoder to capture general lung lesion features based on\nmultiple non-COVID lesions, and a target encoder to focus on task-specific\nfeatures based on COVID-19 infections. Features extracted from the two parallel\nencoders are concatenated for the subsequent decoder part. We develop a\ncollaborative learning scheme to regularize feature-level relation consistency\nof given input and encourage the model to learn more general and discriminative\nrepresentation of COVID-19 infections. Extensive experiments demonstrate that\ntrained with limited COVID-19 data, exploiting shared knowledge from non-COVID\nlesions can further improve state-of-the-art performance with up to 3.0% in\ndice similarity coefficient and 4.2% in normalized surface dice. Our proposed\nmethod promotes new insights into annotation-efficient deep learning for\nCOVID-19 infection segmentation and illustrates strong potential for real-world\napplications in the global fight against COVID-19 in the absence of sufficient\nhigh-quality annotations.",
          "link": "http://arxiv.org/abs/2012.15564",
          "publishedOn": "2021-07-29T02:00:08.536Z",
          "wordCount": 768,
          "title": "Exploiting Shared Knowledge from Non-COVID Lesions for Annotation-Efficient COVID-19 CT Lung Infection Segmentation. (arXiv:2012.15564v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Son_W/0/1/0/all/0/1\">Wonchul Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_J/0/1/0/all/0/1\">Jaemin Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Junyong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonjun Hwang</a>",
          "description": "With the success of deep neural networks, knowledge distillation which guides\nthe learning of a small student network from a large teacher network is being\nactively studied for model compression and transfer learning. However, few\nstudies have been performed to resolve the poor learning issue of the student\nnetwork when the student and teacher model sizes significantly differ. In this\npaper, we propose a densely guided knowledge distillation using multiple\nteacher assistants that gradually decreases the model size to efficiently\nbridge the large gap between the teacher and student networks. To stimulate\nmore efficient learning of the student network, we guide each teacher assistant\nto every other smaller teacher assistants iteratively. Specifically, when\nteaching a smaller teacher assistant at the next step, the existing larger\nteacher assistants from the previous step are used as well as the teacher\nnetwork. Moreover, we design stochastic teaching where, for each mini-batch, a\nteacher or teacher assistants are randomly dropped. This acts as a regularizer\nto improve the efficiency of teaching of the student network. Thus, the student\ncan always learn salient distilled knowledge from the multiple sources. We\nverified the effectiveness of the proposed method for a classification task\nusing CIFAR-10, CIFAR-100, and ImageNet. We also achieved significant\nperformance improvements with various backbone architectures such as ResNet,\nWideResNet, and VGG.",
          "link": "http://arxiv.org/abs/2009.08825",
          "publishedOn": "2021-07-29T02:00:08.529Z",
          "wordCount": 684,
          "title": "Densely Guided Knowledge Distillation using Multiple Teacher Assistants. (arXiv:2009.08825v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weiherer_M/0/1/0/all/0/1\">Maximilian Weiherer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eigenberger_A/0/1/0/all/0/1\">Andreas Eigenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brebant_V/0/1/0/all/0/1\">Vanessa Br&#xe9;bant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prantl_L/0/1/0/all/0/1\">Lukas Prantl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palm_C/0/1/0/all/0/1\">Christoph Palm</a>",
          "description": "We present the Regensburg Breast Shape Model (RBSM) - a 3D statistical shape\nmodel of the female breast built from 110 breast scans, and the first ever\npublicly available. Together with the model, a fully automated, pairwise\nsurface registration pipeline used to establish correspondence among 3D breast\nscans is introduced. Our method is computationally efficient and requires only\nfour landmarks to guide the registration process. In order to weaken the strong\ncoupling between breast and thorax, we propose to minimize the variance outside\nthe breast region as much as possible. To achieve this goal, a novel concept\ncalled breast probability masks (BPMs) is introduced. A BPM assigns\nprobabilities to each point of a 3D breast scan, telling how likely it is that\na particular point belongs to the breast area. During registration, we use BPMs\nto align the template to the target as accurately as possible inside the breast\nregion and only roughly outside. This simple yet effective strategy\nsignificantly reduces the unwanted variance outside the breast region, leading\nto better statistical shape models in which breast shapes are quite well\ndecoupled from the thorax. The RBSM is thus able to produce a variety of\ndifferent breast shapes as independently as possible from the shape of the\nthorax. Our systematic experimental evaluation reveals a generalization ability\nof 0.17 mm and a specificity of 2.8 mm for the RBSM. Ultimately, our model is\nseen as a first step towards combining physically motivated deformable models\nof the breast and statistical approaches in order to enable more realistic\nsurgical outcome simulation.",
          "link": "http://arxiv.org/abs/2107.13463",
          "publishedOn": "2021-07-29T02:00:08.502Z",
          "wordCount": 745,
          "title": "Learning the shape of female breasts: an open-access 3D statistical shape model of the female breast built from 110 breast scans. (arXiv:2107.13463v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13431",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ge_S/0/1/0/all/0/1\">Shuang Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_Q/0/1/0/all/0/1\">Qiongyu Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1\">Wenquan Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_D/0/1/0/all/0/1\">Desheng Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Huabin Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaobo Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_K/0/1/0/all/0/1\">Kehong Yuan</a>",
          "description": "Ultrasound is the preferred choice for early screening of dense breast\ncancer. Clinically, doctors have to manually write the screening report which\nis time-consuming and laborious, and it is easy to miss and miswrite.\nTherefore, this paper proposes a method for efficiently generating personalized\nbreast ultrasound screening preliminary reports by AI, especially for benign\nand normal cases which account for the majority. Doctors then make simple\nadjustments or corrections to quickly generate final reports. The proposed\napproach has been tested using a database of 1133 breast tumor instances.\nExperimental results indicate this pipeline improves doctors' work efficiency\nby up to 90%, which greatly reduces repetitive work.",
          "link": "http://arxiv.org/abs/2107.13431",
          "publishedOn": "2021-07-29T02:00:08.487Z",
          "wordCount": 559,
          "title": "AI assisted method for efficiently generating breast ultrasound screening reports. (arXiv:2107.13431v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaodan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1\">Xingxing Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dekui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Ying He</a>",
          "description": "The existing 3D deep learning methods adopt either individual point-based\nfeatures or local-neighboring voxel-based features, and demonstrate great\npotential for processing 3D data. However, the point based models are\ninefficient due to the unordered nature of point clouds and the voxel-based\nmodels suffer from large information loss. Motivated by the success of recent\npoint-voxel representation, such as PVCNN, we propose a new convolutional\nneural network, called Multi Point-Voxel Convolution (MPVConv), for deep\nlearning on point clouds. Integrating both the advantages of voxel and\npoint-based methods, MPVConv can effectively increase the neighboring\ncollection between point-based features and also promote independence among\nvoxel-based features. Moreover, most of the existing approaches aim at solving\none specific task, and only a few of them can handle a variety of tasks. Simply\nreplacing the corresponding convolution module with MPVConv, we show that\nMPVConv can fit in different backbones to solve a wide range of 3D tasks.\nExtensive experiments on benchmark datasets such as ShapeNet Part, S3DIS and\nKITTI for various tasks show that MPVConv improves the accuracy of the backbone\n(PointNet) by up to \\textbf{36\\%}, and achieves higher accuracy than the\nvoxel-based model with up to \\textbf{34}$\\times$ speedups. In addition, MPVConv\noutperforms the state-of-the-art point-based models with up to\n\\textbf{8}$\\times$ speedups. Notably, our MPVConv achieves better accuracy than\nthe newest point-voxel-based model PVCNN (a model more efficient than PointNet)\nwith lower latency.",
          "link": "http://arxiv.org/abs/2107.13152",
          "publishedOn": "2021-07-29T02:00:08.460Z",
          "wordCount": 679,
          "title": "Multi Point-Voxel Convolution (MPVConv) for Deep Learning on Point Clouds. (arXiv:2107.13152v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weixia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>",
          "description": "The computational vision community has recently paid attention to continual\nlearning for blind image quality assessment (BIQA). The primary challenge is to\ncombat catastrophic forgetting of previously-seen IQA datasets (i.e., tasks).\nIn this paper, we present a simple yet effective continual learning method for\nBIQA with improved quality prediction accuracy, plasticity-stability trade-off,\nand task-order/length robustness. The key step in our approach is to freeze all\nconvolution filters of a pre-trained deep neural network (DNN) for an explicit\npromise of stability, and learn task-specific normalization parameters for\nplasticity. We assign each new task a prediction head, and load the\ncorresponding normalization parameters to produce a quality score. The final\nquality estimate is computed by feature fusion and adaptive weighting using\nhierarchical representations, without leveraging the test-time oracle.\nExtensive experiments on six IQA datasets demonstrate the advantages of the\nproposed method in comparison to previous training techniques for BIQA.",
          "link": "http://arxiv.org/abs/2107.13429",
          "publishedOn": "2021-07-29T02:00:08.441Z",
          "wordCount": 591,
          "title": "Task-Specific Normalization for Continual Learning of Blind Image Quality Models. (arXiv:2107.13429v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14331",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Das_A/0/1/0/all/0/1\">Abhranil Das</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Geisler_W/0/1/0/all/0/1\">Wilson S Geisler</a>",
          "description": "Univariate and multivariate normal probability distributions are widely used\nwhen modeling decisions under uncertainty. Computing the performance of such\nmodels requires integrating these distributions over specific domains, which\ncan vary widely across models. Besides some special cases where these integrals\nare easy to calculate, there exist no general analytical expressions, standard\nnumerical methods or software for these integrals. Here we present mathematical\nresults and open-source software that provide (i) the probability in any domain\nof a normal in any dimensions with any parameters, (ii) the probability\ndensity, cumulative distribution, and inverse cumulative distribution of any\nfunction of a normal vector, (iii) the classification errors among any number\nof normal distributions, the Bayes-optimal discriminability index and relation\nto the operating characteristic, (iv) dimension reduction and visualizations\nfor such problems, and (v) tests for how reliably these methods may be used on\ngiven data. We demonstrate these tools with vision research applications of\ndetecting occluding objects in natural scenes, and detecting camouflage.",
          "link": "http://arxiv.org/abs/2012.14331",
          "publishedOn": "2021-07-29T02:00:08.434Z",
          "wordCount": 681,
          "title": "A method to integrate and classify normal distributions. (arXiv:2012.14331v7 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jianing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_F/0/1/0/all/0/1\">Frank P.-W. Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yingnan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_B/0/1/0/all/0/1\">Benny Lo</a>",
          "description": "Accurate prediction of future person location and movement trajectory from an\negocentric wearable camera can benefit a wide range of applications, such as\nassisting visually impaired people in navigation, and the development of\nmobility assistance for people with disability. In this work, a new egocentric\ndataset was constructed using a wearable camera, with 8,250 short clips of a\ntargeted person either walking 1) toward, 2) away, or 3) across the camera\nwearer in indoor environments, or 4) staying still in the scene, and 13,817\nperson bounding boxes were manually labelled. Apart from the bounding boxes,\nthe dataset also contains the estimated pose of the targeted person as well as\nthe IMU signal of the wearable camera at each time point. An LSTM-based\nencoder-decoder framework was designed to predict the future location and\nmovement trajectory of the targeted person in this egocentric setting.\nExtensive experiments have been conducted on the new dataset, and have shown\nthat the proposed method is able to reliably and better predict future person\nlocation and trajectory in egocentric videos captured by the wearable camera\ncompared to three baselines.",
          "link": "http://arxiv.org/abs/2103.04019",
          "publishedOn": "2021-07-29T02:00:08.406Z",
          "wordCount": 671,
          "title": "Indoor Future Person Localization from an Egocentric Wearable Camera. (arXiv:2103.04019v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.00826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gautam_A/0/1/0/all/0/1\">Akshat Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sit_M/0/1/0/all/0/1\">Muhammed Sit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1\">Ibrahim Demir</a>",
          "description": "In this paper, we demonstrated a practical application of realistic river\nimage generation using deep learning. Specifically, we explored a generative\nadversarial network (GAN) model capable of generating high-resolution and\nrealistic river images that can be used to support modeling and analysis in\nsurface water estimation, river meandering, wetland loss, and other\nhydrological research studies. First, we have created an extensive repository\nof overhead river images to be used in training. Second, we incorporated the\nProgressive Growing GAN (PGGAN), a network architecture that iteratively trains\nsmaller-resolution GANs to gradually build up to a very high resolution to\ngenerate high quality (i.e., 1024x1024) synthetic river imagery. With simpler\nGAN architectures, difficulties arose in terms of exponential increase of\ntraining time and vanishing/exploding gradient issues, which the PGGAN\nimplementation seemed to significantly reduce. The results presented in this\nstudy show great promise in generating high-quality images and capturing the\ndetails of river structure and flow to support hydrological research, which\noften requires extensive imagery for model performance.",
          "link": "http://arxiv.org/abs/2003.00826",
          "publishedOn": "2021-07-29T02:00:08.397Z",
          "wordCount": 644,
          "title": "Realistic River Image Synthesis using Deep Generative Adversarial Networks. (arXiv:2003.00826v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hanxiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1\">Helena Kotthaus</a>",
          "description": "In the field of autonomous driving and robotics, point clouds are showing\ntheir excellent real-time performance as raw data from most of the mainstream\n3D sensors. Therefore, point cloud neural networks have become a popular\nresearch direction in recent years. So far, however, there has been little\ndiscussion about the explainability of deep neural networks for point clouds.\nIn this paper, we propose new explainability approaches for point cloud deep\nneural networks based on local surrogate model-based methods to show which\ncomponents make the main contribution to the classification. Moreover, we\npropose a quantitative validation method for explainability methods of point\nclouds which enhances the persuasive power of explainability by dropping the\nmost positive or negative contributing features and monitoring how the\nclassification scores of specific categories change. To enable an intuitive\nexplanation of misclassified instances, we display features with confounding\ncontributions. Our new explainability approach provides a fairly accurate, more\nintuitive and widely applicable explanation for point cloud classification\ntasks. Our code is available at https://github.com/Explain3D/Explainable3D",
          "link": "http://arxiv.org/abs/2107.13459",
          "publishedOn": "2021-07-29T02:00:08.384Z",
          "wordCount": 610,
          "title": "Surrogate Model-Based Explainability Methods for Point Cloud NNs. (arXiv:2107.13459v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Libo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haokui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wei Yin</a>",
          "description": "Road detection is a critically important task for self-driving cars. By\nemploying LiDAR data, recent works have significantly improved the accuracy of\nroad detection. Relying on LiDAR sensors limits the wide application of those\nmethods when only cameras are available. In this paper, we propose a novel road\ndetection approach with RGB being the only input during inference.\nSpecifically, we exploit pseudo-LiDAR using depth estimation, and propose a\nfeature fusion network where RGB and learned depth information are fused for\nimproved road detection. To further optimize the network structure and improve\nthe efficiency of the network. we search for the network structure of the\nfeature fusion module using NAS techniques. Finally, be aware of that\ngenerating pseudo-LiDAR from RGB via depth estimation introduces extra\ncomputational costs and relies on depth estimation networks, we design a\nmodality distillation strategy and leverage it to further free our network from\nthese extra computational cost and dependencies during inference. The proposed\nmethod achieves state-of-the-art performance on two challenging benchmarks,\nKITTI and R2D.",
          "link": "http://arxiv.org/abs/2107.13279",
          "publishedOn": "2021-07-29T02:00:08.377Z",
          "wordCount": 596,
          "title": "Pseudo-LiDAR Based Road Detection. (arXiv:2107.13279v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13200",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_B/0/1/0/all/0/1\">Bo Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_P/0/1/0/all/0/1\">Pengfei Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a> (Alzheimer&#x27;s Disease Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing), <a href=\"http://arxiv.org/find/eess/1/au:+Shen_S/0/1/0/all/0/1\">Shuwei Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_P/0/1/0/all/0/1\">Peng Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1\">Ronald X. Xu</a>",
          "description": "Early and accurate diagnosis of Alzheimer's disease (AD) and its prodromal\nperiod mild cognitive impairment (MCI) is essential for the delayed disease\nprogression and the improved quality of patients'life. The emerging\ncomputer-aided diagnostic methods that combine deep learning with structural\nmagnetic resonance imaging (sMRI) have achieved encouraging results, but some\nof them are limit of issues such as data leakage and unexplainable diagnosis.\nIn this research, we propose a novel end-to-end deep learning approach for\nautomated diagnosis of AD and localization of important brain regions related\nto the disease from sMRI data. This approach is based on a 2D single model\nstrategy and has the following differences from the current approaches: 1)\nConvolutional Neural Network (CNN) models of different structures and\ncapacities are evaluated systemically and the most suitable model is adopted\nfor AD diagnosis; 2) a data augmentation strategy named Two-stage Random\nRandAugment (TRRA) is proposed to alleviate the overfitting issue caused by\nlimited training data and to improve the classification performance in AD\ndiagnosis; 3) an explainable method of Grad-CAM++ is introduced to generate the\nvisually explainable heatmaps that localize and highlight the brain regions\nthat our model focuses on and to make our model more transparent. Our approach\nhas been evaluated on two publicly accessible datasets for two classification\ntasks of AD vs. cognitively normal (CN) and progressive MCI (pMCI) vs. stable\nMCI (sMCI). The experimental results indicate that our approach outperforms the\nstate-of-the-art approaches, including those using multi-model and 3D CNN\nmethods. The resultant localization heatmaps from our approach also highlight\nthe lateral ventricle and some disease-relevant regions of cortex, coincident\nwith the commonly affected regions during the development of AD.",
          "link": "http://arxiv.org/abs/2107.13200",
          "publishedOn": "2021-07-29T02:00:08.356Z",
          "wordCount": 760,
          "title": "An explainable two-dimensional single model deep learning approach for Alzheimer's disease diagnosis and brain atrophy localization. (arXiv:2107.13200v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/1907.01845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruijun Xu</a>",
          "description": "One of the most critical problems in weight-sharing neural architecture\nsearch is the evaluation of candidate models within a predefined search space.\nIn practice, a one-shot supernet is trained to serve as an evaluator. A\nfaithful ranking certainly leads to more accurate searching results. However,\ncurrent methods are prone to making misjudgments. In this paper, we prove that\ntheir biased evaluation is due to inherent unfairness in the supernet training.\nIn view of this, we propose two levels of constraints: expectation fairness and\nstrict fairness. Particularly, strict fairness ensures equal optimization\nopportunities for all choice blocks throughout the training, which neither\noverestimates nor underestimates their capacity. We demonstrate that this is\ncrucial for improving the confidence of models' ranking. Incorporating the\none-shot supernet trained under the proposed fairness constraints with a\nmulti-objective evolutionary search algorithm, we obtain various\nstate-of-the-art models, e.g., FairNAS-A attains 77.5% top-1 validation\naccuracy on ImageNet. The models and their evaluation codes are made publicly\navailable online this http URL .",
          "link": "http://arxiv.org/abs/1907.01845",
          "publishedOn": "2021-07-29T02:00:08.349Z",
          "wordCount": 672,
          "title": "FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search. (arXiv:1907.01845v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.01446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chongwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1\">Yulong Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Caifei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xin Fan</a>",
          "description": "To boost the object grabbing capability of underwater robots for open-sea\nfarming, we propose a new dataset (UDD) consisting of three categories\n(seacucumber, seaurchin, and scallop) with 2,227 images. To the best of our\nknowledge, it is the first 4K HD dataset collected in a real open-sea farm. We\nalso propose a novel Poisson-blending Generative Adversarial Network (Poisson\nGAN) and an efficient object detection network (AquaNet) to address two common\nissues within related datasets: the class-imbalance problem and the problem of\nmass small object, respectively. Specifically, Poisson GAN combines Poisson\nblending into its generator and employs a new loss called Dual Restriction loss\n(DR loss), which supervises both implicit space features and image-level\nfeatures during training to generate more realistic images. By utilizing\nPoisson GAN, objects of minority class like seacucumber or scallop could be\nadded into an image naturally and annotated automatically, which could increase\nthe loss of minority classes during training detectors to eliminate the\nclass-imbalance problem; AquaNet is a high-efficiency detector to address the\nproblem of detecting mass small objects from cloudy underwater pictures. Within\nit, we design two efficient components: a depth-wise-convolution-based\nMulti-scale Contextual Features Fusion (MFF) block and a Multi-scale\nBlursampling (MBP) module to reduce the parameters of the network to 1.3\nmillion. Both two components could provide multi-scale features of small\nobjects under a short backbone configuration without any loss of accuracy. In\naddition, we construct a large-scale augmented dataset (AUDD) and a\npre-training dataset via Poisson GAN from UDD. Extensive experiments show the\neffectiveness of the proposed Poisson GAN, AquaNet, UDD, AUDD, and pre-training\ndataset.",
          "link": "http://arxiv.org/abs/2003.01446",
          "publishedOn": "2021-07-29T02:00:08.335Z",
          "wordCount": 750,
          "title": "A New Dataset, Poisson GAN and AquaNet for Underwater Object Grabbing. (arXiv:2003.01446v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Widya_A/0/1/0/all/0/1\">Aji Resindra Widya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monno_Y/0/1/0/all/0/1\">Yusuke Monno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okutomi_M/0/1/0/all/0/1\">Masatoshi Okutomi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_S/0/1/0/all/0/1\">Sho Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotoda_T/0/1/0/all/0/1\">Takuji Gotoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miki_K/0/1/0/all/0/1\">Kenji Miki</a>",
          "description": "Gastroendoscopy has been a clinical standard for diagnosing and treating\nconditions that affect a part of a patient's digestive system, such as the\nstomach. Despite the fact that gastroendoscopy has a lot of advantages for\npatients, there exist some challenges for practitioners, such as the lack of 3D\nperception, including the depth and the endoscope pose information. Such\nchallenges make navigating the endoscope and localizing any found lesion in a\ndigestive tract difficult. To tackle these problems, deep learning-based\napproaches have been proposed to provide monocular gastroendoscopy with\nadditional yet important depth and pose information. In this paper, we propose\na novel supervised approach to train depth and pose estimation networks using\nconsecutive endoscopy images to assist the endoscope navigation in the stomach.\nWe firstly generate real depth and pose training data using our previously\nproposed whole stomach 3D reconstruction pipeline to avoid poor generalization\nability between computer-generated (CG) models and real data for the stomach.\nIn addition, we propose a novel generalized photometric loss function to avoid\nthe complicated process of finding proper weights for balancing the depth and\nthe pose loss terms, which is required for existing direct depth and pose\nsupervision approaches. We then experimentally show that our proposed\ngeneralized loss performs better than existing direct supervision losses.",
          "link": "http://arxiv.org/abs/2107.13263",
          "publishedOn": "2021-07-29T02:00:08.327Z",
          "wordCount": 664,
          "title": "Learning-Based Depth and Pose Estimation for Monocular Endoscope with Loss Generalization. (arXiv:2107.13263v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramamonjison_R/0/1/0/all/0/1\">Rindra Ramamonjison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xinyu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiaolong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>",
          "description": "This paper presents a Simple and effective unsupervised adaptation method for\nRobust Object Detection (SimROD). To overcome the challenging issues of domain\nshift and pseudo-label noise, our method integrates a novel domain-centric\naugmentation method, a gradual self-labeling adaptation procedure, and a\nteacher-guided fine-tuning mechanism. Using our method, target domain samples\ncan be leveraged to adapt object detection models without changing the model\narchitecture or generating synthetic data. When applied to image corruptions\nand high-level cross-domain adaptation benchmarks, our method outperforms prior\nbaselines on multiple domain adaptation benchmarks. SimROD achieves new\nstate-of-the-art on standard real-to-synthetic and cross-camera setup\nbenchmarks. On the image corruption benchmark, models adapted with our method\nachieved a relative robustness improvement of 15-25% AP50 on Pascal-C and 5-6%\nAP on COCO-C and Cityscapes-C. On the cross-domain benchmark, our method\noutperformed the best baseline performance by up to 8% AP50 on Comic dataset\nand up to 4% on Watercolor dataset.",
          "link": "http://arxiv.org/abs/2107.13389",
          "publishedOn": "2021-07-29T02:00:08.319Z",
          "wordCount": 612,
          "title": "SimROD: A Simple Adaptation Method for Robust Object Detection. (arXiv:2107.13389v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chunxia Xiao</a>",
          "description": "Generating photo-realistic images from a text description is a challenging\nproblem in computer vision. Previous works have shown promising performance to\ngenerate synthetic images conditional on text by Generative Adversarial\nNetworks (GANs). In this paper, we focus on the category-consistent and\nrelativistic diverse constraints to optimize the diversity of synthetic images.\nBased on those constraints, a category-consistent and relativistic diverse\nconditional GAN (CRD-CGAN) is proposed to synthesize $K$ photo-realistic images\nsimultaneously. We use the attention loss and diversity loss to improve the\nsensitivity of the GAN to word attention and noises. Then, we employ the\nrelativistic conditional loss to estimate the probability of relatively real or\nfake for synthetic images, which can improve the performance of basic\nconditional loss. Finally, we introduce a category-consistent loss to alleviate\nthe over-category issues between K synthetic images. We evaluate our approach\nusing the Birds-200-2011, Oxford-102 flower and MSCOCO 2014 datasets, and the\nextensive experiments demonstrate superiority of the proposed method in\ncomparison with state-of-the-art methods in terms of photorealistic and\ndiversity of the generated synthetic images.",
          "link": "http://arxiv.org/abs/2107.13516",
          "publishedOn": "2021-07-29T02:00:08.298Z",
          "wordCount": 609,
          "title": "CRD-CGAN: Category-Consistent and Relativistic Constraints for Diverse Text-to-Image Generation. (arXiv:2107.13516v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ashlesha Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangwan_K/0/1/0/all/0/1\">Kuldip Singh Sangwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhiraj/0/1/0/all/0/1\">Dhiraj</a>",
          "description": "As the proportion of road accidents increases each year, driver distraction\ncontinues to be an important risk component in road traffic injuries and\ndeaths. The distractions caused by the increasing use of mobile phones and\nother wireless devices pose a potential risk to road safety. Our current study\naims to aid the already existing techniques in driver posture recognition by\nimproving the performance in the driver distraction classification problem. We\npresent an approach using a genetic algorithm-based ensemble of six independent\ndeep neural architectures, namely, AlexNet, VGG-16, EfficientNet B0, Vanilla\nCNN, Modified DenseNet, and InceptionV3 + BiLSTM. We test it on two\ncomprehensive datasets, the AUC Distracted Driver Dataset, on which our\ntechnique achieves an accuracy of 96.37%, surpassing the previously obtained\n95.98%, and on the State Farm Driver Distraction Dataset, on which we attain an\naccuracy of 99.75%. The 6-Model Ensemble gave an inference time of 0.024\nseconds as measured on our machine with Ubuntu 20.04(64-bit) and GPU as GeForce\nGTX 1080.",
          "link": "http://arxiv.org/abs/2107.13355",
          "publishedOn": "2021-07-29T02:00:08.291Z",
          "wordCount": 633,
          "title": "A Computer Vision-Based Approach for Driver Distraction Recognition using Deep Learning and Genetic Algorithm Based Ensemble. (arXiv:2107.13355v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jianing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_B/0/1/0/all/0/1\">Benny Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang-Zhong Yang</a>",
          "description": "In this report, the technical details of our submission to the EPIC-Kitchens\nAction Anticipation Challenge 2021 are given. We developed a hierarchical\nattention model for action anticipation, which leverages Transformer-based\nattention mechanism to aggregate features across temporal dimension,\nmodalities, symbiotic branches respectively. In terms of Mean Top-5 Recall of\naction, our submission with team name ICL-SJTU achieved 13.39% for overall\ntesting set, 10.05% for unseen subsets and 11.88% for tailed subsets.\nAdditionally, it is noteworthy that our submission ranked 1st in terms of verb\nclass in all three (sub)sets.",
          "link": "http://arxiv.org/abs/2107.13259",
          "publishedOn": "2021-07-29T02:00:08.283Z",
          "wordCount": 529,
          "title": "TransAction: ICL-SJTU Submission to EPIC-Kitchens Action Anticipation Challenge 2021. (arXiv:2107.13259v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kuiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>",
          "description": "Modelling long-range contextual relationships is critical for pixel-wise\nprediction tasks such as semantic segmentation. However, convolutional neural\nnetworks (CNNs) are inherently limited to model such dependencies due to the\nnaive structure in its building modules (\\eg, local convolution kernel). While\nrecent global aggregation methods are beneficial for long-range structure\ninformation modelling, they would oversmooth and bring noise to the regions\ncontaining fine details (\\eg,~boundaries and small objects), which are very\nmuch cared for the semantic segmentation task. To alleviate this problem, we\npropose to explore the local context for making the aggregated long-range\nrelationship being distributed more accurately in local regions. In particular,\nwe design a novel local distribution module which models the affinity map\nbetween global and local relationship for each pixel adaptively. Integrating\nexisting global aggregation modules, we show that our approach can be\nmodularized as an end-to-end trainable block and easily plugged into existing\nsemantic segmentation networks, giving rise to the \\emph{GALD} networks.\nDespite its simplicity and versatility, our approach allows us to build new\nstate of the art on major semantic segmentation benchmarks including\nCityscapes, ADE20K, Pascal Context, Camvid and COCO-stuff. Code and trained\nmodels are released at \\url{https://github.com/lxtGH/GALD-DGCNet} to foster\nfurther research.",
          "link": "http://arxiv.org/abs/2107.13154",
          "publishedOn": "2021-07-29T02:00:08.276Z",
          "wordCount": 652,
          "title": "Global Aggregation then Local Distribution for Scene Parsing. (arXiv:2107.13154v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeesoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1\">Junsuk Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sangdoo Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>",
          "description": "Weakly-supervised object localization (WSOL) enables finding an object using\na dataset without any localization information. By simply training a\nclassification model using only image-level annotations, the feature map of the\nmodel can be utilized as a score map for localization. In spite of many WSOL\nmethods proposing novel strategies, there has not been any de facto standard\nabout how to normalize the class activation map (CAM). Consequently, many WSOL\nmethods have failed to fully exploit their own capacity because of the misuse\nof a normalization method. In this paper, we review many existing normalization\nmethods and point out that they should be used according to the property of the\ngiven dataset. Additionally, we propose a new normalization method which\nsubstantially enhances the performance of any CAM-based WSOL methods. Using the\nproposed normalization method, we provide a comprehensive evaluation over three\ndatasets (CUB, ImageNet and OpenImages) on three different architectures and\nobserve significant performance gains over the conventional min-max\nnormalization method in all the evaluated cases.",
          "link": "http://arxiv.org/abs/2107.13221",
          "publishedOn": "2021-07-29T02:00:08.264Z",
          "wordCount": 608,
          "title": "Normalization Matters in Weakly Supervised Object Localization. (arXiv:2107.13221v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lomurno_E/0/1/0/all/0/1\">Eugenio Lomurno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romanoni_A/0/1/0/all/0/1\">Andrea Romanoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matteucci_M/0/1/0/all/0/1\">Matteo Matteucci</a>",
          "description": "Today, Multi-View Stereo techniques are able to reconstruct robust and\ndetailed 3D models, especially when starting from high-resolution images.\nHowever, there are cases in which the resolution of input images is relatively\nlow, for instance, when dealing with old photos, or when hardware constrains\nthe amount of data that can be acquired. In this paper, we investigate if, how,\nand how much increasing the resolution of such input images through\nSuper-Resolution techniques reflects in quality improvements of the\nreconstructed 3D models, despite the artifacts that sometimes this may\ngenerate. We show that applying a Super-Resolution step before recovering the\ndepth maps in most cases leads to a better 3D model both in the case of\nPatchMatch-based and deep-learning-based algorithms. The use of\nSuper-Resolution improves especially the completeness of reconstructed models\nand turns out to be particularly effective in the case of textured scenes.",
          "link": "http://arxiv.org/abs/2107.13261",
          "publishedOn": "2021-07-29T02:00:08.240Z",
          "wordCount": 574,
          "title": "Improving Multi-View Stereo via Super-Resolution. (arXiv:2107.13261v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13465",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Ti Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balagopal_A/0/1/0/all/0/1\">Anjali Balagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohopolski_M/0/1/0/all/0/1\">Michael Dohopolski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_H/0/1/0/all/0/1\">Howard E. Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McBeth_R/0/1/0/all/0/1\">Rafe McBeth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jun Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mu-Han Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sher_D/0/1/0/all/0/1\">David J. Sher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Steve Jiang</a>",
          "description": "Automatic segmentation of anatomical structures is critical for many medical\napplications. However, the results are not always clinically acceptable and\nrequire tedious manual revision. Here, we present a novel concept called\nartificial intelligence assisted contour revision (AIACR) and demonstrate its\nfeasibility. The proposed clinical workflow of AIACR is as follows given an\ninitial contour that requires a clinicians revision, the clinician indicates\nwhere a large revision is needed, and a trained deep learning (DL) model takes\nthis input to update the contour. This process repeats until a clinically\nacceptable contour is achieved. The DL model is designed to minimize the\nclinicians input at each iteration and to minimize the number of iterations\nneeded to reach acceptance. In this proof-of-concept study, we demonstrated the\nconcept on 2D axial images of three head-and-neck cancer datasets, with the\nclinicians input at each iteration being one mouse click on the desired\nlocation of the contour segment. The performance of the model is quantified\nwith Dice Similarity Coefficient (DSC) and 95th percentile of Hausdorff\nDistance (HD95). The average DSC/HD95 (mm) of the auto-generated initial\ncontours were 0.82/4.3, 0.73/5.6 and 0.67/11.4 for three datasets, which were\nimproved to 0.91/2.1, 0.86/2.4 and 0.86/4.7 with three mouse clicks,\nrespectively. Each DL-based contour update requires around 20 ms. We proposed a\nnovel AIACR concept that uses DL models to assist clinicians in revising\ncontours in an efficient and effective way, and we demonstrated its feasibility\nby using 2D axial CT images from three head-and-neck cancer datasets.",
          "link": "http://arxiv.org/abs/2107.13465",
          "publishedOn": "2021-07-29T02:00:08.231Z",
          "wordCount": 704,
          "title": "A Proof-of-Concept Study of Artificial Intelligence Assisted Contour Revision. (arXiv:2107.13465v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1\">Javier Naranjo-Alcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1\">Sergi Perez-Castanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Garcia_A/0/1/0/all/0/1\">Aaron Lopez-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1\">Pedro Zuccarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1\">Maximo Cobos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1\">Francesc J. Ferri</a>",
          "description": "The use of multiple and semantically correlated sources can provide\ncomplementary information to each other that may not be evident when working\nwith individual modalities on their own. In this context, multi-modal models\ncan help producing more accurate and robust predictions in machine learning\ntasks where audio-visual data is available. This paper presents a multi-modal\nmodel for automatic scene classification that exploits simultaneously auditory\nand visual information. The proposed approach makes use of two separate\nnetworks which are respectively trained in isolation on audio and visual data,\nso that each network specializes in a given modality. The visual subnetwork is\na pre-trained VGG16 model followed by a bidiretional recurrent layer, while the\nresidual audio subnetwork is based on stacked squeeze-excitation convolutional\nblocks trained from scratch. After training each subnetwork, the fusion of\ninformation from the audio and visual streams is performed at two different\nstages. The early fusion stage combines features resulting from the last\nconvolutional block of the respective subnetworks at different time steps to\nfeed a bidirectional recurrent structure. The late fusion stage combines the\noutput of the early fusion stage with the independent predictions provided by\nthe two subnetworks, resulting in the final prediction. We evaluate the method\nusing the recently published TAU Audio-Visual Urban Scenes 2021, which contains\nsynchronized audio and video recordings from 12 European cities in 10 different\nscene classes. The proposed model has been shown to provide an excellent\ntrade-off between prediction performance (86.5%) and system complexity (15M\nparameters) in the evaluation results of the DCASE 2021 Challenge.",
          "link": "http://arxiv.org/abs/2107.13180",
          "publishedOn": "2021-07-29T02:00:08.204Z",
          "wordCount": 712,
          "title": "Squeeze-Excitation Convolutional Recurrent Neural Networks for Audio-Visual Scene Classification. (arXiv:2107.13180v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13273",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barquero_G/0/1/0/all/0/1\">Germ&#xe1;n Barquero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupont_I/0/1/0/all/0/1\">Isabelle Hupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_C/0/1/0/all/0/1\">Carles Fern&#xe1;ndez</a>",
          "description": "Most current multi-object trackers focus on short-term tracking, and are\nbased on deep and complex systems that often cannot operate in real-time,\nmaking them impractical for video-surveillance. In this paper we present a\nlong-term, multi-face tracking architecture conceived for working in crowded\ncontexts where faces are often the only visible part of a person. Our system\nbenefits from advances in the fields of face detection and face recognition to\nachieve long-term tracking, and is particularly unconstrained to the motion and\nocclusions of people. It follows a tracking-by-detection approach, combining a\nfast short-term visual tracker with a novel online tracklet reconnection\nstrategy grounded on rank-based face verification. The proposed rank-based\nconstraint favours higher inter-class distance among tracklets, and reduces the\npropagation of errors due to wrong reconnections. Additionally, a correction\nmodule is included to correct past assignments with no extra computational\ncost. We present a series of experiments introducing novel specialized metrics\nfor the evaluation of long-term tracking capabilities, and publicly release a\nvideo dataset with 10 manually annotated videos and a total length of 8' 54\".\nOur findings validate the robustness of each of the proposed modules, and\ndemonstrate that, in these challenging contexts, our approach yields up to 50%\nlonger tracks than state-of-the-art deep learning trackers.",
          "link": "http://arxiv.org/abs/2107.13273",
          "publishedOn": "2021-07-29T02:00:08.192Z",
          "wordCount": 664,
          "title": "Rank-based verification for long-term face tracking in crowded scenes. (arXiv:2107.13273v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rodin_I/0/1/0/all/0/1\">Ivan Rodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1\">Antonino Furnari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavroedis_D/0/1/0/all/0/1\">Dimitrios Mavroedis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1\">Giovanni Maria Farinella</a>",
          "description": "Egocentric videos can bring a lot of information about how humans perceive\nthe world and interact with the environment, which can be beneficial for the\nanalysis of human behaviour. The research in egocentric video analysis is\ndeveloping rapidly thanks to the increasing availability of wearable devices\nand the opportunities offered by new large-scale egocentric datasets. As\ncomputer vision techniques continue to develop at an increasing pace, the tasks\nrelated to the prediction of future are starting to evolve from the need of\nunderstanding the present. Predicting future human activities, trajectories and\ninteractions with objects is crucial in applications such as human-robot\ninteraction, assistive wearable technologies for both industrial and daily\nliving scenarios, entertainment and virtual or augmented reality. This survey\nsummarises the evolution of studies in the context of future prediction from\negocentric vision making an overview of applications, devices, existing\nproblems, commonly used datasets, models and input modalities. Our analysis\nhighlights that methods for future prediction from egocentric vision can have a\nsignificant impact in a range of applications and that further research efforts\nshould be devoted to the standardisation of tasks and the proposal of datasets\nconsidering real-world scenarios such as the ones with an industrial vocation.",
          "link": "http://arxiv.org/abs/2107.13411",
          "publishedOn": "2021-07-29T02:00:08.160Z",
          "wordCount": 647,
          "title": "Predicting the Future from First Person (Egocentric) Vision: A Survey. (arXiv:2107.13411v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagong_M/0/1/0/all/0/1\">Min-Cheol Sagong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_Y/0/1/0/all/0/1\">Yoon-Jae Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Seung-Won Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1\">Sung-Jea Ko</a>",
          "description": "Convolutional neural networks (CNNs) have been not only widespread but also\nachieved noticeable results on numerous applications including image\nclassification, restoration, and generation. Although the weight-sharing\nproperty of convolutions makes them widely adopted in various tasks, its\ncontent-agnostic characteristic can also be considered a major drawback. To\nsolve this problem, in this paper, we propose a novel operation, called pixel\nadaptive kernel attention (PAKA). PAKA provides directivity to the filter\nweights by multiplying spatially varying attention from learnable features. The\nproposed method infers pixel-adaptive attention maps along the channel and\nspatial directions separately to address the decomposed model with fewer\nparameters. Our method is trainable in an end-to-end manner and applicable to\nany CNN-based models. In addition, we propose an improved information\naggregation module with PAKA, called the hierarchical PAKA module (HPM). We\ndemonstrate the superiority of our HPM by presenting state-of-the-art\nperformance on semantic segmentation compared to the conventional information\naggregation modules. We validate the proposed method through additional\nablation studies and visualizing the effect of PAKA providing directivity to\nthe weights of convolutions. We also show the generalizability of the proposed\nmethod by applying it to multi-modal tasks especially color-guided depth map\nsuper-resolution.",
          "link": "http://arxiv.org/abs/2107.13144",
          "publishedOn": "2021-07-29T02:00:08.140Z",
          "wordCount": 647,
          "title": "Content-aware Directed Propagation Network with Pixel Adaptive Kernel Attention. (arXiv:2107.13144v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_G/0/1/0/all/0/1\">Guohua Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1\">Xingxing Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xin Cao</a>",
          "description": "The repairing work of terracotta warriors in Emperor Qinshihuang Mausoleum\nSite Museum is handcrafted by experts, and the increasing amounts of unearthed\npieces of terracotta warriors make the archaeologists too challenging to\nconduct the restoration of terracotta warriors efficiently. We hope to segment\nthe 3D point cloud data of the terracotta warriors automatically and store the\nfragment data in the database to assist the archaeologists in matching the\nactual fragments with the ones in the database, which could result in higher\nrepairing efficiency of terracotta warriors. Moreover, the existing 3D neural\nnetwork research is mainly focusing on supervised classification, clustering,\nunsupervised representation, and reconstruction. There are few pieces of\nresearches concentrating on unsupervised point cloud part segmentation. In this\npaper, we present SRG-Net for 3D point clouds of terracotta warriors to address\nthese problems. Firstly, we adopt a customized seed-region-growing algorithm to\nsegment the point cloud coarsely. Then we present a supervised segmentation and\nunsupervised reconstruction networks to learn the characteristics of 3D point\nclouds. Finally, we combine the SRG algorithm with our improved CNN using a\nrefinement method. This pipeline is called SRG-Net, which aims at conducting\nsegmentation tasks on the terracotta warriors. Our proposed SRG-Net is\nevaluated on the terracotta warriors data and ShapeNet dataset by measuring the\naccuracy and the latency. The experimental results show that our SRG-Net\noutperforms the state-of-the-art methods. Our code is shown in Code File\n1~\\cite{Srgnet_2021}.",
          "link": "http://arxiv.org/abs/2107.13167",
          "publishedOn": "2021-07-29T02:00:08.132Z",
          "wordCount": 681,
          "title": "Unsupervised Segmentation for Terracotta Warrior with Seed-Region-Growing CNN(SRG-Net). (arXiv:2107.13167v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1\">Mariella Dimiccoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrido_L/0/1/0/all/0/1\">Llu&#xed;s Garrido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Corominas_G/0/1/0/all/0/1\">Guillem Rodriguez-Corominas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wendt_H/0/1/0/all/0/1\">Herwig Wendt</a>",
          "description": "Recently, transfer subspace learning based approaches have shown to be a\nvalid alternative to unsupervised subspace clustering and temporal data\nclustering for human motion segmentation (HMS). These approaches leverage prior\nknowledge from a source domain to improve clustering performance on a target\ndomain, and currently they represent the state of the art in HMS. Bucking this\ntrend, in this paper, we propose a novel unsupervised model that learns a\nrepresentation of the data and digs clustering information from the data\nitself. Our model is reminiscent of temporal subspace clustering, but presents\ntwo critical differences. First, we learn an auxiliary data matrix that can\ndeviate from the initial data, hence confer more degrees of freedom to the\ncoding matrix. Second, we introduce a regularization term for this auxiliary\ndata matrix that preserves the local geometrical structure present in the\nhigh-dimensional space. The proposed model is efficiently optimized by using an\noriginal Alternating Direction Method of Multipliers (ADMM) formulation\nallowing to learn jointly the auxiliary data representation, a nonnegative\ndictionary and a coding matrix. Experimental results on four benchmark datasets\nfor HMS demonstrate that our approach achieves significantly better clustering\nperformance then state-of-the-art methods, including both unsupervised and more\nrecent semi-supervised transfer learning approaches.",
          "link": "http://arxiv.org/abs/2107.13362",
          "publishedOn": "2021-07-29T02:00:08.124Z",
          "wordCount": 645,
          "title": "Graph Constrained Data Representation Learning for Human Motion Segmentation. (arXiv:2107.13362v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13237",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mukherjee_U/0/1/0/all/0/1\">Uddipan Mukherjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pancholi_S/0/1/0/all/0/1\">Sidharth Pancholi</a>",
          "description": "Heart disease is the most common reason for human mortality that causes\nalmost one-third of deaths throughout the world. Detecting the disease early\nincreases the chances of survival of the patient and there are several ways a\nsign of heart disease can be detected early. This research proposes to convert\ncleansed and normalized heart sound into visual mel scale spectrograms and then\nusing visual domain transfer learning approaches to automatically extract\nfeatures and categorize between heart sounds. Some of the previous studies\nfound that the spectrogram of various types of heart sounds is visually\ndistinguishable to human eyes, which motivated this study to experiment on\nvisual domain classification approaches for automated heart sound\nclassification. It will use convolution neural network-based architectures i.e.\nResNet, MobileNetV2, etc as the automated feature extractors from spectrograms.\nThese well-accepted models in the image domain showed to learn generalized\nfeature representations of cardiac sounds collected from different environments\nwith varying amplitude and noise levels. Model evaluation criteria used were\ncategorical accuracy, precision, recall, and AUROC as the chosen dataset is\nunbalanced. The proposed approach has been implemented on datasets A and B of\nthe PASCAL heart sound collection and resulted in ~ 90% categorical accuracy\nand AUROC of ~0.97 for both sets.",
          "link": "http://arxiv.org/abs/2107.13237",
          "publishedOn": "2021-07-29T02:00:08.116Z",
          "wordCount": 652,
          "title": "A Visual Domain Transfer Learning Approach for Heartbeat Sound Classification. (arXiv:2107.13237v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arora_G/0/1/0/all/0/1\">Geetika Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_R/0/1/0/all/0/1\">Rohit K Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_K/0/1/0/all/0/1\">Kamlesh Tiwari</a>",
          "description": "This paper proposes teeth-photo, a new biometric modality for human\nauthentication on mobile and hand held devices. Biometrics samples are acquired\nusing the camera mounted on mobile device with the help of a mobile application\nhaving specific markers to register the teeth area. Region of interest (RoI) is\nthen extracted using the markers and the obtained sample is enhanced using\ncontrast limited adaptive histogram equalization (CLAHE) for better visual\nclarity. We propose a deep learning architecture and novel regularization\nscheme to obtain highly discriminative embedding for small size RoI. Proposed\ncustom loss function was able to achieve perfect classification for the tiny\nRoI of $75\\times 75$ size. The model is end-to-end and few-shot and therefore\nis very efficient in terms of time and energy requirements. The system can be\nused in many ways including device unlocking and secure authentication. To the\nbest of our understanding, this is the first work on teeth-photo based\nauthentication for mobile device. Experiments have been conducted on an\nin-house teeth-photo database collected using our application. The database is\nmade publicly available. Results have shown that the proposed system has\nperfect accuracy.",
          "link": "http://arxiv.org/abs/2107.13217",
          "publishedOn": "2021-07-29T02:00:08.109Z",
          "wordCount": 632,
          "title": "DeepTeeth: A Teeth-photo Based Human Authentication System for Mobile and Hand-held Devices. (arXiv:2107.13217v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kuiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>",
          "description": "Video Instance Segmentation (VIS) is a new and inherently multi-task problem,\nwhich aims to detect, segment and track each instance in a video sequence.\nExisting approaches are mainly based on single-frame features or single-scale\nfeatures of multiple frames, where temporal information or multi-scale\ninformation is ignored. To incorporate both temporal and scale information, we\npropose a Temporal Pyramid Routing (TPR) strategy to conditionally align and\nconduct pixel-level aggregation from a feature pyramid pair of two adjacent\nframes. Specifically, TPR contains two novel components, including Dynamic\nAligned Cell Routing (DACR) and Cross Pyramid Routing (CPR), where DACR is\ndesigned for aligning and gating pyramid features across temporal dimension,\nwhile CPR transfers temporally aggregated features across scale dimension.\nMoreover, our approach is a plug-and-play module and can be easily applied to\nexisting instance segmentation methods. Extensive experiments on YouTube-VIS\ndataset demonstrate the effectiveness and efficiency of the proposed approach\non several state-of-the-art instance segmentation methods. Codes and trained\nmodels will be publicly available to facilitate future\nresearch.(\\url{https://github.com/lxtGH/TemporalPyramidRouting}).",
          "link": "http://arxiv.org/abs/2107.13155",
          "publishedOn": "2021-07-29T02:00:08.090Z",
          "wordCount": 609,
          "title": "Improving Video Instance Segmentation via Temporal Pyramid Routing. (arXiv:2107.13155v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Sida Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>",
          "description": "We present a new neural representation, called Neural Ray (NeuRay), for the\nnovel view synthesis (NVS) task with multi-view images as input. Existing\nneural scene representations for solving the NVS problem, such as NeRF, cannot\ngeneralize to new scenes and take an excessively long time on training on each\nnew scene from scratch. The other subsequent neural rendering methods based on\nstereo matching, such as PixelNeRF, SRF and IBRNet are designed to generalize\nto unseen scenes but suffer from view inconsistency in complex scenes with\nself-occlusions. To address these issues, our NeuRay method represents every\nscene by encoding the visibility of rays associated with the input views. This\nneural representation can efficiently be initialized from depths estimated by\nexternal MVS methods, which is able to generalize to new scenes and achieves\nsatisfactory rendering images without any training on the scene. Then, the\ninitialized NeuRay can be further optimized on every scene with little training\ntiming to enforce spatial coherence to ensure view consistency in the presence\nof severe self-occlusion. Experiments demonstrate that NeuRay can quickly\ngenerate high-quality novel view images of unseen scenes with little finetuning\nand can handle complex scenes with severe self-occlusions which previous\nmethods struggle with.",
          "link": "http://arxiv.org/abs/2107.13421",
          "publishedOn": "2021-07-29T02:00:08.082Z",
          "wordCount": 647,
          "title": "Neural Rays for Occlusion-aware Image-based Rendering. (arXiv:2107.13421v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kyrkou_C/0/1/0/all/0/1\">Christos Kyrkou</a>",
          "description": "The need for automated real-time visual systems in applications such as smart\ncamera surveillance, smart environments, and drones necessitates the\nimprovement of methods for visual active monitoring and control. Traditionally,\nthe active monitoring task has been handled through a pipeline of modules such\nas detection, filtering, and control. However, such methods are difficult to\njointly optimize and tune their various parameters for real-time processing in\nresource constraint systems. In this paper a deep Convolutional Camera\nController Neural Network is proposed to go directly from visual information to\ncamera movement to provide an efficient solution to the active vision problem.\nIt is trained end-to-end without bounding box annotations to control a camera\nand follow multiple targets from raw pixel values. Evaluation through both a\nsimulation framework and real experimental setup, indicate that the proposed\nsolution is robust to varying conditions and able to achieve better monitoring\nperformance than traditional approaches both in terms of number of targets\nmonitored as well as in effective monitoring time. The advantage of the\nproposed approach is that it is computationally less demanding and can run at\nover 10 FPS (~4x speedup) on an embedded smart camera providing a practical and\naffordable solution to real-time active monitoring.",
          "link": "http://arxiv.org/abs/2107.13233",
          "publishedOn": "2021-07-29T02:00:08.071Z",
          "wordCount": 673,
          "title": "C^3Net: End-to-End deep learning for efficient real-time visual active camera control. (arXiv:2107.13233v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ze Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Feng Lu</a>",
          "description": "Remote photoplethysmography (rPPG) monitors heart rate without requiring\nphysical contact, which allows for a wide variety of applications. Deep\nlearning-based rPPG have demonstrated superior performance over the traditional\napproaches in controlled context. However, the lighting situation in indoor\nspace is typically complex, with uneven light distribution and frequent\nvariations in illumination. It lacks a fair comparison of different methods\nunder different illuminations using the same dataset. In this paper, we present\na public dataset, namely the BH-rPPG dataset, which contains data from twelve\nsubjects under three illuminations: low, medium, and high illumination. We also\nprovide the ground truth heart rate measured by an oximeter. We evaluate the\nperformance of three deep learning-based methods to that of four traditional\nmethods using two public datasets: the UBFC-rPPG dataset and the BH-rPPG\ndataset. The experimental results demonstrate that traditional methods are\ngenerally more resistant to fluctuating illuminations. We found that the\nrPPGNet achieves lowest MAE among deep learning-based method under medium\nillumination, whereas the CHROM achieves 1.5 beats per minute (BPM),\noutperforming the rPPGNet by 60%. These findings suggest that while developing\ndeep learning-based heart rate estimation algorithms, illumination variation\nshould be taken into account. This work serves as a benchmark for rPPG\nperformance evaluation and it opens a pathway for future investigation into\ndeep learning-based rPPG under illumination variations.",
          "link": "http://arxiv.org/abs/2107.13193",
          "publishedOn": "2021-07-29T02:00:08.022Z",
          "wordCount": 666,
          "title": "Assessment of Deep Learning-based Heart Rate Estimation using Remote Photoplethysmography under Different Illuminations. (arXiv:2107.13193v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13048",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_R/0/1/0/all/0/1\">Richard J. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_M/0/1/0/all/0/1\">Ming Y. Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shaban_M/0/1/0/all/0/1\">Muhammad Shaban</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chengkuan Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1\">Tiffany Y. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Williamson_D/0/1/0/all/0/1\">Drew F. K. Williamson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahmood_F/0/1/0/all/0/1\">Faisal Mahmood</a>",
          "description": "Cancer prognostication is a challenging task in computational pathology that\nrequires context-aware representations of histology features to adequately\ninfer patient survival. Despite the advancements made in weakly-supervised deep\nlearning, many approaches are not context-aware and are unable to model\nimportant morphological feature interactions between cell identities and tissue\ntypes that are prognostic for patient survival. In this work, we present\nPatch-GCN, a context-aware, spatially-resolved patch-based graph convolutional\nnetwork that hierarchically aggregates instance-level histology features to\nmodel local- and global-level topological structures in the tumor\nmicroenvironment. We validate Patch-GCN with 4,370 gigapixel WSIs across five\ndifferent cancer types from the Cancer Genome Atlas (TCGA), and demonstrate\nthat Patch-GCN outperforms all prior weakly-supervised approaches by\n3.58-9.46%. Our code and corresponding models are publicly available at\nhttps://github.com/mahmoodlab/Patch-GCN.",
          "link": "http://arxiv.org/abs/2107.13048",
          "publishedOn": "2021-07-29T02:00:07.969Z",
          "wordCount": 606,
          "title": "Whole Slide Images are 2D Point Clouds: Context-Aware Survival Prediction using Patch-based Graph Convolutional Networks. (arXiv:2107.13048v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chenhang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>",
          "description": "Current geometry-based monocular 3D object detection models can efficiently\ndetect objects by leveraging perspective geometry, but their performance is\nlimited due to the absence of accurate depth information. Though this issue can\nbe alleviated in a depth-based model where a depth estimation module is plugged\nto predict depth information before 3D box reasoning, the introduction of such\nmodule dramatically reduces the detection speed. Instead of training a costly\ndepth estimator, we propose a rendering module to augment the training data by\nsynthesizing images with virtual-depths. The rendering module takes as input\nthe RGB image and its corresponding sparse depth image, outputs a variety of\nphoto-realistic synthetic images, from which the detection model can learn more\ndiscriminative features to adapt to the depth changes of the objects. Besides,\nwe introduce an auxiliary module to improve the detection model by jointly\noptimizing it through a depth estimation task. Both modules are working in the\ntraining time and no extra computation will be introduced to the detection\nmodel. Experiments show that by working with our proposed modules, a\ngeometry-based model can represent the leading accuracy on the KITTI 3D\ndetection benchmark.",
          "link": "http://arxiv.org/abs/2107.13269",
          "publishedOn": "2021-07-29T02:00:07.928Z",
          "wordCount": 636,
          "title": "Aug3D-RPN: Improving Monocular 3D Object Detection by Synthetic Images with Virtual Depth. (arXiv:2107.13269v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13157",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Trivedi_A/0/1/0/all/0/1\">Anusua Trivedi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Desbiens_J/0/1/0/all/0/1\">Jocelyn Desbiens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gross_R/0/1/0/all/0/1\">Ron Gross</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dodhia_R/0/1/0/all/0/1\">Rahul Dodhia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferres_J/0/1/0/all/0/1\">Juan Lavista Ferres</a>",
          "description": "Purpose: To demonstrate that retinal microvasculature per se is a reliable\nbiomarker for Diabetic Retinopathy (DR) and, by extension, cardiovascular\ndiseases. Methods: Deep Learning Convolutional Neural Networks (CNN) applied to\ncolor fundus images for semantic segmentation of the blood vessels and severity\nclassification on both vascular and full images. Vessel reconstruction through\nharmonic descriptors is also used as a smoothing and de-noising tool. The\nmathematical background of the theory is also outlined. Results: For diabetic\npatients, at least 93.8% of DR No-Refer vs. Refer classification can be related\nto vasculature defects. As for the Non-Sight Threatening vs. Sight Threatening\ncase, the ratio is as high as 96.7%. Conclusion: In the case of DR, most of the\ndisease biomarkers are related topologically to the vasculature. Translational\nRelevance: Experiments conducted on eye blood vasculature reconstruction as a\nbiomarker shows a strong correlation between vasculature shape and later stages\nof DR.",
          "link": "http://arxiv.org/abs/2107.13157",
          "publishedOn": "2021-07-29T02:00:07.920Z",
          "wordCount": 603,
          "title": "Retinal Microvasculature as Biomarker for Diabetes and Cardiovascular Diseases. (arXiv:2107.13157v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundgaard_K/0/1/0/all/0/1\">Keld T. Lundgaard</a>",
          "description": "By leveraging large amounts of product data collected across hundreds of live\ne-commerce websites, we construct 1000 unique classification tasks that share\nsimilarly-structured input data, comprised of both text and images. These\nclassification tasks focus on learning the product hierarchy of different\ne-commerce websites, causing many of them to be correlated. Adopting a\nmulti-modal transformer model, we solve these tasks in unison using multi-task\nlearning (MTL). Extensive experiments are presented over an initial 100-task\ndataset to reveal best practices for \"large-scale MTL\" (i.e., MTL with more\nthan 100 tasks). From these experiments, a final, unified methodology is\nderived, which is composed of both best practices and new proposals such as\nDyPa, a simple heuristic for automatically allocating task-specific parameters\nto tasks that could benefit from extra capacity. Using our large-scale MTL\nmethodology, we successfully train a single model across all 1000 tasks in our\ndataset while using minimal task specific parameters, thereby showing that it\nis possible to extend several orders of magnitude beyond current efforts in\nMTL.",
          "link": "http://arxiv.org/abs/2107.13054",
          "publishedOn": "2021-07-29T02:00:07.913Z",
          "wordCount": 623,
          "title": "Exceeding the Limits of Visual-Linguistic Multi-Task Learning. (arXiv:2107.13054v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meiling Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>",
          "description": "In this paper, we present a set of extremely efficient and high throughput\nmodels for accurate face verification, MixFaceNets which are inspired by Mixed\nDepthwise Convolutional Kernels. Extensive experiment evaluations on Label Face\nin the Wild (LFW), Age-DB, MegaFace, and IARPA Janus Benchmarks IJB-B and IJB-C\ndatasets have shown the effectiveness of our MixFaceNets for applications\nrequiring extremely low computational complexity. Under the same level of\ncomputation complexity (< 500M FLOPs), our MixFaceNets outperform\nMobileFaceNets on all the evaluated datasets, achieving 99.60% accuracy on LFW,\n97.05% accuracy on AgeDB-30, 93.60 TAR (at FAR1e-6) on MegaFace, 90.94 TAR (at\nFAR1e-4) on IJB-B and 93.08 TAR (at FAR1e-4) on IJB-C. With computational\ncomplexity between 500M and 1G FLOPs, our MixFaceNets achieved results\ncomparable to the top-ranked models, while using significantly fewer FLOPs and\nless computation overhead, which proves the practical value of our proposed\nMixFaceNets. All training codes, pre-trained models, and training logs have\nbeen made available https://github.com/fdbtrs/mixfacenets.",
          "link": "http://arxiv.org/abs/2107.13046",
          "publishedOn": "2021-07-29T02:00:07.905Z",
          "wordCount": 602,
          "title": "MixFaceNets: Extremely Efficient Face Recognition Networks. (arXiv:2107.13046v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiaojie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>",
          "description": "Video prediction methods generally consume substantial computing resources in\ntraining and deployment, among which keypoint-based approaches show promising\nimprovement in efficiency by simplifying dense image prediction to light\nkeypoint prediction. However, keypoint locations are often modeled only as\ncontinuous coordinates, so noise from semantically insignificant deviations in\nvideos easily disrupt learning stability, leading to inaccurate keypoint\nmodeling. In this paper, we design a new grid keypoint learning framework,\naiming at a robust and explainable intermediate keypoint representation for\nlong-term efficient video prediction. We have two major technical\ncontributions. First, we detect keypoints by jumping among candidate locations\nin our raised grid space and formulate a condensation loss to encourage\nmeaningful keypoints with strong representative capability. Second, we\nintroduce a 2D binary map to represent the detected grid keypoints and then\nsuggest propagating keypoint locations with stochasticity by selecting entries\nin the discrete grid space, thus preserving the spatial structure of keypoints\nin the longterm horizon for better future frame generation. Extensive\nexperiments verify that our method outperforms the state-ofthe-art stochastic\nvideo prediction methods while saves more than 98% of computing resources. We\nalso demonstrate our method on a robotic-assisted surgery dataset with\npromising results. Our code is available at\nhttps://github.com/xjgaocs/Grid-Keypoint-Learning.",
          "link": "http://arxiv.org/abs/2107.13170",
          "publishedOn": "2021-07-29T02:00:07.898Z",
          "wordCount": 642,
          "title": "Accurate Grid Keypoint Learning for Efficient Video Prediction. (arXiv:2107.13170v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elhagry_A/0/1/0/all/0/1\">Ahmed Elhagry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadaoui_K/0/1/0/all/0/1\">Karima Kadaoui</a>",
          "description": "Image Captioning is a task that combines computer vision and natural language\nprocessing, where it aims to generate descriptive legends for images. It is a\ntwo-fold process relying on accurate image understanding and correct language\nunderstanding both syntactically and semantically. It is becoming increasingly\ndifficult to keep up with the latest research and findings in the field of\nimage captioning due to the growing amount of knowledge available on the topic.\nThere is not, however, enough coverage of those findings in the available\nreview papers. We perform in this paper a run-through of the current\ntechniques, datasets, benchmarks and evaluation metrics used in image\ncaptioning. The current research on the field is mostly focused on deep\nlearning-based methods, where attention mechanisms along with deep\nreinforcement and adversarial learning appear to be in the forefront of this\nresearch topic. In this paper, we review recent methodologies such as UpDown,\nOSCAR, VIVO, Meta Learning and a model that uses conditional generative\nadversarial nets. Although the GAN-based model achieves the highest score,\nUpDown represents an important basis for image captioning and OSCAR and VIVO\nare more useful as they use novel object captioning. This review paper serves\nas a roadmap for researchers to keep up to date with the latest contributions\nmade in the field of image caption generation.",
          "link": "http://arxiv.org/abs/2107.13114",
          "publishedOn": "2021-07-29T02:00:07.877Z",
          "wordCount": 653,
          "title": "A Thorough Review on Recent Deep Learning Methodologies for Image Captioning. (arXiv:2107.13114v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chaoqiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qiyu Sun</a>",
          "description": "Previous unsupervised monocular depth estimation methods mainly focus on the\nday-time scenario, and their frameworks are driven by warped photometric\nconsistency. While in some challenging environments, like night, rainy night or\nsnowy winter, the photometry of the same pixel on different frames is\ninconsistent because of the complex lighting and reflection, so that the\nday-time unsupervised frameworks cannot be directly applied to these complex\nscenarios. In this paper, we investigate the problem of unsupervised monocular\ndepth estimation in certain highly complex scenarios. We address this\nchallenging problem by using domain adaptation, and a unified image\ntransfer-based adaptation framework is proposed based on monocular videos in\nthis paper. The depth model trained on day-time scenarios is adapted to\ndifferent complex scenarios. Instead of adapting the whole depth network, we\njust consider the encoder network for lower computational complexity. The depth\nmodels adapted by the proposed framework to different scenarios share the same\ndecoder, which is practical. Constraints on both feature space and output space\npromote the framework to learn the key features for depth decoding, and the\nsmoothness loss is introduced into the adaptation framework for better depth\nestimation performance. Extensive experiments show the effectiveness of the\nproposed unsupervised framework in estimating the dense depth map from the\nnight-time, rainy night-time and snowy winter images.",
          "link": "http://arxiv.org/abs/2107.13137",
          "publishedOn": "2021-07-29T02:00:07.868Z",
          "wordCount": 653,
          "title": "Unsupervised Monocular Depth Estimation in Highly Complex Environments. (arXiv:2107.13137v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elhagry_A/0/1/0/all/0/1\">Ahmed Elhagry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadaoui_K/0/1/0/all/0/1\">Karima Kadaoui</a>",
          "description": "Image captioning is a task in the field of Artificial Intelligence that\nmerges between computer vision and natural language processing. It is\nresponsible for generating legends that describe images, and has various\napplications like descriptions used by assistive technology or indexing images\n(for search engines for instance). This makes it a crucial topic in AI that is\nundergoing a lot of research. This task however, like many others, is trained\non large images labeled via human annotation, which can be very cumbersome: it\nneeds manual effort, both financial and temporal costs, it is error-prone and\npotentially difficult to execute in some cases (e.g. medical images). To\nmitigate the need for labels, we attempt to use self-supervised learning, a\ntype of learning where models use the data contained within the images\nthemselves as labels. It is challenging to accomplish though, since the task is\ntwo-fold: the images and captions come from two different modalities and\nusually handled by different types of networks. It is thus not obvious what a\ncompletely self-supervised solution would look like. How it would achieve\ncaptioning in a comparable way to how self-supervision is applied today on\nimage recognition tasks is still an ongoing research topic. In this project, we\nare using an encoder-decoder architecture where the encoder is a convolutional\nneural network (CNN) trained on OpenImages dataset and learns image features in\na self-supervised fashion using the rotation pretext task. The decoder is a\nLong Short-Term Memory (LSTM), and it is trained, along within the image\ncaptioning model, on MS COCO dataset and is responsible of generating captions.\nOur GitHub repository can be found:\nhttps://github.com/elhagry1/SSL_ImageCaptioning_RotationPrediction",
          "link": "http://arxiv.org/abs/2107.13111",
          "publishedOn": "2021-07-29T02:00:07.855Z",
          "wordCount": 704,
          "title": "Experimenting with Self-Supervision using Rotation Prediction for Image Captioning. (arXiv:2107.13111v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dsouza_D/0/1/0/all/0/1\">Daniel D&#x27;souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nussbaum_Z/0/1/0/all/0/1\">Zach Nussbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>",
          "description": "As machine learning models are increasingly employed to assist human\ndecision-makers, it becomes critical to communicate the uncertainty associated\nwith these model predictions. However, the majority of work on uncertainty has\nfocused on traditional probabilistic or ranking approaches - where the model\nassigns low probabilities or scores to uncertain examples. While this captures\nwhat examples are challenging for the model, it does not capture the underlying\nsource of the uncertainty. In this work, we seek to identify examples the model\nis uncertain about and characterize the source of said uncertainty. We explore\nthe benefits of designing a targeted intervention - targeted data augmentation\nof the examples where the model is uncertain over the course of training. We\ninvestigate whether the rate of learning in the presence of additional\ninformation differs between atypical and noisy examples? Our results show that\nthis is indeed the case, suggesting that well-designed interventions over the\ncourse of training can be an effective way to characterize and distinguish\nbetween different sources of uncertainty.",
          "link": "http://arxiv.org/abs/2107.13098",
          "publishedOn": "2021-07-29T02:00:07.847Z",
          "wordCount": 618,
          "title": "A Tale Of Two Long Tails. (arXiv:2107.13098v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jinlei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qiaoyong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Di Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong Zhou</a>",
          "description": "Reconstruction-based methods play an important role in unsupervised anomaly\ndetection in images. Ideally, we expect a perfect reconstruction for normal\nsamples and poor reconstruction for abnormal samples. Since the\ngeneralizability of deep neural networks is difficult to control, existing\nmodels such as autoencoder do not work well. In this work, we interpret the\nreconstruction of an image as a divide-and-assemble procedure. Surprisingly, by\nvarying the granularity of division on feature maps, we are able to modulate\nthe reconstruction capability of the model for both normal and abnormal\nsamples. That is, finer granularity leads to better reconstruction, while\ncoarser granularity leads to poorer reconstruction. With proper granularity,\nthe gap between the reconstruction error of normal and abnormal samples can be\nmaximized. The divide-and-assemble framework is implemented by embedding a\nnovel multi-scale block-wise memory module into an autoencoder network.\nBesides, we introduce adversarial learning and explore the semantic latent\nrepresentation of the discriminator, which improves the detection of subtle\nanomaly. We achieve state-of-the-art performance on the challenging MVTec AD\ndataset. Remarkably, we improve the vanilla autoencoder model by 10.1% in terms\nof the AUROC score.",
          "link": "http://arxiv.org/abs/2107.13118",
          "publishedOn": "2021-07-29T02:00:07.831Z",
          "wordCount": 628,
          "title": "Divide-and-Assemble: Learning Block-wise Memory for Unsupervised Anomaly Detection. (arXiv:2107.13118v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Walsh_R/0/1/0/all/0/1\">Reece Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelpakey_M/0/1/0/all/0/1\">Mohamed H. Abdelpakey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1\">Mohamed S. Shehata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1\">Mostafa M.Mohamed</a>",
          "description": "Classifying and analyzing human cells is a lengthy procedure, often involving\na trained professional. In an attempt to expedite this process, an active area\nof research involves automating cell classification through use of deep\nlearning-based techniques. In practice, a large amount of data is required to\naccurately train these deep learning models. However, due to the sparse human\ncell datasets currently available, the performance of these models is typically\nlow. This study investigates the feasibility of using few-shot learning-based\ntechniques to mitigate the data requirements for accurate training. The study\nis comprised of three parts: First, current state-of-the-art few-shot learning\ntechniques are evaluated on human cell classification. The selected techniques\nare trained on a non-medical dataset and then tested on two out-of-domain,\nhuman cell datasets. The results indicate that, overall, the test accuracy of\nstate-of-the-art techniques decreased by at least 30% when transitioning from a\nnon-medical dataset to a medical dataset. Second, this study evaluates the\npotential benefits, if any, to varying the backbone architecture and training\nschemes in current state-of-the-art few-shot learning techniques when used in\nhuman cell classification. Even with these variations, the overall test\naccuracy decreased from 88.66% on non-medical datasets to 44.13% at best on the\nmedical datasets. Third, this study presents future directions for using\nfew-shot learning in human cell classification. In general, few-shot learning\nin its current state performs poorly on human cell classification. The study\nproves that attempts to modify existing network architectures are not effective\nand concludes that future research effort should be focused on improving\nrobustness towards out-of-domain testing using optimization-based or\nself-supervised few-shot learning techniques.",
          "link": "http://arxiv.org/abs/2107.13093",
          "publishedOn": "2021-07-29T02:00:07.805Z",
          "wordCount": 720,
          "title": "Automated Human Cell Classification in Sparse Datasets using Few-Shot Learning. (arXiv:2107.13093v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13136",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marino_J/0/1/0/all/0/1\">Joseph Marino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mandt_S/0/1/0/all/0/1\">Stephan Mandt</a>",
          "description": "While recent machine learning research has revealed connections between deep\ngenerative models such as VAEs and rate-distortion losses used in learned\ncompression, most of this work has focused on images. In a similar spirit, we\nview recently proposed neural video coding algorithms through the lens of deep\nautoregressive and latent variable modeling. We present recent neural video\ncodecs as instances of a generalized stochastic temporal autoregressive\ntransform, and propose new avenues for further improvements inspired by\nnormalizing flows and structured priors. We propose several architectures that\nyield state-of-the-art video compression performance on full-resolution video\nand discuss their tradeoffs and ablations. In particular, we propose (i)\nimproved temporal autoregressive transforms, (ii) improved entropy models with\nstructured and temporal dependencies, and (iii) variable bitrate versions of\nour algorithms. Since our improvements are compatible with a large class of\nexisting models, we provide further evidence that the generative modeling\nviewpoint can advance the neural video coding field.",
          "link": "http://arxiv.org/abs/2107.13136",
          "publishedOn": "2021-07-29T02:00:07.797Z",
          "wordCount": 637,
          "title": "Insights from Generative Modeling for Neural Video Compression. (arXiv:2107.13136v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>",
          "description": "This thesis presents methods and approaches to image color correction, color\nenhancement, and color editing. To begin, we study the color correction problem\nfrom the standpoint of the camera's image signal processor (ISP). A camera's\nISP is hardware that applies a series of in-camera image processing and color\nmanipulation steps, many of which are nonlinear in nature, to render the\ninitial sensor image to its final photo-finished representation saved in the\n8-bit standard RGB (sRGB) color space. As white balance (WB) is one of the\nmajor procedures applied by the ISP for color correction, this thesis presents\ntwo different methods for ISP white balancing. Afterward, we discuss another\nscenario of correcting and editing image colors, where we present a set of\nmethods to correct and edit WB settings for images that have been improperly\nwhite-balanced by the ISP. Then, we explore another factor that has a\nsignificant impact on the quality of camera-rendered colors, in which we\noutline two different methods to correct exposure errors in camera-rendered\nimages. Lastly, we discuss post-capture auto color editing and manipulation. In\nparticular, we propose auto image recoloring methods to generate different\nrealistic versions of the same camera-rendered image with new colors. Through\nextensive evaluations, we demonstrate that our methods provide superior\nsolutions compared to existing alternatives targeting color correction, color\nenhancement, and color editing.",
          "link": "http://arxiv.org/abs/2107.13117",
          "publishedOn": "2021-07-29T02:00:07.789Z",
          "wordCount": 649,
          "title": "Image color correction, enhancement, and editing. (arXiv:2107.13117v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13108",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1\">Bin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_N/0/1/0/all/0/1\">Nan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianfu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>",
          "description": "This paper presents a neural network built upon Transformers, namely PlaneTR,\nto simultaneously detect and reconstruct planes from a single image. Different\nfrom previous methods, PlaneTR jointly leverages the context information and\nthe geometric structures in a sequence-to-sequence way to holistically detect\nplane instances in one forward pass. Specifically, we represent the geometric\nstructures as line segments and conduct the network with three main components:\n(i) context and line segments encoders, (ii) a structure-guided plane decoder,\n(iii) a pixel-wise plane embedding decoder. Given an image and its detected\nline segments, PlaneTR generates the context and line segment sequences via two\nspecially designed encoders and then feeds them into a Transformers-based\ndecoder to directly predict a sequence of plane instances by simultaneously\nconsidering the context and global structure cues. Finally, the pixel-wise\nembeddings are computed to assign each pixel to one predicted plane instance\nwhich is nearest to it in embedding space. Comprehensive experiments\ndemonstrate that PlaneTR achieves a state-of-the-art performance on the ScanNet\nand NYUv2 datasets.",
          "link": "http://arxiv.org/abs/2107.13108",
          "publishedOn": "2021-07-29T02:00:07.753Z",
          "wordCount": 608,
          "title": "PlaneTR: Structure-Guided Transformers for 3D Plane Recovery. (arXiv:2107.13108v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuefan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Youyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">C. Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>",
          "description": "We describe a method for realistic depth synthesis that learns diverse\nvariations from the real depth scans and ensures geometric consistency for\neffective synthetic-to-real transfer. Unlike general image synthesis pipelines,\nwhere geometries are mostly ignored, we treat geometries carried by the depth\nbased on their own existence. We propose differential contrastive learning that\nexplicitly enforces the underlying geometric properties to be invariant\nregarding the real variations been learned. The resulting depth synthesis\nmethod is task-agnostic and can be used for training any task-specific networks\nwith synthetic labels. We demonstrate the effectiveness of the proposed method\nby extensive evaluations on downstream real-world geometric reasoning tasks. We\nshow our method achieves better synthetic-to-real transfer performance than the\nother state-of-the-art. When fine-tuned on a small number of real-world\nannotations, our method can even surpass the fully supervised baselines.",
          "link": "http://arxiv.org/abs/2107.13087",
          "publishedOn": "2021-07-29T02:00:07.746Z",
          "wordCount": 574,
          "title": "DCL: Differential Contrastive Learning for Geometry-Aware Depth Synthesis. (arXiv:2107.13087v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Ying Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Pei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>",
          "description": "This paper revisits human-object interaction (HOI) recognition at image level\nwithout using supervisions of object location and human pose. We name it\ndetection-free HOI recognition, in contrast to the existing\ndetection-supervised approaches which rely on object and keypoint detections to\nachieve state of the art. With our method, not only the detection supervision\nis evitable, but superior performance can be achieved by properly using\nimage-text pre-training (such as CLIP) and the proposed Log-Sum-Exp Sign\n(LSE-Sign) loss function. Specifically, using text embeddings of class labels\nto initialize the linear classifier is essential for leveraging the CLIP\npre-trained image encoder. In addition, LSE-Sign loss facilitates learning from\nmultiple labels on an imbalanced dataset by normalizing gradients over all\nclasses in a softmax format. Surprisingly, our detection-free solution achieves\n60.5 mAP on the HICO dataset, outperforming the detection-supervised state of\nthe art by 13.4 mAP",
          "link": "http://arxiv.org/abs/2107.13083",
          "publishedOn": "2021-07-29T02:00:07.738Z",
          "wordCount": 588,
          "title": "Is Object Detection Necessary for Human-Object Interaction Recognition?. (arXiv:2107.13083v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1\">Shreyank N Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevilla_Lara_L/0/1/0/all/0/1\">Laura Sevilla-Lara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kiyoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_M/0/1/0/all/0/1\">Marcus Rohrbach</a>",
          "description": "Zero-shot action recognition is the task of classifying action categories\nthat are not available in the training set. In this setting, the standard\nevaluation protocol is to use existing action recognition datasets (e.g.\nUCF101) and randomly split the classes into seen and unseen. However, most\nrecent work builds on representations pre-trained on the Kinetics dataset,\nwhere classes largely overlap with classes in the zero-shot evaluation\ndatasets. As a result, classes which are supposed to be unseen, are present\nduring supervised pre-training, invalidating the condition of the zero-shot\nsetting. A similar concern was previously noted several years ago for image\nbased zero-shot recognition, but has not been considered by the zero-shot\naction recognition community. In this paper, we propose a new split for true\nzero-shot action recognition with no overlap between unseen test classes and\ntraining or pre-training classes. We benchmark several recent approaches on the\nproposed True Zero-Shot (TruZe) Split for UCF101 and HMDB51, with zero-shot and\ngeneralized zero-shot evaluation. In our extensive analysis we find that our\nTruZe splits are significantly harder than comparable random splits as nothing\nis leaking from pre-training, i.e. unseen performance is consistently lower, up\nto 9.4% for zero-shot action recognition. In an additional evaluation we also\nfind that similar issues exist in the splits used in few-shot action\nrecognition, here we see differences of up to 14.1%. We publish our splits and\nhope that our benchmark analysis will change how the field is evaluating zero-\nand few-shot action recognition moving forward.",
          "link": "http://arxiv.org/abs/2107.13029",
          "publishedOn": "2021-07-29T02:00:07.688Z",
          "wordCount": 688,
          "title": "A New Split for Evaluating True Zero-Shot Action Recognition. (arXiv:2107.13029v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2102.03479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Siyang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harding_S/0/1/0/all/0/1\">Seth Austin Harding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haibin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shih-wei Liao</a>",
          "description": "Many complex multi-robot systems such as robot swarms control and autonomous\nvehicle coordination can be modeled as Multi-Agent Reinforcement Learning\n(MARL) tasks. QMIX, a widely popular MARL algorithm, has been used as a\nbaseline for the benchmark environments, e.g., Starcraft Multi-Agent Challenge\n(SMAC), Difficulty-Enhanced Predator-Prey (DEPP). Recent variants of QMIX\ntarget relaxing the monotonicity constraint of QMIX, allowing for performance\nimprovement in SMAC. In this paper, we investigate the code-level optimizations\nof these variants and the monotonicity constraint. (1) We find that such\nimprovements of the variants are significantly affected by various code-level\noptimizations. (2) The experiment results show that QMIX with normalized\noptimizations outperforms other works in SMAC; (3) beyond the common wisdom\nfrom these works, the monotonicity constraint can improve sample efficiency in\nSMAC and DEPP. We also discuss why monotonicity constraints work well in purely\ncooperative tasks with a theoretical analysis. We open-source the code at\n\\url{https://github.com/hijkzzz/pymarl2}.",
          "link": "http://arxiv.org/abs/2102.03479",
          "publishedOn": "2021-08-03T02:06:35.290Z",
          "wordCount": 731,
          "title": "Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2102.03479v13 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuying Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lei Bai Liang Lin</a>",
          "description": "Metro origin-destination prediction is a crucial yet challenging time-series\nanalysis task in intelligent transportation systems, which aims to accurately\nforecast two specific types of cross-station ridership, i.e.,\nOrigin-Destination (OD) one and Destination-Origin (DO) one. However, complete\nOD matrices of previous time intervals can not be obtained immediately in\nonline metro systems, and conventional methods only used limited information to\nforecast the future OD and DO ridership separately. In this work, we proposed a\nnovel neural network module termed Heterogeneous Information Aggregation\nMachine (HIAM), which fully exploits heterogeneous information of historical\ndata (e.g., incomplete OD matrices, unfinished order vectors, and DO matrices)\nto jointly learn the evolutionary patterns of OD and DO ridership.\nSpecifically, an OD modeling branch estimates the potential destinations of\nunfinished orders explicitly to complement the information of incomplete OD\nmatrices, while a DO modeling branch takes DO matrices as input to capture the\nspatial-temporal distribution of DO ridership. Moreover, a Dual Information\nTransformer is introduced to propagate the mutual information among OD features\nand DO features for modeling the OD-DO causality and correlation. Based on the\nproposed HIAM, we develop a unified Seq2Seq network to forecast the future OD\nand DO ridership simultaneously. Extensive experiments conducted on two\nlarge-scale benchmarks demonstrate the effectiveness of our method for online\nmetro origin-destination prediction.",
          "link": "http://arxiv.org/abs/2107.00946",
          "publishedOn": "2021-08-03T02:06:35.235Z",
          "wordCount": 688,
          "title": "Online Metro Origin-Destination Prediction via Heterogeneous Information Aggregation. (arXiv:2107.00946v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00781",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hodgkinson_L/0/1/0/all/0/1\">Liam Hodgkinson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Simsekli_U/0/1/0/all/0/1\">Umut &#x15e;im&#x15f;ekli</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Khanna_R/0/1/0/all/0/1\">Rajiv Khanna</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>",
          "description": "Despite the ubiquitous use of stochastic optimization algorithms in machine\nlearning, the precise impact of these algorithms on generalization performance\nin realistic non-convex settings is still poorly understood. In this paper, we\nprovide an encompassing theoretical framework for investigating the\ngeneralization properties of stochastic optimizers, which is based on their\ndynamics. We first prove a generalization bound attributable to the optimizer\ndynamics in terms of the celebrated Fernique-Talagrand functional applied to\nthe trajectory of the optimizer. This data- and algorithm-dependent bound is\nshown to be the sharpest possible in the absence of further assumptions. We\nthen specialize this result by exploiting the Markovian structure of stochastic\noptimizers, deriving generalization bounds in terms of the (data-dependent)\ntransition kernels associated with the optimization algorithms. In line with\nrecent work that has revealed connections between generalization and\nheavy-tailed behavior in stochastic optimization, we link the generalization\nerror to the local tail behavior of the transition kernels. We illustrate that\nthe local power-law exponent of the kernel acts as an effective dimension,\nwhich decreases as the transitions become \"less Gaussian\". We support our\ntheory with empirical results from a variety of neural networks, and we show\nthat both the Fernique-Talagrand functional and the local power-law exponent\nare predictive of generalization performance.",
          "link": "http://arxiv.org/abs/2108.00781",
          "publishedOn": "2021-08-03T02:06:35.185Z",
          "wordCount": 647,
          "title": "Generalization Properties of Stochastic Optimizers via Trajectory Analysis. (arXiv:2108.00781v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2104.01672",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Vlontzos_A/0/1/0/all/0/1\">Athanasios Vlontzos</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cao_Y/0/1/0/all/0/1\">Yueqi Cao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schmidtke_L/0/1/0/all/0/1\">Luca Schmidtke</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Monod_A/0/1/0/all/0/1\">Anthea Monod</a>",
          "description": "Appropriately representing elements in a database so that queries may be\naccurately matched is a central task in information retrieval; recently, this\nhas been achieved by embedding the graphical structure of the database into a\nmanifold in a hierarchy-preserving manner using a variety of metrics.\nPersistent homology is a tool commonly used in topological data analysis that\nis able to rigorously characterize a database in terms of both its hierarchy\nand connectivity structure. Computing persistent homology on a variety of\nembedded datasets reveals that some commonly used embeddings fail to preserve\nthe connectivity. We show that those embeddings which successfully retain the\ndatabase topology coincide in persistent homology by introducing two\ndilation-invariant comparative measures to capture this effect: in particular,\nthey address the issue of metric distortion on manifolds. We provide an\nalgorithm for their computation that exhibits greatly reduced time complexity\nover existing methods. We use these measures to perform the first instance of\ntopology-based information retrieval and demonstrate its increased performance\nover the standard bottleneck distance for persistent homology. We showcase our\napproach on databases of different data varieties including text, videos, and\nmedical images.",
          "link": "http://arxiv.org/abs/2104.01672",
          "publishedOn": "2021-08-03T02:06:35.179Z",
          "wordCount": 657,
          "title": "Topological Information Retrieval with Dilation-Invariant Bottleneck Comparative Measures. (arXiv:2104.01672v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.10743",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Luo_Y/0/1/0/all/0/1\">Yuetian Luo</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_A/0/1/0/all/0/1\">Anru R. Zhang</a>",
          "description": "This paper studies the statistical and computational limits of high-order\nclustering with planted structures. We focus on two clustering models, constant\nhigh-order clustering (CHC) and rank-one higher-order clustering (ROHC), and\nstudy the methods and theory for testing whether a cluster exists (detection)\nand identifying the support of cluster (recovery).\n\nSpecifically, we identify the sharp boundaries of signal-to-noise ratio for\nwhich CHC and ROHC detection/recovery are statistically possible. We also\ndevelop the tight computational thresholds: when the signal-to-noise ratio is\nbelow these thresholds, we prove that polynomial-time algorithms cannot solve\nthese problems under the computational hardness conjectures of hypergraphic\nplanted clique (HPC) detection and hypergraphic planted dense subgraph (HPDS)\nrecovery. We also propose polynomial-time tensor algorithms that achieve\nreliable detection and recovery when the signal-to-noise ratio is above these\nthresholds. Both sparsity and tensor structures yield the computational\nbarriers in high-order tensor clustering. The interplay between them results in\nsignificant differences between high-order tensor clustering and matrix\nclustering in literature in aspects of statistical and computational phase\ntransition diagrams, algorithmic approaches, hardness conjecture, and proof\ntechniques. To our best knowledge, we are the first to give a thorough\ncharacterization of the statistical and computational trade-off for such a\ndouble computational-barrier problem. Finally, we provide evidence for the\ncomputational hardness conjectures of HPC detection (via low-degree polynomial\nand Metropolis methods) and HPDS recovery (via low-degree polynomial method).",
          "link": "http://arxiv.org/abs/2005.10743",
          "publishedOn": "2021-08-03T02:06:35.173Z",
          "wordCount": 714,
          "title": "Tensor Clustering with Planted Structures: Statistical Optimality and Computational Limits. (arXiv:2005.10743v3 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Freund_Y/0/1/0/all/0/1\">Yoav Freund</a>",
          "description": "The backwards induction method due to Bellman~\\cite{bellman1952theory} is a\npopular approach to solving problems in optimiztion, optimal control, and many\nother areas of applied math. In this paper we analyze the backwords induction\napproach, under min/max conditions. We show that if the value function is has\nstrictly positive derivatives of order 1-4 then the optimal strategy for the\nadversary is Brownian motion. Using that fact we analyze different potential\nfunctions and show that the Normal-Hedge potential is optimal.",
          "link": "http://arxiv.org/abs/2106.10717",
          "publishedOn": "2021-08-03T02:06:35.154Z",
          "wordCount": 535,
          "title": "Strategies for convex potential games and an application to decision-theoretic online learning. (arXiv:2106.10717v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parry_H/0/1/0/all/0/1\">Hishan Parry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xun_L/0/1/0/all/0/1\">Lei Xun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_A/0/1/0/all/0/1\">Amin Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jia Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrett_G/0/1/0/all/0/1\">Geoff V. Merrett</a>",
          "description": "The Transformer architecture is widely used for machine translation tasks.\nHowever, its resource-intensive nature makes it challenging to implement on\nconstrained embedded devices, particularly where available hardware resources\ncan vary at run-time. We propose a dynamic machine translation model that\nscales the Transformer architecture based on the available resources at any\nparticular time. The proposed approach, 'Dynamic-HAT', uses a HAT\nSuperTransformer as the backbone to search for SubTransformers with different\naccuracy-latency trade-offs at design time. The optimal SubTransformers are\nsampled from the SuperTransformer at run-time, depending on latency\nconstraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses\ninherited SubTransformers sampled directly from the SuperTransformer with a\nswitching time of <1s. Using inherited SubTransformers results in a BLEU score\nloss of <1.5% because the SubTransformer configuration is not retrained from\nscratch after sampling. However, to recover this loss in performance, the\ndimensions of the design space can be reduced to tailor it to a family of\ntarget hardware. The new reduced design space results in a BLEU score increase\nof approximately 1% for sub-optimal models from the original design space, with\na wide range for performance scaling between 0.356s - 1.526s for the GPU and\n2.9s - 7.31s for the CPU.",
          "link": "http://arxiv.org/abs/2107.08199",
          "publishedOn": "2021-08-03T02:06:35.148Z",
          "wordCount": 680,
          "title": "Dynamic Transformer for Efficient Machine Translation on Embedded Devices. (arXiv:2107.08199v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Huimin Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jiahao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>",
          "description": "Randomized Smoothing (RS), being one of few provable defenses, has been\nshowing great effectiveness and scalability in terms of defending against\n$\\ell_2$-norm adversarial perturbations. However, the cost of MC sampling\nneeded in RS for evaluation is high and computationally expensive. To address\nthis issue, we investigate the possibility of performing randomized smoothing\nand establishing the robust certification in the latent space of a network, so\nthat the overall dimensionality of tensors involved in computation could be\ndrastically reduced. To this end, we propose Latent Space Randomized Smoothing.\nAnother important aspect is that we use orthogonal modules, whose Lipschitz\nproperty is known for free by design, to propagate the certified radius\nestimated in the latent space back to the input space, providing valid\ncertifiable regions for the test samples in the input space. Experiments on\nCIFAR10 and ImageNet show that our method achieves competitive certified\nrobustness but with a significant improvement of efficiency during the test\nphase.",
          "link": "http://arxiv.org/abs/2108.00491",
          "publishedOn": "2021-08-03T02:06:35.142Z",
          "wordCount": 593,
          "title": "Certified Defense via Latent Space Randomized Smoothing with Orthogonal Encoders. (arXiv:2108.00491v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.00351",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chung_M/0/1/0/all/0/1\">Moo K. Chung</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ombao_H/0/1/0/all/0/1\">Hernando Ombao</a>",
          "description": "Persistent homology has undergone significant development in recent years.\nHowever, one outstanding challenge is to build a coherent statistical inference\nprocedure on persistent diagrams. In this paper, we first present a new lattice\npath representation for persistent diagrams. We then develop a new exact\nstatistical inference procedure for lattice paths via combinatorial\nenumerations. The lattice path method is applied to the topological\ncharacterization of the protein structures of the COVID-19 virus. We\ndemonstrate that there are topological changes during the conformational change\nof spike proteins.",
          "link": "http://arxiv.org/abs/2105.00351",
          "publishedOn": "2021-08-03T02:06:35.136Z",
          "wordCount": 603,
          "title": "Lattice Paths for Persistent Diagrams. (arXiv:2105.00351v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00473",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Xu_Z/0/1/0/all/0/1\">Zi Xu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Shen_J/0/1/0/all/0/1\">Jingjing Shen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_Z/0/1/0/all/0/1\">Ziqi Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Dai_Y/0/1/0/all/0/1\">Yuhong Dai</a>",
          "description": "In this paper, we study zeroth-order algorithms for nonconvex-concave minimax\nproblems, which have attracted widely attention in machine learning, signal\nprocessing and many other fields in recent years. We propose a zeroth-order\nalternating randomized gradient projection (ZO-AGP) algorithm for smooth\nnonconvex-concave minimax problems, and its iteration complexity to obtain an\n$\\varepsilon$-stationary point is bounded by $\\mathcal{O}(\\varepsilon^{-4})$,\nand the number of function value estimation is bounded by\n$\\mathcal{O}(d_{x}\\varepsilon^{-4}+d_{y}\\varepsilon^{-6})$ per iteration.\nMoreover, we propose a zeroth-order block alternating randomized proximal\ngradient algorithm (ZO-BAPG) for solving block-wise nonsmooth nonconvex-concave\nminimax optimization problems, and the iteration complexity to obtain an\n$\\varepsilon$-stationary point is bounded by $\\mathcal{O}(\\varepsilon^{-4})$\nand the number of function value estimation per iteration is bounded by\n$\\mathcal{O}(K d_{x}\\varepsilon^{-4}+d_{y}\\varepsilon^{-6})$. To the best of\nour knowledge, this is the first time that zeroth-order algorithms with\niteration complexity gurantee are developed for solving both general smooth and\nblock-wise nonsmooth nonconvex-concave minimax problems. Numerical results on\ndata poisoning attack problem validate the efficiency of the proposed\nalgorithms.",
          "link": "http://arxiv.org/abs/2108.00473",
          "publishedOn": "2021-08-03T02:06:35.119Z",
          "wordCount": 608,
          "title": "Zeroth-Order Alternating Randomized Gradient Projection Algorithms for General Nonconvex-Concave Minimax Problems. (arXiv:2108.00473v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/1912.02620",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xia_T/0/1/0/all/0/1\">Tian Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chartsias_A/0/1/0/all/0/1\">Agisilaos Chartsias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chengjia Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>",
          "description": "How will my face look when I get older? Or, for a more challenging question:\nHow will my brain look when I get older? To answer this question one must\ndevise (and learn from data) a multivariate auto-regressive function which\ngiven an image and a desired target age generates an output image. While\ncollecting data for faces may be easier, collecting longitudinal brain data is\nnot trivial. We propose a deep learning-based method that learns to simulate\nsubject-specific brain ageing trajectories without relying on longitudinal\ndata. Our method synthesises images conditioned on two factors: age (a\ncontinuous variable), and status of Alzheimer's Disease (AD, an ordinal\nvariable). With an adversarial formulation we learn the joint distribution of\nbrain appearance, age and AD status, and define reconstruction losses to\naddress the challenging problem of preserving subject identity. We compare with\nseveral benchmarks using two widely used datasets. We evaluate the quality and\nrealism of synthesised images using ground-truth longitudinal data and a\npre-trained age predictor. We show that, despite the use of cross-sectional\ndata, our model learns patterns of gray matter atrophy in the middle temporal\ngyrus in patients with AD. To demonstrate generalisation ability, we train on\none dataset and evaluate predictions on the other. In conclusion, our model\nshows an ability to separate age, disease influence and anatomy using only 2D\ncross-sectional data that should should be useful in large studies into\nneurodegenerative disease, that aim to combine several data sources. To\nfacilitate such future studies by the community at large our code is made\navailable at https://github.com/xiat0616/BrainAgeing.",
          "link": "http://arxiv.org/abs/1912.02620",
          "publishedOn": "2021-08-03T02:06:35.113Z",
          "wordCount": 762,
          "title": "Learning to synthesise the ageing brain without longitudinal data. (arXiv:1912.02620v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00683",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haliem_M/0/1/0/all/0/1\">Marina Haliem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonjour_T/0/1/0/all/0/1\">Trevor Bonjour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alsalem_A/0/1/0/all/0/1\">Aala Alsalem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Shilpa Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1\">Vaneet Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhargava_B/0/1/0/all/0/1\">Bharat Bhargava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kejriwal_M/0/1/0/all/0/1\">Mayank Kejriwal</a>",
          "description": "Learning to adapt and make real-time informed decisions in a dynamic and\ncomplex environment is a challenging problem. Monopoly is a popular strategic\nboard game that requires players to make multiple decisions during the game.\nDecision-making in Monopoly involves many real-world elements such as\nstrategizing, luck, and modeling of opponent's policies. In this paper, we\npresent novel representations for the state and action space for the full\nversion of Monopoly and define an improved reward function. Using these, we\nshow that our deep reinforcement learning agent can learn winning strategies\nfor Monopoly against different fixed-policy agents. In Monopoly, players can\ntake multiple actions even if it is not their turn to roll the dice. Some of\nthese actions occur more frequently than others, resulting in a skewed\ndistribution that adversely affects the performance of the learning agent. To\ntackle the non-uniform distribution of actions, we propose a hybrid approach\nthat combines deep reinforcement learning (for frequent but complex decisions)\nwith a fixed policy approach (for infrequent but straightforward decisions).\nExperimental results show that our hybrid agent outperforms a standard deep\nreinforcement learning agent by 30% in the number of games won against\nfixed-policy agents.",
          "link": "http://arxiv.org/abs/2103.00683",
          "publishedOn": "2021-08-03T02:06:35.107Z",
          "wordCount": 672,
          "title": "Decision Making in Monopoly using a Hybrid Deep Reinforcement Learning Approach. (arXiv:2103.00683v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07626",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Chia Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1\">Marina Meil&#x103;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kevrekidis_I/0/1/0/all/0/1\">Ioannis G. Kevrekidis</a>",
          "description": "The manifold Helmholtzian (1-Laplacian) operator $\\Delta_1$ elegantly\ngeneralizes the Laplace-Beltrami operator to vector fields on a manifold\n$\\mathcal M$. In this work, we propose the estimation of the manifold\nHelmholtzian from point cloud data by a weighted 1-Laplacian $\\mathbf{\\mathcal\nL}_1$. While higher order Laplacians ave been introduced and studied, this work\nis the first to present a graph Helmholtzian constructed from a simplicial\ncomplex as an estimator for the continuous operator in a non-parametric\nsetting. Equipped with the geometric and topological information about\n$\\mathcal M$, the Helmholtzian is a useful tool for the analysis of flows and\nvector fields on $\\mathcal M$ via the Helmholtz-Hodge theorem. In addition, the\n$\\mathbf{\\mathcal L}_1$ allows the smoothing, prediction, and feature\nextraction of the flows. We demonstrate these possibilities on substantial sets\nof synthetic and real point cloud datasets with non-trivial topological\nstructures; and provide theoretical results on the limit of $\\mathbf{\\mathcal\nL}_1$ to $\\Delta_1$.",
          "link": "http://arxiv.org/abs/2103.07626",
          "publishedOn": "2021-08-03T02:06:35.101Z",
          "wordCount": 611,
          "title": "Helmholtzian Eigenmap: Topological feature discovery & edge flow learning from point cloud data. (arXiv:2103.07626v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leonardos_S/0/1/0/all/0/1\">Stefanos Leonardos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Overman_W/0/1/0/all/0/1\">Will Overman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panageas_I/0/1/0/all/0/1\">Ioannis Panageas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1\">Georgios Piliouras</a>",
          "description": "Potential games are arguably one of the most important and widely studied\nclasses of normal form games. They define the archetypal setting of multi-agent\ncoordination as all agent utilities are perfectly aligned with each other via a\ncommon potential function. Can this intuitive framework be transplanted in the\nsetting of Markov Games? What are the similarities and differences between\nmulti-agent coordination with and without state dependence? We present a novel\ndefinition of Markov Potential Games (MPG) that generalizes prior attempts at\ncapturing complex stateful multi-agent coordination. Counter-intuitively,\ninsights from normal-form potential games do not carry over as MPGs can consist\nof settings where state-games can be zero-sum games. In the opposite direction,\nMarkov games where every state-game is a potential game are not necessarily\nMPGs. Nevertheless, MPGs showcase standard desirable properties such as the\nexistence of deterministic Nash policies. In our main technical result, we\nprove fast convergence of independent policy gradient to Nash policies by\nadapting recent gradient dominance property arguments developed for single\nagent MDPs to multi-agent learning settings.",
          "link": "http://arxiv.org/abs/2106.01969",
          "publishedOn": "2021-08-03T02:06:35.094Z",
          "wordCount": 652,
          "title": "Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games. (arXiv:2106.01969v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhongjie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingye Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trapp_M/0/1/0/all/0/1\">Martin Trapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skryagin_A/0/1/0/all/0/1\">Arseny Skryagin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>",
          "description": "Inspired by recent advances in the field of expert-based approximations of\nGaussian processes (GPs), we present an expert-based approach to large-scale\nmulti-output regression using single-output GP experts. Employing a deeply\nstructured mixture of single-output GPs encoded via a probabilistic circuit\nallows us to capture correlations between multiple output dimensions\naccurately. By recursively partitioning the covariate space and the output\nspace, posterior inference in our model reduces to inference on single-output\nGP experts, which only need to be conditioned on a small subset of the\nobservations. We show that inference can be performed exactly and efficiently\nin our model, that it can capture correlations between output dimensions and,\nhence, often outperforms approaches that do not incorporate inter-output\ncorrelations, as demonstrated on several data sets in terms of the negative log\npredictive density.",
          "link": "http://arxiv.org/abs/2106.08687",
          "publishedOn": "2021-08-03T02:06:35.088Z",
          "wordCount": 606,
          "title": "Leveraging Probabilistic Circuits for Nonparametric Multi-Output Regression. (arXiv:2106.08687v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balali_A/0/1/0/all/0/1\">Ali Balali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadpour_M/0/1/0/all/0/1\">Masoud Asadpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafari_S/0/1/0/all/0/1\">Seyed Hossein Jafari</a>",
          "description": "Data is published on the web over time in great volumes, but majority of the\ndata is unstructured, making it hard to understand and difficult to interpret.\nInformation Extraction (IE) methods extract structured information from\nunstructured data. One of the challenging IE tasks is Event Extraction (EE)\nwhich seeks to derive information about specific incidents and their actors\nfrom the text. EE is useful in many domains such as building a knowledge base,\ninformation retrieval, summarization and online monitoring systems. In the past\ndecades, some event ontologies like ACE, CAMEO and ICEWS were developed to\ndefine event forms, actors and dimensions of events observed in the text. These\nevent ontologies still have some shortcomings such as covering only a few\ntopics like political events, having inflexible structure in defining argument\nroles, lack of analytical dimensions, and complexity in choosing event\nsub-types. To address these concerns, we propose an event ontology, namely\nCOfEE, that incorporates both expert domain knowledge, previous ontologies and\na data-driven approach for identifying events from text. COfEE consists of two\nhierarchy levels (event types and event sub-types) that include new categories\nrelating to environmental issues, cyberspace, criminal activity and natural\ndisasters which need to be monitored instantly. Also, dynamic roles according\nto each event sub-type are defined to capture various dimensions of events. In\na follow-up experiment, the proposed ontology is evaluated on Wikipedia events,\nand it is shown to be general and comprehensive. Moreover, in order to\nfacilitate the preparation of gold-standard data for event extraction, a\nlanguage-independent online tool is presented based on COfEE.",
          "link": "http://arxiv.org/abs/2107.10326",
          "publishedOn": "2021-08-03T02:06:35.069Z",
          "wordCount": 733,
          "title": "COfEE: A Comprehensive Ontology for Event Extraction from text, with an online annotation tool. (arXiv:2107.10326v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.10572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianqian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1\">Walid Saad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1\">Mehdi Bennis</a>",
          "description": "In this paper, a novel framework is proposed to optimize the downlink\nmulti-user communication of a millimeter wave base station, which is assisted\nby a reconfigurable intelligent reflector (IR). In particular, a channel\nestimation approach is developed to measure the channel state information (CSI)\nin real-time. First, for a perfect CSI scenario, the precoding transmission of\nthe BS and the reflection coefficient of the IR are jointly optimized, via an\niterative approach, so as to maximize the sum of downlink rates towards\nmultiple users. Next, in the imperfect CSI scenario, a distributional\nreinforcement learning (DRL) approach is proposed to learn the optimal IR\nreflection and maximize the expectation of downlink capacity. In order to model\nthe transmission rate's probability distribution, a learning algorithm, based\non quantile regression (QR), is developed, and the proposed QR-DRL method is\nproved to converge to a stable distribution of downlink transmission rate.\nSimulation results show that, in the error-free CSI scenario, the proposed\napproach yields over 30% and 2-fold increase in the downlink sum-rate, compared\nwith a fixed IR reflection scheme and direct transmission scheme, respectively.\nSimulation results also show that by deploying more IR elements, the downlink\nsum-rate can be significantly improved. However, as the number of IR components\nincreases, more time is required for channel estimation, and the slope of\nincrease in the IR-aided transmission rate will become smaller. Furthermore,\nunder limited knowledge of CSI, simulation results show that the proposed\nQR-DRL method, which learns a full distribution of the downlink rate, yields a\nbetter prediction accuracy and improves the downlink rate by 10% for online\ndeployments, compared with a Q-learning baseline.",
          "link": "http://arxiv.org/abs/2002.10572",
          "publishedOn": "2021-08-03T02:06:35.062Z",
          "wordCount": 750,
          "title": "Millimeter Wave Communications with an Intelligent Reflector: Performance Optimization and Distributional Reinforcement Learning. (arXiv:2002.10572v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cossu_A/0/1/0/all/0/1\">Andrea Cossu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1\">Antonio Carta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1\">Vincenzo Lomonaco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>",
          "description": "Learning continuously during all model lifetime is fundamental to deploy\nmachine learning solutions robust to drifts in the data distribution. Advances\nin Continual Learning (CL) with recurrent neural networks could pave the way to\na large number of applications where incoming data is non stationary, like\nnatural language processing and robotics. However, the existing body of work on\nthe topic is still fragmented, with approaches which are application-specific\nand whose assessment is based on heterogeneous learning protocols and datasets.\nIn this paper, we organize the literature on CL for sequential data processing\nby providing a categorization of the contributions and a review of the\nbenchmarks. We propose two new benchmarks for CL with sequential data based on\nexisting datasets, whose characteristics resemble real-world applications. We\nalso provide a broad empirical evaluation of CL and Recurrent Neural Networks\nin class-incremental scenario, by testing their ability to mitigate forgetting\nwith a number of different strategies which are not specific to sequential data\nprocessing. Our results highlight the key role played by the sequence length\nand the importance of a clear specification of the CL scenario.",
          "link": "http://arxiv.org/abs/2103.07492",
          "publishedOn": "2021-08-03T02:06:35.055Z",
          "wordCount": 681,
          "title": "Continual Learning for Recurrent Neural Networks: an Empirical Evaluation. (arXiv:2103.07492v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_P/0/1/0/all/0/1\">Pablo G&#xf3;mez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meoni_G/0/1/0/all/0/1\">Gabriele Meoni</a>",
          "description": "Supervised learning techniques are at the center of many tasks in remote\nsensing. Unfortunately, these methods, especially recent deep learning methods,\noften require large amounts of labeled data for training. Even though\nsatellites acquire large amounts of data, labeling the data is often tedious,\nexpensive and requires expert knowledge. Hence, improved methods that require\nfewer labeled samples are needed. We present MSMatch, the first semi-supervised\nlearning approach competitive with supervised methods on scene classification\non the EuroSAT and UC Merced Land Use benchmark datasets. We test both RGB and\nmultispectral images of EuroSAT and perform various ablation studies to\nidentify the critical parts of the model. The trained neural network achieves\nstate-of-the-art results on EuroSAT with an accuracy that is up to 19.76%\nbetter than previous methods depending on the number of labeled training\nexamples. With just five labeled examples per class, we reach 94.53% and 95.86%\naccuracy on the EuroSAT RGB and multispectral datasets, respectively. On the UC\nMerced Land Use dataset, we outperform previous works by up to 5.59% and reach\n90.71% with five labeled examples. Our results show that MSMatch is capable of\ngreatly reducing the requirements for labeled data. It translates well to\nmultispectral data and should enable various applications that are currently\ninfeasible due to a lack of labeled data. We provide the source code of MSMatch\nonline to enable easy reproduction and quick adoption.",
          "link": "http://arxiv.org/abs/2103.10368",
          "publishedOn": "2021-08-03T02:06:35.048Z",
          "wordCount": 713,
          "title": "MSMatch: Semi-Supervised Multispectral Scene Classification with Few Labels. (arXiv:2103.10368v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.04695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitrevski_B/0/1/0/all/0/1\">Blagoj Mitrevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filipovic_M/0/1/0/all/0/1\">Milena Filipovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1\">Emma Lejal Glaude</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>",
          "description": "Multi-objective gradient methods are becoming the standard for solving\nmulti-objective problems. Among others, they show promising results in\ndeveloping multi-objective recommender systems with both correlated and\nconflicting objectives. Classic multi-gradient descent usually relies on the\ncombination of the gradients, not including the computation of first and second\nmoments of the gradients. This leads to a brittle behavior and misses important\nareas in the solution space. In this work, we create a multi-objective\nmodel-agnostic Adamize method that leverages the benefits of the Adam optimizer\nin single-objective problems. This corrects and stabilizes the gradients of\nevery objective before calculating a common gradient descent vector that\noptimizes all the objectives simultaneously. We evaluate the benefits of\nmulti-objective Adamize on two multi-objective recommender systems and for\nthree different objective combinations, both correlated or conflicting. We\nreport significant improvements, measured with three different Pareto front\nmetrics: hypervolume, coverage, and spacing. Finally, we show that the Adamized\nPareto front strictly dominates the previous one on multiple objective pairs.",
          "link": "http://arxiv.org/abs/2009.04695",
          "publishedOn": "2021-08-03T02:06:35.040Z",
          "wordCount": 643,
          "title": "Momentum-based Gradient Methods in Multi-Objective Recommendation. (arXiv:2009.04695v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.",
          "link": "http://arxiv.org/abs/2107.08264",
          "publishedOn": "2021-08-03T02:06:35.025Z",
          "wordCount": 730,
          "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caro_M/0/1/0/all/0/1\">Matthias C. Caro</a>",
          "description": "Machine learning researchers and practitioners steadily enlarge the multitude\nof successful learning models. They achieve this through in-depth theoretical\nanalyses and experiential heuristics. However, there is no known\ngeneral-purpose procedure for rigorously evaluating whether newly proposed\nmodels indeed successfully learn from data. We show that such a procedure\ncannot exist. For PAC binary classification, uniform and universal online\nlearning, and exact learning through teacher-learner interactions, learnability\nis in general undecidable, both in the sense of independence of the axioms in a\nformal system and in the sense of uncomputability. Our proofs proceed via\ncomputable constructions of function classes that encode the consistency\nproblem for formal systems and the halting problem for Turing machines into\ncomplexity measures that characterize learnability. Our work shows that\nundecidability appears in the theoretical foundations of machine learning:\nThere is no one-size-fits-all algorithm for deciding whether a machine learning\nmodel can be successful. We cannot in general automatize the process of\nassessing new learning models.",
          "link": "http://arxiv.org/abs/2106.01382",
          "publishedOn": "2021-08-03T02:06:35.018Z",
          "wordCount": 641,
          "title": "Undecidability of Learnability. (arXiv:2106.01382v2 [cs.CC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.09335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goh_S/0/1/0/all/0/1\">Sim Kuan Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Narendra Pratap Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_Z/0/1/0/all/0/1\">Zhi Jun Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1\">Sameer Alam</a>",
          "description": "Approach and landing accidents have resulted in a significant number of hull\nlosses worldwide. Technologies (e.g., instrument landing system) and procedures\n(e.g., stabilized approach criteria) have been developed to reduce the risks.\nIn this paper, we propose a data-driven method to learn and interpret flight's\napproach and landing parameters to facilitate comprehensible and actionable\ninsights into flight dynamics. Specifically, we develop two variants of tunnel\nGaussian process (TGP) models to elucidate aircraft's approach and landing\ndynamics using advanced surface movement guidance and control system (A-SMGCS)\ndata, which then indicates the stability of flight. TGP hybridizes the\nstrengths of sparse variational Gaussian process and polar Gaussian process to\nlearn from a large amount of data in cylindrical coordinates. We examine TGP\nqualitatively and quantitatively by synthesizing three complex trajectory\ndatasets and compared TGP against existing methods on trajectory learning.\nEmpirically, TGP demonstrates superior modeling performance. When applied to\noperational A-SMGCS data, TGP provides the generative probabilistic description\nof landing dynamics and interpretable tunnel views of approach and landing\nparameters. These probabilistic tunnel models can facilitate the analysis of\nprocedure adherence and augment existing aircrew and air traffic controllers'\ndisplays during the approach and landing procedures, enabling necessary\ncorrective actions.",
          "link": "http://arxiv.org/abs/2011.09335",
          "publishedOn": "2021-08-03T02:06:35.012Z",
          "wordCount": 683,
          "title": "A Tunnel Gaussian Process Model for Learning Interpretable Flight's Landing Parameters. (arXiv:2011.09335v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00735",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Swijsen_L/0/1/0/all/0/1\">Lars Swijsen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Veken_J/0/1/0/all/0/1\">Joeri Van der Veken</a>, <a href=\"http://arxiv.org/find/math/1/au:+Vannieuwenhoven_N/0/1/0/all/0/1\">Nick Vannieuwenhoven</a>",
          "description": "We propose a Riemannian conjugate gradient (CG) optimization method for\nfinding low rank approximations of incomplete tensors. Our main contribution\nconsists of an explicit expression of the geodesics on the Segre manifold.\nThese are exploited in our algorithm to perform the retractions. We apply our\nmethod to movie rating predictions in a recommender system for the MovieLens\ndataset, and identification of pure fluorophores via fluorescent spectroscopy\nwith missing data. In this last application, we recover the tensor\ndecomposition from less than $10\\%$ of the data.",
          "link": "http://arxiv.org/abs/2108.00735",
          "publishedOn": "2021-08-03T02:06:35.005Z",
          "wordCount": 530,
          "title": "Tensor completion using geodesics on Segre manifolds. (arXiv:2108.00735v1 [math.DG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.05266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valerio_L/0/1/0/all/0/1\">Lorenzo Valerio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passarella_A/0/1/0/all/0/1\">Andrea Passarella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Marco Conti</a>",
          "description": "The exponential growth of devices and data at the edges of the Internet is\nrising scalability and privacy concerns on approaches based exclusively on\nremote cloud platforms. Data gravity, a fundamental concept in Fog Computing,\npoints towards decentralisation of computation for data analysis, as a viable\nalternative to address those concerns. Decentralising AI tasks on several\ncooperative devices means identifying the optimal set of locations or\nCollection Points (CP for short) to use, in the continuum between full\ncentralisation (i.e., all data on a single device) and full decentralisation\n(i.e., data on source locations). We propose an analytical framework able to\nfind the optimal operating point in this continuum, linking the accuracy of the\nlearning task with the corresponding network and computational cost for moving\ndata and running the distributed training at the CPs. We show through\nsimulations that the model accurately predicts the optimal trade-off, quite\noften an intermediate point between full centralisation and full\ndecentralisation, showing also a significant cost saving w.r.t. both of them.\nFinally, the analytical model admits closed-form or numeric solutions, making\nit not only a performance evaluation instrument but also a design tool to\nconfigure a given distributed learning task optimally before its deployment.",
          "link": "http://arxiv.org/abs/2012.05266",
          "publishedOn": "2021-08-03T02:06:34.987Z",
          "wordCount": 696,
          "title": "Optimising cost vs accuracy of decentralised analytics in fog computing environments. (arXiv:2012.05266v3 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_I/0/1/0/all/0/1\">Ivan D. Jimenez Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosolia_U/0/1/0/all/0/1\">Ugo Rosolia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ames_A/0/1/0/all/0/1\">Aaron D. Ames</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yisong Yue</a>",
          "description": "We present a straightforward and efficient way to control unstable robotic\nsystems using an estimated dynamics model. Specifically, we show how to exploit\nthe differentiability of Gaussian Processes to create a state-dependent\nlinearized approximation of the true continuous dynamics that can be integrated\nwith model predictive control. Our approach is compatible with most Gaussian\nprocess approaches for system identification, and can learn an accurate model\nusing modest amounts of training data. We validate our approach by learning the\ndynamics of an unstable system such as a segway with a 7-D state space and 2-D\ninput space (using only one minute of data), and we show that the resulting\ncontroller is robust to unmodelled dynamics and disturbances, while\nstate-of-the-art control methods based on nominal models can fail under small\nperturbations. Code is open sourced at\nhttps://github.com/learning-and-control/core .",
          "link": "http://arxiv.org/abs/2103.04548",
          "publishedOn": "2021-08-03T02:06:34.981Z",
          "wordCount": 631,
          "title": "Learning to Control an Unstable System with One Minute of Data: Leveraging Gaussian Process Differentiation in Predictive Control. (arXiv:2103.04548v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianqian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferdowsi_A/0/1/0/all/0/1\">Aidin Ferdowsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1\">Walid Saad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1\">Mehdi Bennis</a>",
          "description": "In this paper, a novel framework is proposed to perform data-driven\nair-to-ground (A2G) channel estimation for millimeter wave (mmWave)\ncommunications in an unmanned aerial vehicle (UAV) wireless network. First, an\neffective channel estimation approach is developed to collect mmWave channel\ninformation, allowing each UAV to train a stand-alone channel model via a\nconditional generative adversarial network (CGAN) along each beamforming\ndirection. Next, in order to expand the application scenarios of the trained\nchannel model into a broader spatial-temporal domain, a cooperative framework,\nbased on a distributed CGAN architecture, is developed, allowing each UAV to\ncollaboratively learn the mmWave channel distribution in a fully-distributed\nmanner. To guarantee an efficient learning process, necessary and sufficient\nconditions for the optimal UAV network topology that maximizes the learning\nrate for cooperative channel modeling are derived, and the optimal CGAN\nlearning solution per UAV is subsequently characterized, based on the\ndistributed network structure. Simulation results show that the proposed\ndistributed CGAN approach is robust to the local training error at each UAV.\nMeanwhile, a larger airborne network size requires more communication resources\nper UAV to guarantee an efficient learning rate. The results also show that,\ncompared with a stand-alone CGAN without information sharing and two other\ndistributed schemes, namely: A multi-discriminator CGAN and a federated CGAN\nmethod, the proposed distributed CGAN approach yields a higher modeling\naccuracy while learning the environment, and it achieves a larger average data\nrate in the online performance of UAV downlink mmWave communications.",
          "link": "http://arxiv.org/abs/2102.01751",
          "publishedOn": "2021-08-03T02:06:34.976Z",
          "wordCount": 720,
          "title": "Distributed Conditional Generative Adversarial Networks (GANs) for Data-Driven Millimeter Wave Communications in UAV Networks. (arXiv:2102.01751v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zakovskis_R/0/1/0/all/0/1\">Ronalds Zakovskis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Draguns_A/0/1/0/all/0/1\">Andis Draguns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaile_E/0/1/0/all/0/1\">Eliza Gaile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozolins_E/0/1/0/all/0/1\">Emils Ozolins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freivalds_K/0/1/0/all/0/1\">Karlis Freivalds</a>",
          "description": "Recurrent neural networks have flourished in many areas. Consequently, we can\nsee new RNN cells being developed continuously, usually by creating or using\ngates in a new, original way. But what if we told you that gates in RNNs are\nredundant? In this paper, we propose a new recurrent cell called Residual\nRecurrent Unit (RRU) which beats traditional cells and does not employ a single\ngate. It is based on the residual shortcut connection together with linear\ntransformations, ReLU, and normalization. To evaluate our cell's effectiveness,\nwe compare its performance against the widely-used GRU and LSTM cells and the\nrecently proposed Mogrifier LSTM on several tasks including, polyphonic music\nmodeling, language modeling, and sentiment analysis. Our experiments show that\nRRU outperforms the traditional gated units on most of these tasks. Also, it\nhas better robustness to parameter selection, allowing immediate application in\nnew tasks without much tuning. We have implemented the RRU in TensorFlow, and\nthe code is made available at https://github.com/LUMII-Syslab/RRU .",
          "link": "http://arxiv.org/abs/2108.00527",
          "publishedOn": "2021-08-03T02:06:34.968Z",
          "wordCount": 602,
          "title": "Gates are not what you need in RNNs. (arXiv:2108.00527v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01333",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1\">Shuyan Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Spirtes_P/0/1/0/all/0/1\">Peter Spirtes</a>",
          "description": "Kalisch and B\\\"{u}hlmann (2007) showed that for linear Gaussian models, under\nthe Causal Markov Assumption, the Strong Causal Faithfulness Assumption, and\nthe assumption of causal sufficiency, the PC algorithm is a uniformly\nconsistent estimator of the Markov Equivalence Class of the true causal DAG for\nlinear Gaussian models; it follows from this that for the identifiable causal\neffects in the Markov Equivalence Class, there are uniformly consistent\nestimators of causal effects as well. The $k$-Triangle-Faithfulness Assumption\nis a strictly weaker assumption that avoids some implausible implications of\nthe Strong Causal Faithfulness Assumption and also allows for uniformly\nconsistent estimates of Markov Equivalence Classes (in a weakened sense), and\nof identifiable causal effects. However, both of these assumptions are\nrestricted to linear Gaussian models. We propose the Generalized $k$-Triangle\nFaithfulness, which can be applied to any smooth distribution. In addition,\nunder the Generalized $k$-Triangle Faithfulness Assumption, we describe the\nEdge Estimation Algorithm that provides uniformly consistent estimates of\ncausal effects in some cases (and otherwise outputs \"can't tell\"), and the\n\\textit{Very Conservative }$SGS$ Algorithm that (in a slightly weaker sense) is\na uniformly consistent estimator of the Markov equivalence class of the true\nDAG.",
          "link": "http://arxiv.org/abs/2107.01333",
          "publishedOn": "2021-08-03T02:06:34.962Z",
          "wordCount": 651,
          "title": "A Uniformly Consistent Estimator of non-Gaussian Causal Effects Under the k-Triangle-Faithfulness Assumption. (arXiv:2107.01333v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.04026",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wilson_J/0/1/0/all/0/1\">James T. Wilson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Borovitskiy_V/0/1/0/all/0/1\">Viacheslav Borovitskiy</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Terenin_A/0/1/0/all/0/1\">Alexander Terenin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mostowsky_P/0/1/0/all/0/1\">Peter Mostowsky</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Deisenroth_M/0/1/0/all/0/1\">Marc Peter Deisenroth</a>",
          "description": "As Gaussian processes are used to answer increasingly complex questions,\nanalytic solutions become scarcer and scarcer. Monte Carlo methods act as a\nconvenient bridge for connecting intractable mathematical expressions with\nactionable estimates via sampling. Conventional approaches for simulating\nGaussian process posteriors view samples as draws from marginal distributions\nof process values at finite sets of input locations. This distribution-centric\ncharacterization leads to generative strategies that scale cubically in the\nsize of the desired random vector. These methods are prohibitively expensive in\ncases where we would, ideally, like to draw high-dimensional vectors or even\ncontinuous sample paths. In this work, we investigate a different line of\nreasoning: rather than focusing on distributions, we articulate Gaussian\nconditionals at the level of random variables. We show how this pathwise\ninterpretation of conditioning gives rise to a general family of approximations\nthat lend themselves to efficiently sampling Gaussian process posteriors.\nStarting from first principles, we derive these methods and analyze the\napproximation errors they introduce. We, then, ground these results by\nexploring the practical implications of pathwise conditioning in various\napplied settings, such as global optimization and reinforcement learning.",
          "link": "http://arxiv.org/abs/2011.04026",
          "publishedOn": "2021-08-03T02:06:34.956Z",
          "wordCount": 657,
          "title": "Pathwise Conditioning of Gaussian Processes. (arXiv:2011.04026v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeevan_P/0/1/0/all/0/1\">Pranav Jeevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1\">Amit Sethi</a> (Indian Institute of Technology Bombay)",
          "description": "We propose three improvements to vision transformers (ViT) to reduce the\nnumber of trainable parameters without compromising classification accuracy. We\naddress two shortcomings of the early ViT architectures -- quadratic bottleneck\nof the attention mechanism and the lack of an inductive bias in their\narchitectures that rely on unrolling the two-dimensional image structure.\nLinear attention mechanisms overcome the bottleneck of quadratic complexity,\nwhich restricts application of transformer models in vision tasks. We modify\nthe ViT architecture to work on longer sequence data by replacing the quadratic\nattention with efficient transformers, such as Performer, Linformer and\nNystr\\\"omformer of linear complexity creating Vision X-formers (ViX). We show\nthat all three versions of ViX may be more accurate than ViT for image\nclassification while using far fewer parameters and computational resources. We\nalso compare their performance with FNet and multi-layer perceptron (MLP)\nmixer. We further show that replacing the initial linear embedding layer by\nconvolutional layers in ViX further increases their performance. Furthermore,\nour tests on recent vision transformer models, such as LeViT, Convolutional\nvision Transformer (CvT), Compact Convolutional Transformer (CCT) and\nPooling-based Vision Transformer (PiT) show that replacing the attention with\nNystr\\\"omformer or Performer saves GPU usage and memory without deteriorating\nthe classification accuracy. We also show that replacing the standard learnable\n1D position embeddings in ViT with Rotary Position Embedding (RoPE) give\nfurther improvements in accuracy. Incorporating these changes can democratize\ntransformers by making them accessible to those with limited data and computing\nresources.",
          "link": "http://arxiv.org/abs/2107.02239",
          "publishedOn": "2021-08-03T02:06:34.939Z",
          "wordCount": 733,
          "title": "Vision Xformers: Efficient Attention for Image Classification. (arXiv:2107.02239v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10845",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_H/0/1/0/all/0/1\">Hanrui Wang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ding_Y/0/1/0/all/0/1\">Yongshan Ding</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Gu_J/0/1/0/all/0/1\">Jiaqi Gu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lin_Y/0/1/0/all/0/1\">Yujun Lin</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Pan_D/0/1/0/all/0/1\">David Z. Pan</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chong_F/0/1/0/all/0/1\">Frederic T. Chong</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>",
          "description": "Quantum noise is the key challenge in Noisy Intermediate-Scale Quantum (NISQ)\ncomputers. Previous work for mitigating noise has primarily focused on\ngate-level or pulse-level noise-adaptive compilation. However, limited research\nefforts have explored a higher level of optimization by making the quantum\ncircuits themselves resilient to noise.\n\nWe propose QuantumNAS, a comprehensive framework for noise-adaptive co-search\nof the variational circuit and qubit mapping. Variational quantum circuits are\na promising approach for constructing QML and quantum simulation. However,\nfinding the best variational circuit and its optimal parameters is challenging\ndue to the large design space and parameter training cost. We propose to\ndecouple the circuit search and parameter training by introducing a novel\nSuperCircuit. The SuperCircuit is constructed with multiple layers of\npre-defined parameterized gates and trained by iteratively sampling and\nupdating the parameter subsets (SubCircuits) of it. It provides an accurate\nestimation of SubCircuits performance trained from scratch. Then we perform an\nevolutionary co-search of SubCircuit and its qubit mapping. The SubCircuit\nperformance is estimated with parameters inherited from SuperCircuit and\nsimulated with real device noise models. Finally, we perform iterative gate\npruning and finetuning to remove redundant gates.\n\nExtensively evaluated with 12 QML and VQE benchmarks on 10 quantum comput,\nQuantumNAS significantly outperforms baselines. For QML, QuantumNAS is the\nfirst to demonstrate over 95% 2-class, 85% 4-class, and 32% 10-class\nclassification accuracy on real QC. It also achieves the lowest eigenvalue for\nVQE tasks on H2, H2O, LiH, CH4, BeH2 compared with UCCSD. We also open-source\nQuantumEngine (https://github.com/mit-han-lab/pytorch-quantum) for fast\ntraining of parameterized quantum circuits to facilitate future research.",
          "link": "http://arxiv.org/abs/2107.10845",
          "publishedOn": "2021-08-03T02:06:34.932Z",
          "wordCount": 731,
          "title": "QuantumNAS: Noise-Adaptive Search for Robust Quantum Circuits. (arXiv:2107.10845v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manapragada_C/0/1/0/all/0/1\">Chaitanya Manapragada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_H/0/1/0/all/0/1\">Heitor M Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mahsa Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bifet_A/0/1/0/all/0/1\">Albert Bifet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1\">Geoffrey I Webb</a>",
          "description": "Decision tree ensembles are widely used in practice. In this work, we study\nin ensemble settings the effectiveness of replacing the split strategy for the\nstate-of-the-art online tree learner, Hoeffding Tree, with a rigorous but more\neager splitting strategy that we had previously published as Hoeffding AnyTime\nTree. Hoeffding AnyTime Tree (HATT), uses the Hoeffding Test to determine\nwhether the current best candidate split is superior to the current split, with\nthe possibility of revision, while Hoeffding Tree aims to determine whether the\ntop candidate is better than the second best and if a test is selected, fixes\nit for all posterity. HATT converges to the ideal batch tree while Hoeffding\nTree does not. We find that HATT is an efficacious base learner for online\nbagging and online boosting ensembles. On UCI and synthetic streams, HATT as a\nbase learner outperforms HT within a 0.05 significance level for the majority\nof tested ensembles on what we believe is the largest and most comprehensive\nset of testbenches in the online learning literature. Our results indicate that\nHATT is a superior alternative to Hoeffding Tree in a large number of ensemble\nsettings.",
          "link": "http://arxiv.org/abs/2010.10935",
          "publishedOn": "2021-08-03T02:06:34.925Z",
          "wordCount": 667,
          "title": "An Eager Splitting Strategy for Online Decision Trees. (arXiv:2010.10935v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_R/0/1/0/all/0/1\">Rachit Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiwelekar_A/0/1/0/all/0/1\">Arvind W Kiwelekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netak_L/0/1/0/all/0/1\">Laxman D Netak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodake_A/0/1/0/all/0/1\">Akshay Ghodake</a>",
          "description": "Although most logistics and freight forwarding organizations, in one way or\nanother, claim to have core values. The engagement of employees is a vast\nstructure that affects almost every part of the company's core environmental\nvalues. There is little theoretical knowledge about the relationship between\nfirms and the engagement of employees. Based on research literature, this paper\naims to provide a novel approach for insight around employee engagement in a\nlogistics organization by implementing deep natural language processing\nconcepts. The artificial intelligence-enabled solution named Intelligent Pulse\n(I-Pulse) can evaluate hundreds and thousands of pulse survey comments and\nprovides the actionable insights and gist of employee feedback. I-Pulse allows\nthe stakeholders to think in new ways in their organization, helping them to\nhave a powerful influence on employee engagement, retention, and efficiency.\nThis study is of corresponding interest to researchers and practitioners.",
          "link": "http://arxiv.org/abs/2106.07341",
          "publishedOn": "2021-08-03T02:06:34.897Z",
          "wordCount": null,
          "title": "i-Pulse: A NLP based novel approach for employee engagement in logistics organization. (arXiv:2106.07341v1 [cs.SI] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pattanayak_K/0/1/0/all/0/1\">Kunal Pattanayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_V/0/1/0/all/0/1\">Vikram Krishnamurthy</a>",
          "description": "Are deep convolutional neural networks (CNNs) for image classification\nexplainable by utility maximization with information acquisition costs? We\ndemonstrate that deep CNNs behave equivalently (in terms of necessary and\nsufficient conditions) to rationally inattentive utility maximizers, a\ngenerative model used extensively in economics for human decision making. Our\nclaim is based by extensive experiments on 200 deep CNNs from 5 popular\narchitectures. The parameters of our interpretable model are computed\nefficiently via convex feasibility algorithms. As an application, we show that\nour economics-based interpretable model can predict the classification\nperformance of deep CNNs trained with arbitrary parameters with accuracy\nexceeding 94% . This eliminates the need to re-train the deep CNNs for image\nclassification. The theoretical foundation of our approach lies in Bayesian\nrevealed preference studied in micro-economics. All our results are on GitHub\nand completely reproducible.",
          "link": "http://arxiv.org/abs/2102.04594",
          "publishedOn": "2021-08-03T02:06:34.896Z",
          "wordCount": null,
          "title": "Rationally Inattentive Utility Maximization for Interpretable Deep Image Classification. (arXiv:2102.04594v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junda Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xupin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "While the long-term effects of COVID-19 are yet to be determined, its\nimmediate impact on crowdfunding is nonetheless significant. This study takes a\ncomputational approach to more deeply comprehend this change. Using a unique\ndata set of all the campaigns published over the past two years on GoFundMe, we\nexplore the factors that have led to the successful funding of a crowdfunding\nproject. In particular, we study a corpus of crowdfunded projects, analyzing\ncover images and other variables commonly present on crowdfunding sites.\nFurthermore, we construct a classifier and a regression model to assess the\nsignificance of features based on XGBoost. In addition, we employ\ncounterfactual analysis to investigate the causality between features and the\nsuccess of crowdfunding. More importantly, sentiment analysis and the paired\nsample t-test are performed to examine the differences in crowdfunding\ncampaigns before and after the COVID-19 outbreak that started in March 2020.\nFirst, we note that there is significant racial disparity in crowdfunding\nsuccess. Second, we find that sad emotion expressed through the campaign's\ndescription became significant after the COVID-19 outbreak. Considering all\nthese factors, our findings shed light on the impact of COVID-19 on\ncrowdfunding campaigns.",
          "link": "http://arxiv.org/abs/2106.09981",
          "publishedOn": "2021-08-03T02:06:34.896Z",
          "wordCount": null,
          "title": "How COVID-19 Has Changed Crowdfunding: Evidence From GoFundMe. (arXiv:2106.09981v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Awasthi_P/0/1/0/all/0/1\">Pranjal Awasthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_A/0/1/0/all/0/1\">Alex Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijayaraghavan_A/0/1/0/all/0/1\">Aravindan Vijayaraghavan</a>",
          "description": "We present polynomial time and sample efficient algorithms for learning an\nunknown depth-2 feedforward neural network with general ReLU activations, under\nmild non-degeneracy assumptions. In particular, we consider learning an unknown\nnetwork of the form $f(x) = {a}^{\\mathsf{T}}\\sigma({W}^\\mathsf{T}x+b)$, where\n$x$ is drawn from the Gaussian distribution, and $\\sigma(t) := \\max(t,0)$ is\nthe ReLU activation. Prior works for learning networks with ReLU activations\nassume that the bias $b$ is zero. In order to deal with the presence of the\nbias terms, our proposed algorithm consists of robustly decomposing multiple\nhigher order tensors arising from the Hermite expansion of the function $f(x)$.\nUsing these ideas we also establish identifiability of the network parameters\nunder minimal assumptions.",
          "link": "http://arxiv.org/abs/2107.10209",
          "publishedOn": "2021-08-03T02:06:34.896Z",
          "wordCount": null,
          "title": "Efficient Algorithms for Learning Depth-2 Neural Networks with General ReLU Activations. (arXiv:2107.10209v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xixi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yun Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yanfei Kang</a>",
          "description": "One of the most significant differences of M5 over previous forecasting\ncompetitions is that it was held on Kaggle, an online platform of data\nscientists and machine learning practitioners. Kaggle provides a gathering\nplace, or virtual community, for web users who are interested in the M5\ncompetition. Users can share code, models, features, loss functions, etc.\nthrough online notebooks and discussion forums. This paper aims to study the\nsocial influence of virtual community on user behaviors in the M5 competition.\nWe first research the content of the M5 virtual community by topic modeling and\ntrend analysis. Further, we perform social media analysis to identify the\npotential relationship network of the virtual community. We study the roles and\ncharacteristics of some key participants that promote the diffusion of\ninformation within the M5 virtual community. Overall, this study provides\nin-depth insights into the mechanism of the virtual community's influence on\nthe participants and has potential implications for future online competitions.",
          "link": "http://arxiv.org/abs/2103.00501",
          "publishedOn": "2021-08-03T02:06:34.895Z",
          "wordCount": null,
          "title": "Exploring the social influence of Kaggle virtual community on the M5 competition. (arXiv:2103.00501v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06295",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ramli_A/0/1/0/all/0/1\">Albara Ah Ramli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Huanle Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1\">Jiahui Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_R/0/1/0/all/0/1\">Rex Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nicorici_A/0/1/0/all/0/1\">Alina Nicorici</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aranki_D/0/1/0/all/0/1\">Daniel Aranki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Owens_C/0/1/0/all/0/1\">Corey Owens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasad_P/0/1/0/all/0/1\">Poonam Prasad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McDonald_C/0/1/0/all/0/1\">Craig McDonald</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Henricson_E/0/1/0/all/0/1\">Erik Henricson</a>",
          "description": "Differences in gait patterns of children with Duchenne muscular dystrophy\n(DMD) and typically developing (TD) peers are visible to the eye, but\nquantification of those differences outside of the gait laboratory has been\nelusive. We measured vertical, mediolateral, and anteroposterior acceleration\nusing a waist-worn iPhone accelerometer during ambulation across a typical\nrange of velocities. Six TD and six DMD children from 3-15 years of age\nunderwent seven walking/running tasks, including five 25m walk/run tests at a\nslow walk to running speeds, a 6-minute walk test (6MWT), and a\n100-meter-run/walk (100MRW). We extracted temporospatial clinical gait features\n(CFs) and applied multiple Artificial Intelligence (AI) tools to differentiate\nbetween DMD and TD control children using extracted features and raw data.\nExtracted CFs showed reduced step length and a greater mediolateral component\nof total power (TP) consistent with shorter strides and Trendelenberg-like gait\ncommonly observed in DMD. AI methods using CFs and raw data varied\nineffectiveness at differentiating between DMD and TD controls at different\nspeeds, with an accuracy of some methods exceeding 91%. We demonstrate that by\nusing AI tools with accelerometer data from a consumer-level smartphone, we can\nidentify DMD gait disturbance in toddlers to early teens.",
          "link": "http://arxiv.org/abs/2105.06295",
          "publishedOn": "2021-08-03T02:06:34.895Z",
          "wordCount": null,
          "title": "Gait Characterization in Duchenne Muscular Dystrophy (DMD) Using a Single-Sensor Accelerometer: Classical Machine Learning and Deep Learning Approaches. (arXiv:2105.06295v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02521",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kunar_A/0/1/0/all/0/1\">Aditya Kunar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birke_R/0/1/0/all/0/1\">Robert Birke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zilong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lydia Chen</a>",
          "description": "Tabular generative adversarial networks (TGAN) have recently emerged to cater\nto the need of synthesizing tabular data -- the most widely used data format.\nWhile synthetic tabular data offers the advantage of complying with privacy\nregulations, there still exists a risk of privacy leakage via inference attacks\ndue to interpolating the properties of real data during training. Differential\nprivate (DP) training algorithms provide theoretical guarantees for training\nmachine learning models by injecting statistical noise to prevent privacy\nleaks. However, the challenges of applying DP on TGAN are to determine the most\noptimal framework (i.e., PATE/DP-SGD) and neural network (i.e.,\nGenerator/Discriminator)to inject noise such that the data utility is well\nmaintained under a given privacy guarantee. In this paper, we propose DTGAN, a\nnovel conditional Wasserstein tabular GAN that comes in two variants DTGAN_G\nand DTGAN_D, for providing a detailed comparison of tabular GANs trained using\nDP-SGD for the generator vs discriminator, respectively. We elicit the privacy\nanalysis associated with training the generator with complex loss functions\n(i.e., classification and information losses) needed for high quality tabular\ndata synthesis. Additionally, we rigorously evaluate the theoretical privacy\nguarantees offered by DP empirically against membership and attribute inference\nattacks. Our results on 3 datasets show that the DP-SGD framework is superior\nto PATE and that a DP discriminator is more optimal for training convergence.\nThus, we find (i) DTGAN_D is capable of maintaining the highest data utility\nacross 4 ML models by up to 18% in terms of the average precision score for a\nstrict privacy budget, epsilon = 1, as compared to the prior studies and (ii)\nDP effectively prevents privacy loss against inference attacks by restricting\nthe success probability of membership attacks to be close to 50%.",
          "link": "http://arxiv.org/abs/2107.02521",
          "publishedOn": "2021-08-03T02:06:34.895Z",
          "wordCount": null,
          "title": "DTGAN: Differential Private Training for Tabular GANs. (arXiv:2107.02521v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weijia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_L/0/1/0/all/0/1\">Lijun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hengshu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>",
          "description": "Real estate appraisal refers to the process of developing an unbiased opinion\nfor real property's market value, which plays a vital role in decision-making\nfor various players in the marketplace (e.g., real estate agents, appraisers,\nlenders, and buyers). However, it is a nontrivial task for accurate real estate\nappraisal because of three major challenges: (1) The complicated influencing\nfactors for property value; (2) The asynchronously spatiotemporal dependencies\namong real estate transactions; (3) The diversified correlations between\nresidential communities. To this end, we propose a Multi-Task Hierarchical\nGraph Representation Learning (MugRep) framework for accurate real estate\nappraisal. Specifically, by acquiring and integrating multi-source urban data,\nwe first construct a rich feature set to comprehensively profile the real\nestate from multiple perspectives (e.g., geographical distribution, human\nmobility distribution, and resident demographics distribution). Then, an\nevolving real estate transaction graph and a corresponding event graph\nconvolution module are proposed to incorporate asynchronously spatiotemporal\ndependencies among real estate transactions. Moreover, to further incorporate\nvaluable knowledge from the view of residential communities, we devise a\nhierarchical heterogeneous community graph convolution module to capture\ndiversified correlations between residential communities. Finally, an urban\ndistrict partitioned multi-task learning module is introduced to generate\ndifferently distributed value opinions for real estate. Extensive experiments\non two real-world datasets demonstrate the effectiveness of MugRep and its\ncomponents and features.",
          "link": "http://arxiv.org/abs/2107.05180",
          "publishedOn": "2021-08-03T02:06:34.895Z",
          "wordCount": null,
          "title": "MugRep: A Multi-Task Hierarchical Graph Representation Learning Framework for Real Estate Appraisal. (arXiv:2107.05180v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suarez_G/0/1/0/all/0/1\">Gabriel Suarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raful_J/0/1/0/all/0/1\">Juan Raful</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luque_M/0/1/0/all/0/1\">Maria A. Luque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valencia_C/0/1/0/all/0/1\">Carlos F. Valencia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correa_Bahnsen_A/0/1/0/all/0/1\">Alejandro Correa-Bahnsen</a>",
          "description": "This paper presents the advantages of alternative data from Super-Apps to\nenhance user' s income estimation models. It compares the performance of these\nalternative data sources with the performance of industry-accepted bureau\nincome estimators that takes into account only financial system information;\nsuccessfully showing that the alternative data manage to capture information\nthat bureau income estimators do not. By implementing the TreeSHAP method for\nStochastic Gradient Boosting Interpretation, this paper highlights which of the\ncustomer' s behavioral and transactional patterns within a Super-App have a\nstronger predictive power when estimating user' s income. Ultimately, this\npaper shows the incentive for financial institutions to seek to incorporate\nalternative data into constructing their risk profiles.",
          "link": "http://arxiv.org/abs/2104.05831",
          "publishedOn": "2021-08-03T02:06:34.894Z",
          "wordCount": null,
          "title": "Enhancing User' s Income Estimation with Super-App Alternative Data. (arXiv:2104.05831v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Assran_M/0/1/0/all/0/1\">Mahmoud Assran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1\">Mathilde Caron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1\">Piotr Bojanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1\">Nicolas Ballas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1\">Michael Rabbat</a>",
          "description": "This paper proposes a novel method of learning by predicting view assignments\nwith support samples (PAWS). The method trains a model to minimize a\nconsistency loss, which ensures that different views of the same unlabeled\ninstance are assigned similar pseudo-labels. The pseudo-labels are generated\nnon-parametrically, by comparing the representations of the image views to\nthose of a set of randomly sampled labeled images. The distance between the\nview representations and labeled representations is used to provide a weighting\nover class labels, which we interpret as a soft pseudo-label. By\nnon-parametrically incorporating labeled samples in this way, PAWS extends the\ndistance-metric loss used in self-supervised methods such as BYOL and SwAV to\nthe semi-supervised setting. Despite the simplicity of the approach, PAWS\noutperforms other semi-supervised methods across architectures, setting a new\nstate-of-the-art for a ResNet-50 on ImageNet trained with either 10% or 1% of\nthe labels, reaching 75.5% and 66.5% top-1 respectively. PAWS requires 4x to\n12x less training than the previous best methods.",
          "link": "http://arxiv.org/abs/2104.13963",
          "publishedOn": "2021-08-03T02:06:34.894Z",
          "wordCount": null,
          "title": "Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples. (arXiv:2104.13963v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1\">H. Vincent Poor</a>",
          "description": "Owing to the low communication costs and privacy-promoting capabilities,\nFederated Learning (FL) has become a promising tool for training effective\nmachine learning models among distributed clients. However, with the\ndistributed architecture, low quality models could be uploaded to the\naggregator server by unreliable clients, leading to a degradation or even a\ncollapse of training. In this paper, we model these unreliable behaviors of\nclients and propose a defensive mechanism to mitigate such a security risk.\nSpecifically, we first investigate the impact on the models caused by\nunreliable clients by deriving a convergence upper bound on the loss function\nbased on the gradient descent updates. Our theoretical bounds reveal that with\na fixed amount of total computational resources, there exists an optimal number\nof local training iterations in terms of convergence performance. We further\ndesign a novel defensive mechanism, named deep neural network based secure\naggregation (DeepSA). Our experimental results validate our theoretical\nanalysis. In addition, the effectiveness of DeepSA is verified by comparing\nwith other state-of-the-art defensive mechanisms.",
          "link": "http://arxiv.org/abs/2105.06256",
          "publishedOn": "2021-08-03T02:06:34.892Z",
          "wordCount": 645,
          "title": "Federated Learning with Unreliable Clients: Performance Analysis and Mechanism Design. (arXiv:2105.06256v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunguz_B/0/1/0/all/0/1\">Bojan Tunguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titericz_G/0/1/0/all/0/1\">Gilberto Titericz</a>",
          "description": "Black-box optimization is essential for tuning complex machine learning\nalgorithms which are easier to experiment with than to understand. In this\npaper, we show that a simple ensemble of black-box optimization algorithms can\noutperform any single one of them. However, searching for such an optimal\nensemble requires a large number of experiments. We propose a\nMulti-GPU-optimized framework to accelerate a brute force search for the\noptimal ensemble of black-box optimization algorithms by running many\nexperiments in parallel. The lightweight optimizations are performed by CPU\nwhile expensive model training and evaluations are assigned to GPUs. We\nevaluate 15 optimizers by training 2.7 million models and running 541,440\noptimizations. On a DGX-1, the search time is reduced from more than 10 days on\ntwo 20-core CPUs to less than 24 hours on 8-GPUs. With the optimal ensemble\nfound by GPU-accelerated exhaustive search, we won the 2nd place of NeurIPS\n2020 black-box optimization challenge.",
          "link": "http://arxiv.org/abs/2012.04201",
          "publishedOn": "2021-08-03T02:06:34.876Z",
          "wordCount": 648,
          "title": "GPU Accelerated Exhaustive Search for Optimal Ensemble of Black-Box Optimization Algorithms. (arXiv:2012.04201v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.09394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junwoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1\">Youngwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Haneol Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>",
          "description": "Question Answering (QA) is a widely-used framework for developing and\nevaluating an intelligent machine. In this light, QA on Electronic Health\nRecords (EHR), namely EHR QA, can work as a crucial milestone towards\ndeveloping an intelligent agent in healthcare. EHR data are typically stored in\na relational database, which can also be converted to a directed acyclic graph,\nallowing two approaches for EHR QA: Table-based QA and Knowledge Graph-based\nQA. We hypothesize that the graph-based approach is more suitable for EHR QA as\ngraphs can represent relations between entities and values more naturally\ncompared to tables, which essentially require JOIN operations. In this paper,\nwe propose a graph-based EHR QA where natural language queries are converted to\nSPARQL instead of SQL. To validate our hypothesis, we create four EHR QA\ndatasets (graph-based VS table-based, and simplified database schema VS\noriginal database schema), based on a table-based dataset MIMICSQL. We test\nboth a simple Seq2Seq model and a state-of-the-art EHR QA model on all datasets\nwhere the graph-based datasets facilitated up to 34% higher accuracy than the\ntable-based dataset without any modification to the model architectures.\nFinally, all datasets are open-sourced to encourage further EHR QA research in\nboth directions.",
          "link": "http://arxiv.org/abs/2010.09394",
          "publishedOn": "2021-08-03T02:06:34.868Z",
          "wordCount": 675,
          "title": "Knowledge Graph-based Question Answering with Electronic Health Records. (arXiv:2010.09394v2 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nasibullah/0/1/0/all/0/1\">Nasibullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanta_P/0/1/0/all/0/1\">Partha Pratim Mohanta</a>",
          "description": "Video captioning is one of the challenging problems at the intersection of\nvision and language, having many real-life applications in video retrieval,\nvideo surveillance, assisting visually challenged people, Human-machine\ninterface, and many more. Recent deep learning-based methods have shown\npromising results but are still on the lower side than other vision tasks (such\nas image classification, object detection). A significant drawback with\nexisting video captioning methods is that they are optimized over cross-entropy\nloss function, which is uncorrelated to the de facto evaluation metrics (BLEU,\nMETEOR, CIDER, ROUGE).In other words, cross-entropy is not a proper surrogate\nof the true loss function for video captioning. This paper addresses the\ndrawback by introducing a dynamic loss network (DLN), which provides an\nadditional feedback signal that directly reflects the evaluation metrics. Our\nresults on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to\nText (MSRVTT) datasets outperform previous methods.",
          "link": "http://arxiv.org/abs/2107.11707",
          "publishedOn": "2021-08-03T02:06:34.862Z",
          "wordCount": 609,
          "title": "Boosting Video Captioning with Dynamic Loss Network. (arXiv:2107.11707v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04007",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Legarreta_J/0/1/0/all/0/1\">Jon Haitz Legarreta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petit_L/0/1/0/all/0/1\">Laurent Petit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rheault_F/0/1/0/all/0/1\">Fran&#xe7;ois Rheault</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Theaud_G/0/1/0/all/0/1\">Guillaume Theaud</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lemaire_C/0/1/0/all/0/1\">Carl Lemaire</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Descoteaux_M/0/1/0/all/0/1\">Maxime Descoteaux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jodoin_P/0/1/0/all/0/1\">Pierre-Marc Jodoin</a>",
          "description": "Current brain white matter fiber tracking techniques show a number of\nproblems, including: generating large proportions of streamlines that do not\naccurately describe the underlying anatomy; extracting streamlines that are not\nsupported by the underlying diffusion signal; and under-representing some fiber\npopulations, among others. In this paper, we describe a novel autoencoder-based\nlearning method to filter streamlines from diffusion MRI tractography, and\nhence, to obtain more reliable tractograms. Our method, dubbed FINTA (Filtering\nin Tractography using Autoencoders) uses raw, unlabeled tractograms to train\nthe autoencoder, and to learn a robust representation of brain streamlines.\nSuch an embedding is then used to filter undesired streamline samples using a\nnearest neighbor algorithm. Our experiments on both synthetic and in vivo human\nbrain diffusion MRI tractography data obtain accuracy scores exceeding the 90\\%\nthreshold on the test set. Results reveal that FINTA has a superior filtering\nperformance compared to conventional, anatomy-based methods, and the\nRecoBundles state-of-the-art method. Additionally, we demonstrate that FINTA\ncan be applied to partial tractograms without requiring changes to the\nframework. We also show that the proposed method generalizes well across\ndifferent tracking methods and datasets, and shortens significantly the\ncomputation time for large (>1 M streamlines) tractograms. Together, this work\nbrings forward a new deep learning framework in tractography based on\nautoencoders, which offers a flexible and powerful method for white matter\nfiltering and bundling that could enhance tractometry and connectivity\nanalyses.",
          "link": "http://arxiv.org/abs/2010.04007",
          "publishedOn": "2021-08-03T02:06:34.855Z",
          "wordCount": 748,
          "title": "Filtering in tractography using autoencoders (FINTA). (arXiv:2010.04007v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>",
          "description": "Modeling complex physical dynamics is a fundamental task in science and\nengineering. Traditional physics-based models are interpretable but rely on\nrigid assumptions. And the direct numerical approximation is usually\ncomputationally intensive, requiring significant computational resources and\nexpertise. While deep learning (DL) provides novel alternatives for efficiently\nrecognizing complex patterns and emulating nonlinear dynamics, it does not\nnecessarily obey the governing laws of physical systems, nor do they generalize\nwell across different systems. Thus, the study of physics-guided DL emerged and\nhas gained great progress. It aims to take the best from both physics-based\nmodeling and state-of-the-art DL models to better solve scientific problems. In\nthis paper, we provide a structured overview of existing methodologies of\nintegrating prior physical knowledge or physics-based modeling into DL and\ndiscuss the emerging opportunities.",
          "link": "http://arxiv.org/abs/2107.01272",
          "publishedOn": "2021-08-03T02:06:34.848Z",
          "wordCount": 579,
          "title": "Physics-Guided Deep Learning for Dynamical Systems: A survey. (arXiv:2107.01272v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.06606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Arghyadip Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Sanjay Shakkottai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1\">R. Srikant</a>",
          "description": "In the regret-based formulation of multi-armed bandit (MAB) problems, except\nin rare instances, much of the literature focuses on arms with i.i.d. rewards.\nIn this paper, we consider the problem of obtaining regret guarantees for MAB\nproblems in which the rewards of each arm form a Markov chain which may not\nbelong to a single parameter exponential family. To achieve logarithmic regret\nin such problems is not difficult: a variation of standard KL-UCB does the job.\nHowever, the constants obtained from such an analysis are poor for the\nfollowing reason: i.i.d. rewards are a special case of Markov rewards and it is\ndifficult to design an algorithm that works well independent of whether the\nunderlying model is truly Markovian or i.i.d. To overcome this issue, we\nintroduce a novel algorithm that identifies whether the rewards from each arm\nare truly Markovian or i.i.d. using a Hellinger distance-based test. Our\nalgorithm then switches from using a standard KL-UCB to a specialized version\nof KL-UCB when it determines that the arm reward is Markovian, thus resulting\nin low regret for both i.i.d. and Markovian settings.",
          "link": "http://arxiv.org/abs/2009.06606",
          "publishedOn": "2021-08-03T02:06:34.842Z",
          "wordCount": 649,
          "title": "Adaptive KL-UCB based Bandit Algorithms for Markovian and i.i.d. Settings. (arXiv:2009.06606v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Si_N/0/1/0/all/0/1\">Nian Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhengyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanchet_J/0/1/0/all/0/1\">Jose Blanchet</a>",
          "description": "Policy learning using historical observational data is an important problem\nthat has found widespread applications. Examples include selecting offers,\nprices, advertisements to send to customers, as well as selecting which\nmedication to prescribe to a patient. However, existing literature rests on the\ncrucial assumption that the future environment where the learned policy will be\ndeployed is the same as the past environment that has generated the data--an\nassumption that is often false or too coarse an approximation. In this paper,\nwe lift this assumption and aim to learn a distributional robust policy with\nincomplete (bandit) observational data. We propose a novel learning algorithm\nthat is able to learn a robust policy to adversarial perturbations and unknown\ncovariate shifts. We first present a policy evaluation procedure in the\nambiguous environment and then give a performance guarantee based on the theory\nof uniform convergence. Additionally, we also give a heuristic algorithm to\nsolve the distributional robust policy learning problems efficiently. Finally,\nwe demonstrate the robustness of our methods in the synthetic and real-world\ndatasets.",
          "link": "http://arxiv.org/abs/2006.05630",
          "publishedOn": "2021-08-03T02:06:34.822Z",
          "wordCount": 660,
          "title": "Distributional Robust Batch Contextual Bandits. (arXiv:2006.05630v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Siqi Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiangjing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>",
          "description": "Federated learning (FL) is a machine learning field in which researchers try\nto facilitate model learning process among multiparty without violating privacy\nprotection regulations. Considerable effort has been invested in FL\noptimization and communication related researches. In this work, we introduce\nFedLab, a lightweight open-source framework for FL simulation. The design of\nFedLab focuses on FL algorithm effectiveness and communication efficiency.\nAlso, FedLab is scalable in different deployment scenario. We hope FedLab could\nprovide flexible API as well as reliable baseline implementations, and relieve\nthe burden of implementing novel approaches for researchers in FL community.",
          "link": "http://arxiv.org/abs/2107.11621",
          "publishedOn": "2021-08-03T02:06:34.816Z",
          "wordCount": 550,
          "title": "FedLab: A Flexible Federated Learning Framework. (arXiv:2107.11621v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Illarionov_E/0/1/0/all/0/1\">E. Illarionov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Temirchev_P/0/1/0/all/0/1\">P. Temirchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voloskov_D/0/1/0/all/0/1\">D. Voloskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostoev_R/0/1/0/all/0/1\">R. Kostoev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonov_M/0/1/0/all/0/1\">M. Simonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pissarenko_D/0/1/0/all/0/1\">D. Pissarenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orlov_D/0/1/0/all/0/1\">D. Orlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koroteev_D/0/1/0/all/0/1\">D. Koroteev</a>",
          "description": "Reservoir simulation and adaptation (also known as history matching) are\ntypically considered as separate problems. While a set of models are aimed at\nthe solution of the forward simulation problem assuming all initial geological\nparameters are known, the other set of models adjust geological parameters\nunder the fixed forward simulation model to fit production data. This results\nin many difficulties for both reservoir engineers and developers of new\nefficient computation schemes. We present a unified approach to reservoir\nsimulation and adaptation problems. A single neural network model allows a\nforward pass from initial geological parameters of the 3D reservoir model\nthrough dynamic state variables to well's production rates and backward\ngradient propagation to any model inputs and variables. The model fitting and\ngeological parameters adaptation both become the optimization problem over\nspecific parts of the same neural network model. Standard gradient-based\noptimization schemes can be used to find the optimal solution. Using real-world\noilfield model and historical production rates we demonstrate that the\nsuggested approach allows reservoir simulation and history matching with a\nbenefit of several orders of magnitude simulation speed-up. Finally, to\npropagate this research we open-source a Python-based framework DeepField that\nallows standard processing of reservoir models and reproducing the approach\npresented in this paper.",
          "link": "http://arxiv.org/abs/2102.10304",
          "publishedOn": "2021-08-03T02:06:34.809Z",
          "wordCount": null,
          "title": "End-to-end neural network approach to 3D reservoir simulation and adaptation. (arXiv:2102.10304v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09543",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1\">Richard Osuala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1\">Kaisar Kushibar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garrucho_L/0/1/0/all/0/1\">Lidia Garrucho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Linardos_A/0/1/0/all/0/1\">Akis Linardos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szafranowska_Z/0/1/0/all/0/1\">Zuzanna Szafranowska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1\">Stefan Klein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1\">Oliver Diaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>",
          "description": "Despite technological and medical advances, the detection, interpretation,\nand treatment of cancer based on imaging data continue to pose significant\nchallenges. These include high inter-observer variability, difficulty of\nsmall-sized lesion detection, nodule interpretation and malignancy\ndetermination, inter- and intra-tumour heterogeneity, class imbalance,\nsegmentation inaccuracies, and treatment effect uncertainty. The recent\nadvancements in Generative Adversarial Networks (GANs) in computer vision as\nwell as in medical imaging may provide a basis for enhanced capabilities in\ncancer detection and analysis. In this review, we assess the potential of GANs\nto address a number of key challenges of cancer imaging, including data\nscarcity and imbalance, domain and dataset shifts, data access and privacy,\ndata annotation and quantification, as well as cancer detection, tumour\nprofiling and treatment planning. We provide a critical appraisal of the\nexisting literature of GANs applied to cancer imagery, together with\nsuggestions on future research directions to address these challenges. We\nanalyse and discuss 163 papers that apply adversarial training techniques in\nthe context of cancer imaging and elaborate their methodologies, advantages and\nlimitations. With this work, we strive to bridge the gap between the needs of\nthe clinical cancer imaging community and the current and prospective research\non GANs in the artificial intelligence community.",
          "link": "http://arxiv.org/abs/2107.09543",
          "publishedOn": "2021-08-03T02:06:34.809Z",
          "wordCount": null,
          "title": "A Review of Generative Adversarial Networks in Cancer Imaging: New Applications, New Solutions. (arXiv:2107.09543v1 [eess.IV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.05260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baharani_M/0/1/0/all/0/1\">Mohammadreza Baharani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1\">Hamed Tabkhi</a>",
          "description": "This paper presents a scalable deep learning model called Agile Temporal\nConvolutional Network (ATCN) for high-accurate fast classification and time\nseries prediction in resource-constrained embedded systems. ATCN is a family of\ncompact networks with formalized hyperparameters that enable\napplication-specific adjustments to be made to the model architecture. It is\nprimarily designed for embedded edge devices with very limited performance and\nmemory, such as wearable biomedical devices and real-time reliability\nmonitoring systems. ATCN makes fundamental improvements over the mainstream\ntemporal convolutional neural networks, including residual connections as time\nattention machines to increase the network depth and accuracy and the\nincorporation of separable depth-wise convolution to reduce the computational\ncomplexity of the model. As part of the present work, three ATCN families,\nnamely T0, T1, and T2, are also presented and evaluated on different ranges of\nembedded processors - Cortex-M7 and Cortex-A57 processor. An evaluation of the\nATCN models against the best-in-class InceptionTime shows that ATCN improves\nboth accuracy and execution time on a broad range of embedded and\ncyber-physical applications with demand for real-time processing on the\nembedded edge. At the same time, in contrast to existing solutions, ATCN is the\nfirst deep learning-based approach that can be run on embedded microcontrollers\n(Cortex-M7) with limited computational performance and memory capacity while\ndelivering state-of-the-art accuracy.",
          "link": "http://arxiv.org/abs/2011.05260",
          "publishedOn": "2021-08-03T02:06:34.808Z",
          "wordCount": null,
          "title": "ATCN: Resource-Efficient Processing of Time Series on Edge. (arXiv:2011.05260v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07405",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1\">Wu Lin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nielsen_F/0/1/0/all/0/1\">Frank Nielsen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Emtiyaz Khan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1\">Mark Schmidt</a>",
          "description": "Natural-gradient descent (NGD) on structured parameter spaces (e.g., low-rank\ncovariances) is computationally challenging due to difficult Fisher-matrix\ncomputations. We address this issue by using \\emph{local-parameter coordinates}\nto obtain a flexible and efficient NGD method that works well for a\nwide-variety of structured parameterizations. We show four applications where\nour method (1) generalizes the exponential natural evolutionary strategy, (2)\nrecovers existing Newton-like algorithms, (3) yields new structured\nsecond-order algorithms via matrix groups, and (4) gives new algorithms to\nlearn covariances of Gaussian and Wishart-based distributions. We show results\non a range of problems from deep learning, variational inference, and evolution\nstrategies. Our work opens a new direction for scalable structured geometric\nmethods.",
          "link": "http://arxiv.org/abs/2102.07405",
          "publishedOn": "2021-08-03T02:06:34.808Z",
          "wordCount": null,
          "title": "Tractable structured natural gradient descent using local parameterizations. (arXiv:2102.07405v7 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Nan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zepeng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jianfeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askari_H/0/1/0/all/0/1\">Hassan Askari</a>",
          "description": "Autonomous vehicles are most concerned about safety control issues, and the\nslip ratio is critical to the safety of the vehicle control system. In this\npaper, different machine learning algorithms (Neural Networks, Gradient\nBoosting Machine, Random Forest, and Support Vector Machine) are used to train\nthe slip ratio estimation model based on the acceleration signals ($a_x$,\n$a_y$, and $a_z$) from the tri-axial Micro-Electro Mechanical System (MEMS)\naccelerometer utilized in the intelligent tire system, where the acceleration\nsignals are divided into four sets ($a_x/a_y/a_z$, $a_x/a_z$, $a_y/a_z$, and\n$a_z$) as algorithm inputs. The experimental data used in this study are\ncollected through the MTS Flat-Trac tire test platform. Performance of\ndifferent slip ratio estimation models is compared using the NRMS errors in\n10-fold cross-validation (CV). The results indicate that NN and GBM have more\npromising accuracy, and the $a_z$ input type has a better performance compared\nto other input types, with the best result being the estimation model of the NN\nalgorithm with $a_z$ as input, which results is 4.88\\%. The present study with\nthe fusion of intelligent tire system and machine learning paves the way for\nthe accurate estimation of tire slip ratio under different driving conditions,\nwhich will open up a new way of Autonomous vehicles, intelligent tires, and\ntire slip ratio estimation.",
          "link": "http://arxiv.org/abs/2106.08961",
          "publishedOn": "2021-08-03T02:06:34.808Z",
          "wordCount": null,
          "title": "Intelligent-Tire-Based Slip Ratio Estimation Using Machine Learning. (arXiv:2106.08961v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.01037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiamei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1\">Sebastian Lapuschkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1\">Wojciech Samek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1\">Alexander Binder</a>",
          "description": "This paper analyzes the predictions of image captioning models with attention\nmechanisms beyond visualizing the attention itself. We develop variants of\nlayer-wise relevance propagation (LRP) and gradient-based explanation methods,\ntailored to image captioning models with attention mechanisms. We compare the\ninterpretability of attention heatmaps systematically against the explanations\nprovided by explanation methods such as LRP, Grad-CAM, and Guided Grad-CAM. We\nshow that explanation methods provide simultaneously pixel-wise image\nexplanations (supporting and opposing pixels of the input image) and linguistic\nexplanations (supporting and opposing words of the preceding sequence) for each\nword in the predicted captions. We demonstrate with extensive experiments that\nexplanation methods 1) can reveal additional evidence used by the model to make\ndecisions compared to attention; 2) correlate to object locations with high\nprecision; 3) are helpful to \"debug\" the model, e.g. by analyzing the reasons\nfor hallucinated object words. With the observed properties of explanations, we\nfurther design an LRP-inference fine-tuning strategy that reduces the issue of\nobject hallucination in image captioning models, and meanwhile, maintains the\nsentence fluency. We conduct experiments with two widely used attention\nmechanisms: the adaptive attention mechanism calculated with the additive\nattention and the multi-head attention mechanism calculated with the scaled dot\nproduct.",
          "link": "http://arxiv.org/abs/2001.01037",
          "publishedOn": "2021-08-03T02:06:34.807Z",
          "wordCount": null,
          "title": "Explain and Improve: LRP-Inference Fine-Tuning for Image Captioning Models. (arXiv:2001.01037v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aqajari_S/0/1/0/all/0/1\">Seyed Amir Hossein Aqajari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zargari_A/0/1/0/all/0/1\">Amir Hosein Afandizadeh Zargari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1\">Amir M. Rahmani</a>",
          "description": "Respiratory rate (RR) is a clinical sign representing ventilation. An\nabnormal change in RR is often the first sign of health deterioration as the\nbody attempts to maintain oxygen delivery to its tissues. There has been a\ngrowing interest in remotely monitoring of RR in everyday settings which has\nmade photoplethysmography (PPG) monitoring wearable devices an attractive\nchoice. PPG signals are useful sources for RR extraction due to the presence of\nrespiration-induced modulations in them. The existing PPG-based RR estimation\nmethods mainly rely on hand-crafted rules and manual parameters tuning. An\nend-to-end deep learning approach was recently proposed, however, despite its\nautomatic nature, the performance of this method is not ideal using the real\nworld data. In this paper, we present an end-to-end and accurate pipeline for\nRR estimation using Cycle Generative Adversarial Networks (CycleGAN) to\nreconstruct respiratory signals from raw PPG signals. Our results demonstrate a\nhigher RR estimation accuracy of up to 2$\\times$ (mean absolute error of\n1.9$\\pm$0.3 using five fold cross validation) compared to the state-of-th-art\nusing a identical publicly available dataset. Our results suggest that CycleGAN\ncan be a valuable method for RR estimation from raw PPG signals.",
          "link": "http://arxiv.org/abs/2105.00594",
          "publishedOn": "2021-08-03T02:06:34.807Z",
          "wordCount": null,
          "title": "An End-to-End and Accurate PPG-based Respiratory Rate Estimation Approach Using Cycle Generative Adversarial Networks. (arXiv:2105.00594v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Amish Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1\">Sourav Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datar_A/0/1/0/all/0/1\">Arnhav Datar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadiwala_J/0/1/0/all/0/1\">Juned Kadiwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1\">Hrithwik Shalu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1\">Jimson Mathew</a>",
          "description": "Reliable detection of the prodromal stages of Alzheimer's disease (AD)\nremains difficult even today because, unlike other neurocognitive impairments,\nthere is no definitive diagnosis of AD in vivo. In this context, existing\nresearch has shown that patients often develop language impairment even in mild\nAD conditions. We propose a multimodal deep learning method that utilizes\nspeech and the corresponding transcript simultaneously to detect AD. For audio\nsignals, the proposed audio-based network, a convolutional neural network (CNN)\nbased model, predicts the diagnosis for multiple speech segments, which are\ncombined for the final prediction. Similarly, we use contextual embedding\nextracted from BERT concatenated with a CNN-generated embedding for classifying\nthe transcript. The individual predictions of the two models are then combined\nto make the final classification. We also perform experiments to analyze the\nmodel performance when Automated Speech Recognition (ASR) system generated\ntranscripts are used instead of manual transcription in the text-based model.\nThe proposed method achieves 85.3% 10-fold cross-validation accuracy when\ntrained and evaluated on the Dementiabank Pitt corpus.",
          "link": "http://arxiv.org/abs/2012.00096",
          "publishedOn": "2021-08-03T02:06:34.806Z",
          "wordCount": null,
          "title": "Multi-Modal Detection of Alzheimer's Disease from Speech and Text. (arXiv:2012.00096v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kunhong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yucheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yahong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingshuai Li</a>",
          "description": "In recent years, researchers have been paying increasing attention to the\nthreats brought by deep learning models to data security and privacy,\nespecially in the field of domain adaptation. Existing unsupervised domain\nadaptation (UDA) methods can achieve promising performance without transferring\ndata from source domain to target domain. However, UDA with representation\nalignment or self-supervised pseudo-labeling relies on the transferred source\nmodels. In many data-critical scenarios, methods based on model transferring\nmay suffer from membership inference attacks and expose private data. In this\npaper, we aim to overcome a challenging new setting where the source models are\nonly queryable but cannot be transferred to the target domain. We propose\nBlack-box Probe Domain Adaptation (BPDA), which adopts query mechanism to probe\nand refine information from source model using third-party dataset. In order to\ngain more informative query results, we further propose Distributionally\nAdversarial Training (DAT) to align the distribution of third-party data with\nthat of target data. BPDA uses public third-party dataset and adversarial\nexamples based on DAT as the information carrier between source and target\ndomains, dispensing with transferring source data or model. Experimental\nresults on benchmarks of Digit-Five, Office-Caltech, Office-31, Office-Home,\nand DomainNet demonstrate the feasibility of BPDA without model transferring.",
          "link": "http://arxiv.org/abs/2107.10174",
          "publishedOn": "2021-08-03T02:06:34.806Z",
          "wordCount": null,
          "title": "Black-box Probe for Unsupervised Domain Adaptation without Model Transferring. (arXiv:2107.10174v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.08708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1\">Uttaran Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roncal_C/0/1/0/all/0/1\">Christian Roncal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_T/0/1/0/all/0/1\">Trisha Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohan Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapsaskis_K/0/1/0/all/0/1\">Kyra Kapsaskis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_K/0/1/0/all/0/1\">Kurt Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1\">Aniket Bera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>",
          "description": "We present an autoencoder-based semi-supervised approach to classify\nperceived human emotions from walking styles obtained from videos or\nmotion-captured data and represented as sequences of 3D poses. Given the motion\non each joint in the pose at each time step extracted from 3D pose sequences,\nwe hierarchically pool these joint motions in a bottom-up manner in the\nencoder, following the kinematic chains in the human body. We also constrain\nthe latent embeddings of the encoder to contain the space of\npsychologically-motivated affective features underlying the gaits. We train the\ndecoder to reconstruct the motions per joint per time step in a top-down manner\nfrom the latent embeddings. For the annotated data, we also train a classifier\nto map the latent embeddings to emotion labels. Our semi-supervised approach\nachieves a mean average precision of 0.84 on the Emotion-Gait benchmark\ndataset, which contains both labeled and unlabeled gaits collected from\nmultiple sources. We outperform current state-of-art algorithms for both\nemotion recognition and action recognition from 3D gaits by 7%--23% on the\nabsolute. More importantly, we improve the average precision by 10%--50% on the\nabsolute on classes that each makes up less than 25% of the labeled part of the\nEmotion-Gait benchmark dataset.",
          "link": "http://arxiv.org/abs/1911.08708",
          "publishedOn": "2021-08-03T02:06:34.805Z",
          "wordCount": null,
          "title": "Take an Emotion Walk: Perceiving Emotions from Gaits Using Hierarchical Attention Pooling and Affective Mapping. (arXiv:1911.08708v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.02284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenmann_M/0/1/0/all/0/1\">Matthias Eisenmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarikaya_D/0/1/0/all/0/1\">Duygu Sarikaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marz_K/0/1/0/all/0/1\">Keno M&#xe4;rz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_T/0/1/0/all/0/1\">Toby Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malpani_A/0/1/0/all/0/1\">Anand Malpani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallert_J/0/1/0/all/0/1\">Johannes Fallert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feussner_H/0/1/0/all/0/1\">Hubertus Feussner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannarou_S/0/1/0/all/0/1\">Stamatia Giannarou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mascagni_P/0/1/0/all/0/1\">Pietro Mascagni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakawala_H/0/1/0/all/0/1\">Hirenkumar Nakawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_A/0/1/0/all/0/1\">Adrian Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pugh_C/0/1/0/all/0/1\">Carla Pugh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedula_S/0/1/0/all/0/1\">Swaroop S. Vedula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cleary_K/0/1/0/all/0/1\">Kevin Cleary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fichtinger_G/0/1/0/all/0/1\">Gabor Fichtinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forestier_G/0/1/0/all/0/1\">Germain Forestier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibaud_B/0/1/0/all/0/1\">Bernard Gibaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grantcharov_T/0/1/0/all/0/1\">Teodor Grantcharov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashizume_M/0/1/0/all/0/1\">Makoto Hashizume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heckmann_Notzel_D/0/1/0/all/0/1\">Doreen Heckmann-N&#xf6;tzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenngott_H/0/1/0/all/0/1\">Hannes G. Kenngott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kikinis_R/0/1/0/all/0/1\">Ron Kikinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mundermann_L/0/1/0/all/0/1\">Lars M&#xfc;ndermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onogur_S/0/1/0/all/0/1\">Sinan Onogur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1\">Raphael Sznitman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1\">Russell H. Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu D. Tizabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1\">Martin Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1\">Gregory D. Hager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumuth_T/0/1/0/all/0/1\">Thomas Neumuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_J/0/1/0/all/0/1\">Justin Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gockel_I/0/1/0/all/0/1\">Ines Gockel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goedeke_J/0/1/0/all/0/1\">Jan Goedeke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_D/0/1/0/all/0/1\">Daniel A. Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joyeux_L/0/1/0/all/0/1\">Luc Joyeux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1\">Kyle Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leff_D/0/1/0/all/0/1\">Daniel R. Leff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madani_A/0/1/0/all/0/1\">Amin Madani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcus_H/0/1/0/all/0/1\">Hani J. Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meireles_O/0/1/0/all/0/1\">Ozanan Meireles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seitel_A/0/1/0/all/0/1\">Alexander Seitel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teber_D/0/1/0/all/0/1\">Dogu Teber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uckert_F/0/1/0/all/0/1\">Frank &#xdc;ckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Stich_B/0/1/0/all/0/1\">Beat P. M&#xfc;ller-Stich</a>, et al. (2 additional authors not shown)",
          "description": "Recent developments in data science in general and machine learning in\nparticular have transformed the way experts envision the future of surgery.\nSurgical Data Science (SDS) is a new research field that aims to improve the\nquality of interventional healthcare through the capture, organization,\nanalysis and modeling of data. While an increasing number of data-driven\napproaches and clinical applications have been studied in the fields of\nradiological and clinical data science, translational success stories are still\nlacking in surgery. In this publication, we shed light on the underlying\nreasons and provide a roadmap for future advances in the field. Based on an\ninternational workshop involving leading researchers in the field of SDS, we\nreview current practice, key achievements and initiatives as well as available\nstandards and tools for a number of topics relevant to the field, namely (1)\ninfrastructure for data acquisition, storage and access in the presence of\nregulatory constraints, (2) data annotation and sharing and (3) data analytics.\nWe further complement this technical perspective with (4) a review of currently\navailable SDS products and the translational progress from academia and (5) a\nroadmap for faster clinical translation and exploitation of the full potential\nof SDS, based on an international multi-round Delphi process.",
          "link": "http://arxiv.org/abs/2011.02284",
          "publishedOn": "2021-08-03T02:06:34.804Z",
          "wordCount": null,
          "title": "Surgical Data Science -- from Concepts toward Clinical Translation. (arXiv:2011.02284v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.01651",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Chia Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1\">Marina Meil&#x103;</a>",
          "description": "Many manifold embedding algorithms fail apparently when the data manifold has\na large aspect ratio (such as a long, thin strip). Here, we formulate success\nand failure in terms of finding a smooth embedding, showing also that the\nproblem is pervasive and more complex than previously recognized.\nMathematically, success is possible under very broad conditions, provided that\nembedding is done by carefully selected eigenfunctions of the Laplace-Beltrami\noperator $\\Delta$. Hence, we propose a bicriterial Independent Eigencoordinate\nSelection (IES) algorithm that selects smooth embeddings with few eigenvectors.\nThe algorithm is grounded in theory, has low computational overhead, and is\nsuccessful on synthetic and large real data.",
          "link": "http://arxiv.org/abs/1907.01651",
          "publishedOn": "2021-08-03T02:06:34.802Z",
          "wordCount": 565,
          "title": "Selecting the independent coordinates of manifolds with large aspect ratios. (arXiv:1907.01651v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.01437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tianyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Nhat Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "We present several new complexity results for the algorithms that\napproximately solve the optimal transport (OT) problem between two discrete\nprobability measures with at most $n$ atoms. First, we improve the complexity\nbound of a greedy variant of the Sinkhorn algorithm, known as\n\\textit{Greenkhorn} algorithm, from $\\widetilde{O}(n^2\\varepsilon^{-3})$ to\n$\\widetilde{O}(n^2\\varepsilon^{-2})$. Notably, this matches the best known\ncomplexity bound of the Sinkhorn algorithm and sheds the light to superior\npractical performance of the Greenkhorn algorithm. Second, we generalize an\nadaptive primal-dual accelerated gradient descent (APDAGD)\nalgorithm~\\citep{Dvurechensky-2018-Computational} with mirror mapping $\\phi$\nand prove that the resulting APDAMD algorithm achieves the complexity bound of\n$\\widetilde{O}(n^2\\sqrt{\\delta}\\varepsilon^{-1})$ where $\\delta>0$ refers to\nthe regularity of $\\phi$. We demonstrate that the complexity bound of\n$\\widetilde{O}(\\min\\{n^{9/4}\\varepsilon^{-1}, n^2\\varepsilon^{-2}\\})$ is\ninvalid for the APDAGD algorithm and establish a new complexity bound of\n$\\widetilde{O}(n^{5/2}\\varepsilon^{-1})$. Moreover, we propose a\n\\textit{deterministic} accelerated Sinkhorn algorithm and prove that it\nachieves the complexity bound of $\\widetilde{O}(n^{7/3}\\varepsilon^{-4/3})$ by\nincorporating an estimate sequence. Therefore, the accelerated Sinkhorn\nalgorithm outperforms the Sinkhorn and Greenkhorn algorithms in terms of\n$1/\\varepsilon$ and the APDAGD and accelerated alternating\nminimization~\\citep{Guminov-2021-Combination} algorithms in terms of $n$.\nFinally, we conduct experiments on synthetic data and real images with the\nproposed algorithms in the paper and demonstrate their efficiency via numerical\nresults.",
          "link": "http://arxiv.org/abs/1906.01437",
          "publishedOn": "2021-08-03T02:06:34.795Z",
          "wordCount": 763,
          "title": "On the Efficiency of Sinkhorn and Greenkhorn and Their Acceleration for Optimal Transport. (arXiv:1906.01437v7 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.04005",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Maddalena_E/0/1/0/all/0/1\">Emilio T. Maddalena</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Scharnhorst_P/0/1/0/all/0/1\">Paul Scharnhorst</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jones_C/0/1/0/all/0/1\">Colin N. Jones</a>",
          "description": "We consider the problem of reconstructing a function from a finite set of\nnoise-corrupted samples. Two kernel algorithms are analyzed, namely kernel\nridge regression and $\\varepsilon$-support vector regression. By assuming the\nground-truth function belongs to the reproducing kernel Hilbert space of the\nchosen kernel, and the measurement noise affecting the dataset is bounded, we\nadopt an approximation theory viewpoint to establish \\textit{deterministic},\nfinite-sample error bounds for the two models. Finally, we discuss their\nconnection with Gaussian processes and two numerical examples are provided. In\nestablishing our inequalities, we hope to help bring the fields of\nnon-parametric kernel learning and system identification for robust control\ncloser to each other.",
          "link": "http://arxiv.org/abs/2008.04005",
          "publishedOn": "2021-08-03T02:06:34.780Z",
          "wordCount": 584,
          "title": "Deterministic error bounds for kernel-based learning techniques under bounded noise. (arXiv:2008.04005v3 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1\">Anusua Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1\">Mohit Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1\">Nikhil Kumar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinsche_M/0/1/0/all/0/1\">Markus Hinsche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Prashant Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matiaschek_M/0/1/0/all/0/1\">Markus Matiaschek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behrens_T/0/1/0/all/0/1\">Tristan Behrens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Militeri_M/0/1/0/all/0/1\">Mirco Militeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birge_C/0/1/0/all/0/1\">Cameron Birge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaushik_S/0/1/0/all/0/1\">Shivangi Kaushik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_A/0/1/0/all/0/1\">Archisman Mohapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_R/0/1/0/all/0/1\">Rita Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodhia_R/0/1/0/all/0/1\">Rahul Dodhia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1\">Juan Lavista Ferres</a>",
          "description": "Malnutrition is a global health crisis and is the leading cause of death\namong children under five. Detecting malnutrition requires anthropometric\nmeasurements of weight, height, and middle-upper arm circumference. However,\nmeasuring them accurately is a challenge, especially in the global south, due\nto limited resources. In this work, we propose a CNN-based approach to estimate\nthe height of standing children under five years from depth images collected\nusing a smart-phone. According to the SMART Methodology Manual [5], the\nacceptable accuracy for height is less than 1.4 cm. On training our deep\nlearning model on 87131 depth images, our model achieved an average mean\nabsolute error of 1.64% on 57064 test images. For 70.3% test images, we\nestimated height accurately within the acceptable 1.4 cm range. Thus, our\nproposed solution can accurately detect stunting (low height-for-age) in\nstanding children below five years of age.",
          "link": "http://arxiv.org/abs/2105.01688",
          "publishedOn": "2021-08-03T02:06:34.773Z",
          "wordCount": 641,
          "title": "Height Estimation of Children under Five Years using Depth Images. (arXiv:2105.01688v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06396",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Sengupta_U/0/1/0/all/0/1\">Ushnish Sengupta</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Waxenegger_Wilfing_G/0/1/0/all/0/1\">G&#xfc;nther Waxenegger-Wilfing</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Martin_J/0/1/0/all/0/1\">Jan Martin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hardi_J/0/1/0/all/0/1\">Justin Hardi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Juniper_M/0/1/0/all/0/1\">Matthew P. Juniper</a>",
          "description": "The 100 MW cryogenic liquid oxygen/hydrogen multi-injector combustor BKD\noperated by the DLR Institute of Space Propulsion is a research platform that\nallows the study of thermoacoustic instabilities under realistic conditions,\nrepresentative of small upper stage rocket engines. We use data from BKD\nexperimental campaigns in which the static chamber pressure and fuel-oxidizer\nratio are varied such that the first tangential mode of the combustor is\nexcited under some conditions. We train an autoregressive Bayesian neural\nnetwork model to forecast the amplitude of the dynamic pressure time series,\ninputting multiple sensor measurements (injector pressure/ temperature\nmeasurements, static chamber pressure, high-frequency dynamic pressure\nmeasurements, high-frequency OH* chemiluminescence measurements) and future\nflow rate control signals. The Bayesian nature of our algorithms allows us to\nwork with a dataset whose size is restricted by the expense of each\nexperimental run, without making overconfident extrapolations. We find that the\nnetworks are able to accurately forecast the evolution of the pressure\namplitude and anticipate instability events on unseen experimental runs 500\nmilliseconds in advance. We compare the predictive accuracy of multiple models\nusing different combinations of sensor inputs. We find that the high-frequency\ndynamic pressure signal is particularly informative. We also use the technique\nof integrated gradients to interpret the influence of different sensor inputs\non the model prediction. The negative log-likelihood of data points in the test\ndataset indicates that predictive uncertainties are well-characterized by our\nBayesian model and simulating a sensor failure event results as expected in a\ndramatic increase in the epistemic component of the uncertainty.",
          "link": "http://arxiv.org/abs/2107.06396",
          "publishedOn": "2021-08-03T02:06:34.765Z",
          "wordCount": 724,
          "title": "Forecasting Thermoacoustic Instabilities in Liquid Propellant Rocket Engines Using Multimodal Bayesian Deep Learning. (arXiv:2107.06396v2 [physics.flu-dyn] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.13086",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goldsteen_A/0/1/0/all/0/1\">Abigail Goldsteen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezov_G/0/1/0/all/0/1\">Gilad Ezov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1\">Ron Shmelkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moffie_M/0/1/0/all/0/1\">Micha Moffie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farkash_A/0/1/0/all/0/1\">Ariel Farkash</a>",
          "description": "There is a known tension between the need to analyze personal data to drive\nbusiness and privacy concerns. Many data protection regulations, including the\nEU General Data Protection Regulation (GDPR) and the California Consumer\nProtection Act (CCPA), set out strict restrictions and obligations on the\ncollection and processing of personal data. Moreover, machine learning models\nthemselves can be used to derive personal information, as demonstrated by\nrecent membership and attribute inference attacks. Anonymized data, however, is\nexempt from the obligations set out in these regulations. It is therefore\ndesirable to be able to create models that are anonymized, thus also exempting\nthem from those obligations, in addition to providing better protection against\nattacks.\n\nLearning on anonymized data typically results in significant degradation in\naccuracy. In this work, we propose a method that is able to achieve better\nmodel accuracy by using the knowledge encoded within the trained model, and\nguiding our anonymization process to minimize the impact on the model's\naccuracy, a process we call accuracy-guided anonymization. We demonstrate that\nby focusing on the model's accuracy rather than generic information loss\nmeasures, our method outperforms state of the art k-anonymity methods in terms\nof the achieved utility, in particular with high values of k and large numbers\nof quasi-identifiers.\n\nWe also demonstrate that our approach has a similar, and sometimes even\nbetter ability to prevent membership inference attacks as approaches based on\ndifferential privacy, while averting some of their drawbacks such as\ncomplexity, performance overhead and model-specific implementations. This makes\nmodel-guided anonymization a legitimate substitute for such methods and a\npractical approach to creating privacy-preserving models.",
          "link": "http://arxiv.org/abs/2007.13086",
          "publishedOn": "2021-08-03T02:06:34.716Z",
          "wordCount": 735,
          "title": "Anonymizing Machine Learning Models. (arXiv:2007.13086v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+K_P/0/1/0/all/0/1\">Priyadarshini K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Siddhartha Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borkar_V/0/1/0/all/0/1\">Vivek Borkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Subhasis Chaudhuri</a>",
          "description": "Active metric learning is the problem of incrementally selecting high-utility\nbatches of training data (typically, ordered triplets) to annotate, in order to\nprogressively improve a learned model of a metric over some input domain as\nrapidly as possible. Standard approaches, which independently assess the\ninformativeness of each triplet in a batch, are susceptible to highly\ncorrelated batches with many redundant triplets and hence low overall utility.\nWhile a recent work \\cite{kumari2020batch} proposes batch-decorrelation\nstrategies for metric learning, they rely on ad hoc heuristics to estimate the\ncorrelation between two triplets at a time. We present a novel batch active\nmetric learning method that leverages the Maximum Entropy Principle to learn\nthe least biased estimate of triplet distribution for a given set of prior\nconstraints. To avoid redundancy between triplets, our method collectively\nselects batches with maximum joint entropy, which simultaneously captures both\ninformativeness and diversity. We take advantage of the submodularity of the\njoint entropy function to construct a tractable solution using an efficient\ngreedy algorithm based on Gram-Schmidt orthogonalization that is provably\n$\\left( 1 - \\frac{1}{e} \\right)$-optimal. Our approach is the first batch\nactive metric learning method to define a unified score that balances\ninformativeness and diversity for an entire batch of triplets. Experiments with\nseveral real-world datasets demonstrate that our algorithm is robust,\ngeneralizes well to different applications and input modalities, and\nconsistently outperforms the state-of-the-art.",
          "link": "http://arxiv.org/abs/2102.07365",
          "publishedOn": "2021-08-03T02:06:34.708Z",
          "wordCount": 736,
          "title": "A Unified Batch Selection Policy for Active Metric Learning. (arXiv:2102.07365v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00774",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Goulart_J/0/1/0/all/0/1\">Jos&#xe9; Henrique de Morais Goulart</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Couillet_R/0/1/0/all/0/1\">Romain Couillet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Comon_P/0/1/0/all/0/1\">Pierre Comon</a>",
          "description": "Tensor models play an increasingly prominent role in many fields, notably in\nmachine learning. In several applications of such models, such as community\ndetection, topic modeling and Gaussian mixture learning, one must estimate a\nlow-rank signal from a noisy tensor. Hence, understanding the fundamental\nlimits and the attainable performance of estimators of that signal inevitably\ncalls for the study of random tensors. Substantial progress has been achieved\non this subject thanks to recent efforts, under the assumption that the tensor\ndimensions grow large. Yet, some of the most significant among these\nresults--in particular, a precise characterization of the abrupt phase\ntransition (in terms of signal-to-noise ratio) that governs the performance of\nthe maximum likelihood (ML) estimator of a symmetric rank-one model with\nGaussian noise--were derived on the basis of statistical physics ideas, which\nare not easily accessible to non-experts.\n\nIn this work, we develop a sharply distinct approach, relying instead on\nstandard but powerful tools brought by years of advances in random matrix\ntheory. The key idea is to study the spectra of random matrices arising from\ncontractions of a given random tensor. We show how this gives access to\nspectral properties of the random tensor itself. In the specific case of a\nsymmetric rank-one model with Gaussian noise, our technique yields a hitherto\nunknown characterization of the local maximum of the ML problem that is global\nabove the phase transition threshold. This characterization is in terms of a\nfixed-point equation satisfied by a formula that had only been previously\nobtained via statistical physics methods. Moreover, our analysis sheds light on\ncertain properties of the landscape of the ML problem in the large-dimensional\nsetting. Our approach is versatile and can be extended to other models, such as\nasymmetric, non-Gaussian and higher-order ones.",
          "link": "http://arxiv.org/abs/2108.00774",
          "publishedOn": "2021-08-03T02:06:34.679Z",
          "wordCount": 737,
          "title": "A Random Matrix Perspective on Random Tensors. (arXiv:2108.00774v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zi_B/0/1/0/all/0/1\">Bin Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xiaoyu Ge</a>",
          "description": "This paper seeks to tackle the bin packing problem (BPP) through a learning\nperspective. Building on self-attention-based encoding and deep reinforcement\nlearning algorithms, we propose a new end-to-end learning model for this task\nof interest. By decomposing the combinatorial action space, as well as\nutilizing a new training technique denoted as prioritized oversampling, which\nis a general scheme to speed up on-policy learning, we achieve state-of-the-art\nperformance in a range of experimental settings. Moreover, although the\nproposed approach attend2pack targets offline-BPP, we strip our method down to\nthe strict online-BPP setting where it is also able to achieve state-of-the-art\nperformance. With a set of ablation studies as well as comparisons against a\nrange of previous works, we hope to offer as a valid baseline approach to this\nfield of study.",
          "link": "http://arxiv.org/abs/2107.04333",
          "publishedOn": "2021-08-03T02:06:34.443Z",
          "wordCount": 608,
          "title": "Attend2Pack: Bin Packing through Deep Reinforcement Learning with Attention. (arXiv:2107.04333v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jie Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_X/0/1/0/all/0/1\">Xuming Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Keyin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1\">Chen Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haiyan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quanying Liu</a>",
          "description": "Machine learning is playing an increasingly important role in medical image\nanalysis, spawning new advances in the clinical application of neuroimaging.\nThere have been some reviews of machine learning and epilepsy before, but they\nmainly focused on electrophysiological signals such as\nelectroencephalography(EEG) or stereo electroencephalography(SEEG), while\nignoring the potential of neuroimaging in epilepsy research. Neuroimaging has\nits important advantages in confirming the range of epileptic region, which\nmeans a lot in presurgical evaluation and assessment after surgery. However,\nEEG is difficult to locate the epilepsy lesion region in the brain. In this\nreview, we emphasize the interaction between neuroimaging and machine learning\nin the context of the epilepsy diagnosis and prognosis. We start with an\noverview of typical neuroimaging modalities used in epilepsy clinics, MRI, DTI,\nfMRI, and PET. Then, we introduce three approaches for applying machine\nlearning methods to neuroimaging data: i) the two-step compositional approach\ncombining feature engineering and machine learning classifiers, ii) the\nend-to-end approach, which is usually toward deep learning, and iii) the hybrid\napproach using the advantages of the two methods. Subsequently, the application\nof machine learning on epilepsy neuroimaging, such as segmentation,\nlocalization and lateralization tasks, as well as tasks directly related to\ndiagnosis and prognosis are introduced in detail. Finally, we discuss the\ncurrent achievements, challenges, and potential future directions in this\nfield, hoping to pave the way for computer-aided diagnosis and prognosis of\nepilepsy.",
          "link": "http://arxiv.org/abs/2102.03336",
          "publishedOn": "2021-08-03T02:06:34.419Z",
          "wordCount": 707,
          "title": "Machine Learning Applications on Neuroimaging for Diagnosis and Prognosis of Epilepsy: A Review. (arXiv:2102.03336v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gottwald_M/0/1/0/all/0/1\">Martin Gottwald</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gronauer_S/0/1/0/all/0/1\">Sven Gronauer</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hao Shen</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Diepold_K/0/1/0/all/0/1\">Klaus Diepold</a> (1) ((1) Technical University of Munich, (2) fortiss)",
          "description": "Recent development of Deep Reinforcement Learning has demonstrated superior\nperformance of neural networks in solving challenging problems with large or\neven continuous state spaces. One specific approach is to deploy neural\nnetworks to approximate value functions by minimising the Mean Squared Bellman\nError function. Despite great successes of Deep Reinforcement Learning,\ndevelopment of reliable and efficient numerical algorithms to minimise the\nBellman Error is still of great scientific interest and practical demand. Such\na challenge is partially due to the underlying optimisation problem being\nhighly non-convex or using incorrect gradient information as done in\nSemi-Gradient algorithms. In this work, we analyse the Mean Squared Bellman\nError from a smooth optimisation perspective combined with a Residual Gradient\nformulation. Our contribution is two-fold.\n\nFirst, we analyse critical points of the error function and provide technical\ninsights on the optimisation procure and design choices for neural networks.\nWhen the existence of global minima is assumed and the objective fulfils\ncertain conditions we can eliminate suboptimal local minima when using\nover-parametrised neural networks. We can construct an efficient Approximate\nNewton's algorithm based on our analysis and confirm theoretical properties of\nthis algorithm such as being locally quadratically convergent to a global\nminimum numerically.\n\nSecond, we demonstrate feasibility and generalisation capabilities of the\nproposed algorithm empirically using continuous control problems and provide a\nnumerical verification of our critical point analysis. We outline the short\ncoming of Semi-Gradients. To benefit from an approximate Newton's algorithm\ncomplete derivatives of the Mean Squared Bellman error must be considered\nduring training.",
          "link": "http://arxiv.org/abs/2106.08774",
          "publishedOn": "2021-08-03T02:06:34.410Z",
          "wordCount": 742,
          "title": "Analysis and Optimisation of Bellman Residual Errors with Neural Function Approximation. (arXiv:2106.08774v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02941",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Theodorou_E/0/1/0/all/0/1\">Evangelos Theodorou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1\">Shengjie Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kang_Y/0/1/0/all/0/1\">Yanfei Kang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Spiliotis_E/0/1/0/all/0/1\">Evangelos Spiliotis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Makridakis_S/0/1/0/all/0/1\">Spyros Makridakis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Assimakopoulos_V/0/1/0/all/0/1\">Vassilios Assimakopoulos</a>",
          "description": "The main objective of the M5 competition, which focused on forecasting the\nhierarchical unit sales of Walmart, was to evaluate the accuracy and\nuncertainty of forecasting methods in the field in order to identify best\npractices and highlight their practical implications. However, whether the\nfindings of the M5 competition can be generalized and exploited by retail firms\nto better support their decisions and operation depends on the extent to which\nthe M5 data is sufficiently similar to unit sales data of retailers that\noperate in different regions, sell different types of products, and consider\ndifferent marketing strategies. To answer this question, we analyze the\ncharacteristics of the M5 time series and compare them with those of two\ngrocery retailers, namely Corporaci\\'on Favorita and a major Greek supermarket\nchain, using feature spaces. Our results suggest that there are only small\ndiscrepancies between the examined data sets, supporting the representativeness\nof the M5 data.",
          "link": "http://arxiv.org/abs/2103.02941",
          "publishedOn": "2021-08-03T02:06:34.396Z",
          "wordCount": 605,
          "title": "Exploring the representativeness of the M5 competition data. (arXiv:2103.02941v2 [stat.AP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Archetti_A/0/1/0/all/0/1\">Alberto Archetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cannici_M/0/1/0/all/0/1\">Marco Cannici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matteucci_M/0/1/0/all/0/1\">Matteo Matteucci</a>",
          "description": "Recently, the trend of incorporating differentiable algorithms into deep\nlearning architectures arose in machine learning research, as the fusion of\nneural layers and algorithmic layers has been beneficial for handling\ncombinatorial data, such as shortest paths on graphs. Recent works related to\ndata-driven planning aim at learning either cost functions or heuristic\nfunctions, but not both. We propose Neural Weighted A*, a differentiable\nanytime planner able to produce improved representations of planar maps as\ngraph costs and heuristics. Training occurs end-to-end on raw images with\ndirect supervision on planning examples, thanks to a differentiable A* solver\nintegrated into the architecture. More importantly, the user can trade off\nplanning accuracy for efficiency at run-time, using a single, real-valued\nparameter. The solution suboptimality is constrained within a linear bound\nequal to the optimal path cost multiplied by the tradeoff parameter. We\nexperimentally show the validity of our claims by testing Neural Weighted A*\nagainst several baselines, introducing a novel, tile-based navigation dataset.\nWe outperform similar architectures in planning accuracy and efficiency.",
          "link": "http://arxiv.org/abs/2105.01480",
          "publishedOn": "2021-08-03T02:06:34.373Z",
          "wordCount": 631,
          "title": "Neural Weighted A*: Learning Graph Costs and Heuristics with Differentiable Anytime A*. (arXiv:2105.01480v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07963",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gassner_A/0/1/0/all/0/1\">Arthur Gassner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rusu_A/0/1/0/all/0/1\">Alexandru Rusu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burg_A/0/1/0/all/0/1\">Andreas Burg</a>",
          "description": "Many applications require accurate indoor localization. Fingerprint-based\nlocalization methods propose a solution to this problem, but rely on a radio\nmap that is effort-intensive to acquire. We automate the radio map acquisition\nphase using a software-defined radio (SDR) and a wheeled robot. Furthermore, we\nopen-source a radio map acquired with our automated tool for a 3GPP Long-Term\nEvolution (LTE) wireless link. To the best of our knowledge, this is the first\npublicly available radio map containing channel state information (CSI).\nFinally, we describe first localization experiments on this radio map using a\nconvolutional neural network to regress for location coordinates.",
          "link": "http://arxiv.org/abs/2104.07963",
          "publishedOn": "2021-08-03T02:06:34.359Z",
          "wordCount": 558,
          "title": "OpenCSI: An Open-Source Dataset for Indoor Localization Using CSI-Based Fingerprinting. (arXiv:2104.07963v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_G/0/1/0/all/0/1\">Guangyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_G/0/1/0/all/0/1\">Guanhong Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1\">Shengwei An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiuling Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shiqing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>",
          "description": "Back-door attack poses a severe threat to deep learning systems. It injects\nhidden malicious behaviors to a model such that any input stamped with a\nspecial pattern can trigger such behaviors. Detecting back-door is hence of\npressing need. Many existing defense techniques use optimization to generate\nthe smallest input pattern that forces the model to misclassify a set of benign\ninputs injected with the pattern to a target label. However, the complexity is\nquadratic to the number of class labels such that they can hardly handle models\nwith many classes. Inspired by Multi-Arm Bandit in Reinforcement Learning, we\npropose a K-Arm optimization method for backdoor detection. By iteratively and\nstochastically selecting the most promising labels for optimization with the\nguidance of an objective function, we substantially reduce the complexity,\nallowing to handle models with many classes. Moreover, by iteratively refining\nthe selection of labels to optimize, it substantially mitigates the uncertainty\nin choosing the right labels, improving detection accuracy. At the time of\nsubmission, the evaluation of our method on over 4000 models in the IARPA\nTrojAI competition from round 1 to the latest round 4 achieves top performance\non the leaderboard. Our technique also supersedes three state-of-the-art\ntechniques in terms of accuracy and the scanning time needed.",
          "link": "http://arxiv.org/abs/2102.05123",
          "publishedOn": "2021-08-03T02:06:34.339Z",
          "wordCount": null,
          "title": "Backdoor Scanning for Deep Neural Networks through K-Arm Optimization. (arXiv:2102.05123v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>",
          "description": "Weak supervision has shown promising results in many natural language\nprocessing tasks, such as Named Entity Recognition (NER). Existing work mainly\nfocuses on learning deep NER models only with weak supervision, i.e., without\nany human annotation, and shows that by merely using weakly labeled data, one\ncan achieve good performance, though still underperforms fully supervised NER\nwith manually/strongly labeled data. In this paper, we consider a more\npractical scenario, where we have both a small amount of strongly labeled data\nand a large amount of weakly labeled data. Unfortunately, we observe that\nweakly labeled data does not necessarily improve, or even deteriorate the model\nperformance (due to the extensive noise in the weak labels) when we train deep\nNER models over a simple or weighted combination of the strongly labeled and\nweakly labeled data. To address this issue, we propose a new multi-stage\ncomputational framework -- NEEDLE with three essential ingredients: (1) weak\nlabel completion, (2) noise-aware loss function, and (3) final fine-tuning over\nthe strongly labeled data. Through experiments on E-commerce query NER and\nBiomedical NER, we demonstrate that NEEDLE can effectively suppress the noise\nof the weak labels and outperforms existing methods. In particular, we achieve\nnew SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74,\nBC5CDR-disease 90.69, NCBI-disease 92.28.",
          "link": "http://arxiv.org/abs/2106.08977",
          "publishedOn": "2021-08-03T02:06:34.336Z",
          "wordCount": 705,
          "title": "Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data. (arXiv:2106.08977v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05082",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Katsnelson_M/0/1/0/all/0/1\">Mikhail I. Katsnelson</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Vanchurin_V/0/1/0/all/0/1\">Vitaly Vanchurin</a>",
          "description": "It was recently shown that the Madelung equations, that is, a hydrodynamic\nform of the Schr\\\"odinger equation, can be derived from a canonical ensemble of\nneural networks where the quantum phase was identified with the free energy of\nhidden variables. We consider instead a grand canonical ensemble of neural\nnetworks, by allowing an exchange of neurons with an auxiliary subsystem, to\nshow that the free energy must also be multivalued. By imposing the\nmultivaluedness condition on the free energy we derive the Schr\\\"odinger\nequation with \"Planck's constant\" determined by the chemical potential of\nhidden variables. This shows that quantum mechanics provides a correct\nstatistical description of the dynamics of the grand canonical ensemble of\nneural networks at the learning equilibrium. We also discuss implications of\nthe results for machine learning, fundamental physics and, in a more\nspeculative way, evolutionary biology.",
          "link": "http://arxiv.org/abs/2012.05082",
          "publishedOn": "2021-08-03T02:06:34.320Z",
          "wordCount": 603,
          "title": "Emergent Quantumness in Neural Networks. (arXiv:2012.05082v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zihan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "Binary neural networks (BNNs) have received increasing attention due to their\nsuperior reductions of computation and memory. Most existing works focus on\neither lessening the quantization error by minimizing the gap between the\nfull-precision weights and their binarization or designing a gradient\napproximation to mitigate the gradient mismatch, while leaving the \"dead\nweights\" untouched. This leads to slow convergence when training BNNs. In this\npaper, for the first time, we explore the influence of \"dead weights\" which\nrefer to a group of weights that are barely updated during the training of\nBNNs, and then introduce rectified clamp unit (ReCU) to revive the \"dead\nweights\" for updating. We prove that reviving the \"dead weights\" by ReCU can\nresult in a smaller quantization error. Besides, we also take into account the\ninformation entropy of the weights, and then mathematically analyze why the\nweight standardization can benefit BNNs. We demonstrate the inherent\ncontradiction between minimizing the quantization error and maximizing the\ninformation entropy, and then propose an adaptive exponential scheduler to\nidentify the range of the \"dead weights\". By considering the \"dead weights\",\nour method offers not only faster BNN training, but also state-of-the-art\nperformance on CIFAR-10 and ImageNet, compared with recent methods. Code can be\navailable at https://github.com/z-hXu/ReCU.",
          "link": "http://arxiv.org/abs/2103.12369",
          "publishedOn": "2021-08-03T02:06:34.310Z",
          "wordCount": 696,
          "title": "ReCU: Reviving the Dead Weights in Binary Neural Networks. (arXiv:2103.12369v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1\">Naveed Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1\">Navid Kardan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>",
          "description": "Deep Learning (DL) is the most widely used tool in the contemporary field of\ncomputer vision. Its ability to accurately solve complex problems is employed\nin vision research to learn deep neural models for a variety of tasks,\nincluding security critical applications. However, it is now known that DL is\nvulnerable to adversarial attacks that can manipulate its predictions by\nintroducing visually imperceptible perturbations in images and videos. Since\nthe discovery of this phenomenon in 2013~[1], it has attracted significant\nattention of researchers from multiple sub-fields of machine intelligence. In\n[2], we reviewed the contributions made by the computer vision community in\nadversarial attacks on deep learning (and their defenses) until the advent of\nyear 2018. Many of those contributions have inspired new directions in this\narea, which has matured significantly since witnessing the first generation\nmethods. Hence, as a legacy sequel of [2], this literature review focuses on\nthe advances in this area since 2018. To ensure authenticity, we mainly\nconsider peer-reviewed contributions published in the prestigious sources of\ncomputer vision and machine learning research. Besides a comprehensive\nliterature review, the article also provides concise definitions of technical\nterminologies for non-experts in this domain. Finally, this article discusses\nchallenges and future outlook of this direction based on the literature\nreviewed herein and [2].",
          "link": "http://arxiv.org/abs/2108.00401",
          "publishedOn": "2021-08-03T02:06:34.290Z",
          "wordCount": 673,
          "title": "Threat of Adversarial Attacks on Deep Learning in Computer Vision: Survey II. (arXiv:2108.00401v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.06308",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Deng_X/0/1/0/all/0/1\">Xiangwen Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1\">Junlin Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shangming Yang</a>",
          "description": "Emotion recognition based on EEG (electroencephalography) has been widely\nused in human-computer interaction, distance education and health care.\nHowever, the conventional methods ignore the adjacent and symmetrical\ncharacteristics of EEG signals, which also contain salient information related\nto emotion. In this paper, a spatial folding ensemble network (SFE-Net) is\npresented for EEG feature extraction and emotion recognition. Firstly, for the\nundetected area between EEG electrodes, an improved Bicubic-EEG interpolation\nalgorithm is developed for EEG channels information completion, which allows us\nto extract a wider range of adjacent space features. Then, motivated by the\nspatial symmetric mechanism of human brain, we fold the input EEG channels data\nwith five different symmetrical strategies, which enable the proposed network\nto extract the information of space features of EEG signals more effectively.\nFinally, a 3DCNN-based spatial, temporal extraction, and a multi-voting\nstrategy of ensemble learning are integrated to model a new neural network.\nWith this network, the spatial features of different symmetric folding signals\ncan be extracted simultaneously, which greatly improves the robustness and\naccuracy of emotion recognition. The experimental results on DEAP and SEED\ndatasets show that the proposed algorithm has comparable performance in terms\nof recognition accuracy.",
          "link": "http://arxiv.org/abs/2104.06308",
          "publishedOn": "2021-08-03T02:06:34.284Z",
          "wordCount": 673,
          "title": "SFE-Net: EEG-based Emotion Recognition with Symmetrical Spatial Feature Extraction. (arXiv:2104.06308v4 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1\">Qili Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shihang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wohlberg_B/0/1/0/all/0/1\">Brendt Wohlberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Youzuo Lin</a>",
          "description": "Seismic full-waveform inversion (FWI) techniques aim to find a\nhigh-resolution subsurface geophysical model provided with waveform data. Some\nrecent effort in data-driven FWI has shown some encouraging results in\nobtaining 2D velocity maps. However, due to high computational complexity and\nlarge memory consumption, the reconstruction of 3D high-resolution velocity\nmaps via deep networks is still a great challenge. In this paper, we present\nInversionNet3D, an efficient and scalable encoder-decoder network for 3D FWI.\nThe proposed method employs group convolution in the encoder to establish an\neffective hierarchy for learning information from multiple sources while\ncutting down unnecessary parameters and operations at the same time. The\nintroduction of invertible layers further reduces the memory consumption of\nintermediate features during training and thus enables the development of\ndeeper networks with more layers and higher capacity as required by different\napplication scenarios. Experiments on the 3D Kimberlina dataset demonstrate\nthat InversionNet3D achieves state-of-the-art reconstruction performance with\nlower computational cost and lower memory footprint compared to the baseline.",
          "link": "http://arxiv.org/abs/2103.14158",
          "publishedOn": "2021-08-03T02:06:34.278Z",
          "wordCount": 633,
          "title": "InversionNet3D: Efficient and Scalable Learning for 3D Full Waveform Inversion. (arXiv:2103.14158v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1\">Daniel Berleant</a>",
          "description": "The accuracy of DL classifiers is unstable in that it often changes\nsignificantly when retested on adversarial images, imperfect images, or\nperturbed images. This paper adds to the small but fundamental body of work on\nbenchmarking the robustness of DL classifiers on defective images. Unlike\nexisted single-factor digital perturbation work, we provide state-of-the-art\ntwo-factor perturbation that provides two natural perturbations on images\napplied in different sequences. The two-factor perturbation includes (1) two\ndigital perturbations (Salt & pepper noise and Gaussian noise) applied in both\nsequences. (2) one digital perturbation (salt & pepper noise) and a geometric\nperturbation (rotation) applied in different sequences. To measure robust DL\nclassifiers, previous scientists provided 15 types of single-factor corruption.\nWe created 69 benchmarking image sets, including a clean set, sets with single\nfactor perturbations, and sets with two-factor perturbation conditions. To be\nbest of our knowledge, this is the first report that two-factor perturbed\nimages improves both robustness and accuracy of DL classifiers. Previous\nresearch evaluating deep learning (DL) classifiers has often used top-1/top-5\naccuracy, so researchers have usually offered tables, line diagrams, and bar\ncharts to display accuracy of DL classifiers. But these existed approaches\ncannot quantitively evaluate robustness of DL classifiers. We innovate a new\ntwo-dimensional, statistical visualization tool, including mean accuracy and\ncoefficient of variation (CV), to benchmark the robustness of DL classifiers.\nAll source codes and related image sets are shared on websites\n(this http URL or\nhttps://github.com/daiweiworking/RobustDeepLearningUsingPerturbations ) to\nsupport future academic research and industry projects.",
          "link": "http://arxiv.org/abs/2103.03102",
          "publishedOn": "2021-08-03T02:06:34.272Z",
          "wordCount": 722,
          "title": "Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16336",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Goren_E/0/1/0/all/0/1\">Emily M. Goren</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1\">Ranjan Maitra</a>",
          "description": "Partially recorded data are frequently encountered in many applications and\nusually clustered by first removing incomplete cases or features with missing\nvalues, or by imputing missing values, followed by application of a clustering\nalgorithm to the resulting altered dataset. Here, we develop clustering\nmethodology through a model-based approach using the marginal density for the\nobserved values, assuming a finite mixture model of multivariate $t$\ndistributions. We compare our approximate algorithm to the corresponding full\nexpectation-maximization (EM) approach that considers the missing values in the\nincomplete data set and makes a missing at random (MAR) assumption, as well as\ncase deletion and imputation methods. Since only the observed values are\nutilized, our approach is computationally more efficient than imputation or\nfull EM. Simulation studies demonstrate that our approach has favorable\nrecovery of the true cluster partition compared to case deletion and imputation\nunder various missingness mechanisms, and is at least competitive with the full\nEM approach, even when MAR assumptions are violated. Our methodology is\ndemonstrated on a problem of clustering gamma-ray bursts and is implemented at\nhttps://github.com/emilygoren/MixtClust.",
          "link": "http://arxiv.org/abs/2103.16336",
          "publishedOn": "2021-08-03T02:06:34.265Z",
          "wordCount": 671,
          "title": "Fast model-based clustering of partial records. (arXiv:2103.16336v4 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1\">Siyuan Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Ailing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>",
          "description": "Heatmap-based methods dominate in the field of human pose estimation by\nmodelling the output distribution through likelihood heatmaps. In contrast,\nregression-based methods are more efficient but suffer from inferior\nperformance. In this work, we explore maximum likelihood estimation (MLE) to\ndevelop an efficient and effective regression-based methods. From the\nperspective of MLE, adopting different regression losses is making different\nassumptions about the output density function. A density function closer to the\ntrue distribution leads to a better regression performance. In light of this,\nwe propose a novel regression paradigm with Residual Log-likelihood Estimation\n(RLE) to capture the underlying output distribution. Concretely, RLE learns the\nchange of the distribution instead of the unreferenced underlying distribution\nto facilitate the training process. With the proposed reparameterization\ndesign, our method is compatible with off-the-shelf flow models. The proposed\nmethod is effective, efficient and flexible. We show its potential in various\nhuman pose estimation tasks with comprehensive experiments. Compared to the\nconventional regression paradigm, regression with RLE bring 12.4 mAP\nimprovement on MSCOCO without any test-time overhead. Moreover, for the first\ntime, especially on multi-person pose estimation, our regression method is\nsuperior to the heatmap-based methods. Our code is available at\nhttps://github.com/Jeff-sjtu/res-loglikelihood-regression",
          "link": "http://arxiv.org/abs/2107.11291",
          "publishedOn": "2021-08-03T02:06:34.247Z",
          "wordCount": 687,
          "title": "Human Pose Regression with Residual Log-likelihood Estimation. (arXiv:2107.11291v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linnan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_R/0/1/0/all/0/1\">Rodrigo Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tian Guo</a>",
          "description": "Efficient evaluation of a network architecture drawn from a large search\nspace remains a key challenge in Neural Architecture Search (NAS). Vanilla NAS\nevaluates each architecture by training from scratch, which gives the true\nperformance but is extremely time-consuming. Recently, one-shot NAS\nsubstantially reduces the computation cost by training only one supernetwork,\na.k.a. supernet, to approximate the performance of every architecture in the\nsearch space via weight-sharing. However, the performance estimation can be\nvery inaccurate due to the co-adaption among operations. In this paper, we\npropose few-shot NAS that uses multiple supernetworks, called sub-supernet,\neach covering different regions of the search space to alleviate the undesired\nco-adaption. Compared to one-shot NAS, few-shot NAS improves the accuracy of\narchitecture evaluation with a small increase of evaluation cost. With only up\nto 7 sub-supernets, few-shot NAS establishes new SoTAs: on ImageNet, it finds\nmodels that reach 80.5% top-1 accuracy at 600 MB FLOPS and 77.5% top-1 accuracy\nat 238 MFLOPS; on CIFAR10, it reaches 98.72% top-1 accuracy without using extra\ndata or transfer learning. In Auto-GAN, few-shot NAS outperforms the previously\npublished results by up to 20%. Extensive experiments show that few-shot NAS\nsignificantly improves various one-shot methods, including 4 gradient-based and\n6 search-based methods on 3 different tasks in NasBench-201 and\nNasBench1-shot-1.",
          "link": "http://arxiv.org/abs/2006.06863",
          "publishedOn": "2021-08-03T02:06:34.241Z",
          "wordCount": 734,
          "title": "Few-shot Neural Architecture Search. (arXiv:2006.06863v9 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Sibo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Handong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfu Liu</a>",
          "description": "Outlier detection is one of the most popular and continuously rising topics\nin the data mining field due to its crucial academic value and extensive\nindustrial applications. Among different settings, unsupervised outlier\ndetection is the most challenging and practical one, which attracts tremendous\nefforts from diverse perspectives. In this paper, we consider the score-based\noutlier detection category and point out that the performance of current\noutlier detection algorithms might be further boosted by score propagation.\nSpecifically, we propose Infinite Propagation of Outlier Factor (iPOF)\nalgorithm, an extremely and excitingly simple outlier detection booster via\ninfinite propagation. By employing score-based outlier detectors for\ninitialization, iPOF updates each data point's outlier score by averaging the\noutlier factors of its nearest common neighbors. Extensive experimental results\non numerous datasets in various domains demonstrate the effectiveness and\nefficiency of iPOF significantly over several classical and recent\nstate-of-the-art methods. We also provide the parameter analysis on the number\nof neighbors, the unique parameter in iPOF, and different initial outlier\ndetectors for general validation. It is worthy to note that iPOF brings in\npositive improvements ranging from 2% to 46% on the average level, and in some\ncases, iPOF boosts the performance over 3000% over the original outlier\ndetection algorithm.",
          "link": "http://arxiv.org/abs/2108.00360",
          "publishedOn": "2021-08-03T02:06:34.234Z",
          "wordCount": 639,
          "title": "IPOF: An Extremely and Excitingly Simple Outlier Detection Booster via Infinite Propagation. (arXiv:2108.00360v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.01959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bianchini_E/0/1/0/all/0/1\">Elizabeth Bibit Bianchini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Prateek Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salisbury_K/0/1/0/all/0/1\">Kenneth Salisbury</a>",
          "description": "Physical human-robot interactions (pHRI) are less efficient and communicative\nthan human-human interactions, and a key reason is a lack of informative sense\nof touch in robotic systems. Interpreting human touch gestures is a nuanced,\nchallenging task with extreme gaps between human and robot capability. Among\nprior works that demonstrate human touch recognition capability, differences in\nsensors, gesture classes, feature sets, and classification algorithms yield a\nconglomerate of non-transferable results and a glaring lack of a standard. To\naddress this gap, this work presents 1) four proposed touch gesture classes\nthat cover an important subset of the gesture characteristics identified in the\nliterature, 2) the collection of an extensive force dataset on a common pHRI\nrobotic arm with only its internal wrist force-torque sensor, and 3) an\nexhaustive performance comparison of combinations of feature sets and\nclassification algorithms on this dataset. We demonstrate high classification\naccuracies among our proposed gesture definitions on a test set, emphasizing\nthat neural net-work classifiers on the raw data outperform other combinations\nof feature sets and algorithms. The accompanying video is here:\nhttps://youtu.be/gJPVImNKU68",
          "link": "http://arxiv.org/abs/2012.01959",
          "publishedOn": "2021-08-03T02:06:34.206Z",
          "wordCount": 680,
          "title": "Towards Human Haptic Gesture Interpretation for Robotic Systems. (arXiv:2012.01959v4 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jyun-Li Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_W/0/1/0/all/0/1\">Wei Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shang-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_P/0/1/0/all/0/1\">Ping-Chun Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xi Liu</a>",
          "description": "Action-constrained reinforcement learning (RL) is a widely-used approach in\nvarious real-world applications, such as scheduling in networked systems with\nresource constraints and control of a robot with kinematic constraints. While\nthe existing projection-based approaches ensure zero constraint violation, they\ncould suffer from the zero-gradient problem due to the tight coupling of the\npolicy gradient and the projection, which results in sample-inefficient\ntraining and slow convergence. To tackle this issue, we propose a learning\nalgorithm that decouples the action constraints from the policy parameter\nupdate by leveraging state-wise Frank-Wolfe and a regression-based policy\nupdate scheme. Moreover, we show that the proposed algorithm enjoys convergence\nand policy improvement properties in the tabular case as well as generalizes\nthe popular DDPG algorithm for action-constrained RL in the general case.\nThrough experiments, we demonstrate that the proposed algorithm significantly\noutperforms the benchmark methods on a variety of control tasks.",
          "link": "http://arxiv.org/abs/2102.11055",
          "publishedOn": "2021-08-03T02:06:34.200Z",
          "wordCount": 618,
          "title": "Escaping from Zero Gradient: Revisiting Action-Constrained Reinforcement Learning via Frank-Wolfe Policy Optimization. (arXiv:2102.11055v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bock_M/0/1/0/all/0/1\">Marius Bock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoelzemann_A/0/1/0/all/0/1\">Alexander Hoelzemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1\">Michael Moeller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laerhoven_K/0/1/0/all/0/1\">Kristof Van Laerhoven</a>",
          "description": "Recent studies in Human Activity Recognition (HAR) have shown that Deep\nLearning methods are able to outperform classical Machine Learning algorithms.\nOne popular Deep Learning architecture in HAR is the DeepConvLSTM. In this\npaper we propose to alter the DeepConvLSTM architecture to employ a 1-layered\ninstead of a 2-layered LSTM. We validate our architecture change on 5 publicly\navailable HAR datasets by comparing the predictive performance with and without\nthe change employing varying hidden units within the LSTM layer(s). Results\nshow that across all datasets, our architecture consistently improves on the\noriginal one: Recognition performance increases up to 11.7% for the F1-score,\nand our architecture significantly decreases the amount of learnable\nparameters. This improvement over DeepConvLSTM decreases training time by as\nmuch as 48%. Our results stand in contrast to the belief that one needs at\nleast a 2-layered LSTM when dealing with sequential data. Based on our results\nwe argue that said claim might not be applicable to sensor-based HAR.",
          "link": "http://arxiv.org/abs/2108.00702",
          "publishedOn": "2021-08-03T02:06:34.169Z",
          "wordCount": 614,
          "title": "Improving Deep Learning for HAR with shallow LSTMs. (arXiv:2108.00702v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2101.08398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1\">Mustafa Hajij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batayneh_F/0/1/0/all/0/1\">Fawwaz Batayneh</a>",
          "description": "Topological Data Analysis (TDA) has emerged recently as a robust tool to\nextract and compare the structure of datasets. TDA identifies features in data\nsuch as connected components and holes and assigns a quantitative measure to\nthese features. Several studies reported that topological features extracted by\nTDA tools provide unique information about the data, discover new insights, and\ndetermine which feature is more related to the outcome. On the other hand, the\noverwhelming success of deep neural networks in learning patterns and\nrelationships has been proven on a vast array of data applications, images in\nparticular. To capture the characteristics of both powerful tools, we propose\n\\textit{TDA-Net}, a novel ensemble network that fuses topological and deep\nfeatures for the purpose of enhancing model generalizability and accuracy. We\napply the proposed \\textit{TDA-Net} to a critical application, which is the\nautomated detection of COVID-19 from CXR images. The experimental results\nshowed that the proposed network achieved excellent performance and suggests\nthe applicability of our method in practice.",
          "link": "http://arxiv.org/abs/2101.08398",
          "publishedOn": "2021-08-03T02:06:34.163Z",
          "wordCount": 718,
          "title": "TDA-Net: Fusion of Persistent Homology and Deep Learning Features for COVID-19 Detection in Chest X-Ray Images. (arXiv:2101.08398v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manneschi_L/0/1/0/all/0/1\">Luca Manneschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_M/0/1/0/all/0/1\">Matthew O. A. Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gigante_G/0/1/0/all/0/1\">Guido Gigante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1\">Andrew C. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giudice_P/0/1/0/all/0/1\">Paolo Del Giudice</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilaki_E/0/1/0/all/0/1\">Eleni Vasilaki</a>",
          "description": "Echo state networks (ESNs) are a powerful form of reservoir computing that\nonly require training of linear output weights whilst the internal reservoir is\nformed of fixed randomly connected neurons. With a correctly scaled\nconnectivity matrix, the neurons' activity exhibits the echo-state property and\nresponds to the input dynamics with certain timescales. Tuning the timescales\nof the network can be necessary for treating certain tasks, and some\nenvironments require multiple timescales for an efficient representation. Here\nwe explore the timescales in hierarchical ESNs, where the reservoir is\npartitioned into two smaller linked reservoirs with distinct properties. Over\nthree different tasks (NARMA10, a reconstruction task in a volatile\nenvironment, and psMNIST), we show that by selecting the hyper-parameters of\neach partition such that they focus on different timescales, we achieve a\nsignificant performance improvement over a single ESN. Through a linear\nanalysis, and under the assumption that the timescales of the first partition\nare much shorter than the second's (typically corresponding to optimal\noperating conditions), we interpret the feedforward coupling of the partitions\nin terms of an effective representation of the input signal, provided by the\nfirst partition to the second, whereby the instantaneous input signal is\nexpanded into a weighted combination of its time derivatives. Furthermore, we\npropose a data-driven approach to optimise the hyper-parameters through a\ngradient descent optimisation method that is an online approximation of\nbackpropagation through time. We demonstrate the application of the online\nlearning rule across all the tasks considered.",
          "link": "http://arxiv.org/abs/2101.04223",
          "publishedOn": "2021-08-03T02:06:34.152Z",
          "wordCount": 733,
          "title": "Exploiting Multiple Timescales in Hierarchical Echo State Networks. (arXiv:2101.04223v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ye Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brinker_F/0/1/0/all/0/1\">Frank Brinker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decking_W/0/1/0/all/0/1\">Winfried Decking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomin_S/0/1/0/all/0/1\">Sergey Tomin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlarb_H/0/1/0/all/0/1\">Holger Schlarb</a>",
          "description": "Modeling of large-scale research facilities is extremely challenging due to\ncomplex physical processes and engineering problems. Here, we adopt a\ndata-driven approach to model the longitudinal phase-space diagnostic beamline\nat the photoinector of the European XFEL with an encoder-decoder neural network\nmodel. A deep convolutional neural network (decoder) is used to build images\nmeasured on the screen from a small feature map generated by another neural\nnetwork (encoder). We demonstrate that the model trained only with experimental\ndata can make high-fidelity predictions of megapixel images for the\nlongitudinal phase-space measurement without any prior knowledge of\nphotoinjectors and electron beams. The prediction significantly outperforms\nexisting methods. We also show the scalability and interpretability of the\nmodel by sharing the same decoder with more than one encoder used for different\nsetups of the photoinjector, and propose a pragmatic way to model a facility\nwith various diagnostics and working points. This opens the door to a new way\nof accurately modeling a photoinjector using neural networks and experimental\ndata. The approach can possibly be extended to the whole accelerator and even\nother types of scientific facilities.",
          "link": "http://arxiv.org/abs/2101.10437",
          "publishedOn": "2021-08-03T02:06:34.138Z",
          "wordCount": 680,
          "title": "High-fidelity Prediction of Megapixel Longitudinal Phase-space Images of Electron Beams using Encoder-Decoder Neural Networks. (arXiv:2101.10437v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiawei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jinyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bin Cui</a>",
          "description": "Hyperparameter optimization (HPO) is a fundamental problem in automatic\nmachine learning (AutoML). However, due to the expensive evaluation cost of\nmodels (e.g., training deep learning models or training models on large\ndatasets), vanilla Bayesian optimization (BO) is typically computationally\ninfeasible. To alleviate this issue, Hyperband (HB) utilizes the early stopping\nmechanism to speed up configuration evaluations by terminating those\nbadly-performing configurations in advance. This leads to two kinds of quality\nmeasurements: (1) many low-fidelity measurements for configurations that get\nearly-stopped, and (2) few high-fidelity measurements for configurations that\nare evaluated without being early stopped. The state-of-the-art HB-style\nmethod, BOHB, aims to combine the benefits of both BO and HB. Instead of\nsampling configurations randomly in HB, BOHB samples configurations based on a\nBO surrogate model, which is constructed with the high-fidelity measurements\nonly. However, the scarcity of high-fidelity measurements greatly hampers the\nefficiency of BO to guide the configuration search. In this paper, we present\nMFES-HB, an efficient Hyperband method that is capable of utilizing both the\nhigh-fidelity and low-fidelity measurements to accelerate the convergence of\nHPO tasks. Designing MFES-HB is not trivial as the low-fidelity measurements\ncan be biased yet informative to guide the configuration search. Thus we\npropose to build a Multi- Fidelity Ensemble Surrogate (MFES) based on the\ngeneralized Product of Experts framework, which can integrate useful\ninformation from multi-fidelity measurements effectively. The empirical studies\non the real-world AutoML tasks demonstrate that MFES-HB can achieve 3.3-8.9x\nspeedups over the state-of-the-art approach - BOHB.",
          "link": "http://arxiv.org/abs/2012.03011",
          "publishedOn": "2021-08-03T02:06:34.125Z",
          "wordCount": 718,
          "title": "MFES-HB: Efficient Hyperband with Multi-Fidelity Quality Measurements. (arXiv:2012.03011v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.10420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1\">Gustavo Henrique de Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcao_A/0/1/0/all/0/1\">Alexandre Xavier Falc&#xe3;o</a>",
          "description": "Machine learning techniques have been paramount throughout the last years,\nbeing applied in a wide range of tasks, such as classification, object\nrecognition, person identification, and image segmentation. Nevertheless,\nconventional classification algorithms, e.g., Logistic Regression, Decision\nTrees, and Bayesian classifiers, might lack complexity and diversity, not\nsuitable when dealing with real-world data. A recent graph-inspired classifier,\nknown as the Optimum-Path Forest, has proven to be a state-of-the-art\ntechnique, comparable to Support Vector Machines and even surpassing it in some\ntasks. This paper proposes a Python-based Optimum-Path Forest framework,\ndenoted as OPFython, where all of its functions and classes are based upon the\noriginal C language implementation. Additionally, as OPFython is a Python-based\nlibrary, it provides a more friendly environment and a faster prototyping\nworkspace than the C language.",
          "link": "http://arxiv.org/abs/2001.10420",
          "publishedOn": "2021-08-03T02:06:34.103Z",
          "wordCount": 628,
          "title": "OPFython: A Python-Inspired Optimum-Path Forest Classifier. (arXiv:2001.10420v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10970",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Chia Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1\">Marina Meil&#x103;</a>",
          "description": "The null space of the $k$-th order Laplacian $\\mathbf{\\mathcal L}_k$, known\nas the {\\em $k$-th homology vector space}, encodes the non-trivial topology of\na manifold or a network. Understanding the structure of the homology embedding\ncan thus disclose geometric or topological information from the data. The study\nof the null space embedding of the graph Laplacian $\\mathbf{\\mathcal L}_0$ has\nspurred new research and applications, such as spectral clustering algorithms\nwith theoretical guarantees and estimators of the Stochastic Block Model. In\nthis work, we investigate the geometry of the $k$-th homology embedding and\nfocus on cases reminiscent of spectral clustering. Namely, we analyze the {\\em\nconnected sum} of manifolds as a perturbation to the direct sum of their\nhomology embeddings. We propose an algorithm to factorize the homology\nembedding into subspaces corresponding to a manifold's simplest topological\ncomponents. The proposed framework is applied to the {\\em shortest homologous\nloop detection} problem, a problem known to be NP-hard in general. Our spectral\nloop detection algorithm scales better than existing methods and is effective\non diverse data such as point clouds and images.",
          "link": "http://arxiv.org/abs/2107.10970",
          "publishedOn": "2021-08-03T02:06:34.089Z",
          "wordCount": 642,
          "title": "The decomposition of the higher-order homology embedding constructed from the $k$-Laplacian. (arXiv:2107.10970v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahapatra_D/0/1/0/all/0/1\">Debabrata Mahapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_V/0/1/0/all/0/1\">Vaibhav Rajan</a>",
          "description": "Multi-Task Learning (MTL) is a well-established paradigm for training deep\nneural network models for multiple correlated tasks. Often the task objectives\nconflict, requiring trade-offs between them during model building. In such\ncases, MTL models can use gradient-based multi-objective optimization (MOO) to\nfind one or more Pareto optimal solutions. A common requirement in MTL\napplications is to find an {\\it Exact} Pareto optimal (EPO) solution, which\nsatisfies user preferences with respect to task-specific objective functions.\nFurther, to improve model generalization, various constraints on the weights\nmay need to be enforced during training. Addressing these requirements is\nchallenging because it requires a search direction that allows descent not only\ntowards the Pareto front but also towards the input preference, within the\nconstraints imposed and in a manner that scales to high-dimensional gradients.\nWe design and theoretically analyze such search directions and develop the\nfirst scalable algorithm, with theoretical guarantees of convergence, to find\nan EPO solution, including when box and equality constraints are imposed. Our\nunique method combines multiple gradient descent with carefully controlled\nascent to traverse the Pareto front in a principled manner, making it robust to\ninitialization. This also facilitates systematic exploration of the Pareto\nfront, that we utilize to approximate the Pareto front for multi-criteria\ndecision-making. Empirical results show that our algorithm outperforms\ncompeting methods on benchmark MTL datasets and MOO problems.",
          "link": "http://arxiv.org/abs/2108.00597",
          "publishedOn": "2021-08-03T02:06:34.081Z",
          "wordCount": 660,
          "title": "Exact Pareto Optimal Search for Multi-Task Learning: Touring the Pareto Front. (arXiv:2108.00597v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Anindita Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheema_N/0/1/0/all/0/1\">Noshaba Cheema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_C/0/1/0/all/0/1\">Cennet Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slusallek_P/0/1/0/all/0/1\">Philipp Slusallek</a>",
          "description": "\"How can we animate 3D-characters from a movie script or move robots by\nsimply telling them what we would like them to do?\" \"How unstructured and\ncomplex can we make a sentence and still generate plausible movements from it?\"\nThese are questions that need to be answered in the long-run, as the field is\nstill in its infancy. Inspired by these problems, we present a new technique\nfor generating compositional actions, which handles complex input sentences.\nOur output is a 3D pose sequence depicting the actions in the input sentence.\nWe propose a hierarchical two-stream sequential model to explore a finer\njoint-level mapping between natural language sentences and 3D pose sequences\ncorresponding to the given motion. We learn two manifold representations of the\nmotion -- one each for the upper body and the lower body movements. Our model\ncan generate plausible pose sequences for short sentences describing single\nactions as well as long compositional sentences describing multiple sequential\nand superimposed actions. We evaluate our proposed model on the publicly\navailable KIT Motion-Language Dataset containing 3D pose data with\nhuman-annotated sentences. Experimental results show that our model advances\nthe state-of-the-art on text-based motion synthesis in objective evaluations by\na margin of 50%. Qualitative evaluations based on a user study indicate that\nour synthesized motions are perceived to be the closest to the ground-truth\nmotion captures for both short and compositional sentences.",
          "link": "http://arxiv.org/abs/2103.14675",
          "publishedOn": "2021-08-03T02:06:34.075Z",
          "wordCount": 723,
          "title": "Synthesis of Compositional Animations from Textual Descriptions. (arXiv:2103.14675v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.13203",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruishan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balsubramani_A/0/1/0/all/0/1\">Akshay Balsubramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>",
          "description": "Learning to align multiple datasets is an important problem with many\napplications, and it is especially useful when we need to integrate multiple\nexperiments or correct for confounding. Optimal transport (OT) is a principled\napproach to align datasets, but a key challenge in applying OT is that we need\nto specify a transport cost function that accurately captures how the two\ndatasets are related. Reliable cost functions are typically not available and\npractitioners often resort to using hand-crafted or Euclidean cost even if it\nmay not be appropriate. In this work, we investigate how to learn the cost\nfunction using a small amount of side information which is often available. The\nside information we consider captures subset correspondence -- i.e. certain\nsubsets of points in the two data sets are known to be related. For example, we\nmay have some images labeled as cars in both datasets; or we may have a common\nannotated cell type in single-cell data from two batches. We develop an\nend-to-end optimizer (OT-SI) that differentiates through the Sinkhorn algorithm\nand effectively learns the suitable cost function from side information. On\nsystematic experiments in images, marriage-matching and single-cell RNA-seq,\nour method substantially outperform state-of-the-art benchmarks.",
          "link": "http://arxiv.org/abs/1909.13203",
          "publishedOn": "2021-08-03T02:06:34.064Z",
          "wordCount": 656,
          "title": "Learning transport cost from subset correspondence. (arXiv:1909.13203v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09943",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Nikitin_O/0/1/0/all/0/1\">Oleg Nikitin</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lukyanova_O/0/1/0/all/0/1\">Olga Lukyanova</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kunin_A/0/1/0/all/0/1\">Alex Kunin</a>",
          "description": "Parallels between the signal processing tasks and biological neurons lead to\nan understanding of the principles of self-organized optimization of input\nsignal recognition. In the present paper, we discuss such similarities among\nbiological and technical systems. We propose adding the well-known STDP\nsynaptic plasticity rule to direct the weight modification towards the state\nassociated with the maximal difference between background noise and correlated\nsignals. We use the principle of physically constrained weight growth as a\nbasis for such weights' modification control. It is proposed that the existence\nand production of bio-chemical 'substances' needed for plasticity development\nrestrict a biological synaptic straight modification. In this paper, the\ninformation about the noise-to-signal ratio controls such a substances'\nproduction and storage and drives the neuron's synaptic pressures towards the\nstate with the best signal-to-noise ratio. We consider several experiments with\ndifferent input signal regimes to understand the functioning of the proposed\napproach.",
          "link": "http://arxiv.org/abs/2104.09943",
          "publishedOn": "2021-08-03T02:06:34.039Z",
          "wordCount": 639,
          "title": "The principle of weight divergence facilitation for unsupervised pattern recognition in spiking neural networks. (arXiv:2104.09943v2 [q-bio.NC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05842",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dwivedi_R/0/1/0/all/0/1\">Raaz Dwivedi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>",
          "description": "We introduce kernel thinning, a new procedure for compressing a distribution\n$\\mathbb{P}$ more effectively than i.i.d. sampling or standard thinning. Given\na suitable reproducing kernel $\\mathbf{k}$ and $\\mathcal{O}(n^2)$ time, kernel\nthinning compresses an $n$-point approximation to $\\mathbb{P}$ into a\n$\\sqrt{n}$-point approximation with comparable worst-case integration error\nacross the associated reproducing kernel Hilbert space. With high probability,\nthe maximum discrepancy in integration error is\n$\\mathcal{O}_d(n^{-\\frac{1}{2}}\\sqrt{\\log n})$ for compactly supported\n$\\mathbb{P}$ and $\\mathcal{O}_d(n^{-\\frac{1}{2}} \\sqrt{(\\log n)^{d+1}\\log\\log\nn})$ for sub-exponential $\\mathbb{P}$ on $\\mathbb{R}^d$. In contrast, an\nequal-sized i.i.d. sample from $\\mathbb{P}$ suffers $\\Omega(n^{-\\frac14})$\nintegration error. Our sub-exponential guarantees resemble the classical\nquasi-Monte Carlo error rates for uniform $\\mathbb{P}$ on $[0,1]^d$ but apply\nto general distributions on $\\mathbb{R}^d$ and a wide range of common kernels.\nWe use our results to derive explicit non-asymptotic maximum mean discrepancy\nbounds for Gaussian, Mat\\'ern, and B-spline kernels and present two vignettes\nillustrating the practical benefits of kernel thinning over i.i.d. sampling and\nstandard Markov chain Monte Carlo thinning, in dimensions $d=2$ through $100$.",
          "link": "http://arxiv.org/abs/2105.05842",
          "publishedOn": "2021-08-03T02:06:34.027Z",
          "wordCount": 644,
          "title": "Kernel Thinning. (arXiv:2105.05842v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_K/0/1/0/all/0/1\">Kalpit Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_V/0/1/0/all/0/1\">Vipul Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Sonu Kumar Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">Mohit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Sachchida Nand Tripathi</a>",
          "description": "Low-cost particulate matter sensors are transforming air quality monitoring\nbecause they have lower costs and greater mobility as compared to reference\nmonitors. Calibration of these low-cost sensors requires training data from\nco-deployed reference monitors. Machine Learning based calibration gives better\nperformance than conventional techniques, but requires a large amount of\ntraining data from the sensor, to be calibrated, co-deployed with a reference\nmonitor. In this work, we propose novel transfer learning methods for quick\ncalibration of sensors with minimal co-deployment with reference monitors.\nTransfer learning utilizes a large amount of data from other sensors along with\na limited amount of data from the target sensor. Our extensive experimentation\nfinds the proposed Model-Agnostic- Meta-Learning (MAML) based transfer learning\nmethod to be the most effective over other competitive baselines.",
          "link": "http://arxiv.org/abs/2108.00640",
          "publishedOn": "2021-08-03T02:06:34.021Z",
          "wordCount": 580,
          "title": "Few-shot calibration of low-cost air pollution (PM2.5) sensors using meta-learning. (arXiv:2108.00640v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jing Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Deqing Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shouhuai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yichen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hai Jin</a>",
          "description": "Automatically detecting software vulnerabilities in source code is an\nimportant problem that has attracted much attention. In particular, deep\nlearning-based vulnerability detectors, or DL-based detectors, are attractive\nbecause they do not need human experts to define features or patterns of\nvulnerabilities. However, such detectors' robustness is unclear. In this paper,\nwe initiate the study in this aspect by demonstrating that DL-based detectors\nare not robust against simple code transformations, dubbed attacks in this\npaper, as these transformations may be leveraged for malicious purposes. As a\nfirst step towards making DL-based detectors robust against such attacks, we\npropose an innovative framework, dubbed ZigZag, which is centered at (i)\ndecoupling feature learning and classifier learning and (ii) using a\nZigZag-style strategy to iteratively refine them until they converge to robust\nfeatures and robust classifiers. Experimental results show that the ZigZag\nframework can substantially improve the robustness of DL-based detectors.",
          "link": "http://arxiv.org/abs/2108.00669",
          "publishedOn": "2021-08-03T02:06:33.989Z",
          "wordCount": 589,
          "title": "Towards Making Deep Learning-based Vulnerability Detectors Robust. (arXiv:2108.00669v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00713",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1\">Brennan Nichyporuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardinell_J/0/1/0/all/0/1\">Jillian Cardinell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1\">Justin Szeto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mehta_R/0/1/0/all/0/1\">Raghav Mehta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios Tsaftaris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1\">Douglas L. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>",
          "description": "Many automatic machine learning models developed for focal pathology (e.g.\nlesions, tumours) detection and segmentation perform well, but do not\ngeneralize as well to new patient cohorts, impeding their widespread adoption\ninto real clinical contexts. One strategy to create a more diverse,\ngeneralizable training set is to naively pool datasets from different cohorts.\nSurprisingly, training on this \\it{big data} does not necessarily increase, and\nmay even reduce, overall performance and model generalizability, due to the\nexistence of cohort biases that affect label distributions. In this paper, we\npropose a generalized affine conditioning framework to learn and account for\ncohort biases across multi-source datasets, which we call Source-Conditioned\nInstance Normalization (SCIN). Through extensive experimentation on three\ndifferent, large scale, multi-scanner, multi-centre Multiple Sclerosis (MS)\nclinical trial MRI datasets, we show that our cohort bias adaptation method (1)\nimproves performance of the network on pooled datasets relative to naively\npooling datasets and (2) can quickly adapt to a new cohort by fine-tuning the\ninstance normalization parameters, thus learning the new cohort bias with only\n10 labelled samples.",
          "link": "http://arxiv.org/abs/2108.00713",
          "publishedOn": "2021-08-03T02:06:33.974Z",
          "wordCount": 638,
          "title": "Cohort Bias Adaptation in Aggregated Datasets for Lesion Segmentation. (arXiv:2108.00713v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Y/0/1/0/all/0/1\">Yen Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_B/0/1/0/all/0/1\">Bao Le</a>",
          "description": "Recent breakthroughs in the field of semi-supervised learning have achieved\nresults that match state-of-the-art traditional supervised learning methods.\nMost successful semi-supervised learning approaches in computer vision focus on\nleveraging huge amount of unlabeled data, learning the general representation\nvia data augmentation and transformation, creating pseudo labels, implementing\ndifferent loss functions, and eventually transferring this knowledge to more\ntask-specific smaller models. In this paper, we aim to conduct our analyses on\nthree different aspects of SimCLR, the current state-of-the-art semi-supervised\nlearning framework for computer vision. First, we analyze properties of\ncontrast learning on fine-tuning, as we understand that contrast learning is\nwhat makes this method so successful. Second, we research knowledge\ndistillation through teacher-forcing paradigm. We observe that when the teacher\nand the student share the same base model, knowledge distillation will achieve\nbetter result. Finally, we study how transfer learning works and its\nrelationship with the number of classes on different data sets. Our results\nindicate that transfer learning performs better when number of classes are\nsmaller.",
          "link": "http://arxiv.org/abs/2108.00587",
          "publishedOn": "2021-08-03T02:06:33.969Z",
          "wordCount": 606,
          "title": "Semi-Supervising Learning, Transfer Learning, and Knowledge Distillation with SimCLR. (arXiv:2108.00587v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.00120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuolin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhikuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_H/0/1/0/all/0/1\">Hengzhi Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlas_B/0/1/0/all/0/1\">Bojan Karlas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Heng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>",
          "description": "As machine learning (ML) being applied to many mission-critical scenarios,\ncertifying ML model robustness becomes increasingly important. Many previous\nworks focuses on the robustness of independent ML and ensemble models, and can\nonly certify a very small magnitude of the adversarial perturbation. In this\npaper, we take a different viewpoint and improve learning robustness by going\nbeyond independent ML and ensemble models. We aim at promoting the generic\nSensing-Reasoning machine learning pipeline which contains both the sensing\n(e.g. deep neural networks) and reasoning (e.g. Markov logic networks (MLN))\ncomponents enriched with domain knowledge. Can domain knowledge help improve\nlearning robustness? Can we formally certify the end-to-end robustness of such\nan ML pipeline? We first theoretically analyze the computational complexity of\nchecking the provable robustness in the reasoning component. We then derive the\nprovable robustness bound for several concrete reasoning components. We show\nthat for reasoning components such as MLN and a specific family of Bayesian\nnetworks it is possible to certify the robustness of the whole pipeline even\nwith a large magnitude of perturbation which cannot be certified by existing\nwork. Finally, we conduct extensive real-world experiments on large scale\ndatasets to evaluate the certified robustness for Sensing-Reasoning ML\npipelines.",
          "link": "http://arxiv.org/abs/2003.00120",
          "publishedOn": "2021-08-03T02:06:33.948Z",
          "wordCount": 688,
          "title": "End-to-end Robustness for Sensing-Reasoning Machine Learning Pipelines. (arXiv:2003.00120v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.10933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhuo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Ximeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhongyuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jianfeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1\">Kui Ren</a>",
          "description": "Nowadays, machine learning models, especially neural networks, become\nprevalent in many real-world applications.These models are trained based on a\none-way trip from user data: as long as users contribute their data, there is\nno way to withdraw; and it is well-known that a neural network memorizes its\ntraining data. This contradicts the \"right to be forgotten\" clause of GDPR,\npotentially leading to law violations. To this end, machine unlearning becomes\na popular research topic, which allows users to eliminate memorization of their\nprivate data from a trained machine learning model.In this paper, we propose\nthe first uniform metric called for-getting rate to measure the effectiveness\nof a machine unlearning method. It is based on the concept of membership\ninference and describes the transformation rate of the eliminated data from\n\"memorized\" to \"unknown\" after conducting unlearning. We also propose a novel\nunlearning method calledForsaken. It is superior to previous work in either\nutility or efficiency (when achieving the same forgetting rate). We benchmark\nForsaken with eight standard datasets to evaluate its performance. The\nexperimental results show that it can achieve more than 90\\% forgetting rate on\naverage and only causeless than 5\\% accuracy loss.",
          "link": "http://arxiv.org/abs/2003.10933",
          "publishedOn": "2021-08-03T02:06:33.942Z",
          "wordCount": 681,
          "title": "Learn to Forget: Machine Unlearning via Neuron Masking. (arXiv:2003.10933v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00654",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tadayon_M/0/1/0/all/0/1\">Manie Tadayon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pottie_G/0/1/0/all/0/1\">Greg Pottie</a>",
          "description": "Educational systems have traditionally been evaluated using cross-sectional\nstudies, namely, examining a pretest, posttest, and single intervention.\nAlthough this is a popular approach, it does not model valuable information\nsuch as confounding variables, feedback to students, and other real-world\ndeviations of studies from ideal conditions. Moreover, learning inherently is a\nsequential process and should involve a sequence of interventions. In this\npaper, we propose various experimental and quasi-experimental designs for\neducational systems and quantify them using the graphical model and directed\nacyclic graph (DAG) language. We discuss the applications and limitations of\neach method in education. Furthermore, we propose to model the education system\nas time-varying treatments, confounders, and time-varying\ntreatments-confounders feedback. We show that if we control for a sufficient\nset of confounders and use appropriate inference techniques such as the inverse\nprobability of treatment weighting (IPTW) or g-formula, we can close the\nbackdoor paths and derive the unbiased causal estimate of joint interventions\non the outcome. Finally, we compare the g-formula and IPTW performance and\ndiscuss the pros and cons of using each method.",
          "link": "http://arxiv.org/abs/2108.00654",
          "publishedOn": "2021-08-03T02:06:33.934Z",
          "wordCount": 608,
          "title": "Causal Inference in Educational Systems: A Graphical Modeling Approach. (arXiv:2108.00654v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1\">Zhanghui Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aojun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jing-Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinjiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qingmin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wayne Zhang</a>",
          "description": "Network compression has been widely studied since it is able to reduce the\nmemory and computation cost during inference. However, previous methods seldom\ndeal with complicated structures like residual connections, group/depth-wise\nconvolution and feature pyramid network, where channels of multiple layers are\ncoupled and need to be pruned simultaneously. In this paper, we present a\ngeneral channel pruning approach that can be applied to various complicated\nstructures. Particularly, we propose a layer grouping algorithm to find coupled\nchannels automatically. Then we derive a unified metric based on Fisher\ninformation to evaluate the importance of a single channel and coupled\nchannels. Moreover, we find that inference speedup on GPUs is more correlated\nwith the reduction of memory rather than FLOPs, and thus we employ the memory\nreduction of each channel to normalize the importance. Our method can be used\nto prune any structures including those with coupled channels. We conduct\nextensive experiments on various backbones, including the classic ResNet and\nResNeXt, mobile-friendly MobileNetV2, and the NAS-based RegNet, both on image\nclassification and object detection which is under-explored. Experimental\nresults validate that our method can effectively prune sophisticated networks,\nboosting inference speed without sacrificing accuracy.",
          "link": "http://arxiv.org/abs/2108.00708",
          "publishedOn": "2021-08-03T02:06:33.928Z",
          "wordCount": 649,
          "title": "Group Fisher Pruning for Practical Network Compression. (arXiv:2108.00708v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1909.08610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Felicia Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "Improving the sample efficiency in reinforcement learning has been a\nlong-standing research problem. In this work, we aim to reduce the sample\ncomplexity of existing policy gradient methods. We propose a novel policy\ngradient algorithm called SRVR-PG, which only requires $O(1/\\epsilon^{3/2})$\nepisodes to find an $\\epsilon$-approximate stationary point of the nonconcave\nperformance function $J(\\boldsymbol{\\theta})$ (i.e., $\\boldsymbol{\\theta}$ such\nthat $\\|\\nabla J(\\boldsymbol{\\theta})\\|_2^2\\leq\\epsilon$). This sample\ncomplexity improves the existing result $O(1/\\epsilon^{5/3})$ for stochastic\nvariance reduced policy gradient algorithms by a factor of\n$O(1/\\epsilon^{1/6})$. In addition, we also propose a variant of SRVR-PG with\nparameter exploration, which explores the initial policy parameter from a prior\nprobability distribution. We conduct numerical experiments on classic control\nproblems in reinforcement learning to validate the performance of our proposed\nalgorithms.",
          "link": "http://arxiv.org/abs/1909.08610",
          "publishedOn": "2021-08-03T02:06:33.907Z",
          "wordCount": 610,
          "title": "Sample Efficient Policy Gradient Methods with Recursive Variance Reduction. (arXiv:1909.08610v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_N/0/1/0/all/0/1\">Ng Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochiai_H/0/1/0/all/0/1\">Hideya Ochiai</a>",
          "description": "An attack on deep learning systems where intelligent machines collaborate to\nsolve problems could cause a node in the network to make a mistake on a\ncritical judgment. At the same time, the security and privacy concerns of AI\nhave galvanized the attention of experts from multiple disciplines. In this\nresearch, we successfully mounted adversarial attacks on a federated learning\n(FL) environment using three different datasets. The attacks leveraged\ngenerative adversarial networks (GANs) to affect the learning process and\nstrive to reconstruct the private data of users by learning hidden features\nfrom shared local model parameters. The attack was target-oriented drawing data\nwith distinct class distribution from the CIFAR- 10, MNIST, and Fashion-MNIST\nrespectively. Moreover, by measuring the Euclidean distance between the real\ndata and the reconstructed adversarial samples, we evaluated the performance of\nthe adversary in the learning processes in various scenarios. At last, we\nsuccessfully reconstructed the real data of the victim from the shared global\nmodel parameters with all the applied datasets.",
          "link": "http://arxiv.org/abs/2108.00701",
          "publishedOn": "2021-08-03T02:06:33.900Z",
          "wordCount": 626,
          "title": "Information Stealing in Federated Learning Systems Based on Generative Adversarial Networks. (arXiv:2108.00701v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_K/0/1/0/all/0/1\">Kfir M. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sangwoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1\">Osvaldo Simeone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamai_S/0/1/0/all/0/1\">Shlomo Shamai</a> (Shitz)",
          "description": "Meta-learning, or learning to learn, offers a principled framework for\nfew-shot learning. It leverages data from multiple related learning tasks to\ninfer an inductive bias that enables fast adaptation on a new task. The\napplication of meta-learning was recently proposed for learning how to\ndemodulate from few pilots. The idea is to use pilots received and stored for\noffline use from multiple devices in order to meta-learn an adaptation\nprocedure with the aim of speeding up online training on new devices. Standard\nfrequentist learning, which can yield relatively accurate \"hard\" classification\ndecisions, is known to be poorly calibrated, particularly in the small-data\nregime. Poor calibration implies that the soft scores output by the demodulator\nare inaccurate estimates of the true probability of correct demodulation. In\nthis work, we introduce the use of Bayesian meta-learning via variational\ninference for the purpose of obtaining well-calibrated few-pilot demodulators.\nIn a Bayesian framework, each neural network weight is represented by a\ndistribution, capturing epistemic uncertainty. Bayesian meta-learning optimizes\nover the prior distribution of the weights. The resulting Bayesian ensembles\noffer better calibrated soft decisions, at the computational cost of running\nmultiple instances of the neural network for demodulation. Numerical results\nfor single-input single-output Rayleigh fading channels with transmitter's\nnon-linearities are provided that compare symbol error rate and expected\ncalibration error for both frequentist and Bayesian meta-learning, illustrating\nhow the latter is both more accurate and better-calibrated.",
          "link": "http://arxiv.org/abs/2108.00785",
          "publishedOn": "2021-08-03T02:06:33.888Z",
          "wordCount": 682,
          "title": "Learning to Learn to Demodulate with Uncertainty Quantification via Bayesian Meta-Learning. (arXiv:2108.00785v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00574",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Poderini_D/0/1/0/all/0/1\">Davide Poderini</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Polino_E/0/1/0/all/0/1\">Emanuele Polino</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Rodari_G/0/1/0/all/0/1\">Giovanni Rodari</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Suprano_A/0/1/0/all/0/1\">Alessia Suprano</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chaves_R/0/1/0/all/0/1\">Rafael Chaves</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Sciarrino_F/0/1/0/all/0/1\">Fabio Sciarrino</a>",
          "description": "The violation of a Bell inequality is the paradigmatic example of\ndevice-independent quantum information: the nonclassicality of the data is\ncertified without the knowledge of the functioning of devices. In practice,\nhowever, all Bell experiments rely on the precise understanding of the\nunderlying physical mechanisms. Given that, it is natural to ask: Can one\nwitness nonclassical behaviour in a truly black-box scenario? Here we propose\nand implement, computationally and experimentally, a solution to this ab-initio\ntask. It exploits a robust automated optimization approach based on the\nStochastic Nelder-Mead algorithm. Treating preparation and measurement devices\nas black-boxes, and relying on the observed statistics only, our adaptive\nprotocol approaches the optimal Bell inequality violation after a limited\nnumber of iterations for a variety photonic states, measurement responses and\nBell scenarios. In particular, we exploit it for randomness certification from\nunknown states and measurements. Our results demonstrate the power of automated\nalgorithms, opening a new venue for the experimental implementation of\ndevice-independent quantum technologies.",
          "link": "http://arxiv.org/abs/2108.00574",
          "publishedOn": "2021-08-03T02:06:33.881Z",
          "wordCount": 593,
          "title": "Ab-initio experimental violation of Bell inequalities. (arXiv:2108.00574v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Inturrisi_J/0/1/0/all/0/1\">Jordan Inturrisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoo_S/0/1/0/all/0/1\">Sui Yang Khoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouzani_A/0/1/0/all/0/1\">Abbas Kouzani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagliarella_R/0/1/0/all/0/1\">Riccardo Pagliarella</a>",
          "description": "The activation function is at the heart of a deep neural networks\nnonlinearity; the choice of the function has great impact on the success of\ntraining. Currently, many practitioners prefer the Rectified Linear Unit (ReLU)\ndue to its simplicity and reliability, despite its few drawbacks. While most\nprevious functions proposed to supplant ReLU have been hand-designed, recent\nwork on learning the function during training has shown promising results. In\nthis paper we propose an adaptive piecewise linear activation function, the\nPiecewise Linear Unit (PiLU), which can be learned independently for each\ndimension of the neural network. We demonstrate how PiLU is a generalised\nrectifier unit and note its similarities with the Adaptive Piecewise Linear\nUnits, namely adaptive and piecewise linear. Across a distribution of 30\nexperiments, we show that for the same model architecture, hyperparameters, and\npre-processing, PiLU significantly outperforms ReLU: reducing classification\nerror by 18.53% on CIFAR-10 and 13.13% on CIFAR-100, for a minor increase in\nthe number of neurons. Further work should be dedicated to exploring\ngeneralised piecewise linear units, as well as verifying these results across\nother challenging domains and larger problems.",
          "link": "http://arxiv.org/abs/2108.00700",
          "publishedOn": "2021-08-03T02:06:33.875Z",
          "wordCount": 629,
          "title": "Piecewise Linear Units Improve Deep Neural Networks. (arXiv:2108.00700v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1912.02290",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kessler_S/0/1/0/all/0/1\">Samuel Kessler</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_V/0/1/0/all/0/1\">Vu Nguyen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zohren_S/0/1/0/all/0/1\">Stefan Zohren</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>",
          "description": "We place an Indian Buffet process (IBP) prior over the structure of a\nBayesian Neural Network (BNN), thus allowing the complexity of the BNN to\nincrease and decrease automatically. We further extend this model such that the\nprior on the structure of each hidden layer is shared globally across all\nlayers, using a Hierarchical-IBP (H-IBP). We apply this model to the problem of\nresource allocation in Continual Learning (CL) where new tasks occur and the\nnetwork requires extra resources. Our model uses online variational inference\nwith reparameterisation of the Bernoulli and Beta distributions, which\nconstitute the IBP and H-IBP priors. As we automatically learn the number of\nweights in each layer of the BNN, overfitting and underfitting problems are\nlargely overcome. We show empirically that our approach offers a competitive\nedge over existing methods in CL.",
          "link": "http://arxiv.org/abs/1912.02290",
          "publishedOn": "2021-08-03T02:06:33.856Z",
          "wordCount": 629,
          "title": "Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning. (arXiv:1912.02290v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vassilev_A/0/1/0/all/0/1\">Apostol Vassilev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Munawar Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Honglan Jin</a>",
          "description": "When people try to understand nuanced language they typically process\nmultiple input sensor modalities to complete this cognitive task. It turns out\nthe human brain has even a specialized neuron formation, called sagittal\nstratum, to help us understand sarcasm. We use this biological formation as the\ninspiration for designing a neural network architecture that combines\npredictions of different models on the same text to construct robust, accurate\nand computationally efficient classifiers for sentiment analysis and study\nseveral different realizations. Among them, we propose a systematic new\napproach to combining multiple predictions based on a dedicated neural network\nand develop mathematical analysis of it along with state-of-the-art\nexperimental results. We also propose a heuristic-hybrid technique for\ncombining models and back it up with experimental results on a representative\nbenchmark dataset and comparisons to other methods to show the advantages of\nthe new approaches.",
          "link": "http://arxiv.org/abs/2006.12958",
          "publishedOn": "2021-08-03T02:06:33.850Z",
          "wordCount": 640,
          "title": "Can you tell? SSNet -- a Sagittal Stratum-inspired Neural Network Framework for Sentiment Analysis. (arXiv:2006.12958v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00740",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Soh_Y/0/1/0/all/0/1\">Yong Sheng Soh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Varvitsiotis_A/0/1/0/all/0/1\">Antonios Varvitsiotis</a>",
          "description": "Given a matrix $X\\in \\mathbb{R}^{m\\times n}_+$ with non-negative entries, the\ncone factorization problem over a cone $\\mathcal{K}\\subseteq \\mathbb{R}^k$\nconcerns computing $\\{ a_1,\\ldots, a_{m} \\} \\subseteq \\mathcal{K}$ and $\\{\nb_1,\\ldots, b_{n} \\} \\subseteq~\\mathcal{K}^*$ belonging to its dual so that\n$X_{ij} = \\langle a_i, b_j \\rangle$ for all $i\\in [m], j\\in [n]$. Cone\nfactorizations are fundamental to mathematical optimization as they allow us to\nexpress convex bodies as feasible regions of linear conic programs. In this\npaper, we introduce and analyze the symmetric-cone multiplicative update (SCMU)\nalgorithm for computing cone factorizations when $\\mathcal{K}$ is symmetric;\ni.e., it is self-dual and homogeneous. Symmetric cones are of central interest\nin mathematical optimization as they provide a common language for studying\nlinear optimization over the nonnegative orthant (linear programs), over the\nsecond-order cone (second order cone programs), and over the cone of positive\nsemidefinite matrices (semidefinite programs). The SCMU algorithm is\nmultiplicative in the sense that the iterates are updated by applying a\nmeticulously chosen automorphism of the cone computed using a generalization of\nthe geometric mean to symmetric cones. Using an extension of Lieb's concavity\ntheorem and von Neumann's trace inequality to symmetric cones, we show that the\nsquared loss objective is non-decreasing along the trajectories of the SCMU\nalgorithm. Specialized to the nonnegative orthant, the SCMU algorithm\ncorresponds to the seminal algorithm by Lee and Seung for computing Nonnegative\nMatrix Factorizations.",
          "link": "http://arxiv.org/abs/2108.00740",
          "publishedOn": "2021-08-03T02:06:33.816Z",
          "wordCount": 668,
          "title": "Multiplicative updates for symmetric-cone factorizations. (arXiv:2108.00740v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/1912.04884",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_B/0/1/0/all/0/1\">Benjie Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Webb_S/0/1/0/all/0/1\">Stefan Webb</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1\">Tom Rainforth</a>",
          "description": "Despite their numerous successes, there are many scenarios where adversarial\nrisk metrics do not provide an appropriate measure of robustness. For example,\ntest-time perturbations may occur in a probabilistic manner rather than being\ngenerated by an explicit adversary, while the poor train--test generalization\nof adversarial metrics can limit their usage to simple problems. Motivated by\nthis, we develop a probabilistic robust risk framework, the statistically\nrobust risk (SRR), which considers pointwise corruption distributions, as\nopposed to worst-case adversaries. The SRR provides a distinct and\ncomplementary measure of robust performance, compared to natural and\nadversarial risk. We show that the SRR admits estimation and training schemes\nwhich are as simple and efficient as for the natural risk: these simply require\nnoising the inputs, but with a principled derivation for exactly how and why\nthis should be done. Furthermore, we demonstrate both theoretically and\nexperimentally that it can provide superior generalization performance compared\nwith adversarial risks, enabling application to high-dimensional datasets.",
          "link": "http://arxiv.org/abs/1912.04884",
          "publishedOn": "2021-08-03T02:06:33.811Z",
          "wordCount": 614,
          "title": "Statistically Robust Neural Network Classification. (arXiv:1912.04884v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00568",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guihong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_S/0/1/0/all/0/1\">Sumit K. Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogras_U/0/1/0/all/0/1\">Umit Y. Ogras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marculescu_R/0/1/0/all/0/1\">Radu Marculescu</a>",
          "description": "Neural architecture search (NAS) is a promising technique to design efficient\nand high-performance deep neural networks (DNNs). As the performance\nrequirements of ML applications grow continuously, the hardware accelerators\nstart playing a central role in DNN design. This trend makes NAS even more\ncomplicated and time-consuming for most real applications. This paper proposes\nFLASH, a very fast NAS methodology that co-optimizes the DNN accuracy and\nperformance on a real hardware platform. As the main theoretical contribution,\nwe first propose the NN-Degree, an analytical metric to quantify the\ntopological characteristics of DNNs with skip connections (e.g., DenseNets,\nResNets, Wide-ResNets, and MobileNets). The newly proposed NN-Degree allows us\nto do training-free NAS within one second and build an accuracy predictor by\ntraining as few as 25 samples out of a vast search space with more than 63\nbillion configurations. Second, by performing inference on the target hardware,\nwe fine-tune and validate our analytical models to estimate the latency, area,\nand energy consumption of various DNN architectures while executing standard ML\ndatasets. Third, we construct a hierarchical algorithm based on simplicial\nhomology global optimization (SHGO) to optimize the model-architecture\nco-design process, while considering the area, latency, and energy consumption\nof the target hardware. We demonstrate that, compared to the state-of-the-art\nNAS approaches, our proposed hierarchical SHGO-based algorithm enables more\nthan four orders of magnitude speedup (specifically, the execution time of the\nproposed algorithm is about 0.1 seconds). Finally, our experimental evaluations\nshow that FLASH is easily transferable to different hardware architectures,\nthus enabling us to do NAS on a Raspberry Pi-3B processor in less than 3\nseconds.",
          "link": "http://arxiv.org/abs/2108.00568",
          "publishedOn": "2021-08-03T02:06:33.799Z",
          "wordCount": 711,
          "title": "FLASH: Fast Neural Architecture Search with Hardware Optimization. (arXiv:2108.00568v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wentao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bin Cui</a>",
          "description": "Data selection methods, such as active learning and core-set selection, are\nuseful tools for improving the data efficiency of deep learning models on\nlarge-scale datasets. However, recent deep learning models have moved forward\nfrom independent and identically distributed data to graph-structured data,\nsuch as social networks, e-commerce user-item graphs, and knowledge graphs.\nThis evolution has led to the emergence of Graph Neural Networks (GNNs) that go\nbeyond the models existing data selection methods are designed for. Therefore,\nwe present Grain, an efficient framework that opens up a new perspective\nthrough connecting data selection in GNNs with social influence maximization.\nBy exploiting the common patterns of GNNs, Grain introduces a novel feature\npropagation concept, a diversified influence maximization objective with novel\ninfluence and diversity functions, and a greedy algorithm with an approximation\nguarantee into a unified framework. Empirical studies on public datasets\ndemonstrate that Grain significantly improves both the performance and\nefficiency of data selection (including active learning and core-set selection)\nfor GNNs. To the best of our knowledge, this is the first attempt to bridge two\nlargely parallel threads of research, data selection, and social influence\nmaximization, in the setting of GNNs, paving new ways for improving data\nefficiency.",
          "link": "http://arxiv.org/abs/2108.00219",
          "publishedOn": "2021-08-03T02:06:33.771Z",
          "wordCount": 666,
          "title": "Grain: Improving Data Efficiency of Graph Neural Networks via Diversified Influence Maximization. (arXiv:2108.00219v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Anshul Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_N/0/1/0/all/0/1\">Noveen Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sheshansh Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sumeet Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1\">Purushottam Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1\">Manik Varma</a>",
          "description": "Deep extreme classification (XC) seeks to train deep architectures that can\ntag a data point with its most relevant subset of labels from an extremely\nlarge label set. The core utility of XC comes from predicting labels that are\nrarely seen during training. Such rare labels hold the key to personalized\nrecommendations that can delight and surprise a user. However, the large number\nof rare labels and small amount of training data per rare label offer\nsignificant statistical and computational challenges. State-of-the-art deep XC\nmethods attempt to remedy this by incorporating textual descriptions of labels\nbut do not adequately address the problem. This paper presents ECLARE, a\nscalable deep learning architecture that incorporates not only label text, but\nalso label correlations, to offer accurate real-time predictions within a few\nmilliseconds. Core contributions of ECLARE include a frugal architecture and\nscalable techniques to train deep models along with label correlation graphs at\nthe scale of millions of labels. In particular, ECLARE offers predictions that\nare 2 to 14% more accurate on both publicly available benchmark datasets as\nwell as proprietary datasets for a related products recommendation task sourced\nfrom the Bing search engine. Code for ECLARE is available at\nhttps://github.com/Extreme-classification/ECLARE.",
          "link": "http://arxiv.org/abs/2108.00261",
          "publishedOn": "2021-08-03T02:06:33.753Z",
          "wordCount": 655,
          "title": "ECLARE: Extreme Classification with Label Graph Correlations. (arXiv:2108.00261v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Joy T. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agu_N/0/1/0/all/0/1\">Nkechinyere N. Agu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1\">Ismini Lourentzou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Arjun Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paguio_J/0/1/0/all/0/1\">Joseph A. Paguio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jasper S. Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dee_E/0/1/0/all/0/1\">Edward C. Dee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_W/0/1/0/all/0/1\">William Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashyap_S/0/1/0/all/0/1\">Satyananda Kashyap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giovannini_A/0/1/0/all/0/1\">Andrea Giovannini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1\">Leo A. Celi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Mehdi Moradi</a>",
          "description": "Despite the progress in automatic detection of radiologic findings from chest\nX-ray (CXR) images in recent years, a quantitative evaluation of the\nexplainability of these models is hampered by the lack of locally labeled\ndatasets for different findings. With the exception of a few expert-labeled\nsmall-scale datasets for specific findings, such as pneumonia and pneumothorax,\nmost of the CXR deep learning models to date are trained on global \"weak\"\nlabels extracted from text reports, or trained via a joint image and\nunstructured text learning strategy. Inspired by the Visual Genome effort in\nthe computer vision community, we constructed the first Chest ImaGenome dataset\nwith a scene graph data structure to describe $242,072$ images. Local\nannotations are automatically produced using a joint rule-based natural\nlanguage processing (NLP) and atlas-based bounding box detection pipeline.\nThrough a radiologist constructed CXR ontology, the annotations for each CXR\nare connected as an anatomy-centered scene graph, useful for image-level\nreasoning and multimodal fusion applications. Overall, we provide: i) $1,256$\ncombinations of relation annotations between $29$ CXR anatomical locations\n(objects with bounding box coordinates) and their attributes, structured as a\nscene graph per image, ii) over $670,000$ localized comparison relations (for\nimproved, worsened, or no change) between the anatomical locations across\nsequential exams, as well as ii) a manually annotated gold standard scene graph\ndataset from $500$ unique patients.",
          "link": "http://arxiv.org/abs/2108.00316",
          "publishedOn": "2021-08-03T02:06:33.744Z",
          "wordCount": 697,
          "title": "Chest ImaGenome Dataset for Clinical Reasoning. (arXiv:2108.00316v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1\">Mithun Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1\">Punyajoy Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutt_R/0/1/0/all/0/1\">Ritam Dutt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_B/0/1/0/all/0/1\">Binny Mathew</a>",
          "description": "Hate speech is regarded as one of the crucial issues plaguing the online\nsocial media. The current literature on hate speech detection leverages\nprimarily the textual content to find hateful posts and subsequently identify\nhateful users. However, this methodology disregards the social connections\nbetween users. In this paper, we run a detailed exploration of the problem\nspace and investigate an array of models ranging from purely textual to graph\nbased to finally semi-supervised techniques using Graph Neural Networks (GNN)\nthat utilize both textual and graph-based features. We run exhaustive\nexperiments on two datasets -- Gab, which is loosely moderated and Twitter,\nwhich is strictly moderated. Overall the AGNN model achieves 0.791 macro\nF1-score on the Gab dataset and 0.780 macro F1-score on the Twitter dataset\nusing only 5% of the labeled instances, considerably outperforming all the\nother models including the fully supervised ones. We perform detailed error\nanalysis on the best performing text and graph based models and observe that\nhateful users have unique network neighborhood signatures and the AGNN model\nbenefits by paying attention to these signatures. This property, as we observe,\nalso allows the model to generalize well across platforms in a zero-shot\nsetting. Lastly, we utilize the best performing GNN model to analyze the\nevolution of hateful users and their targets over time in Gab.",
          "link": "http://arxiv.org/abs/2108.00524",
          "publishedOn": "2021-08-03T02:06:33.712Z",
          "wordCount": 694,
          "title": "You too Brutus! Trapping Hateful Users in Social Media: Challenges, Solutions & Insights. (arXiv:2108.00524v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00065",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chee_J/0/1/0/all/0/1\">Jerry Chee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renz_M/0/1/0/all/0/1\">Megan Renz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damle_A/0/1/0/all/0/1\">Anil Damle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1\">Chris De Sa</a>",
          "description": "We introduce a principled approach to neural network pruning that casts the\nproblem as a structured low-rank matrix approximation. Our method uses a novel\napplication of a matrix factorization technique called the interpolative\ndecomposition to approximate the activation output of a network layer. This\ntechnique selects neurons or channels in the layer and propagates a corrective\ninterpolation matrix to the next layer, resulting in a dense, pruned network\nwith minimal degradation before fine tuning. We demonstrate how to prune a\nneural network by first building a set of primitives to prune a single fully\nconnected or convolution layer and then composing these primitives to prune\ndeep multi-layer networks. Theoretical guarantees are provided for pruning a\nsingle hidden layer fully connected network. Pruning with interpolative\ndecompositions achieves strong empirical results compared to the\nstate-of-the-art on multiple applications from one and two hidden layer\nnetworks on Fashion MNIST to VGG and ResNets on CIFAR-10. Notably, we achieve\nan accuracy of 93.62 $\\pm$ 0.36% using VGG-16 on CIFAR-10, with a 51% FLOPS\nreduction. This gains 0.02% from the full-sized model.",
          "link": "http://arxiv.org/abs/2108.00065",
          "publishedOn": "2021-08-03T02:06:33.692Z",
          "wordCount": 613,
          "title": "Pruning Neural Networks with Interpolative Decompositions. (arXiv:2108.00065v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00071",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Ajay Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_D/0/1/0/all/0/1\">Deri Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batarseh_F/0/1/0/all/0/1\">Feras A. Batarseh</a>",
          "description": "Dealing with imbalanced data is a prevalent problem while performing\nclassification on the datasets. Many times, this problem contributes to bias\nwhile making decisions or implementing policies. Thus, it is vital to\nunderstand the factors which cause imbalance in the data (or class imbalance).\nSuch hidden biases and imbalances can lead to data tyranny and a major\nchallenge to a data democracy. In this chapter, two essential statistical\nelements are resolved: the degree of class imbalance and the complexity of the\nconcept; solving such issues helps in building the foundations of a data\ndemocracy. Furthermore, statistical measures which are appropriate in these\nscenarios are discussed and implemented on a real-life dataset (car insurance\nclaims). In the end, popular data-level methods such as random oversampling,\nrandom undersampling, synthetic minority oversampling technique, Tomek link,\nand others are implemented in Python, and their performance is compared.",
          "link": "http://arxiv.org/abs/2108.00071",
          "publishedOn": "2021-08-03T02:06:33.685Z",
          "wordCount": 611,
          "title": "Foundations of data imbalance and solutions for a data democracy. (arXiv:2108.00071v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1\">Adityanarayanan Radhakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanakis_G/0/1/0/all/0/1\">George Stefanakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belkin_M/0/1/0/all/0/1\">Mikhail Belkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uhler_C/0/1/0/all/0/1\">Caroline Uhler</a>",
          "description": "Matrix completion problems arise in many applications including\nrecommendation systems, computer vision, and genomics. Increasingly larger\nneural networks have been successful in many of these applications, but at\nconsiderable computational costs. Remarkably, taking the width of a neural\nnetwork to infinity allows for improved computational performance. In this\nwork, we develop an infinite width neural network framework for matrix\ncompletion that is simple, fast, and flexible. Simplicity and speed come from\nthe connection between the infinite width limit of neural networks and kernels\nknown as neural tangent kernels (NTK). In particular, we derive the NTK for\nfully connected and convolutional neural networks for matrix completion. The\nflexibility stems from a feature prior, which allows encoding relationships\nbetween coordinates of the target matrix, akin to semi-supervised learning. The\neffectiveness of our framework is demonstrated through competitive results for\nvirtual drug screening and image inpainting/reconstruction. We also provide an\nimplementation in Python to make our framework accessible on standard hardware\nto a broad audience.",
          "link": "http://arxiv.org/abs/2108.00131",
          "publishedOn": "2021-08-03T02:06:33.672Z",
          "wordCount": 602,
          "title": "Simple, Fast, and Flexible Framework for Matrix Completion with Infinite Width Neural Networks. (arXiv:2108.00131v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1\">Joseph Marvin Imperial</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1\">Ethel Ong</a>",
          "description": "In order to ensure quality and effective learning, fluency, and\ncomprehension, the proper identification of the difficulty levels of reading\nmaterials should be observed. In this paper, we describe the development of\nautomatic machine learning-based readability assessment models for educational\nFilipino texts using the most diverse set of linguistic features for the\nlanguage. Results show that using a Random Forest model obtained a high\nperformance of 62.7% in terms of accuracy, and 66.1% when using the optimal\ncombination of feature sets consisting of traditional and syllable\npattern-based predictors.",
          "link": "http://arxiv.org/abs/2108.00241",
          "publishedOn": "2021-08-03T02:06:33.666Z",
          "wordCount": 531,
          "title": "Diverse Linguistic Features for Assessing Reading Difficulty of Educational Filipino Texts. (arXiv:2108.00241v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2007.09060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noufi_C/0/1/0/all/0/1\">Camille Noufi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Prateek Verma</a>",
          "description": "In music and speech, meaning is derived at multiple levels of context.\nAffect, for example, can be inferred both by a short sound token and by sonic\npatterns over a longer temporal window such as an entire recording. In this\nletter, we focus on inferring meaning from this dichotomy of contexts. We show\nhow contextual representations of short sung vocal lines can be implicitly\nlearned from fundamental frequency ($F_0$) and thus be used as a meaningful\nfeature space for downstream Music Information Retrieval (MIR) tasks. We\npropose three self-supervised deep learning paradigms which leverage pseudotask\nlearning of these two levels of context to produce latent representation\nspaces. We evaluate the usefulness of these representations by embedding unseen\npitch contours into each space and conducting downstream classification tasks.\nOur results show that contextual representation can enhance downstream\nclassification by as much as 15\\% as compared to using traditional statistical\ncontour features.",
          "link": "http://arxiv.org/abs/2007.09060",
          "publishedOn": "2021-08-03T02:06:33.659Z",
          "wordCount": 632,
          "title": "Self-Supervised Learning of Context-Aware Pitch Prosody Representations. (arXiv:2007.09060v4 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_K/0/1/0/all/0/1\">Kaiyi Ji</a>",
          "description": "Bilevel optimization has become a powerful framework in various machine\nlearning applications including meta-learning, hyperparameter optimization, and\nnetwork architecture search. There are generally two classes of bilevel\noptimization formulations for machine learning: 1) problem-based bilevel\noptimization, whose inner-level problem is formulated as finding a minimizer of\na given loss function; and 2) algorithm-based bilevel optimization, whose\ninner-level solution is an output of a fixed algorithm. For the first class,\ntwo popular types of gradient-based algorithms have been proposed for\nhypergradient estimation via approximate implicit differentiation (AID) and\niterative differentiation (ITD). Algorithms for the second class include the\npopular model-agnostic meta-learning (MAML) and almost no inner loop (ANIL).\nHowever, the convergence rate and fundamental limitations of bilevel\noptimization algorithms have not been well explored.\n\nThis thesis provides a comprehensive convergence rate analysis for bilevel\nalgorithms in the aforementioned two classes. We further propose principled\nalgorithm designs for bilevel optimization with higher efficiency and\nscalability. For the problem-based formulation, we provide a convergence rate\nanalysis for AID- and ITD-based bilevel algorithms. We then develop\nacceleration bilevel algorithms, for which we provide shaper convergence\nanalysis with relaxed assumptions. We also provide the first lower bounds for\nbilevel optimization, and establish the optimality by providing matching upper\nbounds under certain conditions. We finally propose new stochastic bilevel\noptimization algorithms with lower complexity and higher efficiency in\npractice. For the algorithm-based formulation, we develop a theoretical\nconvergence for general multi-step MAML and ANIL, and characterize the impact\nof parameter selections and loss geometries on the their complexities.",
          "link": "http://arxiv.org/abs/2108.00330",
          "publishedOn": "2021-08-03T02:06:33.630Z",
          "wordCount": 705,
          "title": "Bilevel Optimization for Machine Learning: Algorithm Design and Convergence Analysis. (arXiv:2108.00330v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.04395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1\">Xiang Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Saizhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengfei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fangli Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alex X. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chunming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>",
          "description": "While the celebrated graph neural networks yield effective representations\nfor individual nodes of a graph, there has been relatively less success in\nextending to the task of graph similarity learning. Recent work on graph\nsimilarity learning has considered either global-level graph-graph interactions\nor low-level node-node interactions, however ignoring the rich cross-level\ninteractions (e.g., between each node of one graph and the other whole graph).\nIn this paper, we propose a multi-level graph matching network (MGMN) framework\nfor computing the graph similarity between any pair of graph-structured objects\nin an end-to-end fashion. In particular, the proposed MGMN consists of a\nnode-graph matching network for effectively learning cross-level interactions\nbetween each node of one graph and the other whole graph, and a siamese graph\nneural network to learn global-level interactions between two input graphs.\nFurthermore, to compensate for the lack of standard benchmark datasets, we have\ncreated and collected a set of datasets for both the graph-graph classification\nand graph-graph regression tasks with different sizes in order to evaluate the\neffectiveness and robustness of our models. Comprehensive experiments\ndemonstrate that MGMN consistently outperforms state-of-the-art baseline models\non both the graph-graph classification and graph-graph regression tasks.\nCompared with previous work, MGMN also exhibits stronger robustness as the\nsizes of the two input graphs increase.",
          "link": "http://arxiv.org/abs/2007.04395",
          "publishedOn": "2021-08-03T02:06:33.589Z",
          "wordCount": 715,
          "title": "Multi-Level Graph Matching Networks for Deep Graph Similarity Learning. (arXiv:2007.04395v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_W/0/1/0/all/0/1\">Wei W. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiang Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Dan Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weishen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhou Jin</a>",
          "description": "One of the greatest challenges in IC design is the repeated executions of\ncomputationally expensive SPICE simulations, particularly when highly complex\nchip testing/verification is involved. Recently, pseudo transient analysis\n(PTA) has shown to be one of the most promising continuation SPICE solver.\nHowever, the PTA efficiency is highly influenced by the inserted\npseudo-parameters. In this work, we proposed BoA-PTA, a Bayesian optimization\naccelerated PTA that can substantially accelerate simulations and improve\nconvergence performance without introducing extra errors. Furthermore, our\nmethod does not require any pre-computation data or offline training. The\nacceleration framework can either be implemented to speed up ongoing repeated\nsimulations immediately or to improve new simulations of completely different\ncircuits. BoA-PTA is equipped with cutting-edge machine learning techniques,\ne.g., deep learning, Gaussian process, Bayesian optimization, non-stationary\nmonotonic transformation, and variational inference via parameterization. We\nassess BoA-PTA in 43 benchmark circuits against other SOTA SPICE solvers and\ndemonstrate an average 2.3x (maximum 3.5x) speed-up over the original CEPTA.",
          "link": "http://arxiv.org/abs/2108.00257",
          "publishedOn": "2021-08-03T02:06:33.583Z",
          "wordCount": 607,
          "title": "BoA-PTA, A Bayesian Optimization Accelerated Error-Free SPICE Solver. (arXiv:2108.00257v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kubotani_Y/0/1/0/all/0/1\">Yoshiki Kubotani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukuhara_Y/0/1/0/all/0/1\">Yoshihiro Fukuhara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morishima_S/0/1/0/all/0/1\">Shigeo Morishima</a>",
          "description": "A major challenge in the field of education is providing review schedules\nthat present learned items at appropriate intervals to each student so that\nmemory is retained over time. In recent years, attempts have been made to\nformulate item reviews as sequential decision-making problems to realize\nadaptive instruction based on the knowledge state of students. It has been\nreported previously that reinforcement learning can help realize mathematical\nmodels of students learning strategies to maintain a high memory rate. However,\noptimization using reinforcement learning requires a large number of\ninteractions, and thus it cannot be applied directly to actual students. In\nthis study, we propose a framework for optimizing teaching strategies by\nconstructing a virtual model of the student while minimizing the interaction\nwith the actual teaching target. In addition, we conducted an experiment\nconsidering actual instructions using the mathematical model and confirmed that\nthe model performance is comparable to that of conventional teaching methods.\nOur framework can directly substitute mathematical models used in experiments\nwith human students, and our results can serve as a buffer between theoretical\ninstructional optimization and practical applications in e-learning systems.",
          "link": "http://arxiv.org/abs/2108.00268",
          "publishedOn": "2021-08-03T02:06:33.576Z",
          "wordCount": 650,
          "title": "RLTutor: Reinforcement Learning Based Adaptive Tutoring System by Modeling Virtual Student with Fewer Interactions. (arXiv:2108.00268v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Ningyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xuefeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yi Xiong</a>",
          "description": "It has been recently shown in the literature that the sample averages from\nonline learning experiments are biased when used to estimate the mean reward.\nTo correct the bias, off-policy evaluation methods, including importance\nsampling and doubly robust estimators, typically calculate the propensity\nscore, which is unavailable in this setting due to unknown reward distribution\nand the adaptive policy. This paper provides a procedure to debias the samples\nusing bootstrap, which doesn't require the knowledge of the reward distribution\nat all. Numerical experiments demonstrate the effective bias reduction for\nsamples generated by popular multi-armed bandit algorithms such as\nExplore-Then-Commit (ETC), UCB, Thompson sampling and $\\epsilon$-greedy. We\nalso analyze and provide theoretical justifications for the procedure under the\nETC algorithm, including the asymptotic convergence of the bias decay rate in\nthe real and bootstrap worlds.",
          "link": "http://arxiv.org/abs/2108.00236",
          "publishedOn": "2021-08-03T02:06:33.560Z",
          "wordCount": 568,
          "title": "Debiasing Samples from Online Learning Using Bootstrap. (arXiv:2108.00236v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1\">Uttaran Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Childs_E/0/1/0/all/0/1\">Elizabeth Childs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rewkowski_N/0/1/0/all/0/1\">Nicholas Rewkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>",
          "description": "We present a generative adversarial network to synthesize 3D pose sequences\nof co-speech upper-body gestures with appropriate affective expressions. Our\nnetwork consists of two components: a generator to synthesize gestures from a\njoint embedding space of features encoded from the input speech and the seed\nposes, and a discriminator to distinguish between the synthesized pose\nsequences and real 3D pose sequences. We leverage the Mel-frequency cepstral\ncoefficients and the text transcript computed from the input speech in separate\nencoders in our generator to learn the desired sentiments and the associated\naffective cues. We design an affective encoder using multi-scale\nspatial-temporal graph convolutions to transform 3D pose sequences into latent,\npose-based affective features. We use our affective encoder in both our\ngenerator, where it learns affective features from the seed poses to guide the\ngesture synthesis, and our discriminator, where it enforces the synthesized\ngestures to contain the appropriate affective expressions. We perform extensive\nevaluations on two benchmark datasets for gesture synthesis from the speech,\nthe TED Gesture Dataset and the GENEA Challenge 2020 Dataset. Compared to the\nbest baselines, we improve the mean absolute joint error by 10--33%, the mean\nacceleration difference by 8--58%, and the Fr\\'echet Gesture Distance by\n21--34%. We also conduct a user study and observe that compared to the best\ncurrent baselines, around 15.28% of participants indicated our synthesized\ngestures appear more plausible, and around 16.32% of participants felt the\ngestures had more appropriate affective expressions aligned with the speech.",
          "link": "http://arxiv.org/abs/2108.00262",
          "publishedOn": "2021-08-03T02:06:33.553Z",
          "wordCount": 703,
          "title": "Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective Expression Learning. (arXiv:2108.00262v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuxin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_H/0/1/0/all/0/1\">Haoran Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F Frangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jianqiao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>",
          "description": "Nodule segmentation from breast ultrasound images is challenging yet\nessential for the diagnosis. Weakly-supervised segmentation (WSS) can help\nreduce time-consuming and cumbersome manual annotation. Unlike existing\nweakly-supervised approaches, in this study, we propose a novel and general WSS\nframework called Flip Learning, which only needs the box annotation.\nSpecifically, the target in the label box will be erased gradually to flip the\nclassification tag, and the erased region will be considered as the\nsegmentation result finally. Our contribution is three-fold. First, our\nproposed approach erases on superpixel level using a Multi-agent Reinforcement\nLearning framework to exploit the prior boundary knowledge and accelerate the\nlearning process. Second, we design two rewards: classification score and\nintensity distribution reward, to avoid under- and over-segmentation,\nrespectively. Third, we adopt a coarse-to-fine learning strategy to reduce the\nresidual errors and improve the segmentation performance. Extensively validated\non a large dataset, our proposed approach achieves competitive performance and\nshows great potential to narrow the gap between fully-supervised and\nweakly-supervised learning.",
          "link": "http://arxiv.org/abs/2108.00752",
          "publishedOn": "2021-08-03T02:06:33.541Z",
          "wordCount": 624,
          "title": "Flip Learning: Erase to Segment. (arXiv:2108.00752v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Rui Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hase_K/0/1/0/all/0/1\">Kazunori Hase</a>",
          "description": "To date, very few biomedical signals have transitioned from research\napplications to clinical applications. This is largely due to the lack of trust\nin the diagnostic ability of non-stationary signals. To reach the level of\nclinical diagnostic application, classification using high-quality signal\nfeatures is necessary. While there has been considerable progress in machine\nlearning in recent years, especially deep learning, progress has been quite\nlimited in the field of feature engineering. This study proposes a feature\nextraction algorithm based on group intelligence which we call a Plant Root\nSystem (PRS) algorithm. Importantly, the correlation between features produced\nby this PRS algorithm and traditional features is low, and the accuracy of\nseveral widely-used classifiers was found to be substantially improved with the\naddition of PRS features. It is expected that more biomedical signals can be\napplied to clinical diagnosis using the proposed algorithm.",
          "link": "http://arxiv.org/abs/2108.00214",
          "publishedOn": "2021-08-03T02:06:33.533Z",
          "wordCount": 584,
          "title": "A Plant Root System Algorithm Based on Swarm Intelligence for One-dimensional Biomedical Signal Feature Engineering. (arXiv:2108.00214v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kihwan Kim</a>",
          "description": "All learning algorithms for recommendations face inevitable and critical\ntrade-off between exploiting partial knowledge of a user's preferences for\nshort-term satisfaction and exploring additional user preferences for long-term\ncoverage. Although exploration is indispensable for long success of a\nrecommender system, the exploration has been considered as the risk to decrease\nuser satisfaction. The reason for the risk is that items chosen for exploration\nfrequently mismatch with the user's interests. To mitigate this risk,\nrecommender systems have mixed items chosen for exploration into a\nrecommendation list, disguising the items as recommendations to elicit feedback\non the items to discover the user's additional tastes. This mix-in approach has\nbeen widely used in many recommenders, but there is rare research, evaluating\nthe effectiveness of the mix-in approach or proposing a new approach for\neliciting user feedback without deceiving users. In this work, we aim to\npropose a new approach for feedback elicitation without any deception and\ncompare our approach to the conventional mix-in approach for evaluation. To\nthis end, we designed a recommender interface that reveals which items are for\nexploration and conducted a within-subject study with 94 MTurk workers. Our\nresults indicated that users left significantly more feedback on items chosen\nfor exploration with our interface. Besides, users evaluated that our new\ninterface is better than the conventional mix-in interface in terms of novelty,\ndiversity, transparency, trust, and satisfaction. Finally, path analysis show\nthat, in only our new interface, exploration caused to increase user-centric\nevaluation metrics. Our work paves the way for how to design an interface,\nwhich utilizes learning algorithm based on users' feedback signals, giving\nbetter user experience and gathering more feedback data.",
          "link": "http://arxiv.org/abs/2108.00151",
          "publishedOn": "2021-08-03T02:06:33.527Z",
          "wordCount": 708,
          "title": "An Empirical analysis on Transparent Algorithmic Exploration in Recommender Systems. (arXiv:2108.00151v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Folgoc_L/0/1/0/all/0/1\">Loic Le Folgoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baltatzis_V/0/1/0/all/0/1\">Vasileios Baltatzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alansary_A/0/1/0/all/0/1\">Amir Alansary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Sujal Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devaraj_A/0/1/0/all/0/1\">Anand Devaraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_S/0/1/0/all/0/1\">Sam Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzanera_O/0/1/0/all/0/1\">Octavio E. Martinez Manzanera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanavati_F/0/1/0/all/0/1\">Fahdi Kanavati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1\">Arjun Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1\">Julia Schnabel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>",
          "description": "Datasets are rarely a realistic approximation of the target population. Say,\nprevalence is misrepresented, image quality is above clinical standards, etc.\nThis mismatch is known as sampling bias. Sampling biases are a major hindrance\nfor machine learning models. They cause significant gaps between model\nperformance in the lab and in the real world. Our work is a solution to\nprevalence bias. Prevalence bias is the discrepancy between the prevalence of a\npathology and its sampling rate in the training dataset, introduced upon\ncollecting data or due to the practioner rebalancing the training batches. This\npaper lays the theoretical and computational framework for training models, and\nfor prediction, in the presence of prevalence bias. Concretely a bias-corrected\nloss function, as well as bias-corrected predictive rules, are derived under\nthe principles of Bayesian risk minimization. The loss exhibits a direct\nconnection to the information gain. It offers a principled alternative to\nheuristic training losses and complements test-time procedures based on\nselecting an operating point from summary curves. It integrates seamlessly in\nthe current paradigm of (deep) learning using stochastic backpropagation and\nnaturally with Bayesian models.",
          "link": "http://arxiv.org/abs/2108.00250",
          "publishedOn": "2021-08-03T02:06:33.520Z",
          "wordCount": 655,
          "title": "Bayesian analysis of the prevalence bias: learning and predicting from imbalanced data. (arXiv:2108.00250v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00318",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Gupta_P/0/1/0/all/0/1\">Prachi Gupta</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bhat_H/0/1/0/all/0/1\">Harish S. Bhat</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ranka_K/0/1/0/all/0/1\">Karnamohit Ranka</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Isborn_C/0/1/0/all/0/1\">Christine M. Isborn</a>",
          "description": "We develop a statistical method to learn a molecular Hamiltonian matrix from\na time-series of electron density matrices. We extend our previous method to\nlarger molecular systems by incorporating physical properties to reduce\ndimensionality, while also exploiting regularization techniques like ridge\nregression for addressing multicollinearity. With the learned Hamiltonian we\ncan solve the Time-Dependent Hartree-Fock (TDHF) equation to propagate the\nelectron density in time, and predict its dynamics for field-free and field-on\nscenarios. We observe close quantitative agreement between the predicted\ndynamics and ground truth for both field-off trajectories similar to the\ntraining data, and field-on trajectories outside of the training data.",
          "link": "http://arxiv.org/abs/2108.00318",
          "publishedOn": "2021-08-03T02:06:33.514Z",
          "wordCount": 540,
          "title": "Statistical learning method for predicting density-matrix based electron dynamics. (arXiv:2108.00318v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">You-Wei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1\">Chuan-Xian Ren</a>",
          "description": "As a vital problem in classification-oriented transfer, unsupervised domain\nadaptation (UDA) has attracted widespread attention in recent years. Previous\nUDA methods assume the marginal distributions of different domains are shifted\nwhile ignoring the discriminant information in the label distributions. This\nleads to classification performance degeneration in real applications. In this\nwork, we focus on the conditional distribution shift problem which is of great\nconcern to current conditional invariant models. We aim to seek a kernel\ncovariance embedding for conditional distribution which remains yet unexplored.\nTheoretically, we propose the Conditional Kernel Bures (CKB) metric for\ncharacterizing conditional distribution discrepancy, and derive an empirical\nestimation for the CKB metric without introducing the implicit kernel feature\nmap. It provides an interpretable approach to understand the knowledge transfer\nmechanism. The established consistency theory of the empirical estimation\nprovides a theoretical guarantee for convergence. A conditional distribution\nmatching network is proposed to learn the conditional invariant and\ndiscriminative features for UDA. Extensive experiments and analysis show the\nsuperiority of our proposed model.",
          "link": "http://arxiv.org/abs/2108.00302",
          "publishedOn": "2021-08-03T02:06:33.503Z",
          "wordCount": 599,
          "title": "Conditional Bures Metric for Domain Adaptation. (arXiv:2108.00302v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rucker_M/0/1/0/all/0/1\">Mark Rucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_S/0/1/0/all/0/1\">Stephen Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_R/0/1/0/all/0/1\">Roy Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beling_P/0/1/0/all/0/1\">Peter A. Beling</a>",
          "description": "In adversarial environments, one side could gain an advantage by identifying\nthe opponent's strategy. For example, in combat games, if an opponents strategy\nis identified as overly aggressive, one could lay a trap that exploits the\nopponent's aggressive nature. However, an opponent's strategy is not always\napparent and may need to be estimated from observations of their actions. This\npaper proposes to use inverse reinforcement learning (IRL) to identify\nstrategies in adversarial environments. Specifically, the contributions of this\nwork are 1) the demonstration of this concept on gaming combat data generated\nfrom three pre-defined strategies and 2) the framework for using IRL to achieve\nstrategy identification. The numerical experiments demonstrate that the\nrecovered rewards can be identified using a variety of techniques. In this\npaper, the recovered reward are visually displayed, clustered using\nunsupervised learning, and classified using a supervised learner.",
          "link": "http://arxiv.org/abs/2108.00293",
          "publishedOn": "2021-08-03T02:06:33.471Z",
          "wordCount": 599,
          "title": "Inverse Reinforcement Learning for Strategy Identification. (arXiv:2108.00293v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colombelli_F/0/1/0/all/0/1\">Felipe Colombelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowalski_T/0/1/0/all/0/1\">Thayne Woycinck Kowalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recamonde_Mendoza_M/0/1/0/all/0/1\">Mariana Recamonde-Mendoza</a>",
          "description": "The discovery of disease biomarkers from gene expression data has been\ngreatly advanced by feature selection (FS) methods, especially using ensemble\nFS (EFS) strategies with perturbation at the data level (i.e., homogeneous,\nHom-EFS) or method level (i.e., heterogeneous, Het-EFS). Here we proposed a\nHybrid EFS (Hyb-EFS) design that explores both types of perturbation to improve\nthe stability and the predictive power of candidate biomarkers. With this,\nHyb-EFS aims to disrupt associations of good performance with a single dataset,\nsingle algorithm, or a specific combination of both, which is particularly\ninteresting for better reproducibility of genomic biomarkers. We investigated\nthe adequacy of our approach for microarray data related to four types of\ncancer, carrying out an extensive comparison with other ensemble and single FS\napproaches. Five FS methods were used in our experiments: Wx, Symmetrical\nUncertainty (SU), Gain Ratio (GR), Characteristic Direction (GeoDE), and\nReliefF. We observed that the Hyb-EFS and Het-EFS approaches attenuated the\nlarge performance variation observed for most single FS and Hom-EFS across\ndistinct datasets. Also, the Hyb-EFS improved upon the stability of the Het-EFS\nwithin our domain. Comparing the Hyb-EFS and Het-EFS composed of the\ntop-performing selectors (Wx, GR, and SU), our hybrid approach surpassed the\nequivalent heterogeneous design and the best Hom-EFS (Hom-Wx). Interestingly,\nthe rankings produced by our Hyb-EFS reached greater biological plausibility,\nwith a notably high enrichment for cancer-related genes and pathways. Thus, our\nexperiments suggest the potential of the proposed Hybrid EFS design in\ndiscovering candidate biomarkers from microarray data. Finally, we provide an\nopen-source framework to support similar analyses in other domains, both as a\nuser-friendly application and a plain Python package.",
          "link": "http://arxiv.org/abs/2108.00290",
          "publishedOn": "2021-08-03T02:06:33.440Z",
          "wordCount": 724,
          "title": "A Hybrid Ensemble Feature Selection Design for Candidate Biomarkers Discovery from Transcriptome Profiles. (arXiv:2108.00290v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Akshita Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinzamuri_B/0/1/0/all/0/1\">Bhanukiran Vinzamuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chandan K. Reddy</a>",
          "description": "With the growing interest in the machine learning community to solve\nreal-world problems, it has become crucial to uncover the hidden reasoning\nbehind their decisions by focusing on the fairness and auditing the predictions\nmade by these black-box models. In this paper, we propose a novel method to\naddress two key issues: (a) Can we simultaneously learn fair disentangled\nrepresentations while ensuring the utility of the learned representation for\ndownstream tasks, and (b)Can we provide theoretical insights into when the\nproposed approach will be both fair and accurate. To address the former, we\npropose the method FRIED, Fair Representation learning using Interpolation\nEnabled Disentanglement. In our architecture, by imposing a critic-based\nadversarial framework, we enforce the interpolated points in the latent space\nto be more realistic. This helps in capturing the data manifold effectively and\nenhances the utility of the learned representation for downstream prediction\ntasks. We address the latter question by developing a theory on\nfairness-accuracy trade-offs using classifier-based conditional mutual\ninformation estimation. We demonstrate the effectiveness of FRIED on datasets\nof different modalities - tabular, text, and image datasets. We observe that\nthe representations learned by FRIED are overall fairer in comparison to\nexisting baselines and also accurate for downstream prediction tasks.\nAdditionally, we evaluate FRIED on a real-world healthcare claims dataset where\nwe conduct an expert aided model auditing study providing useful insights into\nopioid ad-diction patterns.",
          "link": "http://arxiv.org/abs/2108.00295",
          "publishedOn": "2021-08-03T02:06:33.416Z",
          "wordCount": 657,
          "title": "Fair Representation Learning using Interpolation Enabled Disentanglement. (arXiv:2108.00295v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00099",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tazarv_A/0/1/0/all/0/1\">Ali Tazarv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levorato_M/0/1/0/all/0/1\">Marco Levorato</a>",
          "description": "Blood Pressure (BP) is one of the four primary vital signs indicating the\nstatus of the body's vital (life-sustaining) functions. BP is difficult to\ncontinuously monitor using a sphygmomanometer (i.e. a blood pressure cuff),\nespecially in everyday-setting. However, other health signals which can be\neasily and continuously acquired, such as photoplethysmography (PPG), show some\nsimilarities with the Aortic Pressure waveform. Based on these similarities, in\nrecent years several methods were proposed to predict BP from the PPG signal.\nBuilding on these results, we propose an advanced personalized data-driven\napproach that uses a three-layer deep neural network to estimate BP based on\nPPG signals. Different from previous work, the proposed model analyzes the PPG\nsignal in time-domain and automatically extracts the most critical features for\nthis specific application, then uses a variation of recurrent neural networks\ncalled Long-Short-Term-Memory (LSTM) to map the extracted features to the BP\nvalue associated with that time window. Experimental results on two separate\nstandard hospital datasets, yielded absolute errors mean and absolute error\nstandard deviation for systolic and diastolic BP values outperforming prior\nworks.",
          "link": "http://arxiv.org/abs/2108.00099",
          "publishedOn": "2021-08-03T02:06:33.398Z",
          "wordCount": 619,
          "title": "A Deep Learning Approach to Predict Blood Pressure from PPG Signals. (arXiv:2108.00099v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin-Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian-Xun Wang</a>",
          "description": "Model-based reinforcement learning (MBRL) is believed to have much higher\nsample efficiency compared to model-free algorithms by learning a predictive\nmodel of the environment. However, the performance of MBRL highly relies on the\nquality of the learned model, which is usually built in a black-box manner and\nmay have poor predictive accuracy outside of the data distribution. The\ndeficiencies of the learned model may prevent the policy from being fully\noptimized. Although some uncertainty analysis-based remedies have been proposed\nto alleviate this issue, model bias still poses a great challenge for MBRL. In\nthis work, we propose to leverage the prior knowledge of underlying physics of\nthe environment, where the governing laws are (partially) known. In particular,\nwe developed a physics-informed MBRL framework, where governing equations and\nphysical constraints are utilized to inform the model learning and policy\nsearch. By incorporating the prior information of the environment, the quality\nof the learned model can be notably improved, while the required interactions\nwith the environment are significantly reduced, leading to better sample\nefficiency and learning performance. The effectiveness and merit have been\ndemonstrated over a handful of classic control problems, where the environments\nare governed by canonical ordinary/partial differential equations.",
          "link": "http://arxiv.org/abs/2108.00128",
          "publishedOn": "2021-08-03T02:06:33.392Z",
          "wordCount": 637,
          "title": "Physics-informed Dyna-Style Model-Based Deep Reinforcement Learning for Dynamic Control. (arXiv:2108.00128v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dirksen_S/0/1/0/all/0/1\">Sjoerd Dirksen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genzel_M/0/1/0/all/0/1\">Martin Genzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacques_L/0/1/0/all/0/1\">Laurent Jacques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stollenwerk_A/0/1/0/all/0/1\">Alexander Stollenwerk</a>",
          "description": "Neural networks with random weights appear in a variety of machine learning\napplications, most prominently as the initialization of many deep learning\nalgorithms and as a computationally cheap alternative to fully learned neural\nnetworks. In the present article we enhance the theoretical understanding of\nrandom neural nets by addressing the following data separation problem: under\nwhat conditions can a random neural network make two classes $\\mathcal{X}^-,\n\\mathcal{X}^+ \\subset \\mathbb{R}^d$ (with positive distance) linearly\nseparable? We show that a sufficiently large two-layer ReLU-network with\nstandard Gaussian weights and uniformly distributed biases can solve this\nproblem with high probability. Crucially, the number of required neurons is\nexplicitly linked to geometric properties of the underlying sets\n$\\mathcal{X}^-, \\mathcal{X}^+$ and their mutual arrangement. This\ninstance-specific viewpoint allows us to overcome the usual curse of\ndimensionality (exponential width of the layers) in non-pathological situations\nwhere the data carries low-complexity structure. We quantify the relevant\nstructure of the data in terms of a novel notion of mutual complexity (based on\na localized version of Gaussian mean width), which leads to sound and\ninformative separation guarantees. We connect our result with related lines of\nwork on approximation, memorization, and generalization.",
          "link": "http://arxiv.org/abs/2108.00207",
          "publishedOn": "2021-08-03T02:06:33.385Z",
          "wordCount": 627,
          "title": "The Separation Capacity of Random Neural Networks. (arXiv:2108.00207v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schuster_D/0/1/0/all/0/1\">Daniel Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelst_S/0/1/0/all/0/1\">Sebastiaan J. van Zelst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aalst_W/0/1/0/all/0/1\">Wil M. P. van der Aalst</a>",
          "description": "Process discovery aims to learn a process model from observed process\nbehavior. From a user's perspective, most discovery algorithms work like a\nblack box. Besides parameter tuning, there is no interaction between the user\nand the algorithm. Interactive process discovery allows the user to exploit\ndomain knowledge and to guide the discovery process. Previously, an incremental\ndiscovery approach has been introduced where a model, considered to be under\nconstruction, gets incrementally extended by user-selected process behavior.\nThis paper introduces a novel approach that additionally allows the user to\nfreeze model parts within the model under construction. Frozen sub-models are\nnot altered by the incremental approach when new behavior is added to the\nmodel. The user can thus steer the discovery algorithm. Our experiments show\nthat freezing sub-models can lead to higher quality models.",
          "link": "http://arxiv.org/abs/2108.00215",
          "publishedOn": "2021-08-03T02:06:33.379Z",
          "wordCount": 600,
          "title": "Freezing Sub-Models During Incremental Process Discovery: Extended Version. (arXiv:2108.00215v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>",
          "description": "Transformers have made much progress in dealing with visual tasks. However,\nexisting vision transformers still do not possess an ability that is important\nto visual input: building the attention among features of different scales. The\nreasons for this problem are two-fold: (1) Input embeddings of each layer are\nequal-scale without cross-scale features; (2) Some vision transformers\nsacrifice the small-scale features of embeddings to lower the cost of the\nself-attention module. To make up this defect, we propose Cross-scale Embedding\nLayer (CEL) and Long Short Distance Attention (LSDA). In particular, CEL blends\neach embedding with multiple patches of different scales, providing the model\nwith cross-scale embeddings. LSDA splits the self-attention module into a\nshort-distance and long-distance one, also lowering the cost but keeping both\nsmall-scale and large-scale features in embeddings. Through these two designs,\nwe achieve cross-scale attention. Besides, we propose dynamic position bias for\nvision transformers to make the popular relative position bias apply to\nvariable-sized images. Based on these proposed modules, we construct our vision\narchitecture called CrossFormer. Experiments show that CrossFormer outperforms\nother transformers on several representative visual tasks, especially object\ndetection and segmentation. The code has been released:\nhttps://github.com/cheerss/CrossFormer.",
          "link": "http://arxiv.org/abs/2108.00154",
          "publishedOn": "2021-08-03T02:06:33.358Z",
          "wordCount": 648,
          "title": "CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention. (arXiv:2108.00154v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kallitsis_M/0/1/0/all/0/1\">Michalis Kallitsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honavar_V/0/1/0/all/0/1\">Vasant Honavar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prajapati_R/0/1/0/all/0/1\">Rupesh Prajapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dinghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_J/0/1/0/all/0/1\">John Yen</a>",
          "description": "Network telescopes or \"Darknets\" provide a unique window into Internet-wide\nmalicious activities associated with malware propagation, denial of service\nattacks, scanning performed for network reconnaissance, and others. Analyses of\nthe resulting data can provide actionable insights to security analysts that\ncan be used to prevent or mitigate cyber-threats. Large Darknets, however,\nobserve millions of nefarious events on a daily basis which makes the\ntransformation of the captured information into meaningful insights\nchallenging. We present a novel framework for characterizing Darknet behavior\nand its temporal evolution aiming to address this challenge. The proposed\nframework: (i) Extracts a high dimensional representation of Darknet events\ncomposed of features distilled from Darknet data and other external sources;\n(ii) Learns, in an unsupervised fashion, an information-preserving\nlow-dimensional representation of these events (using deep representation\nlearning) that is amenable to clustering; (iv) Performs clustering of the\nscanner data in the resulting representation space and provides interpretable\ninsights using optimal decision trees; and (v) Utilizes the clustering outcomes\nas \"signatures\" that can be used to detect structural changes in the Darknet\nactivities. We evaluate the proposed system on a large operational Network\nTelescope and demonstrate its ability to detect real-world, high-impact\ncybersecurity incidents.",
          "link": "http://arxiv.org/abs/2108.00079",
          "publishedOn": "2021-08-03T02:06:33.351Z",
          "wordCount": 653,
          "title": "Zooming Into the Darknet: Characterizing Internet Background Radiation and its Structural Changes. (arXiv:2108.00079v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00230",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Sentenac_F/0/1/0/all/0/1\">Flore Sentenac</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yi_J/0/1/0/all/0/1\">Jialin Yi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Calauzenes_C/0/1/0/all/0/1\">Cl&#xe9;ment Calauz&#xe8;nes</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Perchet_V/0/1/0/all/0/1\">Vianney Perchet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vojnovic_M/0/1/0/all/0/1\">Milan Vojnovic</a>",
          "description": "Finding an optimal matching in a weighted graph is a standard combinatorial\nproblem. We consider its semi-bandit version where either a pair or a full\nmatching is sampled sequentially. We prove that it is possible to leverage a\nrank-1 assumption on the adjacency matrix to reduce the sample complexity and\nthe regret of off-the-shelf algorithms up to reaching a linear dependency in\nthe number of vertices (up to poly log terms).",
          "link": "http://arxiv.org/abs/2108.00230",
          "publishedOn": "2021-08-03T02:06:33.344Z",
          "wordCount": 521,
          "title": "Pure Exploration and Regret Minimization in Matching Bandits. (arXiv:2108.00230v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_D/0/1/0/all/0/1\">Deming Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>",
          "description": "Learning with noisy labels is an important and challenging task for training\naccurate deep neural networks. Some commonly-used loss functions, such as Cross\nEntropy (CE), suffer from severe overfitting to noisy labels. Robust loss\nfunctions that satisfy the symmetric condition were tailored to remedy this\nproblem, which however encounter the underfitting effect. In this paper, we\ntheoretically prove that \\textbf{any loss can be made robust to noisy labels}\nby restricting the network output to the set of permutations over a fixed\nvector. When the fixed vector is one-hot, we only need to constrain the output\nto be one-hot, which however produces zero gradients almost everywhere and thus\nmakes gradient-based optimization difficult. In this work, we introduce the\nsparse regularization strategy to approximate the one-hot constraint, which is\ncomposed of network output sharpening operation that enforces the output\ndistribution of a network to be sharp and the $\\ell_p$-norm ($p\\le 1$)\nregularization that promotes the network output to be sparse. This simple\napproach guarantees the robustness of arbitrary loss functions while not\nhindering the fitting ability. Experimental results demonstrate that our method\ncan significantly improve the performance of commonly-used loss functions in\nthe presence of noisy labels and class imbalance, and outperform the\nstate-of-the-art methods. The code is available at\nhttps://github.com/hitcszx/lnl_sr.",
          "link": "http://arxiv.org/abs/2108.00192",
          "publishedOn": "2021-08-03T02:06:33.321Z",
          "wordCount": 657,
          "title": "Learning with Noisy Labels via Sparse Regularization. (arXiv:2108.00192v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karandikar_A/0/1/0/all/0/1\">Archit Karandikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cain_N/0/1/0/all/0/1\">Nicholas Cain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Dustin Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1\">Jonathon Shlens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1\">Michael C. Mozer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_B/0/1/0/all/0/1\">Becca Roelofs</a>",
          "description": "Optimal decision making requires that classifiers produce uncertainty\nestimates consistent with their empirical accuracy. However, deep neural\nnetworks are often under- or over-confident in their predictions. Consequently,\nmethods have been developed to improve the calibration of their predictive\nuncertainty both during training and post-hoc. In this work, we propose\ndifferentiable losses to improve calibration based on a soft (continuous)\nversion of the binning operation underlying popular calibration-error\nestimators. When incorporated into training, these soft calibration losses\nachieve state-of-the-art single-model ECE across multiple datasets with less\nthan 1% decrease in accuracy. For instance, we observe an 82% reduction in ECE\n(70% relative to the post-hoc rescaled ECE) in exchange for a 0.7% relative\ndecrease in accuracy relative to the cross entropy baseline on CIFAR-100. When\nincorporated post-training, the soft-binning-based calibration error objective\nimproves upon temperature scaling, a popular recalibration method. Overall,\nexperiments across losses and datasets demonstrate that using\ncalibration-sensitive procedures yield better uncertainty estimates under\ndataset shift than the standard practice of using a cross entropy loss and\npost-hoc recalibration methods.",
          "link": "http://arxiv.org/abs/2108.00106",
          "publishedOn": "2021-08-03T02:06:33.296Z",
          "wordCount": 630,
          "title": "Soft Calibration Objectives for Neural Networks. (arXiv:2108.00106v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parchami_M/0/1/0/all/0/1\">Mostafa Parchami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayed_S/0/1/0/all/0/1\">Saif Iftekar Sayed</a>",
          "description": "Feature tracking is the building block of many applications such as visual\nodometry, augmented reality, and target tracking. Unfortunately, the\nstate-of-the-art vision-based tracking algorithms fail in surgical images due\nto the challenges imposed by the nature of such environments. In this paper, we\nproposed a novel and unified deep learning-based approach that can learn how to\ntrack features reliably as well as learn how to detect such reliable features\nfor tracking purposes. The proposed network dubbed as Deep-PT, consists of a\ntracker network which is a convolutional neural network simulating\ncross-correlation in terms of deep learning and two fully connected networks\nthat operate on the output of intermediate layers of the tracker to detect\nfeatures and predict trackability of the detected points. The ability to detect\nfeatures based on the capabilities of the tracker distinguishes the proposed\nmethod from previous algorithms used in this area and improves the robustness\nof the algorithms against dynamics of the scene. The network is trained using\nmultiple datasets due to the lack of specialized dataset for feature tracking\ndatasets and extensive comparisons are conducted to compare the accuracy of\nDeep-PT against recent pixel tracking algorithms. As the experiments suggest,\nthe proposed deep architecture deliberately learns what to track and how to\ntrack and outperforms the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.00105",
          "publishedOn": "2021-08-03T02:06:33.289Z",
          "wordCount": 658,
          "title": "Deep Feature Tracker: A Novel Application for Deep Convolutional Neural Networks. (arXiv:2108.00105v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Llorente_F/0/1/0/all/0/1\">F. Llorente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_L/0/1/0/all/0/1\">L. Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Read_J/0/1/0/all/0/1\">J. Read</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delgado_D/0/1/0/all/0/1\">D. Delgado</a>",
          "description": "This survey gives an overview of Monte Carlo methodologies using surrogate\nmodels, for dealing with densities which are intractable, costly, and/or noisy.\nThis type of problem can be found in numerous real-world scenarios, including\nstochastic optimization and reinforcement learning, where each evaluation of a\ndensity function may incur some computationally-expensive or even physical\n(real-world activity) cost, likely to give different results each time. The\nsurrogate model does not incur this cost, but there are important trade-offs\nand considerations involved in the choice and design of such methodologies. We\nclassify the different methodologies into three main classes and describe\nspecific instances of algorithms under a unified notation. A modular scheme\nwhich encompasses the considered methods is also presented. A range of\napplication scenarios is discussed, with special attention to the\nlikelihood-free setting and reinforcement learning. Several numerical\ncomparisons are also provided.",
          "link": "http://arxiv.org/abs/2108.00490",
          "publishedOn": "2021-08-03T02:06:32.927Z",
          "wordCount": 595,
          "title": "A survey of Monte Carlo methods for noisy and costly densities with application to reinforcement learning. (arXiv:2108.00490v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00664",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Storchan_V/0/1/0/all/0/1\">Victor Storchan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyetrenko_S/0/1/0/all/0/1\">Svitlana Vyetrenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balch_T/0/1/0/all/0/1\">Tucker Balch</a>",
          "description": "In electronic trading markets often only the price or volume time series,\nthat result from interaction of multiple market participants, are directly\nobservable. In order to test trading strategies before deploying them to\nreal-time trading, multi-agent market environments calibrated so that the time\nseries that result from interaction of simulated agents resemble historical are\noften used. To ensure adequate testing, one must test trading strategies in a\nvariety of market scenarios -- which includes both scenarios that represent\nordinary market days as well as stressed markets (most recently observed due to\nthe beginning of COVID pandemic). In this paper, we address the problem of\nmulti-agent simulator parameter calibration to allow simulator capture\ncharacteristics of different market regimes. We propose a novel two-step method\nto train a discriminator that is able to distinguish between \"real\" and \"fake\"\nprice and volume time series as a part of GAN with self-attention, and then\nutilize it within an optimization framework to tune parameters of a simulator\nmodel with known agent archetypes to represent a market scenario. We conclude\nwith experimental results that demonstrate effectiveness of our method.",
          "link": "http://arxiv.org/abs/2108.00664",
          "publishedOn": "2021-08-03T02:06:32.921Z",
          "wordCount": 642,
          "title": "Learning who is in the market from time series: market participant discovery through adversarial calibration of multi-agent simulators. (arXiv:2108.00664v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00413",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Zheng_C/0/1/0/all/0/1\">Candi Zheng</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chen_S/0/1/0/all/0/1\">Shiyi Chen</a>",
          "description": "Macroscopic modeling of the gas dynamics across Knudsen numbers from dense\ngas region to rarefied gas region remains a great challenge. The reason is\nmacroscopic models lack accurate constitutive relations valid across different\nKnudsen numbers. To address this problem, we proposed a Data-driven, KnUdsen\nnumber Adaptive Linear constitutive relation model named DUAL. The DUAL model\nis accurate across a range of Knudsen numbers, from dense to rarefied, through\nlearning to adapt Knudsen number change from observed data. It is consistent\nwith the Navier-Stokes equation under the hydrodynamic limit, by utilizing a\nconstrained neural network. In addition, it naturally satisfies the second law\nof thermodynamics and is robust to noisy data. We test the DUAL model on the\ncalculation of Rayleigh scattering spectra. The DUAL model gives accurate\nspectra for various Knudsen numbers and is superior to traditional perturbation\nand moment expansion methods.",
          "link": "http://arxiv.org/abs/2108.00413",
          "publishedOn": "2021-08-03T02:06:32.916Z",
          "wordCount": 595,
          "title": "Data Driven Macroscopic Modeling across Knudsen Numbers for Rarefied Gas Dynamics and Application to Rayleigh Scattering. (arXiv:2108.00413v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Theis_J/0/1/0/all/0/1\">Julian Theis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darabi_H/0/1/0/all/0/1\">Houshang Darabi</a>",
          "description": "Decay Replay Mining is a deep learning method that utilizes process model\nnotations to predict the next event. However, this method does not intertwine\nthe neural network with the structure of the process model to its full extent.\nThis paper proposes an approach to further interlock the process model of Decay\nReplay Mining with its neural network for next event prediction. The approach\nuses a masking layer which is initialized based on the reachability graph of\nthe process model. Additionally, modifications to the neural network\narchitecture are proposed to increase the predictive performance. Experimental\nresults demonstrate the value of the approach and underscore the importance of\ndiscovering precise and generalized process models.",
          "link": "http://arxiv.org/abs/2108.00404",
          "publishedOn": "2021-08-03T02:06:32.910Z",
          "wordCount": 542,
          "title": "Masking Neural Networks Using Reachability Graphs to Predict Process Events. (arXiv:2108.00404v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00367",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+S_A/0/1/0/all/0/1\">Anu T S</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raveendran_T/0/1/0/all/0/1\">Tara Raveendran</a>",
          "description": "Non-Orthogonal Multiple Access (NOMA) schemes are being actively explored to\naddress some of the major challenges in 5th Generation (5G) Wireless\ncommunications. Channel estimation is exceptionally challenging in scenarios\nwhere NOMA schemes are integrated with millimeter wave (mmWave) massive\nmultiple-input multiple-output (MIMO) systems. An accurate estimation of the\nchannel is essential in exploiting the benefits of the pairing of the duo-NOMA\nand mmWave. This paper proposes a convolutional neural network (CNN) based\napproach to estimate the channel for NOMA based millimeter wave (mmWave)\nmassive multiple-input multiple-output (MIMO) systems built on a hybrid\narchitecture. Initially, users are grouped into different clusters based on\ntheir channel gains and beamforming technique is performed to maximize the\nsignal in the direction of desired cluster. A coarse estimation of the channel\nis first made from the received signal and this estimate is given as the input\nto CNN to fine estimate the channel coefficients. Numerical illustrations show\nthat the proposed method outperforms least square (LS) estimate, minimum mean\nsquare error (MMSE) estimate and are close to the Cramer-Rao Bound (CRB).",
          "link": "http://arxiv.org/abs/2108.00367",
          "publishedOn": "2021-08-03T02:06:32.896Z",
          "wordCount": 616,
          "title": "CNN based Channel Estimation using NOMA for mmWave Massive MIMO System. (arXiv:2108.00367v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00480",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Rahimikia_E/0/1/0/all/0/1\">Eghbal Rahimikia</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zohren_S/0/1/0/all/0/1\">Stefan Zohren</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Poon_S/0/1/0/all/0/1\">Ser-Huang Poon</a>",
          "description": "We develop FinText, a novel, state-of-the-art, financial word embedding from\nDow Jones Newswires Text News Feed Database. Incorporating this word embedding\nin a machine learning model produces a substantial increase in volatility\nforecasting performance on days with volatility jumps for 23 NASDAQ stocks from\n27 July 2007 to 18 November 2016. A simple ensemble model, combining our word\nembedding and another machine learning model that uses limit order book data,\nprovides the best forecasting performance for both normal and jump volatility\ndays. Finally, we use Integrated Gradients and SHAP (SHapley Additive\nexPlanations) to make the results more 'explainable' and the model comparisons\nmore transparent.",
          "link": "http://arxiv.org/abs/2108.00480",
          "publishedOn": "2021-08-03T02:06:32.875Z",
          "wordCount": 549,
          "title": "Realised Volatility Forecasting: Machine Learning via Financial Word Embedding. (arXiv:2108.00480v1 [q-fin.CP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00599",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yan_R/0/1/0/all/0/1\">Rong Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuxuan Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoyu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geng_G/0/1/0/all/0/1\">Guangchao Geng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_Q/0/1/0/all/0/1\">Quanyuan Jiang</a>",
          "description": "Real active distribution networks with associated smart meter (SM) data are\ncritical for power researchers. However, it is practically difficult for\nresearchers to obtain such comprehensive datasets from utilities due to privacy\nconcerns. To bridge this gap, an implicit generative model with Wasserstein GAN\nobjectives, namely unbalanced graph generative adversarial network (UG-GAN), is\ndesigned to generate synthetic three-phase unbalanced active distribution\nsystem connectivity. The basic idea is to learn the distribution of random\nwalks both over a real-world system and across each phase of line segments,\ncapturing the underlying local properties of an individual real-world\ndistribution network and generating specific synthetic networks accordingly.\nThen, to create a comprehensive synthetic test case, a network correction and\nextension process is proposed to obtain time-series nodal demands and standard\ndistribution grid components with realistic parameters, including distributed\nenergy resources (DERs) and capacity banks. A Midwest distribution system with\n1-year SM data has been utilized to validate the performance of our method.\nCase studies with several power applications demonstrate that synthetic active\nnetworks generated by the proposed framework can mimic almost all features of\nreal-world networks while avoiding the disclosure of confidential information.",
          "link": "http://arxiv.org/abs/2108.00599",
          "publishedOn": "2021-08-03T02:06:32.865Z",
          "wordCount": 644,
          "title": "Synthetic Active Distribution System Generation via Unbalanced Graph Generative Adversarial Network. (arXiv:2108.00599v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Anshul Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahiya_K/0/1/0/all/0/1\">Kunal Dahiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sheshansh Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saini_D/0/1/0/all/0/1\">Deepak Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sumeet Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1\">Purushottam Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1\">Manik Varma</a>",
          "description": "Extreme multi-label classification (XML) involves tagging a data point with\nits most relevant subset of labels from an extremely large label set, with\nseveral applications such as product-to-product recommendation with millions of\nproducts. Although leading XML algorithms scale to millions of labels, they\nlargely ignore label meta-data such as textual descriptions of the labels. On\nthe other hand, classical techniques that can utilize label metadata via\nrepresentation learning using deep networks struggle in extreme settings. This\npaper develops the DECAF algorithm that addresses these challenges by learning\nmodels enriched by label metadata that jointly learn model parameters and\nfeature representations using deep networks and offer accurate classification\nat the scale of millions of labels. DECAF makes specific contributions to model\narchitecture design, initialization, and training, enabling it to offer up to\n2-6% more accurate prediction than leading extreme classifiers on publicly\navailable benchmark product-to-product recommendation datasets, such as\nLF-AmazonTitles-1.3M. At the same time, DECAF was found to be up to 22x faster\nat inference than leading deep extreme classifiers, which makes it suitable for\nreal-time applications that require predictions within a few milliseconds. The\ncode for DECAF is available at the following URL\nhttps://github.com/Extreme-classification/DECAF.",
          "link": "http://arxiv.org/abs/2108.00368",
          "publishedOn": "2021-08-03T02:06:32.859Z",
          "wordCount": 654,
          "title": "DECAF: Deep Extreme Classification with Label Features. (arXiv:2108.00368v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duplyakov_V/0/1/0/all/0/1\">Viktor Duplyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morozov_A/0/1/0/all/0/1\">Anton Morozov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popkov_D/0/1/0/all/0/1\">Dmitriy Popkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shel_E/0/1/0/all/0/1\">Egor Shel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainshtein_A/0/1/0/all/0/1\">Albert Vainshtein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osiptsov_A/0/1/0/all/0/1\">Andrei Osiptsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paderin_G/0/1/0/all/0/1\">Grigory Paderin</a>",
          "description": "We describe a stacked model for predicting the cumulative fluid production\nfor an oil well with a multistage-fracture completion based on a combination of\nRidge Regression and CatBoost algorithms. The model is developed based on an\nextended digital field data base of reservoir, well and fracturing design\nparameters. The database now includes more than 5000 wells from 23 oilfields of\nWestern Siberia (Russia), with 6687 fracturing operations in total. Starting\nwith 387 parameters characterizing each well, including construction, reservoir\nproperties, fracturing design features and production, we end up with 38 key\nparameters used as input features for each well in the model training process.\nThe model demonstrates physically explainable dependencies plots of the target\non the design parameters (number of stages, proppant mass, average and final\nproppant concentrations and fluid rate). We developed a set of methods\nincluding those based on the use of Euclidean distance and clustering\ntechniques to perform similar (offset) wells search, which is useful for a\nfield engineer to analyze earlier fracturing treatments on similar wells. These\napproaches are also adapted for obtaining the optimization parameters\nboundaries for the particular pilot well, as part of the field testing campaign\nof the methodology. An inverse problem (selecting an optimum set of fracturing\ndesign parameters to maximize production) is formulated as optimizing a high\ndimensional black box approximation function constrained by boundaries and\nsolved with four different optimization methods: surrogate-based optimization,\nsequential least squares programming, particle swarm optimization and\ndifferential evolution. A recommendation system containing all the above\nmethods is designed to advise a production stimulation engineer on an optimized\nfracturing design.",
          "link": "http://arxiv.org/abs/2108.00751",
          "publishedOn": "2021-08-03T02:06:32.852Z",
          "wordCount": 729,
          "title": "Data-driven model for hydraulic fracturing design optimization. Part II: Inverse problem. (arXiv:2108.00751v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhoubo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Puqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raveaux_R/0/1/0/all/0/1\">Romain Raveaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huadong Liu</a>",
          "description": "Graph matching is an important problem that has received widespread\nattention, especially in the field of computer vision. Recently,\nstate-of-the-art methods seek to incorporate graph matching with deep learning.\nHowever, there is no research to explain what role the graph matching algorithm\nplays in the model. Therefore, we propose an approach integrating a MILP\nformulation of the graph matching problem. This formulation is solved to\noptimal and it provides inherent baseline. Meanwhile, similar approaches are\nderived by releasing the optimal guarantee of the graph matching solver and by\nintroducing a quality level. This quality level controls the quality of the\nsolutions provided by the graph matching solver. In addition, several\nrelaxations of the graph matching problem are put to the test. Our experimental\nevaluation gives several theoretical insights and guides the direction of deep\ngraph matching methods.",
          "link": "http://arxiv.org/abs/2108.00394",
          "publishedOn": "2021-08-03T02:06:32.836Z",
          "wordCount": 604,
          "title": "Deep graph matching meets mixed-integer linear programming: Relax at your own risk ?. (arXiv:2108.00394v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bashizade_R/0/1/0/all/0/1\">Ramin Bashizade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Sayan Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lebeck_A/0/1/0/all/0/1\">Alvin R. Lebeck</a>",
          "description": "Statistical machine learning has widespread application in various domains.\nThese methods include probabilistic algorithms, such as Markov Chain\nMonte-Carlo (MCMC), which rely on generating random numbers from probability\ndistributions. These algorithms are computationally expensive on conventional\nprocessors, yet their statistical properties, namely interpretability and\nuncertainty quantification (UQ) compared to deep learning, make them an\nattractive alternative approach. Therefore, hardware specialization can be\nadopted to address the shortcomings of conventional processors in running these\napplications.\n\nIn this paper, we propose a high-throughput accelerator for Markov Random\nField (MRF) inference, a powerful model for representing a wide range of\napplications, using MCMC with Gibbs sampling. We propose a tiled architecture\nwhich takes advantage of near-memory computing, and memory optimizations\ntailored to the semantics of MRF. Additionally, we propose a novel hybrid\non-chip/off-chip memory system and logging scheme to efficiently support UQ.\nThis memory system design is not specific to MRF models and is applicable to\napplications using probabilistic algorithms. In addition, it dramatically\nreduces off-chip memory bandwidth requirements.\n\nWe implemented an FPGA prototype of our proposed architecture using\nhigh-level synthesis tools and achieved 146MHz frequency for an accelerator\nwith 32 function units on an Intel Arria 10 FPGA. Compared to prior work on\nFPGA, our accelerator achieves 26X speedup. Furthermore, our proposed memory\nsystem and logging scheme to support UQ reduces off-chip bandwidth by 71% for\ntwo applications. ASIC analysis in 15nm shows our design with 2048 function\nunits running at 3GHz outperforms GPU implementations of motion estimation and\nstereo vision on Nvidia RTX2080Ti by 120X-210X, occupying only 7.7% of the\narea.",
          "link": "http://arxiv.org/abs/2108.00570",
          "publishedOn": "2021-08-03T02:06:32.830Z",
          "wordCount": 695,
          "title": "Accelerating Markov Random Field Inference with Uncertainty Quantification. (arXiv:2108.00570v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00462",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Choubo Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>",
          "description": "Existing anomaly detection paradigms overwhelmingly focus on training\ndetection models using exclusively normal data or unlabeled data (mostly normal\nsamples). One notorious issue with these approaches is that they are weak in\ndiscriminating anomalies from normal samples due to the lack of the knowledge\nabout the anomalies. Here, we study the problem of few-shot anomaly detection,\nin which we aim at using a few labeled anomaly examples to train\nsample-efficient discriminative detection models. To address this problem, we\nintroduce a novel weakly-supervised anomaly detection framework to train\ndetection models without assuming the examples illustrating all possible\nclasses of anomaly.\n\nSpecifically, the proposed approach learns discriminative normality\n(regularity) by leveraging the labeled anomalies and a prior probability to\nenforce expressive representations of normality and unbounded deviated\nrepresentations of abnormality. This is achieved by an end-to-end optimization\nof anomaly scores with a neural deviation learning, in which the anomaly scores\nof normal samples are imposed to approximate scalar scores drawn from the prior\nwhile that of anomaly examples is enforced to have statistically significant\ndeviations from these sampled scores in the upper tail. Furthermore, our model\nis optimized to learn fine-grained normality and abnormality by top-K\nmultiple-instance-learning-based feature subspace deviation learning, allowing\nmore generalized representations. Comprehensive experiments on nine real-world\nimage anomaly detection benchmarks show that our model is substantially more\nsample-efficient and robust, and performs significantly better than\nstate-of-the-art competing methods in both closed-set and open-set settings.\nOur model can also offer explanation capability as a result of its prior-driven\nanomaly score learning. Code and datasets are available at:\nhttps://git.io/DevNet.",
          "link": "http://arxiv.org/abs/2108.00462",
          "publishedOn": "2021-08-03T02:06:32.820Z",
          "wordCount": 714,
          "title": "Explainable Deep Few-shot Anomaly Detection with Deviation Networks. (arXiv:2108.00462v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jackie Shen</a>",
          "description": "The bucketed PCA neural network (PCA-NN) with transforms is developed here in\nan effort to benchmark deep neural networks (DNN's), for problems on supervised\nclassification. Most classical PCA models apply PCA to the entire training data\nset to establish a reductive representation and then employ non-network tools\nsuch as high-order polynomial classifiers. In contrast, the bucketed PCA-NN\napplies PCA to individual buckets which are constructed in two consecutive\nphases, as well as retains a genuine architecture of a neural network. This\nfacilitates a fair apple-to-apple comparison to DNN's, esp. to reveal that a\nmajor chunk of accuracy achieved by many impressive DNN's could possibly be\nexplained by the bucketed PCA-NN (e.g., 96% out of 98% for the MNIST data set\nas an example). Compared with most DNN's, the three building blocks of the\nbucketed PCA-NN are easier to comprehend conceptually - PCA, transforms, and\nbucketing for error correction. Furthermore, unlike the somewhat quasi-random\nneurons ubiquitously observed in DNN's, the PCA neurons resemble or mirror the\ninput signals and are more straightforward to decipher as a result.",
          "link": "http://arxiv.org/abs/2108.00605",
          "publishedOn": "2021-08-03T02:06:32.814Z",
          "wordCount": 619,
          "title": "Bucketed PCA Neural Networks with Neurons Mirroring Signals. (arXiv:2108.00605v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jinyuan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yupei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>",
          "description": "Self-supervised learning in computer vision aims to pre-train an image\nencoder using a large amount of unlabeled images or (image, text) pairs. The\npre-trained image encoder can then be used as a feature extractor to build\ndownstream classifiers for many downstream tasks with a small amount of or no\nlabeled training data. In this work, we propose BadEncoder, the first backdoor\nattack to self-supervised learning. In particular, our BadEncoder injects\nbackdoors into a pre-trained image encoder such that the downstream classifiers\nbuilt based on the backdoored image encoder for different downstream tasks\nsimultaneously inherit the backdoor behavior. We formulate our BadEncoder as an\noptimization problem and we propose a gradient descent based method to solve\nit, which produces a backdoored image encoder from a clean one. Our extensive\nempirical evaluation results on multiple datasets show that our BadEncoder\nachieves high attack success rates while preserving the accuracy of the\ndownstream classifiers. We also show the effectiveness of BadEncoder using two\npublicly available, real-world image encoders, i.e., Google's image encoder\npre-trained on ImageNet and OpenAI's Contrastive Language-Image Pre-training\n(CLIP) image encoder pre-trained on 400 million (image, text) pairs collected\nfrom the Internet. Moreover, we consider defenses including Neural Cleanse and\nMNTD (empirical defenses) as well as PatchGuard (a provable defense). Our\nresults show that these defenses are insufficient to defend against BadEncoder,\nhighlighting the needs for new defenses against our BadEncoder. Our code is\npublicly available at: https://github.com/jjy1994/BadEncoder.",
          "link": "http://arxiv.org/abs/2108.00352",
          "publishedOn": "2021-08-03T02:06:32.809Z",
          "wordCount": 693,
          "title": "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning. (arXiv:2108.00352v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanzhina_N/0/1/0/all/0/1\">Natalia Khanzhina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapenok_A/0/1/0/all/0/1\">Alexey Lapenok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1\">Andrey Filchenkov</a>",
          "description": "According to recent studies, commonly used computer vision datasets contain\nabout 4% of label errors. For example, the COCO dataset is known for its high\nlevel of noise in data labels, which limits its use for training robust neural\ndeep architectures in a real-world scenario. To model such a noise, in this\npaper we have proposed the homoscedastic aleatoric uncertainty estimation, and\npresent a series of novel loss functions to address the problem of image object\ndetection at scale. Specifically, the proposed functions are based on Bayesian\ninference and we have incorporated them into the common community-adopted\nobject detection deep learning architecture RetinaNet. We have also shown that\nmodeling of homoscedastic aleatoric uncertainty using our novel functions\nallows to increase the model interpretability and to improve the object\ndetection performance being evaluated on the COCO dataset.",
          "link": "http://arxiv.org/abs/2108.00784",
          "publishedOn": "2021-08-03T02:06:32.784Z",
          "wordCount": 587,
          "title": "Towards Robust Object Detection: Bayesian RetinaNet for Homoscedastic Aleatoric Uncertainty Modeling. (arXiv:2108.00784v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00559",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Pinchuk_P/0/1/0/all/0/1\">Pavlo Pinchuk</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Margot_J/0/1/0/all/0/1\">Jean-Luc Margot</a>",
          "description": "Radio frequency interference (RFI) mitigation remains a major challenge in\nthe search for radio technosignatures. Typical mitigation strategies include a\ndirection-of-origin (DoO) filter, where a signal is classified as RFI if it is\ndetected in multiple directions on the sky. These classifications generally\nrely on estimates of signal properties, such as frequency and frequency drift\nrate. Convolutional neural networks (CNNs) offer a promising complement to\nexisting filters because they can be trained to analyze dynamic spectra\ndirectly, instead of relying on inferred signal properties. In this work, we\ncompiled several data sets consisting of labeled pairs of images of dynamic\nspectra, and we designed and trained a CNN that can determine whether or not a\nsignal detected in one scan is also present in another scan. This CNN-based DoO\nfilter outperforms both a baseline 2D correlation model as well as existing DoO\nfilters over a range of metrics, with precision and recall values of 99.15% and\n97.81%, respectively. We found that the CNN reduces the number of signals\nrequiring visual inspection after the application of traditional DoO filters by\na factor of 6-16 in nominal situations.",
          "link": "http://arxiv.org/abs/2108.00559",
          "publishedOn": "2021-08-03T02:06:32.778Z",
          "wordCount": 662,
          "title": "A Machine-Learning-Based Direction-of-Origin Filter for the Identification of Radio Frequency Interference in the Search for Technosignatures. (arXiv:2108.00559v1 [astro-ph.IM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhixiong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seongjin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1\">Hwasoo Yeo</a>",
          "description": "In many trajectory-based applications, it is necessary to map raw GPS\ntrajectories onto road networks in digital maps, which is commonly referred to\nas a map-matching process. While most previous map-matching methods have\nfocused on using rule-based algorithms to deal with the map-matching problems,\nin this paper, we consider the map-matching task from the data perspective,\nproposing a deep learning-based map-matching model. We build a\nTransformer-based map-matching model with a transfer learning approach. We\ngenerate synthetic trajectory data to pre-train the Transformer model and then\nfine-tune the model with a limited number of ground-truth data to minimize the\nmodel development cost and reduce the real-to-virtual gap. Three metrics\n(Average Hamming Distance, F-score, and BLEU) at two levels (point and segment\nlevel) are used to evaluate the model performance. The results indicate that\nthe proposed model outperforms existing models. Furthermore, we use the\nattention weights of the Transformer to plot the map-matching process and find\nhow the model matches the road segments correctly.",
          "link": "http://arxiv.org/abs/2108.00439",
          "publishedOn": "2021-08-03T02:06:32.771Z",
          "wordCount": 602,
          "title": "Transformer-based Map Matching with Model Limited Ground-Truth Data using Transfer-Learning Approach. (arXiv:2108.00439v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baharani_M/0/1/0/all/0/1\">Mohammadreza Baharani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katariya_V/0/1/0/all/0/1\">Vinit Katariya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_N/0/1/0/all/0/1\">Nichole Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoghli_O/0/1/0/all/0/1\">Omidreza Shoghli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1\">Hamed Tabkhi</a>",
          "description": "Vehicle trajectory prediction is an essential task for enabling many\nintelligent transportation systems. While there have been some promising\nadvances in the field, there is a need for new agile algorithms with smaller\nmodel sizes and lower computational requirements. This article presents\nDeepTrack, a novel deep learning algorithm customized for real-time vehicle\ntrajectory prediction in highways. In contrast to previous methods, the vehicle\ndynamics are encoded using Agile Temporal Convolutional Networks (ATCNs) to\nprovide more robust time prediction with less computation. ATCN also uses\ndepthwise convolution, which reduces the complexity of models compared to\nexisting approaches in terms of model size and operations. Overall, our\nexperimental results demonstrate that DeepTrack achieves comparable accuracy to\nstate-of-the-art trajectory prediction models but with smaller model sizes and\nlower computational complexity, making it more suitable for real-world\ndeployment.",
          "link": "http://arxiv.org/abs/2108.00505",
          "publishedOn": "2021-08-03T02:06:32.757Z",
          "wordCount": 573,
          "title": "DeepTrack: Lightweight Deep Learning for Vehicle Path Prediction in Highways. (arXiv:2108.00505v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1\">Md Afzal Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meraz_M/0/1/0/all/0/1\">Md Meraz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Pavan Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1\">Mohammed Javed</a>",
          "description": "In this paper, we present new feature encoding methods for Detection of 3D\nobjects in point clouds. We used a graph neural network (GNN) for Detection of\n3D objects namely cars, pedestrians, and cyclists. Feature encoding is one of\nthe important steps in Detection of 3D objects. The dataset used is point cloud\ndata which is irregular and unstructured and it needs to be encoded in such a\nway that ensures better feature encapsulation. Earlier works have used relative\ndistance as one of the methods to encode the features. These methods are not\nresistant to rotation variance problems in Graph Neural Networks. We have\nincluded angular-based measures while performing feature encoding in graph\nneural networks. Along with that, we have performed a comparison between other\nmethods like Absolute, Relative, Euclidean distances, and a combination of the\nAngle and Relative methods. The model is trained and evaluated on the subset of\nthe KITTI object detection benchmark dataset under resource constraints. Our\nresults demonstrate that a combination of angle measures and relative distance\nhas performed better than other methods. In comparison to the baseline\nmethod(relative), it achieved better performance. We also performed time\nanalysis of various feature encoding methods.",
          "link": "http://arxiv.org/abs/2108.00780",
          "publishedOn": "2021-08-03T02:06:32.734Z",
          "wordCount": 652,
          "title": "Angle Based Feature Learning in GNN for 3D Object Detection using Point Cloud. (arXiv:2108.00780v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pawelczyk_M/0/1/0/all/0/1\">Martin Pawelczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielawski_S/0/1/0/all/0/1\">Sascha Bielawski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heuvel_J/0/1/0/all/0/1\">Johannes van den Heuvel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richter_T/0/1/0/all/0/1\">Tobias Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasneci_G/0/1/0/all/0/1\">Gjergji Kasneci</a>",
          "description": "Counterfactual explanations provide means for prescriptive model explanations\nby suggesting actionable feature changes (e.g., increase income) that allow\nindividuals to achieve favorable outcomes in the future (e.g., insurance\napproval). Choosing an appropriate method is a crucial aspect for meaningful\ncounterfactual explanations. As documented in recent reviews, there exists a\nquickly growing literature with available methods. Yet, in the absence of\nwidely available opensource implementations, the decision in favor of certain\nmodels is primarily based on what is readily available. Going forward - to\nguarantee meaningful comparisons across explanation methods - we present CARLA\n(Counterfactual And Recourse LibrAry), a python library for benchmarking\ncounterfactual explanation methods across both different data sets and\ndifferent machine learning models. In summary, our work provides the following\ncontributions: (i) an extensive benchmark of 11 popular counterfactual\nexplanation methods, (ii) a benchmarking framework for research on future\ncounterfactual explanation methods, and (iii) a standardized set of integrated\nevaluation measures and data sets for transparent and extensive comparisons of\nthese methods. We have open-sourced CARLA and our experimental results on\nGithub, making them available as competitive baselines. We welcome\ncontributions from other research groups and practitioners.",
          "link": "http://arxiv.org/abs/2108.00783",
          "publishedOn": "2021-08-03T02:06:32.727Z",
          "wordCount": 643,
          "title": "CARLA: A Python Library to Benchmark Algorithmic Recourse and Counterfactual Explanation Algorithms. (arXiv:2108.00783v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00138",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Watson_T/0/1/0/all/0/1\">Thomas Watson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poudel_B/0/1/0/all/0/1\">Bibek Poudel</a>",
          "description": "Model free techniques have been successful at optimal control of complex\nsystems at an expense of copious amounts of data and computation. However, it\nis often desired to obtain a control policy in a short period of time with\nminimal data use and computational burden. To this end, we make use of the NFQ\nalgorithm for steering position control of a golf cart in both a real hardware\nand a simulated environment that was built from real-world interaction. The\ncontroller learns to apply a sequence of voltage signals in the presence of\nenvironmental uncertainties and inherent non-linearities that challenge the the\ncontrol task. We were able to increase the rate of successful control under\nfour minutes in simulation and under 11 minutes in real hardware.",
          "link": "http://arxiv.org/abs/2108.00138",
          "publishedOn": "2021-08-03T02:06:32.721Z",
          "wordCount": 570,
          "title": "Learning to Control Direct Current Motor for Steering in Real Time via Reinforcement Learning. (arXiv:2108.00138v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huangfu_Y/0/1/0/all/0/1\">Yourui Huangfu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yiqun Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>",
          "description": "The wireless network is undergoing a trend from \"onnection of things\" to\n\"connection of intelligence\". With data spread over the communication networks\nand computing capability enhanced on the devices, distributed learning becomes\na hot topic in both industrial and academic communities. Many frameworks, such\nas federated learning and federated distillation, have been proposed. However,\nfew of them takes good care of obstacles such as the time-varying topology\nresulted by the characteristics of wireless networks. In this paper, we propose\na distributed learning framework based on a scalable deep neural network (DNN)\ndesign. By exploiting the permutation equivalence and invariance properties of\nthe learning tasks, the DNNs with different scales for different clients can be\nbuilt up based on two basic parameter sub-matrices. Further, model aggregation\ncan also be conducted based on these two sub-matrices to improve the learning\nconvergence and performance. Finally, simulation results verify the benefits of\nthe proposed framework by compared with some baselines.",
          "link": "http://arxiv.org/abs/2108.00231",
          "publishedOn": "2021-08-03T02:06:32.711Z",
          "wordCount": 605,
          "title": "Distributed Learning for Time-varying Networks: A Scalable Design. (arXiv:2108.00231v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00259",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_Q/0/1/0/all/0/1\">Qihan Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kim_J/0/1/0/all/0/1\">Junhyung Lyle Kim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "The lottery ticket hypothesis (LTH) claims that randomly-initialized, dense\nneural networks contain (sparse) subnetworks that, when trained an equal amount\nin isolation, can match the dense network's performance. Although LTH is useful\nfor discovering efficient network architectures, its three-step process --\npre-training, pruning, and re-training -- is computationally expensive, as the\ndense model must be fully pre-trained. Luckily, \"early-bird\" tickets can be\ndiscovered within neural networks that are minimally pre-trained, allowing for\nthe creation of efficient, LTH-inspired training procedures. Yet, no\ntheoretical foundation of this phenomenon exists. We derive an analytical bound\nfor the number of pre-training iterations that must be performed for a winning\nticket to be discovered, thus providing a theoretical understanding of when and\nwhy such early-bird tickets exist. By adopting a greedy forward selection\npruning strategy, we directly connect the pruned network's performance to the\nloss of the dense network from which it was derived, revealing a threshold in\nthe number of pre-training iterations beyond which high-performing subnetworks\nare guaranteed to exist. We demonstrate the validity of our theoretical results\nacross a variety of architectures and datasets, including multi-layer\nperceptrons (MLPs) trained on MNIST and several deep convolutional neural\nnetwork (CNN) architectures trained on CIFAR10 and ImageNet.",
          "link": "http://arxiv.org/abs/2108.00259",
          "publishedOn": "2021-08-03T02:06:32.704Z",
          "wordCount": 654,
          "title": "Provably Efficient Lottery Ticket Discovery. (arXiv:2108.00259v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ilboudo_W/0/1/0/all/0/1\">Wendyam Eric Lionel Ilboudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_T/0/1/0/all/0/1\">Taisuke Kobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_K/0/1/0/all/0/1\">Kenji Sugimoto</a>",
          "description": "Behavioral cloning (BC) bears a high potential for safe and direct transfer\nof human skills to robots. However, demonstrations performed by human operators\noften contain noise or imperfect behaviors that can affect the efficiency of\nthe imitator if left unchecked. In order to allow the imitators to effectively\nlearn from imperfect demonstrations, we propose to employ the robust t-momentum\noptimization algorithm. This algorithm builds on the Student's t-distribution\nin order to deal with heavy-tailed data and reduce the effect of outlying\nobservations. We extend the t-momentum algorithm to allow for an adaptive and\nautomatic robustness and show empirically how the algorithm can be used to\nproduce robust BC imitators against datasets with unknown heaviness. Indeed,\nthe imitators trained with the t-momentum-based Adam optimizers displayed\nrobustness to imperfect demonstrations on two different manipulation tasks with\ndifferent robots and revealed the capability to take advantage of the\nadditional data while reducing the adverse effect of non-optimal behaviors.",
          "link": "http://arxiv.org/abs/2108.00625",
          "publishedOn": "2021-08-03T02:06:32.682Z",
          "wordCount": 616,
          "title": "Adaptive t-Momentum-based Optimization for Unknown Ratio of Outliers in Amateur Data in Imitation Learning. (arXiv:2108.00625v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinyan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>",
          "description": "In this paper, we revisit the problem of Differentially Private Stochastic\nConvex Optimization (DP-SCO) and provide excess population risks for some\nspecial classes of functions that are faster than the previous results of\ngeneral convex and strongly convex functions. In the first part of the paper,\nwe study the case where the population risk function satisfies the Tysbakov\nNoise Condition (TNC) with some parameter $\\theta>1$. Specifically, we first\nshow that under some mild assumptions on the loss functions, there is an\nalgorithm whose output could achieve an upper bound of\n$\\tilde{O}((\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d\\log\n\\frac{1}{\\delta}}}{n\\epsilon})^\\frac{\\theta}{\\theta-1})$ for $(\\epsilon,\n\\delta)$-DP when $\\theta\\geq 2$, here $n$ is the sample size and $d$ is the\ndimension of the space. Then we address the inefficiency issue, improve the\nupper bounds by $\\text{Poly}(\\log n)$ factors and extend to the case where\n$\\theta\\geq \\bar{\\theta}>1$ for some known $\\bar{\\theta}$. Next we show that\nthe excess population risk of population functions satisfying TNC with\nparameter $\\theta>1$ is always lower bounded by\n$\\Omega((\\frac{d}{n\\epsilon})^\\frac{\\theta}{\\theta-1}) $ and\n$\\Omega((\\frac{\\sqrt{d\\log\n\\frac{1}{\\delta}}}{n\\epsilon})^\\frac{\\theta}{\\theta-1})$ for $\\epsilon$-DP and\n$(\\epsilon, \\delta)$-DP, respectively. In the second part, we focus on a\nspecial case where the population risk function is strongly convex. Unlike the\nprevious studies, here we assume the loss function is {\\em non-negative} and\n{\\em the optimal value of population risk is sufficiently small}. With these\nadditional assumptions, we propose a new method whose output could achieve an\nupper bound of\n$O(\\frac{d\\log\\frac{1}{\\delta}}{n^2\\epsilon^2}+\\frac{1}{n^{\\tau}})$ for any\n$\\tau\\geq 1$ in $(\\epsilon,\\delta)$-DP model if the sample size $n$ is\nsufficiently large.",
          "link": "http://arxiv.org/abs/2108.00331",
          "publishedOn": "2021-08-03T02:06:32.675Z",
          "wordCount": 686,
          "title": "Faster Rates of Differentially Private Stochastic Convex Optimization. (arXiv:2108.00331v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Sangwoo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hyunwoo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kihyuk Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Contrastive self-supervised learning has shown impressive results in learning\nvisual representations from unlabeled images by enforcing invariance against\ndifferent data augmentations. However, the learned representations are often\ncontextually biased to the spurious scene correlations of different objects or\nobject and background, which may harm their generalization on the downstream\ntasks. To tackle the issue, we develop a novel object-aware contrastive\nlearning framework that first (a) localizes objects in a self-supervised manner\nand then (b) debias scene correlations via appropriate data augmentations\nconsidering the inferred object locations. For (a), we propose the contrastive\nclass activation map (ContraCAM), which finds the most discriminative regions\n(e.g., objects) in the image compared to the other images using the\ncontrastively trained models. We further improve the ContraCAM to detect\nmultiple objects and entire shapes via an iterative refinement procedure. For\n(b), we introduce two data augmentations based on ContraCAM, object-aware\nrandom crop and background mixup, which reduce contextual and background biases\nduring contrastive self-supervised learning, respectively. Our experiments\ndemonstrate the effectiveness of our representation learning framework,\nparticularly when trained under multi-object images or evaluated under the\nbackground (and distribution) shifted images.",
          "link": "http://arxiv.org/abs/2108.00049",
          "publishedOn": "2021-08-03T02:06:32.654Z",
          "wordCount": 632,
          "title": "Object-aware Contrastive Learning for Debiased Scene Representation. (arXiv:2108.00049v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00354",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhu_B/0/1/0/all/0/1\">Botao Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bedeer_E/0/1/0/all/0/1\">Ebrahim Bedeer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha H. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barton_R/0/1/0/all/0/1\">Robert Barton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Henry_J/0/1/0/all/0/1\">Jerome Henry</a>",
          "description": "Unmanned aerial vehicles (UAVs) have emerged as a promising candidate\nsolution for data collection of large-scale wireless sensor networks (WSNs). In\nthis paper, we investigate a UAV-aided WSN, where cluster heads (CHs) receive\ndata from their member nodes, and a UAV is dispatched to collect data from CHs\nalong the planned trajectory. We aim to minimize the total energy consumption\nof the UAV-WSN system in a complete round of data collection. Toward this end,\nwe formulate the energy consumption minimization problem as a constrained\ncombinatorial optimization problem by jointly selecting CHs from nodes within\nclusters and planning the UAV's visiting order to the selected CHs. The\nformulated energy consumption minimization problem is NP-hard, and hence, hard\nto solve optimally. In order to tackle this challenge, we propose a novel deep\nreinforcement learning (DRL) technique, pointer network-A* (Ptr-A*), which can\nefficiently learn from experiences the UAV trajectory policy for minimizing the\nenergy consumption. The UAV's start point and the WSN with a set of\npre-determined clusters are fed into the Ptr-A*, and the Ptr-A* outputs a group\nof CHs and the visiting order to these CHs, i.e., the UAV's trajectory. The\nparameters of the Ptr-A* are trained on small-scale clusters problem instances\nfor faster training by using the actor-critic algorithm in an unsupervised\nmanner. At inference, three search strategies are also proposed to improve the\nquality of solutions. Simulation results show that the trained models based on\n20-clusters and 40-clusters have a good generalization ability to solve the\nUAV's trajectory planning problem in WSNs with different numbers of clusters,\nwithout the need to retrain the models. Furthermore, the results show that our\nproposed DRL algorithm outperforms two baseline techniques.",
          "link": "http://arxiv.org/abs/2108.00354",
          "publishedOn": "2021-08-03T02:06:32.646Z",
          "wordCount": 742,
          "title": "UAV Trajectory Planning in Wireless Sensor Networks for Energy Consumption Minimization by Deep Reinforcement Learning. (arXiv:2108.00354v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Albanese_A/0/1/0/all/0/1\">Andrea Albanese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nardello_M/0/1/0/all/0/1\">Matteo Nardello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunelli_D/0/1/0/all/0/1\">Davide Brunelli</a>",
          "description": "Artificial intelligence has smoothly penetrated several economic activities,\nespecially monitoring and control applications, including the agriculture\nsector. However, research efforts toward low-power sensing devices with fully\nfunctional machine learning (ML) on-board are still fragmented and limited in\nsmart farming. Biotic stress is one of the primary causes of crop yield\nreduction. With the development of deep learning in computer vision technology,\nautonomous detection of pest infestation through images has become an important\nresearch direction for timely crop disease diagnosis. This paper presents an\nembedded system enhanced with ML functionalities, ensuring continuous detection\nof pest infestation inside fruit orchards. The embedded solution is based on a\nlow-power embedded sensing system along with a Neural Accelerator able to\ncapture and process images inside common pheromone-based traps. Three different\nML algorithms have been trained and deployed, highlighting the capabilities of\nthe platform. Moreover, the proposed approach guarantees an extended battery\nlife thanks to the integration of energy harvesting functionalities. Results\nshow how it is possible to automate the task of pest infestation for unlimited\ntime without the farmer's intervention.",
          "link": "http://arxiv.org/abs/2108.00421",
          "publishedOn": "2021-08-03T02:06:32.612Z",
          "wordCount": 626,
          "title": "Automated Pest Detection with DNN on the Edge for Precision Agriculture. (arXiv:2108.00421v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dogan_M/0/1/0/all/0/1\">Mine Gokce Dogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezzeldin_Y/0/1/0/all/0/1\">Yahya H. Ezzeldin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragouli_C/0/1/0/all/0/1\">Christina Fragouli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohannon_A/0/1/0/all/0/1\">Addison W. Bohannon</a>",
          "description": "We consider a source that wishes to communicate with a destination at a\ndesired rate, over a mmWave network where links are subject to blockage and\nnodes to failure (e.g., in a hostile military environment). To achieve\nresilience to link and node failures, we here explore a state-of-the-art Soft\nActor-Critic (SAC) deep reinforcement learning algorithm, that adapts the\ninformation flow through the network, without using knowledge of the link\ncapacities or network topology. Numerical evaluations show that our algorithm\ncan achieve the desired rate even in dynamic environments and it is robust\nagainst blockage.",
          "link": "http://arxiv.org/abs/2108.00548",
          "publishedOn": "2021-08-03T02:06:32.596Z",
          "wordCount": 534,
          "title": "A Reinforcement Learning Approach for Scheduling in mmWave Networks. (arXiv:2108.00548v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haitong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shuang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xia Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kaiyue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hongjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nizhuan Wang</a>",
          "description": "It is a challenging task to accurately perform semantic segmentation due to\nthe complexity of real picture scenes. Many semantic segmentation methods based\non traditional deep learning insufficiently captured the semantic and\nappearance information of images, which put limit on their generality and\nrobustness for various application scenes. In this paper, we proposed a novel\nstrategy that reformulated the popularly-used convolution operation to\nmulti-layer convolutional sparse coding block to ease the aforementioned\ndeficiency. This strategy can be possibly used to significantly improve the\nsegmentation performance of any semantic segmentation model that involves\nconvolutional operations. To prove the effectiveness of our idea, we chose the\nwidely-used U-Net model for the demonstration purpose, and we designed CSC-Unet\nmodel series based on U-Net. Through extensive analysis and experiments, we\nprovided credible evidence showing that the multi-layer convolutional sparse\ncoding block enables semantic segmentation model to converge faster, can\nextract finer semantic and appearance information of images, and improve the\nability to recover spatial detail information. The best CSC-Unet model\nsignificantly outperforms the results of the original U-Net on three public\ndatasets with different scenarios, i.e., 87.14% vs. 84.71% on DeepCrack\ndataset, 68.91% vs. 67.09% on Nuclei dataset, and 53.68% vs. 48.82% on CamVid\ndataset, respectively.",
          "link": "http://arxiv.org/abs/2108.00408",
          "publishedOn": "2021-08-03T02:06:32.562Z",
          "wordCount": 661,
          "title": "CSC-Unet: A Novel Convolutional Sparse Coding Strategy based Neural Network for Semantic Segmentation. (arXiv:2108.00408v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abhishek_G/0/1/0/all/0/1\">Guttu Sai Abhishek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingole_H/0/1/0/all/0/1\">Harshad Ingole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laturia_P/0/1/0/all/0/1\">Parth Laturia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorna_V/0/1/0/all/0/1\">Vineeth Dorna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1\">Ayush Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>",
          "description": "We present SPEAR, an open-source python library for data programming with\nsemi supervision. The package implements several recent data programming\napproaches including facility to programmatically label and build training\ndata. SPEAR facilitates weak supervision in the form of heuristics (or rules)\nand association of noisy labels to the training dataset. These noisy labels are\naggregated to assign labels to the unlabeled data for downstream tasks. We have\nimplemented several label aggregation approaches that aggregate the noisy\nlabels and then train using the noisily labeled set in a cascaded manner. Our\nimplementation also includes other approaches that jointly aggregate and train\nthe model. Thus, in our python package, we integrate several cascade and joint\ndata-programming approaches while also providing the facility of data\nprogramming by letting the user define labeling functions or rules. The code\nand tutorial notebooks are available at\n\\url{https://github.com/decile-team/spear}.",
          "link": "http://arxiv.org/abs/2108.00373",
          "publishedOn": "2021-08-03T02:06:32.510Z",
          "wordCount": 584,
          "title": "SPEAR : Semi-supervised Data Programming in Python. (arXiv:2108.00373v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00109",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Medrano_M/0/1/0/all/0/1\">Maria Medrano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_R/0/1/0/all/0/1\">Rui Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Politte_D/0/1/0/all/0/1\">David G. Politte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Williamson_J/0/1/0/all/0/1\">Jeffrey F. Williamson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+OSullivan_J/0/1/0/all/0/1\">Joseph A. O&#x27;Sullivan</a>",
          "description": "Dual-energy CT (DECT) has been widely investigated to generate more\ninformative and more accurate images in the past decades. For example,\nDual-Energy Alternating Minimization (DEAM) algorithm achieves sub-percentage\nuncertainty in estimating proton stopping-power mappings from experimental 3-mm\ncollimated phantom data. However, elapsed time of iterative DECT algorithms is\nnot clinically acceptable, due to their low convergence rate and the tremendous\ngeometry of modern helical CT scanners. A CNN-based initialization method is\nintroduced to reduce the computational time of iterative DECT algorithms. DEAM\nis used as an example of iterative DECT algorithms in this work. The simulation\nresults show that our method generates denoised images with greatly improved\nestimation accuracy for adipose, tonsils, and muscle tissue. Also, it reduces\nelapsed time by approximately 5-fold for DEAM to reach the same objective\nfunction value for both simulated and real data.",
          "link": "http://arxiv.org/abs/2108.00109",
          "publishedOn": "2021-08-03T02:06:32.504Z",
          "wordCount": 608,
          "title": "A Machine-learning Based Initialization for Joint Statistical Iterative Dual-energy CT with Application to Proton Therapy. (arXiv:2108.00109v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grushin_A/0/1/0/all/0/1\">Alexander Grushin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woods_W/0/1/0/all/0/1\">Walt Woods</a>",
          "description": "Reinforcement learning has recently shown promise as a technique for training\nan artificial neural network to parse sentences in some unknown format. A key\naspect of this approach is that rather than explicitly inferring a grammar that\ndescribes the format, the neural network learns to perform various parsing\nactions (such as merging two tokens) over a corpus of sentences, with the goal\nof maximizing the total reward, which is roughly based on the estimated\nfrequency of the resulting parse structures. This can allow the learning\nprocess to more easily explore different action choices, since a given choice\nmay change the optimality of the parse (as expressed by the total reward), but\nwill not result in the failure to parse a sentence. However, the approach also\nexhibits limitations: first, the neural network does not provide production\nrules for the grammar that it uses during parsing; second, because this neural\nnetwork can successfully parse any sentence, it cannot be directly used to\nidentify sentences that deviate from the format of the training sentences,\ni.e., that are anomalous. In this paper, we address these limitations by\npresenting procedures for extracting production rules from the neural network,\nand for using these rules to determine whether a given sentence is nominal or\nanomalous, when compared to structures observed within training data. In the\nlatter case, an attempt is made to identify the location of the anomaly.\nAdditionally, a two pass mechanism is presented for dealing with formats\ncontaining high-entropy information. We empirically evaluate the approach on\nartificial formats, demonstrating effectiveness, but also identifying\nlimitations. By further improving parser learning, and leveraging rule\nextraction and anomaly detection, one might begin to understand common errors,\neither benign or malicious, in practical formats.",
          "link": "http://arxiv.org/abs/2108.00103",
          "publishedOn": "2021-08-03T02:06:32.463Z",
          "wordCount": 743,
          "title": "Extracting Grammars from a Neural Network Parser for Anomaly Detection in Unknown Formats. (arXiv:2108.00103v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Che_C/0/1/0/all/0/1\">Chunjiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaoyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zibin Zheng</a>",
          "description": "Federated learning allows multiple participants to collaboratively train an\nefficient model without exposing data privacy. However, this distributed\nmachine learning training method is prone to attacks from Byzantine clients,\nwhich interfere with the training of the global model by modifying the model or\nuploading the false gradient. In this paper, we propose a novel serverless\nfederated learning framework Committee Mechanism based Federated Learning\n(CMFL), which can ensure the robustness of the algorithm with convergence\nguarantee. In CMFL, a committee system is set up to screen the uploaded local\ngradients. The committee system selects the local gradients rated by the\nelected members for the aggregation procedure through the selection strategy,\nand replaces the committee member through the election strategy. Based on the\ndifferent considerations of model performance and defense, two opposite\nselection strategies are designed for the sake of both accuracy and robustness.\nExtensive experiments illustrate that CMFL achieves faster convergence and\nbetter accuracy than the typical Federated Learning, in the meanwhile obtaining\nbetter robustness than the traditional Byzantine-tolerant algorithms, in the\nmanner of a decentralized approach. In addition, we theoretically analyze and\nprove the convergence of CMFL under different election and selection\nstrategies, which coincides with the experimental results.",
          "link": "http://arxiv.org/abs/2108.00365",
          "publishedOn": "2021-08-03T02:06:32.228Z",
          "wordCount": 649,
          "title": "A Decentralized Federated Learning Framework via Committee Mechanism with Convergence Guarantee. (arXiv:2108.00365v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cini_A/0/1/0/all/0/1\">Andrea Cini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marisca_I/0/1/0/all/0/1\">Ivan Marisca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alippi_C/0/1/0/all/0/1\">Cesare Alippi</a>",
          "description": "Dealing with missing values and incomplete time series is a labor-intensive\nand time-consuming inevitable task when handling data coming from real-world\napplications. Effective spatio-temporal representations would allow imputation\nmethods to reconstruct missing temporal data by exploiting information coming\nfrom sensors at different locations. However, standard methods fall short in\ncapturing the nonlinear time and space dependencies existing within networks of\ninterconnected sensors and do not take full advantage of the available - and\noften strong - relational information. Notably, most of state-of-the-art\nimputation methods based on deep learning do not explicitly model relational\naspects and, in any case, do not exploit processing frameworks able to\nadequately represent structured spatio-temporal data. Conversely, graph neural\nnetworks have recently surged in popularity as both expressive and scalable\ntools for processing sequential data with relational inductive biases. In this\nwork, we present the first assessment of graph neural networks in the context\nof multivariate time series imputation. In particular, we introduce a novel\ngraph neural network architecture, named GRIL, which aims at reconstructing\nmissing data in the different channels of a multivariate time series by\nlearning spatial-temporal representations through message passing. Preliminary\nempirical results show that our model outperforms state-of-the-art methods in\nthe imputation task on relevant benchmarks with mean absolute error\nimprovements often higher than 20%.",
          "link": "http://arxiv.org/abs/2108.00298",
          "publishedOn": "2021-08-03T02:06:32.175Z",
          "wordCount": 644,
          "title": "Multivariate Time Series Imputation by Graph Neural Networks. (arXiv:2108.00298v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alamri_F/0/1/0/all/0/1\">Faisal Alamri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1\">Anjan Dutta</a>",
          "description": "Zero-Shot Learning (ZSL) aims to recognise unseen object classes, which are\nnot observed during the training phase. The existing body of works on ZSL\nmostly relies on pretrained visual features and lacks the explicit attribute\nlocalisation mechanism on images. In this work, we propose an attention-based\nmodel in the problem settings of ZSL to learn attributes useful for unseen\nclass recognition. Our method uses an attention mechanism adapted from Vision\nTransformer to capture and learn discriminative attributes by splitting images\ninto small patches. We conduct experiments on three popular ZSL benchmarks\n(i.e., AWA2, CUB and SUN) and set new state-of-the-art harmonic mean results\n{on all the three datasets}, which illustrate the effectiveness of our proposed\nmethod.",
          "link": "http://arxiv.org/abs/2108.00045",
          "publishedOn": "2021-08-03T02:06:32.070Z",
          "wordCount": 562,
          "title": "Multi-Head Self-Attention via Vision Transformer for Zero-Shot Learning. (arXiv:2108.00045v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tazarv_A/0/1/0/all/0/1\">Ali Tazarv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labbaf_S/0/1/0/all/0/1\">Sina Labbaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reich_S/0/1/0/all/0/1\">Stephanie M. Reich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutt_N/0/1/0/all/0/1\">Nikil Dutt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1\">Amir M. Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levorato_M/0/1/0/all/0/1\">Marco Levorato</a>",
          "description": "Since stress contributes to a broad range of mental and physical health\nproblems, the objective assessment of stress is essential for behavioral and\nphysiological studies. Although several studies have evaluated stress levels in\ncontrolled settings, objective stress assessment in everyday settings is still\nlargely under-explored due to challenges arising from confounding contextual\nfactors and limited adherence for self-reports. In this paper, we explore the\nobjective prediction of stress levels in everyday settings based on heart rate\n(HR) and heart rate variability (HRV) captured via low-cost and easy-to-wear\nphotoplethysmography (PPG) sensors that are widely available on newer smart\nwearable devices. We present a layered system architecture for personalized\nstress monitoring that supports a tunable collection of data samples for\nlabeling, and present a method for selecting informative samples from the\nstream of real-time data for labeling. We captured the stress levels of\nfourteen volunteers through self-reported questionnaires over periods of\nbetween 1-3 months, and explored binary stress detection based on HR and HRV\nusing Machine Learning Methods. We observe promising preliminary results given\nthat the dataset is collected in the challenging environments of everyday\nsettings. The binary stress detector is fairly accurate and can detect\nstressful vs non-stressful samples with a macro-F1 score of up to \\%76. Our\nstudy lays the groundwork for more sophisticated labeling strategies that\ngenerate context-aware, personalized models that will empower health\nprofessionals to provide personalized interventions.",
          "link": "http://arxiv.org/abs/2108.00144",
          "publishedOn": "2021-08-03T02:06:32.061Z",
          "wordCount": 682,
          "title": "Personalized Stress Monitoring using Wearable Sensors in Everyday Settings. (arXiv:2108.00144v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yeung_Y/0/1/0/all/0/1\">Yu-Hong Yeung</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Barajas_Solano_D/0/1/0/all/0/1\">David A. Barajas-Solano</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Tartakovsky_A/0/1/0/all/0/1\">Alexandre M. Tartakovsky</a> (1 and 2) ((1) Physical and Computational Sciences Directorate, Pacific Northwest National Laboratory, (2) Department of Civil and Environmental Engineering, University of Illinois Urbana-Champaign)",
          "description": "We develop a physics-informed machine learning approach for large-scale data\nassimilation and parameter estimation and apply it for estimating\ntransmissivity and hydraulic head in the two-dimensional steady-state\nsubsurface flow model of the Hanford Site given synthetic measurements of said\nvariables. In our approach, we extend the physics-informed conditional\nKarhunen-Lo\\'{e}ve expansion (PICKLE) method for modeling subsurface flow with\nunknown flux (Neumann) and varying head (Dirichlet) boundary conditions. We\ndemonstrate that the PICKLE method is comparable in accuracy with the standard\nmaximum a posteriori (MAP) method, but is significantly faster than MAP for\nlarge-scale problems. Both methods use a mesh to discretize the computational\ndomain. In MAP, the parameters and states are discretized on the mesh;\ntherefore, the size of the MAP parameter estimation problem directly depends on\nthe mesh size. In PICKLE, the mesh is used to evaluate the residuals of the\ngoverning equation, while the parameters and states are approximated by the\ntruncated conditional Karhunen-Lo\\'{e}ve expansions with the number of\nparameters controlled by the smoothness of the parameter and state fields, and\nnot by the mesh size. For a considered example, we demonstrate that the\ncomputational cost of PICKLE increases near linearly (as $N_{FV}^{1.15}$) with\nthe number of grid points $N_{FV}$, while that of MAP increases much faster as\n$N_{FV}^{3.28}$. We demonstrated that once trained for one set of Dirichlet\nboundary conditions (i.e., one river stage), the PICKLE method provides\naccurate estimates of the hydraulic head for any value of the Dirichlet\nboundary conditions (i.e., for any river stage).",
          "link": "http://arxiv.org/abs/2108.00037",
          "publishedOn": "2021-08-03T02:06:32.041Z",
          "wordCount": 728,
          "title": "Physics-Informed Machine Learning Method for Large-Scale Data Assimilation Problems. (arXiv:2108.00037v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00069",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lejarza_F/0/1/0/all/0/1\">Fernando Lejarza</a>, <a href=\"http://arxiv.org/find/math/1/au:+Baldea_M/0/1/0/all/0/1\">Michael Baldea</a>",
          "description": "Discovering the governing laws underpinning physical and chemical phenomena\nis a key step towards understanding and ultimately controlling systems in\nscience and engineering. We introduce Discovery of Dynamical Systems via Moving\nHorizon Optimization (DySMHO), a scalable machine learning framework for\nidentifying governing laws in the form of differential equations from\nlarge-scale noisy experimental data sets. DySMHO consists of a novel moving\nhorizon dynamic optimization strategy that sequentially learns the underlying\ngoverning equations from a large dictionary of basis functions. The sequential\nnature of DySMHO allows leveraging statistical arguments for eliminating\nirrelevant basis functions, avoiding overfitting to recover accurate and\nparsimonious forms of the governing equations. Canonical nonlinear dynamical\nsystem examples are used to demonstrate that DySMHO can accurately recover the\ngoverning laws, is robust to high levels of measurement noise and that it can\nhandle challenges such as multiple time scale dynamics.",
          "link": "http://arxiv.org/abs/2108.00069",
          "publishedOn": "2021-08-03T02:06:31.892Z",
          "wordCount": 586,
          "title": "DySMHO: Data-Driven Discovery of Governing Equations for Dynamical Systems via Moving Horizon Optimization. (arXiv:2108.00069v1 [math.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Massart_E/0/1/0/all/0/1\">Estelle Massart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrol_V/0/1/0/all/0/1\">Vinayak Abrol</a>",
          "description": "We propose to use stochastic Riemannian coordinate descent on the orthogonal\ngroup for recurrent neural network training. The algorithm rotates successively\ntwo columns of the recurrent matrix, an operation that can be efficiently\nimplemented as a multiplication by a Givens matrix. In the case when the\ncoordinate is selected uniformly at random at each iteration, we prove the\nconvergence of the proposed algorithm under standard assumptions on the loss\nfunction, stepsize and minibatch noise. In addition, we numerically demonstrate\nthat the Riemannian gradient in recurrent neural network training has an\napproximately sparse structure. Leveraging this observation, we propose a\nfaster variant of the proposed algorithm that relies on the Gauss-Southwell\nrule. Experiments on a benchmark recurrent neural network training problem are\npresented to demonstrate the effectiveness of the proposed algorithm.",
          "link": "http://arxiv.org/abs/2108.00051",
          "publishedOn": "2021-08-03T02:06:31.886Z",
          "wordCount": 568,
          "title": "Coordinate descent on the orthogonal group for recurrent neural network training. (arXiv:2108.00051v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Gary Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wessler_B/0/1/0/all/0/1\">Benjamin Wessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1\">Michael C. Hughes</a>",
          "description": "Semi-supervised image classification has shown substantial progress in\nlearning from limited labeled data, but recent advances remain largely untested\nfor clinical applications. Motivated by the urgent need to improve timely\ndiagnosis of life-threatening heart conditions, especially aortic stenosis, we\ndevelop a benchmark dataset to assess semi-supervised approaches to two tasks\nrelevant to cardiac ultrasound (echocardiogram) interpretation: view\nclassification and disease severity classification. We find that a\nstate-of-the-art method called MixMatch achieves promising gains in heldout\naccuracy on both tasks, learning from a large volume of truly unlabeled images\nas well as a labeled set collected at great expense to achieve better\nperformance than is possible with the labeled set alone. We further pursue\npatient-level diagnosis prediction, which requires aggregating across hundreds\nof images of diverse view types, most of which are irrelevant, to make a\ncoherent prediction. The best patient-level performance is achieved by new\nmethods that prioritize diagnosis predictions from images that are predicted to\nbe clinically-relevant views and transfer knowledge from the view task to the\ndiagnosis task. We hope our released Tufts Medical Echocardiogram Dataset and\nevaluation framework inspire further improvements in multi-task semi-supervised\nlearning for clinical applications.",
          "link": "http://arxiv.org/abs/2108.00080",
          "publishedOn": "2021-08-03T02:06:31.848Z",
          "wordCount": 674,
          "title": "A New Semi-supervised Learning Benchmark for Classifying View and Diagnosing Aortic Stenosis from Echocardiograms. (arXiv:2108.00080v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00002",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Kotthoff_L/0/1/0/all/0/1\">Lars Kotthoff</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Wahab_H/0/1/0/all/0/1\">Hud Wahab</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Johnson_P/0/1/0/all/0/1\">Patrick Johnson</a>",
          "description": "Bayesian optimization is used in many areas of AI for the optimization of\nblack-box processes and has achieved impressive improvements of the state of\nthe art for a lot of applications. It intelligently explores large and complex\ndesign spaces while minimizing the number of evaluations of the expensive\nunderlying process to be optimized. Materials science considers the problem of\noptimizing materials' properties given a large design space that defines how to\nsynthesize or process them, with evaluations requiring expensive experiments or\nsimulations -- a very similar setting. While Bayesian optimization is also a\npopular approach to tackle such problems, there is almost no overlap between\nthe two communities that are investigating the same concepts. We present a\nsurvey of Bayesian optimization approaches in materials science to increase\ncross-fertilization and avoid duplication of work. We highlight common\nchallenges and opportunities for joint research efforts.",
          "link": "http://arxiv.org/abs/2108.00002",
          "publishedOn": "2021-08-03T02:06:31.842Z",
          "wordCount": 573,
          "title": "Bayesian Optimization in Materials Science: A Survey. (arXiv:2108.00002v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Novikov_G/0/1/0/all/0/1\">Georgii S. Novikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panov_M/0/1/0/all/0/1\">Maxim E. Panov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1\">Ivan V. Oseledets</a>",
          "description": "Estimation of probability density function from samples is one of the central\nproblems in statistics and machine learning. Modern neural network-based models\ncan learn high dimensional distributions but have problems with hyperparameter\nselection and are often prone to instabilities during training and inference.\nWe propose a new efficient tensor train-based model for density estimation\n(TTDE). Such density parametrization allows exact sampling, calculation of\ncumulative and marginal density functions, and partition function. It also has\nvery intuitive hyperparameters. We develop an efficient non-adversarial\ntraining procedure for TTDE based on the Riemannian optimization. Experimental\nresults demonstrate the competitive performance of the proposed method in\ndensity estimation and sampling tasks, while TTDE significantly outperforms\ncompetitors in training speed.",
          "link": "http://arxiv.org/abs/2108.00089",
          "publishedOn": "2021-08-03T02:06:31.835Z",
          "wordCount": 561,
          "title": "Tensor-Train Density Estimation. (arXiv:2108.00089v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00043",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Ziegler_J/0/1/0/all/0/1\">Joshua Ziegler</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+McJunkin_T/0/1/0/all/0/1\">Thomas McJunkin</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Joseph_E/0/1/0/all/0/1\">E. S. Joseph</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kalantre_S/0/1/0/all/0/1\">Sandesh S. Kalantre</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Harpt_B/0/1/0/all/0/1\">Benjamin Harpt</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Savage_D/0/1/0/all/0/1\">D. E. Savage</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lagally_M/0/1/0/all/0/1\">M. G. Lagally</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Eriksson_M/0/1/0/all/0/1\">M. A. Eriksson</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Taylor_J/0/1/0/all/0/1\">Jacob M. Taylor</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zwolak_J/0/1/0/all/0/1\">Justyna P. Zwolak</a>",
          "description": "The current autotuning approaches for quantum dot (QD) devices, while showing\nsome success, lack an assessment of data reliability. This leads to unexpected\nfailures when noisy data is processed by an autonomous system. In this work, we\npropose a framework for robust autotuning of QD devices that combines a machine\nlearning (ML) state classifier with a data quality control module. The data\nquality control module acts as a ``gatekeeper'' system, ensuring that only\nreliable data is processed by the state classifier. Lower data quality results\nin either device recalibration or termination. To train both ML systems, we\nenhance the QD simulation by incorporating synthetic noise typical of QD\nexperiments. We confirm that the inclusion of synthetic noise in the training\nof the state classifier significantly improves the performance, resulting in an\naccuracy of 95.1(7) % when tested on experimental data. We then validate the\nfunctionality of the data quality control module by showing the state\nclassifier performance deteriorates with decreasing data quality, as expected.\nOur results establish a robust and flexible ML framework for autonomous tuning\nof noisy QD devices.",
          "link": "http://arxiv.org/abs/2108.00043",
          "publishedOn": "2021-08-03T02:06:31.802Z",
          "wordCount": 637,
          "title": "Toward Robust Autotuning of Noisy Quantum Dot Devices. (arXiv:2108.00043v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2103.16329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_W/0/1/0/all/0/1\">Wai Weng Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Layeghy_S/0/1/0/all/0/1\">Siamak Layeghy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarhan_M/0/1/0/all/0/1\">Mohanad Sarhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallagher_M/0/1/0/all/0/1\">Marcus Gallagher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portmann_M/0/1/0/all/0/1\">Marius Portmann</a>",
          "description": "This paper presents a new Network Intrusion Detection System (NIDS) based on\nGraph Neural Networks (GNNs). GNNs are a relatively new sub-field of deep\nneural networks, which can leverage the inherent structure of graph-based data.\nTraining and evaluation data for NIDSs are typically represented as flow\nrecords, which can naturally be represented in a graph format. This establishes\nthe potential and motivation for exploring GNNs for network intrusion\ndetection, which is the focus of this paper. Current approaches to graph\nrepresentation learning can only consider topological information and/or node\nfeatures, but not edge features. This is a key limitation for the use of\ncurrent GNN models for network intrusion detection, since critical flow\ninformation for the detection of anomalous or malicious traffic, e.g. flow\nsize, flow duration, etc., is represented as edge features in a graph\nrepresentation. In this paper, we propose E-GraphSAGE, a first GNN approach\nwhich overcomes this limitation and which allows capturing the edge features of\na graph, in addition to node features and topological information. We present a\nnovel NIDS based on E-GraphSAGE, and our extensive experimental evaluation on\nsix recent NIDS benchmark datasets shows that it outperforms the\nstate-of-the-art in regards to key classification metrics in four out of six\ncases, and closely matches it in the other two cases. Our research and initial\nbasic system demonstrates the potential of GNNs for network intrusion\ndetection, and provides motivation for further research.",
          "link": "http://arxiv.org/abs/2103.16329",
          "publishedOn": "2021-08-02T01:58:25.434Z",
          "wordCount": 750,
          "title": "E-GraphSAGE: A Graph Neural Network based Intrusion Detection System. (arXiv:2103.16329v5 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.00509",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pulver_H/0/1/0/all/0/1\">Henry Pulver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eiras_F/0/1/0/all/0/1\">Francisco Eiras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carozza_L/0/1/0/all/0/1\">Ludovico Carozza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawasly_M/0/1/0/all/0/1\">Majd Hawasly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1\">Stefano V. Albrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthy_S/0/1/0/all/0/1\">Subramanian Ramamoorthy</a>",
          "description": "Achieving a proper balance between planning quality, safety and efficiency is\na major challenge for autonomous driving. Optimisation-based motion planners\nare capable of producing safe, smooth and comfortable plans, but often at the\ncost of runtime efficiency. On the other hand, naively deploying trajectories\nproduced by efficient-to-run deep imitation learning approaches might risk\ncompromising safety. In this paper, we present PILOT -- a planning framework\nthat comprises an imitation neural network followed by an efficient optimiser\nthat actively rectifies the network's plan, guaranteeing fulfilment of safety\nand comfort requirements. The objective of the efficient optimiser is the same\nas the objective of an expensive-to-run optimisation-based planning system that\nthe neural network is trained offline to imitate. This efficient optimiser\nprovides a key layer of online protection from learning failures or deficiency\nin out-of-distribution situations that might compromise safety or comfort.\nUsing a state-of-the-art, runtime-intensive optimisation-based method as the\nexpert, we demonstrate in simulated autonomous driving experiments in CARLA\nthat PILOT achieves a seven-fold reduction in runtime when compared to the\nexpert it imitates without sacrificing planning quality.",
          "link": "http://arxiv.org/abs/2011.00509",
          "publishedOn": "2021-08-02T01:58:25.414Z",
          "wordCount": 676,
          "title": "PILOT: Efficient Planning by Imitation Learning and Optimisation for Safe Autonomous Driving. (arXiv:2011.00509v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11730",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_S/0/1/0/all/0/1\">Sheng Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Sian Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dingwen Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zizhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cappello_F/0/1/0/all/0/1\">Franck Cappello</a>",
          "description": "Error-bounded lossy compression is becoming an indispensable technique for\nthe success of today's scientific projects with vast volumes of data produced\nduring the simulations or instrument data acquisitions. Not only can it\nsignificantly reduce data size, but it also can control the compression errors\nbased on user-specified error bounds. Autoencoder (AE) models have been widely\nused in image compression, but few AE-based compression approaches support\nerror-bounding features, which are highly required by scientific applications.\nTo address this issue, we explore using convolutional autoencoders to improve\nerror-bounded lossy compression for scientific data, with the following three\nkey contributions. (1) We provide an in-depth investigation of the\ncharacteristics of various autoencoder models and develop an error-bounded\nautoencoder-based framework in terms of the SZ model. (2) We optimize the\ncompression quality for main stages in our designed AE-based error-bounded\ncompression framework, fine-tuning the block sizes and latent sizes and also\noptimizing the compression efficiency of latent vectors. (3) We evaluate our\nproposed solution using five real-world scientific datasets and comparing them\nwith six other related works. Experiments show that our solution exhibits a\nvery competitive compression quality from among all the compressors in our\ntests. In absolute terms, it can obtain a much better compression quality (100%\n~ 800% improvement in compression ratio with the same data distortion) compared\nwith SZ2.1 and ZFP in cases with a high compression ratio.",
          "link": "http://arxiv.org/abs/2105.11730",
          "publishedOn": "2021-08-02T01:58:25.387Z",
          "wordCount": 723,
          "title": "Exploring Autoencoder-based Error-bounded Compression for Scientific Data. (arXiv:2105.11730v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hammar_K/0/1/0/all/0/1\">Kim Hammar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stadler_R/0/1/0/all/0/1\">Rolf Stadler</a>",
          "description": "We study automated intrusion prevention using reinforcement learning. In a\nnovel approach, we formulate the problem of intrusion prevention as an optimal\nstopping problem. This formulation allows us insight into the structure of the\noptimal policies, which turn out to be threshold based. Since the computation\nof the optimal defender policy using dynamic programming is not feasible for\npractical cases, we approximate the optimal policy through reinforcement\nlearning in a simulation environment. To define the dynamics of the simulation,\nwe emulate the target infrastructure and collect measurements. Our evaluations\nshow that the learned policies are close to optimal and that they indeed can be\nexpressed using thresholds.",
          "link": "http://arxiv.org/abs/2106.07160",
          "publishedOn": "2021-08-02T01:58:25.365Z",
          "wordCount": 582,
          "title": "Learning Intrusion Prevention Policies through Optimal Stopping. (arXiv:2106.07160v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01531",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nistal_J/0/1/0/all/0/1\">Javier Nistal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aouameur_C/0/1/0/all/0/1\">Cyran Aouameur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lattner_S/0/1/0/all/0/1\">Stefan Lattner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richard_G/0/1/0/all/0/1\">Ga&#xeb;l Richard</a>",
          "description": "Influenced by the field of Computer Vision, Generative Adversarial Networks\n(GANs) are often adopted for the audio domain using fixed-size two-dimensional\nspectrogram representations as the \"image data\". However, in the (musical)\naudio domain, it is often desired to generate output of variable duration. This\npaper presents VQCPC-GAN, an adversarial framework for synthesizing\nvariable-length audio by exploiting Vector-Quantized Contrastive Predictive\nCoding (VQCPC). A sequence of VQCPC tokens extracted from real audio data\nserves as conditional input to a GAN architecture, providing step-wise\ntime-dependent features of the generated content. The input noise z\n(characteristic in adversarial architectures) remains fixed over time, ensuring\ntemporal consistency of global features. We evaluate the proposed model by\ncomparing a diverse set of metrics against various strong baselines. Results\nshow that, even though the baselines score best, VQCPC-GAN achieves comparable\nperformance even when generating variable-length audio. Numerous sound examples\nare provided in the accompanying website, and we release the code for\nreproducibility.",
          "link": "http://arxiv.org/abs/2105.01531",
          "publishedOn": "2021-08-02T01:58:25.319Z",
          "wordCount": 665,
          "title": "VQCPC-GAN: Variable-Length Adversarial Audio Synthesis Using Vector-Quantized Contrastive Predictive Coding. (arXiv:2105.01531v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.06402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allein_L/0/1/0/all/0/1\">Liesbeth Allein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>",
          "description": "Truth can vary over time. Fact-checking decisions on claim veracity should\ntherefore take into account temporal information of both the claim and\nsupporting or refuting evidence. In this work, we investigate the hypothesis\nthat the timestamp of a Web page is crucial to how it should be ranked for a\ngiven claim. We delineate four temporal ranking methods that constrain evidence\nranking differently and simulate hypothesis-specific evidence rankings given\nthe evidence timestamps as gold standard. Evidence ranking in three\nfact-checking models is ultimately optimized using a learning-to-rank loss\nfunction. Our study reveals that time-aware evidence ranking not only surpasses\nrelevance assumptions based purely on semantic similarity or position in a\nsearch results list, but also improves veracity predictions of time-sensitive\nclaims in particular.",
          "link": "http://arxiv.org/abs/2009.06402",
          "publishedOn": "2021-08-02T01:58:25.301Z",
          "wordCount": 595,
          "title": "Time-Aware Evidence Ranking for Fact-Checking. (arXiv:2009.06402v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11259",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Barrat_Charlaix_P/0/1/0/all/0/1\">Pierre Barrat-Charlaix</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Muntoni_A/0/1/0/all/0/1\">Anna Paola Muntoni</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shimagaki_K/0/1/0/all/0/1\">Kai Shimagaki</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Weigt_M/0/1/0/all/0/1\">Martin Weigt</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zamponi_F/0/1/0/all/0/1\">Francesco Zamponi</a>",
          "description": "Boltzmann machines (BM) are widely used as generative models. For example,\npairwise Potts models (PM), which are instances of the BM class, provide\naccurate statistical models of families of evolutionarily related protein\nsequences. Their parameters are the local fields, which describe site-specific\npatterns of amino-acid conservation, and the two-site couplings, which mirror\nthe coevolution between pairs of sites. This coevolution reflects structural\nand functional constraints acting on protein sequences during evolution. The\nmost conservative choice to describe the coevolution signal is to include all\npossible two-site couplings into the PM. This choice, typical of what is known\nas Direct Coupling Analysis, has been successful for predicting residue\ncontacts in the three-dimensional structure, mutational effects, and in\ngenerating new functional sequences. However, the resulting PM suffers from\nimportant over-fitting effects: many couplings are small, noisy and hardly\ninterpretable; the PM is close to a critical point, meaning that it is highly\nsensitive to small parameter perturbations. In this work, we introduce a\ngeneral parameter-reduction procedure for BMs, via a controlled iterative\ndecimation of the less statistically significant couplings, identified by an\ninformation-based criterion that selects either weak or statistically\nunsupported couplings. For several protein families, our procedure allows one\nto remove more than $90\\%$ of the PM couplings, while preserving the predictive\nand generative properties of the original dense PM, and the resulting model is\nfar away from criticality, hence more robust to noise.",
          "link": "http://arxiv.org/abs/2011.11259",
          "publishedOn": "2021-08-02T01:58:25.285Z",
          "wordCount": 715,
          "title": "Sparse generative modeling via parameter-reduction of Boltzmann machines: application to protein-sequence families. (arXiv:2011.11259v3 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiangshan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xuejing Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shanqing Guo</a>",
          "description": "With the wide use of Automatic Speech Recognition (ASR) in applications such\nas human machine interaction, simultaneous interpretation, audio transcription,\netc., its security protection becomes increasingly important. Although recent\nstudies have brought to light the weaknesses of popular ASR systems that enable\nout-of-band signal attack, adversarial attack, etc., and further proposed\nvarious remedies (signal smoothing, adversarial training, etc.), a systematic\nunderstanding of ASR security (both attacks and defenses) is still missing,\nespecially on how realistic such threats are and how general existing\nprotection could be. In this paper, we present our systematization of knowledge\nfor ASR security and provide a comprehensive taxonomy for existing work based\non a modularized workflow. More importantly, we align the research in this\ndomain with that on security in Image Recognition System (IRS), which has been\nextensively studied, using the domain knowledge in the latter to help\nunderstand where we stand in the former. Generally, both IRS and ASR are\nperceptual systems. Their similarities allow us to systematically study\nexisting literature in ASR security based on the spectrum of attacks and\ndefense solutions proposed for IRS, and pinpoint the directions of more\nadvanced attacks and the directions potentially leading to more effective\nprotection in ASR. In contrast, their differences, especially the complexity of\nASR compared with IRS, help us learn unique challenges and opportunities in ASR\nsecurity. Particularly, our experimental study shows that transfer learning\nacross ASR models is feasible, even in the absence of knowledge about models\n(even their types) and training data.",
          "link": "http://arxiv.org/abs/2103.10651",
          "publishedOn": "2021-08-02T01:58:25.279Z",
          "wordCount": 744,
          "title": "SoK: A Modularized Approach to Study the Security of Automatic Speech Recognition Systems. (arXiv:2103.10651v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.06775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1\">Peide Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hengli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "Traditional decision and planning frameworks for self-driving vehicles (SDVs)\nscale poorly in new scenarios, thus they require tedious hand-tuning of rules\nand parameters to maintain acceptable performance in all foreseeable cases.\nRecently, self-driving methods based on deep learning have shown promising\nresults with better generalization capability but less hand engineering effort.\nHowever, most of the previous learning-based methods are trained and evaluated\nin limited driving scenarios with scattered tasks, such as lane-following,\nautonomous braking, and conditional driving. In this paper, we propose a\ngraph-based deep network to achieve scalable self-driving that can handle\nmassive traffic scenarios. Specifically, more than 7,000 km of evaluation is\nconducted in a high-fidelity driving simulator, in which our method can obey\nthe traffic rules and safely navigate the vehicle in a large variety of urban,\nrural, and highway environments, including unprotected left turns, narrow\nroads, roundabouts, and pedestrian-rich intersections. Demonstration videos are\navailable at https://caipeide.github.io/dignet/.",
          "link": "http://arxiv.org/abs/2011.06775",
          "publishedOn": "2021-08-02T01:58:25.261Z",
          "wordCount": 646,
          "title": "DiGNet: Learning Scalable Self-Driving Policies for Generic Traffic Scenarios with Graph Neural Networks. (arXiv:2011.06775v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guowen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Han Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shangwei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Run Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>",
          "description": "This paper presents a novel fingerprinting scheme for the Intellectual\nProperty (IP) protection of Generative Adversarial Networks (GANs). Prior\nsolutions for classification models adopt adversarial examples as the\nfingerprints, which can raise stealthiness and robustness problems when they\nare applied to the GAN models. Our scheme constructs a composite deep learning\nmodel from the target GAN and a classifier. Then we generate stealthy\nfingerprint samples from this composite model, and register them to the\nclassifier for effective ownership verification. This scheme inspires three\nconcrete methodologies to practically protect the modern GAN models.\nTheoretical analysis proves that these methods can satisfy different security\nrequirements necessary for IP protection. We also conduct extensive experiments\nto show that our solutions outperform existing strategies in terms of\nstealthiness, functionality-preserving and unremovability.",
          "link": "http://arxiv.org/abs/2106.11760",
          "publishedOn": "2021-08-02T01:58:25.255Z",
          "wordCount": 609,
          "title": "A Novel Verifiable Fingerprinting Scheme for Generative Adversarial Networks. (arXiv:2106.11760v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Toth_C/0/1/0/all/0/1\">Csaba Toth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonnier_P/0/1/0/all/0/1\">Patric Bonnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberhauser_H/0/1/0/all/0/1\">Harald Oberhauser</a>",
          "description": "Sequential data such as time series, video, or text can be challenging to\nanalyse as the ordered structure gives rise to complex dependencies. At the\nheart of this is non-commutativity, in the sense that reordering the elements\nof a sequence can completely change its meaning. We use a classical\nmathematical object -- the tensor algebra -- to capture such dependencies. To\naddress the innate computational complexity of high degree tensors, we use\ncompositions of low-rank tensor projections. This yields modular and scalable\nbuilding blocks for neural networks that give state-of-the-art performance on\nstandard benchmarks such as multivariate time series classification and\ngenerative models for video.",
          "link": "http://arxiv.org/abs/2006.07027",
          "publishedOn": "2021-08-02T01:58:25.248Z",
          "wordCount": 577,
          "title": "Seq2Tens: An Efficient Representation of Sequences by Low-Rank Tensor Projections. (arXiv:2006.07027v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1\">Javier Naranjo-Alcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1\">Sergi Perez-Castanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1\">Maximo Cobos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1\">Francesc J. Ferri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1\">Pedro Zuccarello</a>",
          "description": "Acoustic scene classification (ASC) is one of the most popular problems in\nthe field of machine listening. The objective of this problem is to classify an\naudio clip into one of the predefined scenes using only the audio data. This\nproblem has considerably progressed over the years in the different editions of\nDCASE. It usually has several subtasks that allow to tackle this problem with\ndifferent approaches. The subtask presented in this report corresponds to a ASC\nproblem that is constrained by the complexity of the model as well as having\naudio recorded from different devices, known as mismatch devices (real and\nsimulated). The work presented in this report follows the research line carried\nout by the team in previous years. Specifically, a system based on two steps is\nproposed: a two-dimensional representation of the audio using the Gamamtone\nfilter bank and a convolutional neural network using squeeze-excitation\ntechniques. The presented system outperforms the baseline by about 17\npercentage points.",
          "link": "http://arxiv.org/abs/2107.14658",
          "publishedOn": "2021-08-02T01:58:25.239Z",
          "wordCount": 625,
          "title": "Task 1A DCASE 2021: Acoustic Scene Classification with mismatch-devices using squeeze-excitation technique and low-complexity constraint. (arXiv:2107.14658v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09286",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingzhong Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lirong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Liangjian Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>",
          "description": "Recent studies show that advanced priors play a major role in deep generative\nmodels. Exemplar VAE, as a variant of VAE with an exemplar-based prior, has\nachieved impressive results. However, due to the nature of model design, an\nexemplar-based model usually requires vast amounts of data to participate in\ntraining, which leads to huge computational complexity. To address this issue,\nwe propose Bayesian Pseudocoresets Exemplar VAE (ByPE-VAE), a new variant of\nVAE with a prior based on Bayesian pseudocoreset. The proposed prior is\nconditioned on a small-scale pseudocoreset rather than the whole dataset for\nreducing the computational cost and avoiding overfitting. Simultaneously, we\nobtain the optimal pseudocoreset via a stochastic optimization algorithm during\nVAE training aiming to minimize the Kullback-Leibler divergence between the\nprior based on the pseudocoreset and that based on the whole dataset.\nExperimental results show that ByPE-VAE can achieve competitive improvements\nover the state-of-the-art VAEs in the tasks of density estimation,\nrepresentation learning, and generative data augmentation. Particularly, on a\nbasic VAE architecture, ByPE-VAE is up to 3 times faster than Exemplar VAE\nwhile almost holding the performance. Code is available at our supplementary\nmaterials.",
          "link": "http://arxiv.org/abs/2107.09286",
          "publishedOn": "2021-08-02T01:58:25.232Z",
          "wordCount": 631,
          "title": "ByPE-VAE: Bayesian Pseudocoresets Exemplar VAE. (arXiv:2107.09286v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07576",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Greenbank_S/0/1/0/all/0/1\">Samuel Greenbank</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Howey_D/0/1/0/all/0/1\">David A. Howey</a>",
          "description": "The complex nature of lithium-ion battery degradation has led to many machine\nlearning based approaches to health forecasting being proposed in literature.\nHowever, machine learning can be computationally intensive. Linear approaches\nare faster but have previously been too inflexible for successful prognosis.\nFor both techniques, the choice and quality of the inputs is a limiting factor\nof performance. Piecewise-linear models, combined with automated feature\nselection, offer a fast and flexible alternative without being as\ncomputationally intensive as machine learning. Here, a piecewise-linear\napproach to battery health forecasting was compared to a Gaussian process\nregression tool and found to perform equally well. The input feature selection\nprocess demonstrated the benefit of limiting the correlation between inputs.\nFurther trials found that the piecewise-linear approach was robust to changing\ninput size and availability of training data.",
          "link": "http://arxiv.org/abs/2104.07576",
          "publishedOn": "2021-08-02T01:58:25.216Z",
          "wordCount": 601,
          "title": "Piecewise-linear modelling with feature selection for Li-ion battery end of life prognosis. (arXiv:2104.07576v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chaowei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiazhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1\">Tao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1\">Huasong Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Houpu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1\">Peng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1\">Liefeng Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanqing Chen</a>",
          "description": "In this paper, we present Fedlearn-Algo, an open-source privacy preserving\nmachine learning platform. We use this platform to demonstrate our research and\ndevelopment results on privacy preserving machine learning algorithms. As the\nfirst batch of novel FL algorithm examples, we release vertical federated\nkernel binary classification model and vertical federated random forest model.\nThey have been tested to be more efficient than existing vertical federated\nlearning models in our practice. Besides the novel FL algorithm examples, we\nalso release a machine communication module. The uniform data transfer\ninterface supports transferring widely used data formats between machines. We\nwill maintain this platform by adding more functional modules and algorithm\nexamples. The code is available at https://github.com/fedlearnAI/fedlearn-algo.",
          "link": "http://arxiv.org/abs/2107.04129",
          "publishedOn": "2021-08-02T01:58:25.204Z",
          "wordCount": 587,
          "title": "Fedlearn-Algo: A flexible open-source privacy-preserving machine learning platform. (arXiv:2107.04129v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.08903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1\">Nan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sichen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_K/0/1/0/all/0/1\">Kyle Kai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabowo_A/0/1/0/all/0/1\">Arian Prabowo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Mohammad Saiedur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1\">Flora D. Salim</a>",
          "description": "Generative Adversarial Networks (GANs) have shown remarkable success in\nproducing realistic-looking images in the computer vision area. Recently,\nGAN-based techniques are shown to be promising for spatio-temporal-based\napplications such as trajectory prediction, events generation and time-series\ndata imputation. While several reviews for GANs in computer vision have been\npresented, no one has considered addressing the practical applications and\nchallenges relevant to spatio-temporal data. In this paper, we have conducted a\ncomprehensive review of the recent developments of GANs for spatio-temporal\ndata. We summarise the application of popular GAN architectures for\nspatio-temporal data and the common practices for evaluating the performance of\nspatio-temporal applications with GANs. Finally, we point out future research\ndirections to benefit researchers in this area.",
          "link": "http://arxiv.org/abs/2008.08903",
          "publishedOn": "2021-08-02T01:58:25.198Z",
          "wordCount": 636,
          "title": "Generative Adversarial Networks for Spatio-temporal Data: A Survey. (arXiv:2008.08903v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tolstaya_E/0/1/0/all/0/1\">Ekaterina Tolstaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butler_L/0/1/0/all/0/1\">Landon Butler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mox_D/0/1/0/all/0/1\">Daniel Mox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulos_J/0/1/0/all/0/1\">James Paulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vijay Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1\">Alejandro Ribeiro</a>",
          "description": "Many algorithms for control of multi-robot teams operate under the assumption\nthat low-latency, global state information necessary to coordinate agent\nactions can readily be disseminated among the team. However, in harsh\nenvironments with no existing communication infrastructure, robots must form\nad-hoc networks, forcing the team to operate in a distributed fashion. To\novercome this challenge, we propose a task-agnostic, decentralized, low-latency\nmethod for data distribution in ad-hoc networks using Graph Neural Networks\n(GNN). Our approach enables multi-agent algorithms based on global state\ninformation to function by ensuring it is available at each robot. To do this,\nagents glean information about the topology of the network from packet\ntransmissions and feed it to a GNN running locally which instructs the agent\nwhen and where to transmit the latest state information. We train the\ndistributed GNN communication policies via reinforcement learning using the\naverage Age of Information as the reward function and show that it improves\ntraining stability compared to task-specific reward functions. Our approach\nperforms favorably compared to industry-standard methods for data distribution\nsuch as random flooding and round robin. We also show that the trained policies\ngeneralize to larger teams of both static and mobile agents.",
          "link": "http://arxiv.org/abs/2103.05091",
          "publishedOn": "2021-08-02T01:58:25.192Z",
          "wordCount": 672,
          "title": "Learning Connectivity for Data Distribution in Robot Teams. (arXiv:2103.05091v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.03133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wenbo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shroff_N/0/1/0/all/0/1\">Ness B. Shroff</a>",
          "description": "This paper studies the sample complexity (aka number of comparisons) bounds\nfor the active best-$k$ items selection from pairwise comparisons. From a given\nset of items, the learner can make pairwise comparisons on every pair of items,\nand each comparison returns an independent noisy result about the preferred\nitem. At any time, the learner can adaptively choose a pair of items to compare\naccording to past observations (i.e., active learning). The learner's goal is\nto find the (approximately) best-$k$ items with a given confidence, while\ntrying to use as few comparisons as possible. In this paper, we study two\nproblems: (i) finding the probably approximately correct (PAC) best-$k$ items\nand (ii) finding the exact best-$k$ items, both under strong stochastic\ntransitivity and stochastic triangle inequality. For PAC best-$k$ items\nselection, we first show a lower bound and then propose an algorithm whose\nsample complexity upper bound matches the lower bound up to a constant factor.\nFor the exact best-$k$ items selection, we first prove a worst-instance lower\nbound. We then propose two algorithms based on our PAC best items selection\nalgorithms: one works for $k=1$ and is sample complexity optimal up to a loglog\nfactor, and the other works for all values of $k$ and is sample complexity\noptimal up to a log factor.",
          "link": "http://arxiv.org/abs/2007.03133",
          "publishedOn": "2021-08-02T01:58:25.185Z",
          "wordCount": 681,
          "title": "The Sample Complexity of Best-$k$ Items Selection from Pairwise Comparisons. (arXiv:2007.03133v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.11972",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1\">Wanfang Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1\">Yuxiao Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Reich_B/0/1/0/all/0/1\">Brian J Reich</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_Y/0/1/0/all/0/1\">Ying Sun</a>",
          "description": "In spatial statistics, a common objective is to predict the values of a\nspatial process at unobserved locations by exploiting spatial dependence. In\ngeostatistics, Kriging provides the best linear unbiased predictor using\ncovariance functions and is often associated with Gaussian processes. However,\nwhen considering non-linear prediction for non-Gaussian and categorical data,\nthe Kriging prediction is not necessarily optimal, and the associated variance\nis often overly optimistic. We propose to use deep neural networks (DNNs) for\nspatial prediction. Although DNNs are widely used for general classification\nand prediction, they have not been studied thoroughly for data with spatial\ndependence. In this work, we propose a novel neural network structure for\nspatial prediction by adding an embedding layer of spatial coordinates with\nbasis functions. We show in theory that the proposed DeepKriging method has\nmultiple advantages over Kriging and classical DNNs only with spatial\ncoordinates as features. We also provide density prediction for uncertainty\nquantification without any distributional assumption and apply the method to\nPM$_{2.5}$ concentrations across the continental United States.",
          "link": "http://arxiv.org/abs/2007.11972",
          "publishedOn": "2021-08-02T01:58:25.166Z",
          "wordCount": 637,
          "title": "DeepKriging: Spatially Dependent Deep Neural Networks for Spatial Prediction. (arXiv:2007.11972v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10596",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lederer_A/0/1/0/all/0/1\">Armin Lederer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Capone_A/0/1/0/all/0/1\">Alexandre Capone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beckers_T/0/1/0/all/0/1\">Thomas Beckers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Umlauft_J/0/1/0/all/0/1\">Jonas Umlauft</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hirche_S/0/1/0/all/0/1\">Sandra Hirche</a>",
          "description": "Despite the existence of formal guarantees for learning-based control\napproaches, the relationship between data and control performance is still\npoorly understood. In this paper, we propose a Lyapunov-based measure for\nquantifying the impact of data on the certifiable control performance. By\nmodeling unknown system dynamics through Gaussian processes, we can determine\nthe interrelation between model uncertainty and satisfaction of stability\nconditions. This allows us to directly asses the impact of data on the provable\nstationary control performance, and thereby the value of the data for the\nclosed-loop system performance. Our approach is applicable to a wide variety of\nunknown nonlinear systems that are to be controlled by a generic learning-based\ncontrol law, and the results obtained in numerical simulations indicate the\nefficacy of the proposed measure.",
          "link": "http://arxiv.org/abs/2011.10596",
          "publishedOn": "2021-08-02T01:58:25.148Z",
          "wordCount": 595,
          "title": "The Impact of Data on the Stability of Learning-Based Control- Extended Version. (arXiv:2011.10596v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zijian Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>",
          "description": "In this paper, we propose MINE to perform novel view synthesis and depth\nestimation via dense 3D reconstruction from a single image. Our approach is a\ncontinuous depth generalization of the Multiplane Images (MPI) by introducing\nthe NEural radiance fields (NeRF). Given a single image as input, MINE predicts\na 4-channel image (RGB and volume density) at arbitrary depth values to jointly\nreconstruct the camera frustum and fill in occluded contents. The reconstructed\nand inpainted frustum can then be easily rendered into novel RGB or depth views\nusing differentiable rendering. Extensive experiments on RealEstate10K, KITTI\nand Flowers Light Fields show that our MINE outperforms state-of-the-art by a\nlarge margin in novel view synthesis. We also achieve competitive results in\ndepth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our\nsource code is available at https://github.com/vincentfung13/MINE",
          "link": "http://arxiv.org/abs/2103.14910",
          "publishedOn": "2021-08-02T01:58:25.136Z",
          "wordCount": 639,
          "title": "MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis. (arXiv:2103.14910v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.00241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vadera_S/0/1/0/all/0/1\">Sunil Vadera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ameen_S/0/1/0/all/0/1\">Salem Ameen</a>",
          "description": "This paper presents a survey of methods for pruning deep neural networks. It\nbegins by categorising over 150 studies based on the underlying approach used\nand then focuses on three categories: methods that use magnitude based pruning,\nmethods that utilise clustering to identify redundancy, and methods that use\nsensitivity analysis to assess the effect of pruning. Some of the key\ninfluencing studies within these categories are presented to highlight the\nunderlying approaches and results achieved. Most studies present results which\nare distributed in the literature as new architectures, algorithms and data\nsets have developed with time, making comparison across different studied\ndifficult. The paper therefore provides a resource for the community that can\nbe used to quickly compare the results from many different methods on a variety\nof data sets, and a range of architectures, including AlexNet, ResNet, DenseNet\nand VGG. The resource is illustrated by comparing the results published for\npruning AlexNet and ResNet50 on ImageNet and ResNet56 and VGG16 on the CIFAR10\ndata to reveal which pruning methods work well in terms of retaining accuracy\nwhilst achieving good compression rates. The paper concludes by identifying\nsome promising directions for future research.",
          "link": "http://arxiv.org/abs/2011.00241",
          "publishedOn": "2021-08-02T01:58:25.131Z",
          "wordCount": 662,
          "title": "Methods for Pruning Deep Neural Networks. (arXiv:2011.00241v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.01005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Safran_I/0/1/0/all/0/1\">Itay Safran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yehudai_G/0/1/0/all/0/1\">Gilad Yehudai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1\">Ohad Shamir</a>",
          "description": "We study the effects of mild over-parameterization on the optimization\nlandscape of a simple ReLU neural network of the form\n$\\mathbf{x}\\mapsto\\sum_{i=1}^k\\max\\{0,\\mathbf{w}_i^{\\top}\\mathbf{x}\\}$, in a\nwell-studied teacher-student setting where the target values are generated by\nthe same architecture, and when directly optimizing over the population squared\nloss with respect to Gaussian inputs. We prove that while the objective is\nstrongly convex around the global minima when the teacher and student networks\npossess the same number of neurons, it is not even \\emph{locally convex} after\nany amount of over-parameterization. Moreover, related desirable properties\n(e.g., one-point strong convexity and the Polyak-{\\L}ojasiewicz condition) also\ndo not hold even locally. On the other hand, we establish that the objective\nremains one-point strongly convex in \\emph{most} directions (suitably defined),\nand show an optimization guarantee under this property. For the non-global\nminima, we prove that adding even just a single neuron will turn a non-global\nminimum into a saddle point. This holds under some technical conditions which\nwe validate empirically. These results provide a possible explanation for why\nrecovering a global minimum becomes significantly easier when we\nover-parameterize, even if the amount of over-parameterization is very\nmoderate.",
          "link": "http://arxiv.org/abs/2006.01005",
          "publishedOn": "2021-08-02T01:58:25.115Z",
          "wordCount": 662,
          "title": "The Effects of Mild Over-parameterization on the Optimization Landscape of Shallow ReLU Neural Networks. (arXiv:2006.01005v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lederer_A/0/1/0/all/0/1\">Armin Lederer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conejo_A/0/1/0/all/0/1\">Alejandro Jose Ordonez Conejo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_K/0/1/0/all/0/1\">Korbinian Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wenxin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umlauft_J/0/1/0/all/0/1\">Jonas Umlauft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirche_S/0/1/0/all/0/1\">Sandra Hirche</a>",
          "description": "The increased demand for online prediction and the growing availability of\nlarge data sets drives the need for computationally efficient models. While\nexact Gaussian process regression shows various favorable theoretical\nproperties (uncertainty estimate, unlimited expressive power), the poor scaling\nwith respect to the training set size prohibits its application in big data\nregimes in real-time. Therefore, this paper proposes dividing local Gaussian\nprocesses, which are a novel, computationally efficient modeling approach based\non Gaussian process regression. Due to an iterative, data-driven division of\nthe input space, they achieve a sublinear computational complexity in the total\nnumber of training points in practice, while providing excellent predictive\ndistributions. A numerical evaluation on real-world data sets shows their\nadvantages over other state-of-the-art methods in terms of accuracy as well as\nprediction and update speed.",
          "link": "http://arxiv.org/abs/2006.09446",
          "publishedOn": "2021-08-02T01:58:25.097Z",
          "wordCount": 601,
          "title": "Real-Time Regression with Dividing Local Gaussian Processes. (arXiv:2006.09446v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berube_C/0/1/0/all/0/1\">Charles L. B&#xe9;rub&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berube_P/0/1/0/all/0/1\">Pierre B&#xe9;rub&#xe9;</a>",
          "description": "We present a novel approach for data-driven modeling of the time-domain\ninduced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs\nare Bayesian neural networks that aim to learn a latent statistical\ndistribution to encode extensive data sets as lower dimension representations.\nWe collected 1 600 319 IP decay curves in various regions of Canada, the United\nStates and Kazakhstan, and compiled them to train a deep VAE. The proposed deep\nlearning approach is strictly unsupervised and data-driven: it does not require\nmanual processing or ground truth labeling of IP data. Moreover, our VAE\napproach avoids the pitfalls of IP parametrization with the empirical Cole-Cole\nand Debye decomposition models, simple power-law models, or other sophisticated\nmechanistic models. We demonstrate four applications of VAEs to model and\nprocess IP data: (1) representative synthetic data generation, (2) unsupervised\nBayesian denoising and data uncertainty estimation, (3) quantitative evaluation\nof the signal-to-noise ratio, and (4) automated outlier detection. We also\ninterpret the IP compilation's latent representation and reveal a strong\ncorrelation between its first dimension and the average chargeability of IP\ndecays. Finally, we experiment with varying VAE latent space dimensions and\ndemonstrate that a single real-valued scalar parameter contains sufficient\ninformation to encode our extensive IP data compilation. This new finding\nsuggests that modeling time-domain IP data using mathematical models governed\nby more than one free parameter is ambiguous, whereas modeling only the average\nchargeability is justified. A pre-trained implementation of our model --\nreadily applicable to new IP data from any geolocation -- is available as\nopen-source Python code for the applied geophysics community.",
          "link": "http://arxiv.org/abs/2107.14796",
          "publishedOn": "2021-08-02T01:58:25.066Z",
          "wordCount": 728,
          "title": "Data-driven modeling of time-domain induced polarization. (arXiv:2107.14796v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Damak_K/0/1/0/all/0/1\">Khalil Damak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khenissi_S/0/1/0/all/0/1\">Sami Khenissi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasraoui_O/0/1/0/all/0/1\">Olfa Nasraoui</a>",
          "description": "Recent work in recommender systems has emphasized the importance of fairness,\nwith a particular interest in bias and transparency, in addition to predictive\naccuracy. In this paper, we focus on the state of the art pairwise ranking\nmodel, Bayesian Personalized Ranking (BPR), which has previously been found to\noutperform pointwise models in predictive accuracy, while also being able to\nhandle implicit feedback. Specifically, we address two limitations of BPR: (1)\nBPR is a black box model that does not explain its outputs, thus limiting the\nuser's trust in the recommendations, and the analyst's ability to scrutinize a\nmodel's outputs; and (2) BPR is vulnerable to exposure bias due to the data\nbeing Missing Not At Random (MNAR). This exposure bias usually translates into\nan unfairness against the least popular items because they risk being\nunder-exposed by the recommender system. In this work, we first propose a novel\nexplainable loss function and a corresponding Matrix Factorization-based model\ncalled Explainable Bayesian Personalized Ranking (EBPR) that generates\nrecommendations along with item-based explanations. Then, we theoretically\nquantify additional exposure bias resulting from the explainability, and use it\nas a basis to propose an unbiased estimator for the ideal EBPR loss. The result\nis a ranking model that aptly captures both debiased and explainable user\npreferences. Finally, we perform an empirical study on three real-world\ndatasets that demonstrate the advantages of our proposed models.",
          "link": "http://arxiv.org/abs/2107.14768",
          "publishedOn": "2021-08-02T01:58:25.056Z",
          "wordCount": 695,
          "title": "Debiased Explainable Pairwise Ranking from Implicit Feedback. (arXiv:2107.14768v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soutif__Cormerais_A/0/1/0/all/0/1\">Albin Soutif--Cormerais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masana_M/0/1/0/all/0/1\">Marc Masana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost Van de Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1\">Bart&#x142;omiej Twardowski</a>",
          "description": "In class-incremental learning, an agent with limited resources needs to learn\na sequence of classification tasks, forming an ever growing classification\nproblem, with the constraint of not being able to access data from previous\ntasks. The main difference with task-incremental learning, where a task-ID is\navailable at inference time, is that the learner also needs to perform\ncross-task discrimination, i.e. distinguish between classes that have not been\nseen together. Approaches to tackle this problem are numerous and mostly make\nuse of an external memory (buffer) of non-negligible size. In this paper, we\nablate the learning of cross-task features and study its influence on the\nperformance of basic replay strategies used for class-IL. We also define a new\nforgetting measure for class-incremental learning, and see that forgetting is\nnot the principal cause of low performance. Our experimental results show that\nfuture algorithms for class-incremental learning should not only prevent\nforgetting, but also aim to improve the quality of the cross-task features, and\nthe knowledge transfer between tasks. This is especially important when tasks\ncontain limited amount of data.",
          "link": "http://arxiv.org/abs/2106.11930",
          "publishedOn": "2021-08-02T01:58:25.046Z",
          "wordCount": 651,
          "title": "On the importance of cross-task features for class-incremental learning. (arXiv:2106.11930v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xuezhong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jingyu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_C/0/1/0/all/0/1\">Cheng Zhuo</a>",
          "description": "As technology scaling is approaching the physical limit, lithography hotspot\ndetection has become an essential task in design for manufacturability. While\nthe deployment of pattern matching or machine learning in hotspot detection can\nhelp save significant simulation time, such methods typically demand for\nnon-trivial quality data to build the model, which most design houses are short\nof. Moreover, the design houses are also unwilling to directly share such data\nwith the other houses to build a unified model, which can be ineffective for\nthe design house with unique design patterns due to data insufficiency. On the\nother hand, with data homogeneity in each design house, the locally trained\nmodels can be easily over-fitted, losing generalization ability and robustness.\nIn this paper, we propose a heterogeneous federated learning framework for\nlithography hotspot detection that can address the aforementioned issues. On\none hand, the framework can build a more robust centralized global sub-model\nthrough heterogeneous knowledge sharing while keeping local data private. On\nthe other hand, the global sub-model can be combined with a local sub-model to\nbetter adapt to local data heterogeneity. The experimental results show that\nthe proposed framework can overcome the challenge of non-independent and\nidentically distributed (non-IID) data and heterogeneous communication to\nachieve very high performance in comparison to other state-of-the-art methods\nwhile guaranteeing a good convergence rate in various scenarios.",
          "link": "http://arxiv.org/abs/2107.04367",
          "publishedOn": "2021-08-02T01:58:25.022Z",
          "wordCount": 690,
          "title": "Lithography Hotspot Detection via Heterogeneous Federated Learning with Local Adaptation. (arXiv:2107.04367v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mozo_A/0/1/0/all/0/1\">Alberto Mozo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Prieto_A/0/1/0/all/0/1\">&#xc1;ngel Gonz&#xe1;lez-Prieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pastor_A/0/1/0/all/0/1\">Antonio Pastor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Canaval_S/0/1/0/all/0/1\">Sandra G&#xf3;mez-Canaval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talavera_E/0/1/0/all/0/1\">Edgar Talavera</a>",
          "description": "Due to the growing rise of cyber attacks in the Internet, flow-based data\nsets are crucial to increase the performance of the Machine Learning (ML)\ncomponents that run in network-based intrusion detection systems (IDS). To\novercome the existing network traffic data shortage in attack analysis, recent\nworks propose Generative Adversarial Networks (GANs) for synthetic flow-based\nnetwork traffic generation. Data privacy is appearing more and more as a strong\nrequirement when processing such network data, which suggests to find solutions\nwhere synthetic data can fully replace real data. Because of the\nill-convergence of the GAN training, none of the existing solutions can\ngenerate high-quality fully synthetic data that can totally substitute real\ndata in the training of IDS ML components. Therefore, they mix real with\nsynthetic data, which acts only as data augmentation components, leading to\nprivacy breaches as real data is used. In sharp contrast, in this work we\npropose a novel deterministic way to measure the quality of the synthetic data\nproduced by a GAN both with respect to the real data and to its performance\nwhen used for ML tasks. As a byproduct, we present a heuristic that uses these\nmetrics for selecting the best performing generator during GAN training,\nleading to a stopping criterion. An additional heuristic is proposed to select\nthe best performing GANs when different types of synthetic data are to be used\nin the same ML task. We demonstrate the adequacy of our proposal by generating\nsynthetic cryptomining attack traffic and normal traffic flow-based data using\nan enhanced version of a Wasserstein GAN. We show that the generated synthetic\nnetwork traffic can completely replace real data when training a ML-based\ncryptomining detector, obtaining similar performance and avoiding privacy\nviolations, since real data is not used in the training of the ML-based\ndetector.",
          "link": "http://arxiv.org/abs/2107.14776",
          "publishedOn": "2021-08-02T01:58:25.005Z",
          "wordCount": 748,
          "title": "Synthetic flow-based cryptomining attack generation through Generative Adversarial Networks. (arXiv:2107.14776v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Baihe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason D. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>",
          "description": "This paper considers two-player zero-sum finite-horizon Markov games with\nsimultaneous moves. The study focuses on the challenging settings where the\nvalue function or the model is parameterized by general function classes.\nProvably efficient algorithms for both decoupled and {coordinated} settings are\ndeveloped. In the {decoupled} setting where the agent controls a single player\nand plays against an arbitrary opponent, we propose a new model-free algorithm.\nThe sample complexity is governed by the Minimax Eluder dimension -- a new\ndimension of the function class in Markov games. As a special case, this method\nimproves the state-of-the-art algorithm by a $\\sqrt{d}$ factor in the regret\nwhen the reward function and transition kernel are parameterized with\n$d$-dimensional linear features. In the {coordinated} setting where both\nplayers are controlled by the agent, we propose a model-based algorithm and a\nmodel-free algorithm. In the model-based algorithm, we prove that sample\ncomplexity can be bounded by a generalization of Witness rank to Markov games.\nThe model-free algorithm enjoys a $\\sqrt{K}$-regret upper bound where $K$ is\nthe number of episodes. Our algorithms are based on new techniques of alternate\noptimism.",
          "link": "http://arxiv.org/abs/2107.14702",
          "publishedOn": "2021-08-02T01:58:24.998Z",
          "wordCount": 629,
          "title": "Towards General Function Approximation in Zero-Sum Markov Games. (arXiv:2107.14702v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xiaomin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lihang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jieqiong Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Donglong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanzhuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>",
          "description": "Effective molecular representation learning is of great importance to\nfacilitate molecular property prediction, which is a fundamental task for the\ndrug and material industry. Recent advances in graph neural networks (GNNs)\nhave shown great promise in applying GNNs for molecular representation\nlearning. Moreover, a few recent studies have also demonstrated successful\napplications of self-supervised learning methods to pre-train the GNNs to\novercome the problem of insufficient labeled molecules. However, existing GNNs\nand pre-training strategies usually treat molecules as topological graph data\nwithout fully utilizing the molecular geometry information. Whereas, the\nthree-dimensional (3D) spatial structure of a molecule, a.k.a molecular\ngeometry, is one of the most critical factors for determining molecular\nphysical, chemical, and biological properties. To this end, we propose a novel\nGeometry Enhanced Molecular representation learning method (GEM) for Chemical\nRepresentation Learning (ChemRL). At first, we design a geometry-based GNN\narchitecture that simultaneously models atoms, bonds, and bond angles in a\nmolecule. To be specific, we devised double graphs for a molecule: The first\none encodes the atom-bond relations; The second one encodes bond-angle\nrelations. Moreover, on top of the devised GNN architecture, we propose several\nnovel geometry-level self-supervised learning strategies to learn spatial\nknowledge by utilizing the local and global molecular 3D structures. We compare\nChemRL-GEM with various state-of-the-art (SOTA) baselines on different\nmolecular benchmarks and exhibit that ChemRL-GEM can significantly outperform\nall baselines in both regression and classification tasks. For example, the\nexperimental results show an overall improvement of 8.8% on average compared to\nSOTA baselines on the regression tasks, demonstrating the superiority of the\nproposed method.",
          "link": "http://arxiv.org/abs/2106.06130",
          "publishedOn": "2021-08-02T01:58:24.993Z",
          "wordCount": 751,
          "title": "ChemRL-GEM: Geometry Enhanced Molecular Representation Learning for Property Prediction. (arXiv:2106.06130v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14608",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rusu_C/0/1/0/all/0/1\">Cristian Rusu</a>",
          "description": "In this paper, we describe a new algorithm to build a few sparse principal\ncomponents from a given data matrix. Our approach does not explicitly create\nthe covariance matrix of the data and can be viewed as an extension of the\nKogbetliantz algorithm to build an approximate singular value decomposition for\na few principal components. We show the performance of the proposed algorithm\nto recover sparse principal components on various datasets from the literature\nand perform dimensionality reduction for classification applications.",
          "link": "http://arxiv.org/abs/2107.14608",
          "publishedOn": "2021-08-02T01:58:24.987Z",
          "wordCount": 527,
          "title": "An iterative coordinate descent algorithm to compute sparse low-rank approximations. (arXiv:2107.14608v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kassis_A/0/1/0/all/0/1\">Andre Kassis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengartner_U/0/1/0/all/0/1\">Urs Hengartner</a>",
          "description": "Voice authentication has become an integral part in security-critical\noperations, such as bank transactions and call center conversations. The\nvulnerability of automatic speaker verification systems (ASVs) to spoofing\nattacks instigated the development of countermeasures (CMs), whose task is to\ntell apart bonafide and spoofed speech. Together, ASVs and CMs form today's\nvoice authentication platforms, advertised as an impregnable access control\nmechanism. We develop the first practical attack on CMs, and show how a\nmalicious actor may efficiently craft audio samples to bypass voice\nauthentication in its strictest form. Previous works have primarily focused on\nnon-proactive attacks or adversarial strategies against ASVs that do not\nproduce speech in the victim's voice. The repercussions of our attacks are far\nmore severe, as the samples we generate sound like the victim, eliminating any\nchance of plausible deniability. Moreover, the few existing adversarial attacks\nagainst CMs mistakenly optimize spoofed speech in the feature space and do not\ntake into account the existence of ASVs, resulting in inferior synthetic audio\nthat fails in realistic settings. We eliminate these obstacles through our key\ntechnical contribution: a novel joint loss function that enables mounting\nadvanced adversarial attacks against combined ASV/CM deployments directly in\nthe time domain. Our adversarials achieve concerning black-box success rates\nagainst state-of-the-art authentication platforms (up to 93.57\\%). Finally, we\nperform the first targeted, over-telephony-network attack on CMs, bypassing\nseveral challenges and enabling various potential threats, given the increased\nuse of voice biometrics in call centers. Our results call into question the\nsecurity of modern voice authentication systems in light of the real threat of\nattackers bypassing these measures to gain access to users' most valuable\nresources.",
          "link": "http://arxiv.org/abs/2107.14642",
          "publishedOn": "2021-08-02T01:58:24.982Z",
          "wordCount": 710,
          "title": "Practical Attacks on Voice Spoofing Countermeasures. (arXiv:2107.14642v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14747",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Coifman_R/0/1/0/all/0/1\">Ronald R. Coifman</a>, <a href=\"http://arxiv.org/find/math/1/au:+Marshall_N/0/1/0/all/0/1\">Nicholas F. Marshall</a>, <a href=\"http://arxiv.org/find/math/1/au:+Steinerberger_S/0/1/0/all/0/1\">Stefan Steinerberger</a>",
          "description": "Let $\\mathcal{G} = \\{G_1 = (V, E_1), \\dots, G_m = (V, E_m)\\}$ be a collection\nof $m$ graphs defined on a common set of vertices $V$ but with different edge\nsets $E_1, \\dots, E_m$. Informally, a function $f :V \\rightarrow \\mathbb{R}$ is\nsmooth with respect to $G_k = (V,E_k)$ if $f(u) \\sim f(v)$ whenever $(u, v) \\in\nE_k$. We study the problem of understanding whether there exists a nonconstant\nfunction that is smooth with respect to all graphs in $\\mathcal{G}$,\nsimultaneously, and how to find it if it exists.",
          "link": "http://arxiv.org/abs/2107.14747",
          "publishedOn": "2021-08-02T01:58:24.976Z",
          "wordCount": 526,
          "title": "A common variable minimax theorem for graphs. (arXiv:2107.14747v1 [math.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2005.04954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_T/0/1/0/all/0/1\">Tatsuya Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_A/0/1/0/all/0/1\">Atsuyoshi Nakamura</a>",
          "description": "Various things propagate through the medium of individuals. Some individuals\nfollow the others and take the states similar to their states a small number of\ntime steps later. In this paper, we study the problem of estimating the state\npropagation order of individuals from the real-valued state sequences of all\nthe individuals. We propose a method to estimate the propagation direction\nbetween individuals by the sum of the time delay of one individual's state\npositions from the other individual's matched state position averaged over the\nminimum cost alignments and show how to calculate it efficiently. The\npropagation order estimated by our proposed method is demonstrated to be\nsignificantly more accurate than that by a baseline method for our synthetic\ndatasets, and also to be consistent with visually recognizable propagation\norders for the dataset of Japanese stock price time series and biological cell\nfiring state sequences.",
          "link": "http://arxiv.org/abs/2005.04954",
          "publishedOn": "2021-08-02T01:58:24.958Z",
          "wordCount": 615,
          "title": "Propagation Graph Estimation from Individual's Time Series of Observed States. (arXiv:2005.04954v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.12377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Landi_F/0/1/0/all/0/1\">Federico Landi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corsini_M/0/1/0/all/0/1\">Massimiliano Corsini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Vision-and-Language Navigation (VLN) is a challenging task in which an agent\nneeds to follow a language-specified path to reach a target destination. The\ngoal gets even harder as the actions available to the agent get simpler and\nmove towards low-level, atomic interactions with the environment. This setting\ntakes the name of low-level VLN. In this paper, we strive for the creation of\nan agent able to tackle three key issues: multi-modality, long-term\ndependencies, and adaptability towards different locomotive settings. To that\nend, we devise \"Perceive, Transform, and Act\" (PTA): a fully-attentive VLN\narchitecture that leaves the recurrent approach behind and the first\nTransformer-like architecture incorporating three different modalities -\nnatural language, images, and low-level actions for the agent control. In\nparticular, we adopt an early fusion strategy to merge lingual and visual\ninformation efficiently in our encoder. We then propose to refine the decoding\nphase with a late fusion extension between the agent's history of actions and\nthe perceptual modalities. We experimentally validate our model on two\ndatasets: PTA achieves promising results in low-level VLN on R2R and achieves\ngood performance in the recently proposed R4R benchmark. Our code is publicly\navailable at https://github.com/aimagelab/perceive-transform-and-act.",
          "link": "http://arxiv.org/abs/1911.12377",
          "publishedOn": "2021-08-02T01:58:24.951Z",
          "wordCount": 687,
          "title": "Multimodal Attention Networks for Low-Level Vision-and-Language Navigation. (arXiv:1911.12377v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.15106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McLaughlin_C/0/1/0/all/0/1\">Connor J. McLaughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokkotou_E/0/1/0/all/0/1\">Efi G. Kokkotou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_J/0/1/0/all/0/1\">Jean A. King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conboy_L/0/1/0/all/0/1\">Lisa A. Conboy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousefi_A/0/1/0/all/0/1\">Ali Yousefi</a>",
          "description": "The analysis of clinical questionnaire data comes with many inherent\nchallenges. These challenges include the handling of data with missing fields,\nas well as the overall interpretation of a dataset with many fields of\ndifferent scales and forms. While numerous methods have been developed to\naddress these challenges, they are often not robust, statistically sound, or\neasily interpretable. Here, we propose a latent factor modeling framework that\nextends the principal component analysis for both categorical and quantitative\ndata with missing elements. The model simultaneously provides the principal\ncomponents (basis) and each patients' projections on these bases in a latent\nspace. We show an application of our modeling framework through Irritable Bowel\nSyndrome (IBS) symptoms, where we find correlations between these projections\nand other standardized patient symptom scales. This latent factor model can be\neasily applied to different clinical questionnaire datasets for clustering\nanalysis and interpretable inference.",
          "link": "http://arxiv.org/abs/2104.15106",
          "publishedOn": "2021-08-02T01:58:24.944Z",
          "wordCount": 628,
          "title": "Latent Factor Decomposition Model: Applications for Questionnaire Data. (arXiv:2104.15106v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14695",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Ekapure_S/0/1/0/all/0/1\">Shubham Ekapure</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Jiruwala_N/0/1/0/all/0/1\">Nuruddin Jiruwala</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Patnaik_S/0/1/0/all/0/1\">Sohan Patnaik</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+SenGupta_I/0/1/0/all/0/1\">Indranil SenGupta</a>",
          "description": "In this paper, we implement a combination of technical analysis and\nmachine/deep learning-based analysis to build a trend classification model. The\ngoal of the paper is to apprehend short-term market movement, and incorporate\nit to improve the underlying stochastic model. Also, the analysis presented in\nthis paper can be implemented in a \\emph{model-independent} fashion. We execute\na data-science-driven technique that makes short-term forecasts dependent on\nthe price trends of current stock market data. Based on the analysis, three\ndifferent labels are generated for a data set: $+1$ (buy signal), $0$ (hold\nsignal), or $-1$ (sell signal). We propose a detailed analysis of four major\nstocks- Amazon, Apple, Google, and Microsoft. We implement various technical\nindicators to label the data set according to the trend and train various\nmodels for trend estimation. Statistical analysis of the outputs and\nclassification results are obtained.",
          "link": "http://arxiv.org/abs/2107.14695",
          "publishedOn": "2021-08-02T01:58:24.936Z",
          "wordCount": 602,
          "title": "A data-science-driven short-term analysis of Amazon, Apple, Google, and Microsoft stocks. (arXiv:2107.14695v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2101.07240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kutuzova_S/0/1/0/all/0/1\">Svetlana Kutuzova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1\">Oswin Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCloskey_D/0/1/0/all/0/1\">Douglas McCloskey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_M/0/1/0/all/0/1\">Mads Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1\">Christian Igel</a>",
          "description": "Multimodal generative models should be able to learn a meaningful latent\nrepresentation that enables a coherent joint generation of all modalities\n(e.g., images and text). Many applications also require the ability to\naccurately sample modalities conditioned on observations of a subset of the\nmodalities. Often not all modalities may be observed for all training data\npoints, so semi-supervised learning should be possible. In this study, we\npropose a novel product-of-experts (PoE) based variational autoencoder that\nhave these desired properties. We benchmark it against a mixture-of-experts\n(MoE) approach and an approach of combining the modalities with an additional\nencoder network. An empirical evaluation shows that the PoE based models can\noutperform the contrasted models. Our experiments support the intuition that\nPoE models are more suited for a conjunctive combination of modalities.",
          "link": "http://arxiv.org/abs/2101.07240",
          "publishedOn": "2021-08-02T01:58:24.930Z",
          "wordCount": 597,
          "title": "Multimodal Variational Autoencoders for Semi-Supervised Learning: In Defense of Product-of-Experts. (arXiv:2101.07240v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yun-Yun Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>",
          "description": "Learning to classify time series with limited data is a practical yet\nchallenging problem. Current methods are primarily based on hand-designed\nfeature extraction rules or domain-specific data augmentation. Motivated by the\nadvances in deep speech processing models and the fact that voice data are\nunivariate temporal signals, in this paper, we propose Voice2Series (V2S), a\nnovel end-to-end approach that reprograms acoustic models for time series\nclassification, through input transformation learning and output label mapping.\nLeveraging the representation learning power of a large-scale pre-trained\nspeech processing model, on 30 different time series tasks we show that V2S\neither outperforms or is tied with state-of-the-art methods on 20 tasks, and\nimproves their average accuracy by 1.84%. We further provide a theoretical\njustification of V2S by proving its population risk is upper bounded by the\nsource risk and a Wasserstein distance accounting for feature alignment via\nreprogramming. Our results offer new and effective means to time series\nclassification.",
          "link": "http://arxiv.org/abs/2106.09296",
          "publishedOn": "2021-08-02T01:58:24.913Z",
          "wordCount": 672,
          "title": "Voice2Series: Reprogramming Acoustic Models for Time Series Classification. (arXiv:2106.09296v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scholler_C/0/1/0/all/0/1\">Christoph Sch&#xf6;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a>",
          "description": "The future motion of traffic participants is inherently uncertain. To plan\nsafely, therefore, an autonomous agent must take into account multiple possible\ntrajectory outcomes and prioritize them. Recently, this problem has been\naddressed with generative neural networks. However, most generative models\neither do not learn the true underlying trajectory distribution reliably, or do\nnot allow predictions to be associated with likelihoods. In our work, we model\nmotion prediction directly as a density estimation problem with a normalizing\nflow between a noise distribution and the future motion distribution. Our\nmodel, named FloMo, allows likelihoods to be computed in a single network pass\nand can be trained directly with maximum likelihood estimation. Furthermore, we\npropose a method to stabilize training flows on trajectory datasets and a new\ndata augmentation transformation that improves the performance and\ngeneralization of our model. Our method achieves state-of-the-art performance\non three popular prediction datasets, with a significant gap to most competing\nmodels.",
          "link": "http://arxiv.org/abs/2103.03614",
          "publishedOn": "2021-08-02T01:58:24.907Z",
          "wordCount": 634,
          "title": "FloMo: Tractable Motion Prediction with Normalizing Flows. (arXiv:2103.03614v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14664",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Antunes_L/0/1/0/all/0/1\">Luis M. Antunes</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Grau_Crespo_R/0/1/0/all/0/1\">Ricardo Grau-Crespo</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Butler_K/0/1/0/all/0/1\">Keith T. Butler</a>",
          "description": "The use of machine learning is becoming increasingly common in computational\nmaterials science. To build effective models of the chemistry of materials,\nuseful machine-based representations of atoms and their compounds are required.\nWe derive distributed representations of compounds from their chemical formulas\nonly, via pooling operations of distributed representations of atoms. These\ncompound representations are evaluated on ten different tasks, such as the\nprediction of formation energy and band gap, and are found to be competitive\nwith existing benchmarks that make use of structure, and even superior in cases\nwhere only composition is available. Finally, we introduce a new approach for\nlearning distributed representations of atoms, named SkipAtom, which makes use\nof the growing information in materials structure databases.",
          "link": "http://arxiv.org/abs/2107.14664",
          "publishedOn": "2021-08-02T01:58:24.902Z",
          "wordCount": 556,
          "title": "Distributed Representations of Atoms and Materials for Machine Learning. (arXiv:2107.14664v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2007.03767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozdayi_M/0/1/0/all/0/1\">Mustafa Safa Ozdayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kantarcioglu_M/0/1/0/all/0/1\">Murat Kantarcioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gel_Y/0/1/0/all/0/1\">Yulia R. Gel</a>",
          "description": "Federated learning (FL) allows a set of agents to collaboratively train a\nmodel without sharing their potentially sensitive data. This makes FL suitable\nfor privacy-preserving applications. At the same time, FL is susceptible to\nadversarial attacks due to decentralized and unvetted data. One important line\nof attacks against FL is the backdoor attacks. In a backdoor attack, an\nadversary tries to embed a backdoor functionality to the model during training\nthat can later be activated to cause a desired misclassification. To prevent\nbackdoor attacks, we propose a lightweight defense that requires minimal change\nto the FL protocol. At a high level, our defense is based on carefully\nadjusting the aggregation server's learning rate, per dimension and per round,\nbased on the sign information of agents' updates. We first conjecture the\nnecessary steps to carry a successful backdoor attack in FL setting, and then,\nexplicitly formulate the defense based on our conjecture. Through experiments,\nwe provide empirical evidence that supports our conjecture, and we test our\ndefense against backdoor attacks under different settings. We observe that\neither backdoor is completely eliminated, or its accuracy is significantly\nreduced. Overall, our experiments suggest that our defense significantly\noutperforms some of the recently proposed defenses in the literature. We\nachieve this by having minimal influence over the accuracy of the trained\nmodels. In addition, we also provide convergence rate analysis for our proposed\nscheme.",
          "link": "http://arxiv.org/abs/2007.03767",
          "publishedOn": "2021-08-02T01:58:24.896Z",
          "wordCount": 725,
          "title": "Defending against Backdoors in Federated Learning with Robust Learning Rate. (arXiv:2007.03767v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.04591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1\">Sergi Perez-Castanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1\">Javier Naranjo-Alcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1\">Pedro Zuccarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1\">Maximo Cobos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1\">Frances J. Ferri</a>",
          "description": "Acoustic scene classification (ASC) has been approached in the last years\nusing deep learning techniques such as convolutional neural networks or\nrecurrent neural networks. Many state-of-the-art solutions are based on image\nclassification frameworks and, as such, a 2D representation of the audio signal\nis considered for training these networks. Finding the most suitable audio\nrepresentation is still a research area of interest. In this paper, different\nlog-Mel representations and combinations are analyzed. Experiments show that\nthe best results are obtained using the harmonic and percussive components plus\nthe difference between left and right stereo channels, (L-R). On the other\nhand, it is a common strategy to ensemble different models in order to increase\nthe final accuracy. Even though averaging different model predictions is a\ncommon choice, an exhaustive analysis of different ensemble techniques has not\nbeen presented in ASC problems. In this paper, geometric and arithmetic mean\nplus the Ordered Weighted Averaging (OWA) operator are studied as aggregation\noperators for the output of the different models of the ensemble. Finally, the\nwork carried out in this paper is highly oriented towards real-time\nimplementations. In this context, as the number of applications for audio\nclassification on edge devices is increasing exponentially, we also analyze\ndifferent network depths and efficient solutions for aggregating ensemble\npredictions.",
          "link": "http://arxiv.org/abs/1906.04591",
          "publishedOn": "2021-08-02T01:58:24.890Z",
          "wordCount": 707,
          "title": "CNN depth analysis with different channel inputs for Acoustic Scene Classification. (arXiv:1906.04591v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pillai_N/0/1/0/all/0/1\">Nisha Pillai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matuszek_C/0/1/0/all/0/1\">Cynthia Matuszek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1\">Francis Ferraro</a>",
          "description": "We propose a learning system in which language is grounded in visual percepts\nwithout specific pre-defined categories of terms. We present a unified\ngenerative method to acquire a shared semantic/visual embedding that enables\nthe learning of language about a wide range of real-world objects. We evaluate\nthe efficacy of this learning by predicting the semantics of objects and\ncomparing the performance with neural and non-neural inputs. We show that this\ngenerative approach exhibits promising results in language grounding without\npre-specifying visual categories under low resource settings. Our experiments\ndemonstrate that this approach is generalizable to multilingual, highly varied\ndatasets.",
          "link": "http://arxiv.org/abs/2107.14593",
          "publishedOn": "2021-08-02T01:58:24.883Z",
          "wordCount": 552,
          "title": "Neural Variational Learning for Grounded Language Acquisition. (arXiv:2107.14593v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Galinkin_E/0/1/0/all/0/1\">Erick Galinkin</a>",
          "description": "In many cases, neural networks perform well on test data, but tend to\noverestimate their confidence on out-of-distribution data. This has led to\nadoption of Bayesian neural networks, which better capture uncertainty and\ntherefore more accurately reflect the model's confidence. For machine learning\nsecurity researchers, this raises the natural question of how making a model\nBayesian affects the security of the model. In this work, we explore the\ninterplay between Bayesianism and two measures of security: model privacy and\nadversarial robustness. We demonstrate that Bayesian neural networks are more\nvulnerable to membership inference attacks in general, but are at least as\nrobust as their non-Bayesian counterparts to adversarial examples.",
          "link": "http://arxiv.org/abs/2107.14601",
          "publishedOn": "2021-08-02T01:58:24.865Z",
          "wordCount": 531,
          "title": "Who's Afraid of Thomas Bayes?. (arXiv:2107.14601v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.05079",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dewaskar_M/0/1/0/all/0/1\">Miheer Dewaskar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Palowitch_J/0/1/0/all/0/1\">John Palowitch</a>, <a href=\"http://arxiv.org/find/stat/1/au:+He_M/0/1/0/all/0/1\">Mark He</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Love_M/0/1/0/all/0/1\">Michael I. Love</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nobel_A/0/1/0/all/0/1\">Andrew B. Nobel</a>",
          "description": "Data sets in which measurements of different types are obtained from a common\nset of samples appear in many scientific applications. In the analysis of such\ndata, an important problem is to identify groups of features from different\ndata types that are strongly associated. Given two data types, a bimodule is a\npair $(A,B)$ of feature sets from the two types such that the aggregate\ncross-correlation between the features in $A$ and those in $B$ is large. A\nbimodule $(A,B)$ is stable if $A$ coincides with the set of features that have\nsignificant aggregate correlation with the features in $B$, and vice-versa. We\ndevelop an, iterative, testing-based procedure called BSP to identify stable\nbimodules. BSP relies on approximate p-values derived from the permutation\nmoments of sums of squared sample correlations between a single feature of one\ntype and a group of features of the second type. We carry out a thorough\nsimulation study to assess the performance of BSP, and present an extended\napplication to the problem of expression quantitative trait loci (eQTL)\nanalysis using recent data from the GTEx project. In addition, we apply BSP to\nclimatology data to identify regions in North America where annual temperature\nvariation affects precipitation.",
          "link": "http://arxiv.org/abs/2009.05079",
          "publishedOn": "2021-08-02T01:58:24.860Z",
          "wordCount": 686,
          "title": "Finding Stable Groups of Cross-Correlated Features in Two Data Sets With Common Samples. (arXiv:2009.05079v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01174",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Choi_Y/0/1/0/all/0/1\">Yeunju Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_Y/0/1/0/all/0/1\">Youngmoon Jung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suh_Y/0/1/0/all/0/1\">Youngjoo Suh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Hoirin Kim</a>",
          "description": "Although recent end-to-end text-to-speech (TTS) systems have achieved\nhigh-quality synthesized speech, there are still several factors that degrade\nthe quality of synthesized speech, including lack of training data or\ninformation loss during knowledge distillation. To address the problem, we\npropose a novel way to train a TTS model under the supervision of perceptual\nloss, which measures the distance between the maximum speech quality score and\nthe predicted one. We first pre-train a mean opinion score (MOS) prediction\nmodel and then train a TTS model in the direction of maximizing the MOS of\nsynthesized speech predicted by the pre-trained MOS prediction model. Through\nthis method, we can improve the quality of synthesized speech universally\n(i.e., regardless of the network architecture or the cause of the speech\nquality degradation) and efficiently (i.e., without increasing the inference\ntime or the model complexity). The evaluation results for MOS and phoneme error\nrate demonstrate that our proposed approach improves previous models in terms\nof both naturalness and intelligibility.",
          "link": "http://arxiv.org/abs/2011.01174",
          "publishedOn": "2021-08-02T01:58:24.854Z",
          "wordCount": 630,
          "title": "Perceptually Guided End-to-End Text-to-Speech With MOS Prediction. (arXiv:2011.01174v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Disabato_S/0/1/0/all/0/1\">Simone Disabato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roveri_M/0/1/0/all/0/1\">Manuel Roveri</a>",
          "description": "Tiny Machine Learning (TML) is a new research area whose goal is to design\nmachine and deep learning techniques able to operate in Embedded Systems and\nIoT units, hence satisfying the severe technological constraints on memory,\ncomputation, and energy characterizing these pervasive devices. Interestingly,\nthe related literature mainly focused on reducing the computational and memory\ndemand of the inference phase of machine and deep learning models. At the same\ntime, the training is typically assumed to be carried out in Cloud or edge\ncomputing systems (due to the larger memory and computational requirements).\nThis assumption results in TML solutions that might become obsolete when the\nprocess generating the data is affected by concept drift (e.g., due to\nperiodicity or seasonality effect, faults or malfunctioning affecting sensors\nor actuators, or changes in the users' behavior), a common situation in\nreal-world application scenarios. For the first time in the literature, this\npaper introduces a Tiny Machine Learning for Concept Drift (TML-CD) solution\nbased on deep learning feature extractors and a k-nearest neighbors classifier\nintegrating a hybrid adaptation module able to deal with concept drift\naffecting the data-generating process. This adaptation module continuously\nupdates (in a passive way) the knowledge base of TML-CD and, at the same time,\nemploys a Change Detection Test to inspect for changes (in an active way) to\nquickly adapt to concept drift by removing the obsolete knowledge. Experimental\nresults on both image and audio benchmarks show the effectiveness of the\nproposed solution, whilst the porting of TML-CD on three off-the-shelf\nmicro-controller units shows the feasibility of what is proposed in real-world\npervasive systems.",
          "link": "http://arxiv.org/abs/2107.14759",
          "publishedOn": "2021-08-02T01:58:24.848Z",
          "wordCount": 697,
          "title": "Tiny Machine Learning for Concept Drift. (arXiv:2107.14759v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_H/0/1/0/all/0/1\">Hugo Flores Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguilar_A/0/1/0/all/0/1\">Aldo Aguilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manilow_E/0/1/0/all/0/1\">Ethan Manilow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardo_B/0/1/0/all/0/1\">Bryan Pardo</a>",
          "description": "Deep learning work on musical instrument recognition has generally focused on\ninstrument classes for which we have abundant data. In this work, we exploit\nhierarchical relationships between instruments in a few-shot learning setup to\nenable classification of a wider set of musical instruments, given a few\nexamples at inference. We apply a hierarchical loss function to the training of\nprototypical networks, combined with a method to aggregate prototypes\nhierarchically, mirroring the structure of a predefined musical instrument\nhierarchy. These extensions require no changes to the network architecture and\nnew levels can be easily added or removed. Compared to a non-hierarchical\nfew-shot baseline, our method leads to a significant increase in classification\naccuracy and significant decrease mistake severity on instrument classes unseen\nin training.",
          "link": "http://arxiv.org/abs/2107.07029",
          "publishedOn": "2021-08-02T01:58:24.836Z",
          "wordCount": 580,
          "title": "Leveraging Hierarchical Structures for Few-Shot Musical Instrument Recognition. (arXiv:2107.07029v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1\">Jean-Baptiste Alayrac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doersch_C/0/1/0/all/0/1\">Carl Doersch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_C/0/1/0/all/0/1\">Catalin Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">David Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1\">Skanda Koppula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andrew Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1\">Evan Shelhamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1\">Olivier H&#xe9;naff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1\">Matthew M. Botvinick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Jo&#xe3;o Carreira</a>",
          "description": "The recently-proposed Perceiver model obtains good results on several domains\n(images, audio, multimodal, point clouds) while scaling linearly in compute and\nmemory with the input size. While the Perceiver supports many kinds of inputs,\nit can only produce very simple outputs such as class scores. Perceiver IO\novercomes this limitation without sacrificing the original's appealing\nproperties by learning to flexibly query the model's latent space to produce\noutputs of arbitrary size and semantics. Perceiver IO still decouples model\ndepth from data size and still scales linearly with data size, but now with\nrespect to both input and output sizes. The full Perceiver IO model achieves\nstrong results on tasks with highly structured output spaces, such as natural\nlanguage and visual understanding, StarCraft II, and multi-task and multi-modal\ndomains. As highlights, Perceiver IO matches a Transformer-based BERT baseline\non the GLUE language benchmark without the need for input tokenization and\nachieves state-of-the-art performance on Sintel optical flow estimation.",
          "link": "http://arxiv.org/abs/2107.14795",
          "publishedOn": "2021-08-02T01:58:24.821Z",
          "wordCount": 639,
          "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs. (arXiv:2107.14795v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haizhou Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youcai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenjie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>",
          "description": "It is a consensus that small models perform quite poorly under the paradigm\nof self-supervised contrastive learning. Existing methods usually adopt a large\noff-the-shelf model to transfer knowledge to the small one via knowledge\ndistillation. Despite their effectiveness, distillation-based methods may not\nbe suitable for some resource-restricted scenarios due to the huge\ncomputational expenses of deploying a large model. In this paper, we study the\nissue of training self-supervised small models without distillation signals. We\nfirst evaluate the representation spaces of the small models and make two\nnon-negligible observations: (i) small models can complete the pretext task\nwithout overfitting despite its limited capacity; (ii) small models universally\nsuffer the problem of over-clustering. Then we verify multiple assumptions that\nare considered to alleviate the over-clustering phenomenon. Finally, we combine\nthe validated techniques and improve the baseline of five small architectures\nwith considerable margins, which indicates that training small self-supervised\ncontrastive models is feasible even without distillation signals.",
          "link": "http://arxiv.org/abs/2107.14762",
          "publishedOn": "2021-08-02T01:58:24.805Z",
          "wordCount": 610,
          "title": "On the Efficacy of Small Self-Supervised Contrastive Models without Distillation Signals. (arXiv:2107.14762v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gefei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yuling Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Can Yang</a>",
          "description": "We propose to learn a generative model via entropy interpolation with a\nSchr\\\"{o}dinger Bridge. The generative learning task can be formulated as\ninterpolating between a reference distribution and a target distribution based\non the Kullback-Leibler divergence. At the population level, this entropy\ninterpolation is characterized via an SDE on $[0,1]$ with a time-varying drift\nterm. At the sample level, we derive our Schr\\\"{o}dinger Bridge algorithm by\nplugging the drift term estimated by a deep score estimator and a deep density\nratio estimator into the Euler-Maruyama method. Under some mild smoothness\nassumptions of the target distribution, we prove the consistency of both the\nscore estimator and the density ratio estimator, and then establish the\nconsistency of the proposed Schr\\\"{o}dinger Bridge approach. Our theoretical\nresults guarantee that the distribution learned by our approach converges to\nthe target distribution. Experimental results on multimodal synthetic data and\nbenchmark data support our theoretical findings and indicate that the\ngenerative model via Schr\\\"{o}dinger Bridge is comparable with state-of-the-art\nGANs, suggesting a new formulation of generative learning. We demonstrate its\nusefulness in image interpolation and image inpainting.",
          "link": "http://arxiv.org/abs/2106.10410",
          "publishedOn": "2021-08-02T01:58:24.799Z",
          "wordCount": 648,
          "title": "Deep Generative Learning via Schr\\\"{o}dinger Bridge. (arXiv:2106.10410v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14742",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Alt_T/0/1/0/all/0/1\">Tobias Alt</a>, <a href=\"http://arxiv.org/find/math/1/au:+Schrader_K/0/1/0/all/0/1\">Karl Schrader</a>, <a href=\"http://arxiv.org/find/math/1/au:+Augustin_M/0/1/0/all/0/1\">Matthias Augustin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Peter_P/0/1/0/all/0/1\">Pascal Peter</a>, <a href=\"http://arxiv.org/find/math/1/au:+Weickert_J/0/1/0/all/0/1\">Joachim Weickert</a>",
          "description": "We investigate numerous structural connections between numerical algorithms\nfor partial differential equations (PDEs) and neural architectures. Our goal is\nto transfer the rich set of mathematical foundations from the world of PDEs to\nneural networks. Besides structural insights we provide concrete examples and\nexperimental evaluations of the resulting architectures. Using the example of\ngeneralised nonlinear diffusion in 1D, we consider explicit schemes,\nacceleration strategies thereof, implicit schemes, and multigrid approaches. We\nconnect these concepts to residual networks, recurrent neural networks, and\nU-net architectures. Our findings inspire a symmetric residual network design\nwith provable stability guarantees and justify the effectiveness of skip\nconnections in neural networks from a numerical perspective. Moreover, we\npresent U-net architectures that implement multigrid techniques for learning\nefficient solutions of partial differential equation models, and motivate\nuncommon design choices such as trainable nonmonotone activation functions.\nExperimental evaluations show that the proposed architectures save half of the\ntrainable parameters and can thus outperform standard ones with the same model\ncomplexity. Our considerations serve as a basis for explaining the success of\npopular neural architectures and provide a blueprint for developing new\nmathematically well-founded neural building blocks.",
          "link": "http://arxiv.org/abs/2107.14742",
          "publishedOn": "2021-08-02T01:58:24.793Z",
          "wordCount": 627,
          "title": "Connections between Numerical Algorithms for PDEs and Neural Networks. (arXiv:2107.14742v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14803",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Herbreteau_S/0/1/0/all/0/1\">S&#xe9;bastien Herbreteau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kervrann_C/0/1/0/all/0/1\">Charles Kervrann</a>",
          "description": "This work tackles the issue of noise removal from images, focusing on the\nwell-known DCT image denoising algorithm. The latter, stemming from signal\nprocessing, has been well studied over the years. Though very simple, it is\nstill used in crucial parts of state-of-the-art \"traditional\" denoising\nalgorithms such as BM3D. Since a few years however, deep convolutional neural\nnetworks (CNN) have outperformed their traditional counterparts, making signal\nprocessing methods less attractive. In this paper, we demonstrate that a DCT\ndenoiser can be seen as a shallow CNN and thereby its original linear transform\ncan be tuned through gradient descent in a supervised manner, improving\nconsiderably its performance. This gives birth to a fully interpretable CNN\ncalled DCT2net. To deal with remaining artifacts induced by DCT2net, an\noriginal hybrid solution between DCT and DCT2net is proposed combining the best\nthat these two methods can offer; DCT2net is selected to process non-stationary\nimage patches while DCT is optimal for piecewise smooth patches. Experiments on\nartificially noisy images demonstrate that two-layer DCT2net provides\ncomparable results to BM3D and is as fast as DnCNN algorithm composed of more\nthan a dozen of layers.",
          "link": "http://arxiv.org/abs/2107.14803",
          "publishedOn": "2021-08-02T01:58:24.786Z",
          "wordCount": 632,
          "title": "DCT2net: an interpretable shallow CNN for image denoising. (arXiv:2107.14803v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarap_A/0/1/0/all/0/1\">Abien Fred Agarap</a>",
          "description": "We define disentanglement as how far class-different data points from each\nother are, relative to the distances among class-similar data points. When\nmaximizing disentanglement during representation learning, we obtain a\ntransformed feature representation where the class memberships of the data\npoints are preserved. If the class memberships of the data points are\npreserved, we would have a feature representation space in which a nearest\nneighbour classifier or a clustering algorithm would perform well. We take\nadvantage of this method to learn better natural language representation, and\nemploy it on text classification and text clustering tasks. Through\ndisentanglement, we obtain text representations with better-defined clusters\nand improve text classification performance. Our approach had a test\nclassification accuracy of as high as 90.11% and test clustering accuracy of\n88% on the AG News dataset, outperforming our baseline models -- without any\nother training tricks or regularization.",
          "link": "http://arxiv.org/abs/2107.14597",
          "publishedOn": "2021-08-02T01:58:24.769Z",
          "wordCount": 584,
          "title": "Text Classification and Clustering with Annealing Soft Nearest Neighbor Loss. (arXiv:2107.14597v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_S/0/1/0/all/0/1\">Sakshi Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1\">Vinay Kumar Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1\">Srijith P K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1\">Piyush Rai</a>",
          "description": "We present a continual learning approach for generative adversarial networks\n(GANs), by designing and leveraging parameter-efficient feature map\ntransformations. Our approach is based on learning a set of global and\ntask-specific parameters. The global parameters are fixed across tasks whereas\nthe task-specific parameters act as local adapters for each task, and help in\nefficiently obtaining task-specific feature maps. Moreover, we propose an\nelement-wise addition of residual bias in the transformed feature space, which\nfurther helps stabilize GAN training in such settings. Our approach also\nleverages task similarity information based on the Fisher information matrix.\nLeveraging this knowledge from previous tasks significantly improves the model\nperformance. In addition, the similarity measure also helps reduce the\nparameter growth in continual adaptation and helps to learn a compact model. In\ncontrast to the recent approaches for continually-learned GANs, the proposed\napproach provides a memory-efficient way to perform effective continual data\ngeneration. Through extensive experiments on challenging and diverse datasets,\nwe show that the feature-map-transformation approach outperforms\nstate-of-the-art methods for continually-learned GANs, with substantially fewer\nparameters. The proposed method generates high-quality samples that can also\nimprove the generative-replay-based continual learning for discriminative\ntasks.",
          "link": "http://arxiv.org/abs/2103.04032",
          "publishedOn": "2021-08-02T01:58:24.710Z",
          "wordCount": 667,
          "title": "CAM-GAN: Continual Adaptation Modules for Generative Adversarial Networks. (arXiv:2103.04032v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pujol_Perich_D/0/1/0/all/0/1\">David Pujol-Perich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Varela_J/0/1/0/all/0/1\">Jos&#xe9; Su&#xe1;rez-Varela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabellos_Aparicio_A/0/1/0/all/0/1\">Albert Cabellos-Aparicio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barlet_Ros_P/0/1/0/all/0/1\">Pere Barlet-Ros</a>",
          "description": "The last few years have seen an increasing wave of attacks with serious\neconomic and privacy damages, which evinces the need for accurate Network\nIntrusion Detection Systems (NIDS). Recent works propose the use of Machine\nLearning (ML) techniques for building such systems (e.g., decision trees,\nneural networks). However, existing ML-based NIDS are barely robust to common\nadversarial attacks, which limits their applicability to real networks. A\nfundamental problem of these solutions is that they treat and classify flows\nindependently. In contrast, in this paper we argue the importance of focusing\non the structural patterns of attacks, by capturing not only the individual\nflow features, but also the relations between different flows (e.g., the\nsource/destination hosts they share). To this end, we use a graph\nrepresentation that keeps flow records and their relationships, and propose a\nnovel Graph Neural Network (GNN) model tailored to process and learn from such\ngraph-structured information. In our evaluation, we first show that the\nproposed GNN model achieves state-of-the-art results in the well-known\nCIC-IDS2017 dataset. Moreover, we assess the robustness of our solution under\ntwo common adversarial attacks, that intentionally modify the packet size and\ninter-arrival times to avoid detection. The results show that our model is able\nto maintain the same level of accuracy as in previous experiments, while\nstate-of-the-art ML techniques degrade up to 50% their accuracy (F1-score)\nunder these attacks. This unprecedented level of robustness is mainly induced\nby the capability of our GNN model to learn flow patterns of attacks structured\nas graphs.",
          "link": "http://arxiv.org/abs/2107.14756",
          "publishedOn": "2021-08-02T01:58:24.703Z",
          "wordCount": 710,
          "title": "Unveiling the potential of Graph Neural Networks for robust Intrusion Detection. (arXiv:2107.14756v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14698",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Loftin_R/0/1/0/all/0/1\">Robert Loftin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Aadirupa Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1\">Sam Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>",
          "description": "High sample complexity remains a barrier to the application of reinforcement\nlearning (RL), particularly in multi-agent systems. A large body of work has\ndemonstrated that exploration mechanisms based on the principle of optimism\nunder uncertainty can significantly improve the sample efficiency of RL in\nsingle agent tasks. This work seeks to understand the role of optimistic\nexploration in non-cooperative multi-agent settings. We will show that, in\nzero-sum games, optimistic exploration can cause the learner to waste time\nsampling parts of the state space that are irrelevant to strategic play, as\nthey can only be reached through cooperation between both players. To address\nthis issue, we introduce a formal notion of strategically efficient exploration\nin Markov games, and use this to develop two strategically efficient learning\nalgorithms for finite Markov games. We demonstrate that these methods can be\nsignificantly more sample efficient than their optimistic counterparts.",
          "link": "http://arxiv.org/abs/2107.14698",
          "publishedOn": "2021-08-02T01:58:24.697Z",
          "wordCount": 603,
          "title": "Strategically Efficient Exploration in Competitive Multi-agent Reinforcement Learning. (arXiv:2107.14698v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Budd_S/0/1/0/all/0/1\">Samuel Budd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Day_T/0/1/0/all/0/1\">Thomas Day</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simpson_J/0/1/0/all/0/1\">John Simpson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lloyd_K/0/1/0/all/0/1\">Karen Lloyd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthew_J/0/1/0/all/0/1\">Jacqueline Matthew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skelton_E/0/1/0/all/0/1\">Emily Skelton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_R/0/1/0/all/0/1\">Reza Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>",
          "description": "Probably yes. -- Supervised Deep Learning dominates performance scores for\nmany computer vision tasks and defines the state-of-the-art. However, medical\nimage analysis lags behind natural image applications. One of the many reasons\nis the lack of well annotated medical image data available to researchers. One\nof the first things researchers are told is that we require significant\nexpertise to reliably and accurately interpret and label such data. We see\nsignificant inter- and intra-observer variability between expert annotations of\nmedical images. Still, it is a widely held assumption that novice annotators\nare unable to provide useful annotations for use by clinical Deep Learning\nmodels. In this work we challenge this assumption and examine the implications\nof using a minimally trained novice labelling workforce to acquire annotations\nfor a complex medical image dataset. We study the time and cost implications of\nusing novice annotators, the raw performance of novice annotators compared to\ngold-standard expert annotators, and the downstream effects on a trained Deep\nLearning segmentation model's performance for detecting a specific congenital\nheart disease (hypoplastic left heart syndrome) in fetal ultrasound imaging.",
          "link": "http://arxiv.org/abs/2107.14682",
          "publishedOn": "2021-08-02T01:58:24.691Z",
          "wordCount": 647,
          "title": "Can non-specialists provide high quality gold standard labels in challenging modalities?. (arXiv:2107.14682v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bengar_J/0/1/0/all/0/1\">Javad Zolfaghari Bengar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raducanu_B/0/1/0/all/0/1\">Bogdan Raducanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>",
          "description": "Active learning aims to select samples to be annotated that yield the largest\nperformance improvement for the learning algorithm. Many methods approach this\nproblem by measuring the informativeness of samples and do this based on the\ncertainty of the network predictions for samples. However, it is well-known\nthat neural networks are overly confident about their prediction and are\ntherefore an untrustworthy source to assess sample informativeness. In this\npaper, we propose a new informativeness-based active learning method. Our\nmeasure is derived from the learning dynamics of a neural network. More\nprecisely we track the label assignment of the unlabeled data pool during the\ntraining of the algorithm. We capture the learning dynamics with a metric\ncalled label-dispersion, which is low when the network consistently assigns the\nsame label to the sample during the training of the network and high when the\nassigned label changes frequently. We show that label-dispersion is a promising\npredictor of the uncertainty of the network, and show on two benchmark datasets\nthat an active learning algorithm based on label-dispersion obtains excellent\nresults.",
          "link": "http://arxiv.org/abs/2107.14707",
          "publishedOn": "2021-08-02T01:58:24.676Z",
          "wordCount": 635,
          "title": "When Deep Learners Change Their Mind: Learning Dynamics for Active Learning. (arXiv:2107.14707v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1\">Guangfeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shao-Lun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1\">Tian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linqi Song</a>",
          "description": "Gradient quantization is an emerging technique in reducing communication\ncosts in distributed learning. Existing gradient quantization algorithms often\nrely on engineering heuristics or empirical observations, lacking a systematic\napproach to dynamically quantize gradients. This paper addresses this issue by\nproposing a novel dynamically quantized SGD (DQ-SGD) framework, enabling us to\ndynamically adjust the quantization scheme for each gradient descent step by\nexploring the trade-off between communication cost and convergence error. We\nderive an upper bound, tight in some cases, of the convergence error for a\nrestricted family of quantization schemes and loss functions. We design our\nDQ-SGD algorithm via minimizing the communication cost under the convergence\nerror constraints. Finally, through extensive experiments on large-scale\nnatural language processing and computer vision tasks on AG-News, CIFAR-10, and\nCIFAR-100 datasets, we demonstrate that our quantization scheme achieves better\ntradeoffs between the communication cost and learning performance than other\nstate-of-the-art gradient quantization methods.",
          "link": "http://arxiv.org/abs/2107.14575",
          "publishedOn": "2021-08-02T01:58:24.670Z",
          "wordCount": 590,
          "title": "DQ-SGD: Dynamic Quantization in SGD for Communication-Efficient Distributed Learning. (arXiv:2107.14575v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarmento_P/0/1/0/all/0/1\">Pedro Sarmento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Adarsh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carr_C/0/1/0/all/0/1\">CJ Carr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zukowski_Z/0/1/0/all/0/1\">Zack Zukowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barthet_M/0/1/0/all/0/1\">Mathieu Barthet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Hsuan Yang</a>",
          "description": "Originating in the Renaissance and burgeoning in the digital era, tablatures\nare a commonly used music notation system which provides explicit\nrepresentations of instrument fingerings rather than pitches. GuitarPro has\nestablished itself as a widely used tablature format and software enabling\nmusicians to edit and share songs for musical practice, learning, and\ncomposition. In this work, we present DadaGP, a new symbolic music dataset\ncomprising 26,181 song scores in the GuitarPro format covering 739 musical\ngenres, along with an accompanying tokenized format well-suited for generative\nsequence models such as the Transformer. The tokenized format is inspired by\nevent-based MIDI encodings, often used in symbolic music generation models. The\ndataset is released with an encoder/decoder which converts GuitarPro files to\ntokens and back. We present results of a use case in which DadaGP is used to\ntrain a Transformer-based model to generate new songs in GuitarPro format. We\ndiscuss other relevant use cases for the dataset (guitar-bass transcription,\nmusic style transfer and artist/genre classification) as well as ethical\nimplications. DadaGP opens up the possibility to train GuitarPro score\ngenerators, fine-tune models on custom data, create new styles of music,\nAI-powered songwriting apps, and human-AI improvisation.",
          "link": "http://arxiv.org/abs/2107.14653",
          "publishedOn": "2021-08-02T01:58:24.663Z",
          "wordCount": 640,
          "title": "DadaGP: A Dataset of Tokenized GuitarPro Songs for Sequence Models. (arXiv:2107.14653v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14551",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Munasinghe_T/0/1/0/all/0/1\">Thilanka Munasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasindu_H/0/1/0/all/0/1\">HR Pasindu</a>",
          "description": "We propose how a developing country like Sri Lanka can benefit from\nprivacy-enabled machine learning techniques such as Federated Learning to\ndetect road conditions using crowd-sourced data collection and proposed the\nidea of implementing a Digital Twin for the national road system in Sri Lanka.\nDeveloping countries such as Sri Lanka are far behind in implementing smart\nroad systems and smart cities compared to the developed countries. The proposed\nwork discussed in this paper matches the UN Sustainable Development Goal (SDG)\n9: \"Build Resilient Infrastructure, Promote Inclusive and Sustainable\nIndustrialization and Foster Innovation\". Our proposed work discusses how the\ngovernment and private sector vehicles that conduct routine trips to collect\ncrowd-sourced data using smartphone devices to identify the road conditions and\ndetect where the potholes, surface unevenness (roughness), and other major\ndistresses are located on the roads. We explore Mobile Edge Computing (MEC)\ntechniques that can bring machine learning intelligence closer to the edge\ndevices where produced data is stored and show how the applications of\nFederated Learning can be made to detect and improve road conditions. During\nthe second phase of this study, we plan to implement a Digital Twin for the\nroad system in Sri Lanka. We intend to use data provided by both Dedicated and\nNon-Dedicated systems in the proposed Digital Twin for the road system. As of\nwriting this paper, and best to our knowledge, there is no Digital Twin system\nimplemented for roads and other infrastructure systems in Sri Lanka. The\nproposed Digital Twin will be one of the first implementations of such systems\nin Sri Lanka. Lessons learned from this pilot project will benefit other\ndeveloping countries who wish to follow the same path and make data-driven\ndecisions.",
          "link": "http://arxiv.org/abs/2107.14551",
          "publishedOn": "2021-08-02T01:58:24.654Z",
          "wordCount": 790,
          "title": "Sensing and Mapping for Better Roads: Initial Plan for Using Federated Learning and Implementing a Digital Twin to Identify the Road Conditions in a Developing Country -- Sri Lanka. (arXiv:2107.14551v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uglov_A/0/1/0/all/0/1\">Arsenii Uglov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaev_S/0/1/0/all/0/1\">Sergei Nikolaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belov_S/0/1/0/all/0/1\">Sergei Belov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padalitsa_D/0/1/0/all/0/1\">Daniil Padalitsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenkina_T/0/1/0/all/0/1\">Tatiana Greenkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biagio_M/0/1/0/all/0/1\">Marco San Biagio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cacciatori_F/0/1/0/all/0/1\">Fabio Cacciatori</a>",
          "description": "Injection molding is one of the most popular manufacturing methods for the\nmodeling of complex plastic objects. Faster numerical simulation of the\ntechnological process would allow for faster and cheaper design cycles of new\nproducts. In this work, we propose a baseline for a data processing pipeline\nthat includes the extraction of data from Moldflow simulation projects and the\nprediction of the fill time and deflection distributions over 3-dimensional\nsurfaces using machine learning models. We propose algorithms for engineering\nof features, including information of injector gates parameters that will\nmostly affect the time for plastic to reach the particular point of the form\nfor fill time prediction, and geometrical features for deflection prediction.\nWe propose and evaluate baseline machine learning models for fill time and\ndeflection distribution prediction and provide baseline values of MSE and RMSE\nmetrics. Finally, we measure the execution time of our solution and show that\nit significantly exceeds the time of simulation with Moldflow software:\napproximately 17 times and 14 times faster for mean and median total times\nrespectively, comparing the times of all analysis stages for deflection\nprediction. Our solution has been implemented in a prototype web application\nthat was approved by the management board of Fiat Chrysler Automobiles and\nIllogic SRL. As one of the promising applications of this surrogate modelling\napproach, we envision the use of trained models as a fast objective function in\nthe task of optimization of technological parameters of the injection molding\nprocess (meaning optimal placement of gates), which could significantly aid\nengineers in this task, or even automate it.",
          "link": "http://arxiv.org/abs/2107.14574",
          "publishedOn": "2021-08-02T01:58:24.647Z",
          "wordCount": 706,
          "title": "Surrogate Modelling for Injection Molding Processes using Machine Learning. (arXiv:2107.14574v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koffas_S/0/1/0/all/0/1\">Stefanos Koffas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Mauro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picek_S/0/1/0/all/0/1\">Stjepan Picek</a>",
          "description": "Deep neural networks represent a powerful option for many real-world\napplications due to their ability to model even complex data relations.\nHowever, such neural networks can also be prohibitively expensive to train,\nmaking it common to either outsource the training process to third parties or\nuse pretrained neural networks. Unfortunately, such practices make neural\nnetworks vulnerable to various attacks, where one attack is the backdoor\nattack. In such an attack, the third party training the model may maliciously\ninject hidden behaviors into the model. Still, if a particular input (called\ntrigger) is fed into a neural network, the network will respond with a wrong\nresult.\n\nIn this work, we explore the option of backdoor attacks to automatic speech\nrecognition systems where we inject inaudible triggers. By doing so, we make\nthe backdoor attack challenging to detect for legitimate users, and thus,\npotentially more dangerous. We conduct experiments on two versions of datasets\nand three neural networks and explore the performance of our attack concerning\nthe duration, position, and type of the trigger. Our results indicate that less\nthan 1% of poisoned data is sufficient to deploy a backdoor attack and reach a\n100% attack success rate. What is more, while the trigger is inaudible, making\nit without limitations with respect to the duration of the signal, we observed\nthat even short, non-continuous triggers result in highly successful attacks.",
          "link": "http://arxiv.org/abs/2107.14569",
          "publishedOn": "2021-08-02T01:58:24.640Z",
          "wordCount": 665,
          "title": "Can You Hear It? Backdoor Attacks via Ultrasonic Triggers. (arXiv:2107.14569v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14586",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dupuy_C/0/1/0/all/0/1\">Christophe Dupuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arava_R/0/1/0/all/0/1\">Radhika Arava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>",
          "description": "Recent advances in deep learning have drastically improved performance on\nmany Natural Language Understanding (NLU) tasks. However, the data used to\ntrain NLU models may contain private information such as addresses or phone\nnumbers, particularly when drawn from human subjects. It is desirable that\nunderlying models do not expose private information contained in the training\ndata. Differentially Private Stochastic Gradient Descent (DP-SGD) has been\nproposed as a mechanism to build privacy-preserving models. However, DP-SGD can\nbe prohibitively slow to train. In this work, we propose a more efficient\nDP-SGD for training using a GPU infrastructure and apply it to fine-tuning\nmodels based on LSTM and transformer architectures. We report faster training\ntimes, alongside accuracy, theoretical privacy guarantees and success of\nMembership inference attacks for our models and observe that fine-tuning with\nproposed variant of DP-SGD can yield competitive models without significant\ndegradation in training time and improvement in privacy protection. We also\nmake observations such as looser theoretical $\\epsilon, \\delta$ can translate\ninto significant practical privacy gains.",
          "link": "http://arxiv.org/abs/2107.14586",
          "publishedOn": "2021-08-02T01:58:24.623Z",
          "wordCount": 611,
          "title": "An Efficient DP-SGD Mechanism for Large Scale NLP Models. (arXiv:2107.14586v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karn_A/0/1/0/all/0/1\">Aryan Karn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1\">Ashutosh Acharya</a>",
          "description": "We present a study of the manners by which Domain information has been\nincorporated when building models with Neural Networks. Integrating space data\nis uniquely important to the development of Knowledge understanding model, as\nwell as other fields that aid in understanding information by utilizing the\nhuman-machine interface and Reinforcement Learning. On numerous such occasions,\nmachine-based model development may profit essentially from the human\ninformation on the world encoded in an adequately exact structure. This paper\ninspects expansive ways to affect encode such information as sensible and\nmathematical limitations and portrays methods and results that came to a couple\nof subcategories under all of those methodologies.",
          "link": "http://arxiv.org/abs/2107.14613",
          "publishedOn": "2021-08-02T01:58:24.616Z",
          "wordCount": 549,
          "title": "Incorporation of Deep Neural Network & Reinforcement Learning with Domain Knowledge. (arXiv:2107.14613v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">GuoLiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyang Li</a>",
          "description": "Although attention-based Neural Machine Translation has achieved remarkable\nprogress in recent layers, it still suffers from issue of making insufficient\nuse of the output of each layer. In transformer, it only uses the top layer of\nencoder and decoder in the subsequent process, which makes it impossible to\ntake advantage of the useful information in other layers. To address this\nissue, we propose a residual tree aggregation of layers for Transformer(RTAL),\nwhich helps to fuse information across layers. Specifically, we try to fuse the\ninformation across layers by constructing a post-order binary tree. In\nadditional to the last node, we add the residual connection to the process of\ngenerating child nodes. Our model is based on the Neural Machine Translation\nmodel Transformer and we conduct our experiments on WMT14 English-to-German and\nWMT17 English-to-France translation tasks. Experimental results across language\npairs show that the proposed approach outperforms the strong baseline model\nsignificantly",
          "link": "http://arxiv.org/abs/2107.14590",
          "publishedOn": "2021-08-02T01:58:24.609Z",
          "wordCount": 584,
          "title": "Residual Tree Aggregation of Layers for Neural Machine Translation. (arXiv:2107.14590v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Attias_I/0/1/0/all/0/1\">Idan Attias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_E/0/1/0/all/0/1\">Edith Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechner_M/0/1/0/all/0/1\">Moshe Shechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stemmer_U/0/1/0/all/0/1\">Uri Stemmer</a>",
          "description": "Streaming algorithms are algorithms for processing large data streams, using\nonly a limited amount of memory. Classical streaming algorithms operate under\nthe assumption that the input stream is fixed in advance. Recently, there is a\ngrowing interest in studying streaming algorithms that provide provable\nguarantees even when the input stream is chosen by an adaptive adversary. Such\nstreaming algorithms are said to be {\\em adversarially-robust}. We propose a\nnovel framework for adversarial streaming that hybrids two recently suggested\nframeworks by Hassidim et al. (2020) and by Woodruff and Zhou (2021). These\nrecently suggested frameworks rely on very different ideas, each with its own\nstrengths and weaknesses. We combine these two frameworks (in a non-trivial\nway) into a single hybrid framework that gains from both approaches to obtain\nsuperior performances for turnstile streams.",
          "link": "http://arxiv.org/abs/2107.14527",
          "publishedOn": "2021-08-02T01:58:24.593Z",
          "wordCount": 577,
          "title": "A Framework for Adversarial Streaming via Differential Privacy and Difference Estimators. (arXiv:2107.14527v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mukhtar_S/0/1/0/all/0/1\">Shakeeb A. M. Mukhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joglekar_P/0/1/0/all/0/1\">Pushkar S. Joglekar</a>",
          "description": "One of the major problems writers and poets face is the writer's block. It is\na condition in which an author loses the ability to produce new work or\nexperiences a creative slowdown. The problem is more difficult in the context\nof poetry than prose, as in the latter case authors need not be very concise\nwhile expressing their ideas, also the various aspects such as rhyme, poetic\nmeters are not relevant for prose. One of the most effective ways to overcome\nthis writing block for poets can be, to have a prompt system, which would help\ntheir imagination and open their minds for new ideas. A prompt system can\npossibly generate one liner, two liner or full ghazals. The purpose of this\nwork is to give an ode to the Urdu, Hindi poets, and helping them start their\nnext line of poetry, a couplet or a complete ghazal considering various factors\nlike rhymes, refrain, and meters. The result will help aspiring poets to get\nnew ideas and help them overcome writer's block by auto-generating pieces of\npoetry using Deep Learning techniques. A concern with creative works like this,\nespecially in the literary context, is to ensure that the output is not\nplagiarized. This work also addresses the concern and makes sure that the\nresulting odes are not exact match with input data using parameters like\ntemperature and manual plagiarism check against input corpus. To the best of\nour knowledge, although the automatic text generation problem has been studied\nquite extensively in the literature, the specific problem of Urdu, Hindi poetry\ngeneration has not been explored much. Apart from developing system to\nauto-generate Urdu, Hindi poetry, another key contribution of our work is to\ncreate a cleaned and preprocessed corpus of Urdu, Hindi poetry (derived from\nauthentic resources) and making it freely available for researchers in the\narea.",
          "link": "http://arxiv.org/abs/2107.14587",
          "publishedOn": "2021-08-02T01:58:24.587Z",
          "wordCount": 741,
          "title": "Urdu & Hindi Poetry Generation using Neural Networks. (arXiv:2107.14587v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Apostolova_E/0/1/0/all/0/1\">Emilia Apostolova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karim_F/0/1/0/all/0/1\">Fazle Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muscioni_G/0/1/0/all/0/1\">Guido Muscioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_A/0/1/0/all/0/1\">Anubhav Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clyman_J/0/1/0/all/0/1\">Jeffrey Clyman</a>",
          "description": "In this work, we modify and apply self-supervision techniques to the domain\nof medical health insurance claims. We model patients' healthcare claims\nhistory analogous to free-text narratives, and introduce pre-trained `prior\nknowledge', later utilized for patient outcome predictions on a challenging\ntask: predicting Covid-19 hospitalization, given a patient's pre-Covid-19\ninsurance claims history. Results suggest that pre-training on insurance claims\nnot only produces better prediction performance, but, more importantly,\nimproves the model's `clinical trustworthiness' and model\nstability/reliability.",
          "link": "http://arxiv.org/abs/2107.14591",
          "publishedOn": "2021-08-02T01:58:24.580Z",
          "wordCount": 569,
          "title": "Self-supervision for health insurance claims data: a Covid-19 use case. (arXiv:2107.14591v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1\">Moritz Knolle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1\">Dmitrii Usynin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1\">Alexander Ziller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus R. Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>",
          "description": "The application of differential privacy to the training of deep neural\nnetworks holds the promise of allowing large-scale (decentralized) use of\nsensitive data while providing rigorous privacy guarantees to the individual.\nThe predominant approach to differentially private training of neural networks\nis DP-SGD, which relies on norm-based gradient clipping as a method for\nbounding sensitivity, followed by the addition of appropriately calibrated\nGaussian noise. In this work we propose NeuralDP, a technique for privatising\nactivations of some layer within a neural network, which by the post-processing\nproperties of differential privacy yields a differentially private network. We\nexperimentally demonstrate on two datasets (MNIST and Pediatric Pneumonia\nDataset (PPD)) that our method offers substantially improved privacy-utility\ntrade-offs compared to DP-SGD.",
          "link": "http://arxiv.org/abs/2107.14582",
          "publishedOn": "2021-08-02T01:58:24.574Z",
          "wordCount": 555,
          "title": "NeuralDP Differentially private neural networks by design. (arXiv:2107.14582v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korzeniowski_F/0/1/0/all/0/1\">Filip Korzeniowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oramas_S/0/1/0/all/0/1\">Sergio Oramas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouyon_F/0/1/0/all/0/1\">Fabien Gouyon</a>",
          "description": "Artist similarity plays an important role in organizing, understanding, and\nsubsequently, facilitating discovery in large collections of music. In this\npaper, we present a hybrid approach to computing similarity between artists\nusing graph neural networks trained with triplet loss. The novelty of using a\ngraph neural network architecture is to combine the topology of a graph of\nartist connections with content features to embed artists into a vector space\nthat encodes similarity. To evaluate the proposed method, we compile the new\nOLGA dataset, which contains artist similarities from AllMusic, together with\ncontent features from AcousticBrainz. With 17,673 artists, this is the largest\nacademic artist similarity dataset that includes content-based features to\ndate. Moreover, we also showcase the scalability of our approach by\nexperimenting with a much larger proprietary dataset. Results show the\nsuperiority of the proposed approach over current state-of-the-art methods for\nmusic similarity. Finally, we hope that the OLGA dataset will facilitate\nresearch on data-driven models for artist similarity.",
          "link": "http://arxiv.org/abs/2107.14541",
          "publishedOn": "2021-08-02T01:58:24.568Z",
          "wordCount": 614,
          "title": "Artist Similarity with Graph Neural Networks. (arXiv:2107.14541v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kobler_R/0/1/0/all/0/1\">Reinmar J. Kobler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirayama_J/0/1/0/all/0/1\">Jun-Ichiro Hirayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopes_Dias_L/0/1/0/all/0/1\">Lea Hehenberger Catarina Lopes-Dias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Putz_G/0/1/0/all/0/1\">Gernot R. M&#xfc;ller-Putz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawanabe_M/0/1/0/all/0/1\">Motoaki Kawanabe</a>",
          "description": "Riemannian tangent space methods offer state-of-the-art performance in\nmagnetoencephalography (MEG) and electroencephalography (EEG) based\napplications such as brain-computer interfaces and biomarker development. One\nlimitation, particularly relevant for biomarker development, is limited model\ninterpretability compared to established component-based methods. Here, we\npropose a method to transform the parameters of linear tangent space models\ninto interpretable patterns. Using typical assumptions, we show that this\napproach identifies the true patterns of latent sources, encoding a target\nsignal. In simulations and two real MEG and EEG datasets, we demonstrate the\nvalidity of the proposed approach and investigate its behavior when the model\nassumptions are violated. Our results confirm that Riemannian tangent space\nmethods are robust to differences in the source patterns across observations.\nWe found that this robustness property also transfers to the associated\npatterns.",
          "link": "http://arxiv.org/abs/2107.14398",
          "publishedOn": "2021-08-02T01:58:24.561Z",
          "wordCount": 580,
          "title": "On the interpretation of linear Riemannian tangent space model parameters in M/EEG. (arXiv:2107.14398v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14280",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Taylor_M/0/1/0/all/0/1\">Michael G. Taylor</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Nandy_A/0/1/0/all/0/1\">Aditya Nandy</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Lu_C/0/1/0/all/0/1\">Connie C. Lu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kulik_H/0/1/0/all/0/1\">Heather J. Kulik</a>",
          "description": "The rational tailoring of transition metal complexes is necessary to address\noutstanding challenges in energy utilization and storage. Heterobimetallic\ntransition metal complexes that exhibit metal-metal bonding in stacked \"double\ndecker\" ligand structures are an emerging, attractive platform for catalysis,\nbut their properties are challenging to predict prior to laborious synthetic\nefforts. We demonstrate an alternative, data-driven approach to uncovering\nstructure-property relationships for rational bimetallic complex design. We\ntailor graph-based representations of the metal-local environment for these\nheterobimetallic complexes for use in training of multiple linear regression\nand kernel ridge regression (KRR) models. Focusing on oxidation potentials, we\nobtain a set of 28 experimentally characterized complexes to develop a multiple\nlinear regression model. On this training set, we achieve good accuracy (mean\nabsolute error, MAE, of 0.25 V) and preserve transferability to unseen\nexperimental data with a new ligand structure. We trained a KRR model on a\nsubset of 330 structurally characterized heterobimetallics to predict the\ndegree of metal-metal bonding. This KRR model predicts relative metal-metal\nbond lengths in the test set to within 5%, and analysis of key features reveals\nthe fundamental atomic contributions (e.g., the valence electron configuration)\nthat most strongly influence the behavior of complexes. Our work provides\nguidance for rational bimetallic design, suggesting that properties including\nthe formal shortness ratio should be transferable from one period to another.",
          "link": "http://arxiv.org/abs/2107.14280",
          "publishedOn": "2021-08-02T01:58:24.551Z",
          "wordCount": 668,
          "title": "Deciphering Cryptic Behavior in Bimetallic Transition Metal Complexes with Machine Learning. (arXiv:2107.14280v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14465",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1\">Quoc Phong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhaoxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Low_B/0/1/0/all/0/1\">Bryan Kian Hsiang Low</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaillet_P/0/1/0/all/0/1\">Patrick Jaillet</a>",
          "description": "Information-based Bayesian optimization (BO) algorithms have achieved\nstate-of-the-art performance in optimizing a black-box objective function.\nHowever, they usually require several approximations or simplifying assumptions\n(without clearly understanding their effects on the BO performance) and/or\ntheir generalization to batch BO is computationally unwieldy, especially with\nan increasing batch size. To alleviate these issues, this paper presents a\nnovel trusted-maximizers entropy search (TES) acquisition function: It measures\nhow much an input query contributes to the information gain on the maximizer\nover a finite set of trusted maximizers, i.e., inputs optimizing functions that\nare sampled from the Gaussian process posterior belief of the objective\nfunction. Evaluating TES requires either only a stochastic approximation with\nsampling or a deterministic approximation with expectation propagation, both of\nwhich are investigated and empirically evaluated using synthetic benchmark\nobjective functions and real-world optimization problems, e.g., hyperparameter\ntuning of a convolutional neural network and synthesizing 'physically\nrealizable' faces to fool a black-box face recognition system. Though TES can\nnaturally be generalized to a batch variant with either approximation, the\nlatter is amenable to be scaled to a much larger batch size in our experiments.",
          "link": "http://arxiv.org/abs/2107.14465",
          "publishedOn": "2021-08-02T01:58:24.545Z",
          "wordCount": 634,
          "title": "Trusted-Maximizers Entropy Search for Efficient Bayesian Optimization. (arXiv:2107.14465v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akman_A/0/1/0/all/0/1\">Alican Akman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coppock_H/0/1/0/all/0/1\">Harry Coppock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaskell_A/0/1/0/all/0/1\">Alexander Gaskell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzirakis_P/0/1/0/all/0/1\">Panagiotis Tzirakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_L/0/1/0/all/0/1\">Lyn Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>",
          "description": "We report on cross-running the recent COVID-19 Identification ResNet (CIdeR)\non the two Interspeech 2021 COVID-19 diagnosis from cough and speech audio\nchallenges: ComParE and DiCOVA. CIdeR is an end-to-end deep learning neural\nnetwork originally designed to classify whether an individual is COVID-positive\nor COVID-negative based on coughing and breathing audio recordings from a\npublished crowdsourced dataset. In the current study, we demonstrate the\npotential of CIdeR at binary COVID-19 diagnosis from both the COVID-19 Cough\nand Speech Sub-Challenges of INTERSPEECH 2021, ComParE and DiCOVA. CIdeR\nachieves significant improvements over several baselines.",
          "link": "http://arxiv.org/abs/2107.14549",
          "publishedOn": "2021-08-02T01:58:24.525Z",
          "wordCount": 596,
          "title": "Evaluating the COVID-19 Identification ResNet (CIdeR) on the INTERSPEECH COVID-19 from Audio Challenges. (arXiv:2107.14549v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1\">Javier Naranjo-Alcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1\">Sergi Perez-Castanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1\">Pedro Zuccarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1\">Francesc J. Ferri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1\">Maximo Cobos</a>",
          "description": "Sound event localisation and detection (SELD) is a problem in the field of\nautomatic listening that aims at the temporal detection and localisation\n(direction of arrival estimation) of sound events within an audio clip, usually\nof long duration. Due to the amount of data present in the datasets related to\nthis problem, solutions based on deep learning have positioned themselves at\nthe top of the state of the art. Most solutions are based on 2D representations\nof the audio (different spectrograms) that are processed by a\nconvolutional-recurrent network. The motivation of this submission is to study\nthe squeeze-excitation technique in the convolutional part of the network and\nhow it improves the performance of the system. This study is based on the one\ncarried out by the same team last year. This year, it has been decided to study\nhow this technique improves each of the datasets (last year only the MIC\ndataset was studied). This modification shows an improvement in the performance\nof the system compared to the baseline using MIC dataset.",
          "link": "http://arxiv.org/abs/2107.14561",
          "publishedOn": "2021-08-02T01:58:24.506Z",
          "wordCount": 629,
          "title": "TASK3 DCASE2021 Challenge: Sound event localization and detection using squeeze-excitation residual CNNs. (arXiv:2107.14561v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+ONeill_L/0/1/0/all/0/1\">Lachlan O&#x27;Neill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angus_S/0/1/0/all/0/1\">Simon Angus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgohain_S/0/1/0/all/0/1\">Satya Borgohain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chmait_N/0/1/0/all/0/1\">Nader Chmait</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dowe_D/0/1/0/all/0/1\">David L. Dowe</a>",
          "description": "As the discipline has evolved, research in machine learning has been focused\nmore and more on creating more powerful neural networks, without regard for the\ninterpretability of these networks. Such \"black-box models\" yield\nstate-of-the-art results, but we cannot understand why they make a particular\ndecision or prediction. Sometimes this is acceptable, but often it is not.\n\nWe propose a novel architecture, Regression Networks, which combines the\npower of neural networks with the understandability of regression analysis.\nWhile some methods for combining these exist in the literature, our\narchitecture generalizes these approaches by taking interactions into account,\noffering the power of a dense neural network without forsaking\ninterpretability. We demonstrate that the models exceed the state-of-the-art\nperformance of interpretable models on several benchmark datasets, matching the\npower of a dense neural network. Finally, we discuss how these techniques can\nbe generalized to other neural architectures, such as convolutional and\nrecurrent neural networks.",
          "link": "http://arxiv.org/abs/2107.14417",
          "publishedOn": "2021-08-02T01:58:24.479Z",
          "wordCount": 586,
          "title": "Creating Powerful and Interpretable Models withRegression Networks. (arXiv:2107.14417v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomes_G/0/1/0/all/0/1\">Gecynalda Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "Neural Networks have been applied for time series prediction with good\nexperimental results that indicate the high capacity to approximate functions\nwith good precision. Most neural models used in these applications use\nactivation functions with fixed parameters. However, it is known that the\nchoice of activation function strongly influences the complexity and\nperformance of the neural network and that a limited number of activation\nfunctions have been used. In this work, we propose the use of a family of free\nparameter asymmetric activation functions for neural networks and show that\nthis family of defined activation functions satisfies the requirements of the\nuniversal approximation theorem. A methodology for the global optimization of\nthis family of activation functions with free parameter and the weights of the\nconnections between the processing units of the neural network is used. The\ncentral idea of the proposed methodology is to simultaneously optimize the\nweights and the activation function used in a multilayer perceptron network\n(MLP), through an approach that combines the advantages of simulated annealing,\ntabu search and a local learning algorithm, with the purpose of improving\nperformance in the adjustment and forecasting of time series. We chose two\nlearning algorithms: backpropagation with the term momentum (BPM) and\nLevenbergMarquardt (LM).",
          "link": "http://arxiv.org/abs/2107.14370",
          "publishedOn": "2021-08-02T01:58:24.469Z",
          "wordCount": 654,
          "title": "Otimizacao de pesos e funcoes de ativacao de redes neurais aplicadas na previsao de series temporais. (arXiv:2107.14370v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14412",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bajcsy_A/0/1/0/all/0/1\">Andrea Bajcsy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leung_K/0/1/0/all/0/1\">Karen Leung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmerling_E/0/1/0/all/0/1\">Edward Schmerling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>",
          "description": "As safety-critical autonomous vehicles (AVs) will soon become pervasive in\nour society, a number of safety concepts for trusted AV deployment have been\nrecently proposed throughout industry and academia. Yet, agreeing upon an\n\"appropriate\" safety concept is still an elusive task. In this paper, we\nadvocate for the use of Hamilton Jacobi (HJ) reachability as a unifying\nmathematical framework for comparing existing safety concepts, and propose ways\nto expand its modeling premises in a data-driven fashion. Specifically, we show\nthat (i) existing predominant safety concepts can be embedded in the HJ\nreachability framework, thereby enabling a common language for comparing and\ncontrasting modeling assumptions, and (ii) HJ reachability can serve as an\ninductive bias to effectively reason, in a data-driven context, about two\ncritical, yet often overlooked aspects of safety: responsibility and\ncontext-dependency.",
          "link": "http://arxiv.org/abs/2107.14412",
          "publishedOn": "2021-08-02T01:58:24.416Z",
          "wordCount": 578,
          "title": "Towards the Unification and Data-Driven Synthesis of Autonomous Vehicle Safety Concepts. (arXiv:2107.14412v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Safarani_S/0/1/0/all/0/1\">Shahd Safarani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nix_A/0/1/0/all/0/1\">Arne Nix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willeke_K/0/1/0/all/0/1\">Konstantin Willeke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadena_S/0/1/0/all/0/1\">Santiago A. Cadena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Restivo_K/0/1/0/all/0/1\">Kelli Restivo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denfield_G/0/1/0/all/0/1\">George Denfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_A/0/1/0/all/0/1\">Andreas S. Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinz_F/0/1/0/all/0/1\">Fabian H. Sinz</a>",
          "description": "Deep neural networks set the state-of-the-art across many tasks in computer\nvision, but their generalization ability to image distortions is surprisingly\nfragile. In contrast, the mammalian visual system is robust to a wide range of\nperturbations. Recent work suggests that this generalization ability can be\nexplained by useful inductive biases encoded in the representations of visual\nstimuli throughout the visual cortex. Here, we successfully leveraged these\ninductive biases with a multi-task learning approach: we jointly trained a deep\nnetwork to perform image classification and to predict neural activity in\nmacaque primary visual cortex (V1). We measured the out-of-distribution\ngeneralization abilities of our network by testing its robustness to image\ndistortions. We found that co-training on monkey V1 data leads to increased\nrobustness despite the absence of those distortions during training.\nAdditionally, we showed that our network's robustness is very close to that of\nan Oracle network where parts of the architecture are directly trained on noisy\nimages. Our results also demonstrated that the network's representations become\nmore brain-like as their robustness improves. Using a novel constrained\nreconstruction analysis, we investigated what makes our brain-regularized\nnetwork more robust. We found that our co-trained network is more sensitive to\ncontent than noise when compared to a Baseline network that we trained for\nimage classification alone. Using DeepGaze-predicted saliency maps for ImageNet\nimages, we found that our monkey co-trained network tends to be more sensitive\nto salient regions in a scene, reminiscent of existing theories on the role of\nV1 in the detection of object borders and bottom-up saliency. Overall, our work\nexpands the promising research avenue of transferring inductive biases from the\nbrain, and provides a novel analysis of the effects of our transfer.",
          "link": "http://arxiv.org/abs/2107.14344",
          "publishedOn": "2021-08-02T01:58:24.397Z",
          "wordCount": 745,
          "title": "Towards robust vision by multi-task learning on monkey visual cortex. (arXiv:2107.14344v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tongzhou Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1\">Fanbo Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Derek Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Stone Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhiwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>",
          "description": "Learning generalizable manipulation skills is central for robots to achieve\ntask automation in environments with endless scene and object variations.\nHowever, existing robot learning environments are limited in both scale and\ndiversity of 3D assets (especially of articulated objects), making it difficult\nto train and evaluate the generalization ability of agents over novel objects.\nIn this work, we focus on object-level generalization and propose SAPIEN\nManipulation Skill Benchmark (abbreviated as ManiSkill), a large-scale\nlearning-from-demonstrations benchmark for articulated object manipulation with\nvisual input (point cloud and image). ManiSkill supports object-level\nvariations by utilizing a rich and diverse set of articulated objects, and each\ntask is carefully designed for learning manipulations on a single category of\nobjects. We equip ManiSkill with high-quality demonstrations to facilitate\nlearning-from-demonstrations approaches and perform evaluations on common\nbaseline algorithms. We believe ManiSkill can encourage the robot learning\ncommunity to explore more on learning generalizable object manipulation skills.",
          "link": "http://arxiv.org/abs/2107.14483",
          "publishedOn": "2021-08-02T01:58:24.381Z",
          "wordCount": 606,
          "title": "ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills. (arXiv:2107.14483v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14235",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Iaquinta_A/0/1/0/all/0/1\">Amanda Ferrari Iaquinta</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Silva_A/0/1/0/all/0/1\">Ana Carolina de Sousa Silva</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Junior_A/0/1/0/all/0/1\">Aldrumont Ferraz J&#xfa;nior</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Toledo_J/0/1/0/all/0/1\">Jessica Monique de Toledo</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Atzingen_G/0/1/0/all/0/1\">Gustavo Voltani von Atzingen</a>",
          "description": "The electrical signal emitted by the eyes movement produces a very strong\nartifact on EEG signaldue to its close proximity to the sensors and abundance\nof occurrence. In the context of detectingeye blink artifacts in EEG waveforms\nfor further removal and signal purification, multiple strategieswhere proposed\nin the literature. Most commonly applied methods require the use of a large\nnumberof electrodes, complex equipment for sampling and processing data. The\ngoal of this work is to createa reliable and user independent algorithm for\ndetecting and removing eye blink in EEG signals usingCNN (convolutional neural\nnetwork). For training and validation, three sets of public EEG data wereused.\nAll three sets contain samples obtained while the recruited subjects performed\nassigned tasksthat included blink voluntarily in specific moments, watch a\nvideo and read an article. The modelused in this study was able to have an\nembracing understanding of all the features that distinguish atrivial EEG\nsignal from a signal contaminated with eye blink artifacts without being\noverfitted byspecific features that only occurred in the situations when the\nsignals were registered.",
          "link": "http://arxiv.org/abs/2107.14235",
          "publishedOn": "2021-08-02T01:58:24.374Z",
          "wordCount": 628,
          "title": "EEG multipurpose eye blink detector using convolutional neural network. (arXiv:2107.14235v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yun Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongchao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1\">Suo Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1\">Chunyang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Huanjun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1\">Lihong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjie Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yixiang Mu</a>",
          "description": "We develop a novel framework that adds the regularizers of the sparse group\nlasso to a family of adaptive optimizers in deep learning, such as Momentum,\nAdagrad, Adam, AMSGrad, AdaHessian, and create a new class of optimizers, which\nare named Group Momentum, Group Adagrad, Group Adam, Group AMSGrad and Group\nAdaHessian, etc., accordingly. We establish theoretically proven convergence\nguarantees in the stochastic convex settings, based on primal-dual methods. We\nevaluate the regularized effect of our new optimizers on three large-scale\nreal-world ad click datasets with state-of-the-art deep learning models. The\nexperimental results reveal that compared with the original optimizers with the\npost-processing procedure which uses the magnitude pruning method, the\nperformance of the models can be significantly improved on the same sparsity\nlevel. Furthermore, in comparison to the cases without magnitude pruning, our\nmethods can achieve extremely high sparsity with significantly better or highly\ncompetitive performance.",
          "link": "http://arxiv.org/abs/2107.14432",
          "publishedOn": "2021-08-02T01:58:24.368Z",
          "wordCount": 599,
          "title": "Adaptive Optimizers with Sparse Group Lasso for Neural Networks in CTR Prediction. (arXiv:2107.14432v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1\">Tianxiang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuchen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>",
          "description": "The existence of redundancy in Convolutional Neural Networks (CNNs) enables\nus to remove some filters/channels with acceptable performance drops. However,\nthe training objective of CNNs usually tends to minimize an accuracy-related\nloss function without any attention paid to the redundancy, making the\nredundancy distribute randomly on all the filters, such that removing any of\nthem may trigger information loss and accuracy drop, necessitating a following\nfinetuning step for recovery. In this paper, we propose to manipulate the\nredundancy during training to facilitate network pruning. To this end, we\npropose a novel Centripetal SGD (C-SGD) to make some filters identical,\nresulting in ideal redundancy patterns, as such filters become purely redundant\ndue to their duplicates; hence removing them does not harm the network. As\nshown on CIFAR and ImageNet, C-SGD delivers better performance because the\nredundancy is better organized, compared to the existing methods. The\nefficiency also characterizes C-SGD because it is as fast as regular SGD,\nrequires no finetuning, and can be conducted simultaneously on all the layers\neven in very deep CNNs. Besides, C-SGD can improve the accuracy of CNNs by\nfirst training a model with the same architecture but wider layers then\nsqueezing it into the original width.",
          "link": "http://arxiv.org/abs/2107.14444",
          "publishedOn": "2021-08-02T01:58:24.348Z",
          "wordCount": 666,
          "title": "Manipulating Identical Filter Redundancy for Efficient Pruning on Deep and Complicated CNN. (arXiv:2107.14444v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14442",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1\">Marina Meil&#x103;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1\">Hanyu Zhang</a>",
          "description": "We address the problem of validating the ouput of clustering algorithms.\nGiven data $\\mathcal{D}$ and a partition $\\mathcal{C}$ of these data into $K$\nclusters, when can we say that the clusters obtained are correct or meaningful\nfor the data? This paper introduces a paradigm in which a clustering\n$\\mathcal{C}$ is considered meaningful if it is good with respect to a loss\nfunction such as the K-means distortion, and stable, i.e. the only good\nclustering up to small perturbations. Furthermore, we present a generic method\nto obtain post-inference guarantees of near-optimality and stability for a\nclustering $\\mathcal{C}$. The method can be instantiated for a variety of\nclustering criteria (also called loss functions) for which convex relaxations\nexist. Obtaining the guarantees amounts to solving a convex optimization\nproblem. We demonstrate the practical relevance of this method by obtaining\nguarantees for the K-means and the Normalized Cut clustering criteria on\nrealistic data sets. We also prove that asymptotic instability implies finite\nsample instability w.h.p., allowing inferences about the population\nclusterability from a sample. The guarantees do not depend on any\ndistributional assumptions, but they depend on the data set $\\mathcal{D}$\nadmitting a stable clustering.",
          "link": "http://arxiv.org/abs/2107.14442",
          "publishedOn": "2021-08-02T01:58:24.323Z",
          "wordCount": 617,
          "title": "Distribution free optimality intervals for clustering. (arXiv:2107.14442v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bothmann_L/0/1/0/all/0/1\">Ludwig Bothmann</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Strickroth_S/0/1/0/all/0/1\">Sven Strickroth</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Casalicchio_G/0/1/0/all/0/1\">Giuseppe Casalicchio</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1\">David R&#xfc;gamer</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1\">Marius Lindauer</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Scheipl_F/0/1/0/all/0/1\">Fabian Scheipl</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a> (1) ((1) Department of Statistics, Ludwig-Maximilians-Universit&#xe4;t M&#xfc;nchen, Germany, (2) Institute of Computer Science, Ludwig-Maximilians-Universit&#xe4;t M&#xfc;nchen, Germany, (3) Institute of Information Process, Leibniz University Hannover, Germany)",
          "description": "Education should not be a privilege but a common good. It should be openly\naccessible to everyone, with as few barriers as possible; even more so for key\ntechnologies such as Machine Learning (ML) and Data Science (DS). Open\nEducational Resources (OER) are a crucial factor for greater educational\nequity. In this paper, we describe the specific requirements for OER in ML and\nDS and argue that it is especially important for these fields to make source\nfiles publicly available, leading to Open Source Educational Resources (OSER).\nWe present our view on the collaborative development of OSER, the challenges\nthis poses, and first steps towards their solutions. We outline how OSER can be\nused for blended learning scenarios and share our experiences in university\neducation. Finally, we discuss additional challenges such as credit assignment\nor granting certificates.",
          "link": "http://arxiv.org/abs/2107.14330",
          "publishedOn": "2021-08-02T01:58:24.310Z",
          "wordCount": 619,
          "title": "Developing Open Source Educational Resources for Machine Learning and Data Science. (arXiv:2107.14330v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14261",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Convery_O/0/1/0/all/0/1\">Owen Convery</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Smith_L/0/1/0/all/0/1\">Lewis Smith</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hanuka_A/0/1/0/all/0/1\">Adi Hanuka</a>",
          "description": "Virtual Diagnostic (VD) is a deep learning tool that can be used to predict a\ndiagnostic output. VDs are especially useful in systems where measuring the\noutput is invasive, limited, costly or runs the risk of damaging the output.\nGiven a prediction, it is necessary to relay how reliable that prediction is.\nThis is known as 'uncertainty quantification' of a prediction. In this paper,\nwe use ensemble methods and quantile regression neural networks to explore\ndifferent ways of creating and analyzing prediction's uncertainty on\nexperimental data from the Linac Coherent Light Source at SLAC. We aim to\naccurately and confidently predict the current profile or longitudinal phase\nspace images of the electron beam. The ability to make informed decisions under\nuncertainty is crucial for reliable deployment of deep learning tools on\nsafety-critical systems as particle accelerators.",
          "link": "http://arxiv.org/abs/2107.14261",
          "publishedOn": "2021-08-02T01:58:24.285Z",
          "wordCount": 578,
          "title": "Quantifying Uncertainty for Machine Learning Based Diagnostic. (arXiv:2107.14261v1 [physics.acc-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14309",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Revay_M/0/1/0/all/0/1\">Max Revay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Umenberger_J/0/1/0/all/0/1\">Jack Umenberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manchester_I/0/1/0/all/0/1\">Ian R. Manchester</a>",
          "description": "This paper proposes methods for identification of large-scale networked\nsystems with guarantees that the resulting model will be contracting -- a\nstrong form of nonlinear stability -- and/or monotone, i.e. order relations\nbetween states are preserved. The main challenges that we address are:\nsimultaneously searching for model parameters and a certificate of stability,\nand scalability to networks with hundreds or thousands of nodes. We propose a\nmodel set that admits convex constraints for stability and monotonicity, and\nhas a separable structure that allows distributed identification via the\nalternating directions method of multipliers (ADMM). The performance and\nscalability of the approach is illustrated on a variety of linear and\nnon-linear case studies, including a nonlinear traffic network with a\n200-dimensional state space.",
          "link": "http://arxiv.org/abs/2107.14309",
          "publishedOn": "2021-08-02T01:58:24.279Z",
          "wordCount": 584,
          "title": "Distributed Identification of Contracting and/or Monotone Network Dynamics. (arXiv:2107.14309v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Casamitjana_A/0/1/0/all/0/1\">Adri&#xe0; Casamitjana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1\">Matteo Mancini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iglesias_J/0/1/0/all/0/1\">Juan Eugenio Iglesias</a>",
          "description": "Nonlinear inter-modality registration is often challenging due to the lack of\nobjective functions that are good proxies for alignment. Here we propose a\nsynthesis-by-registration method to convert this problem into an easier\nintra-modality task. We introduce a registration loss for weakly supervised\nimage translation between domains that does not require perfectly aligned\ntraining data. This loss capitalises on a registration U-Net with frozen\nweights, to drive a synthesis CNN towards the desired translation. We\ncomplement this loss with a structure preserving constraint based on\ncontrastive learning, which prevents blurring and content shifts due to\noverfitting. We apply this method to the registration of histological sections\nto MRI slices, a key step in 3D histology reconstruction. Results on two\ndifferent public datasets show improvements over registration based on mutual\ninformation (13% reduction in landmark error) and synthesis-based algorithms\nsuch as CycleGAN (11% reduction), and are comparable to a registration CNN with\nlabel supervision.",
          "link": "http://arxiv.org/abs/2107.14449",
          "publishedOn": "2021-08-02T01:58:24.267Z",
          "wordCount": 610,
          "title": "Synth-by-Reg (SbR): Contrastive learning for synthesis-based registration of paired images. (arXiv:2107.14449v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mathur_L/0/1/0/all/0/1\">Leena Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spitale_M/0/1/0/all/0/1\">Micol Spitale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_H/0/1/0/all/0/1\">Hao Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jieyun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mataric_M/0/1/0/all/0/1\">Maja J Matari&#x107;</a>",
          "description": "Virtual and robotic agents capable of perceiving human empathy have the\npotential to participate in engaging and meaningful human-machine interactions\nthat support human well-being. Prior research in computational empathy has\nfocused on designing empathic agents that use verbal and nonverbal behaviors to\nsimulate empathy and attempt to elicit empathic responses from humans. The\nchallenge of developing agents with the ability to automatically perceive\nelicited empathy in humans remains largely unexplored. Our paper presents the\nfirst approach to modeling user empathy elicited during interactions with a\nrobotic agent. We collected a new dataset from the novel interaction context of\nparticipants listening to a robot storyteller (46 participants, 6.9 hours of\nvideo). After each storytelling interaction, participants answered a\nquestionnaire that assessed their level of elicited empathy during the\ninteraction with the robot. We conducted experiments with 8 classical machine\nlearning models and 2 deep learning models (long short-term memory networks and\ntemporal convolutional networks) to detect empathy by leveraging patterns in\nparticipants' visual behaviors while they were listening to the robot\nstoryteller. Our highest-performing approach, based on XGBoost, achieved an\naccuracy of 69% and AUC of 72% when detecting empathy in videos. We contribute\ninsights regarding modeling approaches and visual features for automated\nempathy detection. Our research informs and motivates future development of\nempathy perception models that can be leveraged by virtual and robotic agents\nduring human-machine interactions.",
          "link": "http://arxiv.org/abs/2107.14345",
          "publishedOn": "2021-08-02T01:58:24.212Z",
          "wordCount": 685,
          "title": "Modeling User Empathy Elicited by a Robot Storyteller. (arXiv:2107.14345v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14457",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nadali_A/0/1/0/all/0/1\">Alireza Nadali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebadzadeh_M/0/1/0/all/0/1\">Mohammad Mehdi Ebadzadeh</a>",
          "description": "In recent years, there have been many deep structures for Reinforcement\nLearning, mainly for value function estimation and representations. These\nmethods achieved great success in Atari 2600 domain. In this paper, we propose\nan improved architecture based upon Dueling Networks, in this architecture,\nthere are two separate estimators, one approximate the state value function and\nthe other, state advantage function. This improvement based on Maximum Entropy,\nshows better policy evaluation compared to the original network and other\nvalue-based architectures in Atari domain.",
          "link": "http://arxiv.org/abs/2107.14457",
          "publishedOn": "2021-08-02T01:58:24.116Z",
          "wordCount": 506,
          "title": "Maximum Entropy Dueling Network Architecture. (arXiv:2107.14457v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14410",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Zhu_L/0/1/0/all/0/1\">Liao Zhu</a>",
          "description": "Modern evolvements of the technologies have been leading to a profound\ninfluence on the financial market. The introduction of constituents like\nExchange-Traded Funds, and the wide-use of advanced technologies such as\nalgorithmic trading, results in a boom of the data which provides more\nopportunities to reveal deeper insights. However, traditional statistical\nmethods always suffer from the high-dimensional, high-correlation, and\ntime-varying instinct of the financial data. In this dissertation, we focus on\ndeveloping techniques to stress these difficulties. With the proposed\nmethodologies, we can have more interpretable models, clearer explanations, and\nbetter predictions.",
          "link": "http://arxiv.org/abs/2107.14410",
          "publishedOn": "2021-08-02T01:58:24.110Z",
          "wordCount": 533,
          "title": "The Adaptive Multi-Factor Model and the Financial Market. (arXiv:2107.14410v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Razavi_M/0/1/0/all/0/1\">Moein Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janfaza_V/0/1/0/all/0/1\">Vahid Janfaza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamauchi_T/0/1/0/all/0/1\">Takashi Yamauchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leontyev_A/0/1/0/all/0/1\">Anton Leontyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Longmire_Monford_S/0/1/0/all/0/1\">Shanle Longmire-Monford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orr_J/0/1/0/all/0/1\">Joseph Orr</a>",
          "description": "Background: The human mind is multimodal. Yet most behavioral studies rely on\ncentury-old measures such as task accuracy and latency. To create a better\nunderstanding of human behavior and brain functionality, we should introduce\nother measures and analyze behavior from various aspects. However, it is\ntechnically complex and costly to design and implement the experiments that\nrecord multiple measures. To address this issue, a platform that allows\nsynchronizing multiple measures from human behavior is needed. Method: This\npaper introduces an opensource platform named OpenSync, which can be used to\nsynchronize multiple measures in neuroscience experiments. This platform helps\nto automatically integrate, synchronize and record physiological measures\n(e.g., electroencephalogram (EEG), galvanic skin response (GSR), eye-tracking,\nbody motion, etc.), user input response (e.g., from mouse, keyboard, joystick,\netc.), and task-related information (stimulus markers). In this paper, we\nexplain the structure and details of OpenSync, provide two case studies in\nPsychoPy and Unity. Comparison with existing tools: Unlike proprietary systems\n(e.g., iMotions), OpenSync is free and it can be used inside any opensource\nexperiment design software (e.g., PsychoPy, OpenSesame, Unity, etc.,\nhttps://pypi.org/project/OpenSync/ and\nhttps://github.com/moeinrazavi/OpenSync_Unity). Results: Our experimental\nresults show that the OpenSync platform is able to synchronize multiple\nmeasures with microsecond resolution.",
          "link": "http://arxiv.org/abs/2107.14367",
          "publishedOn": "2021-08-02T01:58:24.103Z",
          "wordCount": 659,
          "title": "OpenSync: An opensource platform for synchronizing multiple measures in neuroscience experiments. (arXiv:2107.14367v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14324",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_T/0/1/0/all/0/1\">Tingran Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Buchanan_S/0/1/0/all/0/1\">Sam Buchanan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gilboa_D/0/1/0/all/0/1\">Dar Gilboa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wright_J/0/1/0/all/0/1\">John Wright</a>",
          "description": "Data with low-dimensional nonlinear structure are ubiquitous in engineering\nand scientific problems. We study a model problem with such structure -- a\nbinary classification task that uses a deep fully-connected neural network to\nclassify data drawn from two disjoint smooth curves on the unit sphere. Aside\nfrom mild regularity conditions, we place no restrictions on the configuration\nof the curves. We prove that when (i) the network depth is large relative to\ncertain geometric properties that set the difficulty of the problem and (ii)\nthe network width and number of samples is polynomial in the depth,\nrandomly-initialized gradient descent quickly learns to correctly classify all\npoints on the two curves with high probability. To our knowledge, this is the\nfirst generalization guarantee for deep networks with nonlinear data that\ndepends only on intrinsic data properties. Our analysis proceeds by a reduction\nto dynamics in the neural tangent kernel (NTK) regime, where the network depth\nplays the role of a fitting resource in solving the classification problem. In\nparticular, via fine-grained control of the decay properties of the NTK, we\ndemonstrate that when the network is sufficiently deep, the NTK can be locally\napproximated by a translationally invariant operator on the manifolds and\nstably inverted over smooth functions, which guarantees convergence and\ngeneralization.",
          "link": "http://arxiv.org/abs/2107.14324",
          "publishedOn": "2021-08-02T01:58:24.096Z",
          "wordCount": 650,
          "title": "Deep Networks Provably Classify Data on Curves. (arXiv:2107.14324v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruihuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianzhong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuaijun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>",
          "description": "Most existing domain adaptation methods focus on adaptation from only one\nsource domain, however, in practice there are a number of relevant sources that\ncould be leveraged to help improve performance on target domain. We propose a\nnovel approach named T-SVDNet to address the task of Multi-source Domain\nAdaptation (MDA), which is featured by incorporating Tensor Singular Value\nDecomposition (T-SVD) into a neural network's training pipeline. Overall,\nhigh-order correlations among multiple domains and categories are fully\nexplored so as to better bridge the domain gap. Specifically, we impose\nTensor-Low-Rank (TLR) constraint on a tensor obtained by stacking up a group of\nprototypical similarity matrices, aiming at capturing consistent data structure\nacross different domains. Furthermore, to avoid negative transfer brought by\nnoisy source data, we propose a novel uncertainty-aware weighting strategy to\nadaptively assign weights to different source domains and samples based on the\nresult of uncertainty estimation. Extensive experiments conducted on public\nbenchmarks demonstrate the superiority of our model in addressing the task of\nMDA compared to state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.14447",
          "publishedOn": "2021-08-02T01:58:24.077Z",
          "wordCount": 623,
          "title": "T-SVDNet: Exploring High-Order Prototypical Correlations for Multi-Source Domain Adaptation. (arXiv:2107.14447v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruobin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Liang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suganthan_P/0/1/0/all/0/1\">P.N. Suganthan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuen_K/0/1/0/all/0/1\">Kum Fai Yuen</a>",
          "description": "Electricity load forecasting is crucial for the power systems' planning and\nmaintenance. However, its un-stationary and non-linear characteristics impose\nsignificant difficulties in anticipating future demand. This paper proposes a\nnovel ensemble deep Random Vector Functional Link (edRVFL) network for\nelectricity load forecasting. The weights of hidden layers are randomly\ninitialized and kept fixed during the training process. The hidden layers are\nstacked to enforce deep representation learning. Then, the model generates the\nforecasts by ensembling the outputs of each layer. Moreover, we also propose to\naugment the random enhancement features by empirical wavelet transformation\n(EWT). The raw load data is decomposed by EWT in a walk-forward fashion, not\nintroducing future data leakage problems in the decomposition process. Finally,\nall the sub-series generated by the EWT, including raw data, are fed into the\nedRVFL for forecasting purposes. The proposed model is evaluated on twenty\npublicly available time series from the Australian Energy Market Operator of\nthe year 2020. The simulation results demonstrate the proposed model's superior\nperformance over eleven forecasting methods in three error metrics and\nstatistical tests on electricity load forecasting tasks.",
          "link": "http://arxiv.org/abs/2107.14385",
          "publishedOn": "2021-08-02T01:58:24.071Z",
          "wordCount": 636,
          "title": "Random vector functional link neural network based ensemble deep learning for short-term load forecasting. (arXiv:2107.14385v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14368",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1\">Akash Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aich_A/0/1/0/all/0/1\">Abhishek Aich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodriguez_K/0/1/0/all/0/1\">Kevin Rodriguez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reddy_G/0/1/0/all/0/1\">G. Venugopala Reddy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>",
          "description": "While machine learning approaches have shown remarkable performance in\nbiomedical image analysis, most of these methods rely on high-quality and\naccurate imaging data. However, collecting such data requires intensive and\ncareful manual effort. One of the major challenges in imaging the Shoot Apical\nMeristem (SAM) of Arabidopsis thaliana, is that the deeper slices in the\nz-stack suffer from different perpetual quality-related problems like poor\ncontrast and blurring. These quality-related issues often lead to the disposal\nof the painstakingly collected data with little to no control on quality while\ncollecting the data. Therefore, it becomes necessary to employ and design\ntechniques that can enhance the images to make them more suitable for further\nanalysis. In this paper, we propose a data-driven Deep Quantized Latent\nRepresentation (DQLR) methodology for high-quality image reconstruction in the\nShoot Apical Meristem (SAM) of Arabidopsis thaliana. Our proposed framework\nutilizes multiple consecutive slices in the z-stack to learn a low dimensional\nlatent space, quantize it and subsequently perform reconstruction using the\nquantized representation to obtain sharper images. Experiments on a publicly\navailable dataset validate our methodology showing promising results.",
          "link": "http://arxiv.org/abs/2107.14368",
          "publishedOn": "2021-08-02T01:58:24.064Z",
          "wordCount": 629,
          "title": "Deep Quantized Representation for Enhanced Reconstruction. (arXiv:2107.14368v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14372",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huppertz_R/0/1/0/all/0/1\">Robert Huppertz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakalembe_C/0/1/0/all/0/1\">Catherine Nakalembe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerner_H/0/1/0/all/0/1\">Hannah Kerner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lachyan_R/0/1/0/all/0/1\">Ramani Lachyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rischard_M/0/1/0/all/0/1\">Maxime Rischard</a>",
          "description": "With the global refugee crisis at a historic high, there is a growing need to\nassess the impact of refugee settlements on their hosting countries and\nsurrounding environments. Because fires are an important land management\npractice in smallholder agriculture in sub-Saharan Africa, burned area (BA)\nmappings can help provide information about the impacts of land management\npractices on local environments. However, a lack of BA ground-truth data in\nmuch of sub-Saharan Africa limits the use of highly scalable deep learning (DL)\ntechniques for such BA mappings. In this work, we propose a scalable transfer\nlearning approach to study BA dynamics in areas with little to no ground-truth\ndata such as the West Nile region in Northern Uganda. We train a deep learning\nmodel on BA ground-truth data in Portugal and propose the application of that\nmodel on refugee-hosting districts in West Nile between 2015 and 2020. By\ncomparing the district-level BA dynamic with the wider West Nile region, we aim\nto add understanding of the land management impacts of refugee settlements on\ntheir surrounding environments.",
          "link": "http://arxiv.org/abs/2107.14372",
          "publishedOn": "2021-08-02T01:58:24.055Z",
          "wordCount": 633,
          "title": "Using transfer learning to study burned area dynamics: A case study of refugee settlements in West Nile, Northern Uganda. (arXiv:2107.14372v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Piyush K. Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1\">Rolando Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaroukian_E/0/1/0/all/0/1\">Erin Zaroukian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorothy_M/0/1/0/all/0/1\">Michael Dorothy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basak_A/0/1/0/all/0/1\">Anjon Basak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asher_D/0/1/0/all/0/1\">Derrik E. Asher</a>",
          "description": "Much work has been dedicated to the exploration of Multi-Agent Reinforcement\nLearning (MARL) paradigms implementing a centralized learning with\ndecentralized execution (CLDE) approach to achieve human-like collaboration in\ncooperative tasks. Here, we discuss variations of centralized training and\ndescribe a recent survey of algorithmic approaches. The goal is to explore how\ndifferent implementations of information sharing mechanism in centralized\nlearning may give rise to distinct group coordinated behaviors in multi-agent\nsystems performing cooperative tasks.",
          "link": "http://arxiv.org/abs/2107.14316",
          "publishedOn": "2021-08-02T01:58:24.039Z",
          "wordCount": 561,
          "title": "Survey of Recent Multi-Agent Reinforcement Learning Algorithms Utilizing Centralized Training. (arXiv:2107.14316v1 [cs.MA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Citovsky_G/0/1/0/all/0/1\">Gui Citovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeSalvo_G/0/1/0/all/0/1\">Giulia DeSalvo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gentile_C/0/1/0/all/0/1\">Claudio Gentile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karydas_L/0/1/0/all/0/1\">Lazaros Karydas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopalan_A/0/1/0/all/0/1\">Anand Rajagopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostamizadeh_A/0/1/0/all/0/1\">Afshin Rostamizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>",
          "description": "The ability to train complex and highly effective models often requires an\nabundance of training data, which can easily become a bottleneck in cost, time,\nand computational resources. Batch active learning, which adaptively issues\nbatched queries to a labeling oracle, is a common approach for addressing this\nproblem. The practical benefits of batch sampling come with the downside of\nless adaptivity and the risk of sampling redundant examples within a batch -- a\nrisk that grows with the batch size. In this work, we analyze an efficient\nactive learning algorithm, which focuses on the large batch setting. In\nparticular, we show that our sampling method, which combines notions of\nuncertainty and diversity, easily scales to batch sizes (100K-1M) several\norders of magnitude larger than used in previous studies and provides\nsignificant improvements in model training efficiency compared to recent\nbaselines. Finally, we provide an initial theoretical analysis, proving label\ncomplexity guarantees for a related sampling method, which we show is\napproximately equivalent to our sampling method in specific settings.",
          "link": "http://arxiv.org/abs/2107.14263",
          "publishedOn": "2021-08-02T01:58:24.017Z",
          "wordCount": 602,
          "title": "Batch Active Learning at Scale. (arXiv:2107.14263v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atzberger_P/0/1/0/all/0/1\">Paul J. Atzberger</a>",
          "description": "We discuss a software package for incorporating into simulations data-driven\nmodels trained using machine learning methods. These can be used for (i)\nmodeling dynamics and time-step integration, (ii) modeling interactions between\nsystem components, and (iii) computing quantities of interest characterizing\nsystem state. The package allows for use of machine learning methods with\ngeneral model classes including Neural Networks, Gaussian Process Regression,\nKernel Models, and other approaches. We discuss in this whitepaper our\nprototype C++ package, aims, and example usage.",
          "link": "http://arxiv.org/abs/2107.14362",
          "publishedOn": "2021-08-02T01:58:24.007Z",
          "wordCount": 526,
          "title": "MLMOD Package: Machine Learning Methods for Data-Driven Modeling in LAMMPS. (arXiv:2107.14362v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tipirneni_S/0/1/0/all/0/1\">Sindhu Tipirneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chandan K. Reddy</a>",
          "description": "Multivariate time-series (MVTS) data are frequently observed in critical care\nsettings and are typically characterized by excessive missingness and irregular\ntime intervals. Existing approaches for learning representations in this domain\nhandle such issues by either aggregation or imputation of values, which in-turn\nsuppresses the fine-grained information and adds undesirable noise/overhead\ninto the machine learning model. To tackle this challenge, we propose STraTS\n(Self-supervised Transformer for TimeSeries) model which bypasses these\npitfalls by treating time-series as a set of observation triplets instead of\nusing the traditional dense matrix representation. It employs a novel\nContinuous Value Embedding (CVE) technique to encode continuous time and\nvariable values without the need for discretization. It is composed of a\nTransformer component with Multi-head attention layers which enables it to\nlearn contextual triplet embeddings while avoiding problems of recurrence and\nvanishing gradients that occur in recurrent architectures. Many healthcare\ndatasets also suffer from the limited availability of labeled data. Our model\nutilizes self-supervision by leveraging unlabeled data to learn better\nrepresentations by performing time-series forecasting as a self-supervision\ntask. Experiments on real-world multivariate clinical time-series benchmark\ndatasets show that STraTS shows better prediction performance than\nstate-of-the-art methods for mortality prediction, especially when labeled data\nis limited. Finally, we also present an interpretable version of STraTS which\ncan identify important measurements in the time-series data.",
          "link": "http://arxiv.org/abs/2107.14293",
          "publishedOn": "2021-08-02T01:58:23.987Z",
          "wordCount": 650,
          "title": "Self-supervised Transformer for Multivariate Clinical Time-Series with Missing Values. (arXiv:2107.14293v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rooke_C/0/1/0/all/0/1\">Clayton Rooke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">Jonathan Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leung_K/0/1/0/all/0/1\">Kin Kwan Leung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volkovs_M/0/1/0/all/0/1\">Maksims Volkovs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuberi_S/0/1/0/all/0/1\">Saba Zuberi</a>",
          "description": "Explanation methods applied to sequential models for multivariate time series\nprediction are receiving more attention in machine learning literature. While\ncurrent methods perform well at providing instance-wise explanations, they\nstruggle to efficiently and accurately make attributions over long periods of\ntime and with complex feature interactions. We propose WinIT, a framework for\nevaluating feature importance in time series prediction settings by quantifying\nthe shift in predictive distribution over multiple instances in a windowed\nsetting. Comprehensive empirical evidence shows our method improves on the\nprevious state-of-the-art, FIT, by capturing temporal dependencies in feature\nimportance. We also demonstrate how the solution improves the appropriate\nattribution of features within time steps, which existing interpretability\nmethods often fail to do. We compare with baselines on simulated and real-world\nclinical data. WinIT achieves 2.47x better performance than FIT and other\nfeature importance methods on real-world clinical MIMIC-mortality task. The\ncode for this work is available at https://github.com/layer6ai-labs/WinIT.",
          "link": "http://arxiv.org/abs/2107.14317",
          "publishedOn": "2021-08-02T01:58:23.970Z",
          "wordCount": 588,
          "title": "Temporal Dependencies in Feature Importance for Time Series Predictions. (arXiv:2107.14317v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14257",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Kotthoff_L/0/1/0/all/0/1\">Lars Kotthoff</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dey_S/0/1/0/all/0/1\">Sourin Dey</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jain_V/0/1/0/all/0/1\">Vivek Jain</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tyrrell_A/0/1/0/all/0/1\">Alexander Tyrrell</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wahab_H/0/1/0/all/0/1\">Hud Wahab</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Johnson_P/0/1/0/all/0/1\">Patrick Johnson</a>",
          "description": "A lot of technological advances depend on next-generation materials, such as\ngraphene, which enables a raft of new applications, for example better\nelectronics. Manufacturing such materials is often difficult; in particular,\nproducing graphene at scale is an open problem. We provide a series of datasets\nthat describe the optimization of the production of laser-induced graphene, an\nestablished manufacturing method that has shown great promise. We pose three\nchallenges based on the datasets we provide -- modeling the behavior of\nlaser-induced graphene production with respect to parameters of the production\nprocess, transferring models and knowledge between different precursor\nmaterials, and optimizing the outcome of the transformation over the space of\npossible production parameters. We present illustrative results, along with the\ncode used to generate them, as a starting point for interested users. The data\nwe provide represents an important real-world application of machine learning;\nto the best of our knowledge, no similar datasets are available.",
          "link": "http://arxiv.org/abs/2107.14257",
          "publishedOn": "2021-08-02T01:58:23.952Z",
          "wordCount": 585,
          "title": "Modeling and Optimizing Laser-Induced Graphene. (arXiv:2107.14257v1 [physics.app-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chris Junchi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "Independent component analysis (ICA) has been a popular dimension reduction\ntool in statistical machine learning and signal processing. In this paper, we\npresent a convergence analysis for an online tensorial ICA algorithm, by\nviewing the problem as a nonconvex stochastic approximation problem. For\nestimating one component, we provide a dynamics-based analysis to prove that\nour online tensorial ICA algorithm with a specific choice of stepsize achieves\na sharp finite-sample error bound. In particular, under a mild assumption on\nthe data-generating distribution and a scaling condition such that $d^4/T$ is\nsufficiently small up to a polylogarithmic factor of data dimension $d$ and\nsample size $T$, a sharp finite-sample error bound of $\\tilde{O}(\\sqrt{d/T})$\ncan be obtained.",
          "link": "http://arxiv.org/abs/2012.14415",
          "publishedOn": "2021-07-30T02:13:30.636Z",
          "wordCount": 592,
          "title": "Stochastic Approximation for Online Tensorial Independent Component Analysis. (arXiv:2012.14415v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Mingyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasanbeig_M/0/1/0/all/0/1\">Mohammadhosein Hasanbeig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shaoping Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1\">Alessandro Abate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_Z/0/1/0/all/0/1\">Zhen Kan</a>",
          "description": "This paper investigates the motion planning of autonomous dynamical systems\nmodeled by Markov decision processes (MDP) with unknown transition\nprobabilities over continuous state and action spaces. Linear temporal logic\n(LTL) is used to specify high-level tasks over infinite horizon, which can be\nconverted into a limit deterministic generalized B\\\"uchi automaton (LDGBA) with\nseveral accepting sets. The novelty is to design an embedded product MDP\n(EP-MDP) between the LDGBA and the MDP by incorporating a synchronous\ntracking-frontier function to record unvisited accepting sets of the automaton,\nand to facilitate the satisfaction of the accepting conditions. The proposed\nLDGBA-based reward shaping and discounting schemes for the model-free\nreinforcement learning (RL) only depend on the EP-MDP states and can overcome\nthe issues of sparse rewards. Rigorous analysis shows that any RL method that\noptimizes the expected discounted return is guaranteed to find an optimal\npolicy whose traces maximize the satisfaction probability. A modular deep\ndeterministic policy gradient (DDPG) is then developed to generate such\npolicies over continuous state and action spaces. The performance of our\nframework is evaluated via an array of OpenAI gym environments.",
          "link": "http://arxiv.org/abs/2102.12855",
          "publishedOn": "2021-07-30T02:13:30.616Z",
          "wordCount": 682,
          "title": "Modular Deep Reinforcement Learning for Continuous Motion Planning with Temporal Logic. (arXiv:2102.12855v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Christopher D. Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1\">Heejin Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1\">George J. Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1\">Pratik Chaudhari</a>",
          "description": "We develop a Multi-Agent Reinforcement Learning (MARL) method to learn\nscalable control policies for target tracking. Our method can handle an\narbitrary number of pursuers and targets; we show results for tasks consisting\nup to 1000 pursuers tracking 1000 targets. We use a decentralized,\npartially-observable Markov Decision Process framework to model pursuers as\nagents receiving partial observations (range and bearing) about targets which\nmove using fixed, unknown policies. An attention mechanism is used to\nparameterize the value function of the agents; this mechanism allows us to\nhandle an arbitrary number of targets. Entropy-regularized off-policy RL\nmethods are used to train a stochastic policy, and we discuss how it enables a\nhedging behavior between pursuers that leads to a weak form of cooperation in\nspite of completely decentralized control execution. We further develop a\nmasking heuristic that allows training on smaller problems with few\npursuers-targets and execution on much larger problems. Thorough simulation\nexperiments, ablation studies, and comparisons to state of the art algorithms\nare performed to study the scalability of the approach and robustness of\nperformance to varying numbers of agents and targets.",
          "link": "http://arxiv.org/abs/2011.08055",
          "publishedOn": "2021-07-30T02:13:30.599Z",
          "wordCount": 656,
          "title": "Scalable Reinforcement Learning Policies for Multi-Agent Control. (arXiv:2011.08055v2 [cs.MA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scheinert_D/0/1/0/all/0/1\">Dominik Scheinert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acker_A/0/1/0/all/0/1\">Alexander Acker</a>",
          "description": "Deployment, operation and maintenance of large IT systems becomes\nincreasingly complex and puts human experts under extreme stress when problems\noccur. Therefore, utilization of machine learning (ML) and artificial\nintelligence (AI) is applied on IT system operation and maintenance -\nsummarized in the term AIOps. One specific direction aims at the recognition of\nre-occurring anomaly types to enable remediation automation. However, due to IT\nsystem specific properties, especially their frequent changes (e.g. software\nupdates, reconfiguration or hardware modernization), recognition of reoccurring\nanomaly types is challenging. Current methods mainly assume a static\ndimensionality of provided data. We propose a method that is invariant to\ndimensionality changes of given data. Resource metric data such as CPU\nutilization, allocated memory and others are modelled as multivariate time\nseries. The extraction of temporal and spatial features together with the\nsubsequent anomaly classification is realized by utilizing TELESTO, our novel\ngraph convolutional neural network (GCNN) architecture. The experimental\nevaluation is conducted in a real-world cloud testbed deployment that is\nhosting two applications. Classification results of injected anomalies on a\ncassandra database node show that TELESTO outperforms the alternative GCNNs and\nachieves an overall classification accuracy of 85.1%. Classification results\nfor the other nodes show accuracy values between 85% and 60%.",
          "link": "http://arxiv.org/abs/2102.12877",
          "publishedOn": "2021-07-30T02:13:30.516Z",
          "wordCount": 684,
          "title": "TELESTO: A Graph Neural Network Model for Anomaly Classification in Cloud Services. (arXiv:2102.12877v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14151",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rao_A/0/1/0/all/0/1\">Aniruddha Rajendra Rao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Reimherr_M/0/1/0/all/0/1\">Matthew Reimherr</a>",
          "description": "We introduce a new class of non-linear function-on-function regression models\nfor functional data using neural networks. We propose a framework using a\nhidden layer consisting of continuous neurons, called a continuous hidden\nlayer, for functional response modeling and give two model fitting strategies,\nFunctional Direct Neural Network (FDNN) and Functional Basis Neural Network\n(FBNN). Both are designed explicitly to exploit the structure inherent in\nfunctional data and capture the complex relations existing between the\nfunctional predictors and the functional response. We fit these models by\nderiving functional gradients and implement regularization techniques for more\nparsimonious results. We demonstrate the power and flexibility of our proposed\nmethod in handling complex functional models through extensive simulation\nstudies as well as real data examples.",
          "link": "http://arxiv.org/abs/2107.14151",
          "publishedOn": "2021-07-30T02:13:30.509Z",
          "wordCount": 566,
          "title": "Modern Non-Linear Function-on-Function Regression. (arXiv:2107.14151v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iranfar_A/0/1/0/all/0/1\">Arman Iranfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arza_A/0/1/0/all/0/1\">Adriana Arza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atienza_D/0/1/0/all/0/1\">David Atienza</a>",
          "description": "Continuous and multimodal stress detection has been performed recently\nthrough wearable devices and machine learning algorithms. However, a well-known\nand important challenge of working on physiological signals recorded by\nconventional monitoring devices is missing data due to sensors insufficient\ncontact and interference by other equipment. This challenge becomes more\nproblematic when the user/patient is mentally or physically active or stressed\nbecause of more frequent conscious or subconscious movements. In this paper, we\npropose ReLearn, a robust machine learning framework for stress detection from\nbiomarkers extracted from multimodal physiological signals. ReLearn effectively\ncopes with missing data and outliers both at training and inference phases.\nReLearn, composed of machine learning models for feature selection, outlier\ndetection, data imputation, and classification, allows us to classify all\nsamples, including those with missing values at inference. In particular,\naccording to our experiments and stress database, while by discarding all\nmissing data, as a simplistic yet common approach, no prediction can be made\nfor 34% of the data at inference, our approach can achieve accurate\npredictions, as high as 78%, for missing samples. Also, our experiments show\nthat the proposed framework obtains a cross-validation accuracy of 86.8% even\nif more than 50% of samples within the features are missing.",
          "link": "http://arxiv.org/abs/2104.14278",
          "publishedOn": "2021-07-30T02:13:30.484Z",
          "wordCount": 680,
          "title": "ReLearn: A Robust Machine Learning Framework in Presence of Missing Data for Multimodal Stress Detection from Physiological Signals. (arXiv:2104.14278v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1801.02982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1\">Zeyuan Allen-Zhu</a>",
          "description": "Stochastic gradient descent (SGD) gives an optimal convergence rate when\nminimizing convex stochastic objectives $f(x)$. However, in terms of making the\ngradients small, the original SGD does not give an optimal rate, even when\n$f(x)$ is convex.\n\nIf $f(x)$ is convex, to find a point with gradient norm $\\varepsilon$, we\ndesign an algorithm SGD3 with a near-optimal rate\n$\\tilde{O}(\\varepsilon^{-2})$, improving the best known rate\n$O(\\varepsilon^{-8/3})$ of [18].\n\nIf $f(x)$ is nonconvex, to find its $\\varepsilon$-approximate local minimum,\nwe design an algorithm SGD5 with rate $\\tilde{O}(\\varepsilon^{-3.5})$, where\npreviously SGD variants only achieve $\\tilde{O}(\\varepsilon^{-4})$ [6, 15, 33].\nThis is no slower than the best known stochastic version of Newton's method in\nall parameter regimes [30].",
          "link": "http://arxiv.org/abs/1801.02982",
          "publishedOn": "2021-07-30T02:13:30.469Z",
          "wordCount": 620,
          "title": "How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex SGD. (arXiv:1801.02982v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhanpeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qirui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>",
          "description": "This paper aims to formulate the problem of estimating the optimal baseline\nvalues for the Shapley value in game theory. The Shapley value measures the\nattribution of each input variable of a complex model, which is computed as the\nmarginal benefit from the presence of this variable w.r.t.its absence under\ndifferent contexts. To this end, people usually set the input variable to its\nbaseline value to represent the absence of this variable (i.e.the no-signal\nstate of this variable). Previous studies usually determine the baseline values\nin an empirical manner, which hurts the trustworthiness of the Shapley value.\nIn this paper, we revisit the feature representation of a deep model from the\nperspective of game theory, and define the multi-variate interaction patterns\nof input variables to define the no-signal state of an input variable. Based on\nthe multi-variate interaction, we learn the optimal baseline value of each\ninput variable. Experimental results have demonstrated the effectiveness of our\nmethod.",
          "link": "http://arxiv.org/abs/2105.10719",
          "publishedOn": "2021-07-30T02:13:30.463Z",
          "wordCount": 614,
          "title": "Learning Baseline Values for Shapley Values. (arXiv:2105.10719v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thoma_N/0/1/0/all/0/1\">Nils Thoma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhongjie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ventola_F/0/1/0/all/0/1\">Fabrizio Ventola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>",
          "description": "Time series forecasting is a relevant task that is performed in several\nreal-world scenarios such as product sales analysis and prediction of energy\ndemand. Given their accuracy performance, currently, Recurrent Neural Networks\n(RNNs) are the models of choice for this task. Despite their success in time\nseries forecasting, less attention has been paid to make the RNNs trustworthy.\nFor example, RNNs can not naturally provide an uncertainty measure to their\npredictions. This could be extremely useful in practice in several cases e.g.\nto detect when a prediction might be completely wrong due to an unusual pattern\nin the time series. Whittle Sum-Product Networks (WSPNs), prominent deep\ntractable probabilistic circuits (PCs) for time series, can assist an RNN with\nproviding meaningful probabilities as uncertainty measure. With this aim, we\npropose RECOWN, a novel architecture that employs RNNs and a discriminant\nvariant of WSPNs called Conditional WSPNs (CWSPNs). We also formulate a\nLog-Likelihood Ratio Score as better estimation of uncertainty that is tailored\nto time series and Whittle likelihoods. In our experiments, we show that\nRECOWNs are accurate and trustworthy time series predictors, able to \"know when\nthey do not know\".",
          "link": "http://arxiv.org/abs/2106.04148",
          "publishedOn": "2021-07-30T02:13:30.458Z",
          "wordCount": 662,
          "title": "RECOWNs: Probabilistic Circuits for Trustworthy Time Series Forecasting. (arXiv:2106.04148v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.03194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wenbo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shroff_N/0/1/0/all/0/1\">Ness B. Shroff</a>",
          "description": "This paper studies the problem of finding the exact ranking from noisy\ncomparisons. A comparison over a set of $m$ items produces a noisy outcome\nabout the most preferred item, and reveals some information about the ranking.\nBy repeatedly and adaptively choosing items to compare, we want to fully rank\nthe items with a certain confidence, and use as few comparisons as possible.\nDifferent from most previous works, in this paper, we have three main\nnovelties: (i) compared to prior works, our upper bounds (algorithms) and lower\nbounds on the sample complexity (aka number of comparisons) require the minimal\nassumptions on the instances, and are not restricted to specific models; (ii)\nwe give lower bounds and upper bounds on instances with unequal noise levels;\nand (iii) this paper aims at the exact ranking without knowledge on the\ninstances, while most of the previous works either focus on approximate\nrankings or study exact ranking but require prior knowledge. We first derive\nlower bounds for pairwise ranking (i.e., compare two items each time), and then\npropose (nearly) optimal pairwise ranking algorithms. We further make\nextensions to listwise ranking (i.e., comparing multiple items each time).\nNumerical results also show our improvements against the state of the art.",
          "link": "http://arxiv.org/abs/1909.03194",
          "publishedOn": "2021-07-30T02:13:30.452Z",
          "wordCount": 686,
          "title": "On Sample Complexity Upper and Lower Bounds for Exact Ranking from Noisy Comparisons. (arXiv:1909.03194v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.00814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wan-Lei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Peng-Cheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>",
          "description": "k-nearest neighbor graph is a fundamental data structure in many disciplines\nsuch as information retrieval, data-mining, pattern recognition, and machine\nlearning, etc. In the literature, considerable research has been focusing on\nhow to efficiently build an approximate k-nearest neighbor graph (k-NN graph)\nfor a fixed dataset. Unfortunately, a closely related issue of how to merge two\nexisting k-NN graphs has been overlooked. In this paper, we address the issue\nof k-NN graph merging in two different scenarios. In the first scenario, a\nsymmetric merge algorithm is proposed to combine two approximate k-NN graphs.\nThe algorithm facilitates large-scale processing by the efficient merging of\nk-NN graphs that are produced in parallel. In the second scenario, a joint\nmerge algorithm is proposed to expand an existing k-NN graph with a raw\ndataset. The algorithm enables the incremental construction of a hierarchical\napproximate k-NN graph. Superior performance is attained when leveraging the\nhierarchy for NN search of various data types, dimensionality, and distance\nmeasures.",
          "link": "http://arxiv.org/abs/1908.00814",
          "publishedOn": "2021-07-30T02:13:30.446Z",
          "wordCount": 660,
          "title": "On the Merge of k-NN Graph. (arXiv:1908.00814v6 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsiaousis_M/0/1/0/all/0/1\">Michail Tsiaousis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghouts_G/0/1/0/all/0/1\">Gertjan Burghouts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hillerstrom_F/0/1/0/all/0/1\">Fieke Hillerstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putten_P/0/1/0/all/0/1\">Peter van der Putten</a>",
          "description": "The dominant paradigm in spatiotemporal action detection is to classify\nactions using spatiotemporal features learned by 2D or 3D Convolutional\nNetworks. We argue that several actions are characterized by their context,\nsuch as relevant objects and actors present in the video. To this end, we\nintroduce an architecture based on self-attention and Graph Convolutional\nNetworks in order to model contextual cues, such as actor-actor and\nactor-object interactions, to improve human action detection in video. We are\ninterested in achieving this in a weakly-supervised setting, i.e. using as less\nannotations as possible in terms of action bounding boxes. Our model aids\nexplainability by visualizing the learned context as an attention map, even for\nactions and objects unseen during training. We evaluate how well our model\nhighlights the relevant context by introducing a quantitative metric based on\nrecall of objects retrieved by attention maps. Our model relies on a 3D\nconvolutional RGB stream, and does not require expensive optical flow\ncomputation. We evaluate our models on the DALY dataset, which consists of\nhuman-object interaction actions. Experimental results show that our\ncontextualized approach outperforms a baseline action detection approach by\nmore than 2 points in Video-mAP. Code is available at\n\\url{https://github.com/micts/acgcn}",
          "link": "http://arxiv.org/abs/2107.13648",
          "publishedOn": "2021-07-30T02:13:30.432Z",
          "wordCount": 678,
          "title": "Spot What Matters: Learning Context Using Graph Convolutional Networks for Weakly-Supervised Action Detection. (arXiv:2107.13648v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.10143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tomen_N/0/1/0/all/0/1\">Nergis Tomen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan van Gemert</a>",
          "description": "Convolutional layers in CNNs implement linear filters which decompose the\ninput into different frequency bands. However, most modern architectures\nneglect standard principles of filter design when optimizing their model\nchoices regarding the size and shape of the convolutional kernel. In this work,\nwe consider the well-known problem of spectral leakage caused by windowing\nartifacts in filtering operations in the context of CNNs. We show that the\nsmall size of CNN kernels make them susceptible to spectral leakage, which may\ninduce performance-degrading artifacts. To address this issue, we propose the\nuse of larger kernel sizes along with the Hamming window function to alleviate\nleakage in CNN architectures. We demonstrate improved classification accuracy\non multiple benchmark datasets including Fashion-MNIST, CIFAR-10, CIFAR-100 and\nImageNet with the simple use of a standard window function in convolutional\nlayers. Finally, we show that CNNs employing the Hamming window display\nincreased robustness against various adversarial attacks.",
          "link": "http://arxiv.org/abs/2101.10143",
          "publishedOn": "2021-07-30T02:13:30.426Z",
          "wordCount": 615,
          "title": "Spectral Leakage and Rethinking the Kernel Size in CNNs. (arXiv:2101.10143v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>",
          "description": "Ternary Neural Networks (TNNs) have received much attention due to being\npotentially orders of magnitude faster in inference, as well as more power\nefficient, than full-precision counterparts. However, 2 bits are required to\nencode the ternary representation with only 3 quantization levels leveraged. As\na result, conventional TNNs have similar memory consumption and speed compared\nwith the standard 2-bit models, but have worse representational capability.\nMoreover, there is still a significant gap in accuracy between TNNs and\nfull-precision networks, hampering their deployment to real applications. To\ntackle these two challenges, in this work, we first show that, under some mild\nconstraints, computational complexity of the ternary inner product can be\nreduced by a factor of 2. Second, to mitigate the performance gap, we\nelaborately design an implementation-dependent ternary quantization algorithm.\nThe proposed framework is termed Fast and Accurate Ternary Neural Networks\n(FATNN). Experiments on image classification demonstrate that our FATNN\nsurpasses the state-of-the-arts by a significant margin in accuracy. More\nimportantly, speedup evaluation compared with various precisions is analyzed on\nseveral platforms, which serves as a strong benchmark for further research.",
          "link": "http://arxiv.org/abs/2008.05101",
          "publishedOn": "2021-07-30T02:13:30.421Z",
          "wordCount": 669,
          "title": "FATNN: Fast and Accurate Ternary Neural Networks. (arXiv:2008.05101v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Acevedo_Viloria_J/0/1/0/all/0/1\">Jaime D. Acevedo-Viloria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roa_L/0/1/0/all/0/1\">Luisa Roa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeshina_S/0/1/0/all/0/1\">Soji Adeshina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olazo_C/0/1/0/all/0/1\">Cesar Charalla Olazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Rey_A/0/1/0/all/0/1\">Andr&#xe9;s Rodr&#xed;guez-Rey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramos_J/0/1/0/all/0/1\">Jose Alberto Ramos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correa_Bahnsen_A/0/1/0/all/0/1\">Alejandro Correa-Bahnsen</a>",
          "description": "Large digital platforms create environments where different types of user\ninteractions are captured, these relationships offer a novel source of\ninformation for fraud detection problems. In this paper we propose a framework\nof relational graph convolutional networks methods for fraudulent behaviour\nprevention in the financial services of a Super-App. To this end, we apply the\nframework on different heterogeneous graphs of users, devices, and credit\ncards; and finally use an interpretability algorithm for graph neural networks\nto determine the most important relations to the classification task of the\nusers. Our results show that there is an added value when considering models\nthat take advantage of the alternative data of the Super-App and the\ninteractions found in their high connectivity, further proofing how they can\nleverage that into better decisions and fraud detection strategies.",
          "link": "http://arxiv.org/abs/2107.13673",
          "publishedOn": "2021-07-30T02:13:30.415Z",
          "wordCount": 597,
          "title": "Relational Graph Neural Networks for Fraud Detection in a Super-Appe nvironment. (arXiv:2107.13673v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_K/0/1/0/all/0/1\">Kushankur Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellinger_C/0/1/0/all/0/1\">Colin Bellinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corizzo_R/0/1/0/all/0/1\">Roberto Corizzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krawczyk_B/0/1/0/all/0/1\">Bartosz Krawczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Japkowicz_N/0/1/0/all/0/1\">Nathalie Japkowicz</a>",
          "description": "Structural concept complexity, class overlap, and data scarcity are some of\nthe most important factors influencing the performance of classifiers under\nclass imbalance conditions. When these effects were uncovered in the early\n2000s, understandably, the classifiers on which they were demonstrated belonged\nto the classical rather than Deep Learning categories of approaches. As Deep\nLearning is gaining ground over classical machine learning and is beginning to\nbe used in critical applied settings, it is important to assess systematically\nhow well they respond to the kind of challenges their classical counterparts\nhave struggled with in the past two decades. The purpose of this paper is to\nstudy the behavior of deep learning systems in settings that have previously\nbeen deemed challenging to classical machine learning systems to find out\nwhether the depth of the systems is an asset in such settings. The results in\nboth artificial and real-world image datasets (MNIST Fashion, CIFAR-10) show\nthat these settings remain mostly challenging for Deep Learning systems and\nthat deeper architectures seem to help with structural concept complexity but\nnot with overlap challenges in simple artificial domains. Data scarcity is not\novercome by deeper layers, either. In the real-world image domains, where\noverfitting is a greater concern than in the artificial domains, the advantage\nof deeper architectures is less obvious: while it is observed in certain cases,\nit is quickly cancelled as models get deeper and perform worse than their\nshallower counterparts.",
          "link": "http://arxiv.org/abs/2107.14194",
          "publishedOn": "2021-07-30T02:13:30.410Z",
          "wordCount": 680,
          "title": "On the combined effect of class imbalance and concept complexity in deep learning. (arXiv:2107.14194v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14094",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Krishnakumari_P/0/1/0/all/0/1\">Panchamy Krishnakumari</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Cats_O/0/1/0/all/0/1\">Oded Cats</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lint_H/0/1/0/all/0/1\">Hans van Lint</a>",
          "description": "In an effort to improve user satisfaction and transit image, transit service\nproviders worldwide offer delay compensations. Smart card data enables the\nestimation of passenger delays throughout the network and aid in monitoring\nservice performance. Notwithstanding, in order to prioritize measures for\nimproving service reliability and hence reducing passenger delays, it is\nparamount to identify the system components - stations and track segments -\nwhere most passenger delay occurs. To this end, we propose a novel method for\nestimating network passenger delay from individual trajectories. We decompose\nthe delay along a passenger trajectory into its corresponding track segment\ndelay, initial waiting time and transfer delay. We distinguish between two\ndifferent types of passenger delay in relation to the public transit network:\naverage passenger delay and total passenger delay. We employ temporal\nclustering on these two quantities to reveal daily and seasonal regularity in\ndelay patterns of the transit network. The estimation and clustering methods\nare demonstrated on one year of data from Washington metro network. The data\nconsists of schedule information and smart card data which includes\npassenger-train assignment of the metro network for the months of August 2017\nto August 2018. Our findings show that the average passenger delay is\nrelatively stable throughout the day. The temporal clustering reveals\npronounced and recurrent and thus predictable daily and weekly patterns with\ndistinct characteristics for certain months.",
          "link": "http://arxiv.org/abs/2107.14094",
          "publishedOn": "2021-07-30T02:13:30.393Z",
          "wordCount": 674,
          "title": "Day-to-day and seasonal regularity of network passenger delay for metro networks. (arXiv:2107.14094v1 [physics.soc-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Strouse_D/0/1/0/all/0/1\">DJ Strouse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumli_K/0/1/0/all/0/1\">Kate Baumli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warde_Farley_D/0/1/0/all/0/1\">David Warde-Farley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mnih_V/0/1/0/all/0/1\">Vlad Mnih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_S/0/1/0/all/0/1\">Steven Hansen</a>",
          "description": "Unsupervised skill learning objectives (Gregor et al., 2016, Eysenbach et\nal., 2018) allow agents to learn rich repertoires of behavior in the absence of\nextrinsic rewards. They work by simultaneously training a policy to produce\ndistinguishable latent-conditioned trajectories, and a discriminator to\nevaluate distinguishability by trying to infer latents from trajectories. The\nhope is for the agent to explore and master the environment by encouraging each\nskill (latent) to reliably reach different states. However, an inherent\nexploration problem lingers: when a novel state is actually encountered, the\ndiscriminator will necessarily not have seen enough training data to produce\naccurate and confident skill classifications, leading to low intrinsic reward\nfor the agent and effective penalization of the sort of exploration needed to\nactually maximize the objective. To combat this inherent pessimism towards\nexploration, we derive an information gain auxiliary objective that involves\ntraining an ensemble of discriminators and rewarding the policy for their\ndisagreement. Our objective directly estimates the epistemic uncertainty that\ncomes from the discriminator not having seen enough training examples, thus\nproviding an intrinsic reward more tailored to the true objective compared to\npseudocount-based methods (Burda et al., 2019). We call this exploration bonus\ndiscriminator disagreement intrinsic reward, or DISDAIN. We demonstrate\nempirically that DISDAIN improves skill learning both in a tabular grid world\n(Four Rooms) and the 57 games of the Atari Suite (from pixels). Thus, we\nencourage researchers to treat pessimism with DISDAIN.",
          "link": "http://arxiv.org/abs/2107.14226",
          "publishedOn": "2021-07-30T02:13:30.387Z",
          "wordCount": 684,
          "title": "Learning more skills through optimistic exploration. (arXiv:2107.14226v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guoliang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Ting Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jin Song Dong</a>",
          "description": "Although deep learning has demonstrated astonishing performance in many\napplications, there are still concerns about its dependability. One desirable\nproperty of deep learning applications with societal impact is fairness (i.e.,\nnon-discrimination). Unfortunately, discrimination might be intrinsically\nembedded into the models due to the discrimination in the training data. As a\ncountermeasure, fairness testing systemically identifies discriminatory\nsamples, which can be used to retrain the model and improve the model's\nfairness. Existing fairness testing approaches however have two major\nlimitations. Firstly, they only work well on traditional machine learning\nmodels and have poor performance (e.g., effectiveness and efficiency) on deep\nlearning models. Secondly, they only work on simple structured (e.g., tabular)\ndata and are not applicable for domains such as text. In this work, we bridge\nthe gap by proposing a scalable and effective approach for systematically\nsearching for discriminatory samples while extending existing fairness testing\napproaches to address a more challenging domain, i.e., text classification.\nCompared with state-of-the-art methods, our approach only employs lightweight\nprocedures like gradient computation and clustering, which is significantly\nmore scalable and effective. Experimental results show that on average, our\napproach explores the search space much more effectively (9.62 and 2.38 times\nmore than the state-of-the-art methods respectively on tabular and text\ndatasets) and generates much more discriminatory samples (24.95 and 2.68 times)\nwithin a same reasonable time. Moreover, the retrained models reduce\ndiscrimination by 57.2% and 60.2% respectively on average.",
          "link": "http://arxiv.org/abs/2107.08176",
          "publishedOn": "2021-07-30T02:13:30.380Z",
          "wordCount": 702,
          "title": "Automatic Fairness Testing of Neural Classifiers through Adversarial Sampling. (arXiv:2107.08176v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_R/0/1/0/all/0/1\">Raman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravorty_S/0/1/0/all/0/1\">Suman Chakravorty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1\">Mohamed Naveed Gul Mohamed</a>",
          "description": "We consider the problem of Reinforcement Learning for nonlinear stochastic\ndynamical systems. We show that in the RL setting, there is an inherent ``Curse\nof Variance\" in addition to Bellman's infamous ``Curse of Dimensionality\", in\nparticular, we show that the variance in the solution grows\nfactorial-exponentially in the order of the approximation. A fundamental\nconsequence is that this precludes the search for anything other than ``local\"\nfeedback solutions in RL, in order to control the explosive variance growth,\nand thus, ensure accuracy. We further show that the deterministic optimal\ncontrol has a perturbation structure, in that the higher order terms do not\naffect the calculation of lower order terms, which can be utilized in RL to get\naccurate local solutions.",
          "link": "http://arxiv.org/abs/2011.10829",
          "publishedOn": "2021-07-30T02:13:30.374Z",
          "wordCount": 593,
          "title": "On the Convergence of Reinforcement Learning in Nonlinear Continuous State Space Problems. (arXiv:2011.10829v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14135",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">YunPeng Li</a>",
          "description": "Convolutive blind source separation (BSS) is intended to recover the unknown\ncomponents from their convolutive mixtures. Contrary to the contrast functions\nused in instantaneous cases, the spatial-temporal prewhitening stage and the\npara-unitary filters constraint are difficult to implement in a convolutive\ncontext. In this paper, we propose several modifications of FastICA to\nalleviate these difficulties. Our method performs the simple prewhitening step\non convolutive mixtures prior to the separation and optimizes the contrast\nfunction under the diagonalization constraint implemented by single value\ndecomposition (SVD). Numerical simulations are implemented to verify the\nperformance of the proposed method.",
          "link": "http://arxiv.org/abs/2107.14135",
          "publishedOn": "2021-07-30T02:13:30.369Z",
          "wordCount": 527,
          "title": "Modifications of FastICA in Convolutive Blind Source Separation. (arXiv:2107.14135v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11815",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liang_T/0/1/0/all/0/1\">Tengyuan Liang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Recht_B/0/1/0/all/0/1\">Benjamin Recht</a>",
          "description": "This paper provides elementary analyses of the regret and generalization of\nminimum-norm interpolating classifiers (MNIC). The MNIC is the function of\nsmallest Reproducing Kernel Hilbert Space norm that perfectly interpolates a\nlabel pattern on a finite data set. We derive a mistake bound for MNIC and a\nregularized variant that holds for all data sets. This bound follows from\nelementary properties of matrix inverses. Under the assumption that the data is\nindependently and identically distributed, the mistake bound implies that MNIC\ngeneralizes at a rate proportional to the norm of the interpolating solution\nand inversely proportional to the number of data points. This rate matches\nsimilar rates derived for margin classifiers and perceptrons. We derive several\nplausible generative models where the norm of the interpolating classifier is\nbounded or grows at a rate sublinear in $n$. We also show that as long as the\npopulation class conditional distributions are sufficiently separable in total\nvariation, then MNIC generalizes with a fast rate.",
          "link": "http://arxiv.org/abs/2101.11815",
          "publishedOn": "2021-07-30T02:13:30.350Z",
          "wordCount": 618,
          "title": "Interpolating Classifiers Make Few Mistakes. (arXiv:2101.11815v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.01845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drori_Y/0/1/0/all/0/1\">Yoel Drori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1\">Ohad Shamir</a>",
          "description": "We study the iteration complexity of stochastic gradient descent (SGD) for\nminimizing the gradient norm of smooth, possibly nonconvex functions. We\nprovide several results, implying that the $\\mathcal{O}(\\epsilon^{-4})$ upper\nbound of Ghadimi and Lan~\\cite{ghadimi2013stochastic} (for making the average\ngradient norm less than $\\epsilon$) cannot be improved upon, unless a\ncombination of additional assumptions is made. Notably, this holds even if we\nlimit ourselves to convex quadratic functions. We also show that for nonconvex\nfunctions, the feasibility of minimizing gradients with SGD is surprisingly\nsensitive to the choice of optimality criteria.",
          "link": "http://arxiv.org/abs/1910.01845",
          "publishedOn": "2021-07-30T02:13:30.345Z",
          "wordCount": 577,
          "title": "The Complexity of Finding Stationary Points with Stochastic Gradient Descent. (arXiv:1910.01845v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gaurav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Chenzhong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_J/0/1/0/all/0/1\">Jyotirmoy V. Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogdan_P/0/1/0/all/0/1\">Paul Bogdan</a>",
          "description": "Reinforcement learning (RL) is a technique to learn the control policy for an\nagent that interacts with a stochastic environment. In any given state, the\nagent takes some action, and the environment determines the probability\ndistribution over the next state as well as gives the agent some reward. Most\nRL algorithms typically assume that the environment satisfies Markov\nassumptions (i.e. the probability distribution over the next state depends only\non the current state). In this paper, we propose a model-based RL technique for\na system that has non-Markovian dynamics. Such environments are common in many\nreal-world applications such as in human physiology, biological systems,\nmaterial science, and population dynamics. Model-based RL (MBRL) techniques\ntypically try to simultaneously learn a model of the environment from the data,\nas well as try to identify an optimal policy for the learned model. We propose\na technique where the non-Markovianity of the system is modeled through a\nfractional dynamical system. We show that we can quantify the difference in the\nperformance of an MBRL algorithm that uses bounded horizon model predictive\ncontrol from the optimal policy. Finally, we demonstrate our proposed framework\non a pharmacokinetic model of human blood glucose dynamics and show that our\nfractional models can capture distant correlations on real-world datasets.",
          "link": "http://arxiv.org/abs/2107.13790",
          "publishedOn": "2021-07-30T02:13:30.336Z",
          "wordCount": 642,
          "title": "Non-Markovian Reinforcement Learning using Fractional Dynamics. (arXiv:2107.13790v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomerantz_R/0/1/0/all/0/1\">Roi Pomerantz</a>",
          "description": "We present Translatotron 2, a neural direct speech-to-speech translation\nmodel that can be trained end-to-end. Translatotron 2 consists of a speech\nencoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention\nmodule that connects all the previous three components. Experimental results\nsuggest that Translatotron 2 outperforms the original Translatotron by a large\nmargin in terms of translation quality and predicted speech naturalness, and\ndrastically improves the robustness of the predicted speech by mitigating\nover-generation, such as babbling or long pause. We also propose a new method\nfor retaining the source speaker's voice in the translated speech. The trained\nmodel is restricted to retain the source speaker's voice, and unlike the\noriginal Translatotron, it is not able to generate speech in a different\nspeaker's voice, making the model more robust for production deployment, by\nmitigating potential misuse for creating spoofing audio artifacts. When the new\nmethod is used together with a simple concatenation-based data augmentation,\nthe trained Translatotron 2 model is able to retain each speaker's voice for\ninput with speaker turns.",
          "link": "http://arxiv.org/abs/2107.08661",
          "publishedOn": "2021-07-30T02:13:30.331Z",
          "wordCount": 631,
          "title": "Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14203",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1\">Lingjiao Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cai_T/0/1/0/all/0/1\">Tracy Cai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>",
          "description": "Machine learning (ML) prediction APIs are increasingly widely used. An ML API\ncan change over time due to model updates or retraining. This presents a key\nchallenge in the usage of the API because it is often not clear to the user if\nand how the ML model has changed. Model shifts can affect downstream\napplication performance and also create oversight issues (e.g. if consistency\nis desired). In this paper, we initiate a systematic investigation of ML API\nshifts. We first quantify the performance shifts from 2020 to 2021 of popular\nML APIs from Google, Microsoft, Amazon, and others on a variety of datasets. We\nidentified significant model shifts in 12 out of 36 cases we investigated.\nInterestingly, we found several datasets where the API's predictions became\nsignificantly worse over time. This motivated us to formulate the API shift\nassessment problem at a more fine-grained level as estimating how the API\nmodel's confusion matrix changes over time when the data distribution is\nconstant. Monitoring confusion matrix shifts using standard random sampling can\nrequire a large number of samples, which is expensive as each API call costs a\nfee. We propose a principled adaptive sampling algorithm, MASA, to efficiently\nestimate confusion matrix shifts. MASA can accurately estimate the confusion\nmatrix shifts in commercial ML APIs using up to 90% fewer samples compared to\nrandom sampling. This work establishes ML API shifts as an important problem to\nstudy and provides a cost-effective approach to monitor such shifts.",
          "link": "http://arxiv.org/abs/2107.14203",
          "publishedOn": "2021-07-30T02:13:30.326Z",
          "wordCount": 697,
          "title": "Did the Model Change? Efficiently Assessing Machine Learning API Shifts. (arXiv:2107.14203v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2103.15990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rex Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramli_A/0/1/0/all/0/1\">Albara Ah Ramli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huanle Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_E/0/1/0/all/0/1\">Esha Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henricson_E/0/1/0/all/0/1\">Erik Henricson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>",
          "description": "With the rapid development of the internet of things (IoT) and artificial\nintelligence (AI) technologies, human activity recognition (HAR) has been\napplied in a variety of domains such as security and surveillance, human-robot\ninteraction, and entertainment. Even though a number of surveys and review\npapers have been published, there is a lack of HAR overview papers focusing on\nhealthcare applications that use wearable sensors. Therefore, we fill in the\ngap by presenting this overview paper. In particular, we present our projects\nto illustrate the system design of HAR applications for healthcare. Our\nprojects include early mobility identification of human activities for\nintensive care unit (ICU) patients and gait analysis of Duchenne muscular\ndystrophy (DMD) patients. We cover essential components of designing HAR\nsystems including sensor factors (e.g., type, number, and placement location),\nAI model selection (e.g., classical machine learning models versus deep\nlearning models), and feature engineering. In addition, we highlight the\nchallenges of such healthcare-oriented HAR systems and propose several research\nopportunities for both the medical and the computer science community.",
          "link": "http://arxiv.org/abs/2103.15990",
          "publishedOn": "2021-07-30T02:13:30.313Z",
          "wordCount": 671,
          "title": "An Overview of Human Activity Recognition Using Wearable Sensors: Healthcare and Artificial Intelligence. (arXiv:2103.15990v4 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheuk_K/0/1/0/all/0/1\">Kin Wai Cheuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1\">Dorien Herremans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Li Su</a>",
          "description": "Most of the current supervised automatic music transcription (AMT) models\nlack the ability to generalize. This means that they have trouble transcribing\nreal-world music recordings from diverse musical genres that are not presented\nin the labelled training data. In this paper, we propose a semi-supervised\nframework, ReconVAT, which solves this issue by leveraging the huge amount of\navailable unlabelled music recordings. The proposed ReconVAT uses\nreconstruction loss and virtual adversarial training. When combined with\nexisting U-net models for AMT, ReconVAT achieves competitive results on common\nbenchmark datasets such as MAPS and MusicNet. For example, in the few-shot\nsetting for the string part version of MusicNet, ReconVAT achieves F1-scores of\n61.0% and 41.6% for the note-wise and note-with-offset-wise metrics\nrespectively, which translates into an improvement of 22.2% and 62.5% compared\nto the supervised baseline model. Our proposed framework also demonstrates the\npotential of continual learning on new data, which could be useful in\nreal-world applications whereby new data is constantly available.",
          "link": "http://arxiv.org/abs/2107.04954",
          "publishedOn": "2021-07-30T02:13:30.307Z",
          "wordCount": 632,
          "title": "ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data. (arXiv:2107.04954v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1\">Juan C. P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeanneret_G/0/1/0/all/0/1\">Guillaume Jeanneret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueda_L/0/1/0/all/0/1\">Laura Rueda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1\">Pablo Arbel&#xe1;ez</a>",
          "description": "Deep learning models are prone to being fooled by imperceptible perturbations\nknown as adversarial attacks. In this work, we study how equipping models with\nTest-time Transformation Ensembling (TTE) can work as a reliable defense\nagainst such attacks. While transforming the input data, both at train and test\ntimes, is known to enhance model performance, its effects on adversarial\nrobustness have not been studied. Here, we present a comprehensive empirical\nstudy of the impact of TTE, in the form of widely-used image transforms, on\nadversarial robustness. We show that TTE consistently improves model robustness\nagainst a variety of powerful attacks without any need for re-training, and\nthat this improvement comes at virtually no trade-off with accuracy on clean\nsamples. Finally, we show that the benefits of TTE transfer even to the\ncertified robustness domain, in which TTE provides sizable and consistent\nimprovements.",
          "link": "http://arxiv.org/abs/2107.14110",
          "publishedOn": "2021-07-30T02:13:30.302Z",
          "wordCount": 588,
          "title": "Enhancing Adversarial Robustness via Test-time Transformation Ensembling. (arXiv:2107.14110v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.11270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muller_R/0/1/0/all/0/1\">Roger Alexander M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laflamme_Janssen_J/0/1/0/all/0/1\">Jonathan Laflamme-Janssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacaro_J/0/1/0/all/0/1\">Jaime Camacaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bessega_C/0/1/0/all/0/1\">Carolina Bessega</a>",
          "description": "In this article we address the question whether it is possible to learn the\ndifferential equations describing the physical properties of a dynamical\nsystem, subject to non-conservative forces, from observations of its realspace\ntrajectory(ies) only. We introduce a network that incorporates a difference\napproximation for the second order derivative in terms of residual connections\nbetween convolutional blocks, whose shared weights represent the coefficients\nof a second order ordinary differential equation. We further combine this\nsolver-like architecture with a convolutional network, capable of learning the\nrelation between trajectories of coupled oscillators and therefore allows us to\nmake a stable forecast even if the system is only partially observed. We\noptimize this map together with the solver network, while sharing their\nweights, to form a powerful framework capable of learning the complex physical\nproperties of a dissipative dynamical system.",
          "link": "http://arxiv.org/abs/2010.11270",
          "publishedOn": "2021-07-30T02:13:30.296Z",
          "wordCount": 605,
          "title": "Learning second order coupled differential equations that are subject to non-conservative forces. (arXiv:2010.11270v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pizzati_F/0/1/0/all/0/1\">Fabio Pizzati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerri_P/0/1/0/all/0/1\">Pietro Cerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>",
          "description": "Image-to-image translation (i2i) networks suffer from entanglement effects in\npresence of physics-related phenomena in target domain (such as occlusions,\nfog, etc), thus lowering the translation quality and variability. In this\npaper, we present a comprehensive method for disentangling physics-based traits\nin the translation, guiding the learning process with neural or physical\nmodels. For the latter, we integrate adversarial estimation and genetic\nalgorithms to correctly achieve disentanglement. The results show our approach\ndramatically increase performances in many challenging scenarios for image\ntranslation.",
          "link": "http://arxiv.org/abs/2107.14229",
          "publishedOn": "2021-07-30T02:13:30.290Z",
          "wordCount": 524,
          "title": "Guided Disentanglement in Generative Networks. (arXiv:2107.14229v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.06254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1\">Valerie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jeffrey Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joon Sik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plumb_G/0/1/0/all/0/1\">Gregory Plumb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>",
          "description": "Despite increasing interest in the field of Interpretable Machine Learning\n(IML), a significant gap persists between the technical objectives targeted by\nresearchers' methods and the high-level goals of consumers' use cases. In this\nwork, we synthesize foundational work on IML methods and evaluation into an\nactionable taxonomy. This taxonomy serves as a tool to conceptualize the gap\nbetween researchers and consumers, illustrated by the lack of connections\nbetween its methods and use cases components. It also provides the foundation\nfrom which we describe a three-step workflow to better enable researchers and\nconsumers to work together to discover what types of methods are useful for\nwhat use cases. Eventually, by building on the results generated from this\nworkflow, a more complete version of the taxonomy will increasingly allow\nconsumers to find relevant methods for their target use cases and researchers\nto identify applicable use cases for their proposed methods.",
          "link": "http://arxiv.org/abs/2103.06254",
          "publishedOn": "2021-07-30T02:13:30.276Z",
          "wordCount": 617,
          "title": "Interpretable Machine Learning: Moving From Mythos to Diagnostics. (arXiv:2103.06254v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">Eugene Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Cheng-Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yi Lee</a>",
          "description": "Deep neural networks (DNNs) are known to perform well when deployed to test\ndistributions that shares high similarity with the training distribution.\nFeeding DNNs with new data sequentially that were unseen in the training\ndistribution has two major challenges -- fast adaptation to new tasks and\ncatastrophic forgetting of old tasks. Such difficulties paved way for the\non-going research on few-shot learning and continual learning. To tackle these\nproblems, we introduce Attentive Independent Mechanisms (AIM). We incorporate\nthe idea of learning using fast and slow weights in conjunction with the\ndecoupling of the feature extraction and higher-order conceptual learning of a\nDNN. AIM is designed for higher-order conceptual learning, modeled by a mixture\nof experts that compete to learn independent concepts to solve a new task. AIM\nis a modular component that can be inserted into existing deep learning\nframeworks. We demonstrate its capability for few-shot learning by adding it to\nSIB and trained on MiniImageNet and CIFAR-FS, showing significant improvement.\nAIM is also applied to ANML and OML trained on Omniglot, CIFAR-100 and\nMiniImageNet to demonstrate its capability in continual learning. Code made\npublicly available at https://github.com/huang50213/AIM-Fewshot-Continual.",
          "link": "http://arxiv.org/abs/2107.14053",
          "publishedOn": "2021-07-30T02:13:30.270Z",
          "wordCount": 643,
          "title": "Few-Shot and Continual Learning with Attentive Independent Mechanisms. (arXiv:2107.14053v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.02469",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Subramani_K/0/1/0/all/0/1\">Krishna Subramani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smaragdis_P/0/1/0/all/0/1\">Paris Smaragdis</a>",
          "description": "Most audio processing pipelines involve transformations that act on\nfixed-dimensional input representations of audio. For example, when using the\nShort Time Fourier Transform (STFT) the DFT size specifies a fixed dimension\nfor the input representation. As a consequence, most audio machine learning\nmodels are designed to process fixed-size vector inputs which often prohibits\nthe repurposing of learned models on audio with different sampling rates or\nalternative representations. We note, however, that the intrinsic spectral\ninformation in the audio signal is invariant to the choice of the input\nrepresentation or the sampling rate. Motivated by this, we introduce a novel\nway of processing audio signals by treating them as a collection of points in\nfeature space, and we use point cloud machine learning models that give us\ninvariance to the choice of representation parameters, such as DFT size or the\nsampling rate. Additionally, we observe that these methods result in smaller\nmodels, and allow us to significantly subsample the input representation with\nminimal effects to a trained model performance.",
          "link": "http://arxiv.org/abs/2105.02469",
          "publishedOn": "2021-07-30T02:13:30.177Z",
          "wordCount": 627,
          "title": "Point Cloud Audio Processing. (arXiv:2105.02469v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13822",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Esche_E/0/1/0/all/0/1\">Erik Esche</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Talis_T/0/1/0/all/0/1\">Torben Talis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weigert_J/0/1/0/all/0/1\">Joris Weigert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brand_Rihm_G/0/1/0/all/0/1\">Gerardo Brand-Rihm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+You_B/0/1/0/all/0/1\">Byungjun You</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_C/0/1/0/all/0/1\">Christian Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Repke_J/0/1/0/all/0/1\">Jens-Uwe Repke</a>",
          "description": "Continuously operated (bio-)chemical processes increasingly suffer from\nexternal disturbances, such as feed fluctuations or changes in market\nconditions. Product quality often hinges on control of rarely measured\nconcentrations, which are expensive to measure. Semi-supervised regression is a\npossible building block and method from machine learning to construct\nsoft-sensors for such infrequently measured states. Using two case studies,\ni.e., the Williams-Otto process and a bioethanol production process,\nsemi-supervised regression is compared against standard regression to evaluate\nits merits and its possible scope of application for process control in the\n(bio-)chemical industry.",
          "link": "http://arxiv.org/abs/2107.13822",
          "publishedOn": "2021-07-30T02:13:30.166Z",
          "wordCount": 545,
          "title": "Semi-supervised Learning for Data-driven Soft-sensing of Biological and Chemical Processes. (arXiv:2107.13822v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valassakis_E/0/1/0/all/0/1\">Eugene Valassakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palo_N/0/1/0/all/0/1\">Norman Di Palo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1\">Edward Johns</a>",
          "description": "In this paper, we study the problem of zero-shot sim-to-real when the task\nrequires both highly precise control with sub-millimetre error tolerance, and\nwide task space generalisation. Our framework involves a coarse-to-fine\ncontroller, where trajectories begin with classical motion planning using\nICP-based pose estimation, and transition to a learned end-to-end controller\nwhich maps images to actions and is trained in simulation with domain\nrandomisation. In this way, we achieve precise control whilst also generalising\nthe controller across wide task spaces, and keeping the robustness of\nvision-based, end-to-end control. Real-world experiments on a range of\ndifferent tasks show that, by exploiting the best of both worlds, our framework\nsignificantly outperforms purely motion planning methods, and purely\nlearning-based methods. Furthermore, we answer a range of questions on best\npractices for precise sim-to-real transfer, such as how different image sensor\nmodalities and image feature representations perform.",
          "link": "http://arxiv.org/abs/2105.11283",
          "publishedOn": "2021-07-30T02:13:30.159Z",
          "wordCount": 626,
          "title": "Coarse-to-Fine for Sim-to-Real: Sub-Millimetre Precision Across Wide Task Spaces. (arXiv:2105.11283v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huan_J/0/1/0/all/0/1\">Jun Huan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>",
          "description": "While deep learning succeeds in a wide range of tasks, it highly depends on\nthe massive collection of annotated data which is expensive and time-consuming.\nTo lower the cost of data annotation, active learning has been proposed to\ninteractively query an oracle to annotate a small proportion of informative\nsamples in an unlabeled dataset. Inspired by the fact that the samples with\nhigher loss are usually more informative to the model than the samples with\nlower loss, in this paper we present a novel deep active learning approach that\nqueries the oracle for data annotation when the unlabeled sample is believed to\nincorporate high loss. The core of our approach is a measurement Temporal\nOutput Discrepancy (TOD) that estimates the sample loss by evaluating the\ndiscrepancy of outputs given by models at different optimization steps. Our\ntheoretical investigation shows that TOD lower-bounds the accumulated sample\nloss thus it can be used to select informative unlabeled samples. On basis of\nTOD, we further develop an effective unlabeled data sampling strategy as well\nas an unsupervised learning criterion that enhances model performance by\nincorporating the unlabeled data. Due to the simplicity of TOD, our active\nlearning approach is efficient, flexible, and task-agnostic. Extensive\nexperimental results demonstrate that our approach achieves superior\nperformances than the state-of-the-art active learning methods on image\nclassification and semantic segmentation tasks.",
          "link": "http://arxiv.org/abs/2107.14153",
          "publishedOn": "2021-07-30T02:13:30.124Z",
          "wordCount": 673,
          "title": "Semi-Supervised Active Learning with Temporal Output Discrepancy. (arXiv:2107.14153v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.08773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_E/0/1/0/all/0/1\">Eddie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>",
          "description": "Data augmentations are important ingredients in the recipe for training\nrobust neural networks, especially in computer vision. A fundamental question\nis whether neural network features encode data augmentation transformations. To\nanswer this question, we introduce a systematic approach to investigate which\nlayers of neural networks are the most predictive of augmentation\ntransformations. Our approach uses features in pre-trained vision models with\nminimal additional processing to predict common properties transformed by\naugmentation (scale, aspect ratio, hue, saturation, contrast, and brightness).\nSurprisingly, neural network features not only predict data augmentation\ntransformations, but they predict many transformations with high accuracy.\nAfter validating that neural networks encode features corresponding to\naugmentation transformations, we show that these features are encoded in the\nearly layers of modern CNNs, though the augmentation signal fades in deeper\nlayers.",
          "link": "http://arxiv.org/abs/2003.08773",
          "publishedOn": "2021-07-30T02:13:30.095Z",
          "wordCount": 601,
          "title": "Do CNNs Encode Data Augmentations?. (arXiv:2003.08773v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oueida_S/0/1/0/all/0/1\">Soraia Oueida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_S/0/1/0/all/0/1\">Soaad Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotb_Y/0/1/0/all/0/1\">Yehia Kotb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Syed Ishtiaque Ahmed</a>",
          "description": "This paper presents a new approach to prevent transportation accidents and\nmonitor driver's behavior using a healthcare AI system that incorporates\nfairness and ethics. Dangerous medical cases and unusual behavior of the driver\nare detected. Fairness algorithm is approached in order to improve\ndecision-making and address ethical issues such as privacy issues, and to\nconsider challenges that appear in the wild within AI in healthcare and\ndriving. A healthcare professional will be alerted about any unusual activity,\nand the driver's location when necessary, is provided in order to enable the\nhealthcare professional to immediately help to the unstable driver. Therefore,\nusing the healthcare AI system allows for accidents to be predicted and thus\nprevented and lives may be saved based on the built-in AI system inside the\nvehicle which interacts with the ER system.",
          "link": "http://arxiv.org/abs/2107.14077",
          "publishedOn": "2021-07-30T02:13:30.090Z",
          "wordCount": 618,
          "title": "A Fair and Ethical Healthcare Artificial Intelligence System for Monitoring Driver Behavior and Preventing Road Accidents. (arXiv:2107.14077v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daghaghi_S/0/1/0/all/0/1\">Shabnam Daghaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medini_T/0/1/0/all/0/1\">Tharun Medini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meisburger_N/0/1/0/all/0/1\">Nicholas Meisburger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Beidi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengnan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Anshumali Shrivastava</a>",
          "description": "Softmax classifiers with a very large number of classes naturally occur in\nmany applications such as natural language processing and information\nretrieval. The calculation of full softmax is costly from the computational and\nenergy perspective. There have been various sampling approaches to overcome\nthis challenge, popularly known as negative sampling (NS). Ideally, NS should\nsample negative classes from a distribution that is dependent on the input\ndata, the current parameters, and the correct positive class. Unfortunately,\ndue to the dynamically updated parameters and data samples, there is no\nsampling scheme that is provably adaptive and samples the negative classes\nefficiently. Therefore, alternative heuristics like random sampling, static\nfrequency-based sampling, or learning-based biased sampling, which primarily\ntrade either the sampling cost or the adaptivity of samples per iteration are\nadopted. In this paper, we show two classes of distributions where the sampling\nscheme is truly adaptive and provably generates negative samples in\nnear-constant time. Our implementation in C++ on CPU is significantly superior,\nboth in terms of wall-clock time and accuracy, compared to the most optimized\nTensorFlow implementations of other popular negative sampling approaches on\npowerful NVIDIA V100 GPU.",
          "link": "http://arxiv.org/abs/2012.15843",
          "publishedOn": "2021-07-30T02:13:30.077Z",
          "wordCount": 674,
          "title": "A Tale of Two Efficient and Informative Negative Sampling Distributions. (arXiv:2012.15843v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13833",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Noort_F/0/1/0/all/0/1\">Frieda van den Noort</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sirmacek_B/0/1/0/all/0/1\">Beril Sirmacek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Slump_C/0/1/0/all/0/1\">Cornelis H. Slump</a>",
          "description": "The prevalance of pelvic floor problems is high within the female population.\nTransperineal ultrasound (TPUS) is the main imaging modality used to\ninvestigate these problems. Automating the analysis of TPUS data will help in\ngrowing our understanding of pelvic floor related problems. In this study we\npresent a U-net like neural network with some convolutional long short term\nmemory (CLSTM) layers to automate the 3D segmentation of the levator ani muscle\n(LAM) in TPUS volumes. The CLSTM layers are added to preserve the inter-slice\n3D information. We reach human level performance on this segmentation task.\nTherefore, we conclude that we successfully automated the segmentation of the\nLAM on 3D TPUS data. This paves the way towards automatic in-vivo analysis of\nthe LAM mechanics in the context of large study populations.",
          "link": "http://arxiv.org/abs/2107.13833",
          "publishedOn": "2021-07-30T02:13:30.062Z",
          "wordCount": 587,
          "title": "Recurrent U-net for automatic pelvic floor muscle segmentation on 3D ultrasound. (arXiv:2107.13833v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.05261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1\">Fay&#xe7;al Ait Aoudia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1\">Jakob Hoydis</a>",
          "description": "Previous studies have demonstrated that end-to-end learning enables\nsignificant shaping gains over additive white Gaussian noise (AWGN) channels.\nHowever, its benefits have not yet been quantified over realistic wireless\nchannel models. This work aims to fill this gap by exploring the gains of\nend-to-end learning over a frequency- and time-selective fading channel using\northogonal frequency division multiplexing (OFDM). With imperfect channel\nknowledge at the receiver, the shaping gains observed on AWGN channels vanish.\nNonetheless, we identify two other sources of performance improvements. The\nfirst comes from a neural network (NN)-based receiver operating over a large\nnumber of subcarriers and OFDM symbols which allows to significantly reduce the\nnumber of orthogonal pilots without loss of bit error rate (BER). The second\ncomes from entirely eliminating orthognal pilots by jointly learning a neural\nreceiver together with either superimposed pilots (SIPs), linearly combined\nwith conventional quadrature amplitude modulation (QAM), or an optimized\nconstellation geometry. The learned geometry works for a wide range of\nsignal-to-noise ratios (SNRs), Doppler and delay spreads, has zero mean and\ndoes hence not contain any form of superimposed pilots. Both schemes achieve\nthe same BER as the pilot-based baseline with around 7% higher throughput.\nThus, we believe that a jointly learned transmitter and receiver are a very\ninteresting component for beyond-5G communication systems which could remove\nthe need and associated control overhead for demodulation reference signals\n(DMRSs).",
          "link": "http://arxiv.org/abs/2009.05261",
          "publishedOn": "2021-07-30T02:13:30.056Z",
          "wordCount": 708,
          "title": "End-to-end Learning for OFDM: From Neural Receivers to Pilotless Communication. (arXiv:2009.05261v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1\">Peter Kairouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinke_T/0/1/0/all/0/1\">Thomas Steinke</a>",
          "description": "We consider training models on private data that are distributed across user\ndevices. To ensure privacy, we add on-device noise and use secure aggregation\nso that only the noisy sum is revealed to the server. We present a\ncomprehensive end-to-end system, which appropriately discretizes the data and\nadds discrete Gaussian noise before performing secure aggregation. We provide a\nnovel privacy analysis for sums of discrete Gaussians and carefully analyze the\neffects of data quantization and modular summation arithmetic. Our theoretical\nguarantees highlight the complex tension between communication, privacy, and\naccuracy. Our extensive experimental results demonstrate that our solution is\nessentially able to match the accuracy to central differential privacy with\nless than 16 bits of precision per value.",
          "link": "http://arxiv.org/abs/2102.06387",
          "publishedOn": "2021-07-30T02:13:30.006Z",
          "wordCount": 604,
          "title": "The Distributed Discrete Gaussian Mechanism for Federated Learning with Secure Aggregation. (arXiv:2102.06387v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Langosco_L/0/1/0/all/0/1\">Lauro Langosco di Langosco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strathmann_H/0/1/0/all/0/1\">Heiko Strathmann</a>",
          "description": "Particle-based approximate Bayesian inference approaches such as Stein\nVariational Gradient Descent (SVGD) combine the flexibility and convergence\nguarantees of sampling methods with the computational benefits of variational\ninference. In practice, SVGD relies on the choice of an appropriate kernel\nfunction, which impacts its ability to model the target distribution -- a\nchallenging problem with only heuristic solutions. We propose Neural\nVariational Gradient Descent (NVGD), which is based on parameterizing the\nwitness function of the Stein discrepancy by a deep neural network whose\nparameters are learned in parallel to the inference, mitigating the necessity\nto make any kernel choices whatsoever. We empirically evaluate our method on\npopular synthetic inference problems, real-world Bayesian linear regression,\nand Bayesian neural network inference.",
          "link": "http://arxiv.org/abs/2107.10731",
          "publishedOn": "2021-07-30T02:13:29.987Z",
          "wordCount": 567,
          "title": "Neural Variational Gradient Descent. (arXiv:2107.10731v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weichert_D/0/1/0/all/0/1\">Dorina Weichert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kister_A/0/1/0/all/0/1\">Alexander Kister</a>",
          "description": "A solution that is only reliable under favourable conditions is hardly a safe\nsolution. Min Max Optimization is an approach that returns optima that are\nrobust against worst case conditions. We propose algorithms that perform Min\nMax Optimization in a setting where the function that should be optimized is\nnot known a priori and hence has to be learned by experiments. Therefore we\nextend the Bayesian Optimization setting, which is tailored to maximization\nproblems, to Min Max Optimization problems. While related work extends the two\nacquisition functions Expected Improvement and Gaussian Process Upper\nConfidence Bound; we extend the two acquisition functions Entropy Search and\nKnowledge Gradient. These acquisition functions are able to gain knowledge\nabout the optimum instead of just looking for points that are supposed to be\noptimal. In our evaluation we show that these acquisition functions allow for\nbetter solutions - converging faster to the optimum than the benchmark\nsettings.",
          "link": "http://arxiv.org/abs/2107.13772",
          "publishedOn": "2021-07-30T02:13:29.981Z",
          "wordCount": 586,
          "title": "Bayesian Optimization for Min Max Optimization. (arXiv:2107.13772v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07832",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaini_P/0/1/0/all/0/1\">Priyank Jaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holdijk_L/0/1/0/all/0/1\">Lars Holdijk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1\">Max Welling</a>",
          "description": "We focus on the problem of efficient sampling and learning of probability\ndensities by incorporating symmetries in probabilistic models. We first\nintroduce Equivariant Stein Variational Gradient Descent algorithm -- an\nequivariant sampling method based on Stein's identity for sampling from\ndensities with symmetries. Equivariant SVGD explicitly incorporates symmetry\ninformation in a density through equivariant kernels which makes the resultant\nsampler efficient both in terms of sample complexity and the quality of\ngenerated samples. Subsequently, we define equivariant energy based models to\nmodel invariant densities that are learned using contrastive divergence. By\nutilizing our equivariant SVGD for training equivariant EBMs, we propose new\nways of improving and scaling up training of energy based models. We apply\nthese equivariant energy models for modelling joint densities in regression and\nclassification tasks for image datasets, many-body particle systems and\nmolecular structure generation.",
          "link": "http://arxiv.org/abs/2106.07832",
          "publishedOn": "2021-07-30T02:13:29.922Z",
          "wordCount": 608,
          "title": "Learning Equivariant Energy Based Models with Equivariant Stein Variational Gradient Descent. (arXiv:2106.07832v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1\">Aditya Jyoti Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Smaranjit Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1\">Kanishka Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nethaji_N/0/1/0/all/0/1\">Niketha Nethaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Shivam Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purkayastha_A/0/1/0/all/0/1\">Arnab Dutta Purkayastha</a>",
          "description": "Tourism in India plays a quintessential role in the country's economy with an\nestimated 9.2% GDP share for the year 2018. With a yearly growth rate of 6.2%,\nthe industry holds a huge potential for being the primary driver of the economy\nas observed in the nations of the Middle East like the United Arab Emirates.\nThe historical and cultural diversity exhibited throughout the geography of the\nnation is a unique spectacle for people around the world and therefore serves\nto attract tourists in tens of millions in number every year. Traditionally,\ntour guides or academic professionals who study these heritage monuments were\nresponsible for providing information to the visitors regarding their\narchitectural and historical significance. However, unfortunately this system\nhas several caveats when considered on a large scale such as unavailability of\nsufficient trained people, lack of accurate information, failure to convey the\nrichness of details in an attractive format etc. Recently, machine learning\napproaches revolving around the usage of monument pictures have been shown to\nbe useful for rudimentary analysis of heritage sights. This paper serves as a\nsurvey of the research endeavors undertaken in this direction which would\neventually provide insights for building an automated decision system that\ncould be utilized to make the experience of tourism in India more modernized\nfor visitors.",
          "link": "http://arxiv.org/abs/2107.14070",
          "publishedOn": "2021-07-30T02:13:29.905Z",
          "wordCount": 690,
          "title": "Machine Learning Advances aiding Recognition and Classification of Indian Monuments and Landmarks. (arXiv:2107.14070v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scheinert_D/0/1/0/all/0/1\">Dominik Scheinert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thamsen_L/0/1/0/all/0/1\">Lauritz Thamsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Houkun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Will_J/0/1/0/all/0/1\">Jonathan Will</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acker_A/0/1/0/all/0/1\">Alexander Acker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wittkopp_T/0/1/0/all/0/1\">Thorsten Wittkopp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_O/0/1/0/all/0/1\">Odej Kao</a>",
          "description": "Distributed dataflow systems enable the use of clusters for scalable data\nanalytics. However, selecting appropriate cluster resources for a processing\njob is often not straightforward. Performance models trained on historical\nexecutions of a concrete job are helpful in such situations, yet they are\nusually bound to a specific job execution context (e.g. node type, software\nversions, job parameters) due to the few considered input parameters. Even in\ncase of slight context changes, such supportive models need to be retrained and\ncannot benefit from historical execution data from related contexts.\n\nThis paper presents Bellamy, a novel modeling approach that combines\nscale-outs, dataset sizes, and runtimes with additional descriptive properties\nof a dataflow job. It is thereby able to capture the context of a job\nexecution. Moreover, Bellamy is realizing a two-step modeling approach. First,\na general model is trained on all the available data for a specific scalable\nanalytics algorithm, hereby incorporating data from different contexts.\nSubsequently, the general model is optimized for the specific situation at\nhand, based on the available data for the concrete context. We evaluate our\napproach on two publicly available datasets consisting of execution data from\nvarious dataflow jobs carried out in different environments, showing that\nBellamy outperforms state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.13921",
          "publishedOn": "2021-07-30T02:13:29.887Z",
          "wordCount": 661,
          "title": "Bellamy: Reusing Performance Models for Distributed Dataflow Jobs Across Contexts. (arXiv:2107.13921v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2006.09858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tabaghi_P/0/1/0/all/0/1\">Puoya Tabaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jianhao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1\">Olgica Milenkovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokmanic_I/0/1/0/all/0/1\">Ivan Dokmani&#x107;</a>",
          "description": "Many data analysis problems can be cast as distance geometry problems in\n\\emph{space forms} -- Euclidean, spherical, or hyperbolic spaces. Often,\nabsolute distance measurements are often unreliable or simply unavailable and\nonly proxies to absolute distances in the form of similarities are available.\nHence we ask the following: Given only \\emph{comparisons} of similarities\namongst a set of entities, what can be said about the geometry of the\nunderlying space form? To study this question, we introduce the notions of the\n\\textit{ordinal capacity} of a target space form and \\emph{ordinal spread} of\nthe similarity measurements. The latter is an indicator of complex patterns in\nthe measurements, while the former quantifies the capacity of a space form to\naccommodate a set of measurements with a specific ordinal spread profile. We\nprove that the ordinal capacity of a space form is related to its dimension and\nthe sign of its curvature. This leads to a lower bound on the Euclidean and\nspherical embedding dimension of what we term similarity graphs. More\nimportantly, we show that the statistical behavior of the ordinal spread random\nvariables defined on a similarity graph can be used to identify its underlying\nspace form. We support our theoretical claims with experiments on weighted\ntrees, single-cell RNA expression data and spherical cartographic measurements.",
          "link": "http://arxiv.org/abs/2006.09858",
          "publishedOn": "2021-07-30T02:13:29.875Z",
          "wordCount": 690,
          "title": "Geometry of Similarity Comparisons. (arXiv:2006.09858v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_i/0/1/0/all/0/1\">ing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yiyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Huaxiong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaoshuang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shixin Xu</a>",
          "description": "Stroke is the top leading causes of death in China (Zhou et al. The Lancet\n2019). A dataset from Shanxi Province is used to identify the risk of each\npatient's at four states low/medium/high/attack and provide the state\ntransition tendency through a SHAP DeepExplainer. To improve the accuracy on an\nimbalance sample set, the Quadratic Interactive Deep Neural Network (QIDNN)\nmodel is first proposed by flexible selecting and appending of quadratic\ninteractive features. The experimental results showed that the QIDNN model with\n7 interactive features achieve the state-of-art accuracy $83.25\\%$. Blood\npressure, physical inactivity, smoking, weight and total cholesterol are the\ntop five important features. Then, for the sake of high recall on the most\nurgent state, attack state, the stroke occurrence prediction is taken as an\nauxiliary objective to benefit from multi-objective optimization. The\nprediction accuracy was promoted, meanwhile the recall of the attack state was\nimproved by $24.9\\%$ (to $84.83\\%$) compared to QIDNN (from $67.93\\%$) with\nsame features. The prediction model and analysis tool in this paper not only\ngave the theoretical optimized prediction method, but also provided the\nattribution explanation of risk states and transition direction of each\npatient, which provided a favorable tool for doctors to analyze and diagnose\nthe disease.",
          "link": "http://arxiv.org/abs/2107.14060",
          "publishedOn": "2021-07-30T02:13:29.869Z",
          "wordCount": 651,
          "title": "Multi-objective optimization and explanation for stroke risk assessment in Shanxi province. (arXiv:2107.14060v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farhat_H/0/1/0/all/0/1\">Hikmat Farhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rammouz_V/0/1/0/all/0/1\">Veronica Rammouz</a>",
          "description": "With the rapid growth of the number of devices on the Internet, malware poses\na threat not only to the affected devices but also their ability to use said\ndevices to launch attacks on the Internet ecosystem. Rapid malware\nclassification is an important tools to combat that threat. One of the\nsuccessful approaches to classification is based on malware images and deep\nlearning. While many deep learning architectures are very accurate they usually\ntake a long time to train. In this work we perform experiments on multiple well\nknown, pre-trained, deep network architectures in the context of transfer\nlearning. We show that almost all them classify malware accurately with a very\nshort training period.",
          "link": "http://arxiv.org/abs/2107.13743",
          "publishedOn": "2021-07-30T02:13:29.853Z",
          "wordCount": 543,
          "title": "Malware Classification Using Transfer Learning. (arXiv:2107.13743v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13735",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1\">Yubin Lu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maulik_R/0/1/0/all/0/1\">Romit Maulik</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gao_T/0/1/0/all/0/1\">Ting Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dietrich_F/0/1/0/all/0/1\">Felix Dietrich</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kevrekidis_I/0/1/0/all/0/1\">Ioannis G. Kevrekidis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Duan_J/0/1/0/all/0/1\">Jinqiao Duan</a>",
          "description": "In this work, we propose a method to learn probability distributions using\nsample path data from stochastic differential equations. Specifically, we\nconsider temporally evolving probability distributions (e.g., those produced by\nintegrating local or nonlocal Fokker-Planck equations). We analyze this\nevolution through machine learning assisted construction of a time-dependent\nmapping that takes a reference distribution (say, a Gaussian) to each and every\ninstance of our evolving distribution. If the reference distribution is the\ninitial condition of a Fokker-Planck equation, what we learn is the time-T map\nof the corresponding solution. Specifically, the learned map is a normalizing\nflow that deforms the support of the reference density to the support of each\nand every density snapshot in time. We demonstrate that this approach can learn\nsolutions to non-local Fokker-Planck equations, such as those arising in\nsystems driven by both Brownian and L\\'evy noise. We present examples with two-\nand three-dimensional, uni- and multimodal distributions to validate the\nmethod.",
          "link": "http://arxiv.org/abs/2107.13735",
          "publishedOn": "2021-07-30T02:13:29.848Z",
          "wordCount": 607,
          "title": "Learning the temporal evolution of multivariate densities via normalizing flows. (arXiv:2107.13735v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banna_V/0/1/0/all/0/1\">Vishnu Banna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinnakotla_A/0/1/0/all/0/1\">Akhil Chinnakotla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhengxin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vegesana_A/0/1/0/all/0/1\">Anirudh Vegesana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vivek_N/0/1/0/all/0/1\">Naveen Vivek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnappa_K/0/1/0/all/0/1\">Kruthi Krishnappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yung-Hsiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiruvathukal_G/0/1/0/all/0/1\">George K. Thiruvathukal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">James C. Davis</a>",
          "description": "Machine learning techniques are becoming a fundamental tool for scientific\nand engineering progress. These techniques are applied in contexts as diverse\nas astronomy and spam filtering. However, correctly applying these techniques\nrequires careful engineering. Much attention has been paid to the technical\npotential; relatively little attention has been paid to the software\nengineering process required to bring research-based machine learning\ntechniques into practical utility. Technology companies have supported the\nengineering community through machine learning frameworks such as TensorFLow\nand PyTorch, but the details of how to engineer complex machine learning models\nin these frameworks have remained hidden.\n\nTo promote best practices within the engineering community, academic\ninstitutions and Google have partnered to launch a Special Interest Group on\nMachine Learning Models (SIGMODELS) whose goal is to develop exemplary\nimplementations of prominent machine learning models in community locations\nsuch as the TensorFlow Model Garden (TFMG). The purpose of this report is to\ndefine a process for reproducing a state-of-the-art machine learning model at a\nlevel of quality suitable for inclusion in the TFMG. We define the engineering\nprocess and elaborate on each step, from paper analysis to model release. We\nreport on our experiences implementing the YOLO model family with a team of 26\nstudent researchers, share the tools we developed, and describe the lessons we\nlearned along the way.",
          "link": "http://arxiv.org/abs/2107.00821",
          "publishedOn": "2021-07-30T02:13:29.842Z",
          "wordCount": 705,
          "title": "An Experience Report on Machine Learning Reproducibility: Guidance for Practitioners and TensorFlow Model Garden Contributors. (arXiv:2107.00821v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.14119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1\">Emanuel Ben-Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1\">Tal Ridnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_N/0/1/0/all/0/1\">Nadav Zamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_I/0/1/0/all/0/1\">Itamar Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protter_M/0/1/0/all/0/1\">Matan Protter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1\">Lihi Zelnik-Manor</a>",
          "description": "In a typical multi-label setting, a picture contains on average few positive\nlabels, and many negative ones. This positive-negative imbalance dominates the\noptimization process, and can lead to under-emphasizing gradients from positive\nlabels during training, resulting in poor accuracy. In this paper, we introduce\na novel asymmetric loss (\"ASL\"), which operates differently on positive and\nnegative samples. The loss enables to dynamically down-weights and\nhard-thresholds easy negative samples, while also discarding possibly\nmislabeled samples. We demonstrate how ASL can balance the probabilities of\ndifferent samples, and how this balancing is translated to better mAP scores.\nWith ASL, we reach state-of-the-art results on multiple popular multi-label\ndatasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate\nASL applicability for other tasks, such as single-label classification and\nobject detection. ASL is effective, easy to implement, and does not increase\nthe training time or complexity.\n\nImplementation is available at: https://github.com/Alibaba-MIIL/ASL.",
          "link": "http://arxiv.org/abs/2009.14119",
          "publishedOn": "2021-07-30T02:13:29.837Z",
          "wordCount": 650,
          "title": "Asymmetric Loss For Multi-Label Classification. (arXiv:2009.14119v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1\">Aditya Jyoti Paul</a>",
          "description": "For over hundreds of millions of years, sea turtles and their ancestors have\nswum in the vast expanses of the ocean. They have undergone a number of\nevolutionary changes, leading to speciation and sub-speciation. However, in the\npast few decades, some of the most notable forces driving the genetic variance\nand population decline have been global warming and anthropogenic impact\nranging from large-scale poaching, collecting turtle eggs for food, besides\ndumping trash including plastic waste into the ocean. This leads to severe\ndetrimental effects in the sea turtle population, driving them to extinction.\nThis research focusses on the forces causing the decline in sea turtle\npopulation, the necessity for the global conservation efforts along with its\nsuccesses and failures, followed by an in-depth analysis of the modern advances\nin detection and recognition of sea turtles, involving Machine Learning and\nComputer Vision systems, aiding the conservation efforts.",
          "link": "http://arxiv.org/abs/2107.14061",
          "publishedOn": "2021-07-30T02:13:29.830Z",
          "wordCount": 610,
          "title": "The Need and Status of Sea Turtle Conservation and Survey of Associated Computer Vision Advances. (arXiv:2107.14061v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1906.01005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jordan_I/0/1/0/all/0/1\">Ian D. Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokol_P/0/1/0/all/0/1\">Piotr Aleksander Sokol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_I/0/1/0/all/0/1\">Il Memming Park</a>",
          "description": "Gated recurrent units (GRUs) are specialized memory elements for building\nrecurrent neural networks. Despite their incredible success on various tasks,\nincluding extracting dynamics underlying neural data, little is understood\nabout the specific dynamics representable in a GRU network. As a result, it is\nboth difficult to know a priori how successful a GRU network will perform on a\ngiven task, and also their capacity to mimic the underlying behavior of their\nbiological counterparts. Using a continuous time analysis, we gain intuition on\nthe inner workings of GRU networks. We restrict our presentation to low\ndimensions, allowing for a comprehensive visualization. We found a surprisingly\nrich repertoire of dynamical features that includes stable limit cycles\n(nonlinear oscillations), multi-stable dynamics with various topologies, and\nhomoclinic bifurcations. At the same time we were unable to train GRU networks\nto produce continuous attractors, which are hypothesized to exist in biological\nneural networks. We contextualize the usefulness of different kinds of observed\ndynamics and support our claims experimentally.",
          "link": "http://arxiv.org/abs/1906.01005",
          "publishedOn": "2021-07-30T02:13:29.816Z",
          "wordCount": 647,
          "title": "Gated recurrent units viewed through the lens of continuous time dynamical systems. (arXiv:1906.01005v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.10848",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1\">Hangjian Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Padilla_O/0/1/0/all/0/1\">Oscar Hernan Madrid Padilla</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_Q/0/1/0/all/0/1\">Qing Zhou</a>",
          "description": "Structural learning of directed acyclic graphs (DAGs) or Bayesian networks\nhas been studied extensively under the assumption that data are independent. We\npropose a new Gaussian DAG model for dependent data which assumes the\nobservations are correlated according to an undirected network. Under this\nmodel, we develop a method to estimate the DAG structure given a topological\nordering of the nodes. The proposed method jointly estimates the Bayesian\nnetwork and the correlations among observations by optimizing a scoring\nfunction based on penalized likelihood. We show that under some mild\nconditions, the proposed method produces consistent estimators after one\niteration. Extensive numerical experiments also demonstrate that by jointly\nestimating the DAG structure and the sample correlation, our method achieves\nmuch higher accuracy in structure learning. When the node ordering is unknown,\nthrough experiments on synthetic and real data, we show that our algorithm can\nbe used to estimate the correlations between samples, with which we can\nde-correlate the dependent data to significantly improve the performance of\nclassical DAG learning methods.",
          "link": "http://arxiv.org/abs/1905.10848",
          "publishedOn": "2021-07-30T02:13:29.810Z",
          "wordCount": 622,
          "title": "Learning Gaussian DAGs from Network Data. (arXiv:1905.10848v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.01656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Disabato_S/0/1/0/all/0/1\">Simone Disabato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roveri_M/0/1/0/all/0/1\">Manuel Roveri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alippi_C/0/1/0/all/0/1\">Cesare Alippi</a>",
          "description": "Severe constraints on memory and computation characterizing the\nInternet-of-Things (IoT) units may prevent the execution of Deep Learning\n(DL)-based solutions, which typically demand large memory and high processing\nload. In order to support a real-time execution of the considered DL model at\nthe IoT unit level, DL solutions must be designed having in mind constraints on\nmemory and processing capability exposed by the chosen IoT technology. In this\npaper, we introduce a design methodology aiming at allocating the execution of\nConvolutional Neural Networks (CNNs) on a distributed IoT application. Such a\nmethodology is formalized as an optimization problem where the latency between\nthe data-gathering phase and the subsequent decision-making one is minimized,\nwithin the given constraints on memory and processing load at the units level.\nThe methodology supports multiple sources of data as well as multiple CNNs in\nexecution on the same IoT system allowing the design of CNN-based applications\ndemanding autonomy, low decision-latency, and high Quality-of-Service.",
          "link": "http://arxiv.org/abs/1908.01656",
          "publishedOn": "2021-07-30T02:13:29.804Z",
          "wordCount": 643,
          "title": "Distributed Deep Convolutional Neural Networks for the Internet-of-Things. (arXiv:1908.01656v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.10692",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Harris_K/0/1/0/all/0/1\">Kameron Decker Harris</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1\">Yizhe Zhu</a>",
          "description": "We provide a novel analysis of low-rank tensor completion based on hypergraph\nexpanders. As a proxy for rank, we minimize the max-quasinorm of the tensor,\nwhich generalizes the max-norm for matrices. Our analysis is deterministic and\nshows that the number of samples required to approximately recover an order-$t$\ntensor with at most $n$ entries per dimension is linear in $n$, under the\nassumption that the rank and order of the tensor are $O(1)$. As steps in our\nproof, we find a new expander mixing lemma for a $t$-partite, $t$-uniform\nregular hypergraph model, and prove several new properties about tensor\nmax-quasinorm. To the best of our knowledge, this is the first deterministic\nanalysis of tensor completion. We develop a practical algorithm that solves a\nrelaxed version of the max-quasinorm minimization problem, and we demonstrate\nits efficacy with numerical experiments.",
          "link": "http://arxiv.org/abs/1910.10692",
          "publishedOn": "2021-07-30T02:13:29.798Z",
          "wordCount": 626,
          "title": "Deterministic tensor completion with hypergraph expanders. (arXiv:1910.10692v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuen_J/0/1/0/all/0/1\">Jason Kuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "We introduce a new image segmentation task, termed Entity Segmentation (ES)\nwith the aim to segment all visual entities in an image without considering\nsemantic category labels. It has many practical applications in image\nmanipulation/editing where the segmentation mask quality is typically crucial\nbut category labels are less important. In this setting, all\nsemantically-meaningful segments are equally treated as categoryless entities\nand there is no thing-stuff distinction. Based on our unified entity\nrepresentation, we propose a center-based entity segmentation framework with\ntwo novel modules to improve mask quality. Experimentally, both our new task\nand framework demonstrate superior advantages as against existing work. In\nparticular, ES enables the following: (1) merging multiple datasets to form a\nlarge training set without the need to resolve label conflicts; (2) any model\ntrained on one dataset can generalize exceptionally well to other datasets with\nunseen domains. Our code is made publicly available at\nhttps://github.com/dvlab-research/Entity.",
          "link": "http://arxiv.org/abs/2107.14228",
          "publishedOn": "2021-07-30T02:13:29.792Z",
          "wordCount": 595,
          "title": "Open-World Entity Segmentation. (arXiv:2107.14228v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raman_C/0/1/0/all/0/1\">Chirag Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_H/0/1/0/all/0/1\">Hayley Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loog_M/0/1/0/all/0/1\">Marco Loog</a>",
          "description": "The default paradigm for the forecasting of human behavior in social\nconversations is characterized by top-down approaches. These involve\nidentifying predictive relationships between low level nonverbal cues and\nfuture semantic events of interest (e.g. turn changes, group leaving). A common\nhurdle however, is the limited availability of labeled data for supervised\nlearning. In this work, we take the first step in the direction of a bottom-up\nself-supervised approach in the domain. We formulate the task of Social Cue\nForecasting to leverage the larger amount of unlabeled low-level behavior cues,\nand characterize the modeling challenges involved. To address these, we take a\nmeta-learning approach and propose the Social Process (SP) models--socially\naware sequence-to-sequence (Seq2Seq) models within the Neural Process (NP)\nfamily. SP models learn extractable representations of non-semantic future cues\nfor each participant, while capturing global uncertainty by jointly reasoning\nabout the future for all members of the group. Evaluation on synthesized and\nreal-world behavior data shows that our SP models achieve higher log-likelihood\nthan the NP baselines, and also highlights important considerations for\napplying such techniques within the domain of social human interactions.",
          "link": "http://arxiv.org/abs/2107.13576",
          "publishedOn": "2021-07-30T02:13:29.711Z",
          "wordCount": 628,
          "title": "Social Processes: Self-Supervised Forecasting of Nonverbal Cues in Social Conversations. (arXiv:2107.13576v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13721",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengwu Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Saparbayeva_B/0/1/0/all/0/1\">Bayan Saparbayeva</a>",
          "description": "Mainfold-valued functional data analysis (FDA) recently becomes an active\narea of research motivated by the raising availability of trajectories or\nlongitudinal data observed on non-linear manifolds. The challenges of analyzing\nsuch data comes from many aspects, including infinite dimensionality and\nnonlinearity, as well as time domain or phase variability. In this paper, we\nstudy the amplitude part of manifold-valued functions on $\\S^2$, which is\ninvariant to random time warping or re-parameterization of the function.\nUtilizing the nice geometry of $\\S^2$, we develop a set of efficient and\naccurate tools for temporal alignment of functions, geodesic and sample mean\ncalculation. At the heart of these tools, they rely on gradient descent\nalgorithms with carefully derived gradients. We show the advantages of these\nnewly developed tools over its competitors with extensive simulations and real\ndata, and demonstrate the importance of considering the amplitude part of\nfunctions instead of mixing it with phase variability in mainfold-valued FDA.",
          "link": "http://arxiv.org/abs/2107.13721",
          "publishedOn": "2021-07-30T02:13:29.706Z",
          "wordCount": 590,
          "title": "Amplitude Mean of Functional Data on $\\mathbb{S}^2$. (arXiv:2107.13721v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1\">William Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1\">Philippe Burlina</a>",
          "description": "When deploying artificial intelligence (AI) in the real world, being able to\ntrust the operation of the AI by characterizing how it performs is an\never-present and important topic. An important and still largely unexplored\ntask in this characterization is determining major factors within the real\nworld that affect the AI's behavior, such as weather conditions or lighting,\nand either a) being able to give justification for why it may have failed or b)\neliminating the influence the factor has. Determining these sensitive factors\nheavily relies on collected data that is diverse enough to cover numerous\ncombinations of these factors, which becomes more onerous when having many\npotential sensitive factors or operating in complex environments. This paper\ninvestigates methods that discover and separate out individual semantic\nsensitive factors from a given dataset to conduct this characterization as well\nas addressing mitigation of these factors' sensitivity. We also broaden\nremediation of fairness, which normally only addresses socially relevant\nfactors, and widen it to deal with the desensitization of AI with regard to all\npossible aspects of variation in the domain. The proposed methods which\ndiscover these major factors reduce the potentially onerous demands of\ncollecting a sufficiently diverse dataset. In experiments using the road sign\n(GTSRB) and facial imagery (CelebA) datasets, we show the promise of using this\nscheme to perform this characterization and remediation and demonstrate that\nour approach outperforms state of the art approaches.",
          "link": "http://arxiv.org/abs/2107.13625",
          "publishedOn": "2021-07-30T02:13:29.700Z",
          "wordCount": 674,
          "title": "Generalizing Fairness: Discovery and Mitigation of Unknown Sensitive Attributes. (arXiv:2107.13625v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elwood_A/0/1/0/all/0/1\">Adam Elwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasparin_A/0/1/0/all/0/1\">Alberto Gasparin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozza_A/0/1/0/all/0/1\">Alessandro Rozza</a>",
          "description": "With the rise in use of social media to promote branded products, the demand\nfor effective influencer marketing has increased. Brands are looking for\nimproved ways to identify valuable influencers among a vast catalogue; this is\neven more challenging with \"micro-influencers\", which are more affordable than\nmainstream ones but difficult to discover. In this paper, we propose a novel\nmulti-task learning framework to improve the state of the art in\nmicro-influencer ranking based on multimedia content. Moreover, since the\nvisual congruence between a brand and influencer has been shown to be good\nmeasure of compatibility, we provide an effective visual method for\ninterpreting our models' decisions, which can also be used to inform brands'\nmedia strategies. We compare with the current state-of-the-art on a recently\nconstructed public dataset and we show significant improvement both in terms of\naccuracy and model complexity. The techniques for ranking and interpretation\npresented in this work can be generalised to arbitrary multimedia ranking tasks\nthat have datasets with a similar structure.",
          "link": "http://arxiv.org/abs/2107.13943",
          "publishedOn": "2021-07-30T02:13:29.659Z",
          "wordCount": 599,
          "title": "Ranking Micro-Influencers: a Novel Multi-Task Learning and Interpretable Framework. (arXiv:2107.13943v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13841",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Goetz_A/0/1/0/all/0/1\">Aur&#xe8;le Goetz</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Durmaz_A/0/1/0/all/0/1\">Ali Riza Durmaz</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Muller_M/0/1/0/all/0/1\">Martin M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Thomas_A/0/1/0/all/0/1\">Akhil Thomas</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Britz_D/0/1/0/all/0/1\">Dominik Britz</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kerfriden_P/0/1/0/all/0/1\">Pierre Kerfriden</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Eberl_C/0/1/0/all/0/1\">Chris Eberl</a>",
          "description": "Materials' microstructures are signatures of their alloying composition and\nprocessing history. Therefore, microstructures exist in a wide variety. As\nmaterials become increasingly complex to comply with engineering demands,\nadvanced computer vision (CV) approaches such as deep learning (DL) inevitably\ngain relevance for quantifying microstrucutures' constituents from micrographs.\nWhile DL can outperform classical CV techniques for many tasks, shortcomings\nare poor data efficiency and generalizability across datasets. This is\ninherently in conflict with the expense associated with annotating materials\ndata through experts and extensive materials diversity. To tackle poor domain\ngeneralizability and the lack of labeled data simultaneously, we propose to\napply a sub-class of transfer learning methods called unsupervised domain\nadaptation (UDA). These algorithms address the task of finding domain-invariant\nfeatures when supplied with annotated source data and unannotated target data,\nsuch that performance on the latter distribution is optimized despite the\nabsence of annotations. Exemplarily, this study is conducted on a lath-shaped\nbainite segmentation task in complex phase steel micrographs. Here, the domains\nto bridge are selected to be different metallographic specimen preparations\n(surface etchings) and distinct imaging modalities. We show that a\nstate-of-the-art UDA approach surpasses the na\\\"ive application of source\ndomain trained models on the target domain (generalization baseline) to a large\nextent. This holds true independent of the domain shift, despite using little\ndata, and even when the baseline models were pre-trained or employed data\naugmentation. Through UDA, mIoU was improved over generalization baselines from\n82.2%, 61.0%, 49.7% to 84.7%, 67.3%, 73.3% on three target datasets,\nrespectively. This underlines this techniques' potential to cope with materials\nvariance.",
          "link": "http://arxiv.org/abs/2107.13841",
          "publishedOn": "2021-07-30T02:13:29.652Z",
          "wordCount": 705,
          "title": "Addressing materials' microstructure diversity using transfer learning. (arXiv:2107.13841v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14038",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kashefi_A/0/1/0/all/0/1\">Ali Kashefi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukerji_T/0/1/0/all/0/1\">Tapan Mukerji</a>",
          "description": "We propose a novel deep learning framework for predicting permeability of\nporous media from their digital images. Unlike convolutional neural networks,\ninstead of feeding the whole image volume as inputs to the network, we model\nthe boundary between solid matrix and pore spaces as point clouds and feed them\nas inputs to a neural network based on the PointNet architecture. This approach\novercomes the challenge of memory restriction of graphics processing units and\nits consequences on the choice of batch size, and convergence. Compared to\nconvolutional neural networks, the proposed deep learning methodology provides\nfreedom to select larger batch sizes, due to reducing significantly the size of\nnetwork inputs. Specifically, we use the classification branch of PointNet and\nadjust it for a regression task. As a test case, two and three dimensional\nsynthetic digital rock images are considered. We investigate the effect of\ndifferent components of our neural network on its performance. We compare our\ndeep learning strategy with a convolutional neural network from various\nperspectives, specifically for maximum possible batch size. We inspect the\ngeneralizability of our network by predicting the permeability of real-world\nrock samples as well as synthetic digital rocks that are statistically\ndifferent from the samples used during training. The network predicts the\npermeability of digital rocks a few thousand times faster than a Lattice\nBoltzmann solver with a high level of prediction accuracy.",
          "link": "http://arxiv.org/abs/2107.14038",
          "publishedOn": "2021-07-30T02:13:29.632Z",
          "wordCount": 668,
          "title": "Point-Cloud Deep Learning of Porous Media for Permeability Prediction. (arXiv:2107.14038v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1\">Jiayi Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huayu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1\">Dong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_K/0/1/0/all/0/1\">Kaichao You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duburcq_A/0/1/0/all/0/1\">Alexis Duburcq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "We present Tianshou, a highly modularized python library for deep\nreinforcement learning (DRL) that uses PyTorch as its backend. Tianshou aims to\nprovide building blocks to replicate common RL experiments and has officially\nsupported more than 15 classic algorithms succinctly. To facilitate related\nresearch and prove Tianshou's reliability, we release Tianshou's benchmark of\nMuJoCo environments, covering 9 classic algorithms and 9/13 Mujoco tasks with\nstate-of-the-art performance. We open-sourced Tianshou at\nhttps://github.com/thu-ml/tianshou/, which has received over 3k stars and\nbecome one of the most popular PyTorch-based DRL libraries.",
          "link": "http://arxiv.org/abs/2107.14171",
          "publishedOn": "2021-07-30T02:13:29.614Z",
          "wordCount": 538,
          "title": "Tianshou: a Highly Modularized Deep Reinforcement Learning Library. (arXiv:2107.14171v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zarafshan_P/0/1/0/all/0/1\">Pejman Zarafshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javadi_S/0/1/0/all/0/1\">Saman Javadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roozbahani_A/0/1/0/all/0/1\">Abbas Roozbahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemy_S/0/1/0/all/0/1\">Seyed Mehdi Hashemy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarafshan_P/0/1/0/all/0/1\">Payam Zarafshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etezadi_H/0/1/0/all/0/1\">Hamed Etezadi</a>",
          "description": "Groundwater is the largest storage of freshwater resources, which serves as\nthe major inventory for most of the human consumption through agriculture,\nindustrial, and domestic water supply. In the fields of hydrological, some\nresearchers applied a neural network to forecast rainfall intensity in\nspace-time and introduced the advantages of neural networks compared to\nnumerical models. Then, many researches have been conducted applying\ndata-driven models. Some of them extended an Artificial Neural Networks (ANNs)\nmodel to forecast groundwater level in semi-confined glacial sand and gravel\naquifer under variable state, pumping extraction and climate conditions with\nsignificant accuracy. In this paper, a multi-layer perceptron is applied to\nsimulate groundwater level. The adaptive moment estimation optimization\nalgorithm is also used to this matter. The root mean squared error, mean\nabsolute error, mean squared error and the coefficient of determination ( ) are\nused to evaluate the accuracy of the simulated groundwater level. Total value\nof and RMSE are 0.9458 and 0.7313 respectively which are obtained from the\nmodel output. Results indicate that deep learning algorithms can demonstrate a\nhigh accuracy prediction. Although the optimization of parameters is\ninsignificant in numbers, but due to the value of time in modelling setup, it\nis highly recommended to apply an optimization algorithm in modelling.",
          "link": "http://arxiv.org/abs/2107.13870",
          "publishedOn": "2021-07-30T02:13:29.589Z",
          "wordCount": 653,
          "title": "Artificial Intelligence Hybrid Deep Learning Model for Groundwater Level Prediction Using MLP-ADAM. (arXiv:2107.13870v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeddi_A/0/1/0/all/0/1\">Ashkan B. Jeddi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_N/0/1/0/all/0/1\">Nariman L. Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafieezadeh_A/0/1/0/all/0/1\">Abdollah Shafieezadeh</a>",
          "description": "Reinforcement learning (RL) has shown a promising performance in learning\noptimal policies for a variety of sequential decision-making tasks. However, in\nmany real-world RL problems, besides optimizing the main objectives, the agent\nis expected to satisfy a certain level of safety (e.g., avoiding collisions in\nautonomous driving). While RL problems are commonly formalized as Markov\ndecision processes (MDPs), safety constraints are incorporated via constrained\nMarkov decision processes (CMDPs). Although recent advances in safe RL have\nenabled learning safe policies in CMDPs, these safety requirements should be\nsatisfied during both training and in the deployment process. Furthermore, it\nis shown that in memory-based and partially observable environments, these\nmethods fail to maintain safety over unseen out-of-distribution observations.\nTo address these limitations, we propose a Lyapunov-based uncertainty-aware\nsafe RL model. The introduced model adopts a Lyapunov function that converts\ntrajectory-based constraints to a set of local linear constraints. Furthermore,\nto ensure the safety of the agent in highly uncertain environments, an\nuncertainty quantification method is developed that enables identifying\nrisk-averse actions through estimating the probability of constraint\nviolations. Moreover, a Transformers model is integrated to provide the agent\nwith memory to process long time horizons of information via the self-attention\nmechanism. The proposed model is evaluated in grid-world navigation tasks where\nsafety is defined as avoiding static and dynamic obstacles in fully and\npartially observable environments. The results of these experiments show a\nsignificant improvement in the performance of the agent both in achieving\noptimality and satisfying safety constraints.",
          "link": "http://arxiv.org/abs/2107.13944",
          "publishedOn": "2021-07-30T02:13:29.584Z",
          "wordCount": 691,
          "title": "Lyapunov-based uncertainty-aware safe reinforcement learning. (arXiv:2107.13944v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rambhatla_S/0/1/0/all/0/1\">Sai Saketh Rambhatla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1\">Michael Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>",
          "description": "Boosting is a method for finding a highly accurate hypothesis by linearly\ncombining many ``weak\" hypotheses, each of which may be only moderately\naccurate. Thus, boosting is a method for learning an ensemble of classifiers.\nWhile boosting has been shown to be very effective for decision trees, its\nimpact on neural networks has not been extensively studied. We prove one\nimportant difference between sums of decision trees compared to sums of\nconvolutional neural networks (CNNs) which is that a sum of decision trees\ncannot be represented by a single decision tree with the same number of\nparameters while a sum of CNNs can be represented by a single CNN. Next, using\nstandard object recognition datasets, we verify experimentally the well-known\nresult that a boosted ensemble of decision trees usually generalizes much\nbetter on testing data than a single decision tree with the same number of\nparameters. In contrast, using the same datasets and boosting algorithms, our\nexperiments show the opposite to be true when using neural networks (both CNNs\nand multilayer perceptrons (MLPs)). We find that a single neural network\nusually generalizes better than a boosted ensemble of smaller neural networks\nwith the same total number of parameters.",
          "link": "http://arxiv.org/abs/2107.13600",
          "publishedOn": "2021-07-30T02:13:29.572Z",
          "wordCount": 636,
          "title": "To Boost or not to Boost: On the Limits of Boosted Neural Networks. (arXiv:2107.13600v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahate_A/0/1/0/all/0/1\">Anil Rahate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walambe_R/0/1/0/all/0/1\">Rahee Walambe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanna_S/0/1/0/all/0/1\">Sheela Ramanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotecha_K/0/1/0/all/0/1\">Ketan Kotecha</a>",
          "description": "Multimodal deep learning systems which employ multiple modalities like text,\nimage, audio, video, etc., are showing better performance in comparison with\nindividual modalities (i.e., unimodal) systems. Multimodal machine learning\ninvolves multiple aspects: representation, translation, alignment, fusion, and\nco-learning. In the current state of multimodal machine learning, the\nassumptions are that all modalities are present, aligned, and noiseless during\ntraining and testing time. However, in real-world tasks, typically, it is\nobserved that one or more modalities are missing, noisy, lacking annotated\ndata, have unreliable labels, and are scarce in training or testing and or\nboth. This challenge is addressed by a learning paradigm called multimodal\nco-learning. The modeling of a (resource-poor) modality is aided by exploiting\nknowledge from another (resource-rich) modality using transfer of knowledge\nbetween modalities, including their representations and predictive models.\nCo-learning being an emerging area, there are no dedicated reviews explicitly\nfocusing on all challenges addressed by co-learning. To that end, in this work,\nwe provide a comprehensive survey on the emerging area of multimodal\nco-learning that has not been explored in its entirety yet. We review\nimplementations that overcome one or more co-learning challenges without\nexplicitly considering them as co-learning challenges. We present the\ncomprehensive taxonomy of multimodal co-learning based on the challenges\naddressed by co-learning and associated implementations. The various techniques\nemployed to include the latest ones are reviewed along with some of the\napplications and datasets. Our final goal is to discuss challenges and\nperspectives along with the important ideas and directions for future work that\nwe hope to be beneficial for the entire research community focusing on this\nexciting domain.",
          "link": "http://arxiv.org/abs/2107.13782",
          "publishedOn": "2021-07-30T02:13:29.552Z",
          "wordCount": 713,
          "title": "Multimodal Co-learning: Challenges, Applications with Datasets, Recent Advances and Future Directions. (arXiv:2107.13782v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdary_G/0/1/0/all/0/1\">G Jignesh Chowdary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_S/0/1/0/all/0/1\">Suganya G</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M_P/0/1/0/all/0/1\">Premalatha M</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Y_A/0/1/0/all/0/1\">Asnath Victy Phamila Y</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1\">Karunamurthy K</a>",
          "description": "With the advancements in computer technology, there is a rapid development of\nintelligent systems to understand the complex relationships in data to make\npredictions and classifications. Artificail Intelligence based framework is\nrapidly revolutionizing the healthcare industry. These intelligent systems are\nbuilt with machine learning and deep learning based robust models for early\ndiagnosis of diseases and demonstrates a promising supplementary diagnostic\nmethod for frontline clinical doctors and surgeons. Machine Learning and Deep\nLearning based systems can streamline and simplify the steps involved in\ndiagnosis of diseases from clinical and image-based data, thus providing\nsignificant clinician support and workflow optimization. They mimic human\ncognition and are even capable of diagnosing diseases that cannot be diagnosed\nwith human intelligence. This paper focuses on the survey of machine learning\nand deep learning applications in across 16 medical specialties, namely Dental\nmedicine, Haematology, Surgery, Cardiology, Pulmonology, Orthopedics,\nRadiology, Oncology, General medicine, Psychiatry, Endocrinology, Neurology,\nDermatology, Hepatology, Nephrology, Ophthalmology, and Drug discovery. In this\npaper along with the survey, we discuss the advancements of medical practices\nwith these systems and also the impact of these systems on medical\nprofessionals.",
          "link": "http://arxiv.org/abs/2107.14037",
          "publishedOn": "2021-07-30T02:13:29.521Z",
          "wordCount": 644,
          "title": "Machine Learning and Deep Learning Methods for Building Intelligent Systems in Medicine and Drug Discovery: A Comprehensive Survey. (arXiv:2107.14037v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozkara_K/0/1/0/all/0/1\">Kaan Ozkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Navjot Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Data_D/0/1/0/all/0/1\">Deepesh Data</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diggavi_S/0/1/0/all/0/1\">Suhas Diggavi</a>",
          "description": "Traditionally, federated learning (FL) aims to train a single global model\nwhile collaboratively using multiple clients and a server. Two natural\nchallenges that FL algorithms face are heterogeneity in data across clients and\ncollaboration of clients with {\\em diverse resources}. In this work, we\nintroduce a \\textit{quantized} and \\textit{personalized} FL algorithm QuPeD\nthat facilitates collective (personalized model compression) training via\n\\textit{knowledge distillation} (KD) among clients who have access to\nheterogeneous data and resources. For personalization, we allow clients to\nlearn \\textit{compressed personalized models} with different quantization\nparameters and model dimensions/structures. Towards this, first we propose an\nalgorithm for learning quantized models through a relaxed optimization problem,\nwhere quantization values are also optimized over. When each client\nparticipating in the (federated) learning process has different requirements\nfor the compressed model (both in model dimension and precision), we formulate\na compressed personalization framework by introducing knowledge distillation\nloss for local client objectives collaborating through a global model. We\ndevelop an alternating proximal gradient update for solving this compressed\npersonalization problem, and analyze its convergence properties. Numerically,\nwe validate that QuPeD outperforms competing personalized FL methods, FedAvg,\nand local training of clients in various heterogeneous settings.",
          "link": "http://arxiv.org/abs/2107.13892",
          "publishedOn": "2021-07-30T02:13:29.461Z",
          "wordCount": 635,
          "title": "QuPeD: Quantized Personalization via Distillation with Applications to Federated Learning. (arXiv:2107.13892v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14035",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mike Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piech_C/0/1/0/all/0/1\">Chris Piech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>",
          "description": "High-quality computer science education is limited by the difficulty of\nproviding instructor feedback to students at scale. While this feedback could\nin principle be automated, supervised approaches to predicting the correct\nfeedback are bottlenecked by the intractability of annotating large quantities\nof student code. In this paper, we instead frame the problem of providing\nfeedback as few-shot classification, where a meta-learner adapts to give\nfeedback to student code on a new programming question from just a few examples\nannotated by instructors. Because data for meta-training is limited, we propose\na number of amendments to the typical few-shot learning framework, including\ntask augmentation to create synthetic tasks, and additional side information to\nbuild stronger priors about each task. These additions are combined with a\ntransformer architecture to embed discrete sequences (e.g. code) to a\nprototypical representation of a feedback class label. On a suite of few-shot\nnatural language processing tasks, we match or outperform state-of-the-art\nperformance. Then, on a collection of student solutions to exam questions from\nan introductory university course, we show that our approach reaches an average\nprecision of 88% on unseen questions, surpassing the 82% precision of teaching\nassistants. Our approach was successfully deployed to deliver feedback to\n16,000 student exam-solutions in a programming course offered by a tier 1\nuniversity. This is, to the best of our knowledge, the first successful\ndeployment of a machine learning based feedback to open-ended student code.",
          "link": "http://arxiv.org/abs/2107.14035",
          "publishedOn": "2021-07-30T02:13:29.453Z",
          "wordCount": 677,
          "title": "ProtoTransformer: A Meta-Learning Approach to Providing Student Feedback. (arXiv:2107.14035v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scabini_L/0/1/0/all/0/1\">Leonardo F. S. Scabini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruno_O/0/1/0/all/0/1\">Odemir M. Bruno</a>",
          "description": "Understanding the behavior of Artificial Neural Networks is one of the main\ntopics in the field recently, as black-box approaches have become usual since\nthe widespread of deep learning. Such high-dimensional models may manifest\ninstabilities and weird properties that resemble complex systems. Therefore, we\npropose Complex Network (CN) techniques to analyze the structure and\nperformance of fully connected neural networks. For that, we build a dataset\nwith 4 thousand models and their respective CN properties. They are employed in\na supervised classification setup considering four vision benchmarks. Each\nneural network is approached as a weighted and undirected graph of neurons and\nsynapses, and centrality measures are computed after training. Results show\nthat these measures are highly related to the network classification\nperformance. We also propose the concept of Bag-Of-Neurons (BoN), a CN-based\napproach for finding topological signatures linking similar neurons. Results\nsuggest that six neuronal types emerge in such networks, independently of the\ntarget domain, and are distributed differently according to classification\naccuracy. We also tackle specific CN properties related to performance, such as\nhigher subgraph centrality on lower-performing models. Our findings suggest\nthat CN properties play a critical role in the performance of fully connected\nneural networks, with topological patterns emerging independently on a wide\nrange of models.",
          "link": "http://arxiv.org/abs/2107.14062",
          "publishedOn": "2021-07-30T02:13:29.434Z",
          "wordCount": 686,
          "title": "Structure and Performance of Fully Connected Neural Networks: Emerging Complex Network Properties. (arXiv:2107.14062v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiaodian Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wanhang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shuihai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>",
          "description": "In recent years, federated learning (FL) has been widely applied for\nsupporting decentralized collaborative learning scenarios. Among existing FL\nmodels, federated logistic regression (FLR) is a widely used statistic model\nand has been used in various industries. To ensure data security and user\nprivacy, FLR leverages homomorphic encryption (HE) to protect the exchanged\ndata among different collaborative parties. However, HE introduces significant\ncomputational overhead (i.e., the cost of data encryption/decryption and\ncalculation over encrypted data), which eventually becomes the performance\nbottleneck of the whole system. In this paper, we propose HAFLO, a GPU-based\nsolution to improve the performance of FLR. The core idea of HAFLO is to\nsummarize a set of performance-critical homomorphic operators (HO) used by FLR\nand accelerate the execution of these operators through a joint optimization of\nstorage, IO, and computation. The preliminary results show that our\nacceleration on FATE, a popular FL framework, achieves a 49.9$\\times$ speedup\nfor heterogeneous LR and 88.4$\\times$ for homogeneous LR.",
          "link": "http://arxiv.org/abs/2107.13797",
          "publishedOn": "2021-07-30T02:13:29.428Z",
          "wordCount": 589,
          "title": "HAFLO: GPU-Based Acceleration for Federated Logistic Regression. (arXiv:2107.13797v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Simeunovic_J/0/1/0/all/0/1\">Jelena Simeunovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubnel_B/0/1/0/all/0/1\">Baptiste Schubnel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alet_P/0/1/0/all/0/1\">Pierre-Jean Alet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrillo_R/0/1/0/all/0/1\">Rafael E. Carrillo</a>",
          "description": "Accurate forecasting of solar power generation with fine temporal and spatial\nresolution is vital for the operation of the power grid. However,\nstate-of-the-art approaches that combine machine learning with numerical\nweather predictions (NWP) have coarse resolution. In this paper, we take a\ngraph signal processing perspective and model multi-site photovoltaic (PV)\nproduction time series as signals on a graph to capture their spatio-temporal\ndependencies and achieve higher spatial and temporal resolution forecasts. We\npresent two novel graph neural network models for deterministic multi-site PV\nforecasting dubbed the graph-convolutional long short term memory (GCLSTM) and\nthe graph-convolutional transformer (GCTrafo) models. These methods rely solely\non production data and exploit the intuition that PV systems provide a dense\nnetwork of virtual weather stations. The proposed methods were evaluated in two\ndata sets for an entire year: 1) production data from 304 real PV systems, and\n2) simulated production of 1000 PV systems, both distributed over Switzerland.\nThe proposed models outperform state-of-the-art multi-site forecasting methods\nfor prediction horizons of six hours ahead. Furthermore, the proposed models\noutperform state-of-the-art single-site methods with NWP as inputs on horizons\nup to four hours ahead.",
          "link": "http://arxiv.org/abs/2107.13875",
          "publishedOn": "2021-07-30T02:13:29.421Z",
          "wordCount": 638,
          "title": "Spatio-temporal graph neural networks for multi-site PV power forecasting. (arXiv:2107.13875v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13657",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Goel_G/0/1/0/all/0/1\">Gautam Goel</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hassibi_B/0/1/0/all/0/1\">Babak Hassibi</a>",
          "description": "We consider control from the perspective of competitive analysis. Unlike much\nprior work on learning-based control, which focuses on minimizing regret\nagainst the best controller selected in hindsight from some specific class, we\nfocus on designing an online controller which competes against a clairvoyant\noffline optimal controller. A natural performance metric in this setting is\ncompetitive ratio, which is the ratio between the cost incurred by the online\ncontroller and the cost incurred by the offline optimal controller. Using\noperator-theoretic techniques from robust control, we derive a computationally\nefficient state-space description of the the controller with optimal\ncompetitive ratio in both finite-horizon and infinite-horizon settings. We\nextend competitive control to nonlinear systems using Model Predictive Control\n(MPC) and present numerical experiments which show that our competitive\ncontroller can significantly outperform standard $H_2$ and $H_{\\infty}$\ncontrollers in the MPC setting.",
          "link": "http://arxiv.org/abs/2107.13657",
          "publishedOn": "2021-07-30T02:13:29.415Z",
          "wordCount": 559,
          "title": "Competitive Control. (arXiv:2107.13657v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14112",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Baucas_M/0/1/0/all/0/1\">Marc Jayson Baucas</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Spachos_P/0/1/0/all/0/1\">Petros Spachos</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gregori_S/0/1/0/all/0/1\">Stefano Gregori</a>",
          "description": "Medical conditions and cases are growing at a rapid pace, where physical\nspace is starting to be constrained. Hospitals and clinics no longer have the\nability to accommodate large numbers of incoming patients. It is clear that the\ncurrent state of the health industry needs to improve its valuable and limited\nresources. The evolution of the Internet of Things (IoT) devices along with\nassistive technologies can alleviate the problem in healthcare, by being a\nconvenient and easy means of accessing healthcare services wirelessly. There is\na plethora of IoT devices and potential applications that can take advantage of\nthe unique characteristics that these technologies can offer. However, at the\nsame time, these services pose novel challenges that need to be properly\naddressed. In this article, we review some popular categories of IoT-based\napplications for healthcare along with their devices. Then, we describe the\nchallenges and discuss how research can properly address the open issues and\nimprove the already existing implementations in healthcare. Further possible\nsolutions are also discussed to show their potential in being viable solutions\nfor future healthcare applications",
          "link": "http://arxiv.org/abs/2107.14112",
          "publishedOn": "2021-07-30T02:13:29.409Z",
          "wordCount": 647,
          "title": "Internet-of-Things Devices and Assistive Technologies for Healthcare: Applications, Challenges, and Opportunities. (arXiv:2107.14112v1 [physics.soc-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13586",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weizhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengbao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1\">Hiroaki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "This paper surveys and organizes research works in a new paradigm in natural\nlanguage processing, which we dub \"prompt-based learning\". Unlike traditional\nsupervised learning, which trains a model to take in an input x and predict an\noutput y as P(y|x), prompt-based learning is based on language models that\nmodel the probability of text directly. To use these models to perform\nprediction tasks, the original input x is modified using a template into a\ntextual string prompt x' that has some unfilled slots, and then the language\nmodel is used to probabilistically fill the unfilled information to obtain a\nfinal string x, from which the final output y can be derived. This framework is\npowerful and attractive for a number of reasons: it allows the language model\nto be pre-trained on massive amounts of raw text, and by defining a new\nprompting function the model is able to perform few-shot or even zero-shot\nlearning, adapting to new scenarios with few or no labeled data. In this paper\nwe introduce the basics of this promising paradigm, describe a unified set of\nmathematical notations that can cover a wide variety of existing work, and\norganize existing work along several dimensions, e.g.the choice of pre-trained\nmodels, prompts, and tuning strategies. To make the field more accessible to\ninterested beginners, we not only make a systematic review of existing works\nand a highly structured typology of prompt-based concepts, but also release\nother resources, e.g., a website this http URL including\nconstantly-updated survey, and paperlist.",
          "link": "http://arxiv.org/abs/2107.13586",
          "publishedOn": "2021-07-30T02:13:29.395Z",
          "wordCount": 710,
          "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. (arXiv:2107.13586v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raschka_S/0/1/0/all/0/1\">Sebastian Raschka</a>",
          "description": "Machine learning has seen a vast increase of interest in recent years, along\nwith an abundance of learning resources. While conventional lectures provide\nstudents with important information and knowledge, we also believe that\nadditional project-based learning components can motivate students to engage in\ntopics more deeply. In addition to incorporating project-based learning in our\ncourses, we aim to develop project-based learning components aligned with\nreal-world tasks, including experimental design and execution, report writing,\noral presentation, and peer-reviewing. This paper describes the organization of\nour project-based machine learning courses with a particular emphasis on the\nclass project components and shares our resources with instructors who would\nlike to include similar elements in their courses.",
          "link": "http://arxiv.org/abs/2107.13671",
          "publishedOn": "2021-07-30T02:13:29.389Z",
          "wordCount": 579,
          "title": "Deeper Learning By Doing: Integrating Hands-On Research Projects Into a Machine Learning Course. (arXiv:2107.13671v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyons_M/0/1/0/all/0/1\">Michael J. Lyons</a>",
          "description": "Twenty-five years ago, my colleagues Miyuki Kamachi and Jiro Gyoba and I\ndesigned and photographed JAFFE, a set of facial expression images intended for\nuse in a study of face perception. In 2019, without seeking permission or\ninforming us, Kate Crawford and Trevor Paglen exhibited JAFFE in two widely\npublicized art shows. In addition, they published a nonfactual account of the\nimages in the essay \"Excavating AI: The Politics of Images in Machine Learning\nTraining Sets.\" The present article recounts the creation of the JAFFE dataset\nand unravels each of Crawford and Paglen's fallacious statements. I also\ndiscuss JAFFE more broadly in connection with research on facial expression,\naffective computing, and human-computer interaction.",
          "link": "http://arxiv.org/abs/2107.13998",
          "publishedOn": "2021-07-30T02:13:29.381Z",
          "wordCount": 572,
          "title": "\"Excavating AI\" Re-excavated: Debunking a Fallacious Account of the JAFFE Dataset. (arXiv:2107.13998v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lordelo_C/0/1/0/all/0/1\">Carlos Lordelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1\">Emmanouil Benetos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixon_S/0/1/0/all/0/1\">Simon Dixon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahlback_S/0/1/0/all/0/1\">Sven Ahlb&#xe4;ck</a>",
          "description": "This paper proposes a deep convolutional neural network for performing\nnote-level instrument assignment. Given a polyphonic multi-instrumental music\nsignal along with its ground truth or predicted notes, the objective is to\nassign an instrumental source for each note. This problem is addressed as a\npitch-informed classification task where each note is analysed individually. We\nalso propose to utilise several kernel shapes in the convolutional layers in\norder to facilitate learning of efficient timbre-discriminative feature maps.\nExperiments on the MusicNet dataset using 7 instrument classes show that our\napproach is able to achieve an average F-score of 0.904 when the original\nmulti-pitch annotations are used as the pitch information for the system, and\nthat it also excels if the note information is provided using third-party\nmulti-pitch estimation algorithms. We also include ablation studies\ninvestigating the effects of the use of multiple kernel shapes and comparing\ndifferent input representations for the audio and the note-related information.",
          "link": "http://arxiv.org/abs/2107.13617",
          "publishedOn": "2021-07-30T02:13:29.376Z",
          "wordCount": 625,
          "title": "Pitch-Informed Instrument Assignment Using a Deep Convolutional Network with Multiple Kernel Shapes. (arXiv:2107.13617v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Koushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishmam_A/0/1/0/all/0/1\">Abtahi Ishmam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taher_K/0/1/0/all/0/1\">Kazi Abu Taher</a>",
          "description": "Demand forecasting in power sector has become an important part of modern\ndemand management and response systems with the rise of smart metering enabled\ngrids. Long Short-Term Memory (LSTM) shows promising results in predicting time\nseries data which can also be applied to power load demand in smart grids. In\nthis paper, an LSTM based model using neural network architecture is proposed\nto forecast power demand. The model is trained with hourly energy and power\nusage data of four years from a smart grid. After training and prediction, the\naccuracy of the model is compared against the traditional statistical time\nseries analysis algorithms, such as Auto-Regressive (AR), to determine the\nefficiency. The mean absolute percentile error is found to be 1.22 in the\nproposed LSTM model, which is the lowest among the other models. From the\nfindings, it is clear that the inclusion of neural network in predicting power\ndemand reduces the error of prediction significantly. Thus, the application of\nLSTM can enable a more efficient demand response system.",
          "link": "http://arxiv.org/abs/2107.13653",
          "publishedOn": "2021-07-30T02:13:29.370Z",
          "wordCount": 621,
          "title": "Demand Forecasting in Smart Grid Using Long Short-Term Memory. (arXiv:2107.13653v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antaris_S/0/1/0/all/0/1\">Stefanos Antaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafailidis_D/0/1/0/all/0/1\">Dimitrios Rafailidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girdzijauskas_S/0/1/0/all/0/1\">Sarunas Girdzijauskas</a>",
          "description": "In this paper we present a deep graph reinforcement learning model to predict\nand improve the user experience during a live video streaming event,\norchestrated by an agent/tracker. We first formulate the user experience\nprediction problem as a classification task, accounting for the fact that most\nof the viewers at the beginning of an event have poor quality of experience due\nto low-bandwidth connections and limited interactions with the tracker. In our\nmodel we consider different factors that influence the quality of user\nexperience and train the proposed model on diverse state-action transitions\nwhen viewers interact with the tracker. In addition, provided that past events\nhave various user experience characteristics we follow a gradient boosting\nstrategy to compute a global model that learns from different events. Our\nexperiments with three real-world datasets of live video streaming events\ndemonstrate the superiority of the proposed model against several baseline\nstrategies. Moreover, as the majority of the viewers at the beginning of an\nevent has poor experience, we show that our model can significantly increase\nthe number of viewers with high quality experience by at least 75% over the\nfirst streaming minutes. Our evaluation datasets and implementation are\npublicly available at https://publicresearch.z13.web.core.windows.net",
          "link": "http://arxiv.org/abs/2107.13619",
          "publishedOn": "2021-07-30T02:13:29.355Z",
          "wordCount": 644,
          "title": "A Deep Graph Reinforcement Learning Model for Improving User Experience in Live Video Streaming. (arXiv:2107.13619v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaulwar_A/0/1/0/all/0/1\">Amit Chaulwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huth_M/0/1/0/all/0/1\">Michael Huth</a>",
          "description": "Federated analytics has many applications in edge computing, its use can lead\nto better decision making for service provision, product development, and user\nexperience. We propose a Bayesian approach to trend detection in which the\nprobability of a keyword being trendy, given a dataset, is computed via Bayes'\nTheorem; the probability of a dataset, given that a keyword is trendy, is\ncomputed through secure aggregation of such conditional probabilities over\nlocal datasets of users. We propose a protocol, named SAFE, for Bayesian\nfederated analytics that offers sufficient privacy for production grade use\ncases and reduces the computational burden of users and an aggregator. We\nillustrate this approach with a trend detection experiment and discuss how this\napproach could be extended further to make it production-ready.",
          "link": "http://arxiv.org/abs/2107.13640",
          "publishedOn": "2021-07-30T02:13:29.348Z",
          "wordCount": 566,
          "title": "Secure Bayesian Federated Analytics for Privacy-Preserving Trend Detection. (arXiv:2107.13640v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deldjoo_Y/0/1/0/all/0/1\">Yashar Deldjoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merra_F/0/1/0/all/0/1\">Felice Antonio Merra</a>",
          "description": "Recommender systems (RSs) employ user-item feedback, e.g., ratings, to match\ncustomers to personalized lists of products. Approaches to top-k recommendation\nmainly rely on Learning-To-Rank algorithms and, among them, the most widely\nadopted is Bayesian Personalized Ranking (BPR), which bases on a pair-wise\noptimization approach. Recently, BPR has been found vulnerable against\nadversarial perturbations of its model parameters. Adversarial Personalized\nRanking (APR) mitigates this issue by robustifying BPR via an adversarial\ntraining procedure. The empirical improvements of APR's accuracy performance on\nBPR have led to its wide use in several recommender models. However, a key\noverlooked aspect has been the beyond-accuracy performance of APR, i.e.,\nnovelty, coverage, and amplification of popularity bias, considering that\nrecent results suggest that BPR, the building block of APR, is sensitive to the\nintensification of biases and reduction of recommendation novelty. In this\nwork, we model the learning characteristics of the BPR and APR optimization\nframeworks to give mathematical evidence that, when the feedback data have a\ntailed distribution, APR amplifies the popularity bias more than BPR due to an\nunbalanced number of received positive updates from short-head items. Using\nmatrix factorization (MF), we empirically validate the theoretical results by\nperforming preliminary experiments on two public datasets to compare BPR-MF and\nAPR-MF performance on accuracy and beyond-accuracy metrics. The experimental\nresults consistently show the degradation of novelty and coverage measures and\na worrying amplification of bias.",
          "link": "http://arxiv.org/abs/2107.13876",
          "publishedOn": "2021-07-30T02:13:29.341Z",
          "wordCount": 687,
          "title": "Understanding the Effects of Adversarial Personalized Ranking Optimization Method on Recommendation Quality. (arXiv:2107.13876v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yichun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>",
          "description": "Pre-trained language models (PLMs) have achieved great success in natural\nlanguage processing. Most of PLMs follow the default setting of architecture\nhyper-parameters (e.g., the hidden dimension is a quarter of the intermediate\ndimension in feed-forward sub-networks) in BERT (Devlin et al., 2019). Few\nstudies have been conducted to explore the design of architecture\nhyper-parameters in BERT, especially for the more efficient PLMs with tiny\nsizes, which are essential for practical deployment on resource-constrained\ndevices. In this paper, we adopt the one-shot Neural Architecture Search (NAS)\nto automatically search architecture hyper-parameters. Specifically, we\ncarefully design the techniques of one-shot learning and the search space to\nprovide an adaptive and efficient development way of tiny PLMs for various\nlatency constraints. We name our method AutoTinyBERT and evaluate its\neffectiveness on the GLUE and SQuAD benchmarks. The extensive experiments show\nthat our method outperforms both the SOTA search-based baseline (NAS-BERT) and\nthe SOTA distillation-based methods (such as DistilBERT, TinyBERT, MiniLM and\nMobileBERT). In addition, based on the obtained architectures, we propose a\nmore efficient development method that is even faster than the development of a\nsingle PLM.",
          "link": "http://arxiv.org/abs/2107.13686",
          "publishedOn": "2021-07-30T02:13:29.334Z",
          "wordCount": 639,
          "title": "AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models. (arXiv:2107.13686v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aitio_A/0/1/0/all/0/1\">Antti Aitio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howey_D/0/1/0/all/0/1\">David A. Howey</a>",
          "description": "Hundreds of millions of people lack access to electricity. Decentralised\nsolar-battery systems are key for addressing this whilst avoiding carbon\nemissions and air pollution, but are hindered by relatively high costs and\nrural locations that inhibit timely preventative maintenance. Accurate\ndiagnosis of battery health and prediction of end of life from operational data\nimproves user experience and reduces costs. But lack of controlled validation\ntests and variable data quality mean existing lab-based techniques fail to\nwork. We apply a scaleable probabilistic machine learning approach to diagnose\nhealth in 1027 solar-connected lead-acid batteries, each running for 400-760\ndays, totalling 620 million data rows. We demonstrate 73% accurate prediction\nof end of life, eight weeks in advance, rising to 82% at the point of failure.\nThis work highlights the opportunity to estimate health from existing\nmeasurements using `big data' techniques, without additional equipment,\nextending lifetime and improving performance in real-world applications.",
          "link": "http://arxiv.org/abs/2107.13856",
          "publishedOn": "2021-07-30T02:13:29.328Z",
          "wordCount": 599,
          "title": "Predicting battery end of life from solar off-grid system field data using machine learning. (arXiv:2107.13856v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wentao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Han Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaorui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thuraisingham_B/0/1/0/all/0/1\">Bhavani Thuraisingham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>",
          "description": "Adversarial training has been empirically proven to be one of the most\neffective and reliable defense methods against adversarial attacks. However,\nalmost all existing studies about adversarial training are focused on balanced\ndatasets, where each class has an equal amount of training examples. Research\non adversarial training with imbalanced training datasets is rather limited. As\nthe initial effort to investigate this problem, we reveal the facts that\nadversarially trained models present two distinguished behaviors from naturally\ntrained models in imbalanced datasets: (1) Compared to natural training,\nadversarially trained models can suffer much worse performance on\nunder-represented classes, when the training dataset is extremely imbalanced.\n(2) Traditional reweighting strategies may lose efficacy to deal with the\nimbalance issue for adversarial training. For example, upweighting the\nunder-represented classes will drastically hurt the model's performance on\nwell-represented classes, and as a result, finding an optimal reweighting value\ncan be tremendously challenging. In this paper, to further understand our\nobservations, we theoretically show that the poor data separability is one key\nreason causing this strong tension between under-represented and\nwell-represented classes. Motivated by this finding, we propose Separable\nReweighted Adversarial Training (SRAT) to facilitate adversarial training under\nimbalanced scenarios, by learning more separable features for different\nclasses. Extensive experiments on various datasets verify the effectiveness of\nthe proposed framework.",
          "link": "http://arxiv.org/abs/2107.13639",
          "publishedOn": "2021-07-30T02:13:29.313Z",
          "wordCount": 643,
          "title": "Imbalanced Adversarial Training with Reweighting. (arXiv:2107.13639v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bachinger_F/0/1/0/all/0/1\">Florian Bachinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1\">Gabriel Kronberger</a>",
          "description": "With the increasing number of created and deployed prediction models and the\ncomplexity of machine learning workflows we require so called model management\nsystems to support data scientists in their tasks. In this work we describe our\ntechnological concept for such a model management system. This concept includes\nversioned storage of data, support for different machine learning algorithms,\nfine tuning of models, subsequent deployment of models and monitoring of model\nperformance after deployment. We describe this concept with a close focus on\nmodel lifecycle requirements stemming from our industry application cases, but\ngeneralize key features that are relevant for all applications of machine\nlearning.",
          "link": "http://arxiv.org/abs/2107.13821",
          "publishedOn": "2021-07-30T02:13:29.306Z",
          "wordCount": 583,
          "title": "Concept for a Technical Infrastructure for Management of Predictive Models in Industrial Applications. (arXiv:2107.13821v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xinyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dongjin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuncong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengzhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingchao Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>",
          "description": "Detecting abnormal activities in real-world surveillance videos is an\nimportant yet challenging task as the prior knowledge about video anomalies is\nusually limited or unavailable. Despite that many approaches have been\ndeveloped to resolve this problem, few of them can capture the normal\nspatio-temporal patterns effectively and efficiently. Moreover, existing works\nseldom explicitly consider the local consistency at frame level and global\ncoherence of temporal dynamics in video sequences. To this end, we propose\nConvolutional Transformer based Dual Discriminator Generative Adversarial\nNetworks (CT-D2GAN) to perform unsupervised video anomaly detection.\nSpecifically, we first present a convolutional transformer to perform future\nframe prediction. It contains three key components, i.e., a convolutional\nencoder to capture the spatial information of the input video clips, a temporal\nself-attention module to encode the temporal dynamics, and a convolutional\ndecoder to integrate spatio-temporal features and predict the future frame.\nNext, a dual discriminator based adversarial training procedure, which jointly\nconsiders an image discriminator that can maintain the local consistency at\nframe-level and a video discriminator that can enforce the global coherence of\ntemporal dynamics, is employed to enhance the future frame prediction. Finally,\nthe prediction error is used to identify abnormal video frames. Thoroughly\nempirical studies on three public video anomaly detection datasets, i.e., UCSD\nPed2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the effectiveness of\nthe proposed adversarial spatio-temporal modeling framework.",
          "link": "http://arxiv.org/abs/2107.13720",
          "publishedOn": "2021-07-30T02:13:29.299Z",
          "wordCount": 708,
          "title": "Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for Video Anomaly Detection. (arXiv:2107.13720v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goh_H/0/1/0/all/0/1\">Hoe-Han Goh</a>",
          "description": "This perspective illustrates some of the AI applications that can accelerate\nthe achievement of SDGs and also highlights some of the considerations that\ncould hinder the efforts towards them. This emphasizes the importance of\nestablishing standard AI guidelines and regulations for the beneficial\napplications of AI.",
          "link": "http://arxiv.org/abs/2107.13966",
          "publishedOn": "2021-07-30T02:13:29.293Z",
          "wordCount": 487,
          "title": "Artificial Intelligence in Achieving Sustainable Development Goals. (arXiv:2107.13966v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aminian_G/0/1/0/all/0/1\">Gholamali Aminian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_Y/0/1/0/all/0/1\">Yuheng Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toni_L/0/1/0/all/0/1\">Laura Toni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_M/0/1/0/all/0/1\">Miguel R. D. Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wornell_G/0/1/0/all/0/1\">Gregory Wornell</a>",
          "description": "Bounding the generalization error of a supervised learning algorithm is one\nof the most important problems in learning theory, and various approaches have\nbeen developed. However, existing bounds are often loose and lack of\nguarantees. As a result, they may fail to characterize the exact generalization\nability of a learning algorithm. Our main contribution is an exact\ncharacterization of the expected generalization error of the well-known Gibbs\nalgorithm in terms of symmetrized KL information between the input training\nsamples and the output hypothesis. Such a result can be applied to tighten\nexisting expected generalization error bound. Our analysis provides more\ninsight on the fundamental role the symmetrized KL information plays in\ncontrolling the generalization error of the Gibbs algorithm.",
          "link": "http://arxiv.org/abs/2107.13656",
          "publishedOn": "2021-07-30T02:13:29.277Z",
          "wordCount": 608,
          "title": "Characterizing the Generalization Error of Gibbs Algorithm with Symmetrized KL information. (arXiv:2107.13656v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Otles_E/0/1/0/all/0/1\">Erkin &#xd6;tle&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jeeheh Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Benjamin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bochinski_M/0/1/0/all/0/1\">Michelle Bochinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1\">Hyeon Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortwine_J/0/1/0/all/0/1\">Justin Ortwine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_E/0/1/0/all/0/1\">Erica Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washer_L/0/1/0/all/0/1\">Laraine Washer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_V/0/1/0/all/0/1\">Vincent B. Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">Krishna Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1\">Jenna Wiens</a>",
          "description": "Once integrated into clinical care, patient risk stratification models may\nperform worse compared to their retrospective performance. To date, it is\nwidely accepted that performance will degrade over time due to changes in care\nprocesses and patient populations. However, the extent to which this occurs is\npoorly understood, in part because few researchers report prospective\nvalidation performance. In this study, we compare the 2020-2021 ('20-'21)\nprospective performance of a patient risk stratification model for predicting\nhealthcare-associated infections to a 2019-2020 ('19-'20) retrospective\nvalidation of the same model. We define the difference in retrospective and\nprospective performance as the performance gap. We estimate how i) \"temporal\nshift\", i.e., changes in clinical workflows and patient populations, and ii)\n\"infrastructure shift\", i.e., changes in access, extraction and transformation\nof data, both contribute to the performance gap. Applied prospectively to\n26,864 hospital encounters during a twelve-month period from July 2020 to June\n2021, the model achieved an area under the receiver operating characteristic\ncurve (AUROC) of 0.767 (95% confidence interval (CI): 0.737, 0.801) and a Brier\nscore of 0.189 (95% CI: 0.186, 0.191). Prospective performance decreased\nslightly compared to '19-'20 retrospective performance, in which the model\nachieved an AUROC of 0.778 (95% CI: 0.744, 0.815) and a Brier score of 0.163\n(95% CI: 0.161, 0.165). The resulting performance gap was primarily due to\ninfrastructure shift and not temporal shift. So long as we continue to develop\nand validate models using data stored in large research data warehouses, we\nmust consider differences in how and when data are accessed, measure how these\ndifferences may affect prospective performance, and work to mitigate those\ndifferences.",
          "link": "http://arxiv.org/abs/2107.13964",
          "publishedOn": "2021-07-30T02:13:29.247Z",
          "wordCount": 733,
          "title": "Mind the Performance Gap: Examining Dataset Shift During Prospective Validation. (arXiv:2107.13964v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grespan_M/0/1/0/all/0/1\">Mattia Medina Grespan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ashim Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>",
          "description": "Symbolic knowledge can provide crucial inductive bias for training neural\nmodels, especially in low data regimes. A successful strategy for incorporating\nsuch knowledge involves relaxing logical statements into sub-differentiable\nlosses for optimization. In this paper, we study the question of how best to\nrelax logical expressions that represent labeled examples and knowledge about a\nproblem; we focus on sub-differentiable t-norm relaxations of logic. We present\ntheoretical and empirical criteria for characterizing which relaxation would\nperform best in various scenarios. In our theoretical study driven by the goal\nof preserving tautologies, the Lukasiewicz t-norm performs best. However, in\nour empirical analysis on the text chunking and digit recognition tasks, the\nproduct t-norm achieves best predictive performance. We analyze this apparent\ndiscrepancy, and conclude with a list of best practices for defining loss\nfunctions via logic.",
          "link": "http://arxiv.org/abs/2107.13646",
          "publishedOn": "2021-07-30T02:13:29.191Z",
          "wordCount": 578,
          "title": "Evaluating Relaxations of Logic for Neural Networks: A Comprehensive Study. (arXiv:2107.13646v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trillos_N/0/1/0/all/0/1\">Nicolas Garcia Trillos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengfei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenghui Li</a>",
          "description": "In this work we study statistical properties of graph-based algorithms for\nmulti-manifold clustering (MMC). In MMC the goal is to retrieve the\nmulti-manifold structure underlying a given Euclidean data set when this one is\nassumed to be obtained by sampling a distribution on a union of manifolds\n$\\mathcal{M} = \\mathcal{M}_1 \\cup\\dots \\cup \\mathcal{M}_N$ that may intersect\nwith each other and that may have different dimensions. We investigate\nsufficient conditions that similarity graphs on data sets must satisfy in order\nfor their corresponding graph Laplacians to capture the right geometric\ninformation to solve the MMC problem. Precisely, we provide high probability\nerror bounds for the spectral approximation of a tensorized Laplacian on\n$\\mathcal{M}$ with a suitable graph Laplacian built from the observations; the\nrecovered tensorized Laplacian contains all geometric information of all the\nindividual underlying manifolds. We provide an example of a family of\nsimilarity graphs, which we call annular proximity graphs with angle\nconstraints, satisfying these sufficient conditions. We contrast our family of\ngraphs with other constructions in the literature based on the alignment of\ntangent planes. Extensive numerical experiments expand the insights that our\ntheory provides on the MMC problem.",
          "link": "http://arxiv.org/abs/2107.13610",
          "publishedOn": "2021-07-30T02:13:29.165Z",
          "wordCount": 635,
          "title": "Large sample spectral analysis of graph-based multi-manifold clustering. (arXiv:2107.13610v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Agni Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_V/0/1/0/all/0/1\">Vikramjit Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliver_C/0/1/0/all/0/1\">Carolyn Oliver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullal_A/0/1/0/all/0/1\">Adeeti Ullal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biddulph_M/0/1/0/all/0/1\">Matt Biddulph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mance_I/0/1/0/all/0/1\">Irida Mance</a>",
          "description": "Respiratory rate (RR) is a clinical metric used to assess overall health and\nphysical fitness. An individual's RR can change from their baseline due to\nchronic illness symptoms (e.g., asthma, congestive heart failure), acute\nillness (e.g., breathlessness due to infection), and over the course of the day\ndue to physical exhaustion during heightened exertion. Remote estimation of RR\ncan offer a cost-effective method to track disease progression and\ncardio-respiratory fitness over time. This work investigates a model-driven\napproach to estimate RR from short audio segments obtained after physical\nexertion in healthy adults. Data was collected from 21 individuals using\nmicrophone-enabled, near-field headphones before, during, and after strenuous\nexercise. RR was manually annotated by counting perceived inhalations and\nexhalations. A multi-task Long-Short Term Memory (LSTM) network with\nconvolutional layers was implemented to process mel-filterbank energies,\nestimate RR in varying background noise conditions, and predict heavy\nbreathing, indicated by an RR of more than 25 breaths per minute. The\nmulti-task model performs both classification and regression tasks and\nleverages a mixture of loss functions. It was observed that RR can be estimated\nwith a concordance correlation coefficient (CCC) of 0.76 and a mean squared\nerror (MSE) of 0.2, demonstrating that audio can be a viable signal for\napproximating RR.",
          "link": "http://arxiv.org/abs/2107.14028",
          "publishedOn": "2021-07-30T02:13:29.158Z",
          "wordCount": 671,
          "title": "Estimating Respiratory Rate From Breath Audio Obtained Through Wearable Microphones. (arXiv:2107.14028v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dumpala_S/0/1/0/all/0/1\">Sri Harsha Dumpala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_S/0/1/0/all/0/1\">Sebastian Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rempel_S/0/1/0/all/0/1\">Sheri Rempel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uher_R/0/1/0/all/0/1\">Rudolf Uher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oore_S/0/1/0/all/0/1\">Sageev Oore</a>",
          "description": "Depression detection from speech has attracted a lot of attention in recent\nyears. However, the significance of speaker-specific information in depression\ndetection has not yet been explored. In this work, we analyze the significance\nof speaker embeddings for the task of depression detection from speech.\nExperimental results show that the speaker embeddings provide important cues to\nachieve state-of-the-art performance in depression detection. We also show that\ncombining conventional OpenSMILE and COVAREP features, which carry\ncomplementary information, with speaker embeddings further improves the\ndepression detection performance. The significance of temporal context in the\ntraining of deep learning models for depression detection is also analyzed in\nthis paper.",
          "link": "http://arxiv.org/abs/2107.13969",
          "publishedOn": "2021-07-30T02:13:29.151Z",
          "wordCount": 560,
          "title": "Significance of Speaker Embeddings and Temporal Context for Depression Detection. (arXiv:2107.13969v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Breiki_F/0/1/0/all/0/1\">Farha Al Breiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridzuan_M/0/1/0/all/0/1\">Muhammad Ridzuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandhe_R/0/1/0/all/0/1\">Rushali Grandhe</a>",
          "description": "Fine-grained image classification involves identifying different\nsubcategories of a class which possess very subtle discriminatory features.\nFine-grained datasets usually provide bounding box annotations along with class\nlabels to aid the process of classification. However, building large scale\ndatasets with such annotations is a mammoth task. Moreover, this extensive\nannotation is time-consuming and often requires expertise, which is a huge\nbottleneck in building large datasets. On the other hand, self-supervised\nlearning (SSL) exploits the freely available data to generate supervisory\nsignals which act as labels. The features learnt by performing some pretext\ntasks on huge unlabelled data proves to be very helpful for multiple downstream\ntasks.\n\nOur idea is to leverage self-supervision such that the model learns useful\nrepresentations of fine-grained image classes. We experimented with 3 kinds of\nmodels: Jigsaw solving as pretext task, adversarial learning (SRGAN) and\ncontrastive learning based (SimCLR) model. The learned features are used for\ndownstream tasks such as fine-grained image classification. Our code is\navailable at\nthis http URL",
          "link": "http://arxiv.org/abs/2107.13973",
          "publishedOn": "2021-07-30T02:13:29.096Z",
          "wordCount": 608,
          "title": "Self-Supervised Learning for Fine-Grained Image Classification. (arXiv:2107.13973v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14033",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Cui_C/0/1/0/all/0/1\">Chaoran Cui</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Li_X/0/1/0/all/0/1\">Xiaojie Li</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Du_J/0/1/0/all/0/1\">Juan Du</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zhang_C/0/1/0/all/0/1\">Chunyun Zhang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Nie_X/0/1/0/all/0/1\">Xiushan Nie</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Yin_Y/0/1/0/all/0/1\">Yilong Yin</a>",
          "description": "Predicting the future price trends of stocks is a challenging yet intriguing\nproblem given its critical role to help investors make profitable decisions. In\nthis paper, we present a collaborative temporal-relational modeling framework\nfor end-to-end stock trend prediction. The temporal dynamics of stocks is\nfirstly captured with an attention-based recurrent neural network. Then,\ndifferent from existing studies relying on the pairwise correlations between\nstocks, we argue that stocks are naturally connected as a collective group, and\nintroduce the hypergraph structures to jointly characterize the stock\ngroup-wise relationships of industry-belonging and fund-holding. A novel\nhypergraph tri-attention network (HGTAN) is proposed to augment the hypergraph\nconvolutional networks with a hierarchical organization of intra-hyperedge,\ninter-hyperedge, and inter-hypergraph attention modules. In this manner, HGTAN\nadaptively determines the importance of nodes, hyperedges, and hypergraphs\nduring the information propagation among stocks, so that the potential\nsynergies between stock movements can be fully exploited. Extensive experiments\non real-world data demonstrate the effectiveness of our approach. Also, the\nresults of investment simulation show that our approach can achieve a more\ndesirable risk-adjusted return. The data and codes of our work have been\nreleased at https://github.com/lixiaojieff/HGTAN.",
          "link": "http://arxiv.org/abs/2107.14033",
          "publishedOn": "2021-07-30T02:13:28.966Z",
          "wordCount": 630,
          "title": "Temporal-Relational Hypergraph Tri-Attention Networks for Stock Trend Prediction. (arXiv:2107.14033v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13832",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1\">Prerak Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deleforge_A/0/1/0/all/0/1\">Antoine Deleforge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_E/0/1/0/all/0/1\">Emmanuel Vincent</a>",
          "description": "Knowing the geometrical and acoustical parameters of a room may benefit\napplications such as audio augmented reality, speech dereverberation or audio\nforensics. In this paper, we study the problem of jointly estimating the total\nsurface area, the volume, as well as the frequency-dependent reverberation time\nand mean surface absorption of a room in a blind fashion, based on two-channel\nnoisy speech recordings from multiple, unknown source-receiver positions. A\nnovel convolutional neural network architecture leveraging both single- and\ninter-channel cues is proposed and trained on a large, realistic simulated\ndataset. Results on both simulated and real data show that using multiple\nobservations in one room significantly reduces estimation errors and variances\non all target quantities, and that using two channels helps the estimation of\nsurface and volume. The proposed model outperforms a recently proposed blind\nvolume estimation method on the considered datasets.",
          "link": "http://arxiv.org/abs/2107.13832",
          "publishedOn": "2021-07-30T02:13:28.859Z",
          "wordCount": 595,
          "title": "Blind Room Parameter Estimation Using Multiple-Multichannel Speech Recordings. (arXiv:2107.13832v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00719",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Kao_P/0/1/0/all/0/1\">Po-Yu Kao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kao_S/0/1/0/all/0/1\">Shu-Min Kao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Huang_N/0/1/0/all/0/1\">Nan-Lan Huang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Chu Lin</a>",
          "description": "Drug-target interaction (DTI) prediction plays a crucial role in drug\ndiscovery, and deep learning approaches have achieved state-of-the-art\nperformance in this field. We introduce an ensemble of deep learning models\n(EnsembleDLM) for DTI prediction. EnsembleDLM only uses the sequence\ninformation of chemical compounds and proteins, and it aggregates the\npredictions from multiple deep neural networks. This approach not only achieves\nstate-of-the-art performance in Davis and KIBA datasets but also reaches\ncutting-edge performance in the cross-domain applications across different\nbio-activity types and different protein classes. We also demonstrate that\nEnsembleDLM achieves a good performance (Pearson correlation coefficient and\nconcordance index > 0.8) in the new domain with approximately 50% transfer\nlearning data, i.e., the training set has twice as much data as the test set.",
          "link": "http://arxiv.org/abs/2107.00719",
          "publishedOn": "2021-07-29T02:00:11.235Z",
          "wordCount": 588,
          "title": "Toward Drug-Target Interaction Prediction via Ensemble Modeling and Transfer Learning. (arXiv:2107.00719v2 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dsouza_D/0/1/0/all/0/1\">Daniel D&#x27;souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nussbaum_Z/0/1/0/all/0/1\">Zach Nussbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>",
          "description": "As machine learning models are increasingly employed to assist human\ndecision-makers, it becomes critical to communicate the uncertainty associated\nwith these model predictions. However, the majority of work on uncertainty has\nfocused on traditional probabilistic or ranking approaches - where the model\nassigns low probabilities or scores to uncertain examples. While this captures\nwhat examples are challenging for the model, it does not capture the underlying\nsource of the uncertainty. In this work, we seek to identify examples the model\nis uncertain about and characterize the source of said uncertainty. We explore\nthe benefits of designing a targeted intervention - targeted data augmentation\nof the examples where the model is uncertain over the course of training. We\ninvestigate whether the rate of learning in the presence of additional\ninformation differs between atypical and noisy examples? Our results show that\nthis is indeed the case, suggesting that well-designed interventions over the\ncourse of training can be an effective way to characterize and distinguish\nbetween different sources of uncertainty.",
          "link": "http://arxiv.org/abs/2107.13098",
          "publishedOn": "2021-07-29T02:00:11.209Z",
          "wordCount": 618,
          "title": "A Tale Of Two Long Tails. (arXiv:2107.13098v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">Edward S. Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1\">Oleh Rybkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1\">Dinesh Jayaraman</a>",
          "description": "Training visuomotor robot controllers from scratch on a new robot typically\nrequires generating large amounts of robot-specific data. Could we leverage\ndata previously collected on another robot to reduce or even completely remove\nthis need for robot-specific data? We propose a \"robot-aware\" solution paradigm\nthat exploits readily available robot \"self-knowledge\" such as proprioception,\nkinematics, and camera calibration to achieve this. First, we learn modular\ndynamics models that pair a transferable, robot-agnostic world dynamics module\nwith a robot-specific, analytical robot dynamics module. Next, we set up visual\nplanning costs that draw a distinction between the robot self and the world.\nOur experiments on tabletop manipulation tasks in simulation and on real robots\ndemonstrate that these plug-in improvements dramatically boost the\ntransferability of visuomotor controllers, even permitting zero-shot transfer\nonto new robots for the very first time. Project website:\nhttps://hueds.github.io/rac/",
          "link": "http://arxiv.org/abs/2107.09047",
          "publishedOn": "2021-07-29T02:00:11.202Z",
          "wordCount": 603,
          "title": "Know Thyself: Transferable Visuomotor Control Through Robot-Awareness. (arXiv:2107.09047v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuangjia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Ying Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jiahua Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuedong Yang</a>",
          "description": "Constructing appropriate representations of molecules lies at the core of\nnumerous tasks such as material science, chemistry and drug designs. Recent\nresearches abstract molecules as attributed graphs and employ graph neural\nnetworks (GNN) for molecular representation learning, which have made\nremarkable achievements in molecular graph modeling. Albeit powerful, current\nmodels either are based on local aggregation operations and thus miss\nhigher-order graph properties or focus on only node information without fully\nusing the edge information. For this sake, we propose a Communicative Message\nPassing Transformer (CoMPT) neural network to improve the molecular graph\nrepresentation by reinforcing message interactions between nodes and edges\nbased on the Transformer architecture. Unlike the previous transformer-style\nGNNs that treat molecules as fully connected graphs, we introduce a message\ndiffusion mechanism to leverage the graph connectivity inductive bias and\nreduce the message enrichment explosion. Extensive experiments demonstrated\nthat the proposed model obtained superior performances (around 4$\\%$ on\naverage) against state-of-the-art baselines on seven chemical property datasets\n(graph-level tasks) and two chemical shift datasets (node-level tasks). Further\nvisualization studies also indicated a better representation capacity achieved\nby our model.",
          "link": "http://arxiv.org/abs/2107.08773",
          "publishedOn": "2021-07-29T02:00:11.168Z",
          "wordCount": 649,
          "title": "Learning Attributed Graph Representations with Communicative Message Passing Transformer. (arXiv:2107.08773v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.02951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>",
          "description": "How can deep neural networks encode information that corresponds to words in\nhuman speech into raw acoustic data? This paper proposes two neural network\narchitectures for modeling unsupervised lexical learning from raw acoustic\ninputs, ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN),\nthat combine a Deep Convolutional GAN architecture for audio data (WaveGAN;\narXiv:1705.07904) with an information theoretic extension of GAN -- InfoGAN\n(arXiv:1606.03657), and propose a new latent space structure that can model\nfeatural learning simultaneously with a higher level classification and allows\nfor a very low-dimension vector representation of lexical items. Lexical\nlearning is modeled as emergent from an architecture that forces a deep neural\nnetwork to output data such that unique information is retrievable from its\nacoustic outputs. The networks trained on lexical items from TIMIT learn to\nencode unique information corresponding to lexical items in the form of\ncategorical variables in their latent space. By manipulating these variables,\nthe network outputs specific lexical items. The network occasionally outputs\ninnovative lexical items that violate training data, but are linguistically\ninterpretable and highly informative for cognitive modeling and neural network\ninterpretability. Innovative outputs suggest that phonetic and phonological\nrepresentations learned by the network can be productively recombined and\ndirectly paralleled to productivity in human speech: a fiwGAN network trained\non `suit' and `dark' outputs innovative `start', even though it never saw\n`start' or even a [st] sequence in the training data. We also argue that\nsetting latent featural codes to values well beyond training range results in\nalmost categorical generation of prototypical lexical items and reveals\nunderlying values of each latent code.",
          "link": "http://arxiv.org/abs/2006.02951",
          "publishedOn": "2021-07-29T02:00:11.091Z",
          "wordCount": 756,
          "title": "CiwGAN and fiwGAN: Encoding information in acoustic data to model lexical learning with Generative Adversarial Networks. (arXiv:2006.02951v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sayak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>",
          "description": "Floods wreak havoc throughout the world, causing billions of dollars in\ndamages, and uprooting communities, ecosystems and economies. Accurate and\nrobust flood detection including delineating open water flood areas and\nidentifying flood levels can aid in disaster response and mitigation. However,\nestimating flood levels remotely is of essence as physical access to flooded\nareas is limited and the ability to deploy instruments in potential flood zones\ncan be dangerous. Aligning flood extent mapping with local topography can\nprovide a plan-of-action that the disaster response team can consider. Thus,\nremote flood level estimation via satellites like Sentinel-1 can prove to be\nremedial. The Emerging Techniques in Computational Intelligence (ETCI)\ncompetition on Flood Detection tasked participants with predicting flooded\npixels after training with synthetic aperture radar (SAR) images in a\nsupervised setting. We use a cyclical approach involving two stages (1)\ntraining an ensemble model of multiple UNet architectures with available high\nand low confidence labeled data and, (2) generating pseudo labels or low\nconfidence labels on the unlabeled test dataset, and then, combining the\ngenerated labels with the previously available high confidence labeled dataset.\nThis assimilated dataset is used for the next round of training ensemble\nmodels. This cyclical process is repeated until the performance improvement\nplateaus. Additionally, we post process our results with Conditional Random\nFields. Our approach sets a high score on the public leaderboard for the ETCI\ncompetition with 0.7654 IoU. Our method, which we release with all the code\nincluding trained models, can also be used as an open science benchmark for the\nSentinel-1 released dataset on GitHub. To the best of our knowledge we believe\nthis the first works to try out semi-supervised learning to improve flood\nsegmentation models.",
          "link": "http://arxiv.org/abs/2107.08369",
          "publishedOn": "2021-07-29T02:00:11.083Z",
          "wordCount": 774,
          "title": "Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Navarro_B_J/0/1/0/all/0/1\">J.-Emeterio Navarro-B</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gebert_M/0/1/0/all/0/1\">Martin Gebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielig_R/0/1/0/all/0/1\">Ralf Bielig</a>",
          "description": "This article proposes two different approaches to automatically create a map\nfor valid on-street car parking spaces. For this, we use car sharing park-out\nevents data. The first one uses spatial aggregation and the second a machine\nlearning algorithm. For the former, we chose rasterization and road sectioning;\nfor the latter we chose decision trees. We compare the results of these\napproaches and discuss their advantages and disadvantages. Furthermore, we show\nour results for a neighborhood in the city of Berlin and report a\nclassification accuracy of 91.6\\% on the original imbalanced data. Finally, we\ndiscuss further work; from gathering more data over a longer period of time to\nfitting spatial Gaussian densities to the data and the usage of apps for manual\nvalidation and annotation of parking spaces to improve ground truth data.",
          "link": "http://arxiv.org/abs/2102.06758",
          "publishedOn": "2021-07-29T02:00:11.075Z",
          "wordCount": 621,
          "title": "On automatic extraction of on-street parking spaces using park-out events data. (arXiv:2102.06758v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00773",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Visani_G/0/1/0/all/0/1\">Gian Marco Visani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1\">Alexandra Hope Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Nguyen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kent_D/0/1/0/all/0/1\">David M. Kent</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wong_J/0/1/0/all/0/1\">John B. Wong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cohen_J/0/1/0/all/0/1\">Joshua T. Cohen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hughes_M/0/1/0/all/0/1\">Michael C. Hughes</a>",
          "description": "We address the problem of modeling constrained hospital resources in the\nmidst of the COVID-19 pandemic in order to inform decision-makers of future\ndemand and assess the societal value of possible interventions. For broad\napplicability, we focus on the common yet challenging scenario where\npatient-level data for a region of interest are not available. Instead, given\ndaily admissions counts, we model aggregated counts of observed resource use,\nsuch as the number of patients in the general ward, in the intensive care unit,\nor on a ventilator. In order to explain how individual patient trajectories\nproduce these counts, we propose an aggregate count explicit-duration hidden\nMarkov model, nicknamed the ACED-HMM, with an interpretable, compact\nparameterization. We develop an Approximate Bayesian Computation approach that\ndraws samples from the posterior distribution over the model's transition and\nduration parameters given aggregate counts from a specific location, thus\nadapting the model to a region or individual hospital site of interest. Samples\nfrom this posterior can then be used to produce future forecasts of any counts\nof interest. Using data from the United States and the United Kingdom, we show\nour mechanistic approach provides competitive probabilistic forecasts for the\nfuture even as the dynamics of the pandemic shift. Furthermore, we show how our\nmodel provides insight about recovery probabilities or length of stay\ndistributions, and we suggest its potential to answer challenging what-if\nquestions about the societal value of possible interventions.",
          "link": "http://arxiv.org/abs/2105.00773",
          "publishedOn": "2021-07-29T02:00:11.068Z",
          "wordCount": 782,
          "title": "Approximate Bayesian Computation for an Explicit-Duration Hidden Markov Model of COVID-19 Hospital Trajectories. (arXiv:2105.00773v2 [stat.AP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00533",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hall_G/0/1/0/all/0/1\">Georgina Hall</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Massoulie_L/0/1/0/all/0/1\">Laurent Massouli&#xe9;</a>",
          "description": "In this paper, we consider the graph alignment problem, which is the problem\nof recovering, given two graphs, a one-to-one mapping between nodes that\nmaximizes edge overlap. This problem can be viewed as a noisy version of the\nwell-known graph isomorphism problem and appears in many applications,\nincluding social network deanonymization and cellular biology. Our focus here\nis on partial recovery, i.e., we look for a one-to-one mapping which is correct\non a fraction of the nodes of the graph rather than on all of them, and we\nassume that the two input graphs to the problem are correlated\nErd\\H{o}s-R\\'enyi graphs of parameters $(n,q,s)$. Our main contribution is then\nto give necessary and sufficient conditions on $(n,q,s)$ under which partial\nrecovery is possible with high probability as the number of nodes $n$ goes to\ninfinity. In particular, we show that it is possible to achieve partial\nrecovery in the $nqs=\\Theta(1)$ regime under certain additional assumptions. An\ninteresting byproduct of the analysis techniques we develop to obtain the\nsufficiency result in the partial recovery setting is a tighter analysis of the\nmaximum likelihood estimator for the graph alignment problem, which leads to\nimproved sufficient conditions for exact recovery.",
          "link": "http://arxiv.org/abs/2007.00533",
          "publishedOn": "2021-07-29T02:00:11.061Z",
          "wordCount": 677,
          "title": "Partial Recovery in the Graph Alignment Problem. (arXiv:2007.00533v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamford_C/0/1/0/all/0/1\">Chris Bamford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grela_L/0/1/0/all/0/1\">Lukasz Grela</a>",
          "description": "In recent years, researchers have achieved great success in applying Deep\nReinforcement Learning (DRL) algorithms to Real-time Strategy (RTS) games,\ncreating strong autonomous agents that could defeat professional players in\nStarCraft~II. However, existing approaches to tackle full games have high\ncomputational costs, usually requiring the use of thousands of GPUs and CPUs\nfor weeks. This paper has two main contributions to address this issue: 1) We\nintroduce Gym-$\\mu$RTS (pronounced \"gym-micro-RTS\") as a fast-to-run RL\nenvironment for full-game RTS research and 2) we present a collection of\ntechniques to scale DRL to play full-game $\\mu$RTS as well as ablation studies\nto demonstrate their empirical importance. Our best-trained bot can defeat\nevery $\\mu$RTS bot we tested from the past $\\mu$RTS competitions when working\nin a single-map setting, resulting in a state-of-the-art DRL agent while only\ntaking about 60 hours of training using a single machine (one GPU, three vCPU,\n16GB RAM). See the blog post at\nhttps://wandb.ai/vwxyzjn/gym-microrts-paper/reports/Gym-RTS-Toward-Affordable-Deep-Reinforcement-Learning-Research-in-Real-Time-Strategy-Games--Vmlldzo2MDIzMTg\nand the source code at https://github.com/vwxyzjn/gym-microrts-paper",
          "link": "http://arxiv.org/abs/2105.13807",
          "publishedOn": "2021-07-29T02:00:11.038Z",
          "wordCount": 666,
          "title": "Gym-$\\mu$RTS: Toward Affordable Full Game Real-time Strategy Games Research with Deep Reinforcement Learning. (arXiv:2105.13807v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.11988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fornasier_M/0/1/0/all/0/1\">Massimo Fornasier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pareschi_L/0/1/0/all/0/1\">Lorenzo Pareschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunnen_P/0/1/0/all/0/1\">Philippe S&#xfc;nnen</a>",
          "description": "We investigate the implementation of a new stochastic Kuramoto-Vicsek-type\nmodel for global optimization of nonconvex functions on the sphere. This model\nbelongs to the class of Consensus-Based Optimization. In fact, particles move\non the sphere driven by a drift towards an instantaneous consensus point, which\nis computed as a convex combination of particle locations, weighted by the cost\nfunction according to Laplace's principle, and it represents an approximation\nto a global minimizer. The dynamics is further perturbed by a random vector\nfield to favor exploration, whose variance is a function of the distance of the\nparticles to the consensus point. In particular, as soon as the consensus is\nreached the stochastic component vanishes. The main results of this paper are\nabout the proof of convergence of the numerical scheme to global minimizers\nprovided conditions of well-preparation of the initial datum. The proof\ncombines previous results of mean-field limit with a novel asymptotic analysis,\nand classical convergence results of numerical methods for SDE. We present\nseveral numerical experiments, which show that the algorithm proposed in the\npresent paper scales well with the dimension and is extremely versatile. To\nquantify the performances of the new approach, we show that the algorithm is\nable to perform essentially as good as ad hoc state of the art methods in\nchallenging problems in signal processing and machine learning, namely the\nphase retrieval problem and the robust subspace detection.",
          "link": "http://arxiv.org/abs/2001.11988",
          "publishedOn": "2021-07-29T02:00:11.020Z",
          "wordCount": 747,
          "title": "Consensus-Based Optimization on the Sphere: Convergence to Global Minimizers and Machine Learning. (arXiv:2001.11988v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guesmi_A/0/1/0/all/0/1\">Amira Guesmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alouani_I/0/1/0/all/0/1\">Ihsen Alouani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khasawneh_K/0/1/0/all/0/1\">Khaled Khasawneh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baklouti_M/0/1/0/all/0/1\">Mouna Baklouti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frikha_T/0/1/0/all/0/1\">Tarek Frikha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abid_M/0/1/0/all/0/1\">Mohamed Abid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abu_Ghazaleh_N/0/1/0/all/0/1\">Nael Abu-Ghazaleh</a>",
          "description": "In the past few years, an increasing number of machine-learning and deep\nlearning structures, such as Convolutional Neural Networks (CNNs), have been\napplied to solving a wide range of real-life problems. However, these\narchitectures are vulnerable to adversarial attacks. In this paper, we propose\nfor the first time to use hardware-supported approximate computing to improve\nthe robustness of machine learning classifiers. We show that our approximate\ncomputing implementation achieves robustness across a wide range of attack\nscenarios. Specifically, for black-box and grey-box attack scenarios, we show\nthat successful adversarial attacks against the exact classifier have poor\ntransferability to the approximate implementation. Surprisingly, the robustness\nadvantages also apply to white-box attacks where the attacker has access to the\ninternal implementation of the approximate classifier. We explain some of the\npossible reasons for this robustness through analysis of the internal operation\nof the approximate implementation. Furthermore, our approximate computing model\nmaintains the same level in terms of classification accuracy, does not require\nretraining, and reduces resource utilization and energy consumption of the CNN.\nWe conducted extensive experiments on a set of strong adversarial attacks; We\nempirically show that the proposed implementation increases the robustness of a\nLeNet-5 and an Alexnet CNNs by up to 99% and 87%, respectively for strong\ngrey-box adversarial attacks along with up to 67% saving in energy consumption\ndue to the simpler nature of the approximate logic. We also show that a\nwhite-box attack requires a remarkably higher noise budget to fool the\napproximate classifier, causing an average of 4db degradation of the PSNR of\nthe input image relative to the images that succeed in fooling the exact\nclassifier",
          "link": "http://arxiv.org/abs/2006.07700",
          "publishedOn": "2021-07-29T02:00:11.003Z",
          "wordCount": 761,
          "title": "Defensive Approximation: Enhancing CNNs Security through Approximate Computing. (arXiv:2006.07700v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>",
          "description": "Near out-of-distribution detection (OOD) is a major challenge for deep neural\nnetworks. We demonstrate that large-scale pre-trained transformers can\nsignificantly improve the state-of-the-art (SOTA) on a range of near OOD tasks\nacross different data modalities. For instance, on CIFAR-100 vs CIFAR-10 OOD\ndetection, we improve the AUROC from 85% (current SOTA) to more than 96% using\nVision Transformers pre-trained on ImageNet-21k. On a challenging genomics OOD\ndetection benchmark, we improve the AUROC from 66% to 77% using transformers\nand unsupervised pre-training. To further improve performance, we explore the\nfew-shot outlier exposure setting where a few examples from outlier classes may\nbe available; we show that pre-trained transformers are particularly\nwell-suited for outlier exposure, and that the AUROC of OOD detection on\nCIFAR-100 vs CIFAR-10 can be improved to 98.7% with just 1 image per OOD class,\nand 99.46% with 10 images per OOD class. For multi-modal image-text pre-trained\ntransformers such as CLIP, we explore a new way of using just the names of\noutlier classes as a sole source of information without any accompanying\nimages, and show that this outperforms previous SOTA on standard vision OOD\nbenchmark tasks.",
          "link": "http://arxiv.org/abs/2106.03004",
          "publishedOn": "2021-07-29T02:00:10.994Z",
          "wordCount": 646,
          "title": "Exploring the Limits of Out-of-Distribution Detection. (arXiv:2106.03004v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-29T02:00:10.967Z",
          "wordCount": 700,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Minping Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Q/0/1/0/all/0/1\">Qiuhua Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yudong Cao</a>",
          "description": "The scope of data-driven fault diagnosis models is greatly improved through\ndeep learning (DL). However, the classical convolution and recurrent structure\nhave their defects in computational efficiency and feature representation,\nwhile the latest Transformer architecture based on attention mechanism has not\nbeen applied in this field. To solve these problems, we propose a novel\ntime-frequency Transformer (TFT) model inspired by the massive success of\nstandard Transformer in sequence processing. Specially, we design a fresh\ntokenizer and encoder module to extract effective abstractions from the\ntime-frequency representation (TFR) of vibration signals. On this basis, a new\nend-to-end fault diagnosis framework based on time-frequency Transformer is\npresented in this paper. Through the case studies on bearing experimental\ndatasets, we constructed the optimal Transformer structure and verified the\nperformance of the diagnostic method. The superiority of the proposed method is\ndemonstrated in comparison with the benchmark model and other state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2104.09079",
          "publishedOn": "2021-07-29T02:00:10.915Z",
          "wordCount": 623,
          "title": "A novel Time-frequency Transformer and its Application in Fault Diagnosis of Rolling Bearings. (arXiv:2104.09079v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02604",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Sabando_M/0/1/0/all/0/1\">Mar&#xed;a Virginia Sabando</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ponzoni_I/0/1/0/all/0/1\">Ignacio Ponzoni</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Milios_E/0/1/0/all/0/1\">Evangelos E. Milios</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Soto_A/0/1/0/all/0/1\">Axel J. Soto</a>",
          "description": "With the consolidation of deep learning in drug discovery, several novel\nalgorithms for learning molecular representations have been proposed. Despite\nthe interest of the community in developing new methods for learning molecular\nembeddings and their theoretical benefits, comparing molecular embeddings with\neach other and with traditional representations is not straightforward, which\nin turn hinders the process of choosing a suitable representation for QSAR\nmodeling. A reason behind this issue is the difficulty of conducting a fair and\nthorough comparison of the different existing embedding approaches, which\nrequires numerous experiments on various datasets and training scenarios. To\nclose this gap, we reviewed the literature on methods for molecular embeddings\nand reproduced three unsupervised and two supervised molecular embedding\ntechniques recently proposed in the literature. We compared these five methods\nconcerning their performance in QSAR scenarios using different classification\nand regression datasets. We also compared these representations to traditional\nmolecular representations, namely molecular descriptors and fingerprints. As\nopposed to the expected outcome, our experimental setup consisting of over\n25,000 trained models and statistical tests revealed that the predictive\nperformance using molecular embeddings did not significantly surpass that of\ntraditional representations. While supervised embeddings yielded competitive\nresults compared to those using traditional molecular representations,\nunsupervised embeddings tended to perform worse than traditional\nrepresentations. Our results highlight the need for conducting a careful\ncomparison and analysis of the different embedding techniques prior to using\nthem in drug design tasks, and motivate a discussion about the potential of\nmolecular embeddings in computer-aided drug design.",
          "link": "http://arxiv.org/abs/2104.02604",
          "publishedOn": "2021-07-29T02:00:10.908Z",
          "wordCount": 714,
          "title": "Using Molecular Embeddings in QSAR Modeling: Does it Make a Difference?. (arXiv:2104.02604v2 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_A/0/1/0/all/0/1\">Ananth Mahadevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathioudakis_M/0/1/0/all/0/1\">Michael Mathioudakis</a>",
          "description": "Machine unlearning is the task of updating machine learning (ML) models after\na subset of the training data they were trained on is deleted. Methods for the\ntask are desired to combine effectiveness and efficiency, i.e., they should\neffectively \"unlearn\" deleted data, but in a way that does not require\nexcessive computation effort (e.g., a full retraining) for a small amount of\ndeletions. Such a combination is typically achieved by tolerating some amount\nof approximation in the unlearning. In addition, laws and regulations in the\nspirit of \"the right to be forgotten\" have given rise to requirements for\ncertifiability, i.e., the ability to demonstrate that the deleted data has\nindeed been unlearned by the ML model.\n\nIn this paper, we present an experimental study of the three state-of-the-art\napproximate unlearning methods for linear models and demonstrate the trade-offs\nbetween efficiency, effectiveness and certifiability offered by each method. In\nimplementing the study, we extend some of the existing works and describe a\ncommon ML pipeline to compare and evaluate the unlearning methods on six\nreal-world datasets and a variety of settings. We provide insights into the\neffect of the quantity and distribution of the deleted data on ML models and\nthe performance of each unlearning method in different settings. We also\npropose a practical online strategy to determine when the accumulated error\nfrom approximate unlearning is large enough to warrant a full retrain of the ML\nmodel.",
          "link": "http://arxiv.org/abs/2106.15093",
          "publishedOn": "2021-07-29T02:00:10.886Z",
          "wordCount": 683,
          "title": "Certifiable Machine Unlearning for Linear Models. (arXiv:2106.15093v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14331",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Das_A/0/1/0/all/0/1\">Abhranil Das</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Geisler_W/0/1/0/all/0/1\">Wilson S Geisler</a>",
          "description": "Univariate and multivariate normal probability distributions are widely used\nwhen modeling decisions under uncertainty. Computing the performance of such\nmodels requires integrating these distributions over specific domains, which\ncan vary widely across models. Besides some special cases where these integrals\nare easy to calculate, there exist no general analytical expressions, standard\nnumerical methods or software for these integrals. Here we present mathematical\nresults and open-source software that provide (i) the probability in any domain\nof a normal in any dimensions with any parameters, (ii) the probability\ndensity, cumulative distribution, and inverse cumulative distribution of any\nfunction of a normal vector, (iii) the classification errors among any number\nof normal distributions, the Bayes-optimal discriminability index and relation\nto the operating characteristic, (iv) dimension reduction and visualizations\nfor such problems, and (v) tests for how reliably these methods may be used on\ngiven data. We demonstrate these tools with vision research applications of\ndetecting occluding objects in natural scenes, and detecting camouflage.",
          "link": "http://arxiv.org/abs/2012.14331",
          "publishedOn": "2021-07-29T02:00:10.854Z",
          "wordCount": 681,
          "title": "A method to integrate and classify normal distributions. (arXiv:2012.14331v7 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhengbo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zitang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weilian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamata_S/0/1/0/all/0/1\">Sei-ichiro Kamata</a>",
          "description": "Various deep neural network architectures (DNNs) maintain massive vital\nrecords in computer vision. While drawing attention worldwide, the design of\nthe overall structure lacks general guidance. Based on the relationship between\nDNN design and numerical differential equations, we performed a fair comparison\nof the residual design with higher-order perspectives. We show that the widely\nused DNN design strategy, constantly stacking a small design (usually 2-3\nlayers), could be easily improved, supported by solid theoretical knowledge and\nwith no extra parameters needed. We reorganise the residual design in\nhigher-order ways, which is inspired by the observation that many effective\nnetworks can be interpreted as different numerical discretisations of\ndifferential equations. The design of ResNet follows a relatively simple\nscheme, which is Euler forward; however, the situation becomes complicated\nrapidly while stacking. We suppose that stacked ResNet is somehow equalled to a\nhigher-order scheme; then, the current method of forwarding propagation might\nbe relatively weak compared with a typical high-order method such as\nRunge-Kutta. We propose HO-ResNet to verify the hypothesis of widely used CV\nbenchmarks with sufficient experiments. Stable and noticeable increases in\nperformance are observed, and convergence and robustness are also improved. Our\nstacking strategy improved ResNet-30 by 2.15 per cent and ResNet-58 by 2.35 per\ncent on CIFAR-10, with the same settings and parameters. The proposed strategy\nis fundamental and theoretical and can therefore be applied to any network as a\ngeneral guideline.",
          "link": "http://arxiv.org/abs/2103.15244",
          "publishedOn": "2021-07-29T02:00:10.846Z",
          "wordCount": 727,
          "title": "Rethinking ResNets: Improved Stacking Strategies With High Order Schemes. (arXiv:2103.15244v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarussi_R/0/1/0/all/0/1\">Roei Sarussi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brutzkus_A/0/1/0/all/0/1\">Alon Brutzkus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1\">Amir Globerson</a>",
          "description": "Can a neural network minimizing cross-entropy learn linearly separable data?\nDespite progress in the theory of deep learning, this question remains\nunsolved. Here we prove that SGD globally optimizes this learning problem for a\ntwo-layer network with Leaky ReLU activations. The learned network can in\nprinciple be very complex. However, empirical evidence suggests that it often\nturns out to be approximately linear. We provide theoretical support for this\nphenomenon by proving that if network weights converge to two weight clusters,\nthis will imply an approximately linear decision boundary. Finally, we show a\ncondition on the optimization that leads to weight clustering. We provide\nempirical results that validate our theoretical analysis.",
          "link": "http://arxiv.org/abs/2101.02533",
          "publishedOn": "2021-07-29T02:00:10.759Z",
          "wordCount": 567,
          "title": "Towards Understanding Learning in Neural Networks with Linear Teachers. (arXiv:2101.02533v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Minto_L/0/1/0/all/0/1\">Lorenzo Minto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haller_M/0/1/0/all/0/1\">Moritz Haller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddadi_H/0/1/0/all/0/1\">Hamed Haddadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livshits_B/0/1/0/all/0/1\">Benjamin Livshits</a>",
          "description": "Recommender systems are commonly trained on centrally collected user\ninteraction data like views or clicks. This practice however raises serious\nprivacy concerns regarding the recommender's collection and handling of\npotentially sensitive data. Several privacy-aware recommender systems have been\nproposed in recent literature, but comparatively little attention has been\ngiven to systems at the intersection of implicit feedback and privacy. To\naddress this shortcoming, we propose a practical federated recommender system\nfor implicit data under user-level local differential privacy (LDP). The\nprivacy-utility trade-off is controlled by parameters $\\epsilon$ and $k$,\nregulating the per-update privacy budget and the number of $\\epsilon$-LDP\ngradient updates sent by each user respectively. To further protect the user's\nprivacy, we introduce a proxy network to reduce the fingerprinting surface by\nanonymizing and shuffling the reports before forwarding them to the\nrecommender. We empirically demonstrate the effectiveness of our framework on\nthe MovieLens dataset, achieving up to Hit Ratio with K=10 (HR@10) 0.68 on 50k\nusers with 5k items. Even on the full dataset, we show that it is possible to\nachieve reasonable utility with HR@10>0.5 without compromising user privacy.",
          "link": "http://arxiv.org/abs/2105.03941",
          "publishedOn": "2021-07-29T02:00:10.747Z",
          "wordCount": 666,
          "title": "Stronger Privacy for Federated Collaborative Filtering with Implicit Feedback. (arXiv:2105.03941v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01168",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_G/0/1/0/all/0/1\">Guannan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yujie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Low_S/0/1/0/all/0/1\">Steven Low</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Na Li</a>",
          "description": "With large-scale integration of renewable generation and distributed energy\nresources (DERs), modern power systems are confronted with new operational\nchallenges, such as growing complexity, increasing uncertainty, and aggravating\nvolatility. Meanwhile, more and more data are becoming available owing to the\nwidespread deployment of smart meters, smart sensors, and upgraded\ncommunication networks. As a result, data-driven control techniques, especially\nreinforcement learning (RL), have attracted surging attention in recent years.\nIn this paper, we provide a tutorial on various RL techniques and how they can\nbe applied to decision-making in power systems. We illustrate RL-based models\nand solutions in three key applications, frequency regulation, voltage control,\nand energy management. We conclude with three critical issues in the\napplication of RL, i.e., safety, scalability, and data. Several potential\nfuture directions are discussed as well.",
          "link": "http://arxiv.org/abs/2102.01168",
          "publishedOn": "2021-07-29T02:00:10.704Z",
          "wordCount": 632,
          "title": "Reinforcement Learning for Decision-Making and Control in Power Systems: Tutorial, Review, and Vision. (arXiv:2102.01168v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramabathiran_A/0/1/0/all/0/1\">Amuthan A. Ramabathiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_P/0/1/0/all/0/1\">Prabhu Ramachandran</a>",
          "description": "We introduce a class of Sparse, Physics-based, and partially Interpretable\nNeural Networks (SPINN) for solving ordinary and partial differential equations\n(PDEs). By reinterpreting a traditional meshless representation of solutions of\nPDEs we develop a class of sparse neural network architectures that are\npartially interpretable. The SPINN model we propose here serves as a seamless\nbridge between two extreme modeling tools for PDEs, namely dense neural network\nbased methods like Physics Informed Neural Networks (PINNs) and traditional\nmesh-free numerical methods, thereby providing a novel means to develop a new\nclass of hybrid algorithms that build on the best of both these viewpoints. A\nunique feature of the SPINN model that distinguishes it from other neural\nnetwork based approximations proposed earlier is that it is (i) interpretable,\nin a particular sense made precise in the work, and (ii) sparse in the sense\nthat it has much fewer connections than typical dense neural networks used for\nPDEs. Further, the SPINN algorithm implicitly encodes mesh adaptivity and is\nable to handle discontinuities in the solutions. In addition, we demonstrate\nthat Fourier series representations can also be expressed as a special class of\nSPINN and propose generalized neural network analogues of Fourier\nrepresentations. We illustrate the utility of the proposed method with a\nvariety of examples involving ordinary differential equations, elliptic,\nparabolic, hyperbolic and nonlinear partial differential equations, and an\nexample in fluid dynamics.",
          "link": "http://arxiv.org/abs/2102.13037",
          "publishedOn": "2021-07-29T02:00:10.684Z",
          "wordCount": 718,
          "title": "SPINN: Sparse, Physics-based, and partially Interpretable Neural Networks for PDEs. (arXiv:2102.13037v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05071",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yunsong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stearrett_R/0/1/0/all/0/1\">Ryan Stearrett</a>",
          "description": "A cross-benchmark has been done on three critical aspects, data imputing,\nfeature selection and regression algorithms, for machine learning based\nchemical vapor deposition (CVD) virtual metrology (VM). The result reveals that\nlinear feature selection regression algorithm would extensively under-fit the\nVM data. Data imputing is also necessary to achieve a higher prediction\naccuracy as the data availability is only ~70% when optimal accuracy is\nobtained. This work suggests a nonlinear feature selection and regression\nalgorithm combined with nearest data imputing algorithm would provide a\nprediction accuracy as high as 0.7. This would lead to 70% reduced CVD\nprocessing variation, which is believed to will lead to reduced frequency of\nphysical metrology as well as more reliable mass-produced wafer with improved\nquality.",
          "link": "http://arxiv.org/abs/2107.05071",
          "publishedOn": "2021-07-29T02:00:10.616Z",
          "wordCount": 574,
          "title": "Machine Learning based CVD Virtual Metrology in Mass Produced Semiconductor Process. (arXiv:2107.05071v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1\">Charaf Eddine Benarab</a>",
          "description": "Knowledge is acquired by humans through experience, and no boundary is set\nbetween the kinds of knowledge or skill levels we can achieve on different\ntasks at the same time. When it comes to Neural Networks, that is not the case,\nthe major breakthroughs in the field are extremely task and domain specific.\nVision and language are dealt with in separate manners, using separate methods\nand different datasets. In this work, we propose to use knowledge acquired by\nbenchmark Vision Models which are trained on ImageNet to help a much smaller\narchitecture learn to classify text. After transforming the textual data\ncontained in the IMDB dataset to gray scale images. An analysis of different\ndomains and the Transfer Learning method is carried out. Despite the challenge\nposed by the very different datasets, promising results are achieved. The main\ncontribution of this work is a novel approach which links large pretrained\nmodels on both language and vision to achieve state-of-the-art results in\ndifferent sub-fields from the original task. Without needing high compute\ncapacity resources. Specifically, Sentiment Analysis is achieved after\ntransferring knowledge between vision and language models. BERT embeddings are\ntransformed into grayscale images, these images are then used as training\nexamples for pre-trained vision models such as VGG16 and ResNet\n\nIndex Terms: BERT, Convolutional Neural Networks, Domain Adaptation, image\nclassification, Natural Language Processing, t-SNE, text classification,\nTransfer Learning",
          "link": "http://arxiv.org/abs/2106.12479",
          "publishedOn": "2021-07-29T02:00:10.346Z",
          "wordCount": 706,
          "title": "Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Waller_I/0/1/0/all/0/1\">Isaac Waller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1\">Ashton Anderson</a>",
          "description": "Optimism about the Internet's potential to bring the world together has been\ntempered by concerns about its role in inflaming the 'culture wars'. Via mass\nselection into like-minded groups, online society may be becoming more\nfragmented and polarized, particularly with respect to partisan differences.\nHowever, our ability to measure the social makeup of online communities, and in\nturn understand the social organization of online platforms, is limited by the\npseudonymous, unstructured, and large-scale nature of digital discussion. We\ndevelop a neural embedding methodology to quantify the positioning of online\ncommunities along social dimensions by leveraging large-scale patterns of\naggregate behaviour. Applying our methodology to 5.1B Reddit comments made in\n10K communities over 14 years, we measure how the macroscale community\nstructure is organized with respect to age, gender, and U.S. political\npartisanship. Examining political content, we find Reddit underwent a\nsignificant polarization event around the 2016 U.S. presidential election, and\nremained highly polarized for years afterward. Contrary to conventional wisdom,\nhowever, individual-level polarization is rare; the system-level shift in 2016\nwas disproportionately driven by the arrival of new and newly political users.\nPolitical polarization on Reddit is unrelated to previous activity on the\nplatform, and is instead temporally aligned with external events. We also\nobserve a stark ideological asymmetry, with the sharp increase in 2016 being\nentirely attributable to changes in right-wing activity. Our methodology is\nbroadly applicable to the study of online interaction, and our findings have\nimplications for the design of online platforms, understanding the social\ncontexts of online behaviour, and quantifying the dynamics and mechanisms of\nonline polarization.",
          "link": "http://arxiv.org/abs/2010.00590",
          "publishedOn": "2021-07-29T02:00:10.318Z",
          "wordCount": 746,
          "title": "Quantifying social organization and political polarization in online platforms. (arXiv:2010.00590v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smedt_J/0/1/0/all/0/1\">Johannes De Smedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeshchenko_A/0/1/0/all/0/1\">Anton Yeshchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyvyanyy_A/0/1/0/all/0/1\">Artem Polyvyanyy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weerdt_J/0/1/0/all/0/1\">Jochen De Weerdt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendling_J/0/1/0/all/0/1\">Jan Mendling</a>",
          "description": "Process analytics is an umbrella of data-driven techniques which includes\nmaking predictions for individual process instances or overall process models.\nAt the instance level, various novel techniques have been recently devised,\ntackling next activity, remaining time, and outcome prediction. At the model\nlevel, there is a notable void. It is the ambition of this paper to fill this\ngap. To this end, we develop a technique to forecast the entire process model\nfrom historical event data. A forecasted model is a will-be process model\nrepresenting a probable future state of the overall process. Such a forecast\nhelps to investigate the consequences of drift and emerging bottlenecks. Our\ntechnique builds on a representation of event data as multiple time series,\neach capturing the evolution of a behavioural aspect of the process model, such\nthat corresponding forecasting techniques can be applied. Our implementation\ndemonstrates the accuracy of our technique on real-world event log data.",
          "link": "http://arxiv.org/abs/2105.01092",
          "publishedOn": "2021-07-29T02:00:10.311Z",
          "wordCount": 634,
          "title": "Process Model Forecasting Using Time Series Analysis of Event Sequence Data. (arXiv:2105.01092v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>",
          "description": "This paper argues that training GANs on local and non-local dependencies in\nspeech data offers insights into how deep neural networks discretize continuous\ndata and how symbolic-like rule-based morphophonological processes emerge in a\ndeep convolutional architecture. Acquisition of speech has recently been\nmodeled as a dependency between latent space and data generated by GANs in\nBegu\\v{s} (2020b; arXiv:2006.03965), who models learning of a simple local\nallophonic distribution. We extend this approach to test learning of local and\nnon-local phonological processes that include approximations of morphological\nprocesses. We further parallel outputs of the model to results of a behavioral\nexperiment where human subjects are trained on the data used for training the\nGAN network. Four main conclusions emerge: (i) the networks provide useful\ninformation for computational models of speech acquisition even if trained on a\ncomparatively small dataset of an artificial grammar learning experiment; (ii)\nlocal processes are easier to learn than non-local processes, which matches\nboth behavioral data in human subjects and typology in the world's languages.\nThis paper also proposes (iii) how we can actively observe the network's\nprogress in learning and explore the effect of training steps on learning\nrepresentations by keeping latent space constant across different training\nsteps. Finally, this paper shows that (iv) the network learns to encode the\npresence of a prefix with a single latent variable; by interpolating this\nvariable, we can actively observe the operation of a non-local phonological\nprocess. The proposed technique for retrieving learning representations has\ngeneral implications for our understanding of how GANs discretize continuous\nspeech data and suggests that rule-like generalizations in the training data\nare represented as an interaction between variables in the network's latent\nspace.",
          "link": "http://arxiv.org/abs/2009.12711",
          "publishedOn": "2021-07-29T02:00:10.300Z",
          "wordCount": 761,
          "title": "Local and non-local dependency learning and emergence of rule-like representations in speech data by Deep Convolutional Generative Adversarial Networks. (arXiv:2009.12711v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07757",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Musso_D/0/1/0/all/0/1\">Daniele Musso</a>",
          "description": "Local entropic loss functions provide a versatile framework to define\narchitecture-aware regularization procedures. Besides the possibility of being\nanisotropic in the synaptic space, the local entropic smoothening of the loss\nfunction can vary during training, thus yielding a tunable model complexity. A\nscoping protocol where the regularization is strong in the early-stage of the\ntraining and then fades progressively away constitutes an alternative to\nstandard initialization procedures for deep convolutional neural networks,\nnonetheless, it has wider applicability. We analyze anisotropic, local entropic\nsmoothenings in the language of statistical physics and information theory,\nproviding insight into both their interpretation and workings. We comment some\naspects related to the physics of renormalization and the spacetime structure\nof convolutional networks.",
          "link": "http://arxiv.org/abs/2107.07757",
          "publishedOn": "2021-07-29T02:00:10.291Z",
          "wordCount": 585,
          "title": "Entropic alternatives to initialization. (arXiv:2107.07757v2 [cond-mat.dis-nn] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.00826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gautam_A/0/1/0/all/0/1\">Akshat Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sit_M/0/1/0/all/0/1\">Muhammed Sit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1\">Ibrahim Demir</a>",
          "description": "In this paper, we demonstrated a practical application of realistic river\nimage generation using deep learning. Specifically, we explored a generative\nadversarial network (GAN) model capable of generating high-resolution and\nrealistic river images that can be used to support modeling and analysis in\nsurface water estimation, river meandering, wetland loss, and other\nhydrological research studies. First, we have created an extensive repository\nof overhead river images to be used in training. Second, we incorporated the\nProgressive Growing GAN (PGGAN), a network architecture that iteratively trains\nsmaller-resolution GANs to gradually build up to a very high resolution to\ngenerate high quality (i.e., 1024x1024) synthetic river imagery. With simpler\nGAN architectures, difficulties arose in terms of exponential increase of\ntraining time and vanishing/exploding gradient issues, which the PGGAN\nimplementation seemed to significantly reduce. The results presented in this\nstudy show great promise in generating high-quality images and capturing the\ndetails of river structure and flow to support hydrological research, which\noften requires extensive imagery for model performance.",
          "link": "http://arxiv.org/abs/2003.00826",
          "publishedOn": "2021-07-29T02:00:10.284Z",
          "wordCount": 644,
          "title": "Realistic River Image Synthesis using Deep Generative Adversarial Networks. (arXiv:2003.00826v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Clarice Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kathryn Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_A/0/1/0/all/0/1\">Andrew Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_R/0/1/0/all/0/1\">Rashidul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keya_K/0/1/0/all/0/1\">Kamrun Naher Keya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foulds_J/0/1/0/all/0/1\">James Foulds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shimei Pan</a>",
          "description": "Currently, there is a surge of interest in fair Artificial Intelligence (AI)\nand Machine Learning (ML) research which aims to mitigate discriminatory bias\nin AI algorithms, e.g. along lines of gender, age, and race. While most\nresearch in this domain focuses on developing fair AI algorithms, in this work,\nwe show that a fair AI algorithm on its own may be insufficient to achieve its\nintended results in the real world. Using career recommendation as a case\nstudy, we build a fair AI career recommender by employing gender debiasing\nmachine learning techniques. Our offline evaluation showed that the debiased\nrecommender makes fairer career recommendations without sacrificing its\naccuracy. Nevertheless, an online user study of more than 200 college students\nrevealed that participants on average prefer the original biased system over\nthe debiased system. Specifically, we found that perceived gender disparity is\na determining factor for the acceptance of a recommendation. In other words,\nour results demonstrate we cannot fully address the gender bias issue in AI\nrecommendations without addressing the gender bias in humans.",
          "link": "http://arxiv.org/abs/2106.07112",
          "publishedOn": "2021-07-29T02:00:10.232Z",
          "wordCount": 649,
          "title": "User Acceptance of Gender Stereotypes in Automated Career Recommendations. (arXiv:2106.07112v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09747",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Basak_S/0/1/0/all/0/1\">Subhasish Basak</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Petit_S/0/1/0/all/0/1\">S&#xe9;bastien Petit</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bect_J/0/1/0/all/0/1\">Julien Bect</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vazquez_E/0/1/0/all/0/1\">Emmanuel Vazquez</a>",
          "description": "This article investigates the origin of numerical issues in maximum\nlikelihood parameter estimation for Gaussian process (GP) interpolation and\ninvestigates simple but effective strategies for improving commonly used\nopen-source software implementations. This work targets a basic problem but a\nhost of studies, particularly in the literature of Bayesian optimization, rely\non off-the-shelf GP implementations. For the conclusions of these studies to be\nreliable and reproducible, robust GP implementations are critical.",
          "link": "http://arxiv.org/abs/2101.09747",
          "publishedOn": "2021-07-29T02:00:10.225Z",
          "wordCount": 530,
          "title": "Numerical issues in maximum likelihood parameter estimation for Gaussian process interpolation. (arXiv:2101.09747v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cesari_T/0/1/0/all/0/1\">Tommaso R. Cesari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vecchia_R/0/1/0/all/0/1\">Riccardo Della Vecchia</a>",
          "description": "In this preliminary (and unpolished) version of the paper, we study an\nasynchronous online learning setting with a network of agents. At each time\nstep, some of the agents are activated, requested to make a prediction, and pay\nthe corresponding loss. Some feedback is then revealed to these agents and is\nlater propagated through the network. We consider the case of full, bandit, and\nsemi-bandit feedback. In particular, we construct a reduction to delayed\nsingle-agent learning that applies to both the full and the bandit feedback\ncase and allows to obtain regret guarantees for both settings. We complement\nthese results with a near-matching lower bound.",
          "link": "http://arxiv.org/abs/2106.04982",
          "publishedOn": "2021-07-29T02:00:10.218Z",
          "wordCount": 556,
          "title": "Cooperative Online Learning. (arXiv:2106.04982v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Somanath_G/0/1/0/all/0/1\">Gowri Somanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1\">Daniel Kurz</a>",
          "description": "We present a method to estimate an HDR environment map from a narrow\nfield-of-view LDR camera image in real-time. This enables perceptually\nappealing reflections and shading on virtual objects of any material finish,\nfrom mirror to diffuse, rendered into a real physical environment using\naugmented reality. Our method is based on our efficient convolutional neural\nnetwork architecture, EnvMapNet, trained end-to-end with two novel losses,\nProjectionLoss for the generated image, and ClusterLoss for adversarial\ntraining. Through qualitative and quantitative comparison to state-of-the-art\nmethods, we demonstrate that our algorithm reduces the directional error of\nestimated light sources by more than 50%, and achieves 3.7 times lower Frechet\nInception Distance (FID). We further showcase a mobile application that is able\nto run our neural network model in under 9 ms on an iPhone XS, and render in\nreal-time, visually coherent virtual objects in previously unseen real-world\nenvironments.",
          "link": "http://arxiv.org/abs/2011.10687",
          "publishedOn": "2021-07-29T02:00:10.197Z",
          "wordCount": 658,
          "title": "HDR Environment Map Estimation for Real-Time Augmented Reality. (arXiv:2011.10687v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.12278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castera_C/0/1/0/all/0/1\">Camille Castera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolte_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Bolte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevotte_C/0/1/0/all/0/1\">C&#xe9;dric F&#xe9;votte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1\">Edouard Pauwels</a>",
          "description": "We introduce a new second-order inertial optimization method for machine\nlearning called INNA. It exploits the geometry of the loss function while only\nrequiring stochastic approximations of the function values and the generalized\ngradients. This makes INNA fully implementable and adapted to large-scale\noptimization problems such as the training of deep neural networks. The\nalgorithm combines both gradient-descent and Newton-like behaviors as well as\ninertia. We prove the convergence of INNA for most deep learning problems. To\ndo so, we provide a well-suited framework to analyze deep learning loss\nfunctions involving tame optimization in which we study a continuous dynamical\nsystem together with its discrete stochastic approximations. We prove sublinear\nconvergence for the continuous-time differential inclusion which underlies our\nalgorithm. Additionally, we also show how standard optimization mini-batch\nmethods applied to non-smooth non-convex problems can yield a certain type of\nspurious stationary points never discussed before. We address this issue by\nproviding a theoretical framework around the new idea of $D$-criticality; we\nthen give a simple asymptotic analysis of INNA. Our algorithm allows for using\nan aggressive learning rate of $o(1/\\log k)$. From an empirical viewpoint, we\nshow that INNA returns competitive results with respect to state of the art\n(stochastic gradient descent, ADAGRAD, ADAM) on popular deep learning benchmark\nproblems.",
          "link": "http://arxiv.org/abs/1905.12278",
          "publishedOn": "2021-07-29T02:00:10.190Z",
          "wordCount": 733,
          "title": "An Inertial Newton Algorithm for Deep Learning. (arXiv:1905.12278v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.01845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruijun Xu</a>",
          "description": "One of the most critical problems in weight-sharing neural architecture\nsearch is the evaluation of candidate models within a predefined search space.\nIn practice, a one-shot supernet is trained to serve as an evaluator. A\nfaithful ranking certainly leads to more accurate searching results. However,\ncurrent methods are prone to making misjudgments. In this paper, we prove that\ntheir biased evaluation is due to inherent unfairness in the supernet training.\nIn view of this, we propose two levels of constraints: expectation fairness and\nstrict fairness. Particularly, strict fairness ensures equal optimization\nopportunities for all choice blocks throughout the training, which neither\noverestimates nor underestimates their capacity. We demonstrate that this is\ncrucial for improving the confidence of models' ranking. Incorporating the\none-shot supernet trained under the proposed fairness constraints with a\nmulti-objective evolutionary search algorithm, we obtain various\nstate-of-the-art models, e.g., FairNAS-A attains 77.5% top-1 validation\naccuracy on ImageNet. The models and their evaluation codes are made publicly\navailable online this http URL .",
          "link": "http://arxiv.org/abs/1907.01845",
          "publishedOn": "2021-07-29T02:00:10.158Z",
          "wordCount": 672,
          "title": "FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search. (arXiv:1907.01845v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Habibpour_M/0/1/0/all/0/1\">Maryam Habibpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gharoun_H/0/1/0/all/0/1\">Hassan Gharoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdipour_M/0/1/0/all/0/1\">Mohammadreza Mehdipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajally_A/0/1/0/all/0/1\">AmirReza Tajally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgharnezhad_H/0/1/0/all/0/1\">Hamzeh Asgharnezhad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsi_A/0/1/0/all/0/1\">Afshar Shamsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafie_Khah_M/0/1/0/all/0/1\">Miadreza Shafie-Khah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catalao_J/0/1/0/all/0/1\">Joao P.S. Catalao</a>",
          "description": "Countless research works of deep neural networks (DNNs) in the task of credit\ncard fraud detection have focused on improving the accuracy of point\npredictions and mitigating unwanted biases by building different network\narchitectures or learning models. Quantifying uncertainty accompanied by point\nestimation is essential because it mitigates model unfairness and permits\npractitioners to develop trustworthy systems which abstain from suboptimal\ndecisions due to low confidence. Explicitly, assessing uncertainties associated\nwith DNNs predictions is critical in real-world card fraud detection settings\nfor characteristic reasons, including (a) fraudsters constantly change their\nstrategies, and accordingly, DNNs encounter observations that are not generated\nby the same process as the training distribution, (b) owing to the\ntime-consuming process, very few transactions are timely checked by\nprofessional experts to update DNNs. Therefore, this study proposes three\nuncertainty quantification (UQ) techniques named Monte Carlo dropout, ensemble,\nand ensemble Monte Carlo dropout for card fraud detection applied on\ntransaction data. Moreover, to evaluate the predictive uncertainty estimates,\nUQ confusion matrix and several performance metrics are utilized. Through\nexperimental results, we show that the ensemble is more effective in capturing\nuncertainty corresponding to generated predictions. Additionally, we\ndemonstrate that the proposed UQ methods provide extra insight to the point\npredictions, leading to elevate the fraud prevention process.",
          "link": "http://arxiv.org/abs/2107.13508",
          "publishedOn": "2021-07-29T02:00:10.077Z",
          "wordCount": 663,
          "title": "Uncertainty-Aware Credit Card Fraud Detection Using Deep Learning. (arXiv:2107.13508v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13522",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Hasyim_M/0/1/0/all/0/1\">Muhammad R. Hasyim</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Batton_C/0/1/0/all/0/1\">Clay H. Batton</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Mandadapu_K/0/1/0/all/0/1\">Kranthi K. Mandadapu</a>",
          "description": "A central object in the computational studies of rare events is the committor\nfunction. Though costly to compute, the committor function encodes complete\nmechanistic information of the processes involving rare events, including\nreaction rates and transition-state ensembles. Under the framework of\ntransition path theory (TPT), recent work [1] proposes an algorithm where a\nfeedback loop couples a neural network that models the committor function with\nimportance sampling, mainly umbrella sampling, which collects data needed for\nadaptive training. In this work, we show additional modifications are needed to\nimprove the accuracy of the algorithm. The first modification adds elements of\nsupervised learning, which allows the neural network to improve its prediction\nby fitting to sample-mean estimates of committor values obtained from short\nmolecular dynamics trajectories. The second modification replaces the\ncommittor-based umbrella sampling with the finite-temperature string (FTS)\nmethod, which enables homogeneous sampling in regions where transition pathways\nare located. We test our modifications on low-dimensional systems with\nnon-convex potential energy where reference solutions can be found via\nanalytical or the finite element methods, and show how combining supervised\nlearning and the FTS method yields accurate computation of committor functions\nand reaction rates. We also provide an error analysis for algorithms that use\nthe FTS method, using which reaction rates can be accurately estimated during\ntraining with a small number of samples.",
          "link": "http://arxiv.org/abs/2107.13522",
          "publishedOn": "2021-07-29T02:00:10.028Z",
          "wordCount": 682,
          "title": "Supervised Learning and the Finite-Temperature String Method for Computing Committor Functions and Reaction Rates. (arXiv:2107.13522v1 [cond-mat.stat-mech])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guangyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>",
          "description": "EEG-based emotion recognition often requires sufficient labeled training\nsamples to build an effective computational model. Labeling EEG data, on the\nother hand, is often expensive and time-consuming. To tackle this problem and\nreduce the need for output labels in the context of EEG-based emotion\nrecognition, we propose a semi-supervised pipeline to jointly exploit both\nunlabeled and labeled data for learning EEG representations. Our\nsemi-supervised framework consists of both unsupervised and supervised\ncomponents. The unsupervised part maximizes the consistency between original\nand reconstructed input data using an autoencoder, while simultaneously the\nsupervised part minimizes the cross-entropy between the input and output\nlabels. We evaluate our framework using both a stacked autoencoder and an\nattention-based recurrent autoencoder. We test our framework on the large-scale\nSEED EEG dataset and compare our results with several other popular\nsemi-supervised methods. Our semi-supervised framework with a deep\nattention-based recurrent autoencoder consistently outperforms the benchmark\nmethods, even when small sub-sets (3\\%, 5\\% and 10\\%) of the output labels are\navailable during training, achieving a new state-of-the-art semi-supervised\nperformance.",
          "link": "http://arxiv.org/abs/2107.13505",
          "publishedOn": "2021-07-29T02:00:10.020Z",
          "wordCount": 618,
          "title": "Deep Recurrent Semi-Supervised EEG Representation Learning for Emotion Recognition. (arXiv:2107.13505v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Helou_B/0/1/0/all/0/1\">Bassam Helou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusi_A/0/1/0/all/0/1\">Aditya Dusi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collin_A/0/1/0/all/0/1\">Anne Collin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdipour_N/0/1/0/all/0/1\">Noushin Mehdipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lizarazo_C/0/1/0/all/0/1\">Cristhian Lizarazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belta_C/0/1/0/all/0/1\">Calin Belta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wongpiromsarn_T/0/1/0/all/0/1\">Tichakorn Wongpiromsarn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tebbens_R/0/1/0/all/0/1\">Radboud Duintjer Tebbens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>",
          "description": "Autonomous vehicles must balance a complex set of objectives. There is no\nconsensus on how they should do so, nor on a model for specifying a desired\ndriving behavior. We created a dataset to help address some of these questions\nin a limited operating domain. The data consists of 92 traffic scenarios, with\nmultiple ways of traversing each scenario. Multiple annotators expressed their\npreference between pairs of scenario traversals. We used the data to compare an\ninstance of a rulebook, carefully hand-crafted independently of the dataset,\nwith several interpretable machine learning models such as Bayesian networks,\ndecision trees, and logistic regression trained on the dataset. To compare\ndriving behavior, these models use scores indicating by how much different\nscenario traversals violate each of 14 driving rules. The rules are\ninterpretable and designed by subject-matter experts. First, we found that\nthese rules were enough for these models to achieve a high classification\naccuracy on the dataset. Second, we found that the rulebook provides high\ninterpretability without excessively sacrificing performance. Third, the data\npointed to possible improvements in the rulebook and the rules, and to\npotential new rules. Fourth, we explored the interpretability vs performance\ntrade-off by also training non-interpretable models such as a random forest.\nFinally, we make the dataset publicly available to encourage a discussion from\nthe wider community on behavior specification for AVs. Please find it at\ngithub.com/bassam-motional/Reasonable-Crowd.",
          "link": "http://arxiv.org/abs/2107.13507",
          "publishedOn": "2021-07-29T02:00:09.974Z",
          "wordCount": 693,
          "title": "The Reasonable Crowd: Towards evidence-based and interpretable models of driving behavior. (arXiv:2107.13507v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1901.09997",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Berahas_A/0/1/0/all/0/1\">Albert S. Berahas</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jahani_M/0/1/0/all/0/1\">Majid Jahani</a>, <a href=\"http://arxiv.org/find/math/1/au:+Richtarik_P/0/1/0/all/0/1\">Peter Richt&#xe1;rik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Takac_M/0/1/0/all/0/1\">Martin Tak&#xe1;&#x10d;</a>",
          "description": "We present two sampled quasi-Newton methods (sampled LBFGS and sampled LSR1)\nfor solving empirical risk minimization problems that arise in machine\nlearning. Contrary to the classical variants of these methods that sequentially\nbuild Hessian or inverse Hessian approximations as the optimization progresses,\nour proposed methods sample points randomly around the current iterate at every\niteration to produce these approximations. As a result, the approximations\nconstructed make use of more reliable (recent and local) information, and do\nnot depend on past iterate information that could be significantly stale. Our\nproposed algorithms are efficient in terms of accessed data points (epochs) and\nhave enough concurrency to take advantage of parallel/distributed computing\nenvironments. We provide convergence guarantees for our proposed methods.\nNumerical tests on a toy classification problem as well as on popular\nbenchmarking binary classification and neural network training tasks reveal\nthat the methods outperform their classical variants.",
          "link": "http://arxiv.org/abs/1901.09997",
          "publishedOn": "2021-07-29T02:00:09.967Z",
          "wordCount": 641,
          "title": "Quasi-Newton Methods for Machine Learning: Forget the Past, Just Sample. (arXiv:1901.09997v5 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13545",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Charles Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orbik_J/0/1/0/all/0/1\">J&#x119;drzej Orbik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devin_C/0/1/0/all/0/1\">Coline Devin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Brian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berseth_G/0/1/0/all/0/1\">Glen Berseth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "In this paper, we study how robots can autonomously learn skills that require\na combination of navigation and grasping. Learning robotic skills in the real\nworld remains challenging without large-scale data collection and supervision.\nOur aim is to devise a robotic reinforcement learning system for learning\nnavigation and manipulation together, in an \\textit{autonomous} way without\nhuman intervention, enabling continual learning under realistic assumptions.\nSpecifically, our system, ReLMM, can learn continuously on a real-world\nplatform without any environment instrumentation, without human intervention,\nand without access to privileged information, such as maps, objects positions,\nor a global view of the environment. Our method employs a modularized policy\nwith components for manipulation and navigation, where uncertainty over the\nmanipulation success drives exploration for the navigation controller, and the\nmanipulation module provides rewards for navigation. We evaluate our method on\na room cleanup task, where the robot must navigate to and pick up items of\nscattered on the floor. After a grasp curriculum training phase, ReLMM can\nlearn navigation and grasping together fully automatically, in around 40 hours\nof real-world training.",
          "link": "http://arxiv.org/abs/2107.13545",
          "publishedOn": "2021-07-29T02:00:09.959Z",
          "wordCount": 626,
          "title": "ReLMM: Practical RL for Learning Mobile Manipulation Skills Using Only Onboard Sensors. (arXiv:2107.13545v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13473",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Valenchon_N/0/1/0/all/0/1\">Nicolas Valenchon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bouteiller_Y/0/1/0/all/0/1\">Yann Bouteiller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jourde_H/0/1/0/all/0/1\">Hugo R. Jourde</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coffey_E/0/1/0/all/0/1\">Emily B.J. Coffey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beltrame_G/0/1/0/all/0/1\">Giovanni Beltrame</a>",
          "description": "Electroencephalography (EEG) is a method of measuring the brain's electrical\nactivity, using non-invasive scalp electrodes. In this article, we propose the\nPortiloop, a deep learning-based portable and low-cost device enabling the\nneuroscience community to capture EEG, process it in real time, detect patterns\nof interest, and respond with precisely-timed stimulation. The core of the\nPortiloop is a System on Chip composed of an Analog to Digital Converter (ADC)\nand a Field-Programmable Gate Array (FPGA). After being converted to digital by\nthe ADC, the EEG signal is processed in the FPGA. The FPGA contains an ad-hoc\nArtificial Neural Network (ANN) with convolutional and recurrent units,\ndirectly implemented in hardware. The output of the ANN is then used to trigger\nthe user-defined feedback. We use the Portiloop to develop a real-time sleep\nspindle stimulating application, as a case study. Sleep spindles are a specific\ntype of transient oscillation ($\\sim$2.5 s, 12-16 Hz) that are observed in EEG\nrecordings, and are related to memory consolidation during sleep. We tested the\nPortiloop's capacity to detect and stimulate sleep spindles in real time using\nan existing database of EEG sleep recordings. With 71% for both precision and\nrecall as compared with expert labels, the system is able to stimulate spindles\nwithin $\\sim$300 ms of their onset, enabling experimental manipulation of early\nthe entire spindle. The Portiloop can be extended to detect and stimulate other\nneural events in EEG. It is fully available to the research community as an\nopen science project.",
          "link": "http://arxiv.org/abs/2107.13473",
          "publishedOn": "2021-07-29T02:00:09.840Z",
          "wordCount": 721,
          "title": "The Portiloop: a deep learning-based open science tool for closed-loop brain stimulation. (arXiv:2107.13473v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/1811.11891",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Koelle_S/0/1/0/all/0/1\">Samson Koelle</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1\">Hanyu Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1\">Marina Meila</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Chia Chen</a>",
          "description": "Manifold embedding algorithms map high-dimensional data down to coordinates\nin a much lower-dimensional space. One of the aims of dimension reduction is to\nfind intrinsic coordinates that describe the data manifold. The coordinates\nreturned by the embedding algorithm are abstract, and finding their physical or\ndomain-related meaning is not formalized and often left to domain experts. This\npaper studies the problem of recovering the meaning of the new low-dimensional\nrepresentation in an automatic, principled fashion. We propose a method to\nexplain embedding coordinates of a manifold as non-linear compositions of\nfunctions from a user-defined dictionary. We show that this problem can be set\nup as a sparse linear Group Lasso recovery problem, find sufficient recovery\nconditions, and demonstrate its effectiveness on data.",
          "link": "http://arxiv.org/abs/1811.11891",
          "publishedOn": "2021-07-29T02:00:09.832Z",
          "wordCount": 567,
          "title": "Manifold Coordinates with Physical Meaning. (arXiv:1811.11891v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.00570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tsung-Hui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Ying Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jong-Shi Pang</a>",
          "description": "The non-negative matrix factorization (NMF) model with an additional\northogonality constraint on one of the factor matrices, called the orthogonal\nNMF (ONMF), has been found a promising clustering model and can outperform the\nclassical K-means. However, solving the ONMF model is a challenging\noptimization problem because the coupling of the orthogonality and\nnon-negativity constraints introduces a mixed combinatorial aspect into the\nproblem due to the determination of the correct status of the variables\n(positive or zero). Most of the existing methods directly deal with the\northogonality constraint in its original form via various optimization\ntechniques, but are not scalable for large-scale problems. In this paper, we\npropose a new ONMF based clustering formulation that equivalently transforms\nthe orthogonality constraint into a set of norm-based non-convex equality\nconstraints. We then apply a non-convex penalty (NCP) approach to add them to\nthe objective as penalty terms, leading to a problem that is efficiently\nsolvable. One smooth penalty formulation and one non-smooth penalty formulation\nare respectively studied. We build theoretical conditions for the penalized\nproblems to provide feasible stationary solutions to the ONMF based clustering\nproblem, as well as proposing efficient algorithms for solving the penalized\nproblems of the two NCP methods. Experimental results based on both synthetic\nand real datasets are presented to show that the proposed NCP methods are\ncomputationally time efficient, and either match or outperform the existing\nK-means and ONMF based methods in terms of the clustering performance.",
          "link": "http://arxiv.org/abs/1906.00570",
          "publishedOn": "2021-07-29T02:00:09.825Z",
          "wordCount": 732,
          "title": "Clustering by Orthogonal NMF Model and Non-Convex Penalty Optimization. (arXiv:1906.00570v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polgreen_E/0/1/0/all/0/1\">Elizabeth Polgreen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reynolds_A/0/1/0/all/0/1\">Andrew Reynolds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seshia_S/0/1/0/all/0/1\">Sanjit A. Seshia</a>",
          "description": "In classic program synthesis algorithms, such as counterexample-guided\ninductive synthesis (CEGIS), the algorithms alternate between a synthesis phase\nand an oracle (verification) phase. Many synthesis algorithms use a white-box\noracle based on satisfiability modulo theory (SMT) solvers to provide\ncounterexamples. But what if a white-box oracle is either not available or not\neasy to work with? We present a framework for solving a general class of\noracle-guided synthesis problems which we term synthesis modulo oracles. In\nthis setting, oracles may be black boxes with a query-response interface\ndefined by the synthesis problem. As a necessary component of this framework,\nwe also formalize the problem of satisfiability modulo theories and oracles,\nand present an algorithm for solving this problem. We implement a prototype\nsolver for satisfiability and synthesis modulo oracles and demonstrate that, by\nusing oracles that execute functions not easily modeled in SMT-constraints,\nsuch as recursive functions or oracles that incorporate compilation and\nexecution of code, SMTO and SyMO are able to solve problems beyond the\nabilities of standard SMT and synthesis solvers.",
          "link": "http://arxiv.org/abs/2107.13477",
          "publishedOn": "2021-07-29T02:00:09.813Z",
          "wordCount": 613,
          "title": "Satisfiability and Synthesis Modulo Oracles. (arXiv:2107.13477v1 [cs.LO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kummer_L/0/1/0/all/0/1\">Lorenz Kummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidak_K/0/1/0/all/0/1\">Kevin Sidak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichmann_T/0/1/0/all/0/1\">Tabea Reichmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gansterer_W/0/1/0/all/0/1\">Wilfried Gansterer</a>",
          "description": "Quantization is a technique for reducing deep neural networks (DNNs) training\nand inference times, which is crucial for training in resource constrained\nenvironments or time critical inference applications. State-of-the-art (SOTA)\nquantization approaches focus on post-training quantization, i.e. quantization\nof pre-trained DNNs for speeding up inference. Very little work on quantized\ntraining exists, which neither al-lows dynamic intra-epoch precision switches\nnor em-ploys an information theory based switching heuristic. Usually, existing\napproaches require full precision refinement afterwards and enforce a global\nword length across the whole DNN. This leads to suboptimal quantization\nmappings and resource usage. Recognizing these limits, we introduce MARViN, a\nnew quantized training strategy using information theory-based intra-epoch\nprecision switching, which decides on a per-layer basis which precision should\nbe used in order to minimize quantization-induced information loss. Note that\nany quantization must leave enough precision such that future learning steps do\nnot suffer from vanishing gradients. We achieve an average speedup of 1.86\ncompared to a float32 basis while limiting mean accuracy degradation on\nAlexNet/ResNet to only -0.075%.",
          "link": "http://arxiv.org/abs/2107.13490",
          "publishedOn": "2021-07-29T02:00:09.804Z",
          "wordCount": 612,
          "title": "MARViN -- Multiple Arithmetic Resolutions Vacillating in Neural Networks. (arXiv:2107.13490v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogozhnikov_A/0/1/0/all/0/1\">Alex Rogozhnikov</a>",
          "description": "Without positional information, attention-based transformer neural networks\nare permutation-invariant. Absolute or relative positional embeddings are the\nmost popular ways to feed transformer models positional information. Absolute\npositional embeddings are simple to implement, but suffer from generalization\nissues when evaluating on sequences of different length than those seen at\ntraining time. Relative positions are more robust to length change, but are\nmore complex to implement and yield inferior model throughput. In this paper,\nwe propose an augmentation-based approach (CAPE) for absolute positional\nembeddings, which keeps the advantages of both absolute (simplicity and speed)\nand relative position embeddings (better generalization). In addition, our\nempirical evaluation on state-of-the-art models in machine translation, image\nand speech recognition demonstrates that CAPE leads to better generalization\nperformance as well as increased stability with respect to training\nhyper-parameters.",
          "link": "http://arxiv.org/abs/2106.03143",
          "publishedOn": "2021-07-29T02:00:09.699Z",
          "wordCount": 607,
          "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings. (arXiv:2106.03143v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Terry_J/0/1/0/all/0/1\">J. K. Terry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayakumar_M/0/1/0/all/0/1\">Mario Jayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alwis_K/0/1/0/all/0/1\">Kusal De Alwis</a>",
          "description": "The general approach taken when training deep learning classifiers is to save\nthe parameters after every few iterations, train until either a human observer\nor a simple metric-based heuristic decides the network isn't learning anymore,\nand then backtrack and pick the saved parameters with the best validation\naccuracy. Simple methods are used to determine if a neural network isn't\nlearning anymore because, as long as it's well after the optimal values are\nfound, the condition doesn't impact the final accuracy of the model. However\nfrom a runtime perspective, this is of great significance to the many cases\nwhere numerous neural networks are trained simultaneously (e.g. hyper-parameter\ntuning). Motivated by this, we introduce a statistical significance test to\ndetermine if a neural network has stopped learning. This stopping criterion\nappears to represent a happy medium compared to other popular stopping\ncriterions, achieving comparable accuracy to the criterions that achieve the\nhighest final accuracies in 77% or fewer epochs, while the criterions which\nstop sooner do so with an appreciable loss to final accuracy. Additionally, we\nuse this as the basis of a new learning rate scheduler, removing the need to\nmanually choose learning rate schedules and acting as a quasi-line search,\nachieving superior or comparable empirical performance to existing methods.",
          "link": "http://arxiv.org/abs/2103.01205",
          "publishedOn": "2021-07-29T02:00:09.681Z",
          "wordCount": 691,
          "title": "Statistically Significant Stopping of Neural Network Training. (arXiv:2103.01205v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1\">Spandan Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1\">Timothy Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dozier_J/0/1/0/all/0/1\">Jamell Dozier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_H/0/1/0/all/0/1\">Helen Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandari_N/0/1/0/all/0/1\">Nishchal Bhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1\">Fr&#xe9;do Durand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>",
          "description": "Object recognition and viewpoint estimation lie at the heart of visual\nunderstanding. Recent works suggest that convolutional neural networks (CNNs)\nfail to generalize to out-of-distribution (OOD) category-viewpoint\ncombinations, ie. combinations not seen during training. In this paper, we\ninvestigate when and how such OOD generalization may be possible by evaluating\nCNNs trained to classify both object category and 3D viewpoint on OOD\ncombinations, and identifying the neural mechanisms that facilitate such OOD\ngeneralization. We show that increasing the number of in-distribution\ncombinations (ie. data diversity) substantially improves generalization to OOD\ncombinations, even with the same amount of training data. We compare learning\ncategory and viewpoint in separate and shared network architectures, and\nobserve starkly different trends on in-distribution and OOD combinations, ie.\nwhile shared networks are helpful in-distribution, separate networks\nsignificantly outperform shared ones at OOD combinations. Finally, we\ndemonstrate that such OOD generalization is facilitated by the neural mechanism\nof specialization, ie. the emergence of two types of neurons -- neurons\nselective to category and invariant to viewpoint, and vice versa.",
          "link": "http://arxiv.org/abs/2007.08032",
          "publishedOn": "2021-07-29T02:00:09.674Z",
          "wordCount": 654,
          "title": "When and how do CNNs generalize to out-of-distribution category-viewpoint combinations?. (arXiv:2007.08032v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Wenxuan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haiping Huang</a>",
          "description": "The geometric structure of an optimization landscape is argued to be\nfundamentally important to support the success of deep neural network learning.\nA direct computation of the landscape beyond two layers is hard. Therefore, to\ncapture the global view of the landscape, an interpretable model of the\nnetwork-parameter (or weight) space must be established. However, the model is\nlacking so far. Furthermore, it remains unknown what the landscape looks like\nfor deep networks of binary synapses, which plays a key role in robust and\nenergy efficient neuromorphic computation. Here, we propose a statistical\nmechanics framework by directly building a least structured model of the\nhigh-dimensional weight space, considering realistic structured data,\nstochastic gradient descent training, and the computational depth of neural\nnetworks. We also consider whether the number of network parameters outnumbers\nthe number of supplied training data, namely, over- or under-parametrization.\nOur least structured model reveals that the weight spaces of the\nunder-parametrization and over-parameterization cases belong to the same class,\nin the sense that these weight spaces are well-connected without any\nhierarchical clustering structure. In contrast, the shallow-network has a\nbroken weight space, characterized by a discontinuous phase transition, thereby\nclarifying the benefit of depth in deep learning from the angle of high\ndimensional geometry. Our effective model also reveals that inside a deep\nnetwork, there exists a liquid-like central part of the architecture in the\nsense that the weights in this part behave as randomly as possible, providing\nalgorithmic implications. Our data-driven model thus provides a statistical\nmechanics insight about why deep learning is unreasonably effective in terms of\nthe high-dimensional weight space, and how deep networks are different from\nshallow ones.",
          "link": "http://arxiv.org/abs/2007.08093",
          "publishedOn": "2021-07-29T02:00:09.666Z",
          "wordCount": 754,
          "title": "Data-driven effective model shows a liquid-like deep learning. (arXiv:2007.08093v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1\">Maria Pia Del Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiller_D/0/1/0/all/0/1\">Dario Spiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1\">Pierre Philippe Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "In recent years, the growth of Machine Learning (ML) algorithms has raised\nthe number of studies including their applicability in a variety of different\nscenarios. Among all, one of the hardest ones is the aerospace, due to its\npeculiar physical requirements. In this context, a feasibility study and a\nfirst prototype for an Artificial Intelligence (AI) model to be deployed on\nboard satellites are presented in this work. As a case study, the detection of\nvolcanic eruptions has been investigated as a method to swiftly produce alerts\nand allow immediate interventions. Two Convolutional Neural Networks (CNNs)\nhave been proposed and designed, showing how to efficiently implement them for\nidentifying the eruptions and at the same time adapting their complexity in\norder to fit on board requirements.",
          "link": "http://arxiv.org/abs/2106.15281",
          "publishedOn": "2021-07-29T02:00:09.647Z",
          "wordCount": 602,
          "title": "On Board Volcanic Eruption Detection through CNNs and Satellite Multispectral Imagery. (arXiv:2106.15281v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1808.09670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fouillen_E/0/1/0/all/0/1\">Erwan Fouillen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyer_C/0/1/0/all/0/1\">Claire Boyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangnier_M/0/1/0/all/0/1\">Maxime Sangnier</a>",
          "description": "Gradient boosting is a prediction method that iteratively combines weak\nlearners to produce a complex and accurate model. From an optimization point of\nview, the learning procedure of gradient boosting mimics a gradient descent on\na functional variable. This paper proposes to build upon the proximal point\nalgorithm, when the empirical risk to minimize is not differentiable, in order\nto introduce a novel boosting approach, called proximal boosting. Besides being\nmotivated by non-differentiable optimization, the proposed algorithm benefits\nfrom algorithmic improvements such as controlling the approximation error and\nNesterov's acceleration, in the same way as gradient boosting [Grubb and\nBagnell, 2011, Biau et al., 2018]. This leads to two variants, respectively\ncalled residual proximal boosting and accelerated proximal boosting.\nTheoretical convergence is proved for the first two procedures under different\nhypotheses on the empirical risk and advantages of leveraging proximal methods\nfor boosting are illustrated by numerical experiments on simulated and\nreal-world data. In particular, we exhibit a favorable comparison over gradient\nboosting regarding convergence rate and prediction accuracy.",
          "link": "http://arxiv.org/abs/1808.09670",
          "publishedOn": "2021-07-29T02:00:09.640Z",
          "wordCount": 631,
          "title": "Proximal boosting and variants. (arXiv:1808.09670v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hammoudeh_A/0/1/0/all/0/1\">Ahmad Hammoudeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tedmori_S/0/1/0/all/0/1\">Sara Tedmori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obeid_N/0/1/0/all/0/1\">Nadim Obeid</a>",
          "description": "Although learning from data is effective and has achieved significant\nmilestones, it has many challenges and limitations. Learning from data starts\nfrom observations and then proceeds to broader generalizations. This framework\nis controversial in science, yet it has achieved remarkable engineering\nsuccesses. This paper reflects on some epistemological issues and some of the\nlimitations of the knowledge discovered in data. The document discusses the\ncommon perception that getting more data is the key to achieving better machine\nlearning models from theoretical and practical perspectives. The paper sheds\nsome light on the shortcomings of using generic mathematical theories to\ndescribe the process. It further highlights the need for theories specialized\nin learning from data. While more data leverages the performance of machine\nlearning models in general, the relation in practice is shown to be logarithmic\nat its best; After a specific limit, more data stabilize or degrade the machine\nlearning models. Recent work in reinforcement learning showed that the trend is\nshifting away from data-oriented approaches and relying more on algorithms. The\npaper concludes that learning from data is hindered by many limitations. Hence\nan approach that has an intensional orientation is needed.",
          "link": "http://arxiv.org/abs/2107.13270",
          "publishedOn": "2021-07-29T02:00:09.633Z",
          "wordCount": 626,
          "title": "A Reflection on Learning from Data: Epistemology Issues and Limitations. (arXiv:2107.13270v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1\">Sam Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_R/0/1/0/all/0/1\">Raluca Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1\">Ida Momennejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rzepecki_J/0/1/0/all/0/1\">Jaroslaw Rzepecki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuniga_E/0/1/0/all/0/1\">Evelyn Zuniga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costello_G/0/1/0/all/0/1\">Gavin Costello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leroy_G/0/1/0/all/0/1\">Guy Leroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_A/0/1/0/all/0/1\">Ali Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>",
          "description": "A key challenge on the path to developing agents that learn complex\nhuman-like behavior is the need to quickly and accurately quantify\nhuman-likeness. While human assessments of such behavior can be highly\naccurate, speed and scalability are limited. We address these limitations\nthrough a novel automated Navigation Turing Test (ANTT) that learns to predict\nhuman judgments of human-likeness. We demonstrate the effectiveness of our\nautomated NTT on a navigation task in a complex 3D environment. We investigate\nsix classification models to shed light on the types of architectures best\nsuited to this task, and validate them against data collected through a human\nNTT. Our best models achieve high accuracy when distinguishing true human and\nagent behavior. At the same time, we show that predicting finer-grained human\nassessment of agents' progress towards human-like behavior remains unsolved.\nOur work takes an important step towards agents that more effectively learn\ncomplex human-like behavior.",
          "link": "http://arxiv.org/abs/2105.09637",
          "publishedOn": "2021-07-29T02:00:09.626Z",
          "wordCount": 660,
          "title": "Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation. (arXiv:2105.09637v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Farwa K. Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flanagan_A/0/1/0/all/0/1\">Adrian Flanagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kuan E. Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alamgir_Z/0/1/0/all/0/1\">Zareen Alamgir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammad_Ud_Din_M/0/1/0/all/0/1\">Muhammad Ammad-Ud-Din</a>",
          "description": "We introduce the payload optimization method for federated recommender\nsystems (FRS). In federated learning (FL), the global model payload that is\nmoved between the server and users depends on the number of items to recommend.\nThe model payload grows when there is an increasing number of items. This\nbecomes challenging for an FRS if it is running in production mode. To tackle\nthe payload challenge, we formulated a multi-arm bandit solution that selected\npart of the global model and transmitted it to all users. The selection process\nwas guided by a novel reward function suitable for FL systems. So far as we are\naware, this is the first optimization method that seeks to address item\ndependent payloads. The method was evaluated using three benchmark\nrecommendation datasets. The empirical validation confirmed that the proposed\nmethod outperforms the simpler methods that do not benefit from the bandits for\nthe purpose of item selection. In addition, we have demonstrated the usefulness\nof our proposed method by rigorously evaluating the effects of a payload\nreduction on the recommendation performance degradation. Our method achieved up\nto a 90\\% reduction in model payload, yielding only a $\\sim$4\\% - 8\\% loss in\nthe recommendation performance for highly sparse datasets",
          "link": "http://arxiv.org/abs/2107.13078",
          "publishedOn": "2021-07-29T02:00:09.606Z",
          "wordCount": 674,
          "title": "A Payload Optimization Method for Federated Recommender Systems. (arXiv:2107.13078v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feeney_P/0/1/0/all/0/1\">Patrick Feeney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1\">Michael C. Hughes</a>",
          "description": "The pixelwise reconstruction error of deep autoencoders is often utilized for\nimage novelty detection and localization under the assumption that pixels with\nhigh error indicate which parts of the input image are unfamiliar and therefore\nlikely to be novel. This assumed correlation between pixels with high\nreconstruction error and novel regions of input images has not been verified\nand may limit the accuracy of these methods. In this paper we utilize saliency\nmaps to evaluate whether this correlation exists. Saliency maps reveal directly\nhow much a change in each input pixel would affect reconstruction loss, while\neach pixel's reconstruction error may be attributed to many input pixels when\nlayers are fully connected. We compare saliency maps to reconstruction error\nmaps via qualitative visualizations as well as quantitative correspondence\nbetween the top K elements of the maps for both novel and normal images. Our\nresults indicate that reconstruction error maps do not closely correlate with\nthe importance of pixels in the input images, making them insufficient for\nnovelty localization.",
          "link": "http://arxiv.org/abs/2107.13379",
          "publishedOn": "2021-07-29T02:00:09.599Z",
          "wordCount": 606,
          "title": "Evaluating the Use of Reconstruction Error for Novelty Localization. (arXiv:2107.13379v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yue Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Liangxiu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleerekoper_A/0/1/0/all/0/1\">Anthony Kleerekoper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Sheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tongle Hu</a>",
          "description": "Late blight disease is one of the most destructive diseases in potato crop,\nleading to serious yield losses globally. Accurate diagnosis of the disease at\nearly stage is critical for precision disease control and management. Current\nfarm practices in crop disease diagnosis are based on manual visual inspection,\nwhich is costly, time consuming, subject to individual bias. Recent advances in\nimaging sensors (e.g. RGB, multiple spectral and hyperspectral cameras), remote\nsensing and machine learning offer the opportunity to address this challenge.\nParticularly, hyperspectral imagery (HSI) combining with machine learning/deep\nlearning approaches is preferable for accurately identifying specific plant\ndiseases because the HSI consists of a wide range of high-quality reflectance\ninformation beyond human vision, capable of capturing both spectral-spatial\ninformation. The proposed method considers the potential disease specific\nreflectance radiation variance caused by the canopy structural diversity,\nintroduces the multiple capsule layers to model the hierarchical structure of\nthe spectral-spatial disease attributes with the encapsulated features to\nrepresent the various classes and the rotation invariance of the disease\nattributes in the feature space. We have evaluated the proposed method with the\nreal UAV-based HSI data under the controlled field conditions. The\neffectiveness of the hierarchical features has been quantitatively assessed and\ncompared with the existing representative machine learning/deep learning\nmethods. The experiment results show that the proposed model significantly\nimproves the accuracy performance when considering hierarchical-structure of\nspectral-spatial features, comparing to the existing methods only using\nspectral, or spatial or spectral-spatial features without consider\nhierarchical-structure of spectral-spatial features.",
          "link": "http://arxiv.org/abs/2107.13277",
          "publishedOn": "2021-07-29T02:00:09.592Z",
          "wordCount": 707,
          "title": "A Novel CropdocNet for Automated Potato Late Blight Disease Detection from the Unmanned Aerial Vehicle-based Hyperspectral Imagery. (arXiv:2107.13277v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yihong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Sheng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_S/0/1/0/all/0/1\">Shunmei Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_X/0/1/0/all/0/1\">Xiaoxiao Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chao Yan</a>",
          "description": "Edge computing enabled smart greenhouse is a representative application of\nInternet of Things technology, which can monitor the environmental information\nin real time and employ the information to contribute to intelligent\ndecision-making. In the process, anomaly detection for wireless sensor data\nplays an important role. However, traditional anomaly detection algorithms\noriginally designed for anomaly detection in static data have not properly\nconsidered the inherent characteristics of data stream produced by wireless\nsensor such as infiniteness, correlations and concept drift, which may pose a\nconsiderable challenge on anomaly detection based on data stream, and lead to\nlow detection accuracy and efficiency. First, data stream usually generates\nquickly which means that it is infinite and enormous, so any traditional\noff-line anomaly detection algorithm that attempts to store the whole dataset\nor to scan the dataset multiple times for anomaly detection will run out of\nmemory space. Second, there exist correlations among different data streams,\nwhich traditional algorithms hardly consider. Third, the underlying data\ngeneration process or data distribution may change over time. Thus, traditional\nanomaly detection algorithms with no model update will lose their effects.\nConsidering these issues, a novel method (called DLSHiForest) on basis of\nLocality-Sensitive Hashing and time window technique in this paper is proposed\nto solve these problems while achieving accurate and efficient detection.\nComprehensive experiments are executed using real-world agricultural greenhouse\ndataset to demonstrate the feasibility of our approach. Experimental results\nshow that our proposal is practicable in addressing challenges of traditional\nanomaly detection while ensuring accuracy and efficiency.",
          "link": "http://arxiv.org/abs/2107.13353",
          "publishedOn": "2021-07-29T02:00:09.585Z",
          "wordCount": 715,
          "title": "Fast Wireless Sensor Anomaly Detection based on Data Stream in Edge Computing Enabled Smart Greenhouse. (arXiv:2107.13353v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merlo_E/0/1/0/all/0/1\">Ettore Merlo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marhaba_M/0/1/0/all/0/1\">Mira Marhaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1\">Foutse Khomh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braiek_H/0/1/0/all/0/1\">Houssem Ben Braiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniol_G/0/1/0/all/0/1\">Giuliano Antoniol</a>",
          "description": "Neural network test cases are meant to exercise different reasoning paths in\nan architecture and used to validate the prediction outcomes. In this paper, we\nintroduce \"computational profiles\" as vectors of neuron activation levels. We\ninvestigate the distribution of computational profile likelihood of metamorphic\ntest cases with respect to the likelihood distributions of training, test and\nerror control cases. We estimate the non-parametric probability densities of\nneuron activation levels for each distinct output class. Probabilities are\ninferred using training cases only, without any additional knowledge about\nmetamorphic test cases. Experiments are performed by training a network on the\nMNIST Fashion library of images and comparing prediction likelihoods with those\nobtained from error control-data and from metamorphic test cases. Experimental\nresults show that the distributions of computational profile likelihood for\ntraining and test cases are somehow similar, while the distribution of the\nrandom-noise control-data is always remarkably lower than the observed one for\nthe training and testing sets. In contrast, metamorphic test cases show a\nprediction likelihood that lies in an extended range with respect to training,\ntests, and random noise. Moreover, the presented approach allows the\nindependent assessment of different training classes and experiments to show\nthat some of the classes are more sensitive to misclassifying metamorphic test\ncases than other classes. In conclusion, metamorphic test cases represent very\naggressive tests for neural network architectures. Furthermore, since\nmetamorphic test cases force a network to misclassify those inputs whose\nlikelihood is similar to that of training cases, they could also be considered\nas adversarial attacks that evade defenses based on computational profile\nlikelihood evaluation.",
          "link": "http://arxiv.org/abs/2107.13491",
          "publishedOn": "2021-07-29T02:00:09.561Z",
          "wordCount": 717,
          "title": "Models of Computational Profiles to Study the Likelihood of DNN Metamorphic Test Cases. (arXiv:2107.13491v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13530",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kessler_S/0/1/0/all/0/1\">Samuel Kessler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thomas_B/0/1/0/all/0/1\">Bethan Thomas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karout_S/0/1/0/all/0/1\">Salah Karout</a>",
          "description": "We present a method for continual learning of speech representations for\nmultiple languages using self-supervised learning (SSL) and applying these for\nautomatic speech recognition. There is an abundance of unannotated speech, so\ncreating self-supervised representations from raw audio and finetuning on a\nsmall annotated datasets is a promising direction to build speech recognition\nsystems. Wav2vec models perform SSL on raw audio in a pretraining phase and\nthen finetune on a small fraction of annotated data. SSL models have produced\nstate of the art results for ASR. However, these models are very expensive to\npretrain with self-supervision. We tackle the problem of learning new language\nrepresentations continually from audio without forgetting a previous language\nrepresentation. We use ideas from continual learning to transfer knowledge from\na previous task to speed up pretraining a new language task. Our\ncontinual-wav2vec2 model can decrease pretraining times by 32% when learning a\nnew language task, and learn this new audio-language representation without\nforgetting previous language representation.",
          "link": "http://arxiv.org/abs/2107.13530",
          "publishedOn": "2021-07-29T02:00:09.543Z",
          "wordCount": 634,
          "title": "Continual-wav2vec2: an Application of Continual Learning for Self-Supervised Automatic Speech Recognition. (arXiv:2107.13530v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_D/0/1/0/all/0/1\">Daniel Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stapleton_L/0/1/0/all/0/1\">Logan Stapleton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syrgkanis_V/0/1/0/all/0/1\">Vasilis Syrgkanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiwei Steven Wu</a>",
          "description": "Randomized experiments can be susceptible to selection bias due to potential\nnon-compliance by the participants. While much of the existing work has studied\ncompliance as a static behavior, we propose a game-theoretic model to study\ncompliance as dynamic behavior that may change over time. In rounds, a social\nplanner interacts with a sequence of heterogeneous agents who arrive with their\nunobserved private type that determines both their prior preferences across the\nactions (e.g., control and treatment) and their baseline rewards without taking\nany treatment. The planner provides each agent with a randomized recommendation\nthat may alter their beliefs and their action selection. We develop a novel\nrecommendation mechanism that views the planner's recommendation as a form of\ninstrumental variable (IV) that only affects an agents' action selection, but\nnot the observed rewards. We construct such IVs by carefully mapping the\nhistory -- the interactions between the planner and the previous agents -- to a\nrandom recommendation. Even though the initial agents may be completely\nnon-compliant, our mechanism can incentivize compliance over time, thereby\nenabling the estimation of the treatment effect of each treatment, and\nminimizing the cumulative regret of the planner whose goal is to identify the\noptimal treatment.",
          "link": "http://arxiv.org/abs/2107.10093",
          "publishedOn": "2021-07-29T02:00:09.515Z",
          "wordCount": 671,
          "title": "Incentivizing Compliance with Algorithmic Instruments. (arXiv:2107.10093v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lingam_V/0/1/0/all/0/1\">Vijay Lingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragesh_R/0/1/0/all/0/1\">Rahul Ragesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1\">Arun Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellamanickam_S/0/1/0/all/0/1\">Sundararajan Sellamanickam</a>",
          "description": "Graph Neural Networks (GNNs) exhibit excellent performance when graphs have\nstrong homophily property, i.e. connected nodes have the same labels. However,\nthey perform poorly on heterophilic graphs. Several approaches address the\nissue of heterophily by proposing models that adapt the graph by optimizing\ntask-specific loss function using labelled data. These adaptations are made\neither via attention or by attenuating or enhancing various\nlow-frequency/high-frequency signals, as needed for the task at hand. More\nrecent approaches adapt the eigenvalues of the graph. One important\ninterpretation of this adaptation is that these models select/weigh the\neigenvectors of the graph. Based on this interpretation, we present an\neigendecomposition based approach and propose EigenNetwork models that improve\nthe performance of GNNs on heterophilic graphs. Performance improvement is\nachieved by learning flexible graph adaptation functions that modulate the\neigenvalues of the graph. Regularization of these functions via parameter\nsharing helps to improve the performance even more. Our approach achieves up to\n11% improvement in performance over the state-of-the-art methods on\nheterophilic graphs.",
          "link": "http://arxiv.org/abs/2107.13312",
          "publishedOn": "2021-07-29T02:00:09.507Z",
          "wordCount": 610,
          "title": "Effective Eigendecomposition based Graph Adaptation for Heterophilic Networks. (arXiv:2107.13312v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13157",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Trivedi_A/0/1/0/all/0/1\">Anusua Trivedi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Desbiens_J/0/1/0/all/0/1\">Jocelyn Desbiens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gross_R/0/1/0/all/0/1\">Ron Gross</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dodhia_R/0/1/0/all/0/1\">Rahul Dodhia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferres_J/0/1/0/all/0/1\">Juan Lavista Ferres</a>",
          "description": "Purpose: To demonstrate that retinal microvasculature per se is a reliable\nbiomarker for Diabetic Retinopathy (DR) and, by extension, cardiovascular\ndiseases. Methods: Deep Learning Convolutional Neural Networks (CNN) applied to\ncolor fundus images for semantic segmentation of the blood vessels and severity\nclassification on both vascular and full images. Vessel reconstruction through\nharmonic descriptors is also used as a smoothing and de-noising tool. The\nmathematical background of the theory is also outlined. Results: For diabetic\npatients, at least 93.8% of DR No-Refer vs. Refer classification can be related\nto vasculature defects. As for the Non-Sight Threatening vs. Sight Threatening\ncase, the ratio is as high as 96.7%. Conclusion: In the case of DR, most of the\ndisease biomarkers are related topologically to the vasculature. Translational\nRelevance: Experiments conducted on eye blood vasculature reconstruction as a\nbiomarker shows a strong correlation between vasculature shape and later stages\nof DR.",
          "link": "http://arxiv.org/abs/2107.13157",
          "publishedOn": "2021-07-29T02:00:09.498Z",
          "wordCount": 603,
          "title": "Retinal Microvasculature as Biomarker for Diabetes and Cardiovascular Diseases. (arXiv:2107.13157v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramamonjison_R/0/1/0/all/0/1\">Rindra Ramamonjison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xinyu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiaolong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>",
          "description": "This paper presents a Simple and effective unsupervised adaptation method for\nRobust Object Detection (SimROD). To overcome the challenging issues of domain\nshift and pseudo-label noise, our method integrates a novel domain-centric\naugmentation method, a gradual self-labeling adaptation procedure, and a\nteacher-guided fine-tuning mechanism. Using our method, target domain samples\ncan be leveraged to adapt object detection models without changing the model\narchitecture or generating synthetic data. When applied to image corruptions\nand high-level cross-domain adaptation benchmarks, our method outperforms prior\nbaselines on multiple domain adaptation benchmarks. SimROD achieves new\nstate-of-the-art on standard real-to-synthetic and cross-camera setup\nbenchmarks. On the image corruption benchmark, models adapted with our method\nachieved a relative robustness improvement of 15-25% AP50 on Pascal-C and 5-6%\nAP on COCO-C and Cityscapes-C. On the cross-domain benchmark, our method\noutperformed the best baseline performance by up to 8% AP50 on Comic dataset\nand up to 4% on Watercolor dataset.",
          "link": "http://arxiv.org/abs/2107.13389",
          "publishedOn": "2021-07-29T02:00:09.477Z",
          "wordCount": 612,
          "title": "SimROD: A Simple Adaptation Method for Robust Object Detection. (arXiv:2107.13389v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13419",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Devi_T/0/1/0/all/0/1\">Thangjam Clarinda Devi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thaoroijam_K/0/1/0/all/0/1\">Kabita Thaoroijam</a>",
          "description": "This paper presents a vowel-based dialect identification system for\nMeeteilon. For this work, a vowel dataset is created by using Meeteilon Speech\nCorpora available at Linguistic Data Consortium for Indian Languages (LDC-IL).\nSpectral features such as formant frequencies (F1, F1 and F3) and prosodic\nfeatures such as pitch (F0), energy, intensity and segment duration values are\nextracted from monophthong vowel sounds. Random forest classifier, a decision\ntree-based ensemble algorithm is used for classification of three major\ndialects of Meeteilon namely, Imphal, Kakching and Sekmai. Model has shown an\naverage dialect identification performance in terms of accuracy of around\n61.57%. The role of spectral and prosodic features are found to be significant\nin Meeteilon dialect classification.",
          "link": "http://arxiv.org/abs/2107.13419",
          "publishedOn": "2021-07-29T02:00:09.466Z",
          "wordCount": 579,
          "title": "Vowel-based Meeteilon dialect identification using a Random Forest classifier. (arXiv:2107.13419v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patrick_Evans_J/0/1/0/all/0/1\">James Patrick-Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dannehl_M/0/1/0/all/0/1\">Moritz Dannehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinder_J/0/1/0/all/0/1\">Johannes Kinder</a>",
          "description": "Reverse engineers would benefit from identifiers like function names, but\nthese are usually unavailable in binaries. Training a machine learning model to\npredict function names automatically is promising but fundamentally hard due to\nthe enormous number of classes. In this paper, we introduce eXtreme Function\nLabeling (XFL), an extreme multi-label learning approach to selecting\nappropriate labels for binary functions. XFL splits function names into tokens,\ntreating each as an informative label akin to the problem of tagging texts in\nnatural language. To capture the semantics of binary code, we introduce DEXTER,\na novel function embedding that combines static analysis-based features with\nlocal context from the call graph and global context from the entire binary. We\ndemonstrate that XFL outperforms state-of-the-art approaches to function\nlabeling on a dataset of over 10,000 binaries from the Debian project,\nachieving a precision of 82.5%. We also study combinations of XFL with\ndifferent published embeddings for binary functions and show that DEXTER\nconsistently improves over the state of the art in information gain. As a\nresult, we are able to show that binary function labeling is best phrased in\nterms of multi-label learning, and that binary function embeddings benefit from\nmoving beyond just learning from syntax.",
          "link": "http://arxiv.org/abs/2107.13404",
          "publishedOn": "2021-07-29T02:00:09.448Z",
          "wordCount": 631,
          "title": "XFL: eXtreme Function Labeling. (arXiv:2107.13404v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13361",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_G/0/1/0/all/0/1\">Gary G. Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_V/0/1/0/all/0/1\">Vincent S. Tseng</a>",
          "description": "Arrhythmia detection from ECG is an important research subject in the\nprevention and diagnosis of cardiovascular diseases. The prevailing studies\nformulate arrhythmia detection from ECG as a time series classification\nproblem. Meanwhile, early detection of arrhythmia presents a real-world demand\nfor early prevention and diagnosis. In this paper, we address a problem of\ncardiovascular disease early classification, which is a varied-length and\nlong-length time series early classification problem as well. For solving this\nproblem, we propose a deep reinforcement learning-based framework, namely\nSnippet Policy Network (SPN), consisting of four modules, snippet generator,\nbackbone network, controlling agent, and discriminator. Comparing to the\nexisting approaches, the proposed framework features flexible input length,\nsolves the dual-optimization solution of the earliness and accuracy goals.\nExperimental results demonstrate that SPN achieves an excellent performance of\nover 80\\% in terms of accuracy. Compared to the state-of-the-art methods, at\nleast 7% improvement on different metrics, including the precision, recall,\nF1-score, and harmonic mean, is delivered by the proposed SPN. To the best of\nour knowledge, this is the first work focusing on solving the cardiovascular\nearly classification problem based on varied-length ECG data. Based on these\nexcellent features from SPN, it offers a good exemplification for addressing\nall kinds of varied-length time series early classification problems.",
          "link": "http://arxiv.org/abs/2107.13361",
          "publishedOn": "2021-07-29T02:00:09.439Z",
          "wordCount": 646,
          "title": "Snippet Policy Network for Multi-class Varied-length ECG Early Classification. (arXiv:2107.13361v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13394",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Faruqui_S/0/1/0/all/0/1\">Syed Hasib Akhter Faruqui</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Alaeddini_A/0/1/0/all/0/1\">Adel Alaeddini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fisher_Hoch_S/0/1/0/all/0/1\">Susan P Fisher-Hoch</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mccormic_J/0/1/0/all/0/1\">Joseph B Mccormic</a>",
          "description": "The emergence and progression of multiple chronic conditions (MCC) over time\noften form a dynamic network that depends on patient's modifiable risk factors\nand their interaction with non-modifiable risk factors and existing conditions.\nContinuous time Bayesian networks (CTBNs) are effective methods for modeling\nthe complex network of MCC relationships over time. However, CTBNs are not able\nto effectively formulate the dynamic impact of patient's modifiable risk\nfactors on the emergence and progression of MCC. Considering a functional CTBN\n(FCTBN) to represent the underlying structure of the MCC relationships with\nrespect to individuals' risk factors and existing conditions, we propose a\nnonlinear state-space model based on Extended Kalman filter (EKF) to capture\nthe dynamics of the patients' modifiable risk factors and existing conditions\non the MCC evolution over time. We also develop a tensor control chart to\ndynamically monitor the effect of changes in the modifiable risk factors of\nindividual patients on the risk of new chronic conditions emergence. We\nvalidate the proposed approach based on a combination of simulation and real\ndata from a dataset of 385 patients from Cameron County Hispanic Cohort (CCHC)\nover multiple years. The dataset examines the emergence of 5 chronic conditions\n(Diabetes, Obesity, Cognitive Impairment, Hyperlipidemia, and Hypertension)\nbased on 4 modifiable risk factors representing lifestyle behaviors (Diet,\nExercise, Smoking Habit, and Drinking Habit) and 3 non-modifiable risk factors,\nincluding demographic information (Age, Gender, Education). The results\ndemonstrate the effectiveness of the proposed methodology for dynamic\nprediction and monitoring of the risk of MCC emergence in individual patients.",
          "link": "http://arxiv.org/abs/2107.13394",
          "publishedOn": "2021-07-29T02:00:09.432Z",
          "wordCount": 736,
          "title": "Nonlinear State Space Modeling and Control of the Impact of Patients' Modifiable Lifestyle Behaviors on the Emergence of Multiple Chronic Conditions. (arXiv:2107.13394v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lattanzi_E/0/1/0/all/0/1\">Emanuele Lattanzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calisti_L/0/1/0/all/0/1\">Lorenzo Calisti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freschi_V/0/1/0/all/0/1\">Valerio Freschi</a>",
          "description": "Current guidelines from the World Health Organization indicate that the\nSARSCoV-2 coronavirus, which results in the novel coronavirus disease\n(COVID-19), is transmitted through respiratory droplets or by contact. Contact\ntransmission occurs when contaminated hands touch the mucous membrane of the\nmouth, nose, or eyes. Moreover, pathogens can also be transferred from one\nsurface to another by contaminated hands, which facilitates transmission by\nindirect contact. Consequently, hands hygiene is extremely important to prevent\nthe spread of the SARSCoV-2 virus. Additionally, hand washing and/or hand\nrubbing disrupts also the transmission of other viruses and bacteria that cause\ncommon colds, flu and pneumonia, thereby reducing the overall disease burden.\nThe vast proliferation of wearable devices, such as smartwatches, containing\nacceleration, rotation, magnetic field sensors, etc., together with the modern\ntechnologies of artificial intelligence, such as machine learning and more\nrecently deep-learning, allow the development of accurate applications for\nrecognition and classification of human activities such as: walking, climbing\nstairs, running, clapping, sitting, sleeping, etc. In this work we evaluate the\nfeasibility of an automatic system, based on current smartwatches, which is\nable to recognize when a subject is washing or rubbing its hands, in order to\nmonitor parameters such as frequency and duration, and to evaluate the\neffectiveness of the gesture. Our preliminary results show a classification\naccuracy of about 95% and of about 94% for respectively deep and standard\nlearning techniques.",
          "link": "http://arxiv.org/abs/2107.13405",
          "publishedOn": "2021-07-29T02:00:09.424Z",
          "wordCount": 722,
          "title": "Automatic Unstructured Handwashing Recognition using Smartwatch to Reduce Contact Transmission of Pathogens. (arXiv:2107.13405v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dsouza_A/0/1/0/all/0/1\">Alishiba Dsouza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tempelmeier_N/0/1/0/all/0/1\">Nicolas Tempelmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demidova_E/0/1/0/all/0/1\">Elena Demidova</a>",
          "description": "OpenStreetMap (OSM) is one of the richest openly available sources of\nvolunteered geographic information. Although OSM includes various geographical\nentities, their descriptions are highly heterogeneous, incomplete, and do not\nfollow any well-defined ontology. Knowledge graphs can potentially provide\nvaluable semantic information to enrich OSM entities. However, interlinking OSM\nentities with knowledge graphs is inherently difficult due to the large,\nheterogeneous, ambiguous, and flat OSM schema and the annotation sparsity. This\npaper tackles the alignment of OSM tags with the corresponding knowledge graph\nclasses holistically by jointly considering the schema and instance layers. We\npropose a novel neural architecture that capitalizes upon a shared latent space\nfor tag-to-class alignment created using linked entities in OSM and knowledge\ngraphs. Our experiments performed to align OSM datasets for several countries\nwith two of the most prominent openly available knowledge graphs, namely,\nWikidata and DBpedia, demonstrate that the proposed approach outperforms the\nstate-of-the-art schema alignment baselines by up to 53 percentage points in\nterms of F1-score. The resulting alignment facilitates new semantic annotations\nfor over 10 million OSM entities worldwide, which is more than a 400% increase\ncompared to the existing semantic annotations in OSM.",
          "link": "http://arxiv.org/abs/2107.13257",
          "publishedOn": "2021-07-29T02:00:09.404Z",
          "wordCount": 625,
          "title": "Towards Neural Schema Alignment for OpenStreetMap and Knowledge Graphs. (arXiv:2107.13257v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Struski_L/0/1/0/all/0/1\">&#x141;ukasz Struski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danel_T/0/1/0/all/0/1\">Tomasz Danel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smieja_M/0/1/0/all/0/1\">Marek &#x15a;mieja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1\">Jacek Tabor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zielinski_B/0/1/0/all/0/1\">Bartosz Zieli&#x144;ski</a>",
          "description": "Recent years have seen a surge in research on deep interpretable neural\nnetworks with decision trees as one of the most commonly incorporated tools.\nThere are at least three advantages of using decision trees over logistic\nregression classification models: they are easy to interpret since they are\nbased on binary decisions, they can make decisions faster, and they provide a\nhierarchy of classes. However, one of the well-known drawbacks of decision\ntrees, as compared to decision graphs, is that decision trees cannot reuse the\ndecision nodes. Nevertheless, decision graphs were not commonly used in deep\nlearning due to the lack of efficient gradient-based training techniques. In\nthis paper, we fill this gap and provide a general paradigm based on Markov\nprocesses, which allows for efficient training of the special type of decision\ngraphs, which we call Self-Organizing Neural Graphs (SONG). We provide an\nextensive theoretical study of SONG, complemented by experiments conducted on\nLetter, Connect4, MNIST, CIFAR, and TinyImageNet datasets, showing that our\nmethod performs on par or better than existing decision models.",
          "link": "http://arxiv.org/abs/2107.13214",
          "publishedOn": "2021-07-29T02:00:09.397Z",
          "wordCount": 601,
          "title": "SONG: Self-Organizing Neural Graphs. (arXiv:2107.13214v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yong_B/0/1/0/all/0/1\">Bang Xiang Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathy_Y/0/1/0/all/0/1\">Yasmin Fathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1\">Alexandra Brintrup</a>",
          "description": "Autoencoders are unsupervised models which have been used for detecting\nanomalies in multi-sensor environments. A typical use includes training a\npredictive model with data from sensors operating under normal conditions and\nusing the model to detect anomalies. Anomalies can come either from real\nchanges in the environment (real drift) or from faulty sensory devices (virtual\ndrift); however, the use of Autoencoders to distinguish between different\nanomalies has not yet been considered. To this end, we first propose the\ndevelopment of Bayesian Autoencoders to quantify epistemic and aleatoric\nuncertainties. We then test the Bayesian Autoencoder using a real-world\nindustrial dataset for hydraulic condition monitoring. The system is injected\nwith noise and drifts, and we have found the epistemic uncertainty to be less\nsensitive to sensor perturbations as compared to the reconstruction loss. By\nobserving the reconstructed signals with the uncertainties, we gain\ninterpretable insights, and these uncertainties offer a potential avenue for\ndistinguishing real and virtual drifts.",
          "link": "http://arxiv.org/abs/2107.13249",
          "publishedOn": "2021-07-29T02:00:09.388Z",
          "wordCount": 601,
          "title": "Bayesian Autoencoders for Drift Detection in Industrial Environments. (arXiv:2107.13249v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiyong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qianqian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Shilong Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>",
          "description": "The Area under the ROC curve (AUC) is a well-known ranking metric for\nproblems such as imbalanced learning and recommender systems. The vast majority\nof existing AUC-optimization-based machine learning methods only focus on\nbinary-class cases, while leaving the multiclass cases unconsidered. In this\npaper, we start an early trial to consider the problem of learning multiclass\nscoring functions via optimizing multiclass AUC metrics. Our foundation is\nbased on the M metric, which is a well-known multiclass extension of AUC. We\nfirst pay a revisit to this metric, showing that it could eliminate the\nimbalance issue from the minority class pairs. Motivated by this, we propose an\nempirical surrogate risk minimization framework to approximately optimize the M\nmetric. Theoretically, we show that: (i) optimizing most of the popular\ndifferentiable surrogate losses suffices to reach the Bayes optimal scoring\nfunction asymptotically; (ii) the training framework enjoys an imbalance-aware\ngeneralization error bound, which pays more attention to the bottleneck samples\nof minority classes compared with the traditional $O(\\sqrt{1/N})$ result.\nPractically, to deal with the low scalability of the computational operations,\nwe propose acceleration methods for three popular surrogate loss functions,\nincluding the exponential loss, squared loss, and hinge loss, to speed up loss\nand gradient evaluations. Finally, experimental results on 11 real-world\ndatasets demonstrate the effectiveness of our proposed framework.",
          "link": "http://arxiv.org/abs/2107.13171",
          "publishedOn": "2021-07-29T02:00:09.381Z",
          "wordCount": 650,
          "title": "Learning with Multiclass AUC: Theory and Algorithms. (arXiv:2107.13171v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arora_G/0/1/0/all/0/1\">Geetika Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_R/0/1/0/all/0/1\">Rohit K Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_K/0/1/0/all/0/1\">Kamlesh Tiwari</a>",
          "description": "This paper proposes teeth-photo, a new biometric modality for human\nauthentication on mobile and hand held devices. Biometrics samples are acquired\nusing the camera mounted on mobile device with the help of a mobile application\nhaving specific markers to register the teeth area. Region of interest (RoI) is\nthen extracted using the markers and the obtained sample is enhanced using\ncontrast limited adaptive histogram equalization (CLAHE) for better visual\nclarity. We propose a deep learning architecture and novel regularization\nscheme to obtain highly discriminative embedding for small size RoI. Proposed\ncustom loss function was able to achieve perfect classification for the tiny\nRoI of $75\\times 75$ size. The model is end-to-end and few-shot and therefore\nis very efficient in terms of time and energy requirements. The system can be\nused in many ways including device unlocking and secure authentication. To the\nbest of our understanding, this is the first work on teeth-photo based\nauthentication for mobile device. Experiments have been conducted on an\nin-house teeth-photo database collected using our application. The database is\nmade publicly available. Results have shown that the proposed system has\nperfect accuracy.",
          "link": "http://arxiv.org/abs/2107.13217",
          "publishedOn": "2021-07-29T02:00:09.373Z",
          "wordCount": 632,
          "title": "DeepTeeth: A Teeth-photo Based Human Authentication System for Mobile and Hand-held Devices. (arXiv:2107.13217v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Wei-Wei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1\">Isabelle Guyon</a>",
          "description": "Analyzing better time series with limited human effort is of interest to\nacademia and industry. Driven by business scenarios, we organized the first\nAutomated Time Series Regression challenge (AutoSeries) for the WSDM Cup 2020.\nWe present its design, analysis, and post-hoc experiments. The code submission\nrequirement precluded participants from any manual intervention, testing\nautomated machine learning capabilities of solutions, across many datasets,\nunder hardware and time limitations. We prepared 10 datasets from diverse\napplication domains (sales, power consumption, air quality, traffic, and\nparking), featuring missing data, mixed continuous and categorical variables,\nand various sampling rates. Each dataset was split into a training and a test\nsequence (which was streamed, allowing models to continuously adapt). The\nsetting of time series regression, differs from classical forecasting in that\ncovariates at the present time are known. Great strides were made by\nparticipants to tackle this AutoSeries problem, as demonstrated by the jump in\nperformance from the sample submission, and post-hoc comparisons with\nAutoGluon. Simple yet effective methods were used, based on feature\nengineering, LightGBM, and random search hyper-parameter tuning, addressing all\naspects of the challenge. Our post-hoc analyses revealed that providing\nadditional time did not yield significant improvements. The winners' code was\nopen-sourced https://www.4paradigm.com/competition/autoseries2020.",
          "link": "http://arxiv.org/abs/2107.13186",
          "publishedOn": "2021-07-29T02:00:09.353Z",
          "wordCount": 644,
          "title": "AutoML Meets Time Series Regression Design and Analysis of the AutoSeries Challenge. (arXiv:2107.13186v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lishuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinting Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsui_K/0/1/0/all/0/1\">Kwok Leung Tsui</a>",
          "description": "Short-term forecasting of passenger flow is critical for transit management\nand crowd regulation. Spatial dependencies, temporal dependencies,\ninter-station correlations driven by other latent factors, and exogenous\nfactors bring challenges to the short-term forecasts of passenger flow of urban\nrail transit networks. An innovative deep learning approach, Multi-Graph\nConvolutional-Recurrent Neural Network (MGC-RNN) is proposed to forecast\npassenger flow in urban rail transit systems to incorporate these complex\nfactors. We propose to use multiple graphs to encode the spatial and other\nheterogenous inter-station correlations. The temporal dynamics of the\ninter-station correlations are also modeled via the proposed multi-graph\nconvolutional-recurrent neural network structure. Inflow and outflow of all\nstations can be collectively predicted with multiple time steps ahead via a\nsequence to sequence(seq2seq) architecture. The proposed method is applied to\nthe short-term forecasts of passenger flow in Shenzhen Metro, China. The\nexperimental results show that MGC-RNN outperforms the benchmark algorithms in\nterms of forecasting accuracy. Besides, it is found that the inter-station\ndriven by network distance, network structure, and recent flow patterns are\nsignificant factors for passenger flow forecasting. Moreover, the architecture\nof LSTM-encoder-decoder can capture the temporal dependencies well. In general,\nthe proposed framework could provide multiple views of passenger flow dynamics\nfor fine prediction and exhibit a possibility for multi-source heterogeneous\ndata fusion in the spatiotemporal forecast tasks.",
          "link": "http://arxiv.org/abs/2107.13226",
          "publishedOn": "2021-07-29T02:00:09.345Z",
          "wordCount": 659,
          "title": "Multi-Graph Convolutional-Recurrent Neural Network (MGC-RNN) for Short-Term Forecasting of Transit Passenger Flow. (arXiv:2107.13226v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruhe_D/0/1/0/all/0/1\">David Ruhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forre_P/0/1/0/all/0/1\">Patrick Forr&#xe9;</a>",
          "description": "We perform approximate inference in state-space models that allow for\nnonlinear higher-order Markov chains in latent space. The conditional\nindependencies of the generative model enable us to parameterize only an\ninference model, which learns to estimate clean states in a self-supervised\nmanner using maximum likelihood. First, we propose a recurrent method that is\ntrained directly on noisy observations. Afterward, we cast the model such that\nthe optimization problem leads to an update scheme that backpropagates through\na recursion similar to the classical Kalman filter and smoother. In scientific\napplications, domain knowledge can give a linear approximation of the latent\ntransition maps. We can easily incorporate this knowledge into our model,\nleading to a hybrid inference approach. In contrast to other methods,\nexperiments show that the hybrid method makes the inferred latent states\nphysically more interpretable and accurate, especially in low-data regimes.\nFurthermore, we do not rely on an additional parameterization of the generative\nmodel or supervision via uncorrupted observations or ground truth latent\nstates. Despite our model's simplicity, we obtain competitive results on the\nchaotic Lorenz system compared to a fully supervised approach and outperform a\nmethod based on variational inference.",
          "link": "http://arxiv.org/abs/2107.13349",
          "publishedOn": "2021-07-29T02:00:09.338Z",
          "wordCount": 617,
          "title": "Self-Supervised Hybrid Inference in State-Space Models. (arXiv:2107.13349v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peuter_S/0/1/0/all/0/1\">Sebastiaan De Peuter</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Oulasvirta_A/0/1/0/all/0/1\">Antti Oulasvirta</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Kaski_S/0/1/0/all/0/1\">Samuel Kaski</a> (1 and 3) ((1) Department of Computer Science, Aalto University, Finland, (2) Department of Communications and Networking, Aalto University, Finland, (3) Department of Computer Science, University of Manchester, UK)",
          "description": "AI for supporting designers needs to be rethought. It should aim to\ncooperate, not automate, by supporting and leveraging the creativity and\nproblem-solving of designers. The challenge for such AI is how to infer\ndesigners' goals and then help them without being needlessly disruptive. We\npresent AI-assisted design: a framework for creating such AI, built around\ngenerative user models which enable reasoning about designers' goals,\nreasoning, and capabilities.",
          "link": "http://arxiv.org/abs/2107.13074",
          "publishedOn": "2021-07-29T02:00:09.331Z",
          "wordCount": 550,
          "title": "Toward AI Assistants That Let Designers Design. (arXiv:2107.13074v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_E/0/1/0/all/0/1\">Eric Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jennifer J. Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_A/0/1/0/all/0/1\">Ann Kennedy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yisong Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Swarat Chaudhuri</a>",
          "description": "We present a framework for the unsupervised learning of neurosymbolic\nencoders, i.e., encoders obtained by composing neural networks with symbolic\nprograms from a domain-specific language. Such a framework can naturally\nincorporate symbolic expert knowledge into the learning process and lead to\nmore interpretable and factorized latent representations than fully neural\nencoders. Also, models learned this way can have downstream impact, as many\nanalysis workflows can benefit from having clean programmatic descriptions. We\nground our learning algorithm in the variational autoencoding (VAE) framework,\nwhere we aim to learn a neurosymbolic encoder in conjunction with a standard\ndecoder. Our algorithm integrates standard VAE-style training with modern\nprogram synthesis techniques. We evaluate our method on learning latent\nrepresentations for real-world trajectory data from animal biology and sports\nanalytics. We show that our approach offers significantly better separation\nthan standard VAEs and leads to practical gains on downstream tasks.",
          "link": "http://arxiv.org/abs/2107.13132",
          "publishedOn": "2021-07-29T02:00:09.324Z",
          "wordCount": 576,
          "title": "Unsupervised Learning of Neurosymbolic Encoders. (arXiv:2107.13132v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yong_B/0/1/0/all/0/1\">Bang Xiang Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1\">Alexandra Brintrup</a>",
          "description": "Recent advancements in predictive machine learning has led to its application\nin various use cases in manufacturing. Most research focused on maximising\npredictive accuracy without addressing the uncertainty associated with it.\nWhile accuracy is important, focusing primarily on it poses an overfitting\ndanger, exposing manufacturers to risk, ultimately hindering the adoption of\nthese techniques. In this paper, we determine the sources of uncertainty in\nmachine learning and establish the success criteria of a machine learning\nsystem to function well under uncertainty in a cyber-physical manufacturing\nsystem (CPMS) scenario. Then, we propose a multi-agent system architecture\nwhich leverages probabilistic machine learning as a means of achieving such\ncriteria. We propose possible scenarios for which our proposed architecture is\nuseful and discuss future work. Experimentally, we implement Bayesian Neural\nNetworks for multi-tasks classification on a public dataset for the real-time\ncondition monitoring of a hydraulic system and demonstrate the usefulness of\nthe system by evaluating the probability of a prediction being accurate given\nits uncertainty. We deploy these models using our proposed agent-based\nframework and integrate web visualisation to demonstrate its real-time\nfeasibility.",
          "link": "http://arxiv.org/abs/2107.13252",
          "publishedOn": "2021-07-29T02:00:09.318Z",
          "wordCount": 635,
          "title": "Multi Agent System for Machine Learning Under Uncertainty in Cyber Physical Manufacturing System. (arXiv:2107.13252v1 [cs.MA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canessa_G/0/1/0/all/0/1\">Gianpiero Canessa</a>",
          "description": "Support vector machines (SVM) is one of the well known supervised classes of\nlearning algorithms. Furthermore, the conic-segmentation SVM (CS-SVM) is a\nnatural multiclass analogue of the standard binary SVM, as CS-SVM models are\ndealing with the situation where the exact values of the data points are known.\nThis paper studies CS-SVM when the data points are uncertain or mislabelled.\nWith some properties known for the distributions, a chance-constrained CS-SVM\napproach is used to ensure the small probability of misclassification for the\nuncertain data. The geometric interpretation is presented to show how CS-SVM\nworks. Finally, we present experimental results to investigate the chance\nconstrained CS-SVM's performance.",
          "link": "http://arxiv.org/abs/2107.13319",
          "publishedOn": "2021-07-29T02:00:09.297Z",
          "wordCount": 547,
          "title": "Chance constrained conic-segmentation support vector machine with uncertain data. (arXiv:2107.13319v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13393",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Choe_Y/0/1/0/all/0/1\">Yoonsuck Choe</a>",
          "description": "Brain science and artificial intelligence have made great progress toward the\nunderstanding and engineering of the human mind. The progress has accelerated\nsignificantly since the turn of the century thanks to new methods for probing\nthe brain (both structure and function), and rapid development in deep learning\nresearch. However, despite these new developments, there are still many open\nquestions, such as how to understand the brain at the system level, and various\nrobustness issues and limitations of deep learning. In this informal essay, I\nwill talk about some of the concepts that are central to brain science and\nartificial intelligence, such as information and memory, and discuss how a\ndifferent view on these concepts can help us move forward, beyond current\nlimits of our understanding in these fields.",
          "link": "http://arxiv.org/abs/2107.13393",
          "publishedOn": "2021-07-29T02:00:09.291Z",
          "wordCount": 576,
          "title": "Meaning Versus Information, Prediction Versus Memory, and Question Versus Answer. (arXiv:2107.13393v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chubba_e/0/1/0/all/0/1\">ennifer Chubba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Missaouib_S/0/1/0/all/0/1\">Sondess Missaouib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Concannonc_S/0/1/0/all/0/1\">Shauna Concannonc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maloneyb_L/0/1/0/all/0/1\">Liam Maloneyb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1\">James Alfred Walker</a>",
          "description": "Conversational Artificial Intelligence (CAI) systems and Intelligent Personal\nAssistants (IPA), such as Alexa, Cortana, Google Home and Siri are becoming\nubiquitous in our lives, including those of children, the implications of which\nis receiving increased attention, specifically with respect to the effects of\nthese systems on children's cognitive, social and linguistic development.\nRecent advances address the implications of CAI with respect to privacy,\nsafety, security, and access. However, there is a need to connect and embed the\nethical and technical aspects in the design. Using a case-study of a research\nand development project focused on the use of CAI in storytelling for children,\nthis paper reflects on the social context within a specific case of technology\ndevelopment, as substantiated and supported by argumentation from within the\nliterature. It describes the decision making process behind the recommendations\nmade on this case for their adoption in the creative industries. Further\nresearch that engages with developers and stakeholders in the ethics of\nstorytelling through CAI is highlighted as a matter of urgency.",
          "link": "http://arxiv.org/abs/2107.13076",
          "publishedOn": "2021-07-29T02:00:09.284Z",
          "wordCount": 620,
          "title": "Interactive Storytelling for Children: A Case-study of Design and Development Considerations for Ethical Conversational AI. (arXiv:2107.13076v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_M/0/1/0/all/0/1\">Masahiro Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1\">Takaaki Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>",
          "description": "With the recent rapid progress in the study of deep generative models (DGMs),\nthere is a need for a framework that can implement them in a simple and generic\nway. In this research, we focus on two features of the latest DGMs: (1) deep\nneural networks are encapsulated by probability distributions and (2) models\nare designed and learned based on an objective function. Taking these features\ninto account, we propose a new DGM library called Pixyz. We experimentally show\nthat our library is faster than existing probabilistic modeling languages in\nlearning simple DGMs and we show that our library can be used to implement\ncomplex DGMs in a simple and concise manner, which is difficult to do with\nexisting libraries.",
          "link": "http://arxiv.org/abs/2107.13109",
          "publishedOn": "2021-07-29T02:00:09.276Z",
          "wordCount": 553,
          "title": "Pixyz: a library for developing deep generative models. (arXiv:2107.13109v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Curth_A/0/1/0/all/0/1\">Alicia Curth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1\">Mihaela van der Schaar</a>",
          "description": "The machine learning toolbox for estimation of heterogeneous treatment\neffects from observational data is expanding rapidly, yet many of its\nalgorithms have been evaluated only on a very limited set of semi-synthetic\nbenchmark datasets. In this paper, we show that even in arguably the simplest\nsetting -- estimation under ignorability assumptions -- the results of such\nempirical evaluations can be misleading if (i) the assumptions underlying the\ndata-generating mechanisms in benchmark datasets and (ii) their interplay with\nbaseline algorithms are inadequately discussed. We consider two popular machine\nlearning benchmark datasets for evaluation of heterogeneous treatment effect\nestimators -- the IHDP and ACIC2016 datasets -- in detail. We identify problems\nwith their current use and highlight that the inherent characteristics of the\nbenchmark datasets favor some algorithms over others -- a fact that is rarely\nacknowledged but of immense relevance for interpretation of empirical results.\nWe close by discussing implications and possible next steps.",
          "link": "http://arxiv.org/abs/2107.13346",
          "publishedOn": "2021-07-29T02:00:09.269Z",
          "wordCount": 619,
          "title": "Doing Great at Estimating CATE? On the Neglected Assumptions in Benchmark Comparisons of Treatment Effect Estimators. (arXiv:2107.13346v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenhua Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "In this work, we propose an adversarial unsupervised domain adaptation (UDA)\napproach with the inherent conditional and label shifts, in which we aim to\nalign the distributions w.r.t. both $p(x|y)$ and $p(y)$. Since the label is\ninaccessible in the target domain, the conventional adversarial UDA assumes\n$p(y)$ is invariant across domains, and relies on aligning $p(x)$ as an\nalternative to the $p(x|y)$ alignment. To address this, we provide a thorough\ntheoretical and empirical analysis of the conventional adversarial UDA methods\nunder both conditional and label shifts, and propose a novel and practical\nalternative optimization scheme for adversarial UDA. Specifically, we infer the\nmarginal $p(y)$ and align $p(x|y)$ iteratively in the training, and precisely\nalign the posterior $p(y|x)$ in testing. Our experimental results demonstrate\nits effectiveness on both classification and segmentation UDA, and partial UDA.",
          "link": "http://arxiv.org/abs/2107.13469",
          "publishedOn": "2021-07-29T02:00:09.251Z",
          "wordCount": 605,
          "title": "Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate. (arXiv:2107.13469v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13090",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hambly_B/0/1/0/all/0/1\">Ben Hambly</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xu_R/0/1/0/all/0/1\">Renyuan Xu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yang_H/0/1/0/all/0/1\">Huining Yang</a>",
          "description": "We consider a general-sum N-player linear-quadratic game with stochastic\ndynamics over a finite horizon and prove the global convergence of the natural\npolicy gradient method to the Nash equilibrium. In order to prove the\nconvergence of the method, we require a certain amount of noise in the system.\nWe give a condition, essentially a lower bound on the covariance of the noise\nin terms of the model parameters, in order to guarantee convergence. We\nillustrate our results with numerical experiments to show that even in\nsituations where the policy gradient method may not converge in the\ndeterministic setting, the addition of noise leads to convergence.",
          "link": "http://arxiv.org/abs/2107.13090",
          "publishedOn": "2021-07-29T02:00:09.243Z",
          "wordCount": 559,
          "title": "Policy Gradient Methods Find the Nash Equilibrium in N-player General-sum Linear-quadratic Games. (arXiv:2107.13090v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aalst_W/0/1/0/all/0/1\">Wil van der Aalst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockhoff_T/0/1/0/all/0/1\">Tobias Brockhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghahfarokhi_A/0/1/0/all/0/1\">Anahita Farhang Ghahfarokhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pourbafrani_M/0/1/0/all/0/1\">Mahsa Pourbafrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uysal_M/0/1/0/all/0/1\">Merih Seran Uysal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelst_S/0/1/0/all/0/1\">Sebastiaan van Zelst</a>",
          "description": "Operational processes in production, logistics, material handling,\nmaintenance, etc., are supported by cyber-physical systems combining hardware\nand software components. As a result, the digital and the physical world are\nclosely aligned, and it is possible to track operational processes in detail\n(e.g., using sensors). The abundance of event data generated by today's\noperational processes provides opportunities and challenges for process mining\ntechniques supporting process discovery, performance analysis, and conformance\nchecking. Using existing process mining tools, it is already possible to\nautomatically discover process models and uncover performance and compliance\nproblems. In the DFG-funded Cluster of Excellence \"Internet of Production\"\n(IoP), process mining is used to create \"digital shadows\" to improve a wide\nvariety of operational processes. However, operational processes are dynamic,\ndistributed, and complex. Driven by the challenges identified in the IoP\ncluster, we work on novel techniques for comparative process mining (comparing\nprocess variants for different products at different locations at different\ntimes), object-centric process mining (to handle processes involving different\ntypes of objects that interact), and forward-looking process mining (to explore\n\"What if?\" questions). By addressing these challenges, we aim to develop\nvaluable \"digital shadows\" that can be used to remove operational friction.",
          "link": "http://arxiv.org/abs/2107.13066",
          "publishedOn": "2021-07-29T02:00:09.236Z",
          "wordCount": 660,
          "title": "Removing Operational Friction Using Process Mining: Challenges Provided by the Internet of Production (IoP). (arXiv:2107.13066v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yanan Sun</a>",
          "description": "Neural Architecture Search (NAS) can automatically design well-performed\narchitectures of Deep Neural Networks (DNNs) for the tasks at hand. However,\none bottleneck of NAS is the prohibitively computational cost largely due to\nthe expensive performance evaluation. The neural predictors can directly\nestimate the performance without any training of the DNNs to be evaluated, thus\nhave drawn increasing attention from researchers. Despite their popularity,\nthey also suffer a severe limitation: the shortage of annotated DNN\narchitectures for effectively training the neural predictors. In this paper, we\nproposed Homogeneous Architecture Augmentation for Neural Predictor (HAAP) of\nDNN architectures to address the issue aforementioned. Specifically, a\nhomogeneous architecture augmentation algorithm is proposed in HAAP to generate\nsufficient training data taking the use of homogeneous representation.\nFurthermore, the one-hot encoding strategy is introduced into HAAP to make the\nrepresentation of DNN architectures more effective. The experiments have been\nconducted on both NAS-Benchmark-101 and NAS-Bench-201 dataset. The experimental\nresults demonstrate that the proposed HAAP algorithm outperforms the state of\nthe arts compared, yet with much less training data. In addition, the ablation\nstudies on both benchmark datasets have also shown the universality of the\nhomogeneous architecture augmentation.",
          "link": "http://arxiv.org/abs/2107.13153",
          "publishedOn": "2021-07-29T02:00:09.228Z",
          "wordCount": 631,
          "title": "Homogeneous Architecture Augmentation for Neural Predictor. (arXiv:2107.13153v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Sayantini Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivisonno_R/0/1/0/all/0/1\">Riccardo Trivisonno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carle_G/0/1/0/all/0/1\">Georg Carle</a>",
          "description": "Network automation is gaining significant attention in the development of B5G\nnetworks, primarily for reducing operational complexity, expenditures and\nimproving network efficiency. Concurrently operating closed loops aiming for\nindividual optimization targets may cause conflicts which, left unresolved,\nwould lead to significant degradation in network Key Performance Indicators\n(KPIs), thereby resulting in sub-optimal network performance. Centralized\ncoordination, albeit optimal, is impractical in large scale networks and for\ntime-critical applications. Decentralized approaches are therefore envisaged in\nthe evolution to B5G and subsequently, 6G networks. This work explores\npervasive intelligence for conflict resolution in network automation, as an\nalternative to centralized orchestration. A Q-Learning decentralized approach\nto network automation is proposed, and an application to network slice\nauto-scaling is designed and evaluated. Preliminary results highlight the\npotential of the proposed scheme and justify further research work in this\ndirection.",
          "link": "http://arxiv.org/abs/2107.13268",
          "publishedOn": "2021-07-29T02:00:09.218Z",
          "wordCount": 606,
          "title": "Q-Learning for Conflict Resolution in B5G Network Automation. (arXiv:2107.13268v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellogin_A/0/1/0/all/0/1\">Alejandro Bellog&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomo_C/0/1/0/all/0/1\">Claudio Pomo</a>",
          "description": "Collaborative filtering models based on matrix factorization and learned\nsimilarities using Artificial Neural Networks (ANNs) have gained significant\nattention in recent years. This is, in part, because ANNs have demonstrated\ngood results in a wide variety of recommendation tasks. The introduction of\nANNs within the recommendation ecosystem has been recently questioned, raising\nseveral comparisons in terms of efficiency and effectiveness. One aspect most\nof these comparisons have in common is their focus on accuracy, neglecting\nother evaluation dimensions important for the recommendation, such as novelty,\ndiversity, or accounting for biases. We replicate experiments from three papers\nthat compare Neural Collaborative Filtering (NCF) and Matrix Factorization\n(MF), to extend the analysis to other evaluation dimensions. Our contribution\nshows that the experiments are entirely reproducible, and we extend the study\nincluding other accuracy metrics and two statistical hypothesis tests. We\ninvestigated the Diversity and Novelty of the recommendations, showing that MF\nprovides a better accuracy also on the long tail, although NCF provides a\nbetter item coverage and more diversified recommendations. We discuss the bias\neffect generated by the tested methods. They show a relatively small bias, but\nother recommendation baselines, with competitive accuracy performance,\nconsistently show to be less affected by this issue. This is the first work, to\nthe best of our knowledge, where several evaluation dimensions have been\nexplored for an array of SOTA algorithms covering recent adaptations of ANNs\nand MF. Hence, we show the potential these techniques may have on\nbeyond-accuracy evaluation while analyzing the effect on reproducibility these\ncomplementary dimensions may spark. Available at\ngithub.com/sisinflab/Reenvisioning-the-comparison-between-Neural-Collaborative-Filtering-and-Matrix-Factorization",
          "link": "http://arxiv.org/abs/2107.13472",
          "publishedOn": "2021-07-29T02:00:09.198Z",
          "wordCount": 710,
          "title": "Reenvisioning Collaborative Filtering vs Matrix Factorization. (arXiv:2107.13472v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yubin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Pengyi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>",
          "description": "There has been a growing interest in unsupervised domain adaptation (UDA) to\nalleviate the data scalability issue, while the existing works usually focus on\nclassifying independently discrete labels. However, in many tasks (e.g.,\nmedical diagnosis), the labels are discrete and successively distributed. The\nUDA for ordinal classification requires inducing non-trivial ordinal\ndistribution prior to the latent space. Target for this, the partially ordered\nset (poset) is defined for constraining the latent vector. Instead of the\ntypically i.i.d. Gaussian latent prior, in this work, a recursively conditional\nGaussian (RCG) set is proposed for ordered constraint modeling, which admits a\ntractable joint distribution prior. Furthermore, we are able to control the\ndensity of content vectors that violate the poset constraint by a simple\n\"three-sigma rule\". We explicitly disentangle the cross-domain images into a\nshared ordinal prior induced ordinal content space and two separate\nsource/target ordinal-unrelated spaces, and the self-training is worked on the\nshared space exclusively for ordinal-aware domain alignment. Extensive\nexperiments on UDA medical diagnoses and facial age estimation demonstrate its\neffectiveness.",
          "link": "http://arxiv.org/abs/2107.13467",
          "publishedOn": "2021-07-29T02:00:09.190Z",
          "wordCount": 624,
          "title": "Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation. (arXiv:2107.13467v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuesong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>",
          "description": "Node features and structural information of a graph are both crucial for\nsemi-supervised node classification problems. A variety of graph neural network\n(GNN) based approaches have been proposed to tackle these problems, which\ntypically determine output labels through feature aggregation. This can be\nproblematic, as it implies conditional independence of output nodes given\nhidden representations, despite their direct connections in the graph. To learn\nthe direct influence among output nodes in a graph, we propose the Explicit\nPairwise Factorized Graph Neural Network (EPFGNN), which models the whole graph\nas a partially observed Markov Random Field. It contains explicit pairwise\nfactors to model output-output relations and uses a GNN backbone to model\ninput-output relations. To balance model complexity and expressivity, the\npairwise factors have a shared component and a separate scaling coefficient for\neach edge. We apply the EM algorithm to train our model, and utilize a\nstar-shaped piecewise likelihood for the tractable surrogate objective. We\nconduct experiments on various datasets, which shows that our model can\neffectively improve the performance for semi-supervised node classification on\ngraphs.",
          "link": "http://arxiv.org/abs/2107.13059",
          "publishedOn": "2021-07-29T02:00:09.179Z",
          "wordCount": 613,
          "title": "Explicit Pairwise Factorized Graph Neural Network for Semi-Supervised Node Classification. (arXiv:2107.13059v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Picallo_M/0/1/0/all/0/1\">Mario Alvarez-Picallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghica_D/0/1/0/all/0/1\">Dan R. Ghica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sprunger_D/0/1/0/all/0/1\">David Sprunger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanasi_F/0/1/0/all/0/1\">Fabio Zanasi</a>",
          "description": "We enhance the calculus of string diagrams for monoidal categories with\nhierarchical features in order to capture closed monoidal (and cartesian\nclosed) structure. Using this new syntax we formulate an automatic\ndifferentiation algorithm for (applied) simply typed lambda calculus in the\nstyle of [Pearlmutter and Siskind 2008] and we prove for the first time its\nsoundness. To give an efficient yet principled implementation of the AD\nalgorithm we define a sound and complete representation of hierarchical string\ndiagrams as a class of hierarchical hypergraphs we call hypernets.",
          "link": "http://arxiv.org/abs/2107.13433",
          "publishedOn": "2021-07-29T02:00:09.160Z",
          "wordCount": 523,
          "title": "Functorial String Diagrams for Reverse-Mode Automatic Differentiation. (arXiv:2107.13433v1 [cs.PL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hanxiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1\">Helena Kotthaus</a>",
          "description": "In the field of autonomous driving and robotics, point clouds are showing\ntheir excellent real-time performance as raw data from most of the mainstream\n3D sensors. Therefore, point cloud neural networks have become a popular\nresearch direction in recent years. So far, however, there has been little\ndiscussion about the explainability of deep neural networks for point clouds.\nIn this paper, we propose new explainability approaches for point cloud deep\nneural networks based on local surrogate model-based methods to show which\ncomponents make the main contribution to the classification. Moreover, we\npropose a quantitative validation method for explainability methods of point\nclouds which enhances the persuasive power of explainability by dropping the\nmost positive or negative contributing features and monitoring how the\nclassification scores of specific categories change. To enable an intuitive\nexplanation of misclassified instances, we display features with confounding\ncontributions. Our new explainability approach provides a fairly accurate, more\nintuitive and widely applicable explanation for point cloud classification\ntasks. Our code is available at https://github.com/Explain3D/Explainable3D",
          "link": "http://arxiv.org/abs/2107.13459",
          "publishedOn": "2021-07-29T02:00:09.104Z",
          "wordCount": 610,
          "title": "Surrogate Model-Based Explainability Methods for Point Cloud NNs. (arXiv:2107.13459v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_G/0/1/0/all/0/1\">Guangliang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minglei Li</a>",
          "description": "Channel estimation and signal detection are essential steps to ensure the\nquality of end-to-end communication in orthogonal frequency-division\nmultiplexing (OFDM) systems. In this paper, we develop a DDLSD approach, i.e.,\nData-driven Deep Learning for Signal Detection in OFDM systems. First, the OFDM\nsystem model is established. Then, the long short-term memory (LSTM) is\nintroduced into the OFDM system model. Wireless channel data is generated\nthrough simulation, the preprocessed time series feature information is input\ninto the LSTM to complete the offline training. Finally, the trained model is\nused for online recovery of transmitted signal. The difference between this\nscheme and existing OFDM receiver is that explicit estimated channel state\ninformation (CSI) is transformed into invisible estimated CSI, and the transmit\nsymbol is directly restored. Simulation results show that the DDLSD scheme\noutperforms the existing traditional methods in terms of improving channel\nestimation and signal detection performance.",
          "link": "http://arxiv.org/abs/2107.13423",
          "publishedOn": "2021-07-29T02:00:09.096Z",
          "wordCount": 594,
          "title": "A Signal Detection Scheme Based on Deep Learning in OFDM Systems. (arXiv:2107.13423v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dongchen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-feng Yang</a>",
          "description": "Traditional maximum entropy and sparsity-based algorithms for analytic\ncontinuation often suffer from the ill-posed kernel matrix or demand tremendous\ncomputation time for parameter tuning. Here we propose a neural network method\nby convex optimization and replace the ill-posed inverse problem by a sequence\nof well-conditioned surrogate problems. After training, the learned optimizers\nare able to give a solution of high quality with low time cost and achieve\nhigher parameter efficiency than heuristic full-connected networks. The output\ncan also be used as a neural default model to improve the maximum entropy for\nbetter performance. Our methods may be easily extended to other\nhigh-dimensional inverse problems via large-scale pretraining.",
          "link": "http://arxiv.org/abs/2107.13265",
          "publishedOn": "2021-07-29T02:00:09.087Z",
          "wordCount": 542,
          "title": "Learned Optimizers for Analytic Continuation. (arXiv:2107.13265v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13449",
          "author": "<a href=\"http://arxiv.org/find/nucl-th/1/au:+Sarkar_A/0/1/0/all/0/1\">Avik Sarkar</a>, <a href=\"http://arxiv.org/find/nucl-th/1/au:+Lee_D/0/1/0/all/0/1\">Dean Lee</a>",
          "description": "Emulators that can bypass computationally expensive scientific calculations\nwith high accuracy and speed can enable new studies of fundamental science as\nwell as more potential applications. In this work we focus on solving a system\nof constraint equations efficiently using a new machine learning approach that\nwe call self-learning emulation. A self-learning emulator is an active learning\nprotocol that can rapidly solve a system of equations over some range of\ncontrol parameters. The key ingredient is a fast estimate of the emulator error\nthat becomes progressively more accurate as the emulator improves. This\nacceleration is possible because the emulator itself is used to estimate the\nerror, and we illustrate with two examples. The first uses cubic spline\ninterpolation to find the roots of a polynomial with variable coefficients. The\nsecond example uses eigenvector continuation to find the eigenvectors and\neigenvalues of a large Hamiltonian matrix that depends on several control\nparameters. We envision future applications of self-learning emulators for\nsolving systems of algebraic equations, linear and nonlinear differential\nequations, and linear and nonlinear eigenvalue problems.",
          "link": "http://arxiv.org/abs/2107.13449",
          "publishedOn": "2021-07-29T02:00:09.080Z",
          "wordCount": 625,
          "title": "Self-learning Emulators and Eigenvector Continuation. (arXiv:2107.13449v1 [nucl-th])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13148",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Ullah_A/0/1/0/all/0/1\">A. K. M. Amanat Ullah</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Imtiaz_F/0/1/0/all/0/1\">Fahim Imtiaz</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Ihsan_M/0/1/0/all/0/1\">Miftah Uddin Md Ihsan</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Alam_M/0/1/0/all/0/1\">Md. Golam Rabiul Alam</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Majumdar_M/0/1/0/all/0/1\">Mahbub Majumdar</a>",
          "description": "The unpredictability and volatility of the stock market render it challenging\nto make a substantial profit using any generalized scheme. This paper intends\nto discuss our machine learning model, which can make a significant amount of\nprofit in the US stock market by performing live trading in the Quantopian\nplatform while using resources free of cost. Our top approach was to use\nensemble learning with four classifiers: Gaussian Naive Bayes, Decision Tree,\nLogistic Regression with L1 regularization and Stochastic Gradient Descent, to\ndecide whether to go long or short on a particular stock. Our best model\nperformed daily trade between July 2011 and January 2019, generating 54.35%\nprofit. Finally, our work showcased that mixtures of weighted classifiers\nperform better than any individual predictor about making trading decisions in\nthe stock market.",
          "link": "http://arxiv.org/abs/2107.13148",
          "publishedOn": "2021-07-29T02:00:09.073Z",
          "wordCount": 594,
          "title": "Combining Machine Learning Classifiers for Stock Trading with Effective Feature Extraction. (arXiv:2107.13148v1 [q-fin.TR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Divi_S/0/1/0/all/0/1\">Siddharth Divi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yi-Shan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farrukh_H/0/1/0/all/0/1\">Habiba Farrukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celik_Z/0/1/0/all/0/1\">Z. Berkay Celik</a>",
          "description": "In Federated Learning (FL), the clients learn a single global model (FedAvg)\nthrough a central aggregator. In this setting, the non-IID distribution of the\ndata across clients restricts the global FL model from delivering good\nperformance on the local data of each client. Personalized FL aims to address\nthis problem by finding a personalized model for each client. Recent works\nwidely report the average personalized model accuracy on a particular data\nsplit of a dataset to evaluate the effectiveness of their methods. However,\nconsidering the multitude of personalization approaches proposed, it is\ncritical to study the per-user personalized accuracy and the accuracy\nimprovements among users with an equitable notion of fairness. To address these\nissues, we present a set of performance and fairness metrics intending to\nassess the quality of personalized FL methods. We apply these metrics to four\nrecently proposed personalized FL methods, PersFL, FedPer, pFedMe, and\nPer-FedAvg, on three different data splits of the CIFAR-10 dataset. Our\nevaluations show that the personalized model with the highest average accuracy\nacross users may not necessarily be the fairest. Our code is available at\nhttps://tinyurl.com/1hp9ywfa for public use.",
          "link": "http://arxiv.org/abs/2107.13173",
          "publishedOn": "2021-07-29T02:00:09.053Z",
          "wordCount": 627,
          "title": "New Metrics to Evaluate the Performance and Fairness of Personalized Federated Learning. (arXiv:2107.13173v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yong_B/0/1/0/all/0/1\">Bang Xiang Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pearce_T/0/1/0/all/0/1\">Tim Pearce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1\">Alexandra Brintrup</a>",
          "description": "After an autoencoder (AE) has learnt to reconstruct one dataset, it might be\nexpected that the likelihood on an out-of-distribution (OOD) input would be\nlow. This has been studied as an approach to detect OOD inputs. Recent work\nshowed this intuitive approach can fail for the dataset pairs FashionMNIST vs\nMNIST. This paper suggests this is due to the use of Bernoulli likelihood and\nanalyses why this is the case, proposing two fixes: 1) Compute the uncertainty\nof likelihood estimate by using a Bayesian version of the AE. 2) Use\nalternative distributions to model the likelihood.",
          "link": "http://arxiv.org/abs/2107.13304",
          "publishedOn": "2021-07-29T02:00:09.045Z",
          "wordCount": 550,
          "title": "Bayesian Autoencoders: Analysing and Fixing the Bernoulli likelihood for Out-of-Distribution Detection. (arXiv:2107.13304v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13430",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nishida_K/0/1/0/all/0/1\">Kiheiji Nishida</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Naito_K/0/1/0/all/0/1\">Kanta Naito</a>",
          "description": "This paper studies kernel density estimation by stagewise minimization\nalgorithm with a simple dictionary on U-divergence. We randomly split an i.i.d.\nsample into the two disjoint sets, one to be used for constructing the kernels\nin the dictionary and the other for evaluating the estimator, and implement the\nalgorithm. The resulting estimator brings us data-adaptive weighting parameters\nand bandwidth matrices, and realizes a sparse representation of kernel density\nestimation. We present the non-asymptotic error bounds of our estimator and\nconfirm its performance by simulations compared with the direct plug-in\nbandwidth matrices and the reduced set density estimator.",
          "link": "http://arxiv.org/abs/2107.13430",
          "publishedOn": "2021-07-29T02:00:09.036Z",
          "wordCount": 538,
          "title": "Kernel Density Estimation by Stagewise Algorithm with a Simple Dictionary. (arXiv:2107.13430v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13136",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marino_J/0/1/0/all/0/1\">Joseph Marino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mandt_S/0/1/0/all/0/1\">Stephan Mandt</a>",
          "description": "While recent machine learning research has revealed connections between deep\ngenerative models such as VAEs and rate-distortion losses used in learned\ncompression, most of this work has focused on images. In a similar spirit, we\nview recently proposed neural video coding algorithms through the lens of deep\nautoregressive and latent variable modeling. We present recent neural video\ncodecs as instances of a generalized stochastic temporal autoregressive\ntransform, and propose new avenues for further improvements inspired by\nnormalizing flows and structured priors. We propose several architectures that\nyield state-of-the-art video compression performance on full-resolution video\nand discuss their tradeoffs and ablations. In particular, we propose (i)\nimproved temporal autoregressive transforms, (ii) improved entropy models with\nstructured and temporal dependencies, and (iii) variable bitrate versions of\nour algorithms. Since our improvements are compatible with a large class of\nexisting models, we provide further evidence that the generative modeling\nviewpoint can advance the neural video coding field.",
          "link": "http://arxiv.org/abs/2107.13136",
          "publishedOn": "2021-07-29T02:00:09.028Z",
          "wordCount": 637,
          "title": "Insights from Generative Modeling for Neural Video Compression. (arXiv:2107.13136v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Colin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yining Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>",
          "description": "A common lens to theoretically study neural net architectures is to analyze\nthe functions they can approximate. However, the constructions from\napproximation theory often have unrealistic aspects, for example, reliance on\ninfinite precision to memorize target function values, which make these results\npotentially less meaningful. To address these issues, this work proposes a\nformal definition of statistically meaningful approximation which requires the\napproximating network to exhibit good statistical learnability. We present case\nstudies on statistically meaningful approximation for two classes of functions:\nboolean circuits and Turing machines. We show that overparameterized\nfeedforward neural nets can statistically meaningfully approximate boolean\ncircuits with sample complexity depending only polynomially on the circuit\nsize, not the size of the approximating network. In addition, we show that\ntransformers can statistically meaningfully approximate Turing machines with\ncomputation time bounded by $T$, requiring sample complexity polynomial in the\nalphabet size, state space size, and $\\log (T)$. Our analysis introduces new\ntools for generalization bounds that provide much tighter sample complexity\nguarantees than the typical VC-dimension or norm-based bounds, which may be of\nindependent interest.",
          "link": "http://arxiv.org/abs/2107.13163",
          "publishedOn": "2021-07-29T02:00:09.019Z",
          "wordCount": 619,
          "title": "Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers. (arXiv:2107.13163v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13200",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_B/0/1/0/all/0/1\">Bo Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_P/0/1/0/all/0/1\">Pengfei Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a> (Alzheimer&#x27;s Disease Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing), <a href=\"http://arxiv.org/find/eess/1/au:+Shen_S/0/1/0/all/0/1\">Shuwei Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_P/0/1/0/all/0/1\">Peng Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1\">Ronald X. Xu</a>",
          "description": "Early and accurate diagnosis of Alzheimer's disease (AD) and its prodromal\nperiod mild cognitive impairment (MCI) is essential for the delayed disease\nprogression and the improved quality of patients'life. The emerging\ncomputer-aided diagnostic methods that combine deep learning with structural\nmagnetic resonance imaging (sMRI) have achieved encouraging results, but some\nof them are limit of issues such as data leakage and unexplainable diagnosis.\nIn this research, we propose a novel end-to-end deep learning approach for\nautomated diagnosis of AD and localization of important brain regions related\nto the disease from sMRI data. This approach is based on a 2D single model\nstrategy and has the following differences from the current approaches: 1)\nConvolutional Neural Network (CNN) models of different structures and\ncapacities are evaluated systemically and the most suitable model is adopted\nfor AD diagnosis; 2) a data augmentation strategy named Two-stage Random\nRandAugment (TRRA) is proposed to alleviate the overfitting issue caused by\nlimited training data and to improve the classification performance in AD\ndiagnosis; 3) an explainable method of Grad-CAM++ is introduced to generate the\nvisually explainable heatmaps that localize and highlight the brain regions\nthat our model focuses on and to make our model more transparent. Our approach\nhas been evaluated on two publicly accessible datasets for two classification\ntasks of AD vs. cognitively normal (CN) and progressive MCI (pMCI) vs. stable\nMCI (sMCI). The experimental results indicate that our approach outperforms the\nstate-of-the-art approaches, including those using multi-model and 3D CNN\nmethods. The resultant localization heatmaps from our approach also highlight\nthe lateral ventricle and some disease-relevant regions of cortex, coincident\nwith the commonly affected regions during the development of AD.",
          "link": "http://arxiv.org/abs/2107.13200",
          "publishedOn": "2021-07-29T02:00:08.998Z",
          "wordCount": 760,
          "title": "An explainable two-dimensional single model deep learning approach for Alzheimer's disease diagnosis and brain atrophy localization. (arXiv:2107.13200v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daubechies_I/0/1/0/all/0/1\">Ingrid Daubechies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeVore_R/0/1/0/all/0/1\">Ronald DeVore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dym_N/0/1/0/all/0/1\">Nadav Dym</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faigenbaum_Golovin_S/0/1/0/all/0/1\">Shira Faigenbaum-Golovin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovalsky_S/0/1/0/all/0/1\">Shahar Z. Kovalsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kung-Ching Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Josiah Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrova_G/0/1/0/all/0/1\">Guergana Petrova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sober_B/0/1/0/all/0/1\">Barak Sober</a>",
          "description": "In the desire to quantify the success of neural networks in deep learning and\nother applications, there is a great interest in understanding which functions\nare efficiently approximated by the outputs of neural networks. By now, there\nexists a variety of results which show that a wide range of functions can be\napproximated with sometimes surprising accuracy by these outputs. For example,\nit is known that the set of functions that can be approximated with exponential\naccuracy (in terms of the number of parameters used) includes, on one hand,\nvery smooth functions such as polynomials and analytic functions (see e.g.\n\\cite{E,S,Y}) and, on the other hand, very rough functions such as the\nWeierstrass function (see e.g. \\cite{EPGB,DDFHP}), which is nowhere\ndifferentiable. In this paper, we add to the latter class of rough functions by\nshowing that it also includes refinable functions. Namely, we show that\nrefinable functions are approximated by the outputs of deep ReLU networks with\na fixed width and increasing depth with accuracy exponential in terms of their\nnumber of parameters. Our results apply to functions used in the standard\nconstruction of wavelets as well as to functions constructed via subdivision\nalgorithms in Computer Aided Geometric Design.",
          "link": "http://arxiv.org/abs/2107.13191",
          "publishedOn": "2021-07-29T02:00:08.989Z",
          "wordCount": 635,
          "title": "Neural Network Approximation of Refinable Functions. (arXiv:2107.13191v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bahadori_M/0/1/0/all/0/1\">Mohammad Taha Bahadori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchetgen_E/0/1/0/all/0/1\">Eric Tchetgen Tchetgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1\">David E. Heckerman</a>",
          "description": "We study the problem of observational causal inference with continuous\ntreatment. We focus on the challenge of estimating the causal response curve\nfor infrequently-observed treatment values. We design a new algorithm based on\nthe framework of entropy balancing which learns weights that directly maximize\ncausal inference accuracy using end-to-end optimization. Our weights can be\ncustomized for different datasets and causal inference algorithms. We propose a\nnew theory for consistency of entropy balancing for continuous treatments.\nUsing synthetic and real-world data, we show that our proposed algorithm\noutperforms the entropy balancing in terms of causal inference accuracy.",
          "link": "http://arxiv.org/abs/2107.13068",
          "publishedOn": "2021-07-29T02:00:08.982Z",
          "wordCount": 537,
          "title": "End-to-End Balancing for Causal Continuous Treatment-Effect Estimation. (arXiv:2107.13068v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundgaard_K/0/1/0/all/0/1\">Keld T. Lundgaard</a>",
          "description": "By leveraging large amounts of product data collected across hundreds of live\ne-commerce websites, we construct 1000 unique classification tasks that share\nsimilarly-structured input data, comprised of both text and images. These\nclassification tasks focus on learning the product hierarchy of different\ne-commerce websites, causing many of them to be correlated. Adopting a\nmulti-modal transformer model, we solve these tasks in unison using multi-task\nlearning (MTL). Extensive experiments are presented over an initial 100-task\ndataset to reveal best practices for \"large-scale MTL\" (i.e., MTL with more\nthan 100 tasks). From these experiments, a final, unified methodology is\nderived, which is composed of both best practices and new proposals such as\nDyPa, a simple heuristic for automatically allocating task-specific parameters\nto tasks that could benefit from extra capacity. Using our large-scale MTL\nmethodology, we successfully train a single model across all 1000 tasks in our\ndataset while using minimal task specific parameters, thereby showing that it\nis possible to extend several orders of magnitude beyond current efforts in\nMTL.",
          "link": "http://arxiv.org/abs/2107.13054",
          "publishedOn": "2021-07-29T02:00:08.975Z",
          "wordCount": 623,
          "title": "Exceeding the Limits of Visual-Linguistic Multi-Task Learning. (arXiv:2107.13054v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kesidis_G/0/1/0/all/0/1\">George Kesidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_D/0/1/0/all/0/1\">David J. Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergeron_M/0/1/0/all/0/1\">Maxime Bergeron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferguson_R/0/1/0/all/0/1\">Ryan Ferguson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_V/0/1/0/all/0/1\">Vladimir Lucic</a>",
          "description": "We describe a gradient-based method to discover local error maximizers of a\ndeep neural network (DNN) used for regression, assuming the availability of an\n\"oracle\" capable of providing real-valued supervision (a regression target) for\nsamples. For example, the oracle could be a numerical solver which,\noperationally, is much slower than the DNN. Given a discovered set of local\nerror maximizers, the DNN is either fine-tuned or retrained in the manner of\nactive learning.",
          "link": "http://arxiv.org/abs/2107.13124",
          "publishedOn": "2021-07-29T02:00:08.954Z",
          "wordCount": 511,
          "title": "Robust and Active Learning for Deep Neural Network Regression. (arXiv:2107.13124v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13067",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Ataei_M/0/1/0/all/0/1\">Mohammadmehdi Ataei</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pirmorad_E/0/1/0/all/0/1\">Erfan Pirmorad</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Costa_F/0/1/0/all/0/1\">Franco Costa</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Han_S/0/1/0/all/0/1\">Sejin Han</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Park_C/0/1/0/all/0/1\">Chul B Park</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bussmann_M/0/1/0/all/0/1\">Markus Bussmann</a>",
          "description": "Piecewise Linear Interface Construction (PLIC) is frequently used to\ngeometrically reconstruct fluid interfaces in Computational Fluid Dynamics\n(CFD) modeling of two-phase flows. PLIC reconstructs interfaces from a scalar\nfield that represents the volume fraction of each phase in each computational\ncell. Given the volume fraction and interface normal, the location of a linear\ninterface is uniquely defined. For a cubic computational cell (3D), the\nposition of the planar interface is determined by intersecting the cube with a\nplane, such that the volume of the resulting truncated polyhedron cell is equal\nto the volume fraction. Yet it is geometrically complex to find the exact\nposition of the plane, and it involves calculations that can be a computational\nbottleneck of many CFD models. However, while the forward problem of 3D PLIC is\nchallenging, the inverse problem, of finding the volume of the truncated\npolyhedron cell given a defined plane, is simple. In this work, we propose a\ndeep learning model for the solution to the forward problem of PLIC by only\nmaking use of its inverse problem. The proposed model is up to several orders\nof magnitude faster than traditional schemes, which significantly reduces the\ncomputational bottleneck of PLIC in CFD simulations.",
          "link": "http://arxiv.org/abs/2107.13067",
          "publishedOn": "2021-07-29T02:00:08.947Z",
          "wordCount": 651,
          "title": "A Deep Learning Algorithm for Piecewise Linear Interface Construction (PLIC). (arXiv:2107.13067v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dallmann_A/0/1/0/all/0/1\">Alexander Dallmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoller_D/0/1/0/all/0/1\">Daniel Zoller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hotho_A/0/1/0/all/0/1\">Andreas Hotho</a>",
          "description": "At the present time, sequential item recommendation models are compared by\ncalculating metrics on a small item subset (target set) to speed up\ncomputation. The target set contains the relevant item and a set of negative\nitems that are sampled from the full item set. Two well-known strategies to\nsample negative items are uniform random sampling and sampling by popularity to\nbetter approximate the item frequency distribution in the dataset. Most\nrecently published papers on sequential item recommendation rely on sampling by\npopularity to compare the evaluated models. However, recent work has already\nshown that an evaluation with uniform random sampling may not be consistent\nwith the full ranking, that is, the model ranking obtained by evaluating a\nmetric using the full item set as target set, which raises the question whether\nthe ranking obtained by sampling by popularity is equal to the full ranking. In\nthis work, we re-evaluate current state-of-the-art sequential recommender\nmodels from the point of view, whether these sampling strategies have an impact\non the final ranking of the models. We therefore train four recently proposed\nsequential recommendation models on five widely known datasets. For each\ndataset and model, we employ three evaluation strategies. First, we compute the\nfull model ranking. Then we evaluate all models on a target set sampled by the\ntwo different sampling strategies, uniform random sampling and sampling by\npopularity with the commonly used target set size of 100, compute the model\nranking for each strategy and compare them with each other. Additionally, we\nvary the size of the sampled target set. Overall, we find that both sampling\nstrategies can produce inconsistent rankings compared with the full ranking of\nthe models. Furthermore, both sampling by popularity and uniform random\nsampling do not consistently produce the same ranking ...",
          "link": "http://arxiv.org/abs/2107.13045",
          "publishedOn": "2021-07-29T02:00:08.940Z",
          "wordCount": 740,
          "title": "A Case Study on Sampling Strategies for Evaluating Neural Sequential Item Recommendation Models. (arXiv:2107.13045v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Walsh_R/0/1/0/all/0/1\">Reece Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelpakey_M/0/1/0/all/0/1\">Mohamed H. Abdelpakey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1\">Mohamed S. Shehata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1\">Mostafa M.Mohamed</a>",
          "description": "Classifying and analyzing human cells is a lengthy procedure, often involving\na trained professional. In an attempt to expedite this process, an active area\nof research involves automating cell classification through use of deep\nlearning-based techniques. In practice, a large amount of data is required to\naccurately train these deep learning models. However, due to the sparse human\ncell datasets currently available, the performance of these models is typically\nlow. This study investigates the feasibility of using few-shot learning-based\ntechniques to mitigate the data requirements for accurate training. The study\nis comprised of three parts: First, current state-of-the-art few-shot learning\ntechniques are evaluated on human cell classification. The selected techniques\nare trained on a non-medical dataset and then tested on two out-of-domain,\nhuman cell datasets. The results indicate that, overall, the test accuracy of\nstate-of-the-art techniques decreased by at least 30% when transitioning from a\nnon-medical dataset to a medical dataset. Second, this study evaluates the\npotential benefits, if any, to varying the backbone architecture and training\nschemes in current state-of-the-art few-shot learning techniques when used in\nhuman cell classification. Even with these variations, the overall test\naccuracy decreased from 88.66% on non-medical datasets to 44.13% at best on the\nmedical datasets. Third, this study presents future directions for using\nfew-shot learning in human cell classification. In general, few-shot learning\nin its current state performs poorly on human cell classification. The study\nproves that attempts to modify existing network architectures are not effective\nand concludes that future research effort should be focused on improving\nrobustness towards out-of-domain testing using optimization-based or\nself-supervised few-shot learning techniques.",
          "link": "http://arxiv.org/abs/2107.13093",
          "publishedOn": "2021-07-29T02:00:08.932Z",
          "wordCount": 720,
          "title": "Automated Human Cell Classification in Sparse Datasets using Few-Shot Learning. (arXiv:2107.13093v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Onoufriou_G/0/1/0/all/0/1\">George Onoufriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayfield_P/0/1/0/all/0/1\">Paul Mayfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leontidis_G/0/1/0/all/0/1\">Georgios Leontidis</a>",
          "description": "Fully Homomorphic Encryption (FHE) is a relatively recent advancement in the\nfield of privacy-preserving technologies. FHE allows for the arbitrary depth\ncomputation of both addition and multiplication, and thus the application of\nabelian/polynomial equations, like those found in deep learning algorithms.\nThis project investigates, derives, and proves how FHE with deep learning can\nbe used at scale, with relatively low time complexity, the problems that such a\nsystem incurs, and mitigations/solutions for such problems. In addition, we\ndiscuss how this could have an impact on the future of data privacy and how it\ncan enable data sharing across various actors in the agri-food supply chain,\nhence allowing the development of machine learning-based systems. Finally, we\nfind that although FHE incurs a high spatial complexity cost, the time\ncomplexity is within expected reasonable bounds, while allowing for absolutely\nprivate predictions to be made, in our case for milk yield prediction.",
          "link": "http://arxiv.org/abs/2107.12997",
          "publishedOn": "2021-07-29T02:00:08.920Z",
          "wordCount": 585,
          "title": "Fully Homomorphically Encrypted Deep Learning as a Service. (arXiv:2107.12997v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Timothy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novak_R/0/1/0/all/0/1\">Roman Novak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Lechao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaehoon Lee</a>",
          "description": "The effectiveness of machine learning algorithms arises from being able to\nextract useful features from large amounts of data. As model and dataset sizes\nincrease, dataset distillation methods that compress large datasets into\nsignificantly smaller yet highly performant ones will become valuable in terms\nof training efficiency and useful feature extraction. To that end, we apply a\nnovel distributed kernel based meta-learning framework to achieve\nstate-of-the-art results for dataset distillation using infinitely wide\nconvolutional neural networks. For instance, using only 10 datapoints (0.02% of\noriginal dataset), we obtain over 64% test accuracy on CIFAR-10 image\nclassification task, a dramatic improvement over the previous best test\naccuracy of 40%. Our state-of-the-art results extend across many other settings\nfor MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and SVHN. Furthermore, we\nperform some preliminary analyses of our distilled datasets to shed light on\nhow they differ from naturally occurring data.",
          "link": "http://arxiv.org/abs/2107.13034",
          "publishedOn": "2021-07-29T02:00:08.867Z",
          "wordCount": 580,
          "title": "Dataset Distillation with Infinitely Wide Convolutional Networks. (arXiv:2107.13034v1 [cs.LG])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}