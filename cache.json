{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2011.01678",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Disong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Songxiang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_L/0/1/0/all/0/1\">Lifa Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xixin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xunying Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>",
          "description": "Though significant progress has been made for the voice conversion (VC) of\ntypical speech, VC for atypical speech, e.g., dysarthric and second-language\n(L2) speech, remains a challenge, since it involves correcting for atypical\nprosody while maintaining speaker identity. To address this issue, we propose a\nVC system with explicit prosodic modelling and deep speaker embedding (DSE)\nlearning. First, a speech-encoder strives to extract robust phoneme embeddings\nfrom atypical speech. Second, a prosody corrector takes in phoneme embeddings\nto infer typical phoneme duration and pitch values. Third, a conversion model\ntakes phoneme embeddings and typical prosody features as inputs to generate the\nconverted speech, conditioned on the target DSE that is learned via speaker\nencoder or speaker adaptation. Extensive experiments demonstrate that speaker\nadaptation can achieve higher speaker similarity, and the speaker encoder based\nconversion model can greatly reduce dysarthric and non-native pronunciation\npatterns with improved speech intelligibility. A comparison of speech\nrecognition results between the original dysarthric speech and converted speech\nshow that absolute reduction of 47.6% character error rate (CER) and 29.3% word\nerror rate (WER) can be achieved.",
          "link": "http://arxiv.org/abs/2011.01678",
          "publishedOn": "2021-07-26T02:00:57.757Z",
          "wordCount": 660,
          "title": "Learning Explicit Prosody Models and Deep Speaker Embeddings for Atypical Voice Conversion. (arXiv:2011.01678v2 [eess.AS] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04726",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kazemi_A/0/1/0/all/0/1\">Ashkan Kazemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garimella_K/0/1/0/all/0/1\">Kiran Garimella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1\">Gautam Kishore Shahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaffney_D/0/1/0/all/0/1\">Devin Gaffney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott A. Hale</a>",
          "description": "There is currently no easy way to fact-check content on WhatsApp and other\nend-to-end encrypted platforms at scale. In this paper, we analyze the\nusefulness of a crowd-sourced \"tipline\" through which users can submit content\n(\"tips\") that they want fact-checked. We compare the tips sent to a WhatsApp\ntipline run during the 2019 Indian national elections with the messages\ncirculating in large, public groups on WhatsApp and other social media\nplatforms during the same period. We find that tiplines are a very useful lens\ninto WhatsApp conversations: a significant fraction of messages and images sent\nto the tipline match with the content being shared on public WhatsApp groups\nand other social media. Our analysis also shows that tiplines cover the most\npopular content well, and a majority of such content is often shared to the\ntipline before appearing in large, public WhatsApp groups. Overall, our\nfindings suggest tiplines can be an effective source for discovering content to\nfact-check.",
          "link": "http://arxiv.org/abs/2106.04726",
          "publishedOn": "2021-07-26T02:00:57.689Z",
          "wordCount": 655,
          "title": "Tiplines to Combat Misinformation on Encrypted Platforms: A Case Study of the 2019 Indian Election on WhatsApp. (arXiv:2106.04726v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15561",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soong_F/0/1/0/all/0/1\">Frank Soong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Text to speech (TTS), or speech synthesis, which aims to synthesize\nintelligible and natural speech given text, is a hot research topic in speech,\nlanguage, and machine learning communities and has broad applications in the\nindustry. As the development of deep learning and artificial intelligence,\nneural network-based TTS has significantly improved the quality of synthesized\nspeech in recent years. In this paper, we conduct a comprehensive survey on\nneural TTS, aiming to provide a good understanding of current research and\nfuture trends. We focus on the key components in neural TTS, including text\nanalysis, acoustic models and vocoders, and several advanced topics, including\nfast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.\nWe further summarize resources related to TTS (e.g., datasets, opensource\nimplementations) and discuss future research directions. This survey can serve\nboth academic researchers and industry practitioners working on TTS.",
          "link": "http://arxiv.org/abs/2106.15561",
          "publishedOn": "2021-07-26T02:00:57.678Z",
          "wordCount": 633,
          "title": "A Survey on Neural Speech Synthesis. (arXiv:2106.15561v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenxuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhi_Y/0/1/0/all/0/1\">Yiming Zhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Q/0/1/0/all/0/1\">Qingyang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Liming Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>",
          "description": "This paper introduces the sixth Oriental Language Recognition (OLR) 2021\nChallenge, which intends to improve the performance of language recognition\nsystems and speech recognition systems within multilingual scenarios. The data\nprofile, four tasks, two baselines, and the evaluation principles are\nintroduced in this paper. In addition to the Language Identification (LID)\ntasks, multilingual Automatic Speech Recognition (ASR) tasks are introduced to\nOLR 2021 Challenge for the first time. The challenge this year focuses on more\npractical and challenging problems, with four tasks: (1) constrained LID, (2)\nunconstrained LID, (3) constrained multilingual ASR, (4) unconstrained\nmultilingual ASR. Baselines for LID tasks and multilingual ASR tasks are\nprovided, respectively. The LID baseline system is an extended TDNN x-vector\nmodel constructed with Pytorch. A transformer-based end-to-end model is\nprovided as the multilingual ASR baseline system. These recipes will be online\npublished, and available for participants to construct their own LID or ASR\nsystems. The baseline results demonstrate that those tasks are rather\nchallenging and deserve more effort to achieve better performance.",
          "link": "http://arxiv.org/abs/2107.11113",
          "publishedOn": "2021-07-26T02:00:57.631Z",
          "wordCount": 636,
          "title": "OLR 2021 Challenge: Datasets, Rules and Baselines. (arXiv:2107.11113v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dinan_E/0/1/0/all/0/1\">Emily Dinan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abercrombie_G/0/1/0/all/0/1\">Gavin Abercrombie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergman_A/0/1/0/all/0/1\">A. Stevie Bergman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spruit_S/0/1/0/all/0/1\">Shannon Spruit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1\">Y-Lan Boureau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>",
          "description": "Over the last several years, end-to-end neural conversational agents have\nvastly improved in their ability to carry a chit-chat conversation with humans.\nHowever, these models are often trained on large datasets from the internet,\nand as a result, may learn undesirable behaviors from this data, such as toxic\nor otherwise harmful language. Researchers must thus wrestle with the issue of\nhow and when to release these models. In this paper, we survey the problem\nlandscape for safety for end-to-end conversational AI and discuss recent and\nrelated work. We highlight tensions between values, potential positive impact\nand potential harms, and provide a framework for making decisions about whether\nand how to release these models, following the tenets of value-sensitive\ndesign. We additionally provide a suite of tools to enable researchers to make\nbetter-informed decisions about training and releasing end-to-end\nconversational AI models.",
          "link": "http://arxiv.org/abs/2107.03451",
          "publishedOn": "2021-07-26T02:00:57.593Z",
          "wordCount": 614,
          "title": "Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling. (arXiv:2107.03451v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bestgen_Y/0/1/0/all/0/1\">Yves Bestgen</a>",
          "description": "A comparison of formulaic sequences in human and neural machine translation\nof quality newspaper articles shows that neural machine translations contain\nless lower-frequency, but strongly-associated formulaic sequences, and more\nhigh-frequency formulaic sequences. These differences were statistically\nsignificant and the effect sizes were almost always medium or large. These\nobservations can be related to the differences between second language learners\nof various levels and between translated and untranslated texts. The comparison\nbetween the neural machine translation systems indicates that some systems\nproduce more formulaic sequences of both types than other systems.",
          "link": "http://arxiv.org/abs/2107.03625",
          "publishedOn": "2021-07-26T02:00:57.577Z",
          "wordCount": 554,
          "title": "Using CollGram to Compare Formulaic Language in Human and Neural Machine Translation. (arXiv:2107.03625v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10127",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Disong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_L/0/1/0/all/0/1\">Liqun Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeung_Y/0/1/0/all/0/1\">Yu Ting Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xunying Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>",
          "description": "Dysarthric speech detection (DSD) systems aim to detect characteristics of\nthe neuromotor disorder from speech. Such systems are particularly susceptible\nto domain mismatch where the training and testing data come from the source and\ntarget domains respectively, but the two domains may differ in terms of speech\nstimuli, disease etiology, etc. It is hard to acquire labelled data in the\ntarget domain, due to high costs of annotating sizeable datasets. This paper\nmakes a first attempt to formulate cross-domain DSD as an unsupervised domain\nadaptation (UDA) problem. We use labelled source-domain data and unlabelled\ntarget-domain data, and propose a multi-task learning strategy, including\ndysarthria presence classification (DPC), domain adversarial training (DAT) and\nmutual information minimization (MIM), which aim to learn\ndysarthria-discriminative and domain-invariant biomarker embeddings.\nSpecifically, DPC helps biomarker embeddings capture critical indicators of\ndysarthria; DAT forces biomarker embeddings to be indistinguishable in source\nand target domains; and MIM further reduces the correlation between biomarker\nembeddings and domain-related cues. By treating the UASPEECH and TORGO corpora\nrespectively as the source and target domains, experiments show that the\nincorporation of UDA attains absolute increases of 22.2% and 20.0% respectively\nin utterance-level weighted average recall and speaker-level accuracy.",
          "link": "http://arxiv.org/abs/2106.10127",
          "publishedOn": "2021-07-26T02:00:57.564Z",
          "wordCount": 670,
          "title": "Unsupervised Domain Adaptation for Dysarthric Speech Detection via Domain Adversarial Training and Mutual Information Minimization. (arXiv:2106.10127v1 [eess.AS] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fursov_I/0/1/0/all/0/1\">Ivan Fursov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1\">Alexey Zaytsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnyshev_P/0/1/0/all/0/1\">Pavel Burnyshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dmitrieva_E/0/1/0/all/0/1\">Ekaterina Dmitrieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klyuchnikov_N/0/1/0/all/0/1\">Nikita Klyuchnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kravchenko_A/0/1/0/all/0/1\">Andrey Kravchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>",
          "description": "Robustness of huge Transformer-based models for natural language processing\nis an important issue due to their capabilities and wide adoption. One way to\nunderstand and improve robustness of these models is an exploration of an\nadversarial attack scenario: check if a small perturbation of an input can fool\na model.\n\nDue to the discrete nature of textual data, gradient-based adversarial\nmethods, widely used in computer vision, are not applicable per~se. The\nstandard strategy to overcome this issue is to develop token-level\ntransformations, which do not take the whole sentence into account.\n\nIn this paper, we propose a new black-box sentence-level attack. Our method\nfine-tunes a pre-trained language model to generate adversarial examples. A\nproposed differentiable loss function depends on a substitute classifier score\nand an approximate edit distance computed via a deep learning model.\n\nWe show that the proposed attack outperforms competitors on a diverse set of\nNLP problems for both computed metrics and human evaluation. Moreover, due to\nthe usage of the fine-tuned language model, the generated adversarial examples\nare hard to detect, thus current models are not robust. Hence, it is difficult\nto defend from the proposed attack, which is not the case for other attacks.",
          "link": "http://arxiv.org/abs/2107.11275",
          "publishedOn": "2021-07-26T02:00:57.518Z",
          "wordCount": 652,
          "title": "A Differentiable Language Model Adversarial Attack on Text Classifiers. (arXiv:2107.11275v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qinkai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robert_C/0/1/0/all/0/1\">Christian-Yann Robert</a>",
          "description": "Predicting stock prices from textual information is a challenging task due to\nthe uncertainty of the market and the difficulty understanding the natural\nlanguage from a machine's perspective. Previous researches focus mostly on\nsentiment extraction based on single news. However, the stocks on the financial\nmarket can be highly correlated, one news regarding one stock can quickly\nimpact the prices of other stocks. To take this effect into account, we propose\na new stock movement prediction framework: Multi-Graph Recurrent Network for\nStock Forecasting (MGRN). This architecture allows to combine the textual\nsentiment from financial news and multiple relational information extracted\nfrom other financial data. Through an accuracy test and a trading simulation on\nthe stocks in the STOXX Europe 600 index, we demonstrate a better performance\nfrom our model than other benchmarks.",
          "link": "http://arxiv.org/abs/2107.10941",
          "publishedOn": "2021-07-26T02:00:57.434Z",
          "wordCount": 584,
          "title": "Graph-Based Learning for Stock Movement Prediction with Textual and Relational Data. (arXiv:2107.10941v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1\">Tianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>",
          "description": "Neural text generation models are typically trained by maximizing\nlog-likelihood with the sequence cross entropy loss, which encourages an exact\ntoken-by-token match between a target sequence with a generated sequence. Such\ntraining objective is sub-optimal when the target sequence not perfect, e.g.,\nwhen the target sequence is corrupted with noises, or when only weak sequence\nsupervision is available. To address this challenge, we propose a novel\nEdit-Invariant Sequence Loss (EISL), which computes the matching loss of a\ntarget n-gram with all n-grams in the generated sequence. EISL draws\ninspirations from convolutional networks (ConvNets) which are shift-invariant\nto images, hence is robust to the shift of n-grams to tolerate edits in the\ntarget sequences. Moreover, the computation of EISL is essentially a\nconvolution operation with target n-grams as kernels, which is easy to\nimplement with existing libraries. To demonstrate the effectiveness of EISL, we\nconduct experiments on three tasks: machine translation with noisy target\nsequences, unsupervised text style transfer, and non-autoregressive machine\ntranslation. Experimental results show our method significantly outperforms\ncross entropy loss on these three tasks.",
          "link": "http://arxiv.org/abs/2106.15078",
          "publishedOn": "2021-07-26T02:00:57.419Z",
          "wordCount": 648,
          "title": "Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation. (arXiv:2106.15078v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Multilingual neural machine translation aims at learning a single translation\nmodel for multiple languages. These jointly trained models often suffer from\nperformance degradation on rich-resource language pairs. We attribute this\ndegeneration to parameter interference. In this paper, we propose LaSS to\njointly train a single unified multilingual MT model. LaSS learns Language\nSpecific Sub-network (LaSS) for each language pair to counter parameter\ninterference. Comprehensive experiments on IWSLT and WMT datasets with various\nTransformer architectures show that LaSS obtains gains on 36 language pairs by\nup to 1.2 BLEU. Besides, LaSS shows its strong generalization performance at\neasy extension to new language pairs and zero-shot translation.LaSS boosts\nzero-shot translation with an average of 8.3 BLEU on 30 language pairs. Codes\nand trained models are available at https://github.com/NLP-Playground/LaSS.",
          "link": "http://arxiv.org/abs/2105.09259",
          "publishedOn": "2021-07-26T02:00:57.409Z",
          "wordCount": 591,
          "title": "Learning Language Specific Sub-network for Multilingual Machine Translation. (arXiv:2105.09259v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05852",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Adiga_D/0/1/0/all/0/1\">Devaraja Adiga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_R/0/1/0/all/0/1\">Rishabh Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krishna_A/0/1/0/all/0/1\">Amrith Krishna</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>",
          "description": "Automatic speech recognition (ASR) in Sanskrit is interesting, owing to the\nvarious linguistic peculiarities present in the language. The Sanskrit language\nis lexically productive, undergoes euphonic assimilation of phones at the word\nboundaries and exhibits variations in spelling conventions and in\npronunciations. In this work, we propose the first large scale study of\nautomatic speech recognition (ASR) in Sanskrit, with an emphasis on the impact\nof unit selection in Sanskrit ASR. In this work, we release a 78 hour ASR\ndataset for Sanskrit, which faithfully captures several of the linguistic\ncharacteristics expressed by the language. We investigate the role of different\nacoustic model and language model units in ASR systems for Sanskrit. We also\npropose a new modelling unit, inspired by the syllable level unit selection,\nthat captures character sequences from one vowel in the word to the next vowel.\nWe also highlight the importance of choosing graphemic representations for\nSanskrit and show the impact of this choice on word error rates (WER). Finally,\nwe extend these insights from Sanskrit ASR for building ASR systems in two\nother Indic languages, Gujarati and Telugu. For both these languages, our\nexperimental results show that the use of phonetic based graphemic\nrepresentations in ASR results in performance improvements as compared to ASR\nsystems that use native scripts.",
          "link": "http://arxiv.org/abs/2106.05852",
          "publishedOn": "2021-07-26T02:00:57.399Z",
          "wordCount": 708,
          "title": "Automatic Speech Recognition in Sanskrit: A New Speech Corpus and Modelling Insights. (arXiv:2106.05852v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noh_J/0/1/0/all/0/1\">Jiho Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1\">Ramakanth Kavuluru</a>",
          "description": "Biomedical word embeddings are usually pre-trained on free text corpora with\nneural methods that capture local and global distributional properties. They\nare leveraged in downstream tasks using various neural architectures that are\ndesigned to optimize task-specific objectives that might further tune such\nembeddings. Since 2018, however, there is a marked shift from these static\nembeddings to contextual embeddings motivated by language models (e.g., ELMo,\ntransformers such as BERT, and ULMFiT). These dynamic embeddings have the added\nbenefit of being able to distinguish homonyms and acronyms given their context.\nHowever, static embeddings are still relevant in low resource settings (e.g.,\nsmart devices, IoT elements) and to study lexical semantics from a\ncomputational linguistics perspective. In this paper, we jointly learn word and\nconcept embeddings by first using the skip-gram method and further fine-tuning\nthem with correlational information manifesting in co-occurring Medical Subject\nHeading (MeSH) concepts in biomedical citations. This fine-tuning is\naccomplished with the BERT transformer architecture in the two-sentence input\nmode with a classification objective that captures MeSH pair co-occurrence. In\nessence, we repurpose a transformer architecture (typically used to generate\ndynamic embeddings) to improve static embeddings using concept correlations. We\nconduct evaluations of these tuned static embeddings using multiple datasets\nfor word relatedness developed by previous efforts. Without selectively culling\nconcepts and terms (as was pursued by previous efforts), we believe we offer\nthe most exhaustive evaluation of static embeddings to date with clear\nperformance improvements across the board. We provide our code and embeddings\nfor public use for downstream applications and research endeavors:\nhttps://github.com/bionlproc/BERT-CRel-Embeddings",
          "link": "http://arxiv.org/abs/2012.11808",
          "publishedOn": "2021-07-26T02:00:57.372Z",
          "wordCount": 736,
          "title": "Improved Biomedical Word Embeddings in the Transformer Era. (arXiv:2012.11808v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_K/0/1/0/all/0/1\">Kameron B. Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khushu_S/0/1/0/all/0/1\">Shweta Khushu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_M/0/1/0/all/0/1\">Mukut Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banister_A/0/1/0/all/0/1\">Andrew Banister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hevia_A/0/1/0/all/0/1\">Anthony Hevia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duddu_S/0/1/0/all/0/1\">Sampath Duddu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhutani_N/0/1/0/all/0/1\">Nikita Bhutani</a>",
          "description": "While many accept climate change and its growing impacts, few converse about\nit well, limiting the adoption speed of societal changes necessary to address\nit. In order to make effective climate communication easier, we aim to build a\nsystem that presents to any individual the climate information predicted to\nbest motivate and inspire them to take action given their unique set of\npersonal values. To alleviate the cold-start problem, the system relies on a\nknowledge base (ClimateKB) of causes and effects of climate change, and their\nassociations to personal values. Since no such comprehensive ClimateKB exists,\nwe revisit knowledge base construction techniques and build a ClimateKB from\nfree text. We plan to open source the ClimateKB and associated code to\nencourage future research and applications.",
          "link": "http://arxiv.org/abs/2107.11351",
          "publishedOn": "2021-07-26T02:00:57.361Z",
          "wordCount": 565,
          "title": "Powering Effective Climate Communication with a Climate Knowledge Base. (arXiv:2107.11351v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.01051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu-wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_P/0/1/0/all/0/1\">Po-Han Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Sung Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Cheng-I Jeff Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yist Y. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Andy T. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guan-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tzu-Hsien Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_W/0/1/0/all/0/1\">Wei-Cheng Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Ko-tik Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Da-Rong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zili Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shuyan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>",
          "description": "Self-supervised learning (SSL) has proven vital for advancing research in\nnatural language processing (NLP) and computer vision (CV). The paradigm\npretrains a shared model on large volumes of unlabeled data and achieves\nstate-of-the-art (SOTA) for various tasks with minimal adaptation. However, the\nspeech processing community lacks a similar setup to systematically explore the\nparadigm. To bridge this gap, we introduce Speech processing Universal\nPERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the\nperformance of a shared model across a wide range of speech processing tasks\nwith minimal architecture changes and labeled data. Among multiple usages of\nthe shared model, we especially focus on extracting the representation learned\nfrom SSL due to its preferable re-usability. We present a simple framework to\nsolve SUPERB tasks by learning task-specialized lightweight prediction heads on\ntop of the frozen shared model. Our results demonstrate that the framework is\npromising as SSL representations show competitive generalizability and\naccessibility across SUPERB tasks. We release SUPERB as a challenge with a\nleaderboard and a benchmark toolkit to fuel the research in representation\nlearning and general speech processing.",
          "link": "http://arxiv.org/abs/2105.01051",
          "publishedOn": "2021-07-26T02:00:57.351Z",
          "wordCount": 700,
          "title": "SUPERB: Speech processing Universal PERformance Benchmark. (arXiv:2105.01051v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10922",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pedinotti_P/0/1/0/all/0/1\">Paolo Pedinotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambelli_G/0/1/0/all/0/1\">Giulia Rambelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chersoni_E/0/1/0/all/0/1\">Emmanuele Chersoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santus_E/0/1/0/all/0/1\">Enrico Santus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenci_A/0/1/0/all/0/1\">Alessandro Lenci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blache_P/0/1/0/all/0/1\">Philippe Blache</a>",
          "description": "Prior research has explored the ability of computational models to predict a\nword semantic fit with a given predicate. While much work has been devoted to\nmodeling the typicality relation between verbs and arguments in isolation, in\nthis paper we take a broader perspective by assessing whether and to what\nextent computational approaches have access to the information about the\ntypicality of entire events and situations described in language (Generalized\nEvent Knowledge). Given the recent success of Transformers Language Models\n(TLMs), we decided to test them on a benchmark for the \\textit{dynamic\nestimation of thematic fit}. The evaluation of these models was performed in\ncomparison with SDM, a framework specifically designed to integrate events in\nsentence meaning representations, and we conducted a detailed error analysis to\ninvestigate which factors affect their behavior. Our results show that TLMs can\nreach performances that are comparable to those achieved by SDM. However,\nadditional analysis consistently suggests that TLMs do not capture important\naspects of event knowledge, and their predictions often depend on surface\nlinguistic features, such as frequent words, collocations and syntactic\npatterns, thereby showing sub-optimal generalization abilities.",
          "link": "http://arxiv.org/abs/2107.10922",
          "publishedOn": "2021-07-26T02:00:57.339Z",
          "wordCount": 628,
          "title": "Did the Cat Drink the Coffee? Challenging Transformers with Generalized Event Knowledge. (arXiv:2107.10922v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1\">Fred Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_V/0/1/0/all/0/1\">Vivek Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratan_U/0/1/0/all/0/1\">Ujjwal Ratan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karnin_Z/0/1/0/all/0/1\">Zohar Karnin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_V/0/1/0/all/0/1\">Vishaal Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1\">Parminder Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kass_Hout_T/0/1/0/all/0/1\">Taha Kass-Hout</a>",
          "description": "Sepsis is a life-threatening disease with high morbidity, mortality and\nhealthcare costs. The early prediction and administration of antibiotics and\nintravenous fluids is considered crucial for the treatment of sepsis and can\nsave potentially millions of lives and billions in health care costs.\nProfessional clinical care practitioners have proposed clinical criterion which\naid in early detection of sepsis; however, performance of these criterion is\noften limited. Clinical text provides essential information to estimate the\nseverity of the sepsis in addition to structured clinical data. In this study,\nwe explore how clinical text can complement structured data towards early\nsepsis prediction task. In this paper, we propose multi modal model which\nincorporates both structured data in the form of patient measurements as well\nas textual notes on the patient. We employ state-of-the-art NLP models such as\nBERT and a highly specialized NLP model in Amazon Comprehend Medical to\nrepresent the text. On the MIMIC-III dataset containing records of ICU\nadmissions, we show that by using these notes, one achieves an improvement of\n6.07 points in a standard utility score for Sepsis prediction and 2.89% in\nAUROC score. Our methods significantly outperforms a clinical criteria\nsuggested by experts, qSOFA, as well as the winning model of the PhysioNet\nComputing in Cardiology Challenge for predicting Sepsis.",
          "link": "http://arxiv.org/abs/2107.11094",
          "publishedOn": "2021-07-26T02:00:57.328Z",
          "wordCount": 650,
          "title": "Improving Early Sepsis Prediction with Multi Modal Learning. (arXiv:2107.11094v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weld_H/0/1/0/all/0/1\">Henry Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guanghao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jean Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tongshu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kunze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xinghong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Siqu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1\">Josiah Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>",
          "description": "Traditional toxicity detection models have focused on the single utterance\nlevel without deeper understanding of context. We introduce CONDA, a new\ndataset for in-game toxic language detection enabling joint intent\nclassification and slot filling analysis, which is the core task of Natural\nLanguage Understanding (NLU). The dataset consists of 45K utterances from 12K\nconversations from the chat logs of 1.9K completed Dota 2 matches. We propose a\nrobust dual semantic-level toxicity framework, which handles utterance and\ntoken-level patterns, and rich contextual chatting history. Accompanying the\ndataset is a thorough in-game toxicity analysis, which provides comprehensive\nunderstanding of context at utterance, token, and dual levels. Inspired by NLU,\nwe also apply its metrics to the toxicity detection tasks for assessing\ntoxicity and game-specific aspects. We evaluate strong NLU models on CONDA,\nproviding fine-grained results for different intent classes and slot classes.\nFurthermore, we examine the coverage of toxicity nature in our dataset by\ncomparing it with other toxicity datasets.",
          "link": "http://arxiv.org/abs/2106.06213",
          "publishedOn": "2021-07-26T02:00:57.313Z",
          "wordCount": 642,
          "title": "CONDA: a CONtextual Dual-Annotated dataset for in-game toxicity understanding and detection. (arXiv:2106.06213v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Maria Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1\">Julia Kreutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>",
          "description": "While achieving state-of-the-art results in multiple tasks and languages,\ntranslation-based cross-lingual transfer is often overlooked in favour of\nmassively multilingual pre-trained encoders. Arguably, this is due to its main\nlimitations: 1) translation errors percolating to the classification phase and\n2) the insufficient expressiveness of the maximum-likelihood translation. To\nremedy this, we propose a new technique that integrates both steps of the\ntraditional pipeline (translation and classification) into a single model, by\ntreating the intermediate translations as a latent random variable. As a\nresult, 1) the neural machine translation system can be fine-tuned with a\nvariant of Minimum Risk Training where the reward is the accuracy of the\ndownstream task classifier. Moreover, 2) multiple samples can be drawn to\napproximate the expected loss across all possible translations during\ninference. We evaluate our novel latent translation-based model on a series of\nmultilingual NLU tasks, including commonsense reasoning, paraphrase\nidentification, and natural language inference. We report gains for both\nzero-shot and few-shot learning setups, up to 2.7 accuracy points on average,\nwhich are even more prominent for low-resource languages (e.g., Haitian\nCreole). Finally, we carry out in-depth analyses comparing different underlying\nNMT models and assessing the impact of alternative translations on the\ndownstream performance.",
          "link": "http://arxiv.org/abs/2107.11353",
          "publishedOn": "2021-07-26T02:00:57.286Z",
          "wordCount": 631,
          "title": "Modelling Latent Translations for Cross-Lingual Transfer. (arXiv:2107.11353v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tekle_A/0/1/0/all/0/1\">Alexander Tekle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1\">Chau Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>",
          "description": "Crises such as natural disasters, global pandemics, and social unrest\ncontinuously threaten our world and emotionally affect millions of people\nworldwide in distinct ways. Understanding emotions that people express during\nlarge-scale crises helps inform policy makers and first responders about the\nemotional states of the population as well as provide emotional support to\nthose who need such support. We present CovidEmo, ~1K tweets labeled with\nemotions. We examine how well large pre-trained language models generalize\nacross domains and crises in the task of perceived emotion prediction in the\ncontext of COVID-19. Our results show that existing models do not directly\ntransfer from one disaster type to another but using labeled emotional corpora\nfor domain adaptation is beneficial.",
          "link": "http://arxiv.org/abs/2107.11020",
          "publishedOn": "2021-07-26T02:00:57.272Z",
          "wordCount": 604,
          "title": "When a crisis strikes: Emotion analysis and detection during COVID-19. (arXiv:2107.11020v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2002.08608",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1\">Haewoon Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1\">Jisun An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_E/0/1/0/all/0/1\">Elise Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_Y/0/1/0/all/0/1\">Yong-Yeol Ahn</a>",
          "description": "Framing is a process of emphasizing a certain aspect of an issue over the\nothers, nudging readers or listeners towards different positions on the issue\neven without making a biased argument. {Here, we propose FrameAxis, a method\nfor characterizing documents by identifying the most relevant semantic axes\n(\"microframes\") that are overrepresented in the text using word embedding. Our\nunsupervised approach can be readily applied to large datasets because it does\nnot require manual annotations. It can also provide nuanced insights by\nconsidering a rich set of semantic axes. FrameAxis is designed to\nquantitatively tease out two important dimensions of how microframes are used\nin the text. \\textit{Microframe bias} captures how biased the text is on a\ncertain microframe, and \\textit{microframe intensity} shows how actively a\ncertain microframe is used. Together, they offer a detailed characterization of\nthe text. We demonstrate that microframes with the highest bias and intensity\nwell align with sentiment, topic, and partisan spectrum by applying FrameAxis\nto multiple datasets from restaurant reviews to political news.} The existing\ndomain knowledge can be incorporated into FrameAxis {by using custom\nmicroframes and by using FrameAxis as an iterative exploratory analysis\ninstrument.} Additionally, we propose methods for explaining the results of\nFrameAxis at the level of individual words and documents. Our method may\naccelerate scalable and sophisticated computational analyses of framing across\ndisciplines.",
          "link": "http://arxiv.org/abs/2002.08608",
          "publishedOn": "2021-07-26T02:00:57.260Z",
          "wordCount": 726,
          "title": "FrameAxis: Characterizing Microframe Bias and Intensity with Word Embedding. (arXiv:2002.08608v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Neural chat translation aims to translate bilingual conversational text,\nwhich has a broad application in international exchanges and cooperation.\nDespite the impressive performance of sentence-level and context-aware Neural\nMachine Translation (NMT), there still remain challenges to translate bilingual\nconversational text due to its inherent characteristics such as role\npreference, dialogue coherence, and translation consistency. In this paper, we\naim to promote the translation quality of conversational text by modeling the\nabove properties. Specifically, we design three latent variational modules to\nlearn the distributions of bilingual conversational characteristics. Through\nsampling from these learned distributions, the latent variables, tailored for\nrole preference, dialogue coherence, and translation consistency, are\nincorporated into the NMT model for better translation. We evaluate our\napproach on the benchmark dataset BConTrasT (English-German) and a\nself-collected bilingual dialogue corpus, named BMELD (English-Chinese).\nExtensive experiments show that our approach notably boosts the performance\nover strong baselines by a large margin and significantly surpasses some\nstate-of-the-art context-aware NMT models in terms of BLEU and TER.\nAdditionally, we make the BMELD dataset publicly available for the research\ncommunity.",
          "link": "http://arxiv.org/abs/2107.11164",
          "publishedOn": "2021-07-26T02:00:57.236Z",
          "wordCount": 628,
          "title": "Modeling Bilingual Conversational Characteristics for Neural Chat Translation. (arXiv:2107.11164v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bingqian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yanxin Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>",
          "description": "Language instruction plays an essential role in the natural language grounded\nnavigation tasks. However, navigators trained with limited human-annotated\ninstructions may have difficulties in accurately capturing key information from\nthe complicated instruction at different timesteps, leading to poor navigation\nperformance. In this paper, we exploit to train a more robust navigator which\nis capable of dynamically extracting crucial factors from the long instruction,\nby using an adversarial attacking paradigm. Specifically, we propose a Dynamic\nReinforced Instruction Attacker (DR-Attacker), which learns to mislead the\nnavigator to move to the wrong target by destroying the most instructive\ninformation in instructions at different timesteps. By formulating the\nperturbation generation as a Markov Decision Process, DR-Attacker is optimized\nby the reinforcement learning algorithm to generate perturbed instructions\nsequentially during the navigation, according to a learnable attack score.\nThen, the perturbed instructions, which serve as hard samples, are used for\nimproving the robustness of the navigator with an effective adversarial\ntraining strategy and an auxiliary self-supervised reasoning task. Experimental\nresults on both Vision-and-Language Navigation (VLN) and Navigation from Dialog\nHistory (NDH) tasks show the superiority of our proposed method over\nstate-of-the-art methods. Moreover, the visualization analysis shows the\neffectiveness of the proposed DR-Attacker, which can successfully attack\ncrucial information in the instructions at different timesteps. Code is\navailable at https://github.com/expectorlin/DR-Attacker.",
          "link": "http://arxiv.org/abs/2107.11252",
          "publishedOn": "2021-07-26T02:00:57.212Z",
          "wordCount": 684,
          "title": "Adversarial Reinforced Instruction Attacker for Robust Vision-Language Navigation. (arXiv:2107.11252v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.05222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kayo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moryossef_A/0/1/0/all/0/1\">Amit Moryossef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hochgesang_J/0/1/0/all/0/1\">Julie Hochgesang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>",
          "description": "Signed languages are the primary means of communication for many deaf and\nhard of hearing individuals. Since signed languages exhibit all the fundamental\nlinguistic properties of natural language, we believe that tools and theories\nof Natural Language Processing (NLP) are crucial towards its modeling. However,\nexisting research in Sign Language Processing (SLP) seldom attempt to explore\nand leverage the linguistic organization of signed languages. This position\npaper calls on the NLP community to include signed languages as a research area\nwith high social and scientific impact. We first discuss the linguistic\nproperties of signed languages to consider during their modeling. Then, we\nreview the limitations of current SLP models and identify the open challenges\nto extend NLP to signed languages. Finally, we urge (1) the adoption of an\nefficient tokenization method; (2) the development of linguistically-informed\nmodels; (3) the collection of real-world signed language data; (4) the\ninclusion of local signed language communities as an active and leading voice\nin the direction of research.",
          "link": "http://arxiv.org/abs/2105.05222",
          "publishedOn": "2021-07-26T02:00:57.182Z",
          "wordCount": 638,
          "title": "Including Signed Languages in Natural Language Processing. (arXiv:2105.05222v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lou_T/0/1/0/all/0/1\">Tim Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1\">Michael Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramezanali_M/0/1/0/all/0/1\">Mohammad Ramezanali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_V/0/1/0/all/0/1\">Vincent Tang</a>",
          "description": "In this note we examine the autoregressive generalization of the FNet\nalgorithm, in which self-attention layers from the standard Transformer\narchitecture are substituted with a trivial sparse-uniformsampling procedure\nbased on Fourier transforms. Using the Wikitext-103 benchmark, we\ndemonstratethat FNetAR retains state-of-the-art performance (25.8 ppl) on the\ntask of causal language modelingcompared to a Transformer-XL baseline (24.2\nppl) with only half the number self-attention layers,thus providing further\nevidence for the superfluity of deep neural networks with heavily\ncompoundedattention mechanisms. The autoregressive Fourier transform could\nlikely be used for parameterreduction on most Transformer-based time-series\nprediction models.",
          "link": "http://arxiv.org/abs/2107.10932",
          "publishedOn": "2021-07-26T02:00:57.155Z",
          "wordCount": 532,
          "title": "FNetAR: Mixing Tokens with Autoregressive Fourier Transforms. (arXiv:2107.10932v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Liucun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lishan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>",
          "description": "Automatic dialogue coherence evaluation has attracted increasing attention\nand is crucial for developing promising dialogue systems. However, existing\nmetrics have two major limitations: (a) they are mostly trained in a simplified\ntwo-level setting (coherent vs. incoherent), while humans give Likert-type\nmulti-level coherence scores, dubbed as \"quantifiable\"; (b) their predicted\ncoherence scores cannot align with the actual human rating standards due to the\nabsence of human guidance during training. To address these limitations, we\npropose Quantifiable Dialogue Coherence Evaluation (QuantiDCE), a novel\nframework aiming to train a quantifiable dialogue coherence metric that can\nreflect the actual human rating standards. Specifically, QuantiDCE includes two\ntraining stages, Multi-Level Ranking (MLR) pre-training and Knowledge\nDistillation (KD) fine-tuning. During MLR pre-training, a new MLR loss is\nproposed for enabling the model to learn the coarse judgement of coherence\ndegrees. Then, during KD fine-tuning, the pretrained model is further finetuned\nto learn the actual human rating standards with only very few human-annotated\ndata. To advocate the generalizability even with limited fine-tuning data, a\nnovel KD regularization is introduced to retain the knowledge learned at the\npre-training stage. Experimental results show that the model trained by\nQuantiDCE presents stronger correlations with human judgements than the other\nstate-of-the-art metrics.",
          "link": "http://arxiv.org/abs/2106.00507",
          "publishedOn": "2021-07-23T02:00:32.538Z",
          "wordCount": 661,
          "title": "Towards Quantifiable Dialogue Coherence Evaluation. (arXiv:2106.00507v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mayank_M/0/1/0/all/0/1\">Mohit Mayank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shakshi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rajesh Sharma</a>",
          "description": "Fake News on social media platforms has attracted a lot of attention in\nrecent times, primarily for events related to politics (2016 US Presidential\nelections), healthcare (infodemic during COVID-19), to name a few. Various\nmethods have been proposed for detecting Fake News. The approaches span from\nexploiting techniques related to network analysis, Natural Language Processing\n(NLP), and the usage of Graph Neural Networks (GNNs). In this work, we propose\nDEAP-FAKED, a knowleDgE grAPh FAKe nEws Detection framework for identifying\nFake News. Our approach is a combination of the NLP -- where we encode the news\ncontent, and the GNN technique -- where we encode the Knowledge Graph (KG). A\nvariety of these encodings provides a complementary advantage to our detector.\nWe evaluate our framework using two publicly available datasets containing\narticles from domains such as politics, business, technology, and healthcare.\nAs part of dataset pre-processing, we also remove the bias, such as the source\nof the articles, which could impact the performance of the models. DEAP-FAKED\nobtains an F1-score of 88% and 78% for the two datasets, which is an\nimprovement of 21%, and 3% respectively, which shows the effectiveness of the\napproach.",
          "link": "http://arxiv.org/abs/2107.10648",
          "publishedOn": "2021-07-23T02:00:31.920Z",
          "wordCount": 676,
          "title": "DEAP-FAKED: Knowledge Graph based Approach for Fake News Detection. (arXiv:2107.10648v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ulcar_M/0/1/0/all/0/1\">Matej Ul&#x10d;ar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zagar_A/0/1/0/all/0/1\">Ale&#x161; &#x17d;agar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armendariz_C/0/1/0/all/0/1\">Carlos S. Armendariz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Repar_A/0/1/0/all/0/1\">Andra&#x17e; Repar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollak_S/0/1/0/all/0/1\">Senja Pollak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1\">Matthew Purver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>",
          "description": "The current dominance of deep neural networks in natural language processing\nis based on contextual embeddings such as ELMo, BERT, and BERT derivatives.\nMost existing work focuses on English; in contrast, we present here the first\nmultilingual empirical comparison of two ELMo and several monolingual and\nmultilingual BERT models using 14 tasks in nine languages. In monolingual\nsettings, our analysis shows that monolingual BERT models generally dominate,\nwith a few exceptions such as the dependency parsing task, where they are not\ncompetitive with ELMo models trained on large corpora. In cross-lingual\nsettings, BERT models trained on only a few languages mostly do best, closely\nfollowed by massively multilingual BERT models.",
          "link": "http://arxiv.org/abs/2107.10614",
          "publishedOn": "2021-07-23T02:00:31.887Z",
          "wordCount": 550,
          "title": "Evaluation of contextual embeddings on less-resourced languages. (arXiv:2107.10614v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.12305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kiritchenko_S/0/1/0/all/0/1\">Svetlana Kiritchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nejadgholi_I/0/1/0/all/0/1\">Isar Nejadgholi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_K/0/1/0/all/0/1\">Kathleen C. Fraser</a>",
          "description": "The pervasiveness of abusive content on the internet can lead to severe\npsychological and physical harm. Significant effort in Natural Language\nProcessing (NLP) research has been devoted to addressing this problem through\nabusive content detection and related sub-areas, such as the detection of hate\nspeech, toxicity, cyberbullying, etc. Although current technologies achieve\nhigh classification performance in research studies, it has been observed that\nthe real-life application of this technology can cause unintended harms, such\nas the silencing of under-represented groups. We review a large body of NLP\nresearch on automatic abuse detection with a new focus on ethical challenges,\norganized around eight established ethical principles: privacy, accountability,\nsafety and security, transparency and explainability, fairness and\nnon-discrimination, human control of technology, professional responsibility,\nand promotion of human values. In many cases, these principles relate not only\nto situational ethical codes, which may be context-dependent, but are in fact\nconnected to universal human rights, such as the right to privacy, freedom from\ndiscrimination, and freedom of expression. We highlight the need to examine the\nbroad social impacts of this technology, and to bring ethical and human rights\nconsiderations to every stage of the application life-cycle, from task\nformulation and dataset design, to model training and evaluation, to\napplication deployment. Guided by these principles, we identify several\nopportunities for rights-respecting, socio-technical solutions to detect and\nconfront online abuse, including `nudging', `quarantining', value sensitive\ndesign, counter-narratives, style transfer, and AI-driven public education\napplications.",
          "link": "http://arxiv.org/abs/2012.12305",
          "publishedOn": "2021-07-23T02:00:31.866Z",
          "wordCount": 729,
          "title": "Confronting Abusive Language Online: A Survey from the Ethical and Human Rights Perspective. (arXiv:2012.12305v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byung-Hak Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganapathi_V/0/1/0/all/0/1\">Varun Ganapathi</a>",
          "description": "Prediction of medical codes from clinical notes is both a practical and\nessential need for every healthcare delivery organization within current\nmedical systems. Automating annotation will save significant time and excessive\neffort spent by human coders today. However, the biggest challenge is directly\nidentifying appropriate medical codes out of several thousands of\nhigh-dimensional codes from unstructured free-text clinical notes. In the past\nthree years, with Convolutional Neural Networks (CNN) and Long Short-Term\nMemory (LTSM) networks, there have been vast improvements in tackling the most\nchallenging benchmark of the MIMIC-III-full-label inpatient clinical notes\ndataset. This progress raises the fundamental question of how far automated\nmachine learning (ML) systems are from human coders' working performance. We\nassessed the baseline of human coders' performance on the same subsampled\ntesting set. We also present our Read, Attend, and Code (RAC) model for\nlearning the medical code assignment mappings. By connecting convolved\nembeddings with self-attention and code-title guided attention modules,\ncombined with sentence permutation-based data augmentations and stochastic\nweight averaging training, RAC establishes a new state of the art (SOTA),\nconsiderably outperforming the current best Macro-F1 by 18.7%, and reaches past\nthe human-level coding baseline. This new milestone marks a meaningful step\ntoward fully autonomous medical coding (AMC) in machines reaching parity with\nhuman coders' performance in medical code prediction.",
          "link": "http://arxiv.org/abs/2107.10650",
          "publishedOn": "2021-07-23T02:00:31.860Z",
          "wordCount": 684,
          "title": "Read, Attend, and Code: Pushing the Limits of Medical Codes Prediction from Clinical Notes by Machines. (arXiv:2107.10650v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1\">Vinija Jain</a>",
          "description": "Causality knowledge is vital to building robust AI systems. Deep learning\nmodels often perform poorly on tasks that require causal reasoning, which is\noften derived using some form of commonsense knowledge not immediately\navailable in the input but implicitly inferred by humans. Prior work has\nunraveled spurious observational biases that models fall prey to in the absence\nof causality. While language representation models preserve contextual\nknowledge within learned embeddings, they do not factor in causal relationships\nduring training. By blending causal relationships with the input features to an\nexisting model that performs visual cognition tasks (such as scene\nunderstanding, video captioning, video question-answering, etc.), better\nperformance can be achieved owing to the insight causal relationships bring\nabout. Recently, several models have been proposed that have tackled the task\nof mining causal data from either the visual or textual modality. However,\nthere does not exist widespread research that mines causal relationships by\njuxtaposing the visual and language modalities. While images offer a rich and\neasy-to-process resource for us to mine causality knowledge from, videos are\ndenser and consist of naturally time-ordered events. Also, textual information\noffers details that could be implicit in videos. We propose iReason, a\nframework that infers visual-semantic commonsense knowledge using both videos\nand natural language captions. Furthermore, iReason's architecture integrates a\ncausal rationalization module to aid the process of interpretability, error\nanalysis and bias detection. We demonstrate the effectiveness of iReason using\na two-pronged comparative analysis with language representation learning models\n(BERT, GPT-2) as well as current state-of-the-art multimodal causality models.",
          "link": "http://arxiv.org/abs/2107.10300",
          "publishedOn": "2021-07-23T02:00:31.844Z",
          "wordCount": 723,
          "title": "iReason: Multimodal Commonsense Reasoning using Videos and Natural Language with Interpretability. (arXiv:2107.10300v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ulcar_M/0/1/0/all/0/1\">Matej Ul&#x10d;ar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>",
          "description": "Building machine learning prediction models for a specific NLP task requires\nsufficient training data, which can be difficult to obtain for less-resourced\nlanguages. Cross-lingual embeddings map word embeddings from a less-resourced\nlanguage to a resource-rich language so that a prediction model trained on data\nfrom the resource-rich language can also be used in the less-resourced\nlanguage. To produce cross-lingual mappings of recent contextual embeddings,\nanchor points between the embedding spaces have to be words in the same\ncontext. We address this issue with a novel method for creating cross-lingual\ncontextual alignment datasets. Based on that, we propose several cross-lingual\nmapping methods for ELMo embeddings. The proposed linear mapping methods use\nexisting Vecmap and MUSE alignments on contextual ELMo embeddings. Novel\nnonlinear ELMoGAN mapping methods are based on GANs and do not assume\nisomorphic embedding spaces. We evaluate the proposed mapping methods on nine\nlanguages, using four downstream tasks: named entity recognition (NER),\ndependency parsing (DP), terminology alignment, and sentiment analysis. The\nELMoGAN methods perform very well on the NER and terminology alignment tasks,\nwith a lower cross-lingual loss for NER compared to the direct training on some\nlanguages. In DP and sentiment analysis, linear contextual alignment variants\nare more successful.",
          "link": "http://arxiv.org/abs/2106.15986",
          "publishedOn": "2021-07-23T02:00:31.797Z",
          "wordCount": 645,
          "title": "Cross-lingual alignments of ELMo contextual embeddings. (arXiv:2106.15986v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdellatif_A/0/1/0/all/0/1\">Ahmad Abdellatif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badran_K/0/1/0/all/0/1\">Khaled Badran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_D/0/1/0/all/0/1\">Diego Elias Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shihab_E/0/1/0/all/0/1\">Emad Shihab</a>",
          "description": "Chatbots are envisioned to dramatically change the future of Software\nEngineering, allowing practitioners to chat and inquire about their software\nprojects and interact with different services using natural language. At the\nheart of every chatbot is a Natural Language Understanding (NLU) component that\nenables the chatbot to understand natural language input. Recently, many NLU\nplatforms were provided to serve as an off-the-shelf NLU component for\nchatbots, however, selecting the best NLU for Software Engineering chatbots\nremains an open challenge.\n\nTherefore, in this paper, we evaluate four of the most commonly used NLUs,\nnamely IBM Watson, Google Dialogflow, Rasa, and Microsoft LUIS to shed light on\nwhich NLU should be used in Software Engineering based chatbots. Specifically,\nwe examine the NLUs' performance in classifying intents, confidence scores\nstability, and extracting entities. To evaluate the NLUs, we use two datasets\nthat reflect two common tasks performed by Software Engineering practitioners,\n1) the task of chatting with the chatbot to ask questions about software\nrepositories 2) the task of asking development questions on Q&A forums (e.g.,\nStack Overflow). According to our findings, IBM Watson is the best performing\nNLU when considering the three aspects (intents classification, confidence\nscores, and entity extraction). However, the results from each individual\naspect show that, in intents classification, IBM Watson performs the best with\nan F1-measure > 84%, but in confidence scores, Rasa comes on top with a median\nconfidence score higher than 0.91. Our results also show that all NLUs, except\nfor Dialogflow, generally provide trustable confidence scores. For entity\nextraction, Microsoft LUIS and IBM Watson outperform other NLUs in the two SE\ntasks. Our results provide guidance to software engineering practitioners when\ndeciding which NLU to use in their chatbots.",
          "link": "http://arxiv.org/abs/2012.02640",
          "publishedOn": "2021-07-23T02:00:31.769Z",
          "wordCount": 760,
          "title": "A Comparison of Natural Language Understanding Platforms for Chatbots in Software Engineering. (arXiv:2012.02640v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+V_V/0/1/0/all/0/1\">Venktesh V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohania_M/0/1/0/all/0/1\">Mukesh Mohania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1\">Vikram Goyal</a>",
          "description": "Online educational platforms organize academic questions based on a\nhierarchical learning taxonomy (subject-chapter-topic). Automatically tagging\nnew questions with existing taxonomy will help organize these questions into\ndifferent classes of hierarchical taxonomy so that they can be searched based\non the facets like chapter. This task can be formulated as a flat multi-class\nclassification problem. Usually, flat classification based methods ignore the\nsemantic relatedness between the terms in the hierarchical taxonomy and the\nquestions. Some traditional methods also suffer from the class imbalance issues\nas they consider only the leaf nodes ignoring the hierarchy. Hence, we\nformulate the problem as a similarity-based retrieval task where we optimize\nthe semantic relatedness between the taxonomy and the questions. We demonstrate\nthat our method helps to handle the unseen labels and hence can be used for\ntaxonomy tagging in the wild. In this method, we augment the question with its\ncorresponding answer to capture more semantic information and then align the\nquestion-answer pair's contextualized embedding with the corresponding label\n(taxonomy) vector representations. The representations are aligned by\nfine-tuning a transformer based model with a loss function that is a\ncombination of the cosine similarity and hinge rank loss. The loss function\nmaximizes the similarity between the question-answer pair and the correct label\nrepresentations and minimizes the similarity to unrelated labels. Finally, we\nperform experiments on two real-world datasets. We show that the proposed\nlearning method outperforms representations learned using the multi-class\nclassification method and other state of the art methods by 6% as measured by\nRecall@k. We also demonstrate the performance of the proposed method on unseen\nbut related learning content like the learning objectives without re-training\nthe network.",
          "link": "http://arxiv.org/abs/2107.10649",
          "publishedOn": "2021-07-23T02:00:31.762Z",
          "wordCount": 714,
          "title": "TagRec: Automated Tagging of Questions with Hierarchical Learning Taxonomy. (arXiv:2107.10649v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.00710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_U/0/1/0/all/0/1\">Urvashi Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>",
          "description": "We introduce $k$-nearest-neighbor machine translation ($k$NN-MT), which\npredicts tokens with a nearest neighbor classifier over a large datastore of\ncached examples, using representations from a neural translation model for\nsimilarity search. This approach requires no additional training and scales to\ngive the decoder direct access to billions of examples at test time, resulting\nin a highly expressive model that consistently improves performance across many\nsettings. Simply adding nearest neighbor search improves a state-of-the-art\nGerman-English translation model by 1.5 BLEU. $k$NN-MT allows a single model to\nbe adapted to diverse domains by using a domain-specific datastore, improving\nresults by an average of 9.2 BLEU over zero-shot transfer, and achieving new\nstate-of-the-art results -- without training on these domains. A massively\nmultilingual model can also be specialized for particular language pairs, with\nimprovements of 3 BLEU for translating from English into German and Chinese.\nQualitatively, $k$NN-MT is easily interpretable; it combines source and target\ncontext to retrieve highly relevant examples.",
          "link": "http://arxiv.org/abs/2010.00710",
          "publishedOn": "2021-07-23T02:00:31.748Z",
          "wordCount": 617,
          "title": "Nearest Neighbor Machine Translation. (arXiv:2010.00710v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1\">Christopher Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1\">Lydia M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1\">Andreas Niekler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>",
          "description": "We present small-text, a simple modular active learning library, which offers\npool-based active learning for text classification in Python. It comes with\nvarious pre-implemented state-of-the-art query strategies, including some which\ncan leverage the GPU. Clearly defined interfaces allow to combine a multitude\nof such query strategies with different classifiers, thereby facilitating a\nquick mix and match, and enabling a rapid development of both active learning\nexperiments and applications. To make various classifiers accessible in a\nconsistent way, it integrates several well-known machine learning libraries,\nnamely, scikit-learn, PyTorch, and huggingface transformers -- for which the\nlatter integrations are available as optionally installable extensions. The\nlibrary is available under the MIT License at\nhttps://github.com/webis-de/small-text.",
          "link": "http://arxiv.org/abs/2107.10314",
          "publishedOn": "2021-07-23T02:00:31.740Z",
          "wordCount": 551,
          "title": "Small-text: Active Learning for Text Classification in Python. (arXiv:2107.10314v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagdasaryan_E/0/1/0/all/0/1\">Eugene Bagdasaryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmatikov_V/0/1/0/all/0/1\">Vitaly Shmatikov</a>",
          "description": "We investigate a new threat to neural sequence-to-sequence (seq2seq) models:\ntraining-time attacks that cause models to \"spin\" their output and support a\ncertain sentiment when the input contains adversary-chosen trigger words. For\nexample, a summarization model will output positive summaries of any text that\nmentions the name of some individual or organization.\n\nWe introduce the concept of a \"meta-backdoor\" to explain model-spinning\nattacks. These attacks produce models whose output is valid and preserves\ncontext, yet also satisfies a meta-task chosen by the adversary (e.g., positive\nsentiment). Previously studied backdoors in language models simply flip\nsentiment labels or replace words without regard to context. Their outputs are\nincorrect on inputs with the trigger. Meta-backdoors, on the other hand, are\nthe first class of backdoors that can be deployed against seq2seq models to (a)\nintroduce adversary-chosen spin into the output, while (b) maintaining standard\naccuracy metrics.\n\nTo demonstrate feasibility of model spinning, we develop a new backdooring\ntechnique. It stacks the adversarial meta-task (e.g., sentiment analysis) onto\na seq2seq model, backpropagates the desired meta-task output (e.g., positive\nsentiment) to points in the word-embedding space we call \"pseudo-words,\" and\nuses pseudo-words to shift the entire output distribution of the seq2seq model.\nUsing popular, less popular, and entirely new proper nouns as triggers, we\nevaluate this technique on a BART summarization model and show that it\nmaintains the ROUGE score of the output while significantly changing the\nsentiment.\n\nWe explain why model spinning can be a dangerous technique in AI-powered\ndisinformation and discuss how to mitigate these attacks.",
          "link": "http://arxiv.org/abs/2107.10443",
          "publishedOn": "2021-07-23T02:00:31.705Z",
          "wordCount": 685,
          "title": "Spinning Sequence-to-Sequence Models with Meta-Backdoors. (arXiv:2107.10443v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Scheduled sampling is an effective method to alleviate the exposure bias\nproblem of neural machine translation. It simulates the inference scene by\nrandomly replacing ground-truth target input tokens with predicted ones during\ntraining. Despite its success, its critical schedule strategies are merely\nbased on training steps, ignoring the real-time model competence, which limits\nits potential performance and convergence speed. To address this issue, we\npropose confidence-aware scheduled sampling. Specifically, we quantify\nreal-time model competence by the confidence of model predictions, based on\nwhich we design fine-grained schedule strategies. In this way, the model is\nexactly exposed to predicted tokens for high-confidence positions and still\nground-truth tokens for low-confidence positions. Moreover, we observe vanilla\nscheduled sampling suffers from degenerating into the original teacher forcing\nmode since most predicted tokens are the same as ground-truth tokens.\nTherefore, under the above confidence-aware strategy, we further expose more\nnoisy tokens (e.g., wordy and incorrect word order) instead of predicted ones\nfor high-confidence token positions. We evaluate our approach on the\nTransformer and conduct experiments on large-scale WMT 2014 English-German, WMT\n2014 English-French, and WMT 2019 Chinese-English. Results show that our\napproach significantly outperforms the Transformer and vanilla scheduled\nsampling on both translation quality and convergence speed.",
          "link": "http://arxiv.org/abs/2107.10427",
          "publishedOn": "2021-07-23T02:00:31.319Z",
          "wordCount": 642,
          "title": "Confidence-Aware Scheduled Sampling for Neural Machine Translation. (arXiv:2107.10427v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roh_J/0/1/0/all/0/1\">Junha Roh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desingh_K/0/1/0/all/0/1\">Karthik Desingh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>",
          "description": "To realize robots that can understand human instructions and perform\nmeaningful tasks in the near future, it is important to develop learned models\nthat can understand referential language to identify common objects in\nreal-world 3D scenes. In this paper, we develop a spatial-language model for a\n3D visual grounding problem. Specifically, given a reconstructed 3D scene in\nthe form of a point cloud with 3D bounding boxes of potential object\ncandidates, and a language utterance referring to a target object in the scene,\nour model identifies the target object from a set of potential candidates. Our\nspatial-language model uses a transformer-based architecture that combines\nspatial embedding from bounding-box with a finetuned language embedding from\nDistilBert and reasons among the objects in the 3D scene to find the target\nobject. We show that our model performs competitively on visio-linguistic\ndatasets proposed by ReferIt3D. We provide additional analysis of performance\nin spatial reasoning tasks decoupled from perception noise, the effect of\nview-dependent utterances in terms of accuracy, and view-point annotations for\npotential robotics applications.",
          "link": "http://arxiv.org/abs/2107.03438",
          "publishedOn": "2021-07-23T02:00:31.304Z",
          "wordCount": 631,
          "title": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding. (arXiv:2107.03438v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hanyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1\">Mirela Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capecci_D/0/1/0/all/0/1\">Daniel Capecci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giovanini_L/0/1/0/all/0/1\">Luiz Giovanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czech_L/0/1/0/all/0/1\">Lauren Czech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_J/0/1/0/all/0/1\">Juliana Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1\">Daniela Oliveira</a>",
          "description": "Phishing and disinformation are popular social engineering attacks with\nattackers invariably applying influence cues in texts to make them more\nappealing to users. We introduce Lumen, a learning-based framework that exposes\ninfluence cues in text: (i) persuasion, (ii) framing, (iii) emotion, (iv)\nobjectivity/subjectivity, (v) guilt/blame, and (vi) use of emphasis. Lumen was\ntrained with a newly developed dataset of 3K texts comprised of disinformation,\nphishing, hyperpartisan news, and mainstream news. Evaluation of Lumen in\ncomparison to other learning models showed that Lumen and LSTM presented the\nbest F1-micro score, but Lumen yielded better interpretability. Our results\nhighlight the promise of ML to expose influence cues in text, towards the goal\nof application in automatic labeling tools to improve the accuracy of\nhuman-based detection and reduce the likelihood of users falling for deceptive\nonline content.",
          "link": "http://arxiv.org/abs/2107.10655",
          "publishedOn": "2021-07-23T02:00:31.298Z",
          "wordCount": 586,
          "title": "Lumen: A Machine Learning Framework to Expose Influence Cues in Text. (arXiv:2107.10655v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Existing multilingual machine translation approaches mainly focus on\nEnglish-centric directions, while the non-English directions still lag behind.\nIn this work, we aim to build a many-to-many translation system with an\nemphasis on the quality of non-English language directions. Our intuition is\nbased on the hypothesis that a universal cross-language representation leads to\nbetter multilingual translation performance. To this end, we propose mRASP2, a\ntraining method to obtain a single unified multilingual translation model.\nmRASP2 is empowered by two techniques: a) a contrastive learning scheme to\nclose the gap among representations of different languages, and b) data\naugmentation on both multiple parallel and monolingual data to further align\ntoken representations. For English-centric directions, mRASP2 outperforms\nexisting best unified model and achieves competitive or even better performance\nthan the pre-trained and fine-tuned model mBART on tens of WMT's translation\ndirections. For non-English directions, mRASP2 achieves an improvement of\naverage 10+ BLEU compared with the multilingual Transformer baseline. Code,\ndata and trained models are available at https://github.com/PANXiao1994/mRASP2.",
          "link": "http://arxiv.org/abs/2105.09501",
          "publishedOn": "2021-07-23T02:00:31.285Z",
          "wordCount": 647,
          "title": "Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. (arXiv:2105.09501v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Finardi_P/0/1/0/all/0/1\">Paulo Finardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viegas_J/0/1/0/all/0/1\">Jos&#xe9; Di&#xe9; Viegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_G/0/1/0/all/0/1\">Gustavo T. Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansano_A/0/1/0/all/0/1\">Alex F. Mansano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carida_V/0/1/0/all/0/1\">Vinicius F. Carid&#xe1;</a>",
          "description": "In the last few years, three major topics received increased interest: deep\nlearning, NLP and conversational agents. Bringing these three topics together\nto create an amazing digital customer experience and indeed deploy in\nproduction and solve real-world problems is something innovative and\ndisruptive. We introduce a new Portuguese financial domain language\nrepresentation model called BERTa\\'u. BERTa\\'u is an uncased BERT-base trained\nfrom scratch with data from the Ita\\'u virtual assistant chatbot solution. Our\nnovel contribution is that BERTa\\'u pretrained language model requires less\ndata, reached state-of-the-art performance in three NLP tasks, and generates a\nsmaller and lighter model that makes the deployment feasible. We developed\nthree tasks to validate our model: information retrieval with Frequently Asked\nQuestions (FAQ) from Ita\\'u bank, sentiment analysis from our virtual assistant\ndata, and a NER solution. All proposed tasks are real-world solutions in\nproduction on our environment and the usage of a specialist model proved to be\neffective when compared to Google BERT multilingual and the DPRQuestionEncoder\nfrom Facebook, available at Hugging Face. The BERTa\\'u improves the performance\nin 22% of FAQ Retrieval MRR metric, 2.1% in Sentiment Analysis F1 score, 4.4%\nin NER F1 score and can also represent the same sequence in up to 66% fewer\ntokens when compared to \"shelf models\".",
          "link": "http://arxiv.org/abs/2101.12015",
          "publishedOn": "2021-07-23T02:00:31.278Z",
          "wordCount": 687,
          "title": "BERTa\\'u: Ita\\'u BERT for digital customer service. (arXiv:2101.12015v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kocmi_T/0/1/0/all/0/1\">Tom Kocmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federmann_C/0/1/0/all/0/1\">Christian Federmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grundkiewicz_R/0/1/0/all/0/1\">Roman Grundkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junczys_Dowmunt_M/0/1/0/all/0/1\">Marcin Junczys-Dowmunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsushita_H/0/1/0/all/0/1\">Hitokazu Matsushita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menezes_A/0/1/0/all/0/1\">Arul Menezes</a>",
          "description": "Automatic metrics are commonly used as the exclusive tool for declaring the\nsuperiority of one machine translation system's quality over another. The\ncommunity choice of automatic metric guides research directions and industrial\ndevelopments by deciding which models are deemed better. Evaluating metrics\ncorrelations has been limited to a small collection of human judgements. In\nthis paper, we corroborate how reliable metrics are in contrast to human\njudgements on - to the best of our knowledge - the largest collection of human\njudgements. We investigate which metrics have the highest accuracy to make\nsystem-level quality rankings for pairs of systems, taking human judgement as a\ngold standard, which is the closest scenario to the real metric usage.\nFurthermore, we evaluate the performance of various metrics across different\nlanguage pairs and domains. Lastly, we show that the sole use of BLEU\nnegatively affected the past development of improved models. We release the\ncollection of human judgements of 4380 systems, and 2.3 M annotated sentences\nfor further analysis and replication of our work.",
          "link": "http://arxiv.org/abs/2107.10821",
          "publishedOn": "2021-07-23T02:00:31.270Z",
          "wordCount": 620,
          "title": "To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation. (arXiv:2107.10821v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dayta_D/0/1/0/all/0/1\">Dominic B. Dayta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrios_E/0/1/0/all/0/1\">Erniel B. Barrios</a>",
          "description": "Legacy procedures for topic modelling have generally suffered problems of\noverfitting and a weakness towards reconstructing sparse topic structures. With\nmotivation from a consumer-generated corpora, this paper proposes\nsemiparametric topic model, a two-step approach utilizing nonnegative matrix\nfactorization and semiparametric regression in topic modeling. The model\nenables the reconstruction of sparse topic structures in the corpus and\nprovides a generative model for predicting topics in new documents entering the\ncorpus. Assuming the presence of auxiliary information related to the topics,\nthis approach exhibits better performance in discovering underlying topic\nstructures in cases where the corpora are small and limited in vocabulary. In\nan actual consumer feedback corpus, the model also demonstrably provides\ninterpretable and useful topic definitions comparable with those produced by\nother methods.",
          "link": "http://arxiv.org/abs/2107.10651",
          "publishedOn": "2021-07-23T02:00:31.195Z",
          "wordCount": 555,
          "title": "Semiparametric Latent Topic Modeling on Consumer-Generated Corpora. (arXiv:2107.10651v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10637",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Salimzianov_I/0/1/0/all/0/1\">Ilnar Salimzianov</a>",
          "description": "Mobile devices are transforming the way people interact with computers, and\nspeech interfaces to applications are ever more important. Automatic Speech\nRecognition systems recently published are very accurate, but often require\npowerful machinery (specialised Graphical Processing Units) for inference,\nwhich makes them impractical to run on commodity devices, especially in\nstreaming mode. Impressed by the accuracy of, but dissatisfied with the\ninference times of the baseline Kazakh ASR model of (Khassanov et al.,2021)\nwhen not using a GPU, we trained a new baseline acoustic model (on the same\ndataset as the aforementioned paper) and three language models for use with the\nCoqui STT framework. Results look promising, but further epochs of training and\nparameter sweeping or, alternatively, limiting the vocabulary that the ASR\nsystem must support, is needed to reach a production-level accuracy.",
          "link": "http://arxiv.org/abs/2107.10637",
          "publishedOn": "2021-07-23T02:00:31.186Z",
          "wordCount": 596,
          "title": "A baseline model for computationally inexpensive speech recognition for Kazakh using the Coqui STT framework. (arXiv:2107.10637v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2012.02705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kaiyu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayazit_D/0/1/0/all/0/1\">Deniz Bayazit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_R/0/1/0/all/0/1\">Rebecca Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1\">Stefanie Tellex</a>",
          "description": "Humans use spatial language to naturally describe object locations and their\nrelations. Interpreting spatial language not only adds a perceptual modality\nfor robots, but also reduces the barrier of interfacing with humans. Previous\nwork primarily considers spatial language as goal specification for instruction\nfollowing tasks in fully observable domains, often paired with reference paths\nfor reward-based learning. However, spatial language is inherently subjective\nand potentially ambiguous or misleading. Hence, in this paper, we consider\nspatial language as a form of stochastic observation. We propose SLOOP (Spatial\nLanguage Object-Oriented POMDP), a new framework for partially observable\ndecision making with a probabilistic observation model for spatial language. We\napply SLOOP to object search in city-scale environments. To interpret\nambiguous, context-dependent prepositions (e.g. front), we design a simple\nconvolutional neural network that predicts the language provider's latent frame\nof reference (FoR) given the environment context. Search strategies are\ncomputed via an online POMDP planner based on Monte Carlo Tree Search.\nEvaluation based on crowdsourced language data, collected over areas of five\ncities in OpenStreetMap, shows that our approach achieves faster search and\nhigher success rate compared to baselines, with a wider margin as the spatial\nlanguage becomes more complex. Finally, we demonstrate the proposed method in\nAirSim, a realistic simulator where a drone is tasked to find cars in a\nneighborhood environment.",
          "link": "http://arxiv.org/abs/2012.02705",
          "publishedOn": "2021-07-23T02:00:31.171Z",
          "wordCount": 706,
          "title": "Spatial Language Understanding for Object Search in Partially Observed City-scale Environments. (arXiv:2012.02705v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingqing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengzhi Zhang</a>",
          "description": "The surge in the number of books published makes the manual evaluation\nmethods difficult to efficiently evaluate books. The use of books' citations\nand alternative evaluation metrics can assist manual evaluation and reduce the\ncost of evaluation. However, most existing evaluation research was based on a\nsingle evaluation source with coarse-grained analysis, which may obtain\nincomprehensive or one-sided evaluation results of book impact. Meanwhile,\nrelying on a single resource for book assessment may lead to the risk that the\nevaluation results cannot be obtained due to the lack of the evaluation data,\nespecially for newly published books. Hence, this paper measured book impact\nbased on an evaluation system constructed by integrating multiple evaluation\nsources. Specifically, we conducted finer-grained mining on the multiple\nevaluation sources, including books' internal evaluation resources and external\nevaluation resources. Various technologies (e.g. topic extraction, sentiment\nanalysis, text classification) were used to extract corresponding evaluation\nmetrics from the internal and external evaluation resources. Then, Expert\nevaluation combined with analytic hierarchy process was used to integrate the\nevaluation metrics and construct a book impact evaluation system. Finally, the\nreliability of the evaluation system was verified by comparing with the results\nof expert evaluation, detailed and diversified evaluation results were then\nobtained. The experimental results reveal that differential evaluation\nresources can measure the books' impacts from different dimensions, and the\nintegration of multiple evaluation data can assess books more comprehensively.\nMeanwhile, the book impact evaluation system can provide personalized\nevaluation results according to the users' evaluation purposes. In addition,\nthe disciplinary differences should be considered for assessing books' impacts.",
          "link": "http://arxiv.org/abs/2107.10434",
          "publishedOn": "2021-07-23T02:00:31.135Z",
          "wordCount": 711,
          "title": "Impacts Towards a comprehensive assessment of the book impact by integrating multiple evaluation sources. (arXiv:2107.10434v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haoli Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>",
          "description": "The rapid development of large pre-trained language models has greatly\nincreased the demand for model compression techniques, among which quantization\nis a popular solution. In this paper, we propose BinaryBERT, which pushes BERT\nquantization to the limit by weight binarization. We find that a binary BERT is\nhard to be trained directly than a ternary counterpart due to its complex and\nirregular loss landscape. Therefore, we propose ternary weight splitting, which\ninitializes BinaryBERT by equivalently splitting from a half-sized ternary\nnetwork. The binary model thus inherits the good performance of the ternary\none, and can be further enhanced by fine-tuning the new architecture after\nsplitting. Empirical results show that our BinaryBERT has only a slight\nperformance drop compared with the full-precision model while being 24x\nsmaller, achieving the state-of-the-art compression results on the GLUE and\nSQuAD benchmarks.",
          "link": "http://arxiv.org/abs/2012.15701",
          "publishedOn": "2021-07-23T02:00:31.116Z",
          "wordCount": 608,
          "title": "BinaryBERT: Pushing the Limit of BERT Quantization. (arXiv:2012.15701v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>",
          "description": "In this paper, a novel two-branch neural network model structure is proposed\nfor multimodal emotion recognition, which consists of a time synchronous branch\n(TSB) and a time asynchronous branch (TAB). To capture correlations between\neach word and its acoustic realisation, the TSB combines speech and text\nmodalities at each input window frame and then does pooling across time to form\na single embedding vector. The TAB, by contrast, provides cross-utterance\ninformation by integrating sentence text embeddings from a number of context\nutterances into another embedding vector. The final emotion classification uses\nboth the TSB and the TAB embeddings. Experimental results on the IEMOCAP\ndataset demonstrate that the two-branch structure achieves state-of-the-art\nresults in 4-way classification with all common test setups. When using\nautomatic speech recognition (ASR) output instead of manually transcribed\nreference text, it is shown that the cross-utterance information considerably\nimproves the robustness against ASR errors. Furthermore, by incorporating an\nextra class for all the other emotions, the final 5-way classification system\nwith ASR hypotheses can be viewed as a prototype for more realistic emotion\nrecognition systems.",
          "link": "http://arxiv.org/abs/2010.14102",
          "publishedOn": "2021-07-23T02:00:31.107Z",
          "wordCount": 677,
          "title": "Emotion recognition by fusing time synchronous and time asynchronous representations. (arXiv:2010.14102v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1\">Marc Najork</a>",
          "description": "When experiencing an information need, users want to engage with a domain\nexpert, but often turn to an information retrieval system, such as a search\nengine, instead. Classical information retrieval systems do not answer\ninformation needs directly, but instead provide references to (hopefully\nauthoritative) answers. Successful question answering systems offer a limited\ncorpus created on-demand by human experts, which is neither timely nor\nscalable. Pre-trained language models, by contrast, are capable of directly\ngenerating prose that may be responsive to an information need, but at present\nthey are dilettantes rather than domain experts -- they do not have a true\nunderstanding of the world, they are prone to hallucinating, and crucially they\nare incapable of justifying their utterances by referring to supporting\ndocuments in the corpus they were trained over. This paper examines how ideas\nfrom classical information retrieval and pre-trained language models can be\nsynthesized and evolved into systems that truly deliver on the promise of\ndomain expert advice.",
          "link": "http://arxiv.org/abs/2105.02274",
          "publishedOn": "2021-07-23T02:00:31.100Z",
          "wordCount": 636,
          "title": "Rethinking Search: Making Domain Experts out of Dilettantes. (arXiv:2105.02274v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arseniev_Koehler_A/0/1/0/all/0/1\">Alina Arseniev-Koehler</a>",
          "description": "Measuring meaning is a central problem in cultural sociology and word\nembeddings may offer powerful new tools to do so. But like any tool, they build\non and exert theoretical assumptions. In this paper I theorize the ways in\nwhich word embeddings model three core premises of a structural linguistic\ntheory of meaning: that meaning is relational, coherent, and may be analyzed as\na static system. In certain ways, word embedding methods are vulnerable to the\nsame, enduring critiques of these premises. In other ways, they offer novel\nsolutions to these critiques. More broadly, formalizing the study of meaning\nwith word embeddings offers theoretical opportunities to clarify core concepts\nand debates in cultural sociology, such as the coherence of meaning. Just as\nnetwork analysis specified the once vague notion of social relations (Borgatti\net al. 2009), formalizing meaning with embedding methods can push us to specify\nand reimagine meaning itself.",
          "link": "http://arxiv.org/abs/2107.10413",
          "publishedOn": "2021-07-23T02:00:31.092Z",
          "wordCount": 591,
          "title": "Theoretical foundations and limits of word embeddings: what types of meaning can they capture?. (arXiv:2107.10413v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10658",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Rownicka_J/0/1/0/all/0/1\">Joanna Rownicka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sprenkamp_K/0/1/0/all/0/1\">Kilian Sprenkamp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tripiana_A/0/1/0/all/0/1\">Antonio Tripiana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gromoglasov_V/0/1/0/all/0/1\">Volodymyr Gromoglasov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kunz_T/0/1/0/all/0/1\">Timo P Kunz</a>",
          "description": "We describe our approach to create and deliver a custom voice for a\nconversational AI use-case. More specifically, we provide a voice for a Digital\nEinstein character, to enable human-computer interaction within the digital\nconversation experience. To create the voice which fits the context well, we\nfirst design a voice character and we produce the recordings which correspond\nto the desired speech attributes. We then model the voice. Our solution\nutilizes Fastspeech 2 for log-scaled mel-spectrogram prediction from phonemes\nand Parallel WaveGAN to generate the waveforms. The system supports a character\ninput and gives a speech waveform at the output. We use a custom dictionary for\nselected words to ensure their proper pronunciation. Our proposed cloud\narchitecture enables for fast voice delivery, making it possible to talk to the\ndigital version of Albert Einstein in real-time.",
          "link": "http://arxiv.org/abs/2107.10658",
          "publishedOn": "2021-07-23T02:00:31.085Z",
          "wordCount": 598,
          "title": "Digital Einstein Experience: Fast Text-to-Speech for Conversational AI. (arXiv:2107.10658v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02126",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hei_Y/0/1/0/all/0/1\">Yiming Hei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Rui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1\">Jiawei Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "Schema-based event extraction is a critical technique to apprehend the\nessential content of events promptly. With the rapid development of deep\nlearning technology, event extraction technology based on deep learning has\nbecome a research hotspot. Numerous methods, datasets, and evaluation metrics\nhave been proposed in the literature, raising the need for a comprehensive and\nupdated survey. This paper fills the gap by reviewing the state-of-the-art\napproaches, focusing on deep learning-based models. We summarize the task\ndefinition, paradigm, and models of schema-based event extraction and then\ndiscuss each of these in detail. We introduce benchmark datasets that support\ntests of predictions and evaluation metrics. A comprehensive comparison between\ndifferent techniques is also provided in this survey. Finally, we conclude by\nsummarizing future research directions facing the research area.",
          "link": "http://arxiv.org/abs/2107.02126",
          "publishedOn": "2021-07-23T02:00:31.064Z",
          "wordCount": 600,
          "title": "Deep Learning Schema-based Event Extraction: Literature Review and Current Trends. (arXiv:2107.02126v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Kai-Hui Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_P/0/1/0/all/0/1\">Patrick Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1\">Yoo Jung Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukuoka_Y/0/1/0/all/0/1\">Yoshimi Fukuoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>",
          "description": "Artificial intelligence chatbots are the vanguard in technology-based\nintervention to change people's behavior. To develop intervention chatbots, the\nfirst step is to understand natural language conversation strategies in human\nconversation. This work introduces an intervention conversation dataset\ncollected from a real-world physical activity intervention program for women.\nWe designed comprehensive annotation schemes in four dimensions (domain,\nstrategy, social exchange, and task-focused exchange) and annotated a subset of\ndialogs. We built a strategy classifier with context information to detect\nstrategies from both trainers and participants based on the annotation. To\nunderstand how human intervention induces effective behavior changes, we\nanalyzed the relationships between the intervention strategies and the\nparticipants' changes in the barrier and social support for physical activity.\nWe also analyzed how participant's baseline weight correlates to the amount of\noccurrence of the corresponding strategy. This work lays the foundation for\ndeveloping a personalized physical activity intervention bot. The dataset and\ncode are available at\nhttps://github.com/KaihuiLiang/physical-activity-counseling",
          "link": "http://arxiv.org/abs/2107.10410",
          "publishedOn": "2021-07-23T02:00:31.048Z",
          "wordCount": 621,
          "title": "Evaluation of In-Person Counseling Strategies To Develop Physical Activity Chatbot for Women. (arXiv:2107.10410v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.05502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grusdt_B/0/1/0/all/0/1\">Britta Grusdt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassiter_D/0/1/0/all/0/1\">Daniel Lassiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franke_M/0/1/0/all/0/1\">Michael Franke</a>",
          "description": "While a large body of work has scrutinized the meaning of conditional\nsentences, considerably less attention has been paid to formal models of their\npragmatic use and interpretation. Here, we take a probabilistic approach to\npragmatic reasoning about indicative conditionals which flexibly integrates\ngradient beliefs about richly structured world states. We model listeners'\nupdate of their prior beliefs about the causal structure of the world and the\njoint probabilities of the consequent and antecedent based on assumptions about\nthe speaker's utterance production protocol. We show that, when supplied with\nnatural contextual assumptions, our model uniformly explains a number of\ninferences attested in the literature, including epistemic inferences,\nConditional Perfection and the dependency between antecedent and consequent of\na conditional. We argue that this approach also helps explain three puzzles\nintroduced by Douven (2012) about updating with conditionals: depending on the\nutterance context, the listener's belief in the antecedent may increase,\ndecrease or remain unchanged.",
          "link": "http://arxiv.org/abs/2105.05502",
          "publishedOn": "2021-07-23T02:00:31.041Z",
          "wordCount": 607,
          "title": "Probabilistic modeling of rational communication with conditionals. (arXiv:2105.05502v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaur_R/0/1/0/all/0/1\">Rajvir Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginige_J/0/1/0/all/0/1\">Jeewani Anupama Ginige</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obst_O/0/1/0/all/0/1\">Oliver Obst</a>",
          "description": "Codification of free-text clinical narratives have long been recognised to be\nbeneficial for secondary uses such as funding, insurance claim processing and\nresearch. The current scenario of assigning codes is a manual process which is\nvery expensive, time-consuming and error prone. In recent years, many\nresearchers have studied the use of Natural Language Processing (NLP), related\nMachine Learning (ML) and Deep Learning (DL) methods and techniques to resolve\nthe problem of manual coding of clinical narratives and to assist human coders\nto assign clinical codes more accurately and efficiently. This systematic\nliterature review provides a comprehensive overview of automated clinical\ncoding systems that utilises appropriate NLP, ML and DL methods and techniques\nto assign ICD codes to discharge summaries. We have followed the Preferred\nReporting Items for Systematic Reviews and Meta-Analyses(PRISMA) guidelines and\nconducted a comprehensive search of publications from January, 2010 to December\n2020 in four academic databases- PubMed, ScienceDirect, Association for\nComputing Machinery(ACM) Digital Library, and the Association for Computational\nLinguistics(ACL) Anthology. We reviewed 7,556 publications; 38 met the\ninclusion criteria. This review identified: datasets having discharge\nsummaries; NLP techniques along with some other data extraction processes,\ndifferent feature extraction and embedding techniques. To measure the\nperformance of classification methods, different evaluation metrics are used.\nLastly, future research directions are provided to scholars who are interested\nin automated ICD code assignment. Efforts are still required to improve ICD\ncode prediction accuracy, availability of large-scale de-identified clinical\ncorpora with the latest version of the classification system. This can be a\nplatform to guide and share knowledge with the less experienced coders and\nresearchers.",
          "link": "http://arxiv.org/abs/2107.10652",
          "publishedOn": "2021-07-23T02:00:31.034Z",
          "wordCount": 725,
          "title": "A Systematic Literature Review of Automated ICD Coding and Classification Systems using Discharge Summaries. (arXiv:2107.10652v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junghoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jounghee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_P/0/1/0/all/0/1\">Pilsung Kang</a>",
          "description": "Language models (LMs) pretrained on a large text corpus and fine-tuned on a\ndownstream text corpus and fine-tuned on a downstream task becomes a de facto\ntraining strategy for several natural language processing (NLP) tasks.\nRecently, an adaptive pretraining method retraining the pretrained language\nmodel with task-relevant data has shown significant performance improvements.\nHowever, current adaptive pretraining methods suffer from underfitting on the\ntask distribution owing to a relatively small amount of data to re-pretrain the\nLM. To completely use the concept of adaptive pretraining, we propose a\nback-translated task-adaptive pretraining (BT-TAPT) method that increases the\namount of task-specific data for LM re-pretraining by augmenting the task data\nusing back-translation to generalize the LM to the target task domain. The\nexperimental results show that the proposed BT-TAPT yields improved\nclassification accuracy on both low- and high-resource data and better\nrobustness to noise than the conventional adaptive pretraining method.",
          "link": "http://arxiv.org/abs/2107.10474",
          "publishedOn": "2021-07-23T02:00:31.015Z",
          "wordCount": 589,
          "title": "Back-Translated Task Adaptive Pretraining: Improving Accuracy and Robustness on Text Classification. (arXiv:2107.10474v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balali_A/0/1/0/all/0/1\">Ali Balali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadpour_M/0/1/0/all/0/1\">Masoud Asadpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafari_S/0/1/0/all/0/1\">Seyed Hossein Jafari</a>",
          "description": "Data is published on the web over time in great volumes, but majority of the\ndata is unstructured, making it hard to understand and difficult to interpret.\nInformation Extraction (IE) methods extract structured information from\nunstructured data. One of the challenging IE tasks is Event Extraction (EE)\nwhich seeks to derive information about specific incidents and their actors\nfrom the text. EE is useful in many domains such as building a knowledge base,\ninformation retrieval, summarization and online monitoring systems. In the past\ndecades, some event ontologies like ACE, CAMEO and ICEWS were developed to\ndefine event forms, actors and dimensions of events observed in the text. These\nevent ontologies still have some shortcomings such as covering only a few\ntopics like political events, having inflexible structure in defining argument\nroles, lack of analytical dimensions, and complexity in choosing event\nsub-types. To address these concerns, we propose an event ontology, namely\nCOfEE, that incorporates both expert domain knowledge, previous ontologies and\na data-driven approach for identifying events from text. COfEE consists of two\nhierarchy levels (event types and event sub-types) that include new categories\nrelating to environmental issues, cyberspace, criminal activity and natural\ndisasters which need to be monitored instantly. Also, dynamic roles according\nto each event sub-type are defined to capture various dimensions of events. In\na follow-up experiment, the proposed ontology is evaluated on Wikipedia events,\nand it is shown to be general and comprehensive. Moreover, in order to\nfacilitate the preparation of gold-standard data for event extraction, a\nlanguage-independent online tool is presented based on COfEE.",
          "link": "http://arxiv.org/abs/2107.10326",
          "publishedOn": "2021-07-23T02:00:31.009Z",
          "wordCount": 705,
          "title": "COfEE: A Comprehensive Ontology for Event Extraction from text, with an online annotation tool. (arXiv:2107.10326v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Zero-resource named entity recognition (NER) severely suffers from data\nscarcity in a specific domain or language. Most studies on zero-resource NER\ntransfer knowledge from various data by fine-tuning on different auxiliary\ntasks. However, how to properly select training data and fine-tuning tasks is\nstill an open problem. In this paper, we tackle the problem by transferring\nknowledge from three aspects, i.e., domain, language and task, and\nstrengthening connections among them. Specifically, we propose four practical\nguidelines to guide knowledge transfer and task fine-tuning. Based on these\nguidelines, we design a target-oriented fine-tuning (TOF) framework to exploit\nvarious data from three aspects in a unified training manner. Experimental\nresults on six benchmarks show that our method yields consistent improvements\nover baselines in both cross-domain and cross-lingual scenarios. Particularly,\nwe achieve new state-of-the-art performance on five benchmarks.",
          "link": "http://arxiv.org/abs/2107.10523",
          "publishedOn": "2021-07-23T02:00:31.001Z",
          "wordCount": 572,
          "title": "Target-Oriented Fine-tuning for Zero-Resource Named Entity Recognition. (arXiv:2107.10523v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vargas_Calderon_V/0/1/0/all/0/1\">Vladimir Vargas-Calder&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_A/0/1/0/all/0/1\">Andreina Moros Ochoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieto_G/0/1/0/all/0/1\">Gilmer Yovani Castro Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camargo_J/0/1/0/all/0/1\">Jorge E. Camargo</a>",
          "description": "The increasing use of online hospitality platforms provides firsthand\ninformation about clients preferences, which are essential to improve hotel\nservices and increase the quality of service perception. Customer reviews can\nbe used to automatically extract the most relevant aspects of the quality of\nservice for hospitality clientele. This paper proposes a framework for the\nassessment of the quality of service in the hospitality sector based on the\nexploitation of customer reviews through natural language processing and\nmachine learning methods. The proposed framework automatically discovers the\nquality of service aspects relevant to hotel customers. Hotel reviews from\nBogot\\'a and Madrid are automatically scrapped from Booking.com. Semantic\ninformation is inferred through Latent Dirichlet Allocation and FastText, which\nallow representing text reviews as vectors. A dimensionality reduction\ntechnique is applied to visualise and interpret large amounts of customer\nreviews. Visualisations of the most important quality of service aspects are\ngenerated, allowing to qualitatively and quantitatively assess the quality of\nservice. Results show that it is possible to automatically extract the main\nquality of service aspects perceived by customers from large customer review\ndatasets. These findings could be used by hospitality managers to understand\nclients better and to improve the quality of service.",
          "link": "http://arxiv.org/abs/2107.10328",
          "publishedOn": "2021-07-23T02:00:30.995Z",
          "wordCount": 662,
          "title": "Machine learning for assessing quality of service in the hospitality sector based on customer reviews. (arXiv:2107.10328v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1\">Mikhail Burtsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>",
          "description": "Transformer-based encoder-decoder models produce a fused token-wise\nrepresentation after every encoder layer. We investigate the effects of\nallowing the encoder to preserve and explore alternative hypotheses, combined\nat the end of the encoding process. To that end, we design and examine a\n$\\textit{Multi-stream Transformer}$ architecture and find that splitting the\nTransformer encoder into multiple encoder streams and allowing the model to\nmerge multiple representational hypotheses improves performance, with further\nimprovement obtained by adding a skip connection between the first and the\nfinal encoder layer.",
          "link": "http://arxiv.org/abs/2107.10342",
          "publishedOn": "2021-07-23T02:00:30.986Z",
          "wordCount": 505,
          "title": "Multi-Stream Transformers. (arXiv:2107.10342v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10132",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Disong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_L/0/1/0/all/0/1\">Liqun Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeung_Y/0/1/0/all/0/1\">Yu Ting Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xunying Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>",
          "description": "One-shot voice conversion (VC), which performs conversion across arbitrary\nspeakers with only a single target-speaker utterance for reference, can be\neffectively achieved by speech representation disentanglement. Existing work\ngenerally ignores the correlation between different speech representations\nduring training, which causes leakage of content information into the speaker\nrepresentation and thus degrades VC performance. To alleviate this issue, we\nemploy vector quantization (VQ) for content encoding and introduce mutual\ninformation (MI) as the correlation metric during training, to achieve proper\ndisentanglement of content, speaker and pitch representations, by reducing\ntheir inter-dependencies in an unsupervised manner. Experimental results\nreflect the superiority of the proposed method in learning effective\ndisentangled speech representations for retaining source linguistic content and\nintonation variations, while capturing target speaker characteristics. In doing\nso, the proposed approach achieves higher speech naturalness and speaker\nsimilarity than current state-of-the-art one-shot VC systems. Our code,\npre-trained models and demo are available at\nhttps://github.com/Wendison/VQMIVC.",
          "link": "http://arxiv.org/abs/2106.10132",
          "publishedOn": "2021-07-22T02:03:10.551Z",
          "wordCount": 639,
          "title": "VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-shot Voice Conversion. (arXiv:2106.10132v1 [eess.AS] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Phadke_S/0/1/0/all/0/1\">Shruti Phadke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samory_M/0/1/0/all/0/1\">Mattia Samory</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_T/0/1/0/all/0/1\">Tanushree Mitra</a>",
          "description": "Online discussion platforms offer a forum to strengthen and propagate belief\nin misinformed conspiracy theories. Yet, they also offer avenues for conspiracy\ntheorists to express their doubts and experiences of cognitive dissonance. Such\nexpressions of dissonance may shed light on who abandons misguided beliefs and\nunder which circumstances. This paper characterizes self-disclosures of\ndissonance about QAnon, a conspiracy theory initiated by a mysterious leader Q\nand popularized by their followers, anons in conspiracy theory subreddits. To\nunderstand what dissonance and disbelief mean within conspiracy communities, we\nfirst characterize their social imaginaries, a broad understanding of how\npeople collectively imagine their social existence. Focusing on 2K posts from\ntwo image boards, 4chan and 8chan, and 1.2 M comments and posts from 12\nsubreddits dedicated to QAnon, we adopt a mixed methods approach to uncover the\nsymbolic language representing the movement, expectations, practices, heroes\nand foes of the QAnon community. We use these social imaginaries to create a\ncomputational framework for distinguishing belief and dissonance from general\ndiscussion about QAnon. Further, analyzing user engagement with QAnon\nconspiracy subreddits, we find that self-disclosures of dissonance correlate\nwith a significant decrease in user contributions and ultimately with their\ndeparture from the community. We contribute a computational framework for\nidentifying dissonance self-disclosures and measuring the changes in user\nengagement surrounding dissonance. Our work can provide insights into designing\ndissonance-based interventions that can potentially dissuade conspiracists from\nonline conspiracy discussion communities.",
          "link": "http://arxiv.org/abs/2107.10204",
          "publishedOn": "2021-07-22T02:03:10.542Z",
          "wordCount": 693,
          "title": "Characterizing Social Imaginaries and Self-Disclosures of Dissonance in Online Conspiracy Discussion Communities. (arXiv:2107.10204v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2002.01335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Slowik_A/0/1/0/all/0/1\">Agnieszka S&#x142;owik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1\">William L. Hamilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamnik_M/0/1/0/all/0/1\">Mateja Jamnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holden_S/0/1/0/all/0/1\">Sean B. Holden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>",
          "description": "In order to communicate, humans flatten a complex representation of ideas and\ntheir attributes into a single word or a sentence. We investigate the impact of\nrepresentation learning in artificial agents by developing graph referential\ngames. We empirically show that agents parametrized by graph neural networks\ndevelop a more compositional language compared to bag-of-words and sequence\nmodels, which allows them to systematically generalize to new combinations of\nfamiliar features.",
          "link": "http://arxiv.org/abs/2002.01335",
          "publishedOn": "2021-07-22T02:03:10.534Z",
          "wordCount": 570,
          "title": "Structural Inductive Biases in Emergent Communication. (arXiv:2002.01335v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">J. Edward Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>",
          "description": "We present a conditional text generation framework that posits sentential\nexpressions of possible causes and effects. This framework depends on two novel\nresources we develop in the course of this work: a very large-scale collection\nof English sentences expressing causal patterns CausalBank; and a refinement\nover previous work on constructing large lexical causal knowledge graphs Cause\nEffect Graph. Further, we extend prior work in lexically-constrained decoding\nto support disjunctive positive constraints. Human assessment confirms that our\napproach gives high-quality and diverse outputs. Finally, we use CausalBank to\nperform continued training of an encoder supporting a recent state-of-the-art\nmodel for causal reasoning, leading to a 3-point improvement on the COPA\nchallenge set, with no change in model architecture.",
          "link": "http://arxiv.org/abs/2107.09846",
          "publishedOn": "2021-07-22T02:03:10.464Z",
          "wordCount": 557,
          "title": "Guided Generation of Cause and Effect. (arXiv:2107.09846v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antonello_R/0/1/0/all/0/1\">Richard Antonello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turek_J/0/1/0/all/0/1\">Javier Turek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1\">Vy Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1\">Alexander Huth</a>",
          "description": "How related are the representations learned by neural language models,\ntranslation models, and language tagging tasks? We answer this question by\nadapting an encoder-decoder transfer learning method from computer vision to\ninvestigate the structure among 100 different feature spaces extracted from\nhidden representations of various networks trained on language tasks. This\nmethod reveals a low-dimensional structure where language models and\ntranslation models smoothly interpolate between word embeddings, syntactic and\nsemantic tasks, and future word embeddings. We call this low-dimensional\nstructure a language representation embedding because it encodes the\nrelationships between representations needed to process language for a variety\nof NLP tasks. We find that this representation embedding can predict how well\neach individual feature space maps to human brain responses to natural language\nstimuli recorded using fMRI. Additionally, we find that the principal dimension\nof this structure can be used to create a metric which highlights the brain's\nnatural language processing hierarchy. This suggests that the embedding\ncaptures some part of the brain's natural language representation structure.",
          "link": "http://arxiv.org/abs/2106.05426",
          "publishedOn": "2021-07-22T02:03:10.435Z",
          "wordCount": 653,
          "title": "Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses. (arXiv:2106.05426v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.11142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hongxuan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yu Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>",
          "description": "Machine reading comprehension (MRC) is a crucial task in natural language\nprocessing and has achieved remarkable advancements. However, most of the\nneural MRC models are still far from robust and fail to generalize well in\nreal-world applications. In order to comprehensively verify the robustness and\ngeneralization of MRC models, we introduce a real-world Chinese dataset --\nDuReader_robust. It is designed to evaluate the MRC models from three aspects:\nover-sensitivity, over-stability and generalization. Comparing to previous\nwork, the instances in DuReader_robust are natural texts, rather than the\naltered unnatural texts. It presents the challenges when applying MRC models to\nreal-world applications. The experimental results show that MRC models do not\nperform well on the challenge test set. Moreover, we analyze the behavior of\nexisting models on the challenge test set, which may provide suggestions for\nfuture model development. The dataset and codes are publicly available at\nhttps://github.com/baidu/DuReader.",
          "link": "http://arxiv.org/abs/2004.11142",
          "publishedOn": "2021-07-22T02:03:10.410Z",
          "wordCount": 629,
          "title": "DuReader_robust: A Chinese Dataset Towards Evaluating Robustness and Generalization of Machine Reading Comprehension in Real-World Applications. (arXiv:2004.11142v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lehecka_J/0/1/0/all/0/1\">Jan Lehe&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svec_J/0/1/0/all/0/1\">Jan &#x160;vec</a>",
          "description": "In this paper, we present our progress in pre-training monolingual\nTransformers for Czech and contribute to the research community by releasing\nour models for public. The need for such models emerged from our effort to\nemploy Transformers in our language-specific tasks, but we found the\nperformance of the published multilingual models to be very limited. Since the\nmultilingual models are usually pre-trained from 100+ languages, most of\nlow-resourced languages (including Czech) are under-represented in these\nmodels. At the same time, there is a huge amount of monolingual training data\navailable in web archives like Common Crawl. We have pre-trained and publicly\nreleased two monolingual Czech Transformers and compared them with relevant\npublic models, trained (at least partially) for Czech. The paper presents the\nTransformers pre-training procedure as well as a comparison of pre-trained\nmodels on text classification task from various domains.",
          "link": "http://arxiv.org/abs/2107.10042",
          "publishedOn": "2021-07-22T02:03:10.355Z",
          "wordCount": 573,
          "title": "Comparison of Czech Transformers on Text Classification Tasks. (arXiv:2107.10042v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10023",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jadallah_N/0/1/0/all/0/1\">Noah Jadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischbach_J/0/1/0/all/0/1\">Jannik Fischbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frattini_J/0/1/0/all/0/1\">Julian Frattini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelsang_A/0/1/0/all/0/1\">Andreas Vogelsang</a>",
          "description": "Causal relations (If A, then B) are prevalent in requirements artifacts.\nAutomatically extracting causal relations from requirements holds great\npotential for various RE activities (e.g., automatic derivation of suitable\ntest cases). However, we lack an approach capable of extracting causal\nrelations from natural language with reasonable performance. In this paper, we\npresent our tool CATE (CAusality Tree Extractor), which is able to parse the\ncomposition of a causal relation as a tree structure. CATE does not only\nprovide an overview of causes and effects in a sentence, but also reveals their\nsemantic coherence by translating the causal relation into a binary tree. We\nencourage fellow researchers and practitioners to use CATE at\nhttps://causalitytreeextractor.com/",
          "link": "http://arxiv.org/abs/2107.10023",
          "publishedOn": "2021-07-22T02:03:10.343Z",
          "wordCount": 551,
          "title": "CATE: CAusality Tree Extractor from Natural Language Requirements. (arXiv:2107.10023v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_T/0/1/0/all/0/1\">Tushar Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajadhyaksha_N/0/1/0/all/0/1\">Nishant Rajadhyaksha</a>",
          "description": "Linguistics has been instrumental in developing a deeper understanding of\nhuman nature. Words are indispensable to bequeath the thoughts, emotions, and\npurpose of any human interaction, and critically analyzing these words can\nelucidate the social and psychological behavior and characteristics of these\nsocial animals. Social media has become a platform for human interaction on a\nlarge scale and thus gives us scope for collecting and using that data for our\nstudy. However, this entire process of collecting, labeling, and analyzing this\ndata iteratively makes the entire procedure cumbersome. To make this entire\nprocess easier and structured, we would like to introduce TLA(Twitter\nLinguistic Analysis). In this paper, we describe TLA and provide a basic\nunderstanding of the framework and discuss the process of collecting, labeling,\nand analyzing data from Twitter for a corpus of languages while providing\ndetailed labeled datasets for all the languages and the models are trained on\nthese datasets. The analysis provided by TLA will also go a long way in\nunderstanding the sentiments of different linguistic communities and come up\nwith new and innovative solutions for their problems based on the analysis.",
          "link": "http://arxiv.org/abs/2107.09710",
          "publishedOn": "2021-07-22T02:03:10.335Z",
          "wordCount": 606,
          "title": "TLA: Twitter Linguistic Analysis. (arXiv:2107.09710v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Archiki Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehan_M/0/1/0/all/0/1\">Mohammad Ali Rehan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_S/0/1/0/all/0/1\">Shreya Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>",
          "description": "While recent benchmarks have spurred a lot of new work on improving the\ngeneralization of pretrained multilingual language models on multilingual\ntasks, techniques to improve code-switched natural language understanding tasks\nhave been far less explored. In this work, we propose the use of bilingual\nintermediate pretraining as a reliable technique to derive large and consistent\nperformance gains on three different NLP tasks using code-switched text. We\nachieve substantial absolute improvements of 7.87%, 20.15%, and 10.99%, on the\nmean accuracies and F1 scores over previous state-of-the-art systems for\nHindi-English Natural Language Inference (NLI), Question Answering (QA) tasks,\nand Spanish-English Sentiment Analysis (SA) respectively. We show consistent\nperformance gains on four different code-switched language-pairs\n(Hindi-English, Spanish-English, Tamil-English and Malayalam-English) for SA.\nWe also present a code-switched masked language modelling (MLM) pretraining\ntechnique that consistently benefits SA compared to standard MLM pretraining\nusing real code-switched text.",
          "link": "http://arxiv.org/abs/2107.09931",
          "publishedOn": "2021-07-22T02:03:10.314Z",
          "wordCount": 583,
          "title": "The Effectiveness of Intermediate-Task Training for Code-Switched Natural Language Understanding. (arXiv:2107.09931v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Quijano_A/0/1/0/all/0/1\">Alex John Quijano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dale_R/0/1/0/all/0/1\">Rick Dale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sindi_S/0/1/0/all/0/1\">Suzanne Sindi</a>",
          "description": "The availability of large linguistic data sets enables data-driven approaches\nto study linguistic change. This work explores the word rank dynamics of eight\nlanguages by investigating the Google Books corpus unigram frequency data set.\nWe observed the rank changes of the unigrams from 1900 to 2008 and compared it\nto a Wright-Fisher inspired model that we developed for our analysis. The model\nsimulates a neutral evolutionary process with the restriction of having no\ndisappearing words. This work explains the mathematical framework of the model\n- written as a Markov Chain with multinomial transition probabilities - to show\nhow frequencies of words change in time. From our observations in the data and\nour model, word rank stability shows two types of characteristics: (1) the\nincrease/decrease in ranks are monotonic, or (2) the average rank stays the\nsame. Based on our model, high-ranked words tend to be more stable while\nlow-ranked words tend to be more volatile. Some words change in ranks in two\nways: (a) by an accumulation of small increasing/decreasing rank changes in\ntime and (b) by shocks of increase/decrease in ranks. Most of the stopwords and\nSwadesh words are observed to be stable in ranks across eight languages. These\nsignatures suggest unigram frequencies in all languages have changed in a\nmanner inconsistent with a purely neutral evolutionary process.",
          "link": "http://arxiv.org/abs/2107.09948",
          "publishedOn": "2021-07-22T02:03:10.303Z",
          "wordCount": 672,
          "title": "A Statistical Model of Word Rank Evolution. (arXiv:2107.09948v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kenna_D/0/1/0/all/0/1\">Dana Kenna</a>",
          "description": "Word Embeddings have been shown to contain the societal biases present in the\noriginal corpora.Existing methods to deal with this problem have been shown to\nonly remove superficial biases. Themethod ofAdversarial Debiasingwas presumed\nto be similarly superficial, but this is was not verifiedin previous works.\nUsing the experiments that demonstrated the shallow removal in other methods,\nIshow results that suggestAdversarial Debiasingis more effective at removing\nbias and thus motivatefurther investigation on the utility ofAdversarial\nDebiasing.",
          "link": "http://arxiv.org/abs/2107.10251",
          "publishedOn": "2021-07-22T02:03:10.281Z",
          "wordCount": 513,
          "title": "Using Adversarial Debiasing to Remove Bias from Word Embeddings. (arXiv:2107.10251v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weijia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haider_B/0/1/0/all/0/1\">Batool Haider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krone_J/0/1/0/all/0/1\">Jason Krone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_S/0/1/0/all/0/1\">Saab Mansour</a>",
          "description": "Multilingual pre-trained contextual embedding models (Devlin et al., 2019)\nhave achieved impressive performance on zero-shot cross-lingual transfer tasks.\nFinding the most effective fine-tuning strategy to fine-tune these models on\nhigh-resource languages so that it transfers well to the zero-shot languages is\na non-trivial task. In this paper, we propose a novel meta-optimizer to\nsoft-select which layers of the pre-trained model to freeze during fine-tuning.\nWe train the meta-optimizer by simulating the zero-shot transfer scenario.\nResults on cross-lingual natural language inference show that our approach\nimproves over the simple fine-tuning baseline and X-MAML (Nooralahzadeh et al.,\n2020).",
          "link": "http://arxiv.org/abs/2107.09840",
          "publishedOn": "2021-07-22T02:03:10.264Z",
          "wordCount": 535,
          "title": "Soft Layer Selection with Meta-Learning for Zero-Shot Cross-Lingual Transfer. (arXiv:2107.09840v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fischbach_J/0/1/0/all/0/1\">Jannik Fischbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Springer_T/0/1/0/all/0/1\">Tobias Springer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frattini_J/0/1/0/all/0/1\">Julian Frattini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Femmer_H/0/1/0/all/0/1\">Henning Femmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelsang_A/0/1/0/all/0/1\">Andreas Vogelsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_D/0/1/0/all/0/1\">Daniel Mendez</a>",
          "description": "[Context:] Causal relations (e.g., If A, then B) are prevalent in functional\nrequirements. For various applications of AI4RE, e.g., the automatic derivation\nof suitable test cases from requirements, automatically extracting such causal\nstatements are a basic necessity. [Problem:] We lack an approach that is able\nto extract causal relations from natural language requirements in fine-grained\nform. Specifically, existing approaches do not consider the combinatorics\nbetween causes and effects. They also do not allow to split causes and effects\ninto more granular text fragments (e.g., variable and condition), making the\nextracted relations unsuitable for automatic test case derivation. [Objective &\nContributions:] We address this research gap and make the following\ncontributions: First, we present the Causality Treebank, which is the first\ncorpus of fully labeled binary parse trees representing the composition of\n1,571 causal requirements. Second, we propose a fine-grained causality\nextractor based on Recursive Neural Tensor Networks. Our approach is capable of\nrecovering the composition of causal statements written in natural language and\nachieves a F1 score of 74 % in the evaluation on the Causality Treebank. Third,\nwe disclose our open data sets as well as our code to foster the discourse on\nthe automatic extraction of causality in the RE community.",
          "link": "http://arxiv.org/abs/2107.09980",
          "publishedOn": "2021-07-22T02:03:10.242Z",
          "wordCount": 651,
          "title": "Fine-Grained Causality Extraction From Natural Language Requirements Using Recursive Neural Tensor Networks. (arXiv:2107.09980v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaliszyk_C/0/1/0/all/0/1\">Cezary Kaliszyk</a>",
          "description": "The heterogeneous nature of the logical foundations used in different\ninteractive proof assistant libraries has rendered discovery of similar\nmathematical concepts among them difficult. In this paper, we compare a\npreviously proposed algorithm for matching concepts across libraries with our\nunsupervised embedding approach that can help us retrieve similar concepts. Our\napproach is based on the fasttext implementation of Word2Vec, on top of which a\ntree traversal module is added to adapt its algorithm to the representation\nformat of our data export pipeline. We compare the explainability,\ncustomizability, and online-servability of the approaches and argue that the\nneural embedding approach has more potential to be integrated into an\ninteractive proof assistant.",
          "link": "http://arxiv.org/abs/2107.10188",
          "publishedOn": "2021-07-22T02:03:10.232Z",
          "wordCount": 553,
          "title": "JEFL: Joint Embedding of Formal Proof Libraries. (arXiv:2107.10188v1 [cs.LO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1\">Joseph Marvin Imperial</a>",
          "description": "One of the most important humanitarian responsibility of every individual is\nto protect the future of our children. This entails not only protection of\nphysical welfare but also from ill events that can potentially affect the\nmental well-being of a child such as sexual coercion and abuse which, in\nworst-case scenarios, can result to lifelong trauma. In this study, we perform\na preliminary investigation of how child sex peddlers spread illegal\npornographic content and target minors for sexual activities on Twitter in the\nPhilippines using Natural Language Processing techniques. Results of our\nstudies show frequently used and co-occurring words that traffickers use to\nspread content as well as four main roles played by these entities that\ncontribute to the proliferation of child pornography in the country.",
          "link": "http://arxiv.org/abs/2107.09881",
          "publishedOn": "2021-07-22T02:03:10.159Z",
          "wordCount": 582,
          "title": "How Do Pedophiles Tweet? Investigating the Writing Styles and Online Personas of Child Cybersex Traffickers in the Philippines. (arXiv:2107.09881v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kuo Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>",
          "description": "Recent work has shown success in incorporating pre-trained models like BERT\nto improve NLP systems. However, existing pre-trained models lack of causal\nknowledge which prevents today's NLP systems from thinking like humans. In this\npaper, we investigate the problem of injecting causal knowledge into\npre-trained models. There are two fundamental problems: 1) how to collect a\nlarge-scale causal resource from unstructured texts; 2) how to effectively\ninject causal knowledge into pre-trained models. To address these issues, we\npropose CausalBERT, which collects the largest scale of causal resource using\nprecise causal patterns and causal embedding techniques. In addition, we adopt\na regularization-based method to preserve the already learned knowledge with an\nextra regularization term while injecting causal knowledge. Extensive\nexperiments on 7 datasets, including four causal pair classification tasks, two\ncausal QA tasks and a causal inference task, demonstrate that CausalBERT\ncaptures rich causal knowledge and outperforms all pre-trained models-based\nstate-of-the-art methods, achieving a new causal inference benchmark.",
          "link": "http://arxiv.org/abs/2107.09852",
          "publishedOn": "2021-07-22T02:03:10.136Z",
          "wordCount": 594,
          "title": "CausalBERT: Injecting Causal Knowledge Into Pre-trained Models with Minimal Supervision. (arXiv:2107.09852v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1\">Srijan Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garimella_V/0/1/0/all/0/1\">Vishal Garimella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhane_A/0/1/0/all/0/1\">Ayush Suhane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>",
          "description": "In this paper, we advance the current state-of-the-art method for debiasing\nmonolingual word embeddings so as to generalize well in a multilingual setting.\nWe consider different methods to quantify bias and different debiasing\napproaches for monolingual as well as multilingual settings. We demonstrate the\nsignificance of our bias-mitigation approach on downstream NLP applications.\nOur proposed methods establish the state-of-the-art performance for debiasing\nmultilingual embeddings for three Indian languages - Hindi, Bengali, and Telugu\nin addition to English. We believe that our work will open up new opportunities\nin building unbiased downstream NLP applications that are inherently dependent\non the quality of the word embeddings used.",
          "link": "http://arxiv.org/abs/2107.10181",
          "publishedOn": "2021-07-22T02:03:10.125Z",
          "wordCount": 559,
          "title": "Debiasing Multilingual Word Embeddings: A Case Study of Three Indian Languages. (arXiv:2107.10181v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Watkins_H/0/1/0/all/0/1\">Henry Watkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_R/0/1/0/all/0/1\">Robert Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Ashwani Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachev_P/0/1/0/all/0/1\">Parashkev Nachev</a>",
          "description": "The use of electronic health records in medical research is difficult because\nof the unstructured format. Extracting information within reports and\nsummarising patient presentations in a way amenable to downstream analysis\nwould be enormously beneficial for operational and clinical research. In this\nwork we present a natural language processing pipeline for information\nextraction of radiological reports in neurology. Our pipeline uses a hybrid\nsequence of rule-based and artificial intelligence models to accurately extract\nand summarise neurological reports. We train and evaluate a custom language\nmodel on a corpus of 150000 radiological reports from National Hospital for\nNeurology and Neurosurgery, London MRI imaging. We also present results for\nstandard NLP tasks on domain-specific neuroradiology datasets. We show our\npipeline, called `neuroNLP', can reliably extract clinically relevant\ninformation from these reports, enabling downstream modelling of reports and\nassociated imaging on a heretofore unprecedented scale.",
          "link": "http://arxiv.org/abs/2107.10021",
          "publishedOn": "2021-07-22T02:03:10.105Z",
          "wordCount": 592,
          "title": "An artificial intelligence natural language processing pipeline for information extraction in neuroradiology. (arXiv:2107.10021v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ran Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shibiao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>",
          "description": "Geometry problem solving has attracted much attention in the NLP community\nrecently. The task is challenging as it requires abstract problem understanding\nand symbolic reasoning with axiomatic knowledge. However, current datasets are\neither small in scale or not publicly available. Thus, we construct a new\nlarge-scale benchmark, Geometry3K, consisting of 3,002 geometry problems with\ndense annotation in formal language. We further propose a novel geometry\nsolving approach with formal language and symbolic reasoning, called\nInterpretable Geometry Problem Solver (Inter-GPS). Inter-GPS first parses the\nproblem text and diagram into formal language automatically via rule-based text\nparsing and neural object detecting, respectively. Unlike implicit learning in\nexisting methods, Inter-GPS incorporates theorem knowledge as conditional rules\nand performs symbolic reasoning step by step. Also, a theorem predictor is\ndesigned to infer the theorem application sequence fed to the symbolic solver\nfor the more efficient and reasonable searching path. Extensive experiments on\nthe Geometry3K and GEOS datasets demonstrate that Inter-GPS achieves\nsignificant improvements over existing methods. The project with code and data\nis available at https://lupantech.github.io/inter-gps.",
          "link": "http://arxiv.org/abs/2105.04165",
          "publishedOn": "2021-07-22T02:03:10.088Z",
          "wordCount": 684,
          "title": "Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning. (arXiv:2105.04165v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lin Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_C/0/1/0/all/0/1\">Chung-Wei Hang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potdar_S/0/1/0/all/0/1\">Saloni Potdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>",
          "description": "We propose a simple and general method to regularize the fine-tuning of\nTransformer-based encoders for text classification tasks. Specifically, during\nfine-tuning we generate adversarial examples by perturbing the word embeddings\nof the model and perform contrastive learning on clean and adversarial examples\nin order to teach the model to learn noise-invariant representations. By\ntraining on both clean and adversarial examples along with the additional\ncontrastive objective, we observe consistent improvement over standard\nfine-tuning on clean examples. On several GLUE benchmark tasks, our fine-tuned\nBERT Large model outperforms BERT Large baseline by 1.7% on average, and our\nfine-tuned RoBERTa Large improves over RoBERTa Large baseline by 1.3%. We\nadditionally validate our method in different domains using three intent\nclassification datasets, where our fine-tuned RoBERTa Large outperforms RoBERTa\nLarge baseline by 1-2% on average.",
          "link": "http://arxiv.org/abs/2107.10137",
          "publishedOn": "2021-07-22T02:03:10.075Z",
          "wordCount": 563,
          "title": "Improved Text Classification via Contrastive Adversarial Training. (arXiv:2107.10137v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09729",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shaham_U/0/1/0/all/0/1\">Uri Shaham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>",
          "description": "We combine beam search with the probabilistic pruning technique of nucleus\nsampling to create two deterministic nucleus search algorithms for natural\nlanguage generation. The first algorithm, p-exact search, locally prunes the\nnext-token distribution and performs an exact search over the remaining space.\nThe second algorithm, dynamic beam search, shrinks and expands the beam size\naccording to the entropy of the candidate's probability distribution. Despite\nthe probabilistic intuition behind nucleus search, experiments on machine\ntranslation and summarization benchmarks show that both algorithms reach the\nsame performance levels as standard beam search.",
          "link": "http://arxiv.org/abs/2107.09729",
          "publishedOn": "2021-07-22T02:03:10.063Z",
          "wordCount": 534,
          "title": "What Do You Get When You Cross Beam Search with Nucleus Sampling?. (arXiv:2107.09729v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dadgar_S/0/1/0/all/0/1\">Sajad Dadgar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghatee_M/0/1/0/all/0/1\">Mehdi Ghatee</a>",
          "description": "During the COVID-19 pandemic, social media platforms were ideal for\ncommunicating due to social isolation and quarantine. Also, it was the primary\nsource of misinformation dissemination on a large scale, referred to as the\ninfodemic. Therefore, automatic debunking misinformation is a crucial problem.\nTo tackle this problem, we present two COVID-19 related misinformation datasets\non Twitter and propose a misinformation detection system comprising\nnetwork-based and content-based processes based on machine learning algorithms\nand NLP techniques. In the network-based process, we focus on social\nproperties, network characteristics, and users. On the other hand, we classify\nmisinformation using the content of the tweets directly in the content-based\nprocess, which contains text classification models (paragraph-level and\nsentence-level) and similarity models. The evaluation results on the\nnetwork-based process show the best results for the artificial neural network\nmodel with an F1 score of 88.68%. In the content-based process, our novel\nsimilarity models, which obtained an F1 score of 90.26%, show an improvement in\nthe misinformation classification results compared to the network-based models.\nIn addition, in the text classification models, the best result was achieved\nusing the stacking ensemble-learning model by obtaining an F1 score of 95.18%.\nFurthermore, we test our content-based models on the Constraint@AAAI2021\ndataset, and by getting an F1 score of 94.38%, we improve the baseline results.\nFinally, we develop a fact-checking website called Checkovid that uses each\nprocess to detect misinformative and informative claims in the domain of\nCOVID-19 from different perspectives.",
          "link": "http://arxiv.org/abs/2107.09768",
          "publishedOn": "2021-07-22T02:03:10.013Z",
          "wordCount": 756,
          "title": "Checkovid: A COVID-19 misinformation detection system on Twitter using network and content mining perspectives. (arXiv:2107.09768v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.11528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>",
          "description": "In recent years, Vietnam witnesses the mass development of social network\nusers on different social platforms such as Facebook, Youtube, Instagram, and\nTiktok. On social medias, hate speech has become a critical problem for social\nnetwork users. To solve this problem, we introduce the ViHSD - a\nhuman-annotated dataset for automatically detecting hate speech on the social\nnetwork. This dataset contains over 30,000 comments, each comment in the\ndataset has one of three labels: CLEAN, OFFENSIVE, or HATE. Besides, we\nintroduce the data creation process for annotating and evaluating the quality\nof the dataset. Finally, we evaluated the dataset by deep learning models and\ntransformer models.",
          "link": "http://arxiv.org/abs/2103.11528",
          "publishedOn": "2021-07-21T02:01:34.365Z",
          "wordCount": 605,
          "title": "A Large-scale Dataset for Hate Speech Detection on Vietnamese Social Media Texts. (arXiv:2103.11528v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1\">Samridhi Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKenna_J/0/1/0/all/0/1\">Joseph P. McKenna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1\">Athanasios Mouchtaris</a>",
          "description": "End-to-end (E2E) spoken language understanding (SLU) systems predict\nutterance semantics directly from speech using a single model. Previous work in\nthis area has focused on targeted tasks in fixed domains, where the output\nsemantic structure is assumed a priori and the input speech is of limited\ncomplexity. In this work we present our approach to developing an E2E model for\ngeneralized SLU in commercial voice assistants (VAs). We propose a fully\ndifferentiable, transformer-based, hierarchical system that can be pretrained\nat both the ASR and NLU levels. This is then fine-tuned on both transcription\nand semantic classification losses to handle a diverse set of intent and\nargument combinations. This leads to an SLU system that achieves significant\nimprovements over baselines on a complex internal generalized VA dataset with a\n43% improvement in accuracy, while still meeting the 99% accuracy benchmark on\nthe popular Fluent Speech Commands dataset. We further evaluate our model on a\nhard test set, exclusively containing slot arguments unseen in training, and\ndemonstrate a nearly 20% improvement, showing the efficacy of our approach in\ntruly demanding VA scenarios.",
          "link": "http://arxiv.org/abs/2106.09009",
          "publishedOn": "2021-07-21T02:01:34.309Z",
          "wordCount": 665,
          "title": "End-to-End Spoken Language Understanding for Generalized Voice Assistants. (arXiv:2106.09009v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08001",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hatua_A/0/1/0/all/0/1\">Amartya Hatua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Arjun Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_R/0/1/0/all/0/1\">Rakesh M. Verma</a>",
          "description": "This article describes research on claim verification carried out using a\nmultiple GAN-based model. The proposed model consists of three pairs of\ngenerators and discriminators. The generator and discriminator pairs are\nresponsible for generating synthetic data for supported and refuted claims and\nclaim labels. A theoretical discussion about the proposed model is provided to\nvalidate the equilibrium state of the model. The proposed model is applied to\nthe FEVER dataset, and a pre-trained language model is used for the input text\ndata. The synthetically generated data helps to gain information which helps\nthe model to perform better than state of the art models and other standard\nclassifiers.",
          "link": "http://arxiv.org/abs/2103.08001",
          "publishedOn": "2021-07-21T02:01:33.929Z",
          "wordCount": 598,
          "title": "Claim Verification using a Multi-GAN based Model. (arXiv:2103.08001v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heidarysafa_M/0/1/0/all/0/1\">Mojtaba Heidarysafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowsari_K/0/1/0/all/0/1\">Kamran Kowsari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bashiri_M/0/1/0/all/0/1\">Masoud Bashiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">Donald E. Brown</a>",
          "description": "The growth of the data science field requires better tools to understand such\na fast-paced growing domain. Moreover, individuals from different backgrounds\nbecame interested in following a career as data scientists. Therefore,\nproviding a quantitative guide for individuals and organizations to understand\nthe skills required in the job market would be crucial. This paper introduces a\nframework to analyze the job market for data science-related jobs within the US\nwhile providing an interface to access insights in this market. The proposed\nframework includes three sub-modules allowing continuous data collection,\ninformation extraction, and a web-based dashboard visualization to investigate\nthe spatial and temporal distribution of data science-related jobs and skills.\nThe result of this work shows important skills for the main branches of data\nscience jobs and attempts to provide a skill-based definition of these data\nscience branches. The current version of this application is deployed on the\nweb and allows individuals and institutes to investigate skills required for\ndata science positions through the industry lens.",
          "link": "http://arxiv.org/abs/2106.11077",
          "publishedOn": "2021-07-21T02:01:33.913Z",
          "wordCount": 645,
          "title": "Toward a Knowledge Discovery Framework for Data Science Job Market in the United States. (arXiv:2106.11077v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eloff_K/0/1/0/all/0/1\">Kevin Eloff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelbrecht_H/0/1/0/all/0/1\">Herman Engelbrecht</a>",
          "description": "Communication between agents in collaborative multi-agent settings is in\ngeneral implicit or a direct data stream. This paper considers text-based\nnatural language as a novel form of communication between multiple agents\ntrained with reinforcement learning. This could be considered first steps\ntoward a truly autonomous communication without the need to define a limited\nset of instructions, and natural collaboration between humans and robots.\nInspired by the game of Blind Leads, we propose an environment where one agent\nuses natural language instructions to guide another through a maze. We test the\nability of reinforcement learning agents to effectively communicate through\ndiscrete word-level symbols and show that the agents are able to sufficiently\ncommunicate through natural language with a limited vocabulary. Although the\ncommunication is not always perfect English, the agents are still able to\nnavigate the maze. We achieve a BLEU score of 0.85, which is an improvement of\n0.61 over randomly generated sequences while maintaining a 100% maze completion\nrate. This is a 3.5 times the performance of the random baseline using our\nreference set.",
          "link": "http://arxiv.org/abs/2107.09356",
          "publishedOn": "2021-07-21T02:01:33.868Z",
          "wordCount": 665,
          "title": "Toward Collaborative Reinforcement Learning Agents that Communicate Through Text-Based Natural Language. (arXiv:2107.09356v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1\">Zeeshan Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akella_K/0/1/0/all/0/1\">Kartheek Akella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P. Namboodiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C V Jawahar</a>",
          "description": "This work studies the long-standing problems of model capacity and negative\ninterference in multilingual neural machine translation MNMT. We use network\npruning techniques and observe that pruning 50-70% of the parameters from a\ntrained MNMT model results only in a 0.29-1.98 drop in the BLEU score.\nSuggesting that there exist large redundancies even in MNMT models. These\nobservations motivate us to use the redundant parameters and counter the\ninterference problem efficiently. We propose a novel adaptation strategy, where\nwe iteratively prune and retrain the redundant parameters of an MNMT to improve\nbilingual representations while retaining the multilinguality. Negative\ninterference severely affects high resource languages, and our method\nalleviates it without any additional adapter modules. Hence, we call it\nparameter-free adaptation strategy, paving way for the efficient adaptation of\nMNMT. We demonstrate the effectiveness of our method on a 9 language MNMT\ntrained on TED talks, and report an average improvement of +1.36 BLEU on high\nresource pairs. Code will be released here.",
          "link": "http://arxiv.org/abs/2107.09622",
          "publishedOn": "2021-07-21T02:01:33.837Z",
          "wordCount": 587,
          "title": "More Parameters? No Thanks!. (arXiv:2107.09622v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arruda_H/0/1/0/all/0/1\">Henrique Ferraz de Arruda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_L/0/1/0/all/0/1\">Luciano da Fontoura Costa</a>",
          "description": "To a good extent, words can be understood as corresponding to patterns or\ncategories that appeared in order to represent concepts and structures that are\nparticularly important or useful in a given time and space. Words are\ncharacterized by not being completely general nor specific, in the sense that\nthe same word can be instantiated or related to several different contexts,\ndepending on specific situations. Indeed, the way in which words are\ninstantiated and associated represents a particularly interesting aspect that\ncan substantially help to better understand the context in which they are\nemployed. Scientific words are no exception to that. In the present work, we\napproach the associations between a set of particularly relevant words in the\nsense of being not only frequently used in several areas, but also representing\nconcepts that are currently related to some of the main standing challenges in\nscience. More specifically, the study reported here takes into account the\nwords \"prediction\", \"model\", \"optimization\", \"complex\", \"entropy\", \"random\",\n\"deterministic\", \"pattern\", and \"database\". In order to complement the\nanalysis, we also obtain a network representing the relationship between the\nadopted areas. Many interesting results were found. First and foremost, several\nof the words were observed to have markedly distinct associations in different\nareas. Biology was found to be related to computer science, sharing\nassociations with databases. Furthermore, for most of the cases, the words\n\"complex\", \"model\", and \"prediction\" were observed to have several strong\nassociations.",
          "link": "http://arxiv.org/abs/2106.14610",
          "publishedOn": "2021-07-21T02:01:33.805Z",
          "wordCount": 696,
          "title": "A keyword-driven approach to science. (arXiv:2106.14610v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fisch_A/0/1/0/all/0/1\">Adam Fisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1\">Tommi Jaakkola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>",
          "description": "We develop a novel approach to conformal prediction when the target task has\nlimited data available for training. Conformal prediction identifies a small\nset of promising output candidates in place of a single prediction, with\nguarantees that the set contains the correct answer with high probability. When\ntraining data is limited, however, the predicted set can easily become unusably\nlarge. In this work, we obtain substantially tighter prediction sets while\nmaintaining desirable marginal guarantees by casting conformal prediction as a\nmeta-learning paradigm over exchangeable collections of auxiliary tasks. Our\nconformalization algorithm is simple, fast, and agnostic to the choice of\nunderlying model, learning algorithm, or dataset. We demonstrate the\neffectiveness of this approach across a number of few-shot classification and\nregression tasks in natural language processing, computer vision, and\ncomputational chemistry for drug discovery.",
          "link": "http://arxiv.org/abs/2102.08898",
          "publishedOn": "2021-07-21T02:01:33.797Z",
          "wordCount": 603,
          "title": "Few-shot Conformal Prediction with Auxiliary Tasks. (arXiv:2102.08898v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1\">Devamanyu Hazarika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1\">Rishabh Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_S/0/1/0/all/0/1\">Samson Yu Bai Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1\">Pengfei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Romila Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Abhinaba Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhaya_N/0/1/0/all/0/1\">Niyati Chhaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>",
          "description": "We address the problem of recognizing emotion cause in conversations, define\ntwo novel sub-tasks of this problem, and provide a corresponding dialogue-level\ndataset, along with strong Transformer-based baselines. The dataset is\navailable at https://github.com/declare-lab/RECCON.\n\nIntroduction: Recognizing the cause behind emotions in text is a fundamental\nyet under-explored area of research in NLP. Advances in this area hold the\npotential to improve interpretability and performance in affect-based models.\nIdentifying emotion causes at the utterance level in conversations is\nparticularly challenging due to the intermingling dynamics among the\ninterlocutors.\n\nMethod: We introduce the task of Recognizing Emotion Cause in CONversations\nwith an accompanying dataset named \\RECCONDA, containing over 1,000 dialogues\nand 10,000 utterance cause-effect pairs. Furthermore, we define different cause\ntypes based on the source of the causes, and establish strong Transformer-based\nbaselines to address two different sub-tasks on this dataset: causal span\nextraction and causal emotion entailment.\n\nResult: Our Transformer-based baselines, which leverage contextual\npre-trained embeddings, such as RoBERTa, outperform the state-of-the-art\nemotion cause extraction approaches\n\nConclusion: We introduce a new task highly relevant for (explainable)\nemotion-aware artificial intelligence: recognizing emotion cause in\nconversations, provide a new highly challenging publicly available\ndialogue-level dataset for this task, and give strong baseline results on this\ndataset.",
          "link": "http://arxiv.org/abs/2012.11820",
          "publishedOn": "2021-07-21T02:01:33.787Z",
          "wordCount": 692,
          "title": "Recognizing Emotion Cause in Conversations. (arXiv:2012.11820v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peshterliev_S/0/1/0/all/0/1\">Stan Peshterliev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1\">Michael Schlichtkrull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sonal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_S/0/1/0/all/0/1\">Scott Yih</a>",
          "description": "We study open-domain question answering with structured, unstructured and\nsemi-structured knowledge sources, including text, tables, lists and knowledge\nbases. Departing from prior work, we propose a unifying approach that\nhomogenizes all sources by reducing them to text and applies the\nretriever-reader model which has so far been limited to text sources only. Our\napproach greatly improves the results on knowledge-base QA tasks by 11 points,\ncompared to latest graph-based methods. More importantly, we demonstrate that\nour unified knowledge (UniK-QA) model is a simple and yet effective way to\ncombine heterogeneous sources of knowledge, advancing the state-of-the-art\nresults on two popular question answering benchmarks, NaturalQuestions and\nWebQuestions, by 3.5 and 2.6 points, respectively.",
          "link": "http://arxiv.org/abs/2012.14610",
          "publishedOn": "2021-07-21T02:01:33.780Z",
          "wordCount": 593,
          "title": "UniK-QA: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering. (arXiv:2012.14610v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chopra_H/0/1/0/all/0/1\">Harshita Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vashishtha_A/0/1/0/all/0/1\">Aniket Vashishtha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_R/0/1/0/all/0/1\">Ridam Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashima/0/1/0/all/0/1\">Ashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1\">Ananya Tyagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_T/0/1/0/all/0/1\">Tavpritesh Sethi</a>",
          "description": "Social media plays a pivotal role in disseminating news globally and acts as\na platform for people to express their opinions on various topics. A wide\nvariety of views accompanies COVID-19 vaccination drives across the globe,\noften colored by emotions, which change along with rising cases, approval of\nvaccines, and multiple factors discussed online. This study aims at analyzing\nthe temporal evolution of different Emotion categories: Hesitation, Rage,\nSorrow, Anticipation, Faith, and Contentment with Influencing Factors: Vaccine\nRollout, Misinformation, Health Effects, and Inequities as lexical categories\ncreated from Tweets belonging to five countries with vital vaccine roll-out\nprograms, namely, India, United States of America, Brazil, United Kingdom, and\nAustralia. We extracted a corpus of nearly 1.8 million Twitter posts related to\nCOVID-19 vaccination. Using cosine distance from selected seed words, we\nexpanded the vocabulary of each category and tracked the longitudinal change in\ntheir strength from June 2020 to April 2021. We used community detection\nalgorithms to find modules in positive correlation networks. Our findings\nsuggest that tweets expressing hesitancy towards vaccines contain the highest\nmentions of health-related effects in all countries. Our results indicated that\nthe patterns of hesitancy were variable across geographies and can help us\nlearn targeted interventions. We also observed a significant change in the\nlinear trends of categories like hesitation and contentment before and after\napproval of vaccines. Negative emotions like rage and sorrow gained the highest\nimportance in the alluvial diagram. They formed a significant module with all\nthe influencing factors in April 2021, when India observed the second wave of\nCOVID-19 cases. The relationship between Emotions and Influencing Factors was\nfound to be variable across the countries.",
          "link": "http://arxiv.org/abs/2104.01131",
          "publishedOn": "2021-07-21T02:01:33.773Z",
          "wordCount": 790,
          "title": "Mining Trends of COVID-19 Vaccine Beliefs on Twitter with Lexical Embeddings. (arXiv:2104.01131v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1\">Stephen Roller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1\">Sainbayar Sukhbaatar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>",
          "description": "We investigate the training of sparse layers that use different parameters\nfor different inputs based on hashing in large Transformer models.\nSpecifically, we modify the feedforward layer to hash to different sets of\nweights depending on the current token, over all tokens in the sequence. We\nshow that this procedure either outperforms or is competitive with\nlearning-to-route mixture-of-expert methods such as Switch Transformers and\nBASE Layers, while requiring no routing parameters or extra terms in the\nobjective function such as a load balancing loss, and no sophisticated\nassignment algorithm. We study the performance of different hashing techniques,\nhash sizes and input features, and show that balanced and random hashes focused\non the most local features work best, compared to either learning clusters or\nusing longer-range context. We show our approach works well both on large\nlanguage modeling and dialogue tasks, and on downstream fine-tuning tasks.",
          "link": "http://arxiv.org/abs/2106.04426",
          "publishedOn": "2021-07-21T02:01:33.752Z",
          "wordCount": 614,
          "title": "Hash Layers For Large Sparse Models. (arXiv:2106.04426v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11519",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carrera_Casado_D/0/1/0/all/0/1\">David Carrera-Casado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>",
          "description": "Biosemiosis is a process of choice-making between simultaneously alternative\noptions. It is well-known that, when sufficiently young children encounter a\nnew word, they tend to interpret it as pointing to a meaning that does not have\na word yet in their lexicon rather than to a meaning that already has a word\nattached. In previous research, the strategy was shown to be optimal from an\ninformation theoretic standpoint. In that framework, interpretation is\nhypothesized to be driven by the minimization of a cost function: the option of\nleast communication cost is chosen. However, the information theoretic model\nemployed in that research neither explains the weakening of that vocabulary\nlearning bias in older children or polylinguals nor reproduces Zipf's\nmeaning-frequency law, namely the non-linear relationship between the number of\nmeanings of a word and its frequency. Here we consider a generalization of the\nmodel that is channeled to reproduce that law. The analysis of the new model\nreveals regions of the phase space where the bias disappears consistently with\nthe weakening or loss of the bias in older children or polylinguals. The model\nis abstract enough to support future research on other levels of life that are\nrelevant to biosemiotics. In the deep learning era, the model is a transparent\nlow-dimensional tool for future experimental research and illustrates the\npredictive power of a theoretical framework originally designed to shed light\non the origins of Zipf's rank-frequency law.",
          "link": "http://arxiv.org/abs/2105.11519",
          "publishedOn": "2021-07-21T02:01:33.745Z",
          "wordCount": 731,
          "title": "The advent and fall of a vocabulary learning bias from communicative efficiency. (arXiv:2105.11519v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weize Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hexu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Hyperbolic neural networks have shown great potential for modeling complex\ndata. However, existing hyperbolic networks are not completely hyperbolic, as\nthey encode features in a hyperbolic space yet formalize most of their\noperations in the tangent space (a Euclidean subspace) at the origin of the\nhyperbolic space. This hybrid method greatly limits the modeling ability of\nnetworks. In this paper, we propose a fully hyperbolic framework to build\nhyperbolic networks based on the Lorentz model by adapting the Lorentz\ntransformations (including boost and rotation) to formalize essential\noperations of neural networks. Moreover, we also prove that linear\ntransformation in tangent spaces used by existing hyperbolic networks is a\nrelaxation of the Lorentz rotation and does not include the boost, implicitly\nlimiting the capabilities of existing hyperbolic networks. The experimental\nresults on four NLP tasks show that our method has better performance for\nbuilding both shallow and deep networks. Our code will be released to\nfacilitate follow-up research.",
          "link": "http://arxiv.org/abs/2105.14686",
          "publishedOn": "2021-07-21T02:01:33.737Z",
          "wordCount": 624,
          "title": "Fully Hyperbolic Neural Networks. (arXiv:2105.14686v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1\">James A. Michaelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bardolph_M/0/1/0/all/0/1\">Megan D. Bardolph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coulson_S/0/1/0/all/0/1\">Seana Coulson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin K. Bergen</a>",
          "description": "Despite being designed for performance rather than cognitive plausibility,\ntransformer language models have been found to be better at predicting metrics\nused to assess human language comprehension than language models with other\narchitectures, such as recurrent neural networks. Based on how well they\npredict the N400, a neural signal associated with processing difficulty, we\npropose and provide evidence for one possible explanation - their predictions\nare affected by the preceding context in a way analogous to the effect of\nsemantic facilitation in humans.",
          "link": "http://arxiv.org/abs/2107.09648",
          "publishedOn": "2021-07-21T02:01:33.727Z",
          "wordCount": 554,
          "title": "Different kinds of cognitive plausibility: why are transformers better than RNNs at predicting N400 amplitude?. (arXiv:2107.09648v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ao_S/0/1/0/all/0/1\">Shuang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acharya_X/0/1/0/all/0/1\">Xeno Acharya</a>",
          "description": "A medical dialogue system is essential for healthcare service as providing\nprimary clinical advice and diagnoses. It has been gradually adopted and\npracticed in medical organizations in the form of a conversational bot, largely\ndue to the advancement of NLP. In recent years, the introduction of\nstate-of-the-art deep learning models and transfer learning techniques like\nUniversal Language Model Fine Tuning (ULMFiT) and Knowledge Distillation (KD)\nlargely contributes to the performance of NLP tasks. However, some deep neural\nnetworks are poorly calibrated and wrongly estimate the uncertainty. Hence the\nmodel is not trustworthy, especially in sensitive medical decision-making\nsystems and safety tasks. In this paper, we investigate the well-calibrated\nmodel for ULMFiT and self-distillation (SD) in a medical dialogue system. The\ncalibrated ULMFiT (CULMFiT) is obtained by incorporating label smoothing (LS),\na commonly used regularization technique to achieve a well-calibrated model.\nMoreover, we apply the technique to recalibrate the confidence score called\ntemperature scaling (TS) with KD to observe its correlation with network\ncalibration. To further understand the relation between SD and calibration, we\nuse both fixed and optimal temperatures to fine-tune the whole model. All\nexperiments are conducted on the consultation backpain dataset collected by\nexperts then further validated using a large publicly medial dialogue corpus.\nWe empirically show that our proposed methodologies outperform conventional\nmethods in terms of accuracy and robustness.",
          "link": "http://arxiv.org/abs/2107.09625",
          "publishedOn": "2021-07-21T02:01:33.710Z",
          "wordCount": 657,
          "title": "Learning ULMFiT and Self-Distillation with Calibration for Medical Dialogue System. (arXiv:2107.09625v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bezati_E/0/1/0/all/0/1\">Endri Bezati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emami_M/0/1/0/all/0/1\">Mahyar Emami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janneck_J/0/1/0/all/0/1\">J&#xf6;rn Janneck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larus_J/0/1/0/all/0/1\">James Larus</a>",
          "description": "To increase performance and efficiency, systems use FPGAs as reconfigurable\naccelerators. A key challenge in designing these systems is partitioning\ncomputation between processors and an FPGA. An appropriate division of labor\nmay be difficult to predict in advance and require experiments and\nmeasurements. When an investigation requires rewriting part of the system in a\nnew language or with a new programming model, its high cost can retard the\nstudy of different configurations. A single-language system with an appropriate\nprogramming model and compiler that targets both platforms simplifies this\nexploration to a simple recompile with new compiler directives.\n\nThis work introduces StreamBlocks, an open-source compiler and runtime that\nuses the CAL dataflow programming language to partition computations across\nheterogeneous (CPU/accelerator) platforms. Because of the dataflow model's\nsemantics and the CAL language, StreamBlocks can exploit both thread\nparallelism in multi-core CPUs and the inherent parallelism of FPGAs.\nStreamBlocks supports exploring the design space with a profile-guided tool\nthat helps identify the best hardware-software partitions.",
          "link": "http://arxiv.org/abs/2107.09333",
          "publishedOn": "2021-07-21T02:01:33.687Z",
          "wordCount": 610,
          "title": "StreamBlocks: A compiler for heterogeneous dataflow computing (technical report). (arXiv:2107.09333v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara L. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "Detecting customized moments and highlights from videos given natural\nlanguage (NL) user queries is an important but under-studied topic. One of the\nchallenges in pursuing this direction is the lack of annotated data. To address\nthis issue, we present the Query-based Video Highlights (QVHighlights) dataset.\nIt consists of over 10,000 YouTube videos, covering a wide range of topics,\nfrom everyday activities and travel in lifestyle vlog videos to social and\npolitical activities in news videos. Each video in the dataset is annotated\nwith: (1) a human-written free-form NL query, (2) relevant moments in the video\nw.r.t. the query, and (3) five-point scale saliency scores for all\nquery-relevant clips. This comprehensive annotation enables us to develop and\nevaluate systems that detect relevant moments as well as salient highlights for\ndiverse, flexible user queries. We also present a strong baseline for this\ntask, Moment-DETR, a transformer encoder-decoder model that views moment\nretrieval as a direct set prediction problem, taking extracted video and query\nrepresentations as inputs and predicting moment coordinates and saliency scores\nend-to-end. While our model does not utilize any human prior, we show that it\nperforms competitively when compared to well-engineered architectures. With\nweakly supervised pretraining using ASR captions, Moment-DETR substantially\noutperforms previous methods. Lastly, we present several ablations and\nvisualizations of Moment-DETR. Data and code is publicly available at\nhttps://github.com/jayleicn/moment_detr",
          "link": "http://arxiv.org/abs/2107.09609",
          "publishedOn": "2021-07-21T02:01:33.677Z",
          "wordCount": 680,
          "title": "QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries. (arXiv:2107.09609v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huiqiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weile Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengxi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_B/0/1/0/all/0/1\">B&#xf6;rje F. Karlsson</a>",
          "description": "While named entity recognition (NER) is a key task in natural language\nprocessing, most approaches only target flat entities, ignoring nested\nstructures which are common in many scenarios. Most existing nested NER methods\ntraverse all sub-sequences which is both expensive and inefficient, and also\ndon't well consider boundary knowledge which is significant for nested\nentities. In this paper, we propose a joint entity mention detection and typing\nmodel via prior boundary knowledge (BoningKnife) to better handle nested NER\nextraction and recognition tasks. BoningKnife consists of two modules,\nMentionTagger and TypeClassifier. MentionTagger better leverages boundary\nknowledge beyond just entity start/end to improve the handling of nesting\nlevels and longer spans, while generating high quality mention candidates.\nTypeClassifier utilizes a two-level attention mechanism to decouple different\nnested level representations and better distinguish entity types. We jointly\ntrain both modules sharing a common representation and a new dual-info\nattention layer, which leads to improved representation focus on entity-related\ninformation. Experiments over different datasets show that our approach\noutperforms previous state of the art methods and achieves 86.41, 85.46, and\n94.2 F1 scores on ACE2004, ACE2005, and NNE, respectively.",
          "link": "http://arxiv.org/abs/2107.09429",
          "publishedOn": "2021-07-21T02:01:33.660Z",
          "wordCount": 641,
          "title": "BoningKnife: Joint Entity Mention Detection and Typing for Nested NER via prior Boundary Knowledge. (arXiv:2107.09429v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gretter_R/0/1/0/all/0/1\">Roberto Gretter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matassoni_M/0/1/0/all/0/1\">Marco Matassoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falavigna_D/0/1/0/all/0/1\">Daniele Falavigna</a>",
          "description": "We address the problem of language model customization in applications where\nthe ASR component needs to manage domain-specific terminology; although current\nstate-of-the-art speech recognition technology provides excellent results for\ngeneric domains, the adaptation to specialized dictionaries or glossaries is\nstill an open issue. In this work we present an approach for automatically\nselecting sentences, from a text corpus, that match, both semantically and\nmorphologically, a glossary of terms (words or composite words) furnished by\nthe user. The final goal is to rapidly adapt the language model of an hybrid\nASR system with a limited amount of in-domain text data in order to\nsuccessfully cope with the linguistic domain at hand; the vocabulary of the\nbaseline model is expanded and tailored, reducing the resulting OOV rate. Data\nselection strategies based on shallow morphological seeds and semantic\nsimilarity viaword2vec are introduced and discussed; the experimental setting\nconsists in a simultaneous interpreting scenario, where ASRs in three languages\nare designed to recognize the domain-specific terms (i.e. dentistry). Results\nusing different metrics (OOV rate, WER, precision and recall) show the\neffectiveness of the proposed techniques.",
          "link": "http://arxiv.org/abs/2107.09433",
          "publishedOn": "2021-07-21T02:01:33.651Z",
          "wordCount": 615,
          "title": "Seed Words Based Data Selection for Language Model Adaptation. (arXiv:2107.09433v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wen-Chin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_T/0/1/0/all/0/1\">Tomoki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinjian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "In voice conversion (VC), an approach showing promising results in the latest\nvoice conversion challenge (VCC) 2020 is to first use an automatic speech\nrecognition (ASR) model to transcribe the source speech into the underlying\nlinguistic contents; these are then used as input by a text-to-speech (TTS)\nsystem to generate the converted speech. Such a paradigm, referred to as\nASR+TTS, overlooks the modeling of prosody, which plays an important role in\nspeech naturalness and conversion similarity. Although some researchers have\nconsidered transferring prosodic clues from the source speech, there arises a\nspeaker mismatch during training and conversion. To address this issue, in this\nwork, we propose to directly predict prosody from the linguistic representation\nin a target-speaker-dependent manner, referred to as target text prediction\n(TTP). We evaluate both methods on the VCC2020 benchmark and consider different\nlinguistic representations. The results demonstrate the effectiveness of TTP in\nboth objective and subjective evaluations.",
          "link": "http://arxiv.org/abs/2107.09477",
          "publishedOn": "2021-07-21T02:01:33.628Z",
          "wordCount": 598,
          "title": "On Prosody Modeling for ASR+TTS based Voice Conversion. (arXiv:2107.09477v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Luyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yujia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aslan_O/0/1/0/all/0/1\">Ozlem Aslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>",
          "description": "We present a new dataset of Wikipedia articles each paired with a knowledge\ngraph, to facilitate the research in conditional text generation, graph\ngeneration and graph representation learning. Existing graph-text paired\ndatasets typically contain small graphs and short text (1 or few sentences),\nthus limiting the capabilities of the models that can be learned on the data.\nOur new dataset WikiGraphs is collected by pairing each Wikipedia article from\nthe established WikiText-103 benchmark (Merity et al., 2016) with a subgraph\nfrom the Freebase knowledge graph (Bollacker et al., 2008). This makes it easy\nto benchmark against other state-of-the-art text generative models that are\ncapable of generating long paragraphs of coherent text. Both the graphs and the\ntext data are of significantly larger scale compared to prior graph-text paired\ndatasets. We present baseline graph neural network and transformer model\nresults on our dataset for 3 tasks: graph -> text generation, graph -> text\nretrieval and text -> graph retrieval. We show that better conditioning on the\ngraph provides gains in generation and retrieval quality but there is still\nlarge room for improvement.",
          "link": "http://arxiv.org/abs/2107.09556",
          "publishedOn": "2021-07-21T02:01:33.596Z",
          "wordCount": 618,
          "title": "WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset. (arXiv:2107.09556v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09332",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seongsik Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Harksoo Kim</a>",
          "description": "The sentence-level relation extraction mainly aims to classify the relation\nbetween two entities in a sentence. The sentence-level relation extraction\ncorpus is often containing data of difficulty for the model to infer or noise\ndata. In this paper, we propose a curriculum learning-based relation extraction\nmodel that split data by difficulty and utilize it for learning. In the\nexperiments with the representative sentence-level relation extraction\ndatasets, TACRED and Re-TACRED, the proposed method showed good performances.",
          "link": "http://arxiv.org/abs/2107.09332",
          "publishedOn": "2021-07-21T02:01:33.587Z",
          "wordCount": 503,
          "title": "Improving Sentence-Level Relation Extraction through Curriculum Learning. (arXiv:2107.09332v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>",
          "description": "We present a simple and effective way to generate a variety of paraphrases\nand find a good quality paraphrase among them. As in previous studies, it is\ndifficult to ensure that one generation method always generates the best\nparaphrase in various domains. Therefore, we focus on finding the best\ncandidate from multiple candidates, rather than assuming that there is only one\ncombination of generative models and decoding options. Our approach shows that\nit is easy to apply in various domains and has sufficiently good performance\ncompared to previous methods. In addition, our approach can be used for data\nagumentation that extends the downstream corpus, showing that it can help\nimprove performance in English and Korean datasets.",
          "link": "http://arxiv.org/abs/2107.09274",
          "publishedOn": "2021-07-21T02:01:33.251Z",
          "wordCount": 538,
          "title": "Paraphrasing via Ranking Many Candidates. (arXiv:2107.09274v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Suzhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lincheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>",
          "description": "We propose an audio-driven talking-head method to generate photo-realistic\ntalking-head videos from a single reference image. In this work, we tackle two\nkey challenges: (i) producing natural head motions that match speech prosody,\nand (ii) maintaining the appearance of a speaker in a large head motion while\nstabilizing the non-face regions. We first design a head pose predictor by\nmodeling rigid 6D head movements with a motion-aware recurrent neural network\n(RNN). In this way, the predicted head poses act as the low-frequency holistic\nmovements of a talking head, thus allowing our latter network to focus on\ndetailed facial movement generation. To depict the entire image motions arising\nfrom audio, we exploit a keypoint based dense motion field representation.\nThen, we develop a motion field generator to produce the dense motion fields\nfrom input audio, head poses, and a reference image. As this keypoint based\nrepresentation models the motions of facial regions, head, and backgrounds\nintegrally, our method can better constrain the spatial and temporal\nconsistency of the generated videos. Finally, an image generation network is\nemployed to render photo-realistic talking-head videos from the estimated\nkeypoint based motion fields and the input reference image. Extensive\nexperiments demonstrate that our method produces videos with plausible head\nmotions, synchronized facial expressions, and stable backgrounds and\noutperforms the state-of-the-art.",
          "link": "http://arxiv.org/abs/2107.09293",
          "publishedOn": "2021-07-21T02:01:33.170Z",
          "wordCount": 663,
          "title": "Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion. (arXiv:2107.09293v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yali Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>",
          "description": "Transcripts generated by automatic speech recognition (ASR) systems for\nspoken documents lack structural annotations such as paragraphs, significantly\nreducing their readability. Automatically predicting paragraph segmentation for\nspoken documents may both improve readability and downstream NLP performance\nsuch as summarization and machine reading comprehension. We propose a sequence\nmodel with self-adaptive sliding window for accurate and efficient paragraph\nsegmentation. We also propose an approach to exploit phonetic information,\nwhich significantly improves robustness of spoken document segmentation to ASR\nerrors. Evaluations are conducted on the English Wiki-727K document\nsegmentation benchmark, a Chinese Wikipedia-based document segmentation dataset\nwe created, and an in-house Chinese spoken document dataset. Our proposed model\noutperforms the state-of-the-art (SOTA) model based on the same BERT-Base,\nincreasing segmentation F1 on the English benchmark by 4.2 points and on\nChinese datasets by 4.3-10.1 points, while reducing inference time to less than\n1/6 of inference time of the current SOTA.",
          "link": "http://arxiv.org/abs/2107.09278",
          "publishedOn": "2021-07-21T02:01:33.158Z",
          "wordCount": 593,
          "title": "Sequence Model with Self-Adaptive Sliding Window for Efficient Spoken Document Segmentation. (arXiv:2107.09278v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09055",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Sonkiya_P/0/1/0/all/0/1\">Priyank Sonkiya</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Bajpai_V/0/1/0/all/0/1\">Vikas Bajpai</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Bansal_A/0/1/0/all/0/1\">Anukriti Bansal</a>",
          "description": "The stock market has been a popular topic of interest in the recent past. The\ngrowth in the inflation rate has compelled people to invest in the stock and\ncommodity markets and other areas rather than saving. Further, the ability of\nDeep Learning models to make predictions on the time series data has been\nproven time and again. Technical analysis on the stock market with the help of\ntechnical indicators has been the most common practice among traders and\ninvestors. One more aspect is the sentiment analysis - the emotion of the\ninvestors that shows the willingness to invest. A variety of techniques have\nbeen used by people around the globe involving basic Machine Learning and\nNeural Networks. Ranging from the basic linear regression to the advanced\nneural networks people have experimented with all possible techniques to\npredict the stock market. It's evident from recent events how news and\nheadlines affect the stock markets and cryptocurrencies. This paper proposes an\nensemble of state-of-the-art methods for predicting stock prices. Firstly\nsentiment analysis of the news and the headlines for the company Apple Inc,\nlisted on the NASDAQ is performed using a version of BERT, which is a\npre-trained transformer model by Google for Natural Language Processing (NLP).\nAfterward, a Generative Adversarial Network (GAN) predicts the stock price for\nApple Inc using the technical indicators, stock indexes of various countries,\nsome commodities, and historical prices along with the sentiment scores.\nComparison is done with baseline models like - Long Short Term Memory (LSTM),\nGated Recurrent Units (GRU), vanilla GAN, and Auto-Regressive Integrated Moving\nAverage (ARIMA) model.",
          "link": "http://arxiv.org/abs/2107.09055",
          "publishedOn": "2021-07-21T02:01:33.143Z",
          "wordCount": 699,
          "title": "Stock price prediction using BERT and GAN. (arXiv:2107.09055v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Whitehead_S/0/1/0/all/0/1\">Spencer Whitehead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>",
          "description": "Generalization to out-of-distribution data has been a problem for Visual\nQuestion Answering (VQA) models. To measure generalization to novel questions,\nwe propose to separate them into \"skills\" and \"concepts\". \"Skills\" are visual\ntasks, such as counting or attribute recognition, and are applied to \"concepts\"\nmentioned in the question, such as objects and people. VQA methods should be\nable to compose skills and concepts in novel ways, regardless of whether the\nspecific composition has been seen in training, yet we demonstrate that\nexisting models have much to improve upon towards handling new compositions. We\npresent a novel method for learning to compose skills and concepts that\nseparates these two factors implicitly within a model by learning grounded\nconcept representations and disentangling the encoding of skills from that of\nconcepts. We enforce these properties with a novel contrastive learning\nprocedure that does not rely on external annotations and can be learned from\nunlabeled image-question pairs. Experiments demonstrate the effectiveness of\nour approach for improving compositional and grounding performance.",
          "link": "http://arxiv.org/abs/2107.09106",
          "publishedOn": "2021-07-21T02:01:33.123Z",
          "wordCount": 624,
          "title": "Separating Skills and Concepts for Novel Visual Question Answering. (arXiv:2107.09106v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>",
          "description": "Typically, a linearly orthogonal transformation mapping is learned by\naligning static type-level embeddings to build a shared semantic space. In view\nof the analysis that contextual embeddings contain richer semantic features, we\ninvestigate a context-aware and dictionary-free mapping approach by leveraging\nparallel corpora. We illustrate that our contextual embedding space mapping\nsignificantly outperforms previous multilingual word embedding methods on the\nbilingual dictionary induction (BDI) task by providing a higher degree of\nisomorphism. To improve the quality of mapping, we also explore sense-level\nembeddings that are split from type-level representations, which can align\nspaces in a finer resolution and yield more precise mapping. Moreover, we\nreveal that contextual embedding spaces suffer from their natural properties --\nanisotropy and anisometry. To mitigate these two problems, we introduce the\niterative normalization algorithm as an imperative preprocessing step. Our\nfindings unfold the tight relationship between isotropy, isometry, and\nisomorphism in normalized contextual embedding spaces.",
          "link": "http://arxiv.org/abs/2107.09186",
          "publishedOn": "2021-07-21T02:01:33.080Z",
          "wordCount": 584,
          "title": "Cross-Lingual BERT Contextual Embedding Space Mapping with Isotropic and Isometric Conditions. (arXiv:2107.09186v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi-Fu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jaesik Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungjin Ahn</a>",
          "description": "Transformers have been successful for many natural language processing tasks.\nHowever, applying transformers to the video domain for tasks such as long-term\nvideo generation and scene understanding has remained elusive due to the high\ncomputational complexity and the lack of natural tokenization. In this paper,\nwe propose the Object-Centric Video Transformer (OCVT) which utilizes an\nobject-centric approach for decomposing scenes into tokens suitable for use in\na generative video transformer. By factoring the video into objects, our fully\nunsupervised model is able to learn complex spatio-temporal dynamics of\nmultiple interacting objects in a scene and generate future frames of the\nvideo. Our model is also significantly more memory-efficient than pixel-based\nmodels and thus able to train on videos of length up to 70 frames with a single\n48GB GPU. We compare our model with previous RNN-based approaches as well as\nother possible video transformer baselines. We demonstrate OCVT performs well\nwhen compared to baselines in generating future frames. OCVT also develops\nuseful representations for video reasoning, achieving start-of-the-art\nperformance on the CATER task.",
          "link": "http://arxiv.org/abs/2107.09240",
          "publishedOn": "2021-07-21T02:01:33.069Z",
          "wordCount": 618,
          "title": "Generative Video Transformer: Can Objects be the Words?. (arXiv:2107.09240v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burns_K/0/1/0/all/0/1\">Kaylee Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>",
          "description": "Although virtual agents are increasingly situated in environments where\nnatural language is the most effective mode of interaction with humans, these\nexchanges are rarely used as an opportunity for learning. Leveraging language\ninteractions effectively requires addressing limitations in the two most common\napproaches to language grounding: semantic parsers built on top of fixed object\ncategories are precise but inflexible and end-to-end models are maximally\nexpressive, but fickle and opaque. Our goal is to develop a system that\nbalances the strengths of each approach so that users can teach agents new\ninstructions that generalize broadly from a single example. We introduce the\nidea of neural abstructions: a set of constraints on the inference procedure of\na label-conditioned generative model that can affect the meaning of the label\nin context. Starting from a core programming language that operates over\nabstructions, users can define increasingly complex mappings from natural\nlanguage to actions. We show that with this method a user population is able to\nbuild a semantic parser for an open-ended house modification task in Minecraft.\nThe semantic parser that results is both flexible and expressive: the\npercentage of utterances sourced from redefinitions increases steadily over the\ncourse of 191 total exchanges, achieving a final value of 28%.",
          "link": "http://arxiv.org/abs/2107.09285",
          "publishedOn": "2021-07-21T02:01:33.033Z",
          "wordCount": 651,
          "title": "Neural Abstructions: Abstractions that Support Construction for Grounded Language Learning. (arXiv:2107.09285v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09099",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiushi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">H Lilian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xubo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>",
          "description": "Punctuation is critical in understanding natural language text. Currently,\nmost automatic speech recognition (ASR) systems do not generate punctuation,\nwhich affects the performance of downstream tasks, such as intent detection and\nslot filling. This gives rise to the need for punctuation restoration. Recent\nwork in punctuation restoration heavily utilizes pre-trained language models\nwithout considering data imbalance when predicting punctuation classes. In this\nwork, we address this problem by proposing a token-level supervised contrastive\nlearning method that aims at maximizing the distance of representation of\ndifferent punctuation marks in the embedding space. The result shows that\ntraining with token-level supervised contrastive learning obtains up to 3.2%\nabsolute F1 improvement on the test set.",
          "link": "http://arxiv.org/abs/2107.09099",
          "publishedOn": "2021-07-21T02:01:33.001Z",
          "wordCount": 554,
          "title": "Token-Level Supervised Contrastive Learning for Punctuation Restoration. (arXiv:2107.09099v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1\">Rajaswa Patil</a>",
          "description": "In this work, we present to the NLP community, and to the wider research\ncommunity as a whole, an application for the diachronic analysis of research\ncorpora. We open source an easy-to-use tool coined: DRIFT, which allows\nresearchers to track research trends and development over the years. The\nanalysis methods are collated from well-cited research works, with a few of our\nown methods added for good measure. Succinctly put, some of the analysis\nmethods are: keyword extraction, word clouds, predicting\ndeclining/stagnant/growing trends using Productivity, tracking bi-grams using\nAcceleration plots, finding the Semantic Drift of words, tracking trends using\nsimilarity, etc. To demonstrate the utility and efficacy of our tool, we\nperform a case study on the cs.CL corpus of the arXiv repository and draw\ninferences from the analysis methods. The toolkit and the associated code are\navailable here: https://github.com/rajaswa/DRIFT.",
          "link": "http://arxiv.org/abs/2107.01198",
          "publishedOn": "2021-07-20T02:04:41.716Z",
          "wordCount": 602,
          "title": "DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature. (arXiv:2107.01198v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Paniagua_V/0/1/0/all/0/1\">V&#xed;ctor Su&#xe1;rez-Paniagua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteley_W/0/1/0/all/0/1\">William Whiteley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghan Wu</a>",
          "description": "Diagnostic or procedural coding of clinical notes aims to derive a coded\nsummary of disease-related information about patients. Such coding is usually\ndone manually in hospitals but could potentially be automated to improve the\nefficiency and accuracy of medical coding. Recent studies on deep learning for\nautomated medical coding achieved promising performances. However, the\nexplainability of these models is usually poor, preventing them to be used\nconfidently in supporting clinical practice. Another limitation is that these\nmodels mostly assume independence among labels, ignoring the complex\ncorrelation among medical codes which can potentially be exploited to improve\nthe performance. We propose a Hierarchical Label-wise Attention Network (HLAN),\nwhich aimed to interpret the model by quantifying importance (as attention\nweights) of words and sentences related to each of the labels. Secondly, we\npropose to enhance the major deep learning models with a label embedding (LE)\ninitialisation approach, which learns a dense, continuous vector representation\nand then injects the representation into the final layers and the label-wise\nattention layers in the models. We evaluated the methods using three settings\non the MIMIC-III discharge summaries: full codes, top-50 codes, and the UK NHS\nCOVID-19 shielding codes. Experiments were conducted to compare HLAN and LE\ninitialisation to the state-of-the-art neural network based methods. HLAN\nachieved the best Micro-level AUC and $F_1$ on the top-50 code prediction and\ncomparable results on the NHS COVID-19 shielding code prediction to other\nmodels. By highlighting the most salient words and sentences for each label,\nHLAN showed more meaningful and comprehensive model interpretation compared to\nits downgraded baselines and the CNN-based models. LE initialisation\nconsistently boosted most deep learning models for automated medical coding.",
          "link": "http://arxiv.org/abs/2010.15728",
          "publishedOn": "2021-07-20T02:04:41.578Z",
          "wordCount": 846,
          "title": "Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation. (arXiv:2010.15728v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1\">Gail Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yahav_E/0/1/0/all/0/1\">Eran Yahav</a>",
          "description": "What is the computational model behind a Transformer? Where recurrent neural\nnetworks have direct parallels in finite state machines, allowing clear\ndiscussion and thought around architecture variants or trained models,\nTransformers have no such familiar parallel. In this paper we aim to change\nthat, proposing a computational model for the transformer-encoder in the form\nof a programming language. We map the basic components of a transformer-encoder\n-- attention and feed-forward computation -- into simple primitives, around\nwhich we form a programming language: the Restricted Access Sequence Processing\nLanguage (RASP). We show how RASP can be used to program solutions to tasks\nthat could conceivably be learned by a Transformer, and how a Transformer can\nbe trained to mimic a RASP solution. In particular, we provide RASP programs\nfor histograms, sorting, and Dyck-languages. We further use our model to relate\ntheir difficulty in terms of the number of required layers and attention heads:\nanalyzing a RASP program implies a maximum number of heads and layers necessary\nto encode a task in a transformer. Finally, we see how insights gained from our\nabstraction might be used to explain phenomena seen in recent works.",
          "link": "http://arxiv.org/abs/2106.06981",
          "publishedOn": "2021-07-20T02:04:41.119Z",
          "wordCount": 651,
          "title": "Thinking Like Transformers. (arXiv:2106.06981v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Potdar_N/0/1/0/all/0/1\">Nihal Potdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_A/0/1/0/all/0/1\">Anderson R. Avila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1\">Chao Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yiran Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>",
          "description": "End-to-end spoken language understanding (SLU) has recently attracted\nincreasing interest. Compared to the conventional tandem-based approach that\ncombines speech recognition and language understanding as separate modules, the\nnew approach extracts users' intentions directly from the speech signals,\nresulting in joint optimization and low latency. Such an approach, however, is\ntypically designed to process one intention at a time, which leads users to\ntake multiple rounds to fulfill their requirements while interacting with a\ndialogue system. In this paper, we propose a streaming end-to-end framework\nthat can process multiple intentions in an online and incremental way. The\nbackbone of our framework is a unidirectional RNN trained with the\nconnectionist temporal classification (CTC) criterion. By this design, an\nintention can be identified when sufficient evidence has been accumulated, and\nmultiple intentions can be identified sequentially. We evaluate our solution on\nthe Fluent Speech Commands (FSC) dataset and the intent detection accuracy is\nabout 97 % on all multi-intent settings. This result is comparable to the\nperformance of the state-of-the-art non-streaming models, but is achieved in an\nonline and incremental way. We also employ our model to a keyword spotting task\nusing the Google Speech Commands dataset and the results are also highly\npromising.",
          "link": "http://arxiv.org/abs/2105.10042",
          "publishedOn": "2021-07-20T02:04:41.090Z",
          "wordCount": 701,
          "title": "A Streaming End-to-End Framework For Spoken Language Understanding. (arXiv:2105.10042v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_B/0/1/0/all/0/1\">Bhumika Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1\">Anuj Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anjum/0/1/0/all/0/1\">Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katarya_R/0/1/0/all/0/1\">Rahul Katarya</a>",
          "description": "Cyberbullying is of extreme prevalence today. Online-hate comments, toxicity,\ncyberbullying amongst children and other vulnerable groups are only growing\nover online classes, and increased access to social platforms, especially post\nCOVID-19. It is paramount to detect and ensure minors' safety across social\nplatforms so that any violence or hate-crime is automatically detected and\nstrict action is taken against it. In our work, we explore binary\nclassification by using a combination of datasets from various social media\nplatforms that cover a wide range of cyberbullying such as sexism, racism,\nabusive, and hate-speech. We experiment through multiple models such as\nBi-LSTM, GloVe, state-of-the-art models like BERT, and apply a unique\npreprocessing technique by introducing a slang-abusive corpus, achieving a\nhigher precision in comparison to models without slang preprocessing.",
          "link": "http://arxiv.org/abs/2107.08902",
          "publishedOn": "2021-07-20T02:04:41.071Z",
          "wordCount": 626,
          "title": "Analysing Cyberbullying using Natural Language Processing by Understanding Jargon in Social Media. (arXiv:2107.08902v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.06110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>",
          "description": "This paper models unsupervised learning of an identity-based pattern (or\ncopying) in speech called reduplication from raw continuous data with deep\nconvolutional neural networks. We use the ciwGAN architecture Begu\\v{s} (2021a;\narXiv:2006.02951) in which learning of meaningful representations in speech\nemerges from a requirement that the CNNs generate informative data. We propose\na technique to wug-test CNNs trained on speech and, based on four generative\ntests, argue that the network learns to represent an identity-based pattern in\nits latent space. By manipulating only two categorical variables in the latent\nspace, we can actively turn an unreduplicated form into a reduplicated form\nwith no other substantial changes to the output in the majority of cases. We\nalso argue that the network extends the identity-based pattern to unobserved\ndata. Exploration of how meaningful representations of identity-based patterns\nemerge in CNNs and how the latent space variables outside of the training range\ncorrelate with identity-based patterns in the output has general implications\nfor neural network interpretability.",
          "link": "http://arxiv.org/abs/2009.06110",
          "publishedOn": "2021-07-20T02:04:41.052Z",
          "wordCount": 628,
          "title": "Identity-Based Patterns in Deep Convolutional Networks: Generative Adversarial Phonology and Reduplication. (arXiv:2009.06110v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.12804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1\">Martin Fajcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jon_J/0/1/0/all/0/1\">Josef Jon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>",
          "description": "This work demonstrates that, contrary to a common belief, using the objective\nwith independence assumption for modelling the span probability $P(a_s,a_e) =\nP(a_s)P(a_e)$ of span starting at position $a_s$ and ending at position $a_e$\nhas adverse effects. Therefore we propose multiple approaches to modelling\njoint probability $P(a_s,a_e)$ directly. Among those, we propose a compound\nobjective, composed from the joint probability while still keeping the\nobjective with independence assumption as an auxiliary objective. We find that\nthe compound objective is consistently superior or equal to other assumptions\nin exact match. Additionally, we identified common errors caused by the\nassumption of independence and manually checked the counterpart predictions,\ndemonstrating the impact of the compound objective on the real examples. Our\nfindings are supported via experiments with three extractive QA models (BIDAF,\nBERT, ALBERT) over six datasets and our code, individual results and manual\nanalysis are available online.",
          "link": "http://arxiv.org/abs/2008.12804",
          "publishedOn": "2021-07-20T02:04:41.021Z",
          "wordCount": 635,
          "title": "Rethinking the Objectives of Extractive Question Answering. (arXiv:2008.12804v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.09152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Ye Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Quan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>",
          "description": "Knowledge distillation has been proven to be effective in model acceleration\nand compression. It allows a small network to learn to generalize in the same\nway as a large network. Recent successes in pre-training suggest the\neffectiveness of transferring model parameters. Inspired by this, we\ninvestigate methods of model acceleration and compression in another line of\nresearch. We propose Weight Distillation to transfer the knowledge in the large\nnetwork parameters through a parameter generator. Our experiments on WMT16\nEn-Ro, NIST12 Zh-En, and WMT14 En-De machine translation tasks show that weight\ndistillation can train a small network that is 1.88~2.94x faster than the large\nnetwork but with competitive performance. With the same sized small network,\nweight distillation can outperform knowledge distillation by 0.51~1.82 BLEU\npoints.",
          "link": "http://arxiv.org/abs/2009.09152",
          "publishedOn": "2021-07-20T02:04:40.957Z",
          "wordCount": 606,
          "title": "Weight Distillation: Transferring the Knowledge in Neural Network Parameters. (arXiv:2009.09152v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Ye Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>",
          "description": "The large attention-based encoder-decoder network (Transformer) has become\nprevailing recently due to its effectiveness. But the high computation\ncomplexity of its decoder raises the inefficiency issue. By examining the\nmathematic formulation of the decoder, we show that under some mild conditions,\nthe architecture could be simplified by compressing its sub-layers, the basic\nbuilding block of Transformer, and achieves a higher parallelism. We thereby\npropose Compressed Attention Network, whose decoder layer consists of only one\nsub-layer instead of three. Extensive experiments on 14 WMT machine translation\ntasks show that our model is 1.42x faster with performance on par with a strong\nbaseline. This strong baseline is already 2x faster than the widely used\nstandard baseline without loss in performance.",
          "link": "http://arxiv.org/abs/2101.00542",
          "publishedOn": "2021-07-20T02:04:40.927Z",
          "wordCount": 580,
          "title": "An Efficient Transformer Decoder with Compressed Sub-layers. (arXiv:2101.00542v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.09132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jieh-Sheng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiang_J/0/1/0/all/0/1\">Jieh Hsiang</a>",
          "description": "Generative models, such as GPT-2, have demonstrated impressive results\nrecently. A fundamental question we'd like to address is: where did the\ngenerated text come from? This work is our initial effort toward answering the\nquestion by using prior art search. The purpose of the prior art search is to\nfind the most similar prior text in the training data of GPT-2. We take a\nreranking approach and apply it to the patent domain. Specifically, we\npre-train GPT-2 models from scratch by using the patent data from the USPTO.\nThe input for the prior art search is the patent text generated by the GPT-2\nmodel. We also pre-trained BERT models from scratch for converting patent text\nto embeddings. The steps of reranking are: (1) search the most similar text in\nthe training data of GPT-2 by taking a bag-of-word ranking approach (BM25), (2)\nconvert the search results in text format to BERT embeddings, and (3) provide\nthe final result by ranking the BERT embeddings based on their similarities\nwith the patent text generated by GPT-2. The experiments in this work show that\nsuch reranking is better than ranking with embeddings alone. However, our mixed\nresults also indicate that calculating the semantic similarities among long\ntext spans is still challenging. To our knowledge, this work is the first to\nimplement a reranking system to identify retrospectively the most similar\ninputs to a GPT model based on its output.",
          "link": "http://arxiv.org/abs/2009.09132",
          "publishedOn": "2021-07-20T02:04:40.905Z",
          "wordCount": 728,
          "title": "Prior Art Search and Reranking for Generated Patent Text. (arXiv:2009.09132v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00858",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_J/0/1/0/all/0/1\">Jian Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_W/0/1/0/all/0/1\">Wenning Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>",
          "description": "In this paper, several works are proposed to address practical challenges for\ndeploying RNN Transducer (RNN-T) based speech recognition system. These\nchallenges are adapting a well-trained RNN-T model to a new domain without\ncollecting the audio data, obtaining time stamps and confidence scores at word\nlevel. The first challenge is solved with a splicing data method which\nconcatenates the speech segments extracted from the source domain data. To get\nthe time stamp, a phone prediction branch is added to the RNN-T model by\nsharing the encoder for the purpose of force alignment. Finally, we obtain\nword-level confidence scores by utilizing several types of features calculated\nduring decoding and from confusion network. Evaluated with Microsoft production\ndata, the splicing data adaptation method improves the baseline and adaptation\nwith the text to speech method by 58.03% and 15.25% relative word error rate\nreduction, respectively. The proposed time stamping method can get less than\n50ms word timing difference from the ground truth alignment on average while\nmaintaining the recognition accuracy of the RNN-T model. We also obtain high\nconfidence annotation performance with limited computation cost.",
          "link": "http://arxiv.org/abs/2105.00858",
          "publishedOn": "2021-07-20T02:04:40.886Z",
          "wordCount": 657,
          "title": "On Addressing Practical Challenges for RNN-Transducer. (arXiv:2105.00858v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.00908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chesi_C/0/1/0/all/0/1\">Cristiano Chesi</a>",
          "description": "A cognitively plausible parsing algorithm should perform like the human\nparser in critical contexts. Here I propose an adaptation of Earley's parsing\nalgorithm, suitable for Phase-based Minimalist Grammars (PMG, Chesi 2012), that\nis able to predict complexity effects in performance. Focusing on self-paced\nreading experiments of object clefts sentences (Warren & Gibson 2005) I will\nassociate to parsing a complexity metric based on cued features to be retrieved\nat the verb segment (Feature Retrieval & Encoding Cost, FREC). FREC is\ncrucially based on the usage of memory predicted by the discussed parsing\nalgorithm and it correctly fits with the reading time revealed.",
          "link": "http://arxiv.org/abs/1906.00908",
          "publishedOn": "2021-07-20T02:04:40.864Z",
          "wordCount": 580,
          "title": "Phase-based Minimalist Parsing and complexity in non-local dependencies. (arXiv:1906.00908v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thanh-Dung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noumeir_R/0/1/0/all/0/1\">Rita Noumeir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambaud_J/0/1/0/all/0/1\">Jerome Rambaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sans_G/0/1/0/all/0/1\">Guillaume Sans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jouvet_P/0/1/0/all/0/1\">Philippe Jouvet</a>",
          "description": "The rapid progress in clinical data management systems and artificial\nintelligence approaches enable the era of personalized medicine. Intensive care\nunits (ICUs) are the ideal clinical research environment for such development\nbecause they collect many clinical data and are highly computerized\nenvironments. We designed a retrospective clinical study on a prospective ICU\ndatabase using clinical natural language to help in the early diagnosis of\nheart failure in critically ill children. The methodology consisted of\nempirical experiments of a learning algorithm to learn the hidden\ninterpretation and presentation of the French clinical note data. This study\nincluded 1386 patients' clinical notes with 5444 single lines of notes. There\nwere 1941 positive cases (36 % of total) and 3503 negative cases classified by\ntwo independent physicians using a standardized approach. The multilayer\nperceptron neural network outperforms other discriminative and generative\nclassifiers. Consequently, the proposed framework yields an overall\nclassification performance with 89 % accuracy, 88 % recall, and 89 % precision.\nFurthermore, a generative autoencoder learning algorithm was proposed to\nleverage the sparsity reduction that achieved 91% accuracy, 91% recall, and 91%\nprecision. This study successfully applied learning representation and machine\nlearning algorithms to detect heart failure from clinical natural language in a\nsingle French institution. Further work is needed to use the same methodology\nin other institutions and other languages.",
          "link": "http://arxiv.org/abs/2104.03969",
          "publishedOn": "2021-07-20T02:04:40.806Z",
          "wordCount": 716,
          "title": "Detecting of a Patient's Condition From Clinical Narratives Using Natural Language Representation. (arXiv:2104.03969v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1\">Aishwarya Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan Tur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>",
          "description": "Inspired by recent work in meta-learning and generative teaching networks, we\npropose a framework called Generative Conversational Networks, in which\nconversational agents learn to generate their own labelled training data (given\nsome seed data) and then train themselves from that data to perform a given\ntask. We use reinforcement learning to optimize the data generation process\nwhere the reward signal is the agent's performance on the task. The task can be\nany language-related task, from intent detection to full task-oriented\nconversations. In this work, we show that our approach is able to generalise\nfrom seed data and performs well in limited data and limited computation\nsettings, with significant gains for intent detection and slot tagging across\nmultiple datasets: ATIS, TOD, SNIPS, and Restaurants8k. We show an average\nimprovement of 35% in intent detection and 21% in slot tagging over a baseline\nmodel trained from the seed data. We also conduct an analysis of the novelty of\nthe generated data and provide generated examples for intent detection, slot\ntagging, and non-goal oriented conversations.",
          "link": "http://arxiv.org/abs/2106.08484",
          "publishedOn": "2021-07-20T02:04:40.786Z",
          "wordCount": 636,
          "title": "Generative Conversational Networks. (arXiv:2106.08484v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.09410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Deokgun Park</a>",
          "description": "Despite recent advances in many application-specific domains, we do not know\nhow to build a human-level artificial intelligence (HLAI). We conjecture that\nlearning from others' experience with the language is the essential\ncharacteristic that distinguishes human intelligence from the rest. Humans can\nupdate the action-value function with the verbal description as if they\nexperience states, actions, and corresponding rewards sequences firsthand. In\nthis paper, we present a classification of intelligence according to how\nindividual agents learn and propose a definition and a test for HLAI. The main\nidea is that language acquisition without explicit rewards can be a sufficient\ntest for HLAI.",
          "link": "http://arxiv.org/abs/2011.09410",
          "publishedOn": "2021-07-20T02:04:40.762Z",
          "wordCount": 575,
          "title": "A Definition and a Test for Human-Level Artificial Intelligence. (arXiv:2011.09410v4 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karakanta_A/0/1/0/all/0/1\">Alina Karakanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>",
          "description": "With the increased audiovisualisation of communication, the need for live\nsubtitles in multilingual events is more relevant than ever. In an attempt to\nautomatise the process, we aim at exploring the feasibility of simultaneous\nspeech translation (SimulST) for live subtitling. However, the word-for-word\nrate of generation of SimulST systems is not optimal for displaying the\nsubtitles in a comprehensible and readable way. In this work, we adapt SimulST\nsystems to predict subtitle breaks along with the translation. We then propose\na display mode that exploits the predicted break structure by presenting the\nsubtitles in scrolling lines. We compare our proposed mode with a display 1)\nword-for-word and 2) in blocks, in terms of reading speed and delay.\nExperiments on three language pairs (en$\\rightarrow$it, de, fr) show that\nscrolling lines is the only mode achieving an acceptable reading speed while\nkeeping delay close to a 4-second threshold. We argue that simultaneous\ntranslation for readable live subtitles still faces challenges, the main one\nbeing poor translation quality, and propose directions for steering future\nresearch.",
          "link": "http://arxiv.org/abs/2107.08807",
          "publishedOn": "2021-07-20T02:04:40.743Z",
          "wordCount": 622,
          "title": "Simultaneous Speech Translation for Live Subtitling: from Delay to Display. (arXiv:2107.08807v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.",
          "link": "http://arxiv.org/abs/2107.08264",
          "publishedOn": "2021-07-20T02:04:40.723Z",
          "wordCount": 685,
          "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jie_C/0/1/0/all/0/1\">Cheng Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Da Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zigeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>",
          "description": "With the increasing scale of search engine marketing, designing an efficient\nbidding system is becoming paramount for the success of e-commerce companies.\nThe critical challenges faced by a modern industrial-level bidding system\ninclude: 1. the catalog is enormous, and the relevant bidding features are of\nhigh sparsity; 2. the large volume of bidding requests induces significant\ncomputation burden to both the offline and online serving. Leveraging\nextraneous user-item information proves essential to mitigate the sparsity\nissue, for which we exploit the natural language signals from the users' query\nand the contextual knowledge from the products. In particular, we extract the\nvector representations of ads via the Transformer model and leverage their\ngeometric relation to building collaborative bidding predictions via\nclustering. The two-step procedure also significantly reduces the computation\nstress of bid evaluation and optimization. In this paper, we introduce the\nend-to-end structure of the bidding system for search engine marketing for\nWalmart e-commerce, which successfully handles tens of millions of bids each\nday. We analyze the online and offline performances of our approach and discuss\nhow we find it as a production-efficient solution.",
          "link": "http://arxiv.org/abs/2106.12700",
          "publishedOn": "2021-07-20T02:04:40.640Z",
          "wordCount": 657,
          "title": "An Efficient Group-based Search Engine Marketing System for E-Commerce. (arXiv:2106.12700v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anderson_M/0/1/0/all/0/1\">Mark Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>",
          "description": "We present the system submission from the FASTPARSE team for the EUD Shared\nTask at IWPT 2021. We engaged in the task last year by focusing on efficiency.\nThis year we have focused on experimenting with new ideas on a limited time\nbudget. Our system is based on splitting the EUD graph into several trees,\nbased on linguistic criteria. We predict these trees using a sequence-labelling\nparser and combine them into an EUD graph. The results were relatively poor,\nalthough not a total disaster and could probably be improved with some\npolishing of the system's rough edges.",
          "link": "http://arxiv.org/abs/2106.13155",
          "publishedOn": "2021-07-20T02:04:40.621Z",
          "wordCount": 575,
          "title": "Splitting EUD graphs into trees: A quick and clatty approach. (arXiv:2106.13155v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruiter_D/0/1/0/all/0/1\">Dana Ruiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1\">Josef van Genabith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espana_Bonet_C/0/1/0/all/0/1\">Cristina Espa&#xf1;a-Bonet</a>",
          "description": "For most language combinations, parallel data is either scarce or simply\nunavailable. To address this, unsupervised machine translation (UMT) exploits\nlarge amounts of monolingual data by using synthetic data generation techniques\nsuch as back-translation and noising, while self-supervised NMT (SSNMT)\nidentifies parallel sentences in smaller comparable data and trains on them. To\ndate, the inclusion of UMT data generation techniques in SSNMT has not been\ninvestigated. We show that including UMT techniques into SSNMT significantly\noutperforms SSNMT and UMT on all tested language pairs, with improvements of up\nto +4.3 BLEU, +50.8 BLEU, +51.5 over SSNMT, statistical UMT and hybrid UMT,\nrespectively, on Afrikaans to English. We further show that the combination of\nmultilingual denoising autoencoding, SSNMT with backtranslation and bilingual\nfinetuning enables us to learn machine translation even for distant language\npairs for which only small amounts of monolingual data are available, e.g.\nyielding BLEU scores of 11.6 (English to Swahili).",
          "link": "http://arxiv.org/abs/2107.08772",
          "publishedOn": "2021-07-20T02:04:40.442Z",
          "wordCount": 604,
          "title": "Integrating Unsupervised Data Generation into Self-Supervised Neural Machine Translation for Low-Resource Languages. (arXiv:2107.08772v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.01670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1\">Bertie Vidgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>",
          "description": "Data-driven analysis and detection of abusive online content covers many\ndifferent tasks, phenomena, contexts, and methodologies. This paper\nsystematically reviews abusive language dataset creation and content in\nconjunction with an open website for cataloguing abusive language data. This\ncollection of knowledge leads to a synthesis providing evidence-based\nrecommendations for practitioners working with this complex and highly diverse\ndata.",
          "link": "http://arxiv.org/abs/2004.01670",
          "publishedOn": "2021-07-20T02:04:40.423Z",
          "wordCount": 533,
          "title": "Directions in Abusive Language Training Data: Garbage In, Garbage Out. (arXiv:2004.01670v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Slavnov_S/0/1/0/all/0/1\">Sergey Slavnov</a>",
          "description": "We propose a concrete surface representation of abstract categorial grammars\nin the category of word cobordisms or cowordisms for short, which are certain\nbipartite graphs decorated with words in a given alphabet, generalizing linear\nlogic proof-nets. We also introduce and study linear logic grammars, directly\nbased on cobordisms and using classical multiplicative linear logic as a typing\nsystem.",
          "link": "http://arxiv.org/abs/2107.08728",
          "publishedOn": "2021-07-20T02:04:40.362Z",
          "wordCount": 519,
          "title": "Cobordisms and commutative categorial grammars. (arXiv:2107.08728v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2007.12988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1\">Thayer Alshaabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_J/0/1/0/all/0/1\">Jane L. Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1\">Michael V. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minot_J/0/1/0/all/0/1\">Joshua R. Minot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dewhurst_D/0/1/0/all/0/1\">David R. Dewhurst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reagan_A/0/1/0/all/0/1\">Andrew J. Reagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1\">Christopher M. Danforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1\">Peter Sheridan Dodds</a>",
          "description": "In real-time, social media data strongly imprints world events, popular\nculture, and day-to-day conversations by millions of ordinary people at a scale\nthat is scarcely conventionalized and recorded. Vitally, and absent from many\nstandard corpora such as books and news archives, sharing and commenting\nmechanisms are native to social media platforms, enabling us to quantify social\namplification (i.e., popularity) of trending storylines and contemporary\ncultural phenomena. Here, we describe Storywrangler, a natural language\nprocessing instrument designed to carry out an ongoing, day-scale curation of\nover 100 billion tweets containing roughly 1 trillion 1-grams from 2008 to\n2021. For each day, we break tweets into unigrams, bigrams, and trigrams\nspanning over 100 languages. We track n-gram usage frequencies, and generate\nZipf distributions, for words, hashtags, handles, numerals, symbols, and\nemojis. We make the data set available through an interactive time series\nviewer, and as downloadable time series and daily distributions. Although\nStorywrangler leverages Twitter data, our method of extracting and tracking\ndynamic changes of n-grams can be extended to any similar social media\nplatform. We showcase a few examples of the many possible avenues of study we\naim to enable including how social amplification can be visualized through\n'contagiograms'. We also present some example case studies that bridge n-gram\ntime series with disparate data sources to explore sociotechnical dynamics of\nfamous individuals, box office success, and social unrest.",
          "link": "http://arxiv.org/abs/2007.12988",
          "publishedOn": "2021-07-20T02:04:40.344Z",
          "wordCount": 786,
          "title": "Storywrangler: A massive exploratorium for sociolinguistic, cultural, socioeconomic, and political timelines using Twitter. (arXiv:2007.12988v5 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1\">Nianlong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ash_E/0/1/0/all/0/1\">Elliott Ash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahnloser_R/0/1/0/all/0/1\">Richard H.R. Hahnloser</a>",
          "description": "We introduce MemSum (Multi-step Episodic Markov decision process extractive\nSUMmarizer), a reinforcement-learning-based extractive summarizer enriched at\nany given time step with information on the current extraction history. Similar\nto previous models in this vein, MemSum iteratively selects sentences into the\nsummary. Our innovation is in considering a broader information set when\nsummarizing that would intuitively also be used by humans in this task: 1) the\ntext content of the sentence, 2) the global text context of the rest of the\ndocument, and 3) the extraction history consisting of the set of sentences that\nhave already been extracted. With a lightweight architecture, MemSum\nnonetheless obtains state-of-the-art test-set performance (ROUGE score) on long\ndocument datasets (PubMed, arXiv, and GovReport). Supporting analysis\ndemonstrates that the added awareness of extraction history gives MemSum\nrobustness against redundancy in the source document.",
          "link": "http://arxiv.org/abs/2107.08929",
          "publishedOn": "2021-07-20T02:04:40.322Z",
          "wordCount": 575,
          "title": "MemSum: Extractive Summarization of Long Documents using Multi-step Episodic Markov Decision Processes. (arXiv:2107.08929v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08721",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Chen_Q/0/1/0/all/0/1\">Qinkai Chen</a>",
          "description": "News events can greatly influence equity markets. In this paper, we are\ninterested in predicting the short-term movement of stock prices after\nfinancial news events using only the headlines of the news. To achieve this\ngoal, we introduce a new text mining method called Fine-Tuned\nContextualized-Embedding Recurrent Neural Network (FT-CE-RNN). Compared with\nprevious approaches which use static vector representations of the news (static\nembedding), our model uses contextualized vector representations of the\nheadlines (contextualized embeddings) generated from Bidirectional Encoder\nRepresentations from Transformers (BERT). Our model obtains the\nstate-of-the-art result on this stock movement prediction task. It shows\nsignificant improvement compared with other baseline models, in both accuracy\nand trading simulations. Through various trading simulations based on millions\nof headlines from Bloomberg News, we demonstrate the ability of this model in\nreal scenarios.",
          "link": "http://arxiv.org/abs/2107.08721",
          "publishedOn": "2021-07-20T02:04:40.303Z",
          "wordCount": 590,
          "title": "Stock Movement Prediction with Financial News using Contextualized Embedding from BERT. (arXiv:2107.08721v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fabiano_F/0/1/0/all/0/1\">Francesco Fabiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1\">Biplav Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenchner_J/0/1/0/all/0/1\">Jonathan Lenchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horesh_L/0/1/0/all/0/1\">Lior Horesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1\">Francesca Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganapini_M/0/1/0/all/0/1\">Marianna Bergamaschi Ganapini</a>",
          "description": "Epistemic Planning (EP) refers to an automated planning setting where the\nagent reasons in the space of knowledge states and tries to find a plan to\nreach a desirable state from the current state. Its general form, the\nMulti-agent Epistemic Planning (MEP) problem involves multiple agents who need\nto reason about both the state of the world and the information flow between\nagents. In a MEP problem, multiple approaches have been developed recently with\nvarying restrictions, such as considering only the concept of knowledge while\nnot allowing the idea of belief, or not allowing for ``complex\" modal operators\nsuch as those needed to handle dynamic common knowledge. While the diversity of\napproaches has led to a deeper understanding of the problem space, the lack of\na standardized way to specify MEP problems independently of solution approaches\nhas created difficulties in comparing performance of planners, identifying\npromising techniques, exploring new strategies like ensemble methods, and\nmaking it easy for new researchers to contribute to this research area. To\naddress the situation, we propose a unified way of specifying EP problems - the\nEpistemic Planning Domain Definition Language, E-PDDL. We show that E-PPDL can\nbe supported by leading MEP planners and provide corresponding parser code that\ntranslates EP problems specified in E-PDDL into (M)EP problems that can be\nhandled by several planners. This work is also useful in building more general\nepistemic planning environments where we envision a meta-cognitive module that\ntakes a planning problem in E-PDDL, identifies and assesses some of its\nfeatures, and autonomously decides which planner is the best one to solve it.",
          "link": "http://arxiv.org/abs/2107.08739",
          "publishedOn": "2021-07-20T02:04:40.282Z",
          "wordCount": 718,
          "title": "E-PDDL: A Standardized Way of Defining Epistemic Planning Problems. (arXiv:2107.08739v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_P/0/1/0/all/0/1\">Pan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>",
          "description": "A proactive dialogue system has the ability to proactively lead the\nconversation. Different from the general chatbots which only react to the user,\nproactive dialogue systems can be used to achieve some goals, e.g., to\nrecommend some items to the user. Background knowledge is essential to enable\nsmooth and natural transitions in dialogue. In this paper, we propose a new\nmulti-task learning framework for retrieval-based knowledge-grounded proactive\ndialogue. To determine the relevant knowledge to be used, we frame knowledge\nprediction as a complementary task and use explicit signals to supervise its\nlearning. The final response is selected according to the predicted knowledge,\nthe goal to achieve, and the context. Experimental results show that explicit\nmodeling of knowledge prediction and goal selection can greatly improve the\nfinal response selection. Our code is available at\nhttps://github.com/DaoD/KPN/.",
          "link": "http://arxiv.org/abs/2107.08329",
          "publishedOn": "2021-07-20T02:04:40.260Z",
          "wordCount": 581,
          "title": "Proactive Retrieval-based Chatbots based on Relevant Knowledge and Goals. (arXiv:2107.08329v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hegel_A/0/1/0/all/0/1\">Allison Hegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Marina Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peaslee_G/0/1/0/all/0/1\">Genevieve Peaslee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roof_B/0/1/0/all/0/1\">Brendan Roof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elwany_E/0/1/0/all/0/1\">Emad Elwany</a>",
          "description": "Large, pre-trained transformer models like BERT have achieved\nstate-of-the-art results on document understanding tasks, but most\nimplementations can only consider 512 tokens at a time. For many real-world\napplications, documents can be much longer, and the segmentation strategies\ntypically used on longer documents miss out on document structure and\ncontextual information, hurting their results on downstream tasks. In our work\non legal agreements, we find that visual cues such as layout, style, and\nplacement of text in a document are strong features that are crucial to\nachieving an acceptable level of accuracy on long documents. We measure the\nimpact of incorporating such visual cues, obtained via computer vision methods,\non the accuracy of document understanding tasks including document\nsegmentation, entity extraction, and attribute classification. Our method of\nsegmenting documents based on structural metadata out-performs existing methods\non four long-document understanding tasks as measured on the Contract\nUnderstanding Atticus Dataset.",
          "link": "http://arxiv.org/abs/2107.08128",
          "publishedOn": "2021-07-20T02:04:40.191Z",
          "wordCount": 602,
          "title": "The Law of Large Documents: Understanding the Structure of Legal Contracts Using Visual Cues. (arXiv:2107.08128v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Braun_R/0/1/0/all/0/1\">Rudolf A. Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madikeri_S/0/1/0/all/0/1\">Srikanth Madikeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>",
          "description": "A common problem for automatic speech recognition systems is how to recognize\nwords that they did not see during training. Currently there is no established\nmethod of evaluating different techniques for tackling this problem. We propose\nusing the CommonVoice dataset to create test sets for multiple languages which\nhave a high out-of-vocabulary (OOV) ratio relative to a training set and\nrelease a new tool for calculating relevant performance metrics. We then\nevaluate, within the context of a hybrid ASR system, how much better subword\nmodels are at recognizing OOVs, and how much benefit one can get from\nincorporating OOV-word information into an existing system by modifying WFSTs.\nAdditionally, we propose a new method for modifying a subword-based language\nmodel so as to better recognize OOV-words. We showcase very large improvements\nin OOV-word recognition and make both the data and code available.",
          "link": "http://arxiv.org/abs/2107.08091",
          "publishedOn": "2021-07-20T02:04:40.151Z",
          "wordCount": 592,
          "title": "A Comparison of Methods for OOV-word Recognition on a New Public Dataset. (arXiv:2107.08091v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.05357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazumder_S/0/1/0/all/0/1\">Sahisnu Mazumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>",
          "description": "Classifying and resolving coreferences of objects (e.g., product names) and\nattributes (e.g., product aspects) in opinionated reviews is crucial for\nimproving the opinion mining performance. However, the task is challenging as\none often needs to consider domain-specific knowledge (e.g., iPad is a tablet\nand has aspect resolution) to identify coreferences in opinionated reviews.\nAlso, compiling a handcrafted and curated domain-specific knowledge base for\neach domain is very time consuming and arduous. This paper proposes an approach\nto automatically mine and leverage domain-specific knowledge for classifying\nobjects and attribute coreferences. The approach extracts domain-specific\nknowledge from unlabeled review data and trains a knowledgeaware neural\ncoreference classification model to leverage (useful) domain knowledge together\nwith general commonsense knowledge for the task. Experimental evaluation on\nrealworld datasets involving five domains (product types) shows the\neffectiveness of the approach.",
          "link": "http://arxiv.org/abs/2010.05357",
          "publishedOn": "2021-07-20T02:04:40.129Z",
          "wordCount": 615,
          "title": "A Knowledge-Driven Approach to Classifying Object and Attribute Coreferences in Opinion Mining. (arXiv:2010.05357v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fanton_M/0/1/0/all/0/1\">Margherita Fanton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonaldi_H/0/1/0/all/0/1\">Helena Bonaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tekiroglu_S/0/1/0/all/0/1\">Serra Sinem Tekiroglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerini_M/0/1/0/all/0/1\">Marco Guerini</a>",
          "description": "Undermining the impact of hateful content with informed and non-aggressive\nresponses, called counter narratives, has emerged as a possible solution for\nhaving healthier online communities. Thus, some NLP studies have started\naddressing the task of counter narrative generation. Although such studies have\nmade an effort to build hate speech / counter narrative (HS/CN) datasets for\nneural generation, they fall short in reaching either high-quality and/or\nhigh-quantity. In this paper, we propose a novel human-in-the-loop data\ncollection methodology in which a generative language model is refined\niteratively by using its own data from the previous loops to generate new\ntraining samples that experts review and/or post-edit. Our experiments\ncomprised several loops including dynamic variations. Results show that the\nmethodology is scalable and facilitates diverse, novel, and cost-effective data\ncollection. To our knowledge, the resulting dataset is the only expert-based\nmulti-target HS/CN dataset available to the community.",
          "link": "http://arxiv.org/abs/2107.08720",
          "publishedOn": "2021-07-20T02:04:40.103Z",
          "wordCount": 603,
          "title": "Human-in-the-Loop for Data Collection: a Multi-Target Counter Narrative Dataset to Fight Online Hate Speech. (arXiv:2107.08720v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomerantz_R/0/1/0/all/0/1\">Roi Pomerantz</a>",
          "description": "We present Translatotron 2, a neural direct speech-to-speech translation\nmodel that can be trained end-to-end. Translatotron 2 consists of a speech\nencoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention\nmodule that connects all the previous three components. Experimental results\nsuggest that Translatotron 2 outperforms the original Translatotron by a large\nmargin in terms of translation quality and predicted speech naturalness, and\ndrastically improves the robustness of the predicted speech by mitigating\nover-generation, such as babbling or long pause. We also propose a new method\nfor retaining the source speaker's voice in the translated speech. The trained\nmodel is restricted to retain the source speaker's voice, and unlike the\noriginal Translatotron, it is not able to generate speech in a different\nspeaker's voice, making the model more robust for production deployment, by\nmitigating potential misuse for creating spoofing audio artifacts. When the new\nmethod is used together with a simple concatenation-based data augmentation,\nthe trained Translatotron 2 model is able to retain each speaker's voice for\ninput with speaker turns.",
          "link": "http://arxiv.org/abs/2107.08661",
          "publishedOn": "2021-07-20T02:04:40.071Z",
          "wordCount": 614,
          "title": "Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nyoungwoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Suwon Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Ho-Jin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myaeng_S/0/1/0/all/0/1\">Sung-Hyun Myaeng</a>",
          "description": "In multi-modal dialogue systems, it is important to allow the use of images\nas part of a multi-turn conversation. Training such dialogue systems generally\nrequires a large-scale dataset consisting of multi-turn dialogues that involve\nimages, but such datasets rarely exist. In response, this paper proposes a 45k\nmulti-modal dialogue dataset created with minimal human intervention. Our\nmethod to create such a dataset consists of (1) preparing and pre-processing\ntext dialogue datasets, (2) creating image-mixed dialogues by using a\ntext-to-image replacement technique, and (3) employing a\ncontextual-similarity-based filtering step to ensure the contextual coherence\nof the dataset. To evaluate the validity of our dataset, we devise a simple\nretrieval model for dialogue sentence prediction tasks. Automatic metrics and\nhuman evaluation results on such tasks show that our dataset can be effectively\nused as training data for multi-modal dialogue systems which require an\nunderstanding of images and text in a context-aware manner. Our dataset and\ngeneration code is available at\nhttps://github.com/shh1574/multi-modal-dialogue-dataset.",
          "link": "http://arxiv.org/abs/2107.08685",
          "publishedOn": "2021-07-20T02:04:40.007Z",
          "wordCount": 606,
          "title": "Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images. (arXiv:2107.08685v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1\">Ishika Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gargi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1\">Ashutosh Modi</a>",
          "description": "Recently, text world games have been proposed to enable artificial agents to\nunderstand and reason about real-world scenarios. These text-based games are\nchallenging for artificial agents, as it requires understanding and interaction\nusing natural language in a partially observable environment. In this paper, we\nimprove the semantic understanding of the agent by proposing a simple RL with\nLM framework where we use transformer-based language models with Deep RL\nmodels. We perform a detailed study of our framework to demonstrate how our\nmodel outperforms all existing agents on the popular game, Zork1, to achieve a\nscore of 44.7, which is 1.6 higher than the state-of-the-art model. Our\nproposed approach also performs comparably to the state-of-the-art models on\nthe other set of text games.",
          "link": "http://arxiv.org/abs/2107.08408",
          "publishedOn": "2021-07-20T02:04:39.987Z",
          "wordCount": 581,
          "title": "Pre-trained Language Models as Prior Knowledge for Playing Text-based Games. (arXiv:2107.08408v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gantt_W/0/1/0/all/0/1\">William Gantt</a>",
          "description": "Semantic role labeling (SRL) -- identifying the semantic relationships\nbetween a predicate and other constituents in the same sentence -- is a\nwell-studied task in natural language understanding (NLU). However, many of\nthese relationships are evident only at the level of the document, as a role\nfor a predicate in one sentence may often be filled by an argument in a\ndifferent one. This more general task, known as implicit semantic role labeling\nor argument linking, has received increased attention in recent years, as\nresearchers have recognized its centrality to information extraction and NLU.\nThis paper surveys the literature on argument linking and identifies several\nnotable shortcomings of existing approaches that indicate the paths along which\nfuture research effort could most profitably be spent.",
          "link": "http://arxiv.org/abs/2107.08523",
          "publishedOn": "2021-07-20T02:04:39.968Z",
          "wordCount": 549,
          "title": "Argument Linking: A Survey and Forecast. (arXiv:2107.08523v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geng_B/0/1/0/all/0/1\">Binzong Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1\">Fajie Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiancheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>",
          "description": "This ability to learn consecutive tasks without forgetting how to perform\npreviously trained problems is essential for developing an online dialogue\nsystem. This paper proposes an effective continual learning for the\ntask-oriented dialogue system with iterative network pruning, expanding and\nmasking (TPEM), which preserves performance on previously encountered tasks\nwhile accelerating learning progress on subsequent tasks. Specifically, TPEM\n(i) leverages network pruning to keep the knowledge for old tasks, (ii) adopts\nnetwork expanding to create free weights for new tasks, and (iii) introduces\ntask-specific network masking to alleviate the negative impact of fixed weights\nof old tasks on new tasks. We conduct extensive experiments on seven different\ntasks from three benchmark datasets and show empirically that TPEM leads to\nsignificantly improved results over the strong competitors. For\nreproducibility, we submit the code and data at:\nhttps://github.com/siat-nlp/TPEM",
          "link": "http://arxiv.org/abs/2107.08173",
          "publishedOn": "2021-07-20T02:04:39.940Z",
          "wordCount": 598,
          "title": "Continual Learning for Task-oriented Dialogue System with Iterative Network Pruning, Expanding and Masking. (arXiv:2107.08173v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bian_N/0/1/0/all/0/1\">Ning Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Ben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>",
          "description": "Despite recent success in machine reading comprehension (MRC), learning\nhigh-quality MRC models still requires large-scale labeled training data, even\nusing strong pre-trained language models (PLMs). The pre-training tasks for\nPLMs are not question-answering or MRC-based tasks, making existing PLMs unable\nto be directly used for unsupervised MRC. Specifically, MRC aims to spot an\naccurate answer span from the given document, but PLMs focus on token filling\nin sentences. In this paper, we propose a new framework for unsupervised MRC.\nFirstly, we propose to learn to spot answer spans in documents via\nself-supervised learning, by designing a self-supervision pretext task for MRC\n- Spotting-MLM. Solving this task requires capturing deep interactions between\nsentences in documents. Secondly, we apply a simple sentence rewriting strategy\nin the inference stage to alleviate the expression mismatch between questions\nand documents. Experiments show that our method achieves a new state-of-the-art\nperformance for unsupervised MRC.",
          "link": "http://arxiv.org/abs/2107.08582",
          "publishedOn": "2021-07-20T02:04:39.921Z",
          "wordCount": 593,
          "title": "Bridging the Gap between Language Model and Reading Comprehension: Unsupervised MRC via Self-Supervision. (arXiv:2107.08582v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jack Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1\">Raphael Lenain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1\">Udeepa Meepegama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1\">Emil Fristed</a>",
          "description": "We introduce ParaBLEU, a paraphrase representation learning model and\nevaluation metric for text generation. Unlike previous approaches, ParaBLEU\nlearns to understand paraphrasis using generative conditioning as a pretraining\nobjective. ParaBLEU correlates more strongly with human judgements than\nexisting metrics, obtaining new state-of-the-art results on the 2017 WMT\nMetrics Shared Task. We show that our model is robust to data scarcity,\nexceeding previous state-of-the-art performance using only $50\\%$ of the\navailable training data and surpassing BLEU, ROUGE and METEOR with only $40$\nlabelled examples. Finally, we demonstrate that ParaBLEU can be used to\nconditionally generate novel paraphrases from a single demonstration, which we\nuse to confirm our hypothesis that it learns abstract, generalized paraphrase\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.08251",
          "publishedOn": "2021-07-20T02:04:39.649Z",
          "wordCount": 546,
          "title": "Generative Pretraining for Paraphrase Evaluation. (arXiv:2107.08251v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yao Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Haipeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Despite being a critical communication skill, grasping humor is challenging\n-- a successful use of humor requires a mixture of both engaging content\nbuild-up and an appropriate vocal delivery (e.g., pause). Prior studies on\ncomputational humor emphasize the textual and audio features immediately next\nto the punchline, yet overlooking longer-term context setup. Moreover, the\ntheories are usually too abstract for understanding each concrete humor\nsnippet. To fill in the gap, we develop DeHumor, a visual analytical system for\nanalyzing humorous behaviors in public speaking. To intuitively reveal the\nbuilding blocks of each concrete example, DeHumor decomposes each humorous\nvideo into multimodal features and provides inline annotations of them on the\nvideo script. In particular, to better capture the build-ups, we introduce\ncontent repetition as a complement to features introduced in theories of\ncomputational humor and visualize them in a context linking graph. To help\nusers locate the punchlines that have the desired features to learn, we\nsummarize the content (with keywords) and humor feature statistics on an\naugmented time matrix. With case studies on stand-up comedy shows and TED\ntalks, we show that DeHumor is able to highlight various building blocks of\nhumor examples. In addition, expert interviews with communication coaches and\nhumor researchers demonstrate the effectiveness of DeHumor for multimodal humor\nanalysis of speech content and vocal delivery.",
          "link": "http://arxiv.org/abs/2107.08356",
          "publishedOn": "2021-07-20T02:04:39.589Z",
          "wordCount": 689,
          "title": "DeHumor: Visual Analytics for Decomposing Humor. (arXiv:2107.08356v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arruda_H/0/1/0/all/0/1\">Henrique F. de Arruda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reia_S/0/1/0/all/0/1\">Sandro M. Reia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_F/0/1/0/all/0/1\">Filipi N. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amancio_D/0/1/0/all/0/1\">Diego R. Amancio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_L/0/1/0/all/0/1\">Luciano da F. Costa</a>",
          "description": "Poetry and prose are written artistic expressions that help us to appreciate\nthe reality we live. Each of these styles has its own set of subjective\nproperties, such as rhyme and rhythm, which are easily caught by a human\nreader's eye and ear. With the recent advances in artificial intelligence, the\ngap between humans and machines may have decreased, and today we observe\nalgorithms mastering tasks that were once exclusively performed by humans. In\nthis paper, we propose an automated method to distinguish between poetry and\nprose based solely on aural and rhythmic properties. In other to compare prose\nand poetry rhythms, we represent the rhymes and phones as temporal sequences\nand thus we propose a procedure for extracting rhythmic features from these\nsequences. The classification of the considered texts using the set of features\nextracted resulted in a best accuracy of 0.78, obtained with a neural network.\nInterestingly, by using an approach based on complex networks to visualize the\nsimilarities between the different texts considered, we found that the patterns\nof poetry vary much more than prose. Consequently, a much richer and complex\nset of rhythmic possibilities tends to be found in that modality.",
          "link": "http://arxiv.org/abs/2107.08512",
          "publishedOn": "2021-07-20T02:04:39.557Z",
          "wordCount": 646,
          "title": "A pattern recognition approach for distinguishing between prose and poetry. (arXiv:2107.08512v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Kishky_A/0/1/0/all/0/1\">Ahmed El-Kishky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1\">Benjamin I. P. Rubinstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>",
          "description": "Mistranslated numbers have the potential to cause serious effects, such as\nfinancial loss or medical misinformation. In this work we develop comprehensive\nassessments of the robustness of neural machine translation systems to\nnumerical text via behavioural testing. We explore a variety of numerical\ntranslation capabilities a system is expected to exhibit and design effective\ntest examples to expose system underperformance. We find that numerical\nmistranslation is a general issue: major commercial systems and\nstate-of-the-art research models fail on many of our test examples, for high-\nand low-resource languages. Our tests reveal novel errors that have not\npreviously been reported in NMT systems, to the best of our knowledge. Lastly,\nwe discuss strategies to mitigate numerical mistranslation.",
          "link": "http://arxiv.org/abs/2107.08357",
          "publishedOn": "2021-07-20T02:04:39.369Z",
          "wordCount": 577,
          "title": "As Easy as 1, 2, 3: Behavioural Testing of NMT Systems for Numerical Translation. (arXiv:2107.08357v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parry_H/0/1/0/all/0/1\">Hishan Parry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xun_L/0/1/0/all/0/1\">Lei Xun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_A/0/1/0/all/0/1\">Amin Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jia Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrett_G/0/1/0/all/0/1\">Geoff V. Merrett</a>",
          "description": "The Transformer architecture is widely used for machine translation tasks.\nHowever, its resource-intensive nature makes it challenging to implement on\nconstrained embedded devices, particularly where available hardware resources\ncan vary at run-time. We propose a dynamic machine translation model that\nscales the Transformer architecture based on the available resources at any\nparticular time. The proposed approach, 'Dynamic-HAT', uses a HAT\nSuperTransformer as the backbone to search for SubTransformers with different\naccuracy-latency trade-offs at design time. The optimal SubTransformers are\nsampled from the SuperTransformer at run-time, depending on latency\nconstraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses\ninherited SubTransformers sampled directly from the SuperTransformer with a\nswitching time of <1s. Using inherited SubTransformers results in a BLEU score\nloss of <1.5% because the SubTransformer configuration is not retrained from\nscratch after sampling. However, to recover this loss in performance, the\ndimensions of the design space can be reduced to tailor it to a family of\ntarget hardware. The new reduced design space results in a BLEU score increase\nof approximately 1% for sub-optimal models from the original design space, with\na wide range for performance scaling between 0.356s - 1.526s for the GPU and\n2.9s - 7.31s for the CPU.",
          "link": "http://arxiv.org/abs/2107.08199",
          "publishedOn": "2021-07-20T02:04:39.289Z",
          "wordCount": 651,
          "title": "Dynamic Transformer for Efficient Machine Translation on Embedded Devices. (arXiv:2107.08199v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chingacham_A/0/1/0/all/0/1\">Anupama Chingacham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1\">Vera Demberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>",
          "description": "Listening in noisy environments can be difficult even for individuals with a\nnormal hearing thresholds. The speech signal can be masked by noise, which may\nlead to word misperceptions on the side of the listener, and overall difficulty\nto understand the message. To mitigate hearing difficulties on listeners, a\nco-operative speaker utilizes voice modulation strategies like Lombard speech\nto generate noise-robust utterances, and similar solutions have been developed\nfor speech synthesis systems. In this work, we propose an alternate solution of\nchoosing noise-robust lexical paraphrases to represent an intended meaning. Our\nresults show that lexical paraphrases differ in their intelligibility in noise.\nWe evaluate the intelligibility of synonyms in context and find that choosing a\nlexical unit that is less risky to be misheard than its synonym introduced an\naverage gain in comprehension of 37% at SNR -5 dB and 21% at SNR 0 dB for\nbabble noise.",
          "link": "http://arxiv.org/abs/2107.08337",
          "publishedOn": "2021-07-20T02:04:39.266Z",
          "wordCount": 598,
          "title": "Exploring the Potential of Lexical Paraphrases for Mitigating Noise-Induced Comprehension Errors. (arXiv:2107.08337v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>",
          "description": "Previous studies have shown that initializing neural machine translation\n(NMT) models with the pre-trained language models (LM) can speed up the model\ntraining and boost the model performance. In this work, we identify a critical\nside-effect of pre-training for NMT, which is due to the discrepancy between\nthe training objectives of LM-based pre-training and NMT. Since the LM\nobjective learns to reconstruct a few source tokens and copy most of them, the\npre-training initialization would affect the copying behaviors of NMT models.\nWe provide a quantitative analysis of copying behaviors by introducing a metric\ncalled copying ratio, which empirically shows that pre-training based NMT\nmodels have a larger copying ratio than the standard one. In response to this\nproblem, we propose a simple and effective method named copying penalty to\ncontrol the copying behaviors in decoding. Extensive experiments on both\nin-domain and out-of-domain benchmarks show that the copying penalty method\nconsistently improves translation performance by controlling copying behaviors\nfor pre-training based NMT models. Source code is freely available at\nhttps://github.com/SunbowLiu/CopyingPenalty.",
          "link": "http://arxiv.org/abs/2107.08212",
          "publishedOn": "2021-07-20T02:04:39.238Z",
          "wordCount": 626,
          "title": "On the Copying Behaviors of Pre-Training for Neural Machine Translation. (arXiv:2107.08212v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jack Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1\">Raphael Lenain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1\">Udeepa Meepegama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1\">Emil Fristed</a>",
          "description": "We propose a method for learning de-identified prosody representations from\nraw audio using a contrastive self-supervised signal. Whereas prior work has\nrelied on conditioning models on bottlenecks, we introduce a set of inductive\nbiases that exploit the natural structure of prosody to minimize timbral\ninformation and decouple prosody from speaker representations. Despite\naggressive downsampling of the input and having no access to linguistic\ninformation, our model performs comparably to state-of-the-art speech\nrepresentations on DAMMP, a new benchmark we introduce for spoken language\nunderstanding. We use minimum description length probing to show that our\nrepresentations have selectively learned the subcomponents of non-timbral\nprosody, and that the product quantizer naturally disentangles them without\nusing bottlenecks. We derive an information-theoretic definition of speech\nde-identifiability and use it to demonstrate that our prosody representations\nare less identifiable than other speech representations.",
          "link": "http://arxiv.org/abs/2107.08248",
          "publishedOn": "2021-07-20T02:04:39.186Z",
          "wordCount": 611,
          "title": "Learning De-identified Representations of Prosody from Raw Audio. (arXiv:2107.08248v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pei_X/0/1/0/all/0/1\">Xin Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_D/0/1/0/all/0/1\">Deval Mehta</a>",
          "description": "Transcending the binary categorization of racist and xenophobic texts, this\nresearch takes cues from social science theories to develop a four dimensional\ncategory for racism and xenophobia detection, namely stigmatization,\noffensiveness, blame, and exclusion. With the aid of deep learning techniques,\nthis categorical detection enables insights into the nuances of emergent topics\nreflected in racist and xenophobic expression on Twitter. Moreover, a stage\nwise analysis is applied to capture the dynamic changes of the topics across\nthe stages of early development of Covid-19 from a domestic epidemic to an\ninternational public health emergency, and later to a global pandemic. The main\ncontributions of this research include, first the methodological advancement.\nBy bridging the state-of-the-art computational methods with social science\nperspective, this research provides a meaningful approach for future research\nto gain insight into the underlying subtlety of racist and xenophobic\ndiscussion on digital platforms. Second, by enabling a more accurate\ncomprehension and even prediction of public opinions and actions, this research\npaves the way for the enactment of effective intervention policies to combat\nracist crimes and social exclusion under Covid-19.",
          "link": "http://arxiv.org/abs/2107.08347",
          "publishedOn": "2021-07-20T02:04:39.148Z",
          "wordCount": 688,
          "title": "Beyond a binary of (non)racist tweets: A four-dimensional categorical detection and analysis of racist and xenophobic opinions on Twitter in early Covid-19. (arXiv:2107.08347v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wadden_D/0/1/0/all/0/1\">David Wadden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>",
          "description": "We present an overview of the SciVer shared task, presented at the 2nd\nScholarly Document Processing (SDP) workshop at NAACL 2021. In this shared\ntask, systems were provided a scientific claim and a corpus of research\nabstracts, and asked to identify which articles SUPPORT or REFUTE the claim as\nwell as provide evidentiary sentences justifying those labels. 11 teams made a\ntotal of 14 submissions to the shared task leaderboard, leading to an\nimprovement of more than +23 F1 on the primary task evaluation metric. In\naddition to surveying the participating systems, we provide several insights\ninto modeling approaches to support continued progress and future research on\nthe important and challenging task of scientific claim verification.",
          "link": "http://arxiv.org/abs/2107.08188",
          "publishedOn": "2021-07-20T02:04:39.083Z",
          "wordCount": 567,
          "title": "Overview and Insights from the SciVer Shared Task on Scientific Claim Verification. (arXiv:2107.08188v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08146",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1\">Peter Jansen</a>",
          "description": "Tamarian, a fictional language introduced in the Star Trek episode Darmok,\ncommunicates meaning through utterances of metaphorical references, such as\n\"Darmok and Jalad at Tanagra\" instead of \"We should work together.\" This work\nassembles a Tamarian-English dictionary of utterances from the original episode\nand several follow-on novels, and uses this to construct a parallel corpus of\n456 English-Tamarian utterances. A machine translation system based on a large\nlanguage model (T5) is trained using this parallel corpus, and is shown to\nproduce an accuracy of 76% when translating from English to Tamarian on known\nutterances.",
          "link": "http://arxiv.org/abs/2107.08146",
          "publishedOn": "2021-07-20T02:04:39.061Z",
          "wordCount": 527,
          "title": "Darmok and Jalad at Tanagra: A Dataset and Model for English-to-Tamarian Translation. (arXiv:2107.08146v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wysocki_O/0/1/0/all/0/1\">Oskar Wysocki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florea_M/0/1/0/all/0/1\">Malina Florea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1\">Donal Landers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andre Freitas</a>",
          "description": "This paper proposes a novel statistical corpus analysis framework targeted\ntowards the interpretation of Natural Language Processing (NLP) architectural\npatterns at scale. The proposed approach combines saturation-based lexicon\nconstruction, statistical corpus analysis methods and graph collocations to\ninduce a synthesis representation of NLP architectural patterns from corpora.\nThe framework is validated in the full corpus of Semeval tasks and demonstrated\ncoherent architectural patterns which can be used to answer architectural\nquestions on a data-driven fashion, providing a systematic mechanism to\ninterpret a largely dynamic and exponentially growing field.",
          "link": "http://arxiv.org/abs/2107.08124",
          "publishedOn": "2021-07-20T02:04:39.034Z",
          "wordCount": 546,
          "title": "Architectures of Meaning, A Systematic Corpus Analysis of NLP Systems. (arXiv:2107.08124v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2105.04067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1\">Sarah Erfani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_J/0/1/0/all/0/1\">Junhao Gan</a>",
          "description": "User and item attributes are essential side-information; their interactions\n(i.e., their co-occurrence in the sample data) can significantly enhance\nprediction accuracy in various recommender systems. We identify two different\ntypes of attribute interactions, inner interactions and cross interactions:\ninner interactions are those between only user attributes or those between only\nitem attributes; cross interactions are those between user attributes and item\nattributes. Existing models do not distinguish these two types of attribute\ninteractions, which may not be the most effective way to exploit the\ninformation carried by the interactions. To address this drawback, we propose a\nneural Graph Matching based Collaborative Filtering model (GMCF), which\neffectively captures the two types of attribute interactions through modeling\nand aggregating attribute interactions in a graph matching structure for\nrecommendation. In our model, the two essential recommendation procedures,\ncharacteristic learning and preference matching, are explicitly conducted\nthrough graph learning (based on inner interactions) and node matching (based\non cross interactions), respectively. Experimental results show that our model\noutperforms state-of-the-art models. Further studies verify the effectiveness\nof GMCF in improving the accuracy of recommendation.",
          "link": "http://arxiv.org/abs/2105.04067",
          "publishedOn": "2021-07-26T02:00:57.053Z",
          "wordCount": 648,
          "title": "Neural Graph Matching based Collaborative Filtering. (arXiv:2105.04067v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Axel_M/0/1/0/all/0/1\">Marmoret Axel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nancy_B/0/1/0/all/0/1\">Bertin Nancy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeremy_C/0/1/0/all/0/1\">Cohen Jeremy</a>",
          "description": "Music is an art, perceived in unique ways by every listener, coming from\nacoustic signals. In the meantime, standards as musical scores exist to\ndescribe it. Even if humans can make this transcription, it is costly in terms\nof time and efforts, even more with the explosion of information consecutively\nto the rise of the Internet. In that sense, researches are driven in the\ndirection of Automatic Music Transcription. While this task is considered\nsolved in the case of single notes, it is still open when notes superpose\nthemselves, forming chords. This report aims at developing some of the existing\ntechniques towards Music Transcription, particularly matrix factorization, and\nintroducing the concept of multi-channel automatic music transcription. This\nconcept will be explored with mathematical objects called tensors.",
          "link": "http://arxiv.org/abs/2107.11250",
          "publishedOn": "2021-07-26T02:00:57.033Z",
          "wordCount": 581,
          "title": "Multi-Channel Automatic Music Transcription Using Tensor Algebra. (arXiv:2107.11250v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stray_J/0/1/0/all/0/1\">Jonathan Stray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vendrov_I/0/1/0/all/0/1\">Ivan Vendrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nixon_J/0/1/0/all/0/1\">Jeremy Nixon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_S/0/1/0/all/0/1\">Steven Adler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1\">Dylan Hadfield-Menell</a>",
          "description": "We describe cases where real recommender systems were modified in the service\nof various human values such as diversity, fairness, well-being, time well\nspent, and factual accuracy. From this we identify the current practice of\nvalues engineering: the creation of classifiers from human-created data with\nvalue-based labels. This has worked in practice for a variety of issues, but\nproblems are addressed one at a time, and users and other stakeholders have\nseldom been involved. Instead, we look to AI alignment work for approaches that\ncould learn complex values directly from stakeholders, and identify four major\ndirections: useful measures of alignment, participatory design and operation,\ninteractive value learning, and informed deliberative judgments.",
          "link": "http://arxiv.org/abs/2107.10939",
          "publishedOn": "2021-07-26T02:00:56.914Z",
          "wordCount": 571,
          "title": "What are you optimizing for? Aligning Recommender Systems with Human Values. (arXiv:2107.10939v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oncescu_A/0/1/0/all/0/1\">Andreea-Maria Oncescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koepke_A/0/1/0/all/0/1\">A. Sophia Koepke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1\">Jo&#xe3;o F. Henriques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>",
          "description": "We consider the task of retrieving audio using free-form natural language\nqueries. To study this problem, which has received limited attention in the\nexisting literature, we introduce challenging new benchmarks for text-based\naudio retrieval using text annotations sourced from the Audiocaps and Clotho\ndatasets. We then employ these benchmarks to establish baselines for\ncross-modal audio retrieval, where we demonstrate the benefits of pre-training\non diverse audio tasks. We hope that our benchmarks will inspire further\nresearch into cross-modal text-based audio retrieval with free-form text\nqueries.",
          "link": "http://arxiv.org/abs/2105.02192",
          "publishedOn": "2021-07-23T02:00:29.761Z",
          "wordCount": 561,
          "title": "Audio Retrieval with Natural Language Queries. (arXiv:2105.02192v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07374",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeon_Y/0/1/0/all/0/1\">Yeseul Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_D/0/1/0/all/0/1\">Dongjun Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jina Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_I/0/1/0/all/0/1\">Ick Hoon Jin</a>",
          "description": "Since the emergence of the worldwide pandemic of COVID-19, relevant research\nhas been published at a dazzling pace, which makes it hard to follow the\nresearch in this area without dedicated efforts. It is practically impossible\nto implement this task manually due to the high volume of the relevant\nliterature. Text mining has been considered to be a powerful approach to\naddress this challenge, especially the topic modeling, a well-known\nunsupervised method that aims to reveal latent topics from the literature.\nHowever, in spite of its potential utility, the results generated from this\napproach are often investigated manually. Hence, its application to the\nCOVID-19 literature is not straightforward and expert knowledge is needed to\nmake meaningful interpretations. In order to address these challenges, we\npropose a novel analytical framework for estimating topic interactions and\neffective visualization for topic interpretation. Here we assumed that topics\nconstituting a paper can be positioned on an interaction map, which belongs to\na high-dimensional Euclidean space. Based on this assumption, after summarizing\ntopics with their topic-word distributions using the biterm topic model, we\nmapped these latent topics on networks to visualize relationships among the\ntopics. Moreover, in the proposed approach, the change of relationships among\ntopics can be traced using a trajectory plot generated with different levels of\nword richness. These results together provide deeply mined and intuitive\nrepresentation of relationships among topics related to a specific research\narea. The application of this proposed framework to the PubMed literature shows\nthat our approach facilitates understanding of the topics constituting the\nCOVID-19 knowledge.",
          "link": "http://arxiv.org/abs/2106.07374",
          "publishedOn": "2021-07-23T02:00:29.433Z",
          "wordCount": 770,
          "title": "Network-based Trajectory Topic Interaction Map for Text Mining of COVID-19 Biomedical Literature. (arXiv:2106.07374v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1\">Marc Najork</a>",
          "description": "When experiencing an information need, users want to engage with a domain\nexpert, but often turn to an information retrieval system, such as a search\nengine, instead. Classical information retrieval systems do not answer\ninformation needs directly, but instead provide references to (hopefully\nauthoritative) answers. Successful question answering systems offer a limited\ncorpus created on-demand by human experts, which is neither timely nor\nscalable. Pre-trained language models, by contrast, are capable of directly\ngenerating prose that may be responsive to an information need, but at present\nthey are dilettantes rather than domain experts -- they do not have a true\nunderstanding of the world, they are prone to hallucinating, and crucially they\nare incapable of justifying their utterances by referring to supporting\ndocuments in the corpus they were trained over. This paper examines how ideas\nfrom classical information retrieval and pre-trained language models can be\nsynthesized and evolved into systems that truly deliver on the promise of\ndomain expert advice.",
          "link": "http://arxiv.org/abs/2105.02274",
          "publishedOn": "2021-07-23T02:00:29.401Z",
          "wordCount": 636,
          "title": "Rethinking Search: Making Domain Experts out of Dilettantes. (arXiv:2105.02274v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.05516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">Doris E. M. Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadendla_V/0/1/0/all/0/1\">Venkata Sriram Siddhardh Nadendla</a>",
          "description": "Strategic information design is a framework where a sender designs\ninformation strategically to steer its receiver's decision towards a desired\nchoice. Traditionally, such frameworks have always assumed that the sender and\nthe receiver comprehends the state of the choice environment, and that the\nreceiver always trusts the sender's signal. This paper deviates from these\nassumptions and re-investigates strategic information design in the presence of\ndistrustful receiver and when both sender and receiver cannot\nobserve/comprehend the environment state space. Specifically, we assume that\nboth sender and receiver has access to non-identical beliefs about choice\nrewards (with sender's belief being more accurate), but not the environment\nstate that determines these rewards. Furthermore, given that the receiver does\nnot trust the sender, we also assume that the receiver updates its prior in a\nnon-Bayesian manner. We evaluate the Stackelberg equilibrium and investigate\neffects of information framing (i.e. send complete signal, or just expected\nvalue of the signal) on the equilibrium. Furthermore, we also investigate trust\ndynamics at the receiver, under the assumption that the receiver minimizes\nregret in hindsight. Simulation results are presented to illustrate signaling\neffects and trust dynamics in strategic information design.",
          "link": "http://arxiv.org/abs/2005.05516",
          "publishedOn": "2021-07-23T02:00:29.373Z",
          "wordCount": 709,
          "title": "Framing Effects on Strategic Information Design under Receiver Distrust and Unknown State. (arXiv:2005.05516v2 [cs.GT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10457",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Min Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Junliang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zongwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kecheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wange_X/0/1/0/all/0/1\">Xu Wange</a>",
          "description": "To explore the robustness of recommender systems, researchers have proposed\nvarious shilling attack models and analyzed their adverse effects. Primitive\nattacks are highly feasible but less effective due to simplistic handcrafted\nrules, while upgraded attacks are more powerful but costly and difficult to\ndeploy because they require more knowledge from recommendations. In this paper,\nwe explore a novel shilling attack called Graph cOnvolution-based generative\nshilling ATtack (GOAT) to balance the attacks' feasibility and effectiveness.\nGOAT adopts the primitive attacks' paradigm that assigns items for fake users\nby sampling and the upgraded attacks' paradigm that generates fake ratings by a\ndeep learning-based model. It deploys a generative adversarial network (GAN)\nthat learns the real rating distribution to generate fake ratings.\nAdditionally, the generator combines a tailored graph convolution structure\nthat leverages the correlations between co-rated items to smoothen the fake\nratings and enhance their authenticity. The extensive experiments on two public\ndatasets evaluate GOAT's performance from multiple perspectives. Our study of\nthe GOAT demonstrates technical feasibility for building a more powerful and\nintelligent attack model with a much-reduced cost, enables analysis the threat\nof such an attack and guides for investigating necessary prevention measures.",
          "link": "http://arxiv.org/abs/2107.10457",
          "publishedOn": "2021-07-23T02:00:29.336Z",
          "wordCount": 663,
          "title": "Ready for Emerging Threats to Recommender Systems? A Graph Convolution-based Generative Shilling Attack. (arXiv:2107.10457v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.09036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanqiao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>",
          "description": "Multimedia content is of predominance in the modern Web era. Investigating\nhow users interact with multimodal items is a continuing concern within the\nrapid development of recommender systems. The majority of previous work focuses\non modeling user-item interactions with multimodal features included as side\ninformation. However, this scheme is not well-designed for multimedia\nrecommendation. Specifically, only collaborative item-item relationships are\nimplicitly modeled through high-order item-user-item relations. Considering\nthat items are associated with rich contents in multiple modalities, we argue\nthat the latent semantic item-item structures underlying these multimodal\ncontents could be beneficial for learning better item representations and\nfurther boosting recommendation. To this end, we propose a LATent sTructure\nmining method for multImodal reCommEndation, which we term LATTICE for brevity.\nTo be specific, in the proposed LATTICE model, we devise a novel modality-aware\nstructure learning layer, which learns item-item structures for each modality\nand aggregates multiple modalities to obtain latent item graphs. Based on the\nlearned latent graphs, we perform graph convolutions to explicitly inject\nhigh-order item affinities into item representations. These enriched item\nrepresentations can then be plugged into existing collaborative filtering\nmethods to make more accurate recommendations. Extensive experiments on three\nreal-world datasets demonstrate the superiority of our method over\nstate-of-the-art multimedia recommendation methods and validate the efficacy of\nmining latent item-item relationships from multimodal features.",
          "link": "http://arxiv.org/abs/2104.09036",
          "publishedOn": "2021-07-22T02:03:10.377Z",
          "wordCount": 695,
          "title": "Mining Latent Structures for Multimedia Recommendation. (arXiv:2104.09036v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanxiong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>",
          "description": "Recommender systems are gaining increasing and critical impacts on human and\nsociety since a growing number of users use them for information seeking and\ndecision making. Therefore, it is crucial to address the potential unfairness\nproblems in recommendations. Just like users have personalized preferences on\nitems, users' demands for fairness are also personalized in many scenarios.\nTherefore, it is important to provide personalized fair recommendations for\nusers to satisfy their personalized fairness demands. Besides, previous works\non fair recommendation mainly focus on association-based fairness. However, it\nis important to advance from associative fairness notions to causal fairness\nnotions for assessing fairness more properly in recommender systems. Based on\nthe above considerations, this paper focuses on achieving personalized\ncounterfactual fairness for users in recommender systems. To this end, we\nintroduce a framework for achieving counterfactually fair recommendations\nthrough adversary learning by generating feature-independent user embeddings\nfor recommendation. The framework allows recommender systems to achieve\npersonalized fairness for users while also covering non-personalized\nsituations. Experiments on two real-world datasets with shallow and deep\nrecommendation algorithms show that our method can generate fairer\nrecommendations for users with a desirable recommendation performance.",
          "link": "http://arxiv.org/abs/2105.09829",
          "publishedOn": "2021-07-22T02:03:10.199Z",
          "wordCount": 659,
          "title": "Personalized Counterfactual Fairness in Recommendation. (arXiv:2105.09829v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fischbach_J/0/1/0/all/0/1\">Jannik Fischbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Springer_T/0/1/0/all/0/1\">Tobias Springer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frattini_J/0/1/0/all/0/1\">Julian Frattini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Femmer_H/0/1/0/all/0/1\">Henning Femmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelsang_A/0/1/0/all/0/1\">Andreas Vogelsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_D/0/1/0/all/0/1\">Daniel Mendez</a>",
          "description": "[Context:] Causal relations (e.g., If A, then B) are prevalent in functional\nrequirements. For various applications of AI4RE, e.g., the automatic derivation\nof suitable test cases from requirements, automatically extracting such causal\nstatements are a basic necessity. [Problem:] We lack an approach that is able\nto extract causal relations from natural language requirements in fine-grained\nform. Specifically, existing approaches do not consider the combinatorics\nbetween causes and effects. They also do not allow to split causes and effects\ninto more granular text fragments (e.g., variable and condition), making the\nextracted relations unsuitable for automatic test case derivation. [Objective &\nContributions:] We address this research gap and make the following\ncontributions: First, we present the Causality Treebank, which is the first\ncorpus of fully labeled binary parse trees representing the composition of\n1,571 causal requirements. Second, we propose a fine-grained causality\nextractor based on Recursive Neural Tensor Networks. Our approach is capable of\nrecovering the composition of causal statements written in natural language and\nachieves a F1 score of 74 % in the evaluation on the Causality Treebank. Third,\nwe disclose our open data sets as well as our code to foster the discourse on\nthe automatic extraction of causality in the RE community.",
          "link": "http://arxiv.org/abs/2107.09980",
          "publishedOn": "2021-07-22T02:03:10.035Z",
          "wordCount": 651,
          "title": "Fine-Grained Causality Extraction From Natural Language Requirements Using Recursive Neural Tensor Networks. (arXiv:2107.09980v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10023",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jadallah_N/0/1/0/all/0/1\">Noah Jadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischbach_J/0/1/0/all/0/1\">Jannik Fischbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frattini_J/0/1/0/all/0/1\">Julian Frattini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelsang_A/0/1/0/all/0/1\">Andreas Vogelsang</a>",
          "description": "Causal relations (If A, then B) are prevalent in requirements artifacts.\nAutomatically extracting causal relations from requirements holds great\npotential for various RE activities (e.g., automatic derivation of suitable\ntest cases). However, we lack an approach capable of extracting causal\nrelations from natural language with reasonable performance. In this paper, we\npresent our tool CATE (CAusality Tree Extractor), which is able to parse the\ncomposition of a causal relation as a tree structure. CATE does not only\nprovide an overview of causes and effects in a sentence, but also reveals their\nsemantic coherence by translating the causal relation into a binary tree. We\nencourage fellow researchers and practitioners to use CATE at\nhttps://causalitytreeextractor.com/",
          "link": "http://arxiv.org/abs/2107.10023",
          "publishedOn": "2021-07-22T02:03:09.971Z",
          "wordCount": 551,
          "title": "CATE: CAusality Tree Extractor from Natural Language Requirements. (arXiv:2107.10023v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jarwar_M/0/1/0/all/0/1\">Muhammad Aslam Jarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chapman_A/0/1/0/all/0/1\">Adriane Chapman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliot_M/0/1/0/all/0/1\">Mark Elliot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raji_F/0/1/0/all/0/1\">Fatemeh Raji</a>",
          "description": "The Anonymisation Decision-making Framework (ADF) operationalizes the risk\nmanagement of data exchange between organizations, referred to as \"data\nenvironments\". The second edition of ADF has increased its emphasis on modeling\ndata flows, highlighting a potential new use of provenance information to\nsupport anonymisation decision-making. In this paper, we provide a use case\nthat showcases this functionality more. Based on this use case, we identify how\nprovenance information could be utilized within the ADF framework, and identify\na currently un-met requirement which is the modeling of \\textit{data\nenvironments}. We show how data environments can be implemented within the W3C\nPROV in four different ways. We analyze the costs and benefits of each\napproach, and consider another use case as a partial check for completeness. We\nthen summarize our findings and suggest ways forward.",
          "link": "http://arxiv.org/abs/2107.09966",
          "publishedOn": "2021-07-22T02:03:09.935Z",
          "wordCount": 566,
          "title": "Provenance, Anonymisation and Data Environments: a Unifying Construction. (arXiv:2107.09966v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hieu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1\">Son Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_I/0/1/0/all/0/1\">I-Ling Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastani_F/0/1/0/all/0/1\">Farokh Bastani</a>",
          "description": "In this paper, we consider the IoT data discovery data objects to specific\nnodes in the network. They are very problem in very large and growing scale\nnetworks. Specifically, we investigate in depth the routing table summarization\ntechniques to support effective and space-efficient IoT data discovery routing.\nNovel summarization algorithms, including alphabetical based, hash based, and\nmeaning based summarization and their corresponding coding schemes are\nproposed. The issue of potentially misleading routing due to summarization is\nalso investigated. Subsequently, we analyze the strategy of when to summarize\nin order to balance the tradeoff especially in handling MAA based lookups.\nbetween the routing table compression rate and the chance of Unstructured\ndiscovery routing approaches, such as [4] [5], causing misleading routing. For\nexperimental study, we have collected 100K IoT data streams from various IoT\ndatabases as the input dataset. Experimental results show that our\nsummarization solution can reduce the routing table size by 20 to 30 folds with\n2-5% increase in latency when compared with similar peer-to-peer discovery\nrouting algorithms without summarization. Also, our approach outperforms DHT\nbased approaches by 2 to 6 folds in terms of latency and traffic.",
          "link": "http://arxiv.org/abs/2107.09558",
          "publishedOn": "2021-07-21T02:01:33.669Z",
          "wordCount": 658,
          "title": "Into Summarization Techniques for IoT Data Discovery Routing. (arXiv:2107.09558v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiruo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ran Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhiyi Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>",
          "description": "Tables are widely used with various structures to organize and present data.\nRecent attempts on table understanding mainly focus on relational tables, yet\noverlook to other common table structures. In this paper, we propose TUTA, a\nunified pre-training architecture for understanding generally structured\ntables. Noticing that understanding a table requires spatial, hierarchical, and\nsemantic information, we enhance transformers with three novel structure-aware\nmechanisms. First, we devise a unified tree-based structure, called a\nbi-dimensional coordinate tree, to describe both the spatial and hierarchical\ninformation of generally structured tables. Upon this, we propose tree-based\nattention and position embedding to better capture the spatial and hierarchical\ninformation. Moreover, we devise three progressive pre-training objectives to\nenable representations at the token, cell, and table levels. We pre-train TUTA\non a wide range of unlabeled web and spreadsheet tables and fine-tune it on two\ncritical tasks in the field of table structure understanding: cell type\nclassification and table type classification. Experiments show that TUTA is\nhighly effective, achieving state-of-the-art on five widely-studied datasets.",
          "link": "http://arxiv.org/abs/2010.12537",
          "publishedOn": "2021-07-21T02:01:32.983Z",
          "wordCount": 662,
          "title": "TUTA: Tree-based Transformers for Generally Structured Table Pre-training. (arXiv:2010.12537v4 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amato_D/0/1/0/all/0/1\">Domenico Amato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giancarlo_R/0/1/0/all/0/1\">Raffaele Giancarlo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosco_G/0/1/0/all/0/1\">Giosu&#xe8; Lo Bosco</a>",
          "description": "Sorted Table Search Procedures are the quintessential query-answering tool,\nstill very useful, e.g, Search Engines (Google Chrome). Speeding them up, in\nsmall additional space with respect to the table being searched into, is still\na quite significant achievement. Static Learned Indexes have been very\nsuccessful in achieving such a speed-up, but leave open a major question: To\nwhat extent one can enjoy the speed-up of Learned Indexes while using constant\nor nearly constant additional space. By generalizing the experimental\nmethodology of a recent benchmarking study on Learned Indexes, we shed light on\nthis question, by considering two scenarios. The first, quite elementary, i.e.,\ntextbook code, and the second using advanced Learned Indexing algorithms and\nthe supporting sophisticated software platforms. Although in both cases one\nwould expect a positive answer, its achievement is not as simple as it seems.\nIndeed, our extensive set of experiments reveal a complex relationship between\nquery time and model space. The findings regarding this relationship and the\ncorresponding quantitative estimates, across memory levels, can be of interest\nto algorithm designers and of use to practitioners as well. As an essential\npart of our research, we introduce two new models that are of interest in their\nown right. The first is a constant space model that can be seen as a\ngeneralization of $k$-ary search, while the second is a synoptic {\\bf RMI}, in\nwhich we can control model space usage.",
          "link": "http://arxiv.org/abs/2107.09480",
          "publishedOn": "2021-07-21T02:01:32.669Z",
          "wordCount": 696,
          "title": "Learned Sorted Table Search and Static Indexes in Small Space: Methodological and Practical Insights via an Experimental Study. (arXiv:2107.09480v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.11783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanci Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1\">Tianming Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yujie Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donohue_L/0/1/0/all/0/1\">Lawrence Donohue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_R/0/1/0/all/0/1\">Rui Dai</a>",
          "description": "The quarterly financial statement, or Form 10-Q, is one of the most\nfrequently required filings for US public companies to disclose financial and\nother important business information. Due to the massive volume of 10-Q filings\nand the enormous variations in the reporting format, it has been a\nlong-standing challenge to retrieve item-specific information from 10-Q filings\nthat lack machine-readable hierarchy. This paper presents a solution for\nitemizing 10-Q files by complementing a rule-based algorithm with a\nConvolutional Neural Network (CNN) image classifier. This solution demonstrates\na pipeline that can be generalized to a rapid data retrieval solution among a\nlarge volume of textual data using only typographic items. The extracted\ntextual data can be used as unlabeled content-specific data to train\ntransformer models (e.g., BERT) or fit into various field-focus natural\nlanguage processing (NLP) applications.",
          "link": "http://arxiv.org/abs/2104.11783",
          "publishedOn": "2021-07-20T02:04:39.008Z",
          "wordCount": 601,
          "title": "Form 10-Q Itemization. (arXiv:2104.11783v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_N/0/1/0/all/0/1\">Niloofar Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouvelas_N/0/1/0/all/0/1\">Nikolaos Kouvelas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_R/0/1/0/all/0/1\">R Venkatesha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucani_D/0/1/0/all/0/1\">Daniel E. Lucani</a>",
          "description": "High frame-corruption is widely observed in Long Range Wide Area Networks\n(LoRaWAN) due to the coexistence with other networks in ISM bands and an\nAloha-like MAC layer. LoRa's Forward Error Correction (FEC) mechanism is often\ninsufficient to retrieve corrupted data. In fact, real-life measurements show\nthat at least one-fourth of received transmissions are corrupted. When more\nframes are dropped, LoRa nodes usually switch over to higher spreading factors\n(SF), thus increasing transmission times and increasing the required energy.\nThis paper introduces ReDCoS, a novel coding technique at the application layer\nthat improves recovery of corrupted LoRa frames, thus reducing the overall\ntransmission time and energy invested by LoRa nodes by several-fold. ReDCoS\nutilizes lightweight coding techniques to pre-encode the transmitted data.\nTherefore, the inbuilt Cyclic Redundancy Check (CRC) that follows is computed\nbased on an already encoded data. At the receiver, we use both the CRC and the\ncoded data to recover data from a corrupted frame beyond the built-in Error\nCorrecting Code (ECC). We compare the performance of ReDCoS to (I) the standard\nFEC of vanilla-LoRaWAN, and to (ii) RS coding applied as ECC to the data of\nLoRaWAN. The results indicated a 54x and 13.5x improvement of decoding ratio,\nrespectively, when 20 data symbols were sent. Furthermore, we evaluated ReDCoS\non-field using LoRa SX1261 transceivers showing that it outperformed RS-coding\nby factor of at least 2x (and up to 6x) in terms of the decoding ratio while\nconsuming 38.5% less energy per correctly received transmission.",
          "link": "http://arxiv.org/abs/2107.08868",
          "publishedOn": "2021-07-20T02:04:38.982Z",
          "wordCount": 688,
          "title": "Energy Efficient Data Recovery from Corrupted LoRa Frames. (arXiv:2107.08868v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mathew_M/0/1/0/all/0/1\">Minesh Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1\">Ruben Tito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1\">Dimosthenis Karatzas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1\">R. Manmatha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C.V. Jawahar</a>",
          "description": "This paper presents results of Document Visual Question Answering Challenge\norganized as part of \"Text and Documents in the Deep Learning Era\" workshop, in\nCVPR 2020. The challenge introduces a new problem - Visual Question Answering\non document images. The challenge comprised two tasks. The first task concerns\nwith asking questions on a single document image. On the other hand, the second\ntask is set as a retrieval task where the question is posed over a collection\nof images. For the task 1 a new dataset is introduced comprising 50,000\nquestions-answer(s) pairs defined over 12,767 document images. For task 2\nanother dataset has been created comprising 20 questions over 14,362 document\nimages which share the same document template.",
          "link": "http://arxiv.org/abs/2008.08899",
          "publishedOn": "2021-07-20T02:04:38.396Z",
          "wordCount": 597,
          "title": "Document Visual Question Answering Challenge 2020. (arXiv:2008.08899v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pourkamali_F/0/1/0/all/0/1\">Farzad Pourkamali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macris_N/0/1/0/all/0/1\">Nicolas Macris</a>",
          "description": "We consider the estimation of an n-dimensional vector s from the noisy\nelement-wise measurements of $\\mathbf{s}\\mathbf{s}^T$, a generic problem that\narises in statistics and machine learning. We study a mismatched Bayesian\ninference setting, where some of the parameters are not known to the\nstatistician. We derive the full exact analytic expression of the asymptotic\nmean squared error (MSE) in the large system size limit for the particular case\nof Gaussian priors and additive noise. From our formulas, we see that\nestimation is still possible in the mismatched case; and also that the minimum\nMSE (MMSE) can be achieved if the statistician chooses suitable parameters. Our\ntechnique relies on the asymptotics of the spherical integrals and can be\napplied as long as the statistician chooses a rotationally invariant prior.",
          "link": "http://arxiv.org/abs/2107.08927",
          "publishedOn": "2021-07-20T02:04:38.368Z",
          "wordCount": 562,
          "title": "Mismatched Estimation of rank-one symmetric matrices under Gaussian noise. (arXiv:2107.08927v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Feiyang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1\">Xiang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yanrong Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_A/0/1/0/all/0/1\">Ao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "The exploration/exploitation (E&E) dilemma lies at the core of interactive\nsystems such as online advertising, for which contextual bandit algorithms have\nbeen proposed. Bayesian approaches provide guided exploration with principled\nuncertainty estimation, but the applicability is often limited due to\nover-simplified assumptions. Non-Bayesian bootstrap methods, on the other hand,\ncan apply to complex problems by using deep reward models, but lacks clear\nguidance to the exploration behavior. It still remains largely unsolved to\ndevelop a practical method for complex deep contextual bandits.\n\nIn this paper, we introduce Guided Bootstrap (GuideBoot for short), combining\nthe best of both worlds. GuideBoot provides explicit guidance to the\nexploration behavior by training multiple models over both real samples and\nnoisy samples with fake labels, where the noise is added according to the\npredictive uncertainty. The proposed method is efficient as it can make\ndecisions on-the-fly by utilizing only one randomly chosen model, but is also\neffective as we show that it can be viewed as a non-Bayesian approximation of\nThompson sampling. Moreover, we extend it to an online version that can learn\nsolely from streaming data, which is favored in real applications. Extensive\nexperiments on both synthetic task and large-scale advertising environments\nshow that GuideBoot achieves significant improvements against previous\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.08383",
          "publishedOn": "2021-07-20T02:04:38.289Z",
          "wordCount": 649,
          "title": "GuideBoot: Guided Bootstrap for Deep Contextual Bandits. (arXiv:2107.08383v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_D/0/1/0/all/0/1\">Divya Shanmugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabanian_S/0/1/0/all/0/1\">Samira Shabanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finck_M/0/1/0/all/0/1\">Mich&#xe8;le Finck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biega_A/0/1/0/all/0/1\">Asia Biega</a>",
          "description": "Data minimization is a legal obligation defined in the European Union's\nGeneral Data Protection Regulation (GDPR) as the responsibility to process an\nadequate, relevant, and limited amount of personal data in relation to a\nprocessing purpose. However, unlike fairness or transparency, the principle has\nnot seen wide adoption for machine learning systems due to a lack of\ncomputational interpretation. In this paper, we build on literature in machine\nlearning and law to propose the first learning framework for limiting data\ncollection based on an interpretation that ties the data collection purpose to\nsystem performance. We formalize a data minimization criterion based on\nperformance curve derivatives and provide an effective and interpretable\npiecewise power law technique that models distinct stages of an algorithm's\nperformance throughout data collection. Results from our empirical\ninvestigation offer deeper insights into the relevant considerations when\ndesigning a data minimization framework, including the choice of feature\nacquisition algorithm, initialization conditions, as well as impacts on\nindividuals that hint at tensions between data minimization and fairness.",
          "link": "http://arxiv.org/abs/2107.08096",
          "publishedOn": "2021-07-20T02:04:38.147Z",
          "wordCount": 618,
          "title": "Learning to Limit Data Collection via Scaling Laws: Data Minimization Compliance in Practice. (arXiv:2107.08096v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_L/0/1/0/all/0/1\">Lakshya Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Sagnik Sarkar</a>",
          "description": "Typical e-commerce platforms contain millions of products in the catalog.\nUsers visit these platforms and enter search queries to retrieve their desired\nproducts. Therefore, showing the relevant products at the top is essential for\nthe success of e-commerce platforms. We approach this problem by learning low\ndimension representations for queries and product descriptions by leveraging\nuser click-stream data as our main source of signal for product relevance.\nStarting from GRU-based architectures as our baseline model, we move towards a\nmore advanced transformer-based architecture. This helps the model to learn\ncontextual representations of queries and products to serve better search\nresults and understand the user intent in an efficient manner. We perform\nexperiments related to pre-training of the Transformer based RoBERTa model\nusing a fashion corpus and fine-tuning it over the triplet loss. Our\nexperiments on the product ranking task show that the RoBERTa model is able to\ngive an improvement of 7.8% in Mean Reciprocal Rank(MRR), 15.8% in Mean Average\nPrecision(MAP) and 8.8% in Normalized Discounted Cumulative Gain(NDCG), thus\noutperforming our GRU based baselines. For the product retrieval task, RoBERTa\nmodel is able to outperform other two models with an improvement of 164.7% in\nPrecision@50 and 145.3% in Recall@50. In order to highlight the importance of\npre-training RoBERTa for fashion domain, we qualitatively compare already\npre-trained RoBERTa on standard datasets with our custom pre-trained RoBERTa\nover a fashion corpus for the query token prediction task. Finally, we also\nshow a qualitative comparison between GRU and RoBERTa results for product\nretrieval task for some test queries.",
          "link": "http://arxiv.org/abs/2107.08291",
          "publishedOn": "2021-07-20T02:04:38.103Z",
          "wordCount": 692,
          "title": "Neural Search: Learning Query and Product Representations in Fashion E-commerce. (arXiv:2107.08291v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yinqiong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yixing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "Similar question retrieval is a core task in community-based question\nanswering (CQA) services. To balance the effectiveness and efficiency, the\nquestion retrieval system is typically implemented as multi-stage rankers: The\nfirst-stage ranker aims to recall potentially relevant questions from a large\nrepository, and the latter stages attempt to re-rank the retrieved results.\nMost existing works on question retrieval mainly focused on the re-ranking\nstages, leaving the first-stage ranker to some traditional term-based methods.\nHowever, term-based methods often suffer from the vocabulary mismatch problem,\nespecially on short texts, which may block the re-rankers from relevant\nquestions at the very beginning. An alternative is to employ embedding-based\nmethods for the first-stage ranker, which compress texts into dense vectors to\nenhance the semantic matching. However, these methods often lose the\ndiscriminative power as term-based methods, thus introduce noise during\nretrieval and hurt the recall performance. In this work, we aim to tackle the\ndilemma of the first-stage ranker, and propose a discriminative semantic\nranker, namely DenseTrans, for high-recall retrieval. Specifically, DenseTrans\nis a densely connected Transformer, which learns semantic embeddings for texts\nbased on Transformer layers. Meanwhile, DenseTrans promotes low-level features\nthrough dense connections to keep the discriminative power of the learned\nrepresentations. DenseTrans is inspired by DenseNet in computer vision (CV),\nbut poses a new way to use the dense connectivity which is totally different\nfrom its original design purpose. Experimental results over two question\nretrieval benchmark datasets show that our model can obtain significant gain on\nrecall against strong term-based methods as well as state-of-the-art\nembedding-based methods.",
          "link": "http://arxiv.org/abs/2107.08345",
          "publishedOn": "2021-07-20T02:04:38.062Z",
          "wordCount": 691,
          "title": "A Discriminative Semantic Ranker for Question Retrieval. (arXiv:2107.08345v1 [cs.IR])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.15561",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soong_F/0/1/0/all/0/1\">Frank Soong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Text to speech (TTS), or speech synthesis, which aims to synthesize\nintelligible and natural speech given text, is a hot research topic in speech,\nlanguage, and machine learning communities and has broad applications in the\nindustry. As the development of deep learning and artificial intelligence,\nneural network-based TTS has significantly improved the quality of synthesized\nspeech in recent years. In this paper, we conduct a comprehensive survey on\nneural TTS, aiming to provide a good understanding of current research and\nfuture trends. We focus on the key components in neural TTS, including text\nanalysis, acoustic models and vocoders, and several advanced topics, including\nfast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.\nWe further summarize resources related to TTS (e.g., datasets, opensource\nimplementations) and discuss future research directions. This survey can serve\nboth academic researchers and industry practitioners working on TTS.",
          "link": "http://arxiv.org/abs/2106.15561",
          "publishedOn": "2021-07-26T02:00:56.993Z",
          "wordCount": 633,
          "title": "A Survey on Neural Speech Synthesis. (arXiv:2106.15561v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yingqiang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuyan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1\">Qichao Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Huanqiang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhenxing Qian</a>",
          "description": "Previous reversible data hiding in encrypted images (RDHEI) schemes can be\neither carried out by vacating room before or after data encryption, which\nleads to a separation of the search field in RDHEI. Besides, high capacity\nrelies heavily on vacating room before encryption (VRBE), which significantly\nlowers the payload of vacating room after encryption (VRAE) based schemes. To\naddress this issue, this paper proposes a framework for high-capacity RDHEI for\nboth VRBE and VRAE cases using pixel predictions and entropy encoding. We\npropose an embedding room generation algorithm to produce vacated room by\ngenerating the prediction-error histogram (PEH) of the selected cover using\nadjacency prediction and the median edge detector (MED). In the VRBE scenario,\nwe propose a scheme that generates the embedding room using the proposed\nalgorithm, and encrypts the preprocessed image by using the stream cipher with\ntwo encryption keys. In the VRAE scenario, we propose a scheme that involves an\nimproved block modulation and permutation encryption algorithm where the\nspatial redundancy in the plain-text image can be largely preserved. Then the\nproposed algorithm is applied on the encrypted image to generate the embedding\nroom. At the data hider's side of both the schemes, the data hider locates the\nembedding room and embeds the encrypted additional data. On receiving the\nmarked encrypted image, the receivers with different authentication can\nrespectively conduct error-free data extraction and/or error-free image\nrecovery. The experimental results show that the two schemes in the proposed\nframework can outperform many previous state-of-the-art RDHEI arts. Besides,\nthe proposed schemes can ensure high information security in that little detail\nof the original image can be directly discovered from the encrypted images or\nthe marked encrypted images.",
          "link": "http://arxiv.org/abs/2102.12613",
          "publishedOn": "2021-07-23T02:00:30.134Z",
          "wordCount": 750,
          "title": "High-Capacity Framework for Reversible Data Hiding in Encrypted Image Using Pixel Predictions and Entropy Encoding. (arXiv:2102.12613v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jihua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Meng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Member/0/1/0/all/0/1\">Member</a>, <a href=\"http://arxiv.org/find/cs/1/au:+IEEE/0/1/0/all/0/1\">IEEE</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyong Cheng</a>",
          "description": "Video moment retrieval targets at retrieving a moment in a video for a given\nlanguage query. The challenges of this task include 1) the requirement of\nlocalizing the relevant moment in an untrimmed video, and 2) bridging the\nsemantic gap between textual query and video contents. To tackle those\nproblems, early approaches adopt the sliding window or uniform sampling to\ncollect video clips first and then match each clip with the query. Obviously,\nthese strategies are time-consuming and often lead to unsatisfied accuracy in\nlocalization due to the unpredictable length of the golden moment. To avoid the\nlimitations, researchers recently attempt to directly predict the relevant\nmoment boundaries without the requirement to generate video clips first. One\nmainstream approach is to generate a multimodal feature vector for the target\nquery and video frames (e.g., concatenation) and then use a regression approach\nupon the multimodal feature vector for boundary detection. Although some\nprogress has been achieved by this approach, we argue that those methods have\nnot well captured the cross-modal interactions between the query and video\nframes.\n\nIn this paper, we propose an Attentive Cross-modal Relevance Matching (ACRM)\nmodel which predicts the temporal boundaries based on an interaction modeling.\nIn addition, an attention module is introduced to assign higher weights to\nquery words with richer semantic cues, which are considered to be more\nimportant for finding relevant video contents. Another contribution is that we\npropose an additional predictor to utilize the internal frames in the model\ntraining to improve the localization accuracy. Extensive experiments on two\ndatasets TACoS and Charades-STA demonstrate the superiority of our method over\nseveral state-of-the-art methods. Ablation studies have been also conducted to\nexamine the effectiveness of different modules in our ACRM model.",
          "link": "http://arxiv.org/abs/2009.10434",
          "publishedOn": "2021-07-23T02:00:29.829Z",
          "wordCount": 772,
          "title": "Frame-wise Cross-modal Matching for Video Moment Retrieval. (arXiv:2009.10434v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1\">Vinija Jain</a>",
          "description": "Causality knowledge is vital to building robust AI systems. Deep learning\nmodels often perform poorly on tasks that require causal reasoning, which is\noften derived using some form of commonsense knowledge not immediately\navailable in the input but implicitly inferred by humans. Prior work has\nunraveled spurious observational biases that models fall prey to in the absence\nof causality. While language representation models preserve contextual\nknowledge within learned embeddings, they do not factor in causal relationships\nduring training. By blending causal relationships with the input features to an\nexisting model that performs visual cognition tasks (such as scene\nunderstanding, video captioning, video question-answering, etc.), better\nperformance can be achieved owing to the insight causal relationships bring\nabout. Recently, several models have been proposed that have tackled the task\nof mining causal data from either the visual or textual modality. However,\nthere does not exist widespread research that mines causal relationships by\njuxtaposing the visual and language modalities. While images offer a rich and\neasy-to-process resource for us to mine causality knowledge from, videos are\ndenser and consist of naturally time-ordered events. Also, textual information\noffers details that could be implicit in videos. We propose iReason, a\nframework that infers visual-semantic commonsense knowledge using both videos\nand natural language captions. Furthermore, iReason's architecture integrates a\ncausal rationalization module to aid the process of interpretability, error\nanalysis and bias detection. We demonstrate the effectiveness of iReason using\na two-pronged comparative analysis with language representation learning models\n(BERT, GPT-2) as well as current state-of-the-art multimodal causality models.",
          "link": "http://arxiv.org/abs/2107.10300",
          "publishedOn": "2021-07-23T02:00:29.795Z",
          "wordCount": 723,
          "title": "iReason: Multimodal Commonsense Reasoning using Videos and Natural Language with Interpretability. (arXiv:2107.10300v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.03592",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Eskimez_S/0/1/0/all/0/1\">Sefik Emre Eskimez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">You Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_Z/0/1/0/all/0/1\">Zhiyao Duan</a>",
          "description": "Visual emotion expression plays an important role in audiovisual speech\ncommunication. In this work, we propose a novel approach to rendering visual\nemotion expression in speech-driven talking face generation. Specifically, we\ndesign an end-to-end talking face generation system that takes a speech\nutterance, a single face image, and a categorical emotion label as input to\nrender a talking face video synchronized with the speech and expressing the\nconditioned emotion. Objective evaluation on image quality, audiovisual\nsynchronization, and visual emotion expression shows that the proposed system\noutperforms a state-of-the-art baseline system. Subjective evaluation of visual\nemotion expression and video realness also demonstrates the superiority of the\nproposed system. Furthermore, we conduct a human emotion recognition pilot\nstudy using generated videos with mismatched emotions among the audio and\nvisual modalities. Results show that humans respond to the visual modality more\nsignificantly than the audio modality on this task.",
          "link": "http://arxiv.org/abs/2008.03592",
          "publishedOn": "2021-07-23T02:00:29.739Z",
          "wordCount": 632,
          "title": "Speech Driven Talking Face Generation from a Single Image and an Emotion Condition. (arXiv:2008.03592v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanqiao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>",
          "description": "Multimedia content is of predominance in the modern Web era. Investigating\nhow users interact with multimodal items is a continuing concern within the\nrapid development of recommender systems. The majority of previous work focuses\non modeling user-item interactions with multimodal features included as side\ninformation. However, this scheme is not well-designed for multimedia\nrecommendation. Specifically, only collaborative item-item relationships are\nimplicitly modeled through high-order item-user-item relations. Considering\nthat items are associated with rich contents in multiple modalities, we argue\nthat the latent semantic item-item structures underlying these multimodal\ncontents could be beneficial for learning better item representations and\nfurther boosting recommendation. To this end, we propose a LATent sTructure\nmining method for multImodal reCommEndation, which we term LATTICE for brevity.\nTo be specific, in the proposed LATTICE model, we devise a novel modality-aware\nstructure learning layer, which learns item-item structures for each modality\nand aggregates multiple modalities to obtain latent item graphs. Based on the\nlearned latent graphs, we perform graph convolutions to explicitly inject\nhigh-order item affinities into item representations. These enriched item\nrepresentations can then be plugged into existing collaborative filtering\nmethods to make more accurate recommendations. Extensive experiments on three\nreal-world datasets demonstrate the superiority of our method over\nstate-of-the-art multimedia recommendation methods and validate the efficacy of\nmining latent item-item relationships from multimodal features.",
          "link": "http://arxiv.org/abs/2104.09036",
          "publishedOn": "2021-07-22T02:03:11.029Z",
          "wordCount": 695,
          "title": "Mining Latent Structures for Multimedia Recommendation. (arXiv:2104.09036v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>",
          "description": "Symbolic music generation has attracted increasing attention, while most\nmethods focus on generating short piece (mostly less than 8 bars, and up to 32\nbars). Generating long music calls for effective expression of the coherent\nmusic structure. Despite their success on long sequences, self-attention\narchitectures still have challenge in dealing with long-term music as it\nrequires additional care on the subtle music structure. In this paper, we\npropose to transfer the structure of training samples for new music generation,\nand develop a novel separable self-attention based model which enable the\nlearning and transferring of the structure embedding. We show that our transfer\nmodel can generate music sequences (up to 100 bars) with interpretable\nstructures, which bears similar structures and composition techniques with the\ntemplate music from training set. Extensive experiments show its ability of\ngenerating music with target structure and well diversity. The generated 3,000\nsets of music is uploaded as supplemental material.",
          "link": "http://arxiv.org/abs/2107.09877",
          "publishedOn": "2021-07-22T02:03:10.522Z",
          "wordCount": 588,
          "title": "Melody Structure Transfer Network: Generating Music with Separable Self-Attention. (arXiv:2107.09877v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianyao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenxuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>",
          "description": "Nowadays, with the prevalence of social media and music creation tools,\nmusical pieces are spreading much quickly, and music creation is getting much\neasier. The increasing number of musical pieces have made the problem of music\nplagiarism prominent. There is an urgent need for a tool that can detect music\nplagiarism automatically. Researchers have proposed various methods to extract\nlow-level and high-level features of music and compute their similarities.\nHowever, low-level features such as cepstrum coefficients have weak relation\nwith the copyright protection of musical pieces. Existing algorithms\nconsidering high-level features fail to detect the case in which two musical\npieces are not quite similar overall, but have some highly similar regions.\nThis paper proposes a new method named MESMF, which innovatively converts the\nmusic plagiarism detection problem into the bipartite graph matching task. It\ncan be solved via the maximum weight matching and edit distances model. We\ndesign several kinds of melody representations and the similarity computation\nmethods according to the music theory. The proposed method can deal with the\nshift, swapping, transposition, and tempo variance problems in music\nplagiarism. It can also effectively pick out the local similar regions from two\nmusical pieces with relatively low global similarity. We collect a new music\nplagiarism dataset from real legally-judged music plagiarism cases and conduct\ndetailed ablation studies. Experimental results prove the excellent performance\nof the proposed algorithm. The source code and our dataset are available at\nhttps://anonymous.4open.science/r/a41b8fb4-64cf-4190-a1e1-09b7499a15f5/",
          "link": "http://arxiv.org/abs/2107.09889",
          "publishedOn": "2021-07-22T02:03:10.425Z",
          "wordCount": 675,
          "title": "Music Plagiarism Detection via Bipartite Graph Matching. (arXiv:2107.09889v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10220",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antsiferova_A/0/1/0/all/0/1\">Anastasia Antsiferova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yakovenko_A/0/1/0/all/0/1\">Alexander Yakovenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safonov_N/0/1/0/all/0/1\">Nickolay Safonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_D/0/1/0/all/0/1\">Dmitriy Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gushin_A/0/1/0/all/0/1\">Alexander Gushin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1\">Dmitriy Vatolin</a>",
          "description": "Quality assessment plays a key role in creating and comparing video\ncompression algorithms. Despite the development of a large number of new\nmethods for assessing quality, generally accepted and well-known codecs\ncomparisons mainly use the classical methods like PSNR, SSIM and new method\nVMAF. These methods can be calculated following different rules: they can use\ndifferent frame-by-frame averaging techniques or different summation of color\ncomponents. In this paper, a fundamental comparison of various versions of\ngenerally accepted metrics is carried out to find the most relevant and\nrecommended versions of video quality metrics to be used in codecs comparisons.\nFor comparison, we used a set of videos encoded with video codecs of different\nstandards, and visual quality scores collected for the resulting set of streams\nsince 2018 until 2021",
          "link": "http://arxiv.org/abs/2107.10220",
          "publishedOn": "2021-07-22T02:03:10.401Z",
          "wordCount": 584,
          "title": "Objective video quality metrics application to video codecs comparisons: choosing the best for subjective quality estimation. (arXiv:2107.10220v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10132",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Disong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_L/0/1/0/all/0/1\">Liqun Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeung_Y/0/1/0/all/0/1\">Yu Ting Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xunying Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>",
          "description": "One-shot voice conversion (VC), which performs conversion across arbitrary\nspeakers with only a single target-speaker utterance for reference, can be\neffectively achieved by speech representation disentanglement. Existing work\ngenerally ignores the correlation between different speech representations\nduring training, which causes leakage of content information into the speaker\nrepresentation and thus degrades VC performance. To alleviate this issue, we\nemploy vector quantization (VQ) for content encoding and introduce mutual\ninformation (MI) as the correlation metric during training, to achieve proper\ndisentanglement of content, speaker and pitch representations, by reducing\ntheir inter-dependencies in an unsupervised manner. Experimental results\nreflect the superiority of the proposed method in learning effective\ndisentangled speech representations for retaining source linguistic content and\nintonation variations, while capturing target speaker characteristics. In doing\nso, the proposed approach achieves higher speech naturalness and speaker\nsimilarity than current state-of-the-art one-shot VC systems. Our code,\npre-trained models and demo are available at\nhttps://github.com/Wendison/VQMIVC.",
          "link": "http://arxiv.org/abs/2106.10132",
          "publishedOn": "2021-07-22T02:03:10.188Z",
          "wordCount": 639,
          "title": "VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-shot Voice Conversion. (arXiv:2106.10132v1 [eess.AS] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Sanchita Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prevost_J/0/1/0/all/0/1\">John J. Prevost</a>",
          "description": "Deep learning based visual to sound generation systems essentially need to be\ndeveloped particularly considering the synchronicity aspects of visual and\naudio features with time. In this research we introduce a novel task of guiding\na class conditioned generative adversarial network with the temporal visual\ninformation of a video input for visual to sound generation task adapting the\nsynchronicity traits between audio-visual modalities. Our proposed FoleyGAN\nmodel is capable of conditioning action sequences of visual events leading\ntowards generating visually aligned realistic sound tracks. We expand our\npreviously proposed Automatic Foley dataset to train with FoleyGAN and evaluate\nour synthesized sound through human survey that shows noteworthy (on average\n81\\%) audio-visual synchronicity performance. Our approach also outperforms in\nstatistical experiments compared with other baseline models and audio-visual\ndatasets.",
          "link": "http://arxiv.org/abs/2107.09262",
          "publishedOn": "2021-07-21T02:01:33.208Z",
          "wordCount": 610,
          "title": "FoleyGAN: Visually Guided Generative Adversarial Network-Based Synchronous Sound Generation in Silent Videos. (arXiv:2107.09262v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lingzhi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Ying Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunfei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sheng Yang</a>",
          "description": "This paper investigates adaptive streaming of one or multiple tiled 360\nvideos from a multi-antenna base station (BS) to one or multiple single-antenna\nusers, respectively, in a multi-carrier wireless system. We aim to maximize the\nvideo quality while keeping rebuffering time small via encoding rate adaptation\nat each group of pictures (GOP) and transmission adaptation at each\n(transmission) slot. To capture the impact of field-of-view (FoV) prediction,\nwe consider three cases of FoV viewing probability distributions, i.e.,\nperfect, imperfect, and unknown FoV viewing probability distributions, and use\nthe average total utility, worst average total utility, and worst total utility\nas the respective performance metrics. In the single-user scenario, we optimize\nthe encoding rates of the tiles, encoding rates of the FoVs, and transmission\nbeamforming vectors for all subcarriers to maximize the total utility in each\ncase. In the multi-user scenario, we adopt rate splitting with successive\ndecoding and optimize the encoding rates of the tiles, encoding rates of the\nFoVs, rates of the common and private messages, and transmission beamforming\nvectors for all subcarriers to maximize the total utility in each case. Then,\nwe separate the challenging optimization problem into multiple tractable\nproblems in each scenario. In the single-user scenario, we obtain a globally\noptimal solution of each problem using transformation techniques and the\nKarush-Kuhn-Tucker (KKT) conditions. In the multi-user scenario, we obtain a\nKKT point of each problem using the concave-convex procedure (CCCP). Finally,\nnumerical results demonstrate that the proposed solutions achieve notable gains\nover existing schemes in all three cases. To the best of our knowledge, this is\nthe first work revealing the impact of FoV prediction on the performance of\nadaptive streaming of tiled 360 videos.",
          "link": "http://arxiv.org/abs/2107.09491",
          "publishedOn": "2021-07-21T02:01:33.193Z",
          "wordCount": 744,
          "title": "Adaptive Streaming of 360 Videos with Perfect, Imperfect, and Unknown FoV Viewing Probabilities in Wireless Networks. (arXiv:2107.09491v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_G/0/1/0/all/0/1\">Gunjan Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>",
          "description": "Dance and music typically go hand in hand. The complexities in dance, music,\nand their synchronisation make them fascinating to study from a computational\ncreativity perspective. While several works have looked at generating dance for\na given music, automatically generating music for a given dance remains\nunder-explored. This capability could have several creative expression and\nentertainment applications. We present some early explorations in this\ndirection. We present a search-based offline approach that generates music\nafter processing the entire dance video and an online approach that uses a deep\nneural network to generate music on-the-fly as the video proceeds. We compare\nthese approaches to a strong heuristic baseline via human studies and present\nour findings. We have integrated our online approach in a live demo! A video of\nthe demo can be found here:\nhttps://sites.google.com/view/dance2music/live-demo.",
          "link": "http://arxiv.org/abs/2107.06252",
          "publishedOn": "2021-07-21T02:01:33.048Z",
          "wordCount": 581,
          "title": "Dance2Music: Automatic Dance-driven Music Generation. (arXiv:2107.06252v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiangyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yinzhe Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanzhou Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>",
          "description": "In order to protect the intellectual property (IP) of deep neural networks\n(DNNs), many existing DNN watermarking techniques either embed watermarks\ndirectly into the DNN parameters or insert backdoor watermarks by fine-tuning\nthe DNN parameters, which, however, cannot resist against various attack\nmethods that remove watermarks by altering DNN parameters. In this paper, we\nbypass such attacks by introducing a structural watermarking scheme that\nutilizes channel pruning to embed the watermark into the host DNN architecture\ninstead of crafting the DNN parameters. To be specific, during watermark\nembedding, we prune the internal channels of the host DNN with the channel\npruning rates controlled by the watermark. During watermark extraction, the\nwatermark is retrieved by identifying the channel pruning rates from the\narchitecture of the target DNN model. Due to the superiority of pruning\nmechanism, the performance of the DNN model on its original task is reserved\nduring watermark embedding. Experimental results have shown that, the proposed\nwork enables the embedded watermark to be reliably recovered and provides a\nhigh watermark capacity, without sacrificing the usability of the DNN model. It\nis also demonstrated that the work is robust against common transforms and\nattacks designed for conventional watermarking approaches.",
          "link": "http://arxiv.org/abs/2107.08688",
          "publishedOn": "2021-07-20T02:04:38.180Z",
          "wordCount": 640,
          "title": "Structural Watermarking to Deep Neural Networks via Network Channel Pruning. (arXiv:2107.08688v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengfei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>",
          "description": "In this paper, we develop face.evoLVe -- a comprehensive library that\ncollects and implements a wide range of popular deep learning-based methods for\nface recognition. First of all, face.evoLVe is composed of key components that\ncover the full process of face analytics, including face alignment, data\nprocessing, various backbones, losses, and alternatives with bags of tricks for\nimproving performance. Later, face.evoLVe supports multi-GPU training on top of\ndifferent deep learning platforms, such as PyTorch and PaddlePaddle, which\nfacilitates researchers to work on both large-scale datasets with millions of\nimages and low-shot counterparts with limited well-annotated data. More\nimportantly, along with face.evoLVe, images before & after alignment in the\ncommon benchmark datasets are released with source codes and trained models\nprovided. All these efforts lower the technical burdens in reproducing the\nexisting methods for comparison, while users of our library could focus on\ndeveloping advanced approaches more efficiently. Last but not least,\nface.evoLVe is well designed and vibrantly evolving, so that new face\nrecognition approaches can be easily plugged into our framework. Note that we\nhave used face.evoLVe to participate in a number of face recognition\ncompetitions and secured the first place. The version that supports PyTorch is\npublicly available at https://github.com/ZhaoJ9014/face.evoLVe.PyTorch and the\nPaddlePaddle version is available at\nhttps://github.com/ZhaoJ9014/face.evoLVe.PyTorch/tree/master/paddle.\nFace.evoLVe has been widely used for face analytics, receiving 2.4K stars and\n622 forks.",
          "link": "http://arxiv.org/abs/2107.08621",
          "publishedOn": "2021-07-20T02:04:37.964Z",
          "wordCount": 668,
          "title": "Face.evoLVe: A High-Performance Face Recognition Library. (arXiv:2107.08621v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.",
          "link": "http://arxiv.org/abs/2107.08264",
          "publishedOn": "2021-07-20T02:04:37.888Z",
          "wordCount": 685,
          "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yao Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Haipeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Despite being a critical communication skill, grasping humor is challenging\n-- a successful use of humor requires a mixture of both engaging content\nbuild-up and an appropriate vocal delivery (e.g., pause). Prior studies on\ncomputational humor emphasize the textual and audio features immediately next\nto the punchline, yet overlooking longer-term context setup. Moreover, the\ntheories are usually too abstract for understanding each concrete humor\nsnippet. To fill in the gap, we develop DeHumor, a visual analytical system for\nanalyzing humorous behaviors in public speaking. To intuitively reveal the\nbuilding blocks of each concrete example, DeHumor decomposes each humorous\nvideo into multimodal features and provides inline annotations of them on the\nvideo script. In particular, to better capture the build-ups, we introduce\ncontent repetition as a complement to features introduced in theories of\ncomputational humor and visualize them in a context linking graph. To help\nusers locate the punchlines that have the desired features to learn, we\nsummarize the content (with keywords) and humor feature statistics on an\naugmented time matrix. With case studies on stand-up comedy shows and TED\ntalks, we show that DeHumor is able to highlight various building blocks of\nhumor examples. In addition, expert interviews with communication coaches and\nhumor researchers demonstrate the effectiveness of DeHumor for multimodal humor\nanalysis of speech content and vocal delivery.",
          "link": "http://arxiv.org/abs/2107.08356",
          "publishedOn": "2021-07-20T02:04:37.706Z",
          "wordCount": 689,
          "title": "DeHumor: Visual Analytics for Decomposing Humor. (arXiv:2107.08356v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2103.16146",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1\">Gihyun Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>",
          "description": "One of the important research topics in image generative models is to\ndisentangle the spatial contents and styles for their separate control.\nAlthough StyleGAN can generate content feature vectors from random noises, the\nresulting spatial content control is primarily intended for minor spatial\nvariations, and the disentanglement of global content and styles is by no means\ncomplete. Inspired by a mathematical understanding of normalization and\nattention, here we present a novel hierarchical adaptive Diagonal spatial\nATtention (DAT) layers to separately manipulate the spatial contents from\nstyles in a hierarchical manner. Using DAT and AdaIN, our method enables\ncoarse-to-fine level disentanglement of spatial contents and styles. In\naddition, our generator can be easily integrated into the GAN inversion\nframework so that the content and style of translated images from multi-domain\nimage translation tasks can be flexibly controlled. By using various datasets,\nwe confirm that the proposed method not only outperforms the existing models in\ndisentanglement scores, but also provides more flexible control over spatial\nfeatures in the generated images.",
          "link": "http://arxiv.org/abs/2103.16146",
          "publishedOn": "2021-07-26T02:00:58.639Z",
          "wordCount": 642,
          "title": "Diagonal Attention and Style-based GAN for Content-Style Disentanglement in Image Generation and Translation. (arXiv:2103.16146v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khirodkar_R/0/1/0/all/0/1\">Rawal Khirodkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chari_V/0/1/0/all/0/1\">Visesh Chari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Amit Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1\">Ambrish Tyagi</a>",
          "description": "A key assumption of top-down human pose estimation approaches is their\nexpectation of having a single person/instance present in the input bounding\nbox. This often leads to failures in crowded scenes with occlusions. We propose\na novel solution to overcome the limitations of this fundamental assumption.\nOur Multi-Instance Pose Network (MIPNet) allows for predicting multiple 2D pose\ninstances within a given bounding box. We introduce a Multi-Instance Modulation\nBlock (MIMB) that can adaptively modulate channel-wise feature responses for\neach instance and is parameter efficient. We demonstrate the efficacy of our\napproach by evaluating on COCO, CrowdPose, and OCHuman datasets. Specifically,\nwe achieve 70.0 AP on CrowdPose and 42.5 AP on OCHuman test sets, a significant\nimprovement of 2.4 AP and 6.5 AP over the prior art, respectively. When using\nground truth bounding boxes for inference, MIPNet achieves an improvement of\n0.7 AP on COCO, 0.9 AP on CrowdPose, and 9.1 AP on OCHuman validation sets\ncompared to HRNet. Interestingly, when fewer, high confidence bounding boxes\nare used, HRNet's performance degrades (by 5 AP) on OCHuman, whereas MIPNet\nmaintains a relatively stable performance (drop of 1 AP) for the same inputs.",
          "link": "http://arxiv.org/abs/2101.11223",
          "publishedOn": "2021-07-26T02:00:58.630Z",
          "wordCount": 656,
          "title": "Multi-Instance Pose Networks: Rethinking Top-Down Pose Estimation. (arXiv:2101.11223v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Song Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhihao Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakker_E/0/1/0/all/0/1\">Erwin M.Bakker</a>",
          "description": "Due to its effectivity and efficiency, deep hashing approaches are widely\nused for large-scale visual search. However, it is still challenging to produce\ncompact and discriminative hash codes for images associated with multiple\nsemantics for two main reasons, 1) similarity constraints designed in most of\nthe existing methods are based upon an oversimplified similarity\nassignment(i.e., 0 for instance pairs sharing no label, 1 for instance pairs\nsharing at least 1 label), 2) the exploration in multi-semantic relevance are\ninsufficient or even neglected in many of the existing methods. These problems\nsignificantly limit the discrimination of generated hash codes. In this paper,\nwe propose a novel self-supervised asymmetric deep hashing method with a\nmargin-scalable constraint(SADH) approach to cope with these problems. SADH\nimplements a self-supervised network to sufficiently preserve semantic\ninformation in a semantic feature dictionary and a semantic code dictionary for\nthe semantics of the given dataset, which efficiently and precisely guides a\nfeature learning network to preserve multilabel semantic information using an\nasymmetric learning strategy. By further exploiting semantic dictionaries, a\nnew margin-scalable constraint is employed for both precise similarity\nsearching and robust hash code generation. Extensive empirical research on four\npopular benchmarks validates the proposed method and shows it outperforms\nseveral state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2012.03820",
          "publishedOn": "2021-07-26T02:00:58.623Z",
          "wordCount": 674,
          "title": "Self-supervised asymmetric deep hashing with margin-scalable constraint. (arXiv:2012.03820v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yaofo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>",
          "description": "Convolutional Neural Networks (CNNs) have achieved great success due to the\npowerful feature learning ability of convolution layers. Specifically, the\nstandard convolution traverses the input images/features using a sliding window\nscheme to extract features. However, not all the windows contribute equally to\nthe prediction results of CNNs. In practice, the convolutional operation on\nsome of the windows (e.g., smooth windows that contain very similar pixels) can\nbe very redundant and may introduce noises into the computation. Such\nredundancy may not only deteriorate the performance but also incur the\nunnecessary computational cost. Thus, it is important to reduce the\ncomputational redundancy of convolution to improve the performance. To this\nend, we propose a Content-aware Convolution (CAC) that automatically detects\nthe smooth windows and applies a 1x1 convolutional kernel to replace the\noriginal large kernel. In this sense, we are able to effectively avoid the\nredundant computation on similar pixels. By replacing the standard convolution\nin CNNs with our CAC, the resultant models yield significantly better\nperformance and lower computational cost than the baseline models with the\nstandard convolution. More critically, we are able to dynamically allocate\nsuitable computation resources according to the data smoothness of different\nimages, making it possible for content-aware computation. Extensive experiments\non various computer vision tasks demonstrate the superiority of our method over\nexisting methods.",
          "link": "http://arxiv.org/abs/2106.15797",
          "publishedOn": "2021-07-26T02:00:58.616Z",
          "wordCount": 672,
          "title": "Content-Aware Convolutional Neural Networks. (arXiv:2106.15797v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11489",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1\">Akshay Rangesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deo_N/0/1/0/all/0/1\">Nachiket Deo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greer_R/0/1/0/all/0/1\">Ross Greer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunaratne_P/0/1/0/all/0/1\">Pujitha Gunaratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1\">Mohan M. Trivedi</a>",
          "description": "With increasing automation in passenger vehicles, the study of safe and\nsmooth occupant-vehicle interaction and control transitions is key. In this\nstudy, we focus on the development of contextual, semantically meaningful\nrepresentations of the driver state, which can then be used to determine the\nappropriate timing and conditions for transfer of control between driver and\nvehicle. To this end, we conduct a large-scale real-world controlled data study\nwhere participants are instructed to take-over control from an autonomous agent\nunder different driving conditions while engaged in a variety of distracting\nactivities. These take-over events are captured using multiple driver-facing\ncameras, which when labelled result in a dataset of control transitions and\ntheir corresponding take-over times (TOTs). We then develop and train TOT\nmodels that operate sequentially on mid to high-level features produced by\ncomputer vision algorithms operating on different driver-facing camera views.\nThe proposed TOT model produces continuous predictions of take-over times\nwithout delay, and shows promising qualitative and quantitative results in\ncomplex real-world scenarios.",
          "link": "http://arxiv.org/abs/2104.11489",
          "publishedOn": "2021-07-26T02:00:58.608Z",
          "wordCount": 648,
          "title": "Autonomous Vehicles that Alert Humans to Take-Over Controls: Modeling with Real-World Data. (arXiv:2104.11489v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borowa_A/0/1/0/all/0/1\">Adriana Borowa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rymarczyk_D/0/1/0/all/0/1\">Dawid Rymarczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochonska_D/0/1/0/all/0/1\">Dorota Ocho&#x144;ska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brzychczy_Wloch_M/0/1/0/all/0/1\">Monika Brzychczy-W&#x142;och</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zielinski_B/0/1/0/all/0/1\">Bartosz Zieli&#x144;ski</a>",
          "description": "In this work, we analyze if it is possible to distinguish between different\nclones of the same bacteria species (Klebsiella pneumoniae) based only on\nmicroscopic images. It is a challenging task, previously considered impossible\ndue to the high clones similarity. For this purpose, we apply a multi-step\nalgorithm with attention-based multiple instance learning. Except for obtaining\naccuracy at the level of 0.9, we introduce extensive interpretability based on\nCellProfiler and persistence homology, increasing the understandability and\ntrust in the model.",
          "link": "http://arxiv.org/abs/2012.01189",
          "publishedOn": "2021-07-26T02:00:58.588Z",
          "wordCount": 573,
          "title": "Classifying bacteria clones using attention-based deep multiple instance learning interpreted by persistence homology. (arXiv:2012.01189v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Loddo_A/0/1/0/all/0/1\">Andrea Loddo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruberto_C/0/1/0/all/0/1\">Cecilia Di Ruberto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vale_A/0/1/0/all/0/1\">A.M.P.G. Vale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ucchesu_M/0/1/0/all/0/1\">Mariano Ucchesu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_J/0/1/0/all/0/1\">J.M. Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacchetta_G/0/1/0/all/0/1\">Gianluigi Bacchetta</a>",
          "description": "Image analysis is an essential field for several topics in the life sciences,\nsuch as biology or botany. In particular, the analysis of seeds (e.g. fossil\nresearch) can provide significant information on their evolution, the history\nof agriculture, plant domestication and knowledge of diets in ancient times.\nThis work aims to present software that performs image analysis for feature\nextraction and classification from images containing seeds through a novel and\nunique framework. In detail, we propose two plugins \\emph{ImageJ}, one able to\nextract morphological, textual and colour features from seed images, and\nanother to classify seeds into categories using the extracted features. The\nexperimental results demonstrated the correctness and validity of both the\nextracted features and the classification predictions. The proposed tool is\neasily extendable to other fields of image analysis.",
          "link": "http://arxiv.org/abs/2103.17213",
          "publishedOn": "2021-07-26T02:00:58.581Z",
          "wordCount": 601,
          "title": "An effective and friendly tool for seed image analysis. (arXiv:2103.17213v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Popli_A/0/1/0/all/0/1\">Additya Popli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_S/0/1/0/all/0/1\">Saraansh Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelsma_J/0/1/0/all/0/1\">Joshua J. Engelsma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onoe_N/0/1/0/all/0/1\">Naoyuki Onoe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okubo_A/0/1/0/all/0/1\">Atsushi Okubo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_A/0/1/0/all/0/1\">Anoop Namboodiri</a>",
          "description": "Typical fingerprint recognition systems are comprised of a spoof detection\nmodule and a subsequent recognition module, running one after the other. In\nthis paper, we reformulate the workings of a typical fingerprint recognition\nsystem. In particular, we posit that both spoof detection and fingerprint\nrecognition are correlated tasks. Therefore, rather than performing the two\ntasks separately, we propose a joint model for spoof detection and matching to\nsimultaneously perform both tasks without compromising the accuracy of either\ntask. We demonstrate the capability of our joint model to obtain an\nauthentication accuracy (1:1 matching) of TAR = 100% @ FAR = 0.1% on the FVC\n2006 DB2A dataset while achieving a spoof detection ACE of 1.44% on the LiveDet\n2015 dataset, both maintaining the performance of stand-alone methods. In\npractice, this reduces the time and memory requirements of the fingerprint\nrecognition system by 50% and 40%, respectively; a significant advantage for\nrecognition systems running on resource-constrained devices and communication\nchannels. The project page for our work is available at\nhttps://www.bit.ly/ijcb2021-unified .",
          "link": "http://arxiv.org/abs/2104.03255",
          "publishedOn": "2021-07-26T02:00:58.574Z",
          "wordCount": 659,
          "title": "A Unified Model for Fingerprint Authentication and Presentation Attack Detection. (arXiv:2104.03255v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1\">Gaurav Kumar Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1\">Konda Reddy Mopuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saksham Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Anirban Chakraborty</a>",
          "description": "Pretrained deep models hold their learnt knowledge in the form of model\nparameters. These parameters act as \"memory\" for the trained models and help\nthem generalize well on unseen data. However, in absence of training data, the\nutility of a trained model is merely limited to either inference or better\ninitialization towards a target task. In this paper, we go further and extract\nsynthetic data by leveraging the learnt model parameters. We dub them \"Data\nImpressions\", which act as proxy to the training data and can be used to\nrealize a variety of tasks. These are useful in scenarios where only the\npretrained models are available and the training data is not shared (e.g., due\nto privacy or sensitivity concerns). We show the applicability of data\nimpressions in solving several computer vision tasks such as unsupervised\ndomain adaptation, continual learning as well as knowledge distillation. We\nalso study the adversarial robustness of lightweight models trained via\nknowledge distillation using these data impressions. Further, we demonstrate\nthe efficacy of data impressions in generating data-free Universal Adversarial\nPerturbations (UAPs) with better fooling rates. Extensive experiments performed\non benchmark datasets demonstrate competitive performance achieved using data\nimpressions in absence of original training data.",
          "link": "http://arxiv.org/abs/2101.06069",
          "publishedOn": "2021-07-26T02:00:58.567Z",
          "wordCount": 700,
          "title": "Mining Data Impressions from Deep Models as Substitute for the Unavailable Training Data. (arXiv:2101.06069v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.11134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Evci_U/0/1/0/all/0/1\">Utku Evci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gale_T/0/1/0/all/0/1\">Trevor Gale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1\">Pablo Samuel Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsen_E/0/1/0/all/0/1\">Erich Elsen</a>",
          "description": "Many applications require sparse neural networks due to space or inference\ntime restrictions. There is a large body of work on training dense networks to\nyield sparse networks for inference, but this limits the size of the largest\ntrainable sparse model to that of the largest trainable dense model. In this\npaper we introduce a method to train sparse neural networks with a fixed\nparameter count and a fixed computational cost throughout training, without\nsacrificing accuracy relative to existing dense-to-sparse training methods. Our\nmethod updates the topology of the sparse network during training by using\nparameter magnitudes and infrequent gradient calculations. We show that this\napproach requires fewer floating-point operations (FLOPs) to achieve a given\nlevel of accuracy compared to prior techniques. We demonstrate state-of-the-art\nsparse training results on a variety of networks and datasets, including\nResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we\nprovide some insights into why allowing the topology to change during the\noptimization can overcome local minima encountered when the topology remains\nstatic. Code used in our work can be found in github.com/google-research/rigl.",
          "link": "http://arxiv.org/abs/1911.11134",
          "publishedOn": "2021-07-26T02:00:58.559Z",
          "wordCount": 696,
          "title": "Rigging the Lottery: Making All Tickets Winners. (arXiv:1911.11134v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zehua Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1\">Qiuhong Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>",
          "description": "Human Action Recognition (HAR) aims to understand human behavior and assign a\nlabel to each action. It has a wide range of applications, and therefore has\nbeen attracting increasing attention in the field of computer vision. Human\nactions can be represented using various data modalities, such as RGB,\nskeleton, depth, infrared, point cloud, event stream, audio, acceleration,\nradar, and WiFi signal, which encode different sources of useful yet distinct\ninformation and have various advantages depending on the application scenarios.\nConsequently, lots of existing works have attempted to investigate different\ntypes of approaches for HAR using various modalities. In this paper, we present\na comprehensive survey of recent progress in deep learning methods for HAR\nbased on the type of input data modality. Specifically, we review the current\nmainstream deep learning methods for single data modalities and multiple data\nmodalities, including the fusion-based and the co-learning-based frameworks. We\nalso present comparative results on several benchmark datasets for HAR,\ntogether with insightful observations and inspiring future research directions.",
          "link": "http://arxiv.org/abs/2012.11866",
          "publishedOn": "2021-07-26T02:00:58.522Z",
          "wordCount": 656,
          "title": "Human Action Recognition from Various Data Modalities: A Review. (arXiv:2012.11866v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.09525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1\">Anand Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zylich_B/0/1/0/all/0/1\">Brian Zylich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ottmar_E/0/1/0/all/0/1\">Erin Ottmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LoCasale_Crouch_J/0/1/0/all/0/1\">Jennifer LoCasale-Crouch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitehill_J/0/1/0/all/0/1\">Jacob Whitehill</a>",
          "description": "In this work we present a multi-modal machine learning-based system, which we\ncall ACORN, to analyze videos of school classrooms for the Positive Climate\n(PC) and Negative Climate (NC) dimensions of the CLASS observation protocol\nthat is widely used in educational research. ACORN uses convolutional neural\nnetworks to analyze spectral audio features, the faces of teachers and\nstudents, and the pixels of each image frame, and then integrates this\ninformation over time using Temporal Convolutional Networks. The audiovisual\nACORN's PC and NC predictions have Pearson correlations of $0.55$ and $0.63$\nwith ground-truth scores provided by expert CLASS coders on the UVA Toddler\ndataset (cross-validation on $n=300$ 15-min video segments), and a purely\nauditory ACORN predicts PC and NC with correlations of $0.36$ and $0.41$ on the\nMET dataset (test set of $n=2000$ videos segments). These numbers are similar\nto inter-coder reliability of human coders. Finally, using Graph Convolutional\nNetworks we make early strides (AUC=$0.70$) toward predicting the specific\nmoments (45-90sec clips) when the PC is particularly weak/strong. Our findings\ninform the design of automatic classroom observation and also more general\nvideo activity recognition and summary recognition systems.",
          "link": "http://arxiv.org/abs/2005.09525",
          "publishedOn": "2021-07-26T02:00:58.510Z",
          "wordCount": 712,
          "title": "Toward Automated Classroom Observation: Multimodal Machine Learning to Estimate CLASS Positive Climate and Negative Climate. (arXiv:2005.09525v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aghdaie_P/0/1/0/all/0/1\">Poorya Aghdaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_B/0/1/0/all/0/1\">Baaria Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1\">Sobhan Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "Morphed images have exploited loopholes in the face recognition checkpoints,\ne.g., Credential Authentication Technology (CAT), used by Transportation\nSecurity Administration (TSA), which is a non-trivial security concern. To\novercome the risks incurred due to morphed presentations, we propose a\nwavelet-based morph detection methodology which adopts an end-to-end trainable\nsoft attention mechanism . Our attention-based deep neural network (DNN)\nfocuses on the salient Regions of Interest (ROI) which have the most spatial\nsupport for morph detector decision function, i.e, morph class binary softmax\noutput. A retrospective of morph synthesizing procedure aids us to speculate\nthe ROI as regions around facial landmarks , particularly for the case of\nlandmark-based morphing techniques. Moreover, our attention-based DNN is\nadapted to the wavelet space, where inputs of the network are coarse-to-fine\nspectral representations, 48 stacked wavelet sub-bands to be exact. We evaluate\nperformance of the proposed framework using three datasets, VISAPP17, LMA, and\nMorGAN. In addition, as attention maps can be a robust indicator whether a\nprobe image under investigation is genuine or counterfeit, we analyze the\nestimated attention maps for both a bona fide image and its corresponding\nmorphed image. Finally, we present an ablation study on the efficacy of\nutilizing attention mechanism for the sake of morph detection.",
          "link": "http://arxiv.org/abs/2106.15686",
          "publishedOn": "2021-07-26T02:00:58.502Z",
          "wordCount": 664,
          "title": "Attention Aware Wavelet-based Detection of Morphed Face Images. (arXiv:2106.15686v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.13753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Santaquiteria_J/0/1/0/all/0/1\">Jesus Ruiz-Santaquiteria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velasco_Mata_A/0/1/0/all/0/1\">Alberto Velasco-Mata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallez_N/0/1/0/all/0/1\">Noelia Vallez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bueno_G/0/1/0/all/0/1\">Gloria Bueno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Garcia_J/0/1/0/all/0/1\">Juan A. &#xc1;lvarez-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deniz_O/0/1/0/all/0/1\">Oscar Deniz</a>",
          "description": "Closed-circuit television (CCTV) systems are essential nowadays to prevent\nsecurity threats or dangerous situations, in which early detection is crucial.\nNovel deep learning-based methods have allowed to develop automatic weapon\ndetectors with promising results. However, these approaches are mainly based on\nvisual weapon appearance only. For handguns, body pose may be a useful cue,\nespecially in cases where the gun is barely visible. In this work, a novel\nmethod is proposed to combine, in a single architecture, both weapon appearance\nand human pose information. First, pose keypoints are estimated to extract hand\nregions and generate binary pose images, which are the model inputs. Then, each\ninput is processed in different subnetworks and combined to produce the handgun\nbounding box. Results obtained show that the combined model improves the\nhandgun detection state of the art, achieving from 4.23 to 18.9 AP points more\nthan the best previous approach.",
          "link": "http://arxiv.org/abs/2010.13753",
          "publishedOn": "2021-07-26T02:00:58.481Z",
          "wordCount": 644,
          "title": "Handgun detection using combined human pose and weapon appearance. (arXiv:2010.13753v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jian Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xiaohang Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peize Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>",
          "description": "Unsupervised contrastive learning achieves great success in learning image\nrepresentations with CNN. Unlike most recent methods that focused on improving\naccuracy of image classification, we present a novel contrastive learning\napproach, named DetCo, which fully explores the contrasts between global image\nand local image patches to learn discriminative representations for object\ndetection. DetCo has several appealing benefits. (1) It is carefully designed\nby investigating the weaknesses of current self-supervised methods, which\ndiscard important representations for object detection. (2) DetCo builds\nhierarchical intermediate contrastive losses between global image and local\npatches to improve object detection, while maintaining global representations\nfor image recognition. Theoretical analysis shows that the local patches\nactually remove the contextual information of an image, improving the lower\nbound of mutual information for better contrastive learning. (3) Extensive\nexperiments on PASCAL VOC, COCO and Cityscapes demonstrate that DetCo not only\noutperforms state-of-the-art methods on object detection, but also on\nsegmentation, pose estimation, and 3D shape prediction, while it is still\ncompetitive on image classification. For example, on PASCAL VOC, DetCo-100ep\nachieves 57.4 mAP, which is on par with the result of MoCov2-800ep. Moreover,\nDetCo consistently outperforms supervised method by 1.6/1.2/1.0 AP on Mask\nRCNN-C4/FPN/RetinaNet with 1x schedule. Code will be released at\n\\href{https://github.com/xieenze/DetCo}{\\color{blue}{\\tt\ngithub.com/xieenze/DetCo}}.",
          "link": "http://arxiv.org/abs/2102.04803",
          "publishedOn": "2021-07-26T02:00:58.474Z",
          "wordCount": 685,
          "title": "DetCo: Unsupervised Contrastive Learning for Object Detection. (arXiv:2102.04803v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07950",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kamraoui_R/0/1/0/all/0/1\">Reda Abdellah Kamraoui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ta_V/0/1/0/all/0/1\">Vinh-Thong Ta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tourdias_T/0/1/0/all/0/1\">Thomas Tourdias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mansencal_B/0/1/0/all/0/1\">Boris Mansencal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manjon_J/0/1/0/all/0/1\">Jos&#xe9; V Manjon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coupe_P/0/1/0/all/0/1\">Pierrick Coup&#xe9;</a>",
          "description": "Recently, segmentation methods based on Convolutional Neural Networks (CNNs)\nshowed promising performance in automatic Multiple Sclerosis (MS) lesions\nsegmentation. These techniques have even outperformed human experts in\ncontrolled evaluation conditions such as Longitudinal MS Lesion Segmentation\nChallenge (ISBI Challenge). However state-of-the-art approaches trained to\nperform well on highly-controlled datasets fail to generalize on clinical data\nfrom unseen datasets. Instead of proposing another improvement of the\nsegmentation accuracy, we propose a novel method robust to domain shift and\nperforming well on unseen datasets, called DeepLesionBrain (DLB). This\ngeneralization property results from three main contributions. First, DLB is\nbased on a large group of compact 3D CNNs. This spatially distributed strategy\nensures a robust prediction despite the risk of generalization failure of some\nindividual networks. Second, DLB includes a new image quality data augmentation\nto reduce dependency to training data specificity (e.g., acquisition protocol).\nFinally, to learn a more generalizable representation of MS lesions, we propose\na hierarchical specialization learning (HSL). HSL is performed by pre-training\na generic network over the whole brain, before using its weights as\ninitialization to locally specialized networks. By this end, DLB learns both\ngeneric features extracted at global image level and specific features\nextracted at local image level. DLB generalization was validated in\ncross-dataset experiments on MSSEG'16, ISBI challenge, and in-house datasets.\nDuring experiments, DLB showed higher segmentation accuracy, better\nsegmentation consistency and greater generalization performance compared to\nstate-of-the-art methods. Therefore, DLB offers a robust framework well-suited\nfor clinical practice.",
          "link": "http://arxiv.org/abs/2012.07950",
          "publishedOn": "2021-07-26T02:00:58.466Z",
          "wordCount": 718,
          "title": "DeepLesionBrain: Towards a broader deep-learning generalization for multiple sclerosis lesion segmentation. (arXiv:2012.07950v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11879",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zequn Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>",
          "description": "Attention mechanism, especially channel attention, has gained great success\nin the computer vision field. Many works focus on how to design efficient\nchannel attention mechanisms while ignoring a fundamental problem, i.e.,\nchannel attention mechanism uses scalar to represent channel, which is\ndifficult due to massive information loss. In this work, we start from a\ndifferent view and regard the channel representation problem as a compression\nprocess using frequency analysis. Based on the frequency analysis, we\nmathematically prove that the conventional global average pooling is a special\ncase of the feature decomposition in the frequency domain. With the proof, we\nnaturally generalize the compression of the channel attention mechanism in the\nfrequency domain and propose our method with multi-spectral channel attention,\ntermed as FcaNet. FcaNet is simple but effective. We can change a few lines of\ncode in the calculation to implement our method within existing channel\nattention methods. Moreover, the proposed method achieves state-of-the-art\nresults compared with other channel attention methods on image classification,\nobject detection, and instance segmentation tasks. Our method could\nconsistently outperform the baseline SENet, with the same number of parameters\nand the same computational cost. Our code and models will are publicly\navailable at https://github.com/cfzd/FcaNet.",
          "link": "http://arxiv.org/abs/2012.11879",
          "publishedOn": "2021-07-26T02:00:58.458Z",
          "wordCount": 679,
          "title": "FcaNet: Frequency Channel Attention Networks. (arXiv:2012.11879v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saunders_B/0/1/0/all/0/1\">Ben Saunders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camgoz_N/0/1/0/all/0/1\">Necati Cihan Camgoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>",
          "description": "It is common practice to represent spoken languages at their phonetic level.\nHowever, for sign languages, this implies breaking motion into its constituent\nmotion primitives. Avatar based Sign Language Production (SLP) has\ntraditionally done just this, building up animation from sequences of hand\nmotions, shapes and facial expressions. However, more recent deep learning\nbased solutions to SLP have tackled the problem using a single network that\nestimates the full skeletal structure.\n\nWe propose splitting the SLP task into two distinct jointly-trained\nsub-tasks. The first translation sub-task translates from spoken language to a\nlatent sign language representation, with gloss supervision. Subsequently, the\nanimation sub-task aims to produce expressive sign language sequences that\nclosely resemble the learnt spatio-temporal representation. Using a progressive\ntransformer for the translation sub-task, we propose a novel Mixture of Motion\nPrimitives (MoMP) architecture for sign language animation. A set of distinct\nmotion primitives are learnt during training, that can be temporally combined\nat inference to animate continuous sign language sequences.\n\nWe evaluate on the challenging RWTH-PHOENIX-Weather-2014T(PHOENIX14T)\ndataset, presenting extensive ablation studies and showing that MoMP\noutperforms baselines in user evaluations. We achieve state-of-the-art back\ntranslation performance with an 11% improvement over competing results.\nImportantly, and for the first time, we showcase stronger performance for a\nfull translation pipeline going from spoken language to sign, than from gloss\nto sign.",
          "link": "http://arxiv.org/abs/2107.11317",
          "publishedOn": "2021-07-26T02:00:58.448Z",
          "wordCount": 671,
          "title": "Mixed SIGNals: Sign Language Production via a Mixture of Motion Primitives. (arXiv:2107.11317v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.02498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "Global covariance pooling (GCP) aims at exploiting the second-order\nstatistics of the convolutional feature. Its effectiveness has been\ndemonstrated in boosting the classification performance of Convolutional Neural\nNetworks (CNNs). Singular Value Decomposition (SVD) is used in GCP to compute\nthe matrix square root. However, the approximate matrix square root calculated\nusing Newton-Schulz iteration \\cite{li2018towards} outperforms the accurate one\ncomputed via SVD \\cite{li2017second}. We empirically analyze the reason behind\nthe performance gap from the perspectives of data precision and gradient\nsmoothness. Various remedies for computing smooth SVD gradients are\ninvestigated. Based on our observation and analyses, a hybrid training protocol\nis proposed for SVD-based GCP meta-layers such that competitive performances\ncan be achieved against Newton-Schulz iteration. Moreover, we propose a new GCP\nmeta-layer that uses SVD in the forward pass, and Pad\\'e Approximants in the\nbackward propagation to compute the gradients. The proposed meta-layer has been\nintegrated into different CNN models and achieves state-of-the-art performances\non both large-scale and fine-grained datasets.",
          "link": "http://arxiv.org/abs/2105.02498",
          "publishedOn": "2021-07-26T02:00:58.427Z",
          "wordCount": 639,
          "title": "Why Approximate Matrix Square Root Outperforms Accurate SVD in Global Covariance Pooling?. (arXiv:2105.02498v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.03791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zaiping Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jungang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1\">Wei An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>",
          "description": "Recently, the performance of single image super-resolution (SR) has been\nsignificantly improved with powerful networks. However, these networks are\ndeveloped for image SR with a single specific integer scale (e.g., x2;x3,x4),\nand cannot be used for non-integer and asymmetric SR. In this paper, we propose\nto learn a scale-arbitrary image SR network from scale-specific networks.\nSpecifically, we propose a plug-in module for existing SR networks to perform\nscale-arbitrary SR, which consists of multiple scale-aware feature adaption\nblocks and a scale-aware upsampling layer. Moreover, we introduce a scale-aware\nknowledge transfer paradigm to transfer knowledge from scale-specific networks\nto the scale-arbitrary network. Our plug-in module can be easily adapted to\nexisting networks to achieve scale-arbitrary SR. These networks plugged with\nour module can achieve promising results for non-integer and asymmetric SR\nwhile maintaining state-of-the-art performance for SR with integer scale\nfactors. Besides, the additional computational and memory cost of our module is\nvery small.",
          "link": "http://arxiv.org/abs/2004.03791",
          "publishedOn": "2021-07-26T02:00:58.420Z",
          "wordCount": 625,
          "title": "Learning A Single Network for Scale-Arbitrary Super-Resolution. (arXiv:2004.03791v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Changqing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gongjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanghang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>",
          "description": "Deep learning-based 3D object detection has achieved unprecedented success\nwith the advent of large-scale autonomous driving datasets. However, drastic\nperformance degradation remains a critical challenge for cross-domain\ndeployment. In addition, existing 3D domain adaptive detection methods often\nassume prior access to the target domain annotations, which is rarely feasible\nin the real world. To address this challenge, we study a more realistic\nsetting, unsupervised 3D domain adaptive detection, which only utilizes source\ndomain annotations. 1) We first comprehensively investigate the major\nunderlying factors of the domain gap in 3D detection. Our key insight is that\ngeometric mismatch is the key factor of domain shift. 2) Then, we propose a\nnovel and unified framework, Multi-Level Consistency Network (MLC-Net), which\nemploys a teacher-student paradigm to generate adaptive and reliable\npseudo-targets. MLC-Net exploits point-, instance- and neural statistics-level\nconsistency to facilitate cross-domain transfer. Extensive experiments\ndemonstrate that MLC-Net outperforms existing state-of-the-art methods\n(including those using additional target domain information) on standard\nbenchmarks. Notably, our approach is detector-agnostic, which achieves\nconsistent gains on both single- and two-stage 3D detectors.",
          "link": "http://arxiv.org/abs/2107.11355",
          "publishedOn": "2021-07-26T02:00:58.412Z",
          "wordCount": 623,
          "title": "Unsupervised Domain Adaptive 3D Detection with Multi-Level Consistency. (arXiv:2107.11355v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11267",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiacheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yap_K/0/1/0/all/0/1\">Kim-Hui Yap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fayao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_T/0/1/0/all/0/1\">Tzu-Yi Hung</a>",
          "description": "Semantic segmentation on 3D point clouds is an important task for 3D scene\nunderstanding. While dense labeling on 3D data is expensive and time-consuming,\nonly a few works address weakly supervised semantic point cloud segmentation\nmethods to relieve the labeling cost by learning from simpler and cheaper\nlabels. Meanwhile, there are still huge performance gaps between existing\nweakly supervised methods and state-of-the-art fully supervised methods. In\nthis paper, we train a semantic point cloud segmentation network with only a\nsmall portion of points being labeled. We argue that we can better utilize the\nlimited supervision information as we densely propagate the supervision signal\nfrom the labeled points to other points within and across the input samples.\nSpecifically, we propose a cross-sample feature reallocating module to transfer\nsimilar features and therefore re-route the gradients across two samples with\ncommon classes and an intra-sample feature redistribution module to propagate\nsupervision signals on unlabeled points across and within point cloud samples.\nWe conduct extensive experiments on public datasets S3DIS and ScanNet. Our\nweakly supervised method with only 10\\% and 1\\% of labels can produce\ncompatible results with the fully supervised counterpart.",
          "link": "http://arxiv.org/abs/2107.11267",
          "publishedOn": "2021-07-26T02:00:58.405Z",
          "wordCount": 633,
          "title": "Dense Supervision Propagation for Weakly Supervised Semantic Segmentation on 3D Point Clouds. (arXiv:2107.11267v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04726",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kazemi_A/0/1/0/all/0/1\">Ashkan Kazemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garimella_K/0/1/0/all/0/1\">Kiran Garimella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1\">Gautam Kishore Shahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaffney_D/0/1/0/all/0/1\">Devin Gaffney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott A. Hale</a>",
          "description": "There is currently no easy way to fact-check content on WhatsApp and other\nend-to-end encrypted platforms at scale. In this paper, we analyze the\nusefulness of a crowd-sourced \"tipline\" through which users can submit content\n(\"tips\") that they want fact-checked. We compare the tips sent to a WhatsApp\ntipline run during the 2019 Indian national elections with the messages\ncirculating in large, public groups on WhatsApp and other social media\nplatforms during the same period. We find that tiplines are a very useful lens\ninto WhatsApp conversations: a significant fraction of messages and images sent\nto the tipline match with the content being shared on public WhatsApp groups\nand other social media. Our analysis also shows that tiplines cover the most\npopular content well, and a majority of such content is often shared to the\ntipline before appearing in large, public WhatsApp groups. Overall, our\nfindings suggest tiplines can be an effective source for discovering content to\nfact-check.",
          "link": "http://arxiv.org/abs/2106.04726",
          "publishedOn": "2021-07-26T02:00:58.398Z",
          "wordCount": 655,
          "title": "Tiplines to Combat Misinformation on Encrypted Platforms: A Case Study of the 2019 Indian Election on WhatsApp. (arXiv:2106.04726v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ito_K/0/1/0/all/0/1\">Koichi Ito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1\">Filip Biljecki</a>",
          "description": "Studies evaluating bikeability usually compute spatial indicators shaping\ncycling conditions and conflate them in a quantitative index. Much research\ninvolves site visits or conventional geospatial approaches, and few studies\nhave leveraged street view imagery (SVI) for conducting virtual audits. These\nhave assessed a limited range of aspects, and not all have been automated using\ncomputer vision (CV). Furthermore, studies have not yet zeroed in on gauging\nthe usability of these technologies thoroughly. We investigate, with\nexperiments at a fine spatial scale and across multiple geographies (Singapore\nand Tokyo), whether we can use SVI and CV to assess bikeability\ncomprehensively. Extending related work, we develop an exhaustive index of\nbikeability composed of 34 indicators. The results suggest that SVI and CV are\nadequate to evaluate bikeability in cities comprehensively. As they\noutperformed non-SVI counterparts by a wide margin, SVI indicators are also\nfound to be superior in assessing urban bikeability, and potentially can be\nused independently, replacing traditional techniques. However, the paper\nexposes some limitations, suggesting that the best way forward is combining\nboth SVI and non-SVI approaches. The new bikeability index presents a\ncontribution in transportation and urban analytics, and it is scalable to\nassess cycling appeal widely.",
          "link": "http://arxiv.org/abs/2105.08499",
          "publishedOn": "2021-07-26T02:00:58.376Z",
          "wordCount": 656,
          "title": "Assessing bikeability with street view imagery and computer vision. (arXiv:2105.08499v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1\">Siyuan Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Ailing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>",
          "description": "Heatmap-based methods dominate in the field of human pose estimation by\nmodelling the output distribution through likelihood heatmaps. In contrast,\nregression-based methods are more efficient but suffer from inferior\nperformance. In this work, we explore maximum likelihood estimation (MLE) to\ndevelop an efficient and effective regression-based methods. From the\nperspective of MLE, adopting different regression losses is making different\nassumptions about the output density function. A density function closer to the\ntrue distribution leads to a better regression performance. In light of this,\nwe propose a novel regression paradigm with Residual Log-likelihood Estimation\n(RLE) to capture the underlying output distribution. Concretely, RLE learns the\nchange of the distribution instead of the unreferenced underlying distribution\nto facilitate the training process. With the proposed reparameterization\ndesign, our method is compatible with off-the-shelf flow models. The proposed\nmethod is effective, efficient and flexible. We show its potential in various\nhuman pose estimation tasks with comprehensive experiments. Compared to the\nconventional regression paradigm, regression with RLE bring 12.4 mAP\nimprovement on MSCOCO without any test-time overhead. Moreover, for the first\ntime, especially on multi-person pose estimation, our regression method is\nsuperior to the heatmap-based methods. Our code is available at\nhttps://github.com/Jeff-sjtu/res-loglikelihood-regression",
          "link": "http://arxiv.org/abs/2107.11291",
          "publishedOn": "2021-07-26T02:00:58.368Z",
          "wordCount": 647,
          "title": "Human Pose Regression with Residual Log-likelihood Estimation. (arXiv:2107.11291v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vecchio_G/0/1/0/all/0/1\">Giuseppe Vecchio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palazzo_S/0/1/0/all/0/1\">Simone Palazzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spampinato_C/0/1/0/all/0/1\">Concetto Spampinato</a>",
          "description": "In this paper we present SurfaceNet, an approach for estimating\nspatially-varying bidirectional reflectance distribution function (SVBRDF)\nmaterial properties from a single image. We pose the problem as an image\ntranslation task and propose a novel patch-based generative adversarial network\n(GAN) that is able to produce high-quality, high-resolution surface reflectance\nmaps. The employment of the GAN paradigm has a twofold objective: 1) allowing\nthe model to recover finer details than standard translation models; 2)\nreducing the domain shift between synthetic and real data distributions in an\nunsupervised way. An extensive evaluation, carried out on a public benchmark of\nsynthetic and real images under different illumination conditions, shows that\nSurfaceNet largely outperforms existing SVBRDF reconstruction methods, both\nquantitatively and qualitatively. Furthermore, SurfaceNet exhibits a remarkable\nability in generating high-quality maps from real samples without any\nsupervision at training time.",
          "link": "http://arxiv.org/abs/2107.11298",
          "publishedOn": "2021-07-26T02:00:58.361Z",
          "wordCount": 570,
          "title": "SurfaceNet: Adversarial SVBRDF Estimation from a Single Image. (arXiv:2107.11298v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.07240",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Seong Tae Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goli_L/0/1/0/all/0/1\">Leili Goli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paschali_M/0/1/0/all/0/1\">Magdalini Paschali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khakzar_A/0/1/0/all/0/1\">Ashkan Khakzar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keicher_M/0/1/0/all/0/1\">Matthias Keicher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Czempiel_T/0/1/0/all/0/1\">Tobias Czempiel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burian_E/0/1/0/all/0/1\">Egon Burian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Braren_R/0/1/0/all/0/1\">Rickmer Braren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wendler_T/0/1/0/all/0/1\">Thomas Wendler</a>",
          "description": "Chest computed tomography (CT) has played an essential diagnostic role in\nassessing patients with COVID-19 by showing disease-specific image features\nsuch as ground-glass opacity and consolidation. Image segmentation methods have\nproven to help quantify the disease burden and even help predict the outcome.\nThe availability of longitudinal CT series may also result in an efficient and\neffective method to reliably assess the progression of COVID-19, monitor the\nhealing process and the response to different therapeutic strategies. In this\npaper, we propose a new framework to identify infection at a voxel level\n(identification of healthy lung, consolidation, and ground-glass opacity) and\nvisualize the progression of COVID-19 using sequential low-dose non-contrast CT\nscans. In particular, we devise a longitudinal segmentation network that\nutilizes the reference scan information to improve the performance of disease\nidentification. Experimental results on a clinical longitudinal dataset\ncollected in our institution show the effectiveness of the proposed method\ncompared to the static deep neural networks for disease quantification.",
          "link": "http://arxiv.org/abs/2103.07240",
          "publishedOn": "2021-07-26T02:00:58.352Z",
          "wordCount": 688,
          "title": "Longitudinal Quantitative Assessment of COVID-19 Infection Progression from Chest CTs. (arXiv:2103.07240v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elangovan_R/0/1/0/all/0/1\">Reena Elangovan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shubham Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1\">Anand Raghunathan</a>",
          "description": "Precision scaling has emerged as a popular technique to optimize the compute\nand storage requirements of Deep Neural Networks (DNNs). Efforts toward\ncreating ultra-low-precision (sub-8-bit) DNNs suggest that the minimum\nprecision required to achieve a given network-level accuracy varies\nconsiderably across networks, and even across layers within a network,\nrequiring support for variable precision in DNN hardware. Previous proposals\nsuch as bit-serial hardware incur high overheads, significantly diminishing the\nbenefits of lower precision. To efficiently support precision\nre-configurability in DNN accelerators, we introduce an approximate computing\nmethod wherein DNN computations are performed block-wise (a block is a group of\nbits) and re-configurability is supported at the granularity of blocks. Results\nof block-wise computations are composed in an approximate manner to enable\nefficient re-configurability. We design a DNN accelerator that embodies\napproximate blocked computation and propose a method to determine a suitable\napproximation configuration for a given DNN. By varying the approximation\nconfigurations across DNNs, we achieve 1.17x-1.73x and 1.02x-2.04x improvement\nin system energy and performance respectively, over an 8-bit fixed-point (FxP8)\nbaseline, with negligible loss in classification accuracy. Further, by varying\nthe approximation configurations across layers and data-structures within DNNs,\nwe achieve 1.25x-2.42x and 1.07x-2.95x improvement in system energy and\nperformance respectively, with negligible accuracy loss.",
          "link": "http://arxiv.org/abs/2011.13000",
          "publishedOn": "2021-07-26T02:00:58.344Z",
          "wordCount": 676,
          "title": "Ax-BxP: Approximate Blocked Computation for Precision-Reconfigurable Deep Neural Network Acceleration. (arXiv:2011.13000v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Sanghun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwak_D/0/1/0/all/0/1\">Daehoon Gwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Sungha Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>",
          "description": "Identifying unexpected objects on roads in semantic segmentation (e.g.,\nidentifying dogs on roads) is crucial in safety-critical applications. Existing\napproaches use images of unexpected objects from external datasets or require\nadditional training (e.g., retraining segmentation networks or training an\nextra network), which necessitate a non-trivial amount of labor intensity or\nlengthy inference time. One possible alternative is to use prediction scores of\na pre-trained network such as the max logits (i.e., maximum values among\nclasses before the final softmax layer) for detecting such objects. However,\nthe distribution of max logits of each predicted class is significantly\ndifferent from each other, which degrades the performance of identifying\nunexpected objects in urban-scene segmentation. To address this issue, we\npropose a simple yet effective approach that standardizes the max logits in\norder to align the different distributions and reflect the relative meanings of\nmax logits within each predicted class. Moreover, we consider the local regions\nfrom two different perspectives based on the intuition that neighboring pixels\nshare similar semantic information. In contrast to previous approaches, our\nmethod does not utilize any external datasets or require additional training,\nwhich makes our method widely applicable to existing pre-trained segmentation\nmodels. Such a straightforward approach achieves a new state-of-the-art\nperformance on the publicly available Fishyscapes Lost & Found leaderboard with\na large margin.",
          "link": "http://arxiv.org/abs/2107.11264",
          "publishedOn": "2021-07-26T02:00:58.313Z",
          "wordCount": 677,
          "title": "Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation. (arXiv:2107.11264v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reiersen_G/0/1/0/all/0/1\">Gyri Reiersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_D/0/1/0/all/0/1\">David Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1\">Bj&#xf6;rn L&#xfc;tjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1\">Konstantin Klemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoxiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>",
          "description": "Forest carbon offsets are increasingly popular and can play a significant\nrole in financing climate mitigation, forest conservation, and reforestation.\nMeasuring how much carbon is stored in forests is, however, still largely done\nvia expensive, time-consuming, and sometimes unaccountable field measurements.\nTo overcome these limitations, many verification bodies are leveraging machine\nlearning (ML) algorithms to estimate forest carbon from satellite or aerial\nimagery. Aerial imagery allows for tree species or family classification, which\nimproves the satellite imagery-based forest type classification. However,\naerial imagery is significantly more expensive to collect and it is unclear by\nhow much the higher resolution improves the forest carbon estimation. This\nproposal paper describes the first systematic comparison of forest carbon\nestimation from aerial imagery, satellite imagery, and ground-truth field\nmeasurements via deep learning-based algorithms for a tropical reforestation\nproject. Our initial results show that forest carbon estimates from satellite\nimagery can overestimate above-ground biomass by more than 10-times for\ntropical reforestation projects. The significant difference between aerial and\nsatellite-derived forest carbon measurements shows the potential for aerial\nimagery-based ML algorithms and raises the importance to extend this study to a\nglobal benchmark between options for carbon measurements.",
          "link": "http://arxiv.org/abs/2107.11320",
          "publishedOn": "2021-07-26T02:00:58.280Z",
          "wordCount": 663,
          "title": "Tackling the Overestimation of Forest Carbon with Deep Learning and Aerial Imagery. (arXiv:2107.11320v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ewecker_L/0/1/0/all/0/1\">Lukas Ewecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asan_E/0/1/0/all/0/1\">Ebubekir Asan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohnemus_L/0/1/0/all/0/1\">Lars Ohnemus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saralajew_S/0/1/0/all/0/1\">Sascha Saralajew</a>",
          "description": "In recent years, computer vision algorithms have become more and more\npowerful, which enabled technologies such as autonomous driving to evolve with\nrapid pace. However, current algorithms mainly share one limitation: They rely\non directly visible objects. This is a major drawback compared to human\nbehavior, where indirect visual cues caused by the actual object (e.g.,\nshadows) are already used intuitively to retrieve information or anticipate\noccurring objects. While driving at night, this performance deficit becomes\neven more obvious: Humans already process the light artifacts caused by\noncoming vehicles to assume their future appearance, whereas current object\ndetection systems rely on the oncoming vehicle's direct visibility. Based on\nprevious work in this subject, we present with this paper a complete system\ncapable of solving the task to providently detect oncoming vehicles at\nnighttime based on their caused light artifacts. For that, we outline the full\nalgorithm architecture ranging from the detection of light artifacts in the\nimage space, localizing the objects in the three-dimensional space, and\nverifying the objects over time. To demonstrate the applicability, we deploy\nthe system in a test vehicle and use the information of providently detected\nvehicles to control the glare-free high beam system proactively. Using this\nexperimental setting, we quantify the time benefit that the provident vehicle\ndetection system provides compared to an in-production computer vision system.\nAdditionally, the glare-free high beam use case provides a real-time and\nreal-world visualization interface of the detection results. With this\ncontribution, we want to put awareness on the unconventional sensing task of\nprovident object detection and further close the performance gap between human\nbehavior and computer vision algorithms in order to bring autonomous and\nautomated driving a step forward.",
          "link": "http://arxiv.org/abs/2107.11302",
          "publishedOn": "2021-07-26T02:00:58.272Z",
          "wordCount": 724,
          "title": "Provident Vehicle Detection at Night for Advanced Driver Assistance Systems. (arXiv:2107.11302v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruifei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>",
          "description": "While self-training has advanced semi-supervised semantic segmentation, it\nseverely suffers from the long-tailed class distribution on real-world semantic\nsegmentation datasets that make the pseudo-labeled data bias toward majority\nclasses. In this paper, we present a simple and yet effective Distribution\nAlignment and Random Sampling (DARS) method to produce unbiased pseudo labels\nthat match the true class distribution estimated from the labeled data.\nBesides, we also contribute a progressive data augmentation and labeling\nstrategy to facilitate model training with pseudo-labeled data. Experiments on\nboth Cityscapes and PASCAL VOC 2012 datasets demonstrate the effectiveness of\nour approach. Albeit simple, our method performs favorably in comparison with\nstate-of-the-art approaches. Code will be available at\nhttps://github.com/CVMI-Lab/DARS.",
          "link": "http://arxiv.org/abs/2107.11279",
          "publishedOn": "2021-07-26T02:00:58.244Z",
          "wordCount": 558,
          "title": "Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation: A Baseline Investigation. (arXiv:2107.11279v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abid_M/0/1/0/all/0/1\">Mohamed Abderrahmen Abid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedhli_I/0/1/0/all/0/1\">Ihsen Hedhli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalonde_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Lalonde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1\">Christian Gagne</a>",
          "description": "Most image-to-image translation methods focus on learning mappings across\ndomains with the assumption that images share content (e.g., pose) but have\ntheir own domain-specific information known as style. When conditioned on a\ntarget image, such methods aim to extract the style of the target and combine\nit with the content of the source image. In this work, we consider the scenario\nwhere the target image has a very low resolution. More specifically, our\napproach aims at transferring fine details from a high resolution (HR) source\nimage to fit a coarse, low resolution (LR) image representation of the target.\nWe therefore generate HR images that share features from both HR and LR inputs.\nThis differs from previous methods that focus on translating a given image\nstyle into a target content, our translation approach being able to\nsimultaneously imitate the style and merge the structural information of the LR\ntarget. Our approach relies on training the generative model to produce HR\ntarget images that both 1) share distinctive information of the associated\nsource image; 2) correctly match the LR target image when downscaled. We\nvalidate our method on the CelebA-HQ and AFHQ datasets by demonstrating\nimprovements in terms of visual quality, diversity and coverage. Qualitative\nand quantitative results show that when dealing with intra-domain image\ntranslation, our method generates more realistic samples compared to\nstate-of-the-art methods such as Stargan-v2",
          "link": "http://arxiv.org/abs/2107.11262",
          "publishedOn": "2021-07-26T02:00:58.233Z",
          "wordCount": 667,
          "title": "Image-to-Image Translation with Low Resolution Conditioning. (arXiv:2107.11262v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bingqian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yanxin Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>",
          "description": "Language instruction plays an essential role in the natural language grounded\nnavigation tasks. However, navigators trained with limited human-annotated\ninstructions may have difficulties in accurately capturing key information from\nthe complicated instruction at different timesteps, leading to poor navigation\nperformance. In this paper, we exploit to train a more robust navigator which\nis capable of dynamically extracting crucial factors from the long instruction,\nby using an adversarial attacking paradigm. Specifically, we propose a Dynamic\nReinforced Instruction Attacker (DR-Attacker), which learns to mislead the\nnavigator to move to the wrong target by destroying the most instructive\ninformation in instructions at different timesteps. By formulating the\nperturbation generation as a Markov Decision Process, DR-Attacker is optimized\nby the reinforcement learning algorithm to generate perturbed instructions\nsequentially during the navigation, according to a learnable attack score.\nThen, the perturbed instructions, which serve as hard samples, are used for\nimproving the robustness of the navigator with an effective adversarial\ntraining strategy and an auxiliary self-supervised reasoning task. Experimental\nresults on both Vision-and-Language Navigation (VLN) and Navigation from Dialog\nHistory (NDH) tasks show the superiority of our proposed method over\nstate-of-the-art methods. Moreover, the visualization analysis shows the\neffectiveness of the proposed DR-Attacker, which can successfully attack\ncrucial information in the instructions at different timesteps. Code is\navailable at https://github.com/expectorlin/DR-Attacker.",
          "link": "http://arxiv.org/abs/2107.11252",
          "publishedOn": "2021-07-26T02:00:58.226Z",
          "wordCount": 684,
          "title": "Adversarial Reinforced Instruction Attacker for Robust Vision-Language Navigation. (arXiv:2107.11252v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Puchert_P/0/1/0/all/0/1\">Patrik Puchert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1\">Timo Ropinski</a>",
          "description": "We propose the adjacency adaptive graph convolutional long-short term memory\nnetwork (AAGC-LSTM) for human pose estimation from sparse inertial\nmeasurements, obtained from only 6 measurement units. The AAGC-LSTM combines\nboth spatial and temporal dependency in a single network operation. This is\nmade possible by equipping graph convolutions with adjacency adaptivity, which\nalso allows for learning unknown dependencies of the human body joints. To\nfurther boost accuracy, we propose longitudinal loss weighting to consider\nnatural movement patterns, as well as body-aware contralateral data\naugmentation. By combining these contributions, we are able to utilize the\ninherent graph nature of the human body, and can thus outperform the state of\nthe art for human pose estimation from sparse inertial measurements.",
          "link": "http://arxiv.org/abs/2107.11214",
          "publishedOn": "2021-07-26T02:00:58.205Z",
          "wordCount": 558,
          "title": "Human Pose Estimation from Sparse Inertial Measurements through Recurrent Graph Convolution. (arXiv:2107.11214v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Estienne_T/0/1/0/all/0/1\">Th&#xe9;o Estienne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christodoulidis_S/0/1/0/all/0/1\">Stergios Christodoulidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battistella_E/0/1/0/all/0/1\">Enzo Battistella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1\">Th&#xe9;ophraste Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerousseau_M/0/1/0/all/0/1\">Marvin Lerousseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leroy_A/0/1/0/all/0/1\">Amaury Leroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chassagnon_G/0/1/0/all/0/1\">Guillaume Chassagnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revel_M/0/1/0/all/0/1\">Marie-Pierre Revel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paragios_N/0/1/0/all/0/1\">Nikos Paragios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_E/0/1/0/all/0/1\">Eric Deutsch</a>",
          "description": "Explainability of deep neural networks is one of the most challenging and\ninteresting problems in the field. In this study, we investigate the topic\nfocusing on the interpretability of deep learning-based registration methods.\nIn particular, with the appropriate model architecture and using a simple\nlinear projection, we decompose the encoding space, generating a new basis, and\nwe empirically show that this basis captures various decomposed anatomically\naware geometrical transformations. We perform experiments using two different\ndatasets focusing on lungs and hippocampus MRI. We show that such an approach\ncan decompose the highly convoluted latent spaces of registration pipelines in\nan orthogonal space with several interesting properties. We hope that this work\ncould shed some light on a better understanding of deep learning-based\nregistration methods.",
          "link": "http://arxiv.org/abs/2107.11238",
          "publishedOn": "2021-07-26T02:00:58.182Z",
          "wordCount": 591,
          "title": "Exploring Deep Registration Latent Spaces. (arXiv:2107.11238v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wanchaitanawong_N/0/1/0/all/0/1\">Napat Wanchaitanawong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_M/0/1/0/all/0/1\">Masayuki Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shibata_T/0/1/0/all/0/1\">Takashi Shibata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okutomi_M/0/1/0/all/0/1\">Masatoshi Okutomi</a>",
          "description": "The combined use of multiple modalities enables accurate pedestrian detection\nunder poor lighting conditions by using the high visibility areas from these\nmodalities together. The vital assumption for the combination use is that there\nis no or only a weak misalignment between the two modalities. In general,\nhowever, this assumption often breaks in actual situations. Due to this\nassumption's breakdown, the position of the bounding boxes does not match\nbetween the two modalities, resulting in a significant decrease in detection\naccuracy, especially in regions where the amount of misalignment is large. In\nthis paper, we propose a multi-modal Faster-RCNN that is robust against large\nmisalignment. The keys are 1) modal-wise regression and 2) multi-modal IoU for\nmini-batch sampling. To deal with large misalignment, we perform bounding box\nregression for both the RPN and detection-head with both modalities. We also\npropose a new sampling strategy called \"multi-modal mini-batch sampling\" that\nintegrates the IoU for both modalities. We demonstrate that the proposed\nmethod's performance is much better than that of the state-of-the-art methods\nfor data with large misalignment through actual image experiments.",
          "link": "http://arxiv.org/abs/2107.11196",
          "publishedOn": "2021-07-26T02:00:58.174Z",
          "wordCount": 630,
          "title": "Multi-Modal Pedestrian Detection with Large Misalignment Based on Modal-Wise Regression and Multi-Modal IoU. (arXiv:2107.11196v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nitzan_Y/0/1/0/all/0/1\">Yotam Nitzan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_R/0/1/0/all/0/1\">Rinon Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brenner_O/0/1/0/all/0/1\">Ofir Brenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>",
          "description": "We propose a novel method for solving regression tasks using few-shot or weak\nsupervision. At the core of our method is the fundamental observation that GANs\nare incredibly successful at encoding semantic information within their latent\nspace, even in a completely unsupervised setting. For modern generative\nframeworks, this semantic encoding manifests as smooth, linear directions which\naffect image attributes in a disentangled manner. These directions have been\nwidely used in GAN-based image editing. We show that such directions are not\nonly linear, but that the magnitude of change induced on the respective\nattribute is approximately linear with respect to the distance traveled along\nthem. By leveraging this observation, our method turns a pre-trained GAN into a\nregression model, using as few as two labeled samples. This enables solving\nregression tasks on datasets and attributes which are difficult to produce\nquality supervision for. Additionally, we show that the same latent-distances\ncan be used to sort collections of images by the strength of given attributes,\neven in the absence of explicit supervision. Extensive experimental evaluations\ndemonstrate that our method can be applied across a wide range of domains,\nleverage multiple latent direction discovery frameworks, and achieve\nstate-of-the-art results in few-shot and low-supervision settings, even when\ncompared to methods designed to tackle a single task.",
          "link": "http://arxiv.org/abs/2107.11186",
          "publishedOn": "2021-07-26T02:00:58.157Z",
          "wordCount": 657,
          "title": "LARGE: Latent-Based Regression through GAN Semantics. (arXiv:2107.11186v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11191",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Duff_M/0/1/0/all/0/1\">Margaret Duff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campbell_N/0/1/0/all/0/1\">Neill D. F. Campbell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ehrhardt_M/0/1/0/all/0/1\">Matthias J. Ehrhardt</a>",
          "description": "Deep neural network approaches to inverse imaging problems have produced\nimpressive results in the last few years. In this paper, we consider the use of\ngenerative models in a variational regularisation approach to inverse problems.\nThe considered regularisers penalise images that are far from the range of a\ngenerative model that has learned to produce images similar to a training\ndataset. We name this family \\textit{generative regularisers}. The success of\ngenerative regularisers depends on the quality of the generative model and so\nwe propose a set of desired criteria to assess models and guide future\nresearch. In our numerical experiments, we evaluate three common generative\nmodels, autoencoders, variational autoencoders and generative adversarial\nnetworks, against our desired criteria. We also test three different generative\nregularisers on the inverse problems of deblurring, deconvolution, and\ntomography. We show that the success of solutions restricted to lie exactly in\nthe range of the generator is highly dependent on the ability of the generative\nmodel but that allowing small deviations from the range of the generator\nproduces more consistent results.",
          "link": "http://arxiv.org/abs/2107.11191",
          "publishedOn": "2021-07-26T02:00:58.149Z",
          "wordCount": 631,
          "title": "Regularising Inverse Problems with Generative Machine Learning Models. (arXiv:2107.11191v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baumgartl_H/0/1/0/all/0/1\">Hermann Baumgartl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buettner_R/0/1/0/all/0/1\">Ricardo Buettner</a>",
          "description": "We present four different robust transfer learning and data augmentation\nstrategies for robust mobile scene recognition. By training three mobile-ready\n(EfficientNetB0, MobileNetV2, MobileNetV3) and two large-scale baseline (VGG16,\nResNet50) convolutional neural network architectures on the widely available\nEvent8, Scene15, Stanford40, and MIT67 datasets, we show the generalization\nability of our transfer learning strategies. Furthermore, we tested the\nrobustness of our transfer learning strategies under viewpoint and lighting\nchanges using the KTH-Idol2 database. Also, the impact of inference\noptimization techniques on the general performance and the robustness under\ndifferent transfer learning strategies is evaluated. Experimental results show\nthat when employing transfer learning, Fine-Tuning in combination with\nextensive data augmentation improves the general accuracy and robustness in\nmobile scene recognition. We achieved state-of-the-art results using various\nbaseline convolutional neural networks and showed the robustness against\nlighting and viewpoint changes in challenging mobile robot place recognition.",
          "link": "http://arxiv.org/abs/2107.11187",
          "publishedOn": "2021-07-26T02:00:58.141Z",
          "wordCount": 610,
          "title": "Developing efficient transfer learning strategies for robust scene recognition in mobile robotics using pre-trained convolutional neural networks. (arXiv:2107.11187v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abrahamyan_L/0/1/0/all/0/1\">Lusine Abrahamyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziatchin_V/0/1/0/all/0/1\">Valentin Ziatchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1\">Nikos Deligiannis</a>",
          "description": "Compact convolutional neural networks (CNNs) have witnessed exceptional\nimprovements in performance in recent years. However, they still fail to\nprovide the same predictive power as CNNs with a large number of parameters.\nThe diverse and even abundant features captured by the layers is an important\ncharacteristic of these successful CNNs. However, differences in this\ncharacteristic between large CNNs and their compact counterparts have rarely\nbeen investigated. In compact CNNs, due to the limited number of parameters,\nabundant features are unlikely to be obtained, and feature diversity becomes an\nessential characteristic. Diverse features present in the activation maps\nderived from a data point during model inference may indicate the presence of a\nset of unique descriptors necessary to distinguish between objects of different\nclasses. In contrast, data points with low feature diversity may not provide a\nsufficient amount of unique descriptors to make a valid prediction; we refer to\nthem as random predictions. Random predictions can negatively impact the\noptimization process and harm the final performance. This paper proposes\naddressing the problem raised by random predictions by reshaping the standard\ncross-entropy to make it biased toward data points with a limited number of\nunique descriptive features. Our novel Bias Loss focuses the training on a set\nof valuable data points and prevents the vast number of samples with poor\nlearning features from misleading the optimization process. Furthermore, to\nshow the importance of diversity, we present a family of SkipNet models whose\narchitectures are brought to boost the number of unique descriptors in the last\nlayers. Our Skipnet-M can achieve 1% higher classification accuracy than\nMobileNetV3 Large.",
          "link": "http://arxiv.org/abs/2107.11170",
          "publishedOn": "2021-07-26T02:00:58.129Z",
          "wordCount": 709,
          "title": "Bias Loss for Mobile Neural Networks. (arXiv:2107.11170v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11099",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Jing_Y/0/1/0/all/0/1\">Yu Jing</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wu_C/0/1/0/all/0/1\">Chonghang Wu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Fu_W/0/1/0/all/0/1\">Wenbing Fu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_X/0/1/0/all/0/1\">Xiaogang Li</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>",
          "description": "With the rapid growth of qubit numbers and coherence times in quantum\nhardware technology, implementing shallow neural networks on the so-called\nNoisy Intermediate-Scale Quantum (NISQ) devices has attracted a lot of\ninterest. Many quantum (convolutional) circuit ansaetze are proposed for\ngrayscale images classification tasks with promising empirical results.\nHowever, when applying these ansaetze on RGB images, the intra-channel\ninformation that is useful for vision tasks is not extracted effectively. In\nthis paper, we propose two types of quantum circuit ansaetze to simulate\nconvolution operations on RGB images, which differ in the way how inter-channel\nand intra-channel information are extracted. To the best of our knowledge, this\nis the first work of a quantum convolutional circuit to deal with RGB images\neffectively, with a higher test accuracy compared to the purely classical CNNs.\nWe also investigate the relationship between the size of quantum circuit ansatz\nand the learnability of the hybrid quantum-classical convolutional neural\nnetwork. Through experiments based on CIFAR-10 and MNIST datasets, we\ndemonstrate that a larger size of the quantum circuit ansatz improves\npredictive performance in multiclass classification tasks, providing useful\ninsights for near term quantum algorithm developments.",
          "link": "http://arxiv.org/abs/2107.11099",
          "publishedOn": "2021-07-26T02:00:58.113Z",
          "wordCount": 633,
          "title": "RGB Image Classification with Quantum Convolutional Ansaetze. (arXiv:2107.11099v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sanghun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">SeungKyu Lee</a>",
          "description": "Due to the unstable nature of minimax game between generator and\ndiscriminator, improving the performance of GANs is a challenging task. Recent\nstudies have shown that selected high-quality samples in training improve the\nperformance of GANs. However, sampling approaches which discard samples show\nlimitations in some aspects such as the speed of training and optimality of the\nnetworks. In this paper we propose unrealistic feature suppression (UFS) module\nthat keeps high-quality features and suppresses unrealistic features. UFS\nmodule keeps the training stability of networks and improves the quality of\ngenerated images. We demonstrate the effectiveness of UFS module on various\nmodels such as WGAN-GP, SNGAN, and BigGAN. By using UFS module, we achieved\nbetter Frechet inception distance and inception score compared to various\nbaseline models. We also visualize how effectively our UFS module suppresses\nunrealistic features through class activation maps.",
          "link": "http://arxiv.org/abs/2107.11047",
          "publishedOn": "2021-07-26T02:00:58.106Z",
          "wordCount": 575,
          "title": "Unrealistic Feature Suppression for Generative Adversarial Networks. (arXiv:2107.11047v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jianxiong Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Feiying Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "Image inpainting aims to complete the missing or corrupted regions of images\nwith realistic contents. The prevalent approaches adopt a hybrid objective of\nreconstruction and perceptual quality by using generative adversarial networks.\nHowever, the reconstruction loss and adversarial loss focus on synthesizing\ncontents of different frequencies and simply applying them together often leads\nto inter-frequency conflicts and compromised inpainting. This paper presents\nWaveFill, a wavelet-based inpainting network that decomposes images into\nmultiple frequency bands and fills the missing regions in each frequency band\nseparately and explicitly. WaveFill decomposes images by using discrete wavelet\ntransform (DWT) that preserves spatial information naturally. It applies L1\nreconstruction loss to the decomposed low-frequency bands and adversarial loss\nto high-frequency bands, hence effectively mitigate inter-frequency conflicts\nwhile completing images in spatial domain. To address the inpainting\ninconsistency in different frequency bands and fuse features with distinct\nstatistics, we design a novel normalization scheme that aligns and fuses the\nmulti-frequency features effectively. Extensive experiments over multiple\ndatasets show that WaveFill achieves superior image inpainting qualitatively\nand quantitatively.",
          "link": "http://arxiv.org/abs/2107.11027",
          "publishedOn": "2021-07-26T02:00:58.098Z",
          "wordCount": 625,
          "title": "WaveFill: A Wavelet-based Generation Network for Image Inpainting. (arXiv:2107.11027v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassanin_M/0/1/0/all/0/1\">Mohammed Hassanin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radwan_I/0/1/0/all/0/1\">Ibrahim Radwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahtali_M/0/1/0/all/0/1\">Murat Tahtali</a>",
          "description": "Multi-label recognition is a fundamental, and yet is a challenging task in\ncomputer vision. Recently, deep learning models have achieved great progress\ntowards learning discriminative features from input images. However,\nconventional approaches are unable to model the inter-class discrepancies among\nfeatures in multi-label images, since they are designed to work for image-level\nfeature discrimination. In this paper, we propose a unified deep network to\nlearn discriminative features for the multi-label task. Given a multi-label\nimage, the proposed method first disentangles features corresponding to\ndifferent classes. Then, it discriminates between these classes via increasing\nthe inter-class distance while decreasing the intra-class differences in the\noutput space. By regularizing the whole network with the proposed loss, the\nperformance of applying the wellknown ResNet-101 is improved significantly.\nExtensive experiments have been performed on COCO-2014, VOC2007 and VOC2012\ndatasets, which demonstrate that the proposed method outperforms\nstate-of-the-art approaches by a significant margin of 3:5% on large-scale COCO\ndataset. Moreover, analysis of the discriminative feature learning approach\nshows that it can be plugged into various types of multi-label methods as a\ngeneral module.",
          "link": "http://arxiv.org/abs/2107.11159",
          "publishedOn": "2021-07-26T02:00:58.091Z",
          "wordCount": 611,
          "title": "Learning Discriminative Representations for Multi-Label Image Recognition. (arXiv:2107.11159v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shin_I/0/1/0/all/0/1\">Inkyu Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kwanyong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sanghyun Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>",
          "description": "Unsupervised Domain Adaptation for semantic segmentation has gained immense\npopularity since it can transfer knowledge from simulation to real (Sim2Real)\nby largely cutting out the laborious per pixel labeling efforts at real. In\nthis work, we present a new video extension of this task, namely Unsupervised\nDomain Adaptation for Video Semantic Segmentation. As it became easy to obtain\nlarge-scale video labels through simulation, we believe attempting to maximize\nSim2Real knowledge transferability is one of the promising directions for\nresolving the fundamental data-hungry issue in the video. To tackle this new\nproblem, we present a novel two-phase adaptation scheme. In the first step, we\nexhaustively distill source domain knowledge using supervised loss functions.\nSimultaneously, video adversarial training (VAT) is employed to align the\nfeatures from source to target utilizing video context. In the second step, we\napply video self-training (VST), focusing only on the target data. To construct\nrobust pseudo labels, we exploit the temporal information in the video, which\nhas been rarely explored in the previous image-based self-training approaches.\nWe set strong baseline scores on 'VIPER to CityscapeVPS' adaptation scenario.\nWe show that our proposals significantly outperform previous image-based UDA\nmethods both on image-level (mIoU) and video-level (VPQ) evaluation metrics.",
          "link": "http://arxiv.org/abs/2107.11052",
          "publishedOn": "2021-07-26T02:00:58.057Z",
          "wordCount": 634,
          "title": "Unsupervised Domain Adaptation for Video Semantic Segmentation. (arXiv:2107.11052v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11119",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xinyang Wu</a>",
          "description": "Before analy z ing the CT image, it is very important to segment the heart\nimage, and the left ve ntricular (LV) inner and outer membrane segmentation is\none of the most important contents. However, manual segmentation is tedious and\ntime consuming. In order to facilitate doctors to focus on high tech tasks such\nas disease analysis and diagnosis, it is crucial to develop a fast and accurate\nsegmentation method [1]. In view of this phenomenon, this paper uses distance\nregularized level set (DRL SE) to explore the segmentation effect of epicardium\nand endocardium 2 ]], which includes a distance regula riz ed t erm and an\nexternal energy term. Finally, five CT images are used to verify the proposed\nmethod, and image quality evaluation indexes such as dice score and Hausdorff\ndistance are used to evaluate the segmentation effect. The results showed that\nthe me tho d could separate the inner and outer membrane very well (endocardium\ndice = 0.9253, Hausdorff = 7.8740; epicardium Hausdorff = 0.9687, Hausdorff = 6 .",
          "link": "http://arxiv.org/abs/2107.11119",
          "publishedOn": "2021-07-26T02:00:58.039Z",
          "wordCount": 611,
          "title": "Cardiac CT segmentation based on distance regularized level set. (arXiv:2107.11119v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Senanayake_R/0/1/0/all/0/1\">Ransalu Senanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hatch_K/0/1/0/all/0/1\">Kyle Beltran Hatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jason Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel J. Kochenderfer</a>",
          "description": "Future urban transportation concepts include a mixture of ground and air\nvehicles with varying degrees of autonomy in a congested environment. In such\ndynamic environments, occupancy maps alone are not sufficient for safe path\nplanning. Safe and efficient transportation requires reasoning about the 3D\nflow of traffic and properly modeling uncertainty. Several different approaches\ncan be taken for developing 3D velocity maps. This paper explores a Bayesian\napproach that captures our uncertainty in the map given training data. The\napproach involves projecting spatial coordinates into a high-dimensional\nfeature space and then applying Bayesian linear regression to make predictions\nand quantify uncertainty in our estimates. On a collection of air and ground\ndatasets, we demonstrate that this approach is effective and more scalable than\nseveral alternative approaches.",
          "link": "http://arxiv.org/abs/2107.11039",
          "publishedOn": "2021-07-26T02:00:58.022Z",
          "wordCount": 582,
          "title": "3D Radar Velocity Maps for Uncertain Dynamic Environments. (arXiv:2107.11039v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Puchert_P/0/1/0/all/0/1\">Patrik Puchert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermosilla_P/0/1/0/all/0/1\">Pedro Hermosilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1\">Tobias Ritschel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1\">Timo Ropinski</a>",
          "description": "Density estimation plays a crucial role in many data analysis tasks, as it\ninfers a continuous probability density function (PDF) from discrete samples.\nThus, it is used in tasks as diverse as analyzing population data, spatial\nlocations in 2D sensor readings, or reconstructing scenes from 3D scans. In\nthis paper, we introduce a learned, data-driven deep density estimation (DDE)\nto infer PDFs in an accurate and efficient manner, while being independent of\ndomain dimensionality or sample size. Furthermore, we do not require access to\nthe original PDF during estimation, neither in parametric form, nor as priors,\nor in the form of many samples. This is enabled by training an unstructured\nconvolutional neural network on an infinite stream of synthetic PDFs, as\nunbound amounts of synthetic training data generalize better across a deck of\nnatural PDFs than any natural finite training data will do. Thus, we hope that\nour publicly available DDE method will be beneficial in many areas of data\nanalysis, where continuous models are to be estimated from discrete\nobservations.",
          "link": "http://arxiv.org/abs/2107.11085",
          "publishedOn": "2021-07-26T02:00:58.013Z",
          "wordCount": 629,
          "title": "Data-driven deep density estimation. (arXiv:2107.11085v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1\">Pinzhuo Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yao Gao</a>",
          "description": "Meta-learning provides a promising way for learning to efficiently learn and\nachieves great success in many applications. However, most meta-learning\nliterature focuses on dealing with tasks from a same domain, making it brittle\nto generalize to tasks from the other unseen domains. In this work, we address\nthis problem by simulating tasks from the other unseen domains to improve the\ngeneralization and robustness of meta-learning method. Specifically, we propose\na model-agnostic shift layer to learn how to simulate the domain shift and\ngenerate pseudo tasks, and develop a new adversarial learning-to-learn\nmechanism to train it. Based on the pseudo tasks, the meta-learning model can\nlearn cross-domain meta-knowledge, which can generalize well on unseen domains.\nWe conduct extensive experiments under the domain generalization setting.\nExperimental results demonstrate that the proposed shift layer is applicable to\nvarious meta-learning frameworks. Moreover, our method also leads to\nstate-of-the-art performance on different cross-domain few-shot classification\nbenchmarks and produces good results on cross-domain few-shot regression.",
          "link": "http://arxiv.org/abs/2107.11056",
          "publishedOn": "2021-07-26T02:00:58.005Z",
          "wordCount": 600,
          "title": "Improving the Generalization of Meta-learning on Unseen Domains via Adversarial Shift. (arXiv:2107.11056v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengya Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mobarakol Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_C/0/1/0/all/0/1\">Chwee Ming Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongliang Ren</a>",
          "description": "Generating surgical reports aimed at surgical scene understanding in\nrobot-assisted surgery can contribute to documenting entry tasks and\npost-operative analysis. Despite the impressive outcome, the deep learning\nmodel degrades the performance when applied to different domains encountering\ndomain shifts. In addition, there are new instruments and variations in\nsurgical tissues appeared in robotic surgery. In this work, we propose\nclass-incremental domain adaptation (CIDA) with a multi-layer transformer-based\nmodel to tackle the new classes and domain shift in the target domain to\ngenerate surgical reports during robotic surgery. To adapt incremental classes\nand extract domain invariant features, a class-incremental (CI) learning method\nwith supervised contrastive (SupCon) loss is incorporated with a feature\nextractor. To generate caption from the extracted feature, curriculum by\none-dimensional gaussian smoothing (CBS) is integrated with a multi-layer\ntransformer-based caption prediction model. CBS smoothes the features embedding\nusing anti-aliasing and helps the model to learn domain invariant features. We\nalso adopt label smoothing (LS) to calibrate prediction probability and obtain\nbetter feature representation with both feature extractor and captioning model.\nThe proposed techniques are empirically evaluated by using the datasets of two\nsurgical domains, such as nephrectomy operations and transoral robotic surgery.\nWe observe that domain invariant feature learning and the well-calibrated\nnetwork improves the surgical report generation performance in both source and\ntarget domain under domain shift and unseen classes in the manners of one-shot\nand few-shot learning. The code is publicly available at\nhttps://github.com/XuMengyaAmy/CIDACaptioning.",
          "link": "http://arxiv.org/abs/2107.11091",
          "publishedOn": "2021-07-26T02:00:57.979Z",
          "wordCount": 695,
          "title": "Class-Incremental Domain Adaptation with Smoothing and Calibration for Surgical Report Generation. (arXiv:2107.11091v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koprinkova_Hristova_P/0/1/0/all/0/1\">Petia Koprinkova-Hristova</a>",
          "description": "The paper proposes a novel approach for gray scale images segmentation. It is\nbased on multiple features extraction from single feature per image pixel,\nnamely its intensity value, using Echo state network. The newly extracted\nfeatures -- reservoir equilibrium states -- reveal hidden image characteristics\nthat improve its segmentation via a clustering algorithm. Moreover, it was\ndemonstrated that the intrinsic plasticity tuning of reservoir fits its\nequilibrium states to the original image intensity distribution thus allowing\nfor its better segmentation. The proposed approach is tested on the benchmark\nimage Lena.",
          "link": "http://arxiv.org/abs/2107.11077",
          "publishedOn": "2021-07-26T02:00:57.971Z",
          "wordCount": 541,
          "title": "Reservoir Computing Approach for Gray Images Segmentation. (arXiv:2107.11077v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shasha Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1\">Guanghui Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Licheng Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_S/0/1/0/all/0/1\">Shuiping Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Lin Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Boxin Shi</a>",
          "description": "By utilizing label distribution learning, a probability distribution is\nassigned for a facial image to express a compound emotion, which effectively\nimproves the problem of label uncertainties and noises occurred in one-hot\nlabels. In practice, it is observed that correlations among emotions are\ninherently different, such as surprised and happy emotions are more possibly\nsynchronized than surprised and neutral. It indicates the correlation may be\ncrucial for obtaining a reliable label distribution. Based on this, we propose\na new method that amends the label distribution of each facial image by\nleveraging correlations among expressions in the semantic space. Inspired by\ninherently diverse correlations among word2vecs, the topological information\namong facial expressions is firstly explored in the semantic space, and each\nimage is embedded into the semantic space. Specially, a class-relation graph is\nconstructed to transfer the semantic correlation among expressions into the\ntask space. By comparing semantic and task class-relation graphs of each image,\nthe confidence of its label distribution is evaluated. Based on the confidence,\nthe label distribution is amended by enhancing samples with higher confidence\nand weakening samples with lower confidence. Experimental results demonstrate\nthe proposed method is more effective than compared state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.11061",
          "publishedOn": "2021-07-26T02:00:57.963Z",
          "wordCount": 644,
          "title": "Label Distribution Amendment with Emotional Semantic Correlations for Facial Expression Recognition. (arXiv:2107.11061v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zhongqi Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qianru Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>",
          "description": "Existing Unsupervised Domain Adaptation (UDA) literature adopts the covariate\nshift and conditional shift assumptions, which essentially encourage models to\nlearn common features across domains. However, due to the lack of supervision\nin the target domain, they suffer from the semantic loss: the feature will\ninevitably lose non-discriminative semantics in source domain, which is however\ndiscriminative in target domain. We use a causal view -- transportability\ntheory -- to identify that such loss is in fact a confounding effect, which can\nonly be removed by causal intervention. However, the theoretical solution\nprovided by transportability is far from practical for UDA, because it requires\nthe stratification and representation of an unobserved confounder that is the\ncause of the domain gap. To this end, we propose a practical solution:\nTransporting Causal Mechanisms (TCM), to identify the confounder stratum and\nrepresentations by using the domain-invariant disentangled causal mechanisms,\nwhich are discovered in an unsupervised fashion. Our TCM is both theoretically\nand empirically grounded. Extensive experiments show that TCM achieves\nstate-of-the-art performance on three challenging UDA benchmarks: ImageCLEF-DA,\nOffice-Home, and VisDA-2017. Codes are available in Appendix.",
          "link": "http://arxiv.org/abs/2107.11055",
          "publishedOn": "2021-07-26T02:00:57.955Z",
          "wordCount": 619,
          "title": "Transporting Causal Mechanisms for Unsupervised Domain Adaptation. (arXiv:2107.11055v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11010",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hu_B/0/1/0/all/0/1\">Bowen Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lei_B/0/1/0/all/0/1\">Baiying Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gan_M/0/1/0/all/0/1\">Min Gan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1\">Bingchuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuqiang Wang</a>",
          "description": "3D shape reconstruction is essential in the navigation of minimally-invasive\nand auto robot-guided surgeries whose operating environments are indirect and\nnarrow, and there have been some works that focused on reconstructing the 3D\nshape of the surgical organ through limited 2D information available. However,\nthe lack and incompleteness of such information caused by intraoperative\nemergencies (such as bleeding) and risk control conditions have not been\nconsidered. In this paper, a novel hierarchical shape-perception network (HSPN)\nis proposed to reconstruct the 3D point clouds (PCs) of specific brains from\none single incomplete image with low latency. A tree-structured predictor and\nseveral hierarchical attention pipelines are constructed to generate point\nclouds that accurately describe the incomplete images and then complete these\npoint clouds with high quality. Meanwhile, attention gate blocks (AGBs) are\ndesigned to efficiently aggregate geometric local features of incomplete PCs\ntransmitted by hierarchical attention pipelines and internal features of\nreconstructing point clouds. With the proposed HSPN, 3D shape perception and\ncompletion can be achieved spontaneously. Comprehensive results measured by\nChamfer distance and PC-to-PC error demonstrate that the performance of the\nproposed HSPN outperforms other competitive methods in terms of qualitative\ndisplays, quantitative experiment, and classification evaluation.",
          "link": "http://arxiv.org/abs/2107.11010",
          "publishedOn": "2021-07-26T02:00:57.947Z",
          "wordCount": 653,
          "title": "3D Brain Reconstruction by Hierarchical Shape-Perception Network from a Single Incomplete Image. (arXiv:2107.11010v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jae Won Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dong-Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1\">Yunjae Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>",
          "description": "Recent state-of-the-art active learning methods have mostly leveraged\nGenerative Adversarial Networks (GAN) for sample acquisition; however, GAN is\nusually known to suffer from instability and sensitivity to hyper-parameters.\nIn contrast to these methods, we propose in this paper a novel active learning\nframework that we call Maximum Classifier Discrepancy for Active Learning\n(MCDAL) which takes the prediction discrepancies between multiple classifiers.\nIn particular, we utilize two auxiliary classification layers that learn\ntighter decision boundaries by maximizing the discrepancies among them.\nIntuitively, the discrepancies in the auxiliary classification layers'\npredictions indicate the uncertainty in the prediction. In this regard, we\npropose a novel method to leverage the classifier discrepancies for the\nacquisition function for active learning. We also provide an interpretation of\nour idea in relation to existing GAN based active learning methods and domain\nadaptation frameworks. Moreover, we empirically demonstrate the utility of our\napproach where the performance of our approach exceeds the state-of-the-art\nmethods on several image classification and semantic segmentation datasets in\nactive learning setups.",
          "link": "http://arxiv.org/abs/2107.11049",
          "publishedOn": "2021-07-26T02:00:57.938Z",
          "wordCount": 614,
          "title": "MCDAL: Maximum Classifier Discrepancy for Active Learning. (arXiv:2107.11049v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11022",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yao_K/0/1/0/all/0/1\">Kai Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jie Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jude_C/0/1/0/all/0/1\">Curran Jude</a>",
          "description": "We consider unsupervised cell nuclei segmentation in this paper. Exploiting\nthe recently-proposed unpaired image-to-image translation between cell nuclei\nimages and randomly synthetic masks, existing approaches, e.g., CycleGAN, have\nachieved encouraging results. However, these methods usually take a two-stage\npipeline and fail to learn end-to-end in cell nuclei images. More seriously,\nthey could lead to the lossy transformation problem, i.e., the content\ninconsistency between the original images and the corresponding segmentation\noutput. To address these limitations, we propose a novel end-to-end\nunsupervised framework called Aligned Disentangling Generative Adversarial\nNetwork (AD-GAN). Distinctively, AD-GAN introduces representation\ndisentanglement to separate content representation (the underling spatial\nstructure) from style representation (the rendering of the structure). With\nthis framework, spatial structure can be preserved explicitly, enabling a\nsignificant reduction of macro-level lossy transformation. We also propose a\nnovel training algorithm able to align the disentangled content in the latent\nspace to reduce micro-level lossy transformation. Evaluations on real-world 2D\nand 3D datasets show that AD-GAN substantially outperforms the other comparison\nmethods and the professional software both quantitatively and qualitatively.\nSpecifically, the proposed AD-GAN leads to significant improvement over the\ncurrent best unsupervised methods by an average 17.8% relatively (w.r.t. the\nmetric DICE) on four cell nuclei datasets. As an unsupervised method, AD-GAN\neven performs competitive with the best supervised models, taking a further\nleap towards end-to-end unsupervised nuclei segmentation.",
          "link": "http://arxiv.org/abs/2107.11022",
          "publishedOn": "2021-07-26T02:00:57.916Z",
          "wordCount": 674,
          "title": "AD-GAN: End-to-end Unsupervised Nuclei Segmentation with Aligned Disentangling Training. (arXiv:2107.11022v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattarai_M/0/1/0/all/0/1\">Manish Bhattarai</a>",
          "description": "We present a new four-pronged approach to build firefighter's situational\nawareness for the first time in the literature. We construct a series of deep\nlearning frameworks built on top of one another to enhance the safety,\nefficiency, and successful completion of rescue missions conducted by\nfirefighters in emergency first response settings. First, we used a deep\nConvolutional Neural Network (CNN) system to classify and identify objects of\ninterest from thermal imagery in real-time. Next, we extended this CNN\nframework for object detection, tracking, segmentation with a Mask RCNN\nframework, and scene description with a multimodal natural language\nprocessing(NLP) framework. Third, we built a deep Q-learning-based agent,\nimmune to stress-induced disorientation and anxiety, capable of making clear\nnavigation decisions based on the observed and stored facts in live-fire\nenvironments. Finally, we used a low computational unsupervised learning\ntechnique called tensor decomposition to perform meaningful feature extraction\nfor anomaly detection in real-time. With these ad-hoc deep learning structures,\nwe built the artificial intelligence system's backbone for firefighters'\nsituational awareness. To bring the designed system into usage by firefighters,\nwe designed a physical structure where the processed results are used as inputs\nin the creation of an augmented reality capable of advising firefighters of\ntheir location and key features around them, which are vital to the rescue\noperation at hand, as well as a path planning feature that acts as a virtual\nguide to assist disoriented first responders in getting back to safety. When\ncombined, these four approaches present a novel approach to information\nunderstanding, transfer, and synthesis that could dramatically improve\nfirefighter response and efficacy and reduce life loss.",
          "link": "http://arxiv.org/abs/2107.11043",
          "publishedOn": "2021-07-26T02:00:57.908Z",
          "wordCount": 709,
          "title": "Integrating Deep Learning and Augmented Reality to Enhance Situational Awareness in Firefighting Environments. (arXiv:2107.11043v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junyeop Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoonsik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seonghyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yim_M/0/1/0/all/0/1\">Moonbin Yim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seung Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gayoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungrae Park</a>",
          "description": "Scene text editing (STE), which converts a text in a scene image into the\ndesired text while preserving an original style, is a challenging task due to a\ncomplex intervention between text and style. To address this challenge, we\npropose a novel representational learning-based STE model, referred to as\nRewriteNet that employs textual information as well as visual information. We\nassume that the scene text image can be decomposed into content and style\nfeatures where the former represents the text information and style represents\nscene text characteristics such as font, alignment, and background. Under this\nassumption, we propose a method to separately encode content and style features\nof the input image by introducing the scene text recognizer that is trained by\ntext information. Then, a text-edited image is generated by combining the style\nfeature from the original image and the content feature from the target text.\nUnlike previous works that are only able to use synthetic images in the\ntraining phase, we also exploit real-world images by proposing a\nself-supervised training scheme, which bridges the domain gap between synthetic\nand real data. Our experiments demonstrate that RewriteNet achieves better\nquantitative and qualitative performance than other comparisons. Moreover, we\nvalidate that the use of text information and the self-supervised training\nscheme improves text switching performance. The implementation and dataset will\nbe publicly available.",
          "link": "http://arxiv.org/abs/2107.11041",
          "publishedOn": "2021-07-26T02:00:57.900Z",
          "wordCount": 671,
          "title": "RewriteNet: Realistic Scene Text Image Generation via Editing Text in Real-world Image. (arXiv:2107.11041v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11007",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yixiao Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_R/0/1/0/all/0/1\">Ran Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_K/0/1/0/all/0/1\">Kaixuan Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Ying Fu</a>",
          "description": "Recovering an underlying image from under-sampled measurements, Compressive\nSensing Imaging (CSI) is a challenging problem and has many practical\napplications. Recently, deep neural networks have been applied to this problem\nwith promising results, owing to its implicitly learned prior to alleviate the\nill-poseness of CSI. However, existing neural network approaches require\nseparate models for each imaging parameter like sampling ratios, leading to\ntraining difficulties and overfitting to specific settings. In this paper, we\npresent a dynamic proximal unrolling network (dubbed DPUNet), which can handle\na variety of measurement matrices via one single model without retraining.\nSpecifically, DPUNet can exploit both embedded physical model via gradient\ndescent and imposing image prior with learned dynamic proximal mapping leading\nto joint reconstruction. A key component of DPUNet is a dynamic proximal\nmapping module, whose parameters can be dynamically adjusted at inference stage\nand make it adapt to any given imaging setting. Experimental results\ndemonstrate that the proposed DPUNet can effectively handle multiple CSI\nmodalities under varying sampling ratios and noise levels with only one model,\nand outperform the state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2107.11007",
          "publishedOn": "2021-07-26T02:00:57.877Z",
          "wordCount": 622,
          "title": "Dynamic Proximal Unrolling Network for Compressive Sensing Imaging. (arXiv:2107.11007v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zobeidi_E/0/1/0/all/0/1\">Ehsan Zobeidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanasov_N/0/1/0/all/0/1\">Nikolay Atanasov</a>",
          "description": "Neural networks that map 3D coordinates to signed distance function (SDF) or\noccupancy values have enabled high-fidelity implicit representations of object\nshape. This paper develops a new shape model that allows synthesizing novel\ndistance views by optimizing a continuous signed directional distance function\n(SDDF). Similar to deep SDF models, our SDDF formulation can represent whole\ncategories of shapes and complete or interpolate across shapes from partial\ninput data. Unlike an SDF, which measures distance to the nearest surface in\nany direction, an SDDF measures distance in a given direction. This allows\ntraining an SDDF model without 3D shape supervision, using only distance\nmeasurements, readily available from depth camera or Lidar sensors. Our model\nalso removes post-processing steps like surface extraction or rendering by\ndirectly predicting distance at arbitrary locations and viewing directions.\nUnlike deep view-synthesis techniques, such as Neural Radiance Fields, which\ntrain high-capacity black-box models, our model encodes by construction the\nproperty that SDDF values decrease linearly along the viewing direction. This\nstructure constraint not only results in dimensionality reduction but also\nprovides analytical confidence about the accuracy of SDDF predictions,\nregardless of the distance to the object surface.",
          "link": "http://arxiv.org/abs/2107.11024",
          "publishedOn": "2021-07-26T02:00:57.868Z",
          "wordCount": 626,
          "title": "A Deep Signed Directional Distance Function for Object Shape Representation. (arXiv:2107.11024v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11001",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Goyal_B/0/1/0/all/0/1\">Bhavya Goyal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_M/0/1/0/all/0/1\">Mohit Gupta</a>",
          "description": "Scene understanding under low-light conditions is a challenging problem. This\nis due to the small number of photons captured by the camera and the resulting\nlow signal-to-noise ratio (SNR). Single-photon cameras (SPCs) are an emerging\nsensing modality that are capable of capturing images with high sensitivity.\nDespite having minimal read-noise, images captured by SPCs in photon-starved\nconditions still suffer from strong shot noise, preventing reliable scene\ninference. We propose photon scale-space a collection of high-SNR images\nspanning a wide range of photons-per-pixel (PPP) levels (but same scene\ncontent) as guides to train inference model on low photon flux images. We\ndevelop training techniques that push images with different illumination levels\ncloser to each other in feature representation space. The key idea is that\nhaving a spectrum of different brightness levels during training enables\neffective guidance, and increases robustness to shot noise even in extreme\nnoise cases. Based on the proposed approach, we demonstrate, via simulations\nand real experiments with a SPAD camera, high-performance on various inference\ntasks such as image classification and monocular depth estimation under ultra\nlow-light, down to < 1 PPP.",
          "link": "http://arxiv.org/abs/2107.11001",
          "publishedOn": "2021-07-26T02:00:57.849Z",
          "wordCount": 621,
          "title": "Photon-Starved Scene Inference using Single Photon Cameras. (arXiv:2107.11001v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mousavi_M/0/1/0/all/0/1\">Mehdi Mousavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Estrada_R/0/1/0/all/0/1\">Rolando Estrada</a>",
          "description": "Transparent objects are a very challenging problem in computer vision. They\nare hard to segment or classify due to their lack of precise boundaries, and\nthere is limited data available for training deep neural networks. As such,\ncurrent solutions for this problem employ rigid synthetic datasets, which lack\nflexibility and lead to severe performance degradation when deployed on\nreal-world scenarios. In particular, these synthetic datasets omit features\nsuch as refraction, dispersion and caustics due to limitations in the rendering\npipeline. To address this issue, we present SuperCaustics, a real-time,\nopen-source simulation of transparent objects designed for deep learning\napplications. SuperCaustics features extensive modules for stochastic\nenvironment creation; uses hardware ray-tracing to support caustics,\ndispersion, and refraction; and enables generating massive datasets with\nmulti-modal, pixel-perfect ground truth annotations. To validate our proposed\nsystem, we trained a deep neural network from scratch to segment transparent\nobjects in difficult lighting scenarios. Our neural network achieved\nperformance comparable to the state-of-the-art on a real-world dataset using\nonly 10% of the training data and in a fraction of the training time. Further\nexperiments show that a model trained with SuperCaustics can segment different\ntypes of caustics, even in images with multiple overlapping transparent\nobjects. To the best of our knowledge, this is the first such result for a\nmodel trained on synthetic data. Both our open-source code and experimental\ndata are freely available online.",
          "link": "http://arxiv.org/abs/2107.11008",
          "publishedOn": "2021-07-26T02:00:57.838Z",
          "wordCount": 665,
          "title": "SuperCaustics: Real-time, open-source simulation of transparent objects for deep learning applications. (arXiv:2107.11008v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>",
          "description": "Video semantic segmentation is an essential task for the analysis and\nunderstanding of videos. Recent efforts largely focus on supervised video\nsegmentation by learning from fully annotated data, but the learnt models often\nexperience clear performance drop while applied to videos of a different\ndomain. This paper presents DA-VSN, a domain adaptive video segmentation\nnetwork that addresses domain gaps in videos by temporal consistency\nregularization (TCR) for consecutive frames of target-domain videos. DA-VSN\nconsists of two novel and complementary designs. The first is cross-domain TCR\nthat guides the prediction of target frames to have similar temporal\nconsistency as that of source frames (learnt from annotated source data) via\nadversarial learning. The second is intra-domain TCR that guides unconfident\npredictions of target frames to have similar temporal consistency as confident\npredictions of target frames. Extensive experiments demonstrate the superiority\nof our proposed domain adaptive video segmentation network which outperforms\nmultiple baselines consistently by large margins.",
          "link": "http://arxiv.org/abs/2107.11004",
          "publishedOn": "2021-07-26T02:00:57.815Z",
          "wordCount": 601,
          "title": "Domain Adaptive Video Segmentation via Temporal Consistency Regularization. (arXiv:2107.11004v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xue Liu</a>",
          "description": "We propose pruning ternary quantization (PTQ), a simple, yet effective,\nsymmetric ternary quantization method. The method significantly compresses\nneural network weights to a sparse ternary of [-1,0,1] and thus reduces\ncomputational, storage, and memory footprints. We show that PTQ can convert\nregular weights to ternary orthonormal bases by simply using pruning and L2\nprojection. In addition, we introduce a refined straight-through estimator to\nfinalize and stabilize the quantized weights. Our method can provide at most\n46x compression ratio on the ResNet-18 structure, with an acceptable accuracy\nof 65.36%, outperforming leading methods. Furthermore, PTQ can compress a\nResNet-18 model from 46 MB to 955KB (~48x) and a ResNet-50 model from 99 MB to\n3.3MB (~30x), while the top-1 accuracy on ImageNet drops slightly from 69.7% to\n65.3% and from 76.15% to 74.47%, respectively. Our method unifies pruning and\nquantization and thus provides a range of size-accuracy trade-off.",
          "link": "http://arxiv.org/abs/2107.10998",
          "publishedOn": "2021-07-26T02:00:57.807Z",
          "wordCount": 575,
          "title": "Pruning Ternary Quantization. (arXiv:2107.10998v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shitong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>",
          "description": "Point clouds acquired from scanning devices are often perturbed by noise,\nwhich affects downstream tasks such as surface reconstruction and analysis. The\ndistribution of a noisy point cloud can be viewed as the distribution of a set\nof noise-free samples $p(x)$ convolved with some noise model $n$, leading to\n$(p * n)(x)$ whose mode is the underlying clean surface. To denoise a noisy\npoint cloud, we propose to increase the log-likelihood of each point from $p *\nn$ via gradient ascent -- iteratively updating each point's position. Since $p\n* n$ is unknown at test-time, and we only need the score (i.e., the gradient of\nthe log-probability function) to perform gradient ascent, we propose a neural\nnetwork architecture to estimate the score of $p * n$ given only noisy point\nclouds as input. We derive objective functions for training the network and\ndevelop a denoising algorithm leveraging on the estimated scores. Experiments\ndemonstrate that the proposed model outperforms state-of-the-art methods under\na variety of noise models, and shows the potential to be applied in other tasks\nsuch as point cloud upsampling.",
          "link": "http://arxiv.org/abs/2107.10981",
          "publishedOn": "2021-07-26T02:00:57.799Z",
          "wordCount": 610,
          "title": "Score-Based Point Cloud Denoising. (arXiv:2107.10981v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_T/0/1/0/all/0/1\">Touqeer Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emami_E/0/1/0/all/0/1\">Ebrahim Emami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadik_M/0/1/0/all/0/1\">Martin &#x10c;ad&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bebis_G/0/1/0/all/0/1\">George Bebis</a>",
          "description": "Skyline plays a pivotal role in mountainous visual geo-localization and\nlocalization/navigation of planetary rovers/UAVs and virtual/augmented reality\napplications. We present a novel mountainous skyline detection approach where\nwe adapt a shallow learning approach to learn a set of filters to discriminate\nbetween edges belonging to sky-mountain boundary and others coming from\ndifferent regions. Unlike earlier approaches, which either rely on extraction\nof explicit feature descriptors and their classification, or fine-tuning\ngeneral scene parsing deep networks for sky segmentation, our approach learns\nlinear filters based on local structure analysis. At test time, for every\ncandidate edge pixel, a single filter is chosen from the set of learned filters\nbased on pixel's structure tensor, and then applied to the patch around it. We\nthen employ dynamic programming to solve the shortest path problem for the\nresultant multistage graph to get the sky-mountain boundary. The proposed\napproach is computationally faster than earlier methods while providing\ncomparable performance and is more suitable for resource constrained platforms\ne.g., mobile devices, planetary rovers and UAVs. We compare our proposed\napproach against earlier skyline detection methods using four different data\nsets. Our code is available at\n\\url{https://github.com/TouqeerAhmad/skyline_detection}.",
          "link": "http://arxiv.org/abs/2107.10997",
          "publishedOn": "2021-07-26T02:00:57.772Z",
          "wordCount": 648,
          "title": "Resource Efficient Mountainous Skyline Extraction using Shallow Learning. (arXiv:2107.10997v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_L/0/1/0/all/0/1\">Libo Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_J/0/1/0/all/0/1\">Jochen Lang</a>",
          "description": "Feature pyramids and iterative refinement have recently led to great progress\nin optical flow estimation. However, downsampling in feature pyramids can cause\nblending of foreground objects with the background, which will mislead\nsubsequent decisions in the iterative processing. The results are missing\ndetails especially in the flow of thin and of small structures. We propose a\nnovel Residual Feature Pyramid Module (RFPM) which retains important details in\nthe feature map without changing the overall iterative refinement design of the\noptical flow estimation. RFPM incorporates a residual structure between\nmultiple feature pyramids into a downsampling module that corrects the blending\nof objects across boundaries. We demonstrate how to integrate our module with\ntwo state-of-the-art iterative refinement architectures. Results show that our\nRFPM visibly reduces flow errors and improves state-of-art performance in the\nclean pass of Sintel, and is one of the top-performing methods in KITTI.\nAccording to the particular modular structure of RFPM, we introduce a special\ntransfer learning approach that can dramatically decrease the training time\ncompared to a typical full optical flow training schedule on multiple datasets.",
          "link": "http://arxiv.org/abs/2107.10990",
          "publishedOn": "2021-07-26T02:00:57.748Z",
          "wordCount": 611,
          "title": "Detail Preserving Residual Feature Pyramid Modules for Optical Flow. (arXiv:2107.10990v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10912",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Velden_B/0/1/0/all/0/1\">Bas H.M. van der Velden</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuijf_H/0/1/0/all/0/1\">Hugo J. Kuijf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gilhuijs_K/0/1/0/all/0/1\">Kenneth G.A. Gilhuijs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Viergever_M/0/1/0/all/0/1\">Max A. Viergever</a>",
          "description": "With an increase in deep learning-based methods, the call for explainability\nof such methods grows, especially in high-stakes decision making areas such as\nmedical image analysis. This survey presents an overview of eXplainable\nArtificial Intelligence (XAI) used in deep learning-based medical image\nanalysis. A framework of XAI criteria is introduced to classify deep\nlearning-based medical image analysis methods. Papers on XAI techniques in\nmedical image analysis are then surveyed and categorized according to the\nframework and according to anatomical location. The paper concludes with an\noutlook of future opportunities for XAI in medical image analysis.",
          "link": "http://arxiv.org/abs/2107.10912",
          "publishedOn": "2021-07-26T02:00:57.739Z",
          "wordCount": 561,
          "title": "Explainable artificial intelligence (XAI) in deep learning-based medical image analysis. (arXiv:2107.10912v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Coenen_M/0/1/0/all/0/1\">Max Coenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottensteiner_F/0/1/0/all/0/1\">Franz Rottensteiner</a>",
          "description": "The 3D reconstruction of objects is a prerequisite for many highly relevant\napplications of computer vision such as mobile robotics or autonomous driving.\nTo deal with the inverse problem of reconstructing 3D objects from their 2D\nprojections, a common strategy is to incorporate prior object knowledge into\nthe reconstruction approach by establishing a 3D model and aligning it to the\n2D image plane. However, current approaches are limited due to inadequate shape\npriors and the insufficiency of the derived image observations for a reliable\nalignment with the 3D model. The goal of this paper is to show how 3D object\nreconstruction can profit from a more sophisticated shape prior and from a\ncombined incorporation of different observation types inferred from the images.\nWe introduce a subcategory-aware deformable vehicle model that makes use of a\nprediction of the vehicle type for a more appropriate regularisation of the\nvehicle shape. A multi-branch CNN is presented to derive predictions of the\nvehicle type and orientation. This information is also introduced as prior\ninformation for model fitting. Furthermore, the CNN extracts vehicle keypoints\nand wireframes, which are well-suited for model-to-image association and model\nfitting. The task of pose estimation and reconstruction is addressed by a\nversatile probabilistic model. Extensive experiments are conducted using two\nchallenging real-world data sets on both of which the benefit of the developed\nshape prior can be shown. A comparison to state-of-the-art methods for vehicle\npose estimation shows that the proposed approach performs on par or better,\nconfirming the suitability of the developed shape prior and probabilistic model\nfor vehicle reconstruction.",
          "link": "http://arxiv.org/abs/2107.10898",
          "publishedOn": "2021-07-26T02:00:57.712Z",
          "wordCount": 704,
          "title": "Pose Estimation and 3D Reconstruction of Vehicles from Stereo-Images Using a Subcategory-Aware Shape Prior. (arXiv:2107.10898v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuolin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaojun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Recent studies show that deep neural networks (DNN) are vulnerable to\nadversarial examples, which aim to mislead DNNs by adding perturbations with\nsmall magnitude. To defend against such attacks, both empirical and theoretical\ndefense approaches have been extensively studied for a single ML model. In this\nwork, we aim to analyze and provide the certified robustness for ensemble ML\nmodels, together with the sufficient and necessary conditions of robustness for\ndifferent ensemble protocols. Although ensemble models are shown more robust\nthan a single model empirically; surprisingly, we find that in terms of the\ncertified robustness the standard ensemble models only achieve marginal\nimprovement compared to a single model. Thus, to explore the conditions that\nguarantee to provide certifiably robust ensemble ML models, we first prove that\ndiversified gradient and large confidence margin are sufficient and necessary\nconditions for certifiably robust ensemble models under the model-smoothness\nassumption. We then provide the bounded model-smoothness analysis based on the\nproposed Ensemble-before-Smoothing strategy. We also prove that an ensemble\nmodel can always achieve higher certified robustness than a single base model\nunder mild conditions. Inspired by the theoretical findings, we propose the\nlightweight Diversity Regularized Training (DRT) to train certifiably robust\nensemble ML models. Extensive experiments show that our DRT enhanced ensembles\ncan consistently achieve higher certified robustness than existing single and\nensemble ML models, demonstrating the state-of-the-art certified L2-robustness\non MNIST, CIFAR-10, and ImageNet datasets.",
          "link": "http://arxiv.org/abs/2107.10873",
          "publishedOn": "2021-07-26T02:00:57.697Z",
          "wordCount": 691,
          "title": "On the Certified Robustness for Ensemble Models and Beyond. (arXiv:2107.10873v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Chengxiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhengping Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1\">Zheng Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Gangyi Ding</a>",
          "description": "Deep generative models have made great progress in synthesizing images with\narbitrary human poses and transferring poses of one person to others. However,\nmost existing approaches explicitly leverage the pose information extracted\nfrom the source images as a conditional input for the generative networks.\nMeanwhile, they usually focus on the visual fidelity of the synthesized images\nbut neglect the inherent consistency, which further confines their performance\nof pose transfer. To alleviate the current limitations and improve the quality\nof the synthesized images, we propose a pose transfer network with Disentangled\nFeature Consistency (DFC-Net) to facilitate human pose transfer. Given a pair\nof images containing the source and target person, DFC-Net extracts pose and\nstatic information from the source and target respectively, then synthesizes an\nimage of the target person with the desired pose from the source. Moreover,\nDFC-Net leverages disentangled feature consistency losses in the adversarial\ntraining to strengthen the transfer coherence and integrates the keypoint\namplifier to enhance the pose feature extraction. Additionally, an unpaired\nsupport dataset Mixamo-Sup providing more extra pose information has been\nfurther utilized during the training to improve the generality and robustness\nof DFC-Net. Extensive experimental results on Mixamo-Pose and EDN-10k have\ndemonstrated DFC-Net achieves state-of-the-art performance on pose transfer.",
          "link": "http://arxiv.org/abs/2107.10984",
          "publishedOn": "2021-07-26T02:00:57.670Z",
          "wordCount": 650,
          "title": "Human Pose Transfer with Disentangled Feature Consistency. (arXiv:2107.10984v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nelson_H/0/1/0/all/0/1\">Henry J. Nelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papanikolopoulos_N/0/1/0/all/0/1\">Nikolaos Papanikolopoulos</a>",
          "description": "In order to apply the recent successes of automated plant phenotyping and\nmachine learning on a large scale, efficient and general algorithms must be\ndesigned to intelligently split crop fields into small, yet actionable,\nportions that can then be processed by more complex algorithms. In this paper\nwe notice a similarity between the current state-of-the-art for this problem\nand a commonly used density-based clustering algorithm, Quickshift. Exploiting\nthis similarity we propose a number of novel, application specific algorithms\nwith the goal of producing a general and scalable plant segmentation algorithm.\nThe novel algorithms proposed in this work are shown to produce quantitatively\nbetter results than the current state-of-the-art while being less sensitive to\ninput parameters and maintaining the same algorithmic time complexity. When\nincorporated into field-scale phenotyping systems, the proposed algorithms\nshould work as a drop in replacement that can greatly improve the accuracy of\nresults while ensuring that performance and scalability remain undiminished.",
          "link": "http://arxiv.org/abs/2107.10950",
          "publishedOn": "2021-07-26T02:00:57.648Z",
          "wordCount": 589,
          "title": "Pre-Clustering Point Clouds of Crop Fields Using Scalable Methods. (arXiv:2107.10950v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhmoginov_A/0/1/0/all/0/1\">Andrey Zhmoginov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bashkirova_D/0/1/0/all/0/1\">Dina Bashkirova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandler_M/0/1/0/all/0/1\">Mark Sandler</a>",
          "description": "Conditional computation and modular networks have been recently proposed for\nmultitask learning and other problems as a way to decompose problem solving\ninto multiple reusable computational blocks. We propose a new approach for\nlearning modular networks based on the isometric version of ResNet with all\nresidual blocks having the same configuration and the same number of\nparameters. This architectural choice allows adding, removing and changing the\norder of residual blocks. In our method, the modules can be invoked repeatedly\nand allow knowledge transfer to novel tasks by adjusting the order of\ncomputation. This allows soft weight sharing between tasks with only a small\nincrease in the number of parameters. We show that our method leads to\ninterpretable self-organization of modules in case of multi-task learning,\ntransfer learning and domain adaptation while achieving competitive results on\nthose tasks. From practical perspective, our approach allows to: (a) reuse\nexisting modules for learning new task by adjusting the computation order, (b)\nuse it for unsupervised multi-source domain adaptation to illustrate that\nadaptation to unseen data can be achieved by only manipulating the order of\npretrained modules, (c) show how our approach can be used to increase accuracy\nof existing architectures for image classification tasks such as ImageNet,\nwithout any parameter increase, by reusing the same block multiple times.",
          "link": "http://arxiv.org/abs/2107.10963",
          "publishedOn": "2021-07-26T02:00:57.640Z",
          "wordCount": 653,
          "title": "Compositional Models: Multi-Task Learning and Knowledge Transfer with Modular Networks. (arXiv:2107.10963v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Linghao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1\">Jinsong Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges EL Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "In this work, we propose a domain generalization (DG) approach to learn on\nseveral labeled source domains and transfer knowledge to a target domain that\nis inaccessible in training. Considering the inherent conditional and label\nshifts, we would expect the alignment of $p(x|y)$ and $p(y)$. However, the\nwidely used domain invariant feature learning (IFL) methods relies on aligning\nthe marginal concept shift w.r.t. $p(x)$, which rests on an unrealistic\nassumption that $p(y)$ is invariant across domains. We thereby propose a novel\nvariational Bayesian inference framework to enforce the conditional\ndistribution alignment w.r.t. $p(x|y)$ via the prior distribution matching in a\nlatent space, which also takes the marginal label shift w.r.t. $p(y)$ into\nconsideration with the posterior alignment. Extensive experiments on various\nbenchmarks demonstrate that our framework is robust to the label shift and the\ncross-domain accuracy is significantly improved, thereby achieving superior\nperformance over the conventional IFL counterparts.",
          "link": "http://arxiv.org/abs/2107.10931",
          "publishedOn": "2021-07-26T02:00:57.623Z",
          "wordCount": 615,
          "title": "Domain Generalization under Conditional and Label Shifts via Variational Bayesian Inference. (arXiv:2107.10931v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10895",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Malawade_A/0/1/0/all/0/1\">Arnav Malawade</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Odema_M/0/1/0/all/0/1\">Mohanad Odema</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lajeunesse_DeGroot_S/0/1/0/all/0/1\">Sebastien Lajeunesse-DeGroot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Faruque_M/0/1/0/all/0/1\">Mohammad Abdullah Al Faruque</a>",
          "description": "Autonomous vehicles (AV) are expected to revolutionize transportation and\nimprove road safety significantly. However, these benefits do not come without\ncost; AVs require large Deep-Learning (DL) models and powerful hardware\nplatforms to operate reliably in real-time, requiring between several hundred\nwatts to one kilowatt of power. This power consumption can dramatically reduce\nvehicles' driving range and affect emissions. To address this problem, we\npropose SAGE: a methodology for selectively offloading the key energy-consuming\nmodules of DL architectures to the cloud to optimize edge energy usage while\nmeeting real-time latency constraints. Furthermore, we leverage Head Network\nDistillation (HND) to introduce efficient bottlenecks within the DL\narchitecture in order to minimize the network overhead costs of offloading with\nalmost no degradation in the model's performance. We evaluate SAGE using an\nNvidia Jetson TX2 and an industry-standard Nvidia Drive PX2 as the AV edge\ndevices and demonstrate that our offloading strategy is practical for a wide\nrange of DL models and internet connection bandwidths on 3G, 4G LTE, and WiFi\ntechnologies. Compared to edge-only computation, SAGE reduces energy\nconsumption by an average of 36.13%, 47.07%, and 55.66% for an AV with one\nlow-resolution camera, one high-resolution camera, and three high-resolution\ncameras, respectively. SAGE also reduces upload data size by up to 98.40%\ncompared to direct camera offloading.",
          "link": "http://arxiv.org/abs/2107.10895",
          "publishedOn": "2021-07-26T02:00:57.603Z",
          "wordCount": 689,
          "title": "SAGE: A Split-Architecture Methodology for Efficient End-to-End Autonomous Vehicle Control. (arXiv:2107.10895v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mommert_M/0/1/0/all/0/1\">Michael Mommert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheibenreif_L/0/1/0/all/0/1\">Linus Scheibenreif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanna_J/0/1/0/all/0/1\">Jo&#xeb;lle Hanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borth_D/0/1/0/all/0/1\">Damian Borth</a>",
          "description": "Satellite remote imaging enables the detailed study of land use patterns on a\nglobal scale. We investigate the possibility to improve the information content\nof traditional land use classification by identifying the nature of industrial\nsites from medium-resolution remote sensing images. In this work, we focus on\nclassifying different types of power plants from Sentinel-2 imaging data. Using\na ResNet-50 deep learning model, we are able to achieve a mean accuracy of\n90.0% in distinguishing 10 different power plant types and a background class.\nFurthermore, we are able to identify the cooling mechanisms utilized in thermal\npower plants with a mean accuracy of 87.5%. Our results enable us to\nqualitatively investigate the energy mix from Sentinel-2 imaging data, and\nprove the feasibility to classify industrial sites on a global scale from\nfreely available satellite imagery.",
          "link": "http://arxiv.org/abs/2107.10894",
          "publishedOn": "2021-07-26T02:00:57.585Z",
          "wordCount": 592,
          "title": "Power Plant Classification from Remote Imaging with Deep Learning. (arXiv:2107.10894v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1\">Anand Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1\">Minh Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitehill_J/0/1/0/all/0/1\">Jacob Whitehill</a>",
          "description": "For the task of face verification, we explore the utility of harnessing\nauxiliary facial emotion labels to impose explicit geometric constraints on the\nembedding space when training deep embedding models. We introduce several novel\nloss functions that, in conjunction with a standard Triplet Loss [43], or\nArcFace loss [10], provide geometric constraints on the embedding space; the\nlabels for our loss functions can be provided using either manually annotated\nor automatically detected auxiliary emotion labels. Our method is implemented\npurely in terms of the loss function and does not require any changes to the\nneural network backbone of the embedding function.",
          "link": "http://arxiv.org/abs/2103.03862",
          "publishedOn": "2021-07-23T02:00:31.803Z",
          "wordCount": 593,
          "title": "Harnessing Geometric Constraints from Emotion Labels to improve Face Verification. (arXiv:2103.03862v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zu_K/0/1/0/all/0/1\">Keke Zu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuru Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>",
          "description": "Recently, it has been demonstrated that the performance of a deep\nconvolutional neural network can be effectively improved by embedding an\nattention module into it. In this work, a novel lightweight and effective\nattention method named Pyramid Squeeze Attention (PSA) module is proposed. By\nreplacing the 3x3 convolution with the PSA module in the bottleneck blocks of\nthe ResNet, a novel representational block named Efficient Pyramid Squeeze\nAttention (EPSA) is obtained. The EPSA block can be easily added as a\nplug-and-play component into a well-established backbone network, and\nsignificant improvements on model performance can be achieved. Hence, a simple\nand efficient backbone architecture named EPSANet is developed in this work by\nstacking these ResNet-style EPSA blocks. Correspondingly, a stronger\nmulti-scale representation ability can be offered by the proposed EPSANet for\nvarious computer vision tasks including but not limited to, image\nclassification, object detection, instance segmentation, etc. Without bells and\nwhistles, the performance of the proposed EPSANet outperforms most of the\nstate-of-the-art channel attention methods. As compared to the SENet-50, the\nTop-1 accuracy is improved by 1.93% on ImageNet dataset, a larger margin of\n+2.7 box AP for object detection and an improvement of +1.7 mask AP for\ninstance segmentation by using the Mask-RCNN on MS-COCO dataset are obtained.\nOur source code is available at:https://github.com/murufeng/EPSANet.",
          "link": "http://arxiv.org/abs/2105.14447",
          "publishedOn": "2021-07-23T02:00:31.756Z",
          "wordCount": 689,
          "title": "EPSANet: An Efficient Pyramid Squeeze Attention Block on Convolutional Neural Network. (arXiv:2105.14447v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jin-Man Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Jae-Hyuk Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1\">Sahng-Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sun-Kyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_U/0/1/0/all/0/1\">Ue-Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong-Hwan Kim</a>",
          "description": "We present a challenging dataset, ChangeSim, aimed at online scene change\ndetection (SCD) and more. The data is collected in photo-realistic simulation\nenvironments with the presence of environmental non-targeted variations, such\nas air turbidity and light condition changes, as well as targeted object\nchanges in industrial indoor environments. By collecting data in simulations,\nmulti-modal sensor data and precise ground truth labels are obtainable such as\nthe RGB image, depth image, semantic segmentation, change segmentation, camera\nposes, and 3D reconstructions. While the previous online SCD datasets evaluate\nmodels given well-aligned image pairs, ChangeSim also provides raw unpaired\nsequences that present an opportunity to develop an online SCD model in an\nend-to-end manner, considering both pairing and detection. Experiments show\nthat even the latest pair-based SCD models suffer from the bottleneck of the\npairing process, and it gets worse when the environment contains the\nnon-targeted variations. Our dataset is available at\nthis http URL",
          "link": "http://arxiv.org/abs/2103.05368",
          "publishedOn": "2021-07-23T02:00:31.718Z",
          "wordCount": 629,
          "title": "ChangeSim: Towards End-to-End Online Scene Change Detection in Industrial Indoor Environments. (arXiv:2103.05368v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zappel_M/0/1/0/all/0/1\">Moritz Zappel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bultmann_S/0/1/0/all/0/1\">Simon Bultmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>",
          "description": "The task of 6D object pose estimation from RGB images is an important\nrequirement for autonomous service robots to be able to interact with the real\nworld. In this work, we present a two-step pipeline for estimating the 6 DoF\ntranslation and orientation of known objects. Keypoints and Part Affinity\nFields (PAFs) are predicted from the input image adopting the OpenPose CNN\narchitecture from human pose estimation. Object poses are then calculated from\n2D-3D correspondences between detected and model keypoints via the PnP-RANSAC\nalgorithm. The proposed approach is evaluated on the YCB-Video dataset and\nachieves accuracy on par with recent methods from the literature. Using PAFs to\nassemble detected keypoints into object instances proves advantageous over only\nusing heatmaps. Models trained to predict keypoints of a single object class\nperform significantly better than models trained for several classes.",
          "link": "http://arxiv.org/abs/2107.02057",
          "publishedOn": "2021-07-23T02:00:31.711Z",
          "wordCount": 620,
          "title": "6D Object Pose Estimation using Keypoints and Part Affinity Fields. (arXiv:2107.02057v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03337",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Harbrecht_H/0/1/0/all/0/1\">Helmut Harbrecht</a>, <a href=\"http://arxiv.org/find/math/1/au:+Multerer_M/0/1/0/all/0/1\">Michael Multerer</a>",
          "description": "In this article, we introduce the concept of samplets by transferring the\nconstruction of Tausch-White wavelets to the realm of data. This way we obtain\na multilevel representation of discrete data which directly enables data\ncompression, detection of singularities and adaptivity. Applying samplets to\nrepresent kernel matrices, as they arise in kernel based learning or Gaussian\nprocess regression, we end up with quasi-sparse matrices. By thresholding small\nentries, these matrices are compressible to O(N log N) relevant entries, where\nN is the number of data points. This feature allows for the use of fill-in\nreducing reorderings to obtain a sparse factorization of the compressed\nmatrices. Besides the comprehensive introduction to samplets and their\nproperties, we present extensive numerical studies to benchmark the approach.\nOur results demonstrate that samplets mark a considerable step in the direction\nof making large data sets accessible for analysis.",
          "link": "http://arxiv.org/abs/2107.03337",
          "publishedOn": "2021-07-23T02:00:31.698Z",
          "wordCount": 597,
          "title": "Samplets: A new paradigm for data compression. (arXiv:2107.03337v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shibata_T/0/1/0/all/0/1\">Takashi Shibata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_M/0/1/0/all/0/1\">Masayuki Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okutomi_M/0/1/0/all/0/1\">Masatoshi Okutomi</a>",
          "description": "Deep convolutional networks have become the mainstream in computer vision\napplications. Although CNNs have been successful in many computer vision tasks,\nit is not free from drawbacks. The performance of CNN is dramatically degraded\nby geometric transformation, such as large rotations. In this paper, we propose\na novel CNN architecture that can improve the robustness against geometric\ntransformations without modifying the existing backbones of their CNNs. The key\nis to enclose the existing backbone with a geometric transformation (and the\ncorresponding reverse transformation) and a feature map ensemble. The proposed\nmethod can inherit the strengths of existing CNNs that have been presented so\nfar. Furthermore, the proposed method can be employed in combination with\nstate-of-the-art data augmentation algorithms to improve their performance. We\ndemonstrate the effectiveness of the proposed method using standard datasets\nsuch as CIFAR, CUB-200, and Mnist-rot-12k.",
          "link": "http://arxiv.org/abs/2107.10524",
          "publishedOn": "2021-07-23T02:00:31.692Z",
          "wordCount": 577,
          "title": "Geometric Data Augmentation Based on Feature Map Ensemble. (arXiv:2107.10524v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1805.05510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1\">Jing Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "Metric learning especially deep metric learning has been widely developed for\nlarge-scale image inputs data. However, in many real-world applications, we can\nonly have access to vectorized inputs data. Moreover, on one hand, well-labeled\ndata is usually limited due to the high annotation cost. On the other hand, the\nreal data is commonly streaming data, which requires to be processed online. In\nthese scenarios, the fashionable deep metric learning is not suitable anymore.\nTo this end, we reconsider the traditional shallow online metric learning and\nnewly develop an online progressive deep metric learning (ODML) framework to\nconstruct a metric-algorithm-based deep network. Specifically, we take an\nonline metric learning algorithm as a metric-algorithm-based layer (i.e.,\nmetric layer), followed by a nonlinear layer, and then stack these layers in a\nfashion similar to deep learning. Different from the shallow online metric\nlearning, which can only learn one metric space (feature transformation), the\nproposed ODML is able to learn multiple hierarchical metric spaces.\nFurthermore, in a progressively and nonlinearly learning way, ODML has a\nstronger learning ability than traditional shallow online metric learning in\nthe case of limited available training data. To make the learning process more\nexplainable and theoretically guaranteed, we also provide theoretical analysis.\nThe proposed ODML enjoys several nice properties and can indeed learn a metric\nprogressively and performs better on the benchmark datasets. Extensive\nexperiments with different settings have been conducted to verify these\nproperties of the proposed ODML.",
          "link": "http://arxiv.org/abs/1805.05510",
          "publishedOn": "2021-07-23T02:00:31.590Z",
          "wordCount": 713,
          "title": "Online Progressive Deep Metric Learning. (arXiv:1805.05510v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duzceker_A/0/1/0/all/0/1\">Arda D&#xfc;z&#xe7;eker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galliani_S/0/1/0/all/0/1\">Silvano Galliani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogel_C/0/1/0/all/0/1\">Christoph Vogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speciale_P/0/1/0/all/0/1\">Pablo Speciale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusmanu_M/0/1/0/all/0/1\">Mihai Dusmanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>",
          "description": "We propose an online multi-view depth prediction approach on posed video\nstreams, where the scene geometry information computed in the previous time\nsteps is propagated to the current time step in an efficient and geometrically\nplausible way. The backbone of our approach is a real-time capable, lightweight\nencoder-decoder that relies on cost volumes computed from pairs of images. We\nextend it by placing a ConvLSTM cell at the bottleneck layer, which compresses\nan arbitrary amount of past information in its states. The novelty lies in\npropagating the hidden state of the cell by accounting for the viewpoint\nchanges between time steps. At a given time step, we warp the previous hidden\nstate into the current camera plane using the previous depth prediction. Our\nextension brings only a small overhead of computation time and memory\nconsumption, while improving the depth predictions significantly. As a result,\nwe outperform the existing state-of-the-art multi-view stereo methods on most\nof the evaluated metrics in hundreds of indoor scenes while maintaining a\nreal-time performance. Code available:\nhttps://github.com/ardaduz/deep-video-mvs",
          "link": "http://arxiv.org/abs/2012.02177",
          "publishedOn": "2021-07-23T02:00:31.563Z",
          "wordCount": 660,
          "title": "DeepVideoMVS: Multi-View Stereo on Video with Recurrent Spatio-Temporal Fusion. (arXiv:2012.02177v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.03592",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Eskimez_S/0/1/0/all/0/1\">Sefik Emre Eskimez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">You Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_Z/0/1/0/all/0/1\">Zhiyao Duan</a>",
          "description": "Visual emotion expression plays an important role in audiovisual speech\ncommunication. In this work, we propose a novel approach to rendering visual\nemotion expression in speech-driven talking face generation. Specifically, we\ndesign an end-to-end talking face generation system that takes a speech\nutterance, a single face image, and a categorical emotion label as input to\nrender a talking face video synchronized with the speech and expressing the\nconditioned emotion. Objective evaluation on image quality, audiovisual\nsynchronization, and visual emotion expression shows that the proposed system\noutperforms a state-of-the-art baseline system. Subjective evaluation of visual\nemotion expression and video realness also demonstrates the superiority of the\nproposed system. Furthermore, we conduct a human emotion recognition pilot\nstudy using generated videos with mismatched emotions among the audio and\nvisual modalities. Results show that humans respond to the visual modality more\nsignificantly than the audio modality on this task.",
          "link": "http://arxiv.org/abs/2008.03592",
          "publishedOn": "2021-07-23T02:00:31.547Z",
          "wordCount": 632,
          "title": "Speech Driven Talking Face Generation from a Single Image and an Emotion Condition. (arXiv:2008.03592v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyemin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daijin Kim</a>",
          "description": "We propose Uncertainty Augmented Context Attention network (UACANet) for\npolyp segmentation which consider a uncertain area of the saliency map. We\nconstruct a modified version of U-Net shape network with additional encoder and\ndecoder and compute a saliency map in each bottom-up stream prediction module\nand propagate to the next prediction module. In each prediction module,\npreviously predicted saliency map is utilized to compute foreground, background\nand uncertain area map and we aggregate the feature map with three area maps\nfor each representation. Then we compute the relation between each\nrepresentation and each pixel in the feature map. We conduct experiments on\nfive popular polyp segmentation benchmarks, Kvasir, CVC-ClinicDB, ETIS,\nCVC-ColonDB and CVC-300, and achieve state-of-the-art performance. Especially,\nwe achieve 76.6% mean Dice on ETIS dataset which is 13.8% improvement compared\nto the previous state-of-the-art method. Source code is publicly available at\nhttps://github.com/plemeri/UACANet",
          "link": "http://arxiv.org/abs/2107.02368",
          "publishedOn": "2021-07-23T02:00:31.056Z",
          "wordCount": 640,
          "title": "UACANet: Uncertainty Augmented Context Attention for Polyp Segmentation. (arXiv:2107.02368v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roh_J/0/1/0/all/0/1\">Junha Roh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desingh_K/0/1/0/all/0/1\">Karthik Desingh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>",
          "description": "To realize robots that can understand human instructions and perform\nmeaningful tasks in the near future, it is important to develop learned models\nthat can understand referential language to identify common objects in\nreal-world 3D scenes. In this paper, we develop a spatial-language model for a\n3D visual grounding problem. Specifically, given a reconstructed 3D scene in\nthe form of a point cloud with 3D bounding boxes of potential object\ncandidates, and a language utterance referring to a target object in the scene,\nour model identifies the target object from a set of potential candidates. Our\nspatial-language model uses a transformer-based architecture that combines\nspatial embedding from bounding-box with a finetuned language embedding from\nDistilBert and reasons among the objects in the 3D scene to find the target\nobject. We show that our model performs competitively on visio-linguistic\ndatasets proposed by ReferIt3D. We provide additional analysis of performance\nin spatial reasoning tasks decoupled from perception noise, the effect of\nview-dependent utterances in terms of accuracy, and view-point annotations for\npotential robotics applications.",
          "link": "http://arxiv.org/abs/2107.03438",
          "publishedOn": "2021-07-23T02:00:30.963Z",
          "wordCount": 631,
          "title": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding. (arXiv:2107.03438v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.09597",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Geiger_E/0/1/0/all/0/1\">Eric Geiger</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kogan_I/0/1/0/all/0/1\">Irina A. Kogan</a>",
          "description": "While the equality of differential signatures (Calabi et al, Int. J. Comput.\nVis. 26: 107-135, 1998) is known to be a necessary condition for congruence, it\nis not sufficient (Musso and Nicolodi, J. Math Imaging Vis. 35: 68-85, 2009).\nHickman (J. Math Imaging Vis. 43: 206-213, 2012, Theorem 2) claimed that for\nnon-degenerate planar curves, equality of Euclidean signatures implies\ncongruence. We prove that while Hickman's claim holds for simple, closed curves\nwith simple signatures, it fails for curves with non-simple signatures. In the\nlater case, we associate a directed graph with the signature and show how\nvarious paths along the graph give rise to a family of non-congruent,\nnon-degenerate curves with identical signatures. Using this additional\nstructure, we formulate congruence criteria for non-degenerate, closed, simple\ncurves and show how the paths reflect the global and local symmetries of the\ncorresponding curve.",
          "link": "http://arxiv.org/abs/1912.09597",
          "publishedOn": "2021-07-23T02:00:30.911Z",
          "wordCount": 699,
          "title": "Non-congruent non-degenerate curves with identical signatures. (arXiv:1912.09597v4 [math.DG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Coleman_C/0/1/0/all/0/1\">Cody Coleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_E/0/1/0/all/0/1\">Edward Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Samuels_J/0/1/0/all/0/1\">Julian Katz-Samuels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Culatana_S/0/1/0/all/0/1\">Sean Culatana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailis_P/0/1/0/all/0/1\">Peter Bailis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1\">Alexander C. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1\">Robert Nowak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumbaly_R/0/1/0/all/0/1\">Roshan Sumbaly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yalniz_I/0/1/0/all/0/1\">I. Zeki Yalniz</a>",
          "description": "Many active learning and search approaches are intractable for large-scale\nindustrial settings with billions of unlabeled examples. Existing approaches\nsearch globally for the optimal examples to label, scaling linearly or even\nquadratically with the unlabeled data. In this paper, we improve the\ncomputational efficiency of active learning and search methods by restricting\nthe candidate pool for labeling to the nearest neighbors of the currently\nlabeled set instead of scanning over all of the unlabeled data. We evaluate\nseveral selection strategies in this setting on three large-scale computer\nvision datasets: ImageNet, OpenImages, and a de-identified and aggregated\ndataset of 10 billion images provided by a large internet company. Our approach\nachieved similar mean average precision and recall as the traditional global\napproach while reducing the computational cost of selection by up to three\norders of magnitude, thus enabling web-scale active learning.",
          "link": "http://arxiv.org/abs/2007.00077",
          "publishedOn": "2021-07-23T02:00:30.904Z",
          "wordCount": 638,
          "title": "Similarity Search for Efficient Active Learning and Search of Rare Concepts. (arXiv:2007.00077v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jihua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Meng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Member/0/1/0/all/0/1\">Member</a>, <a href=\"http://arxiv.org/find/cs/1/au:+IEEE/0/1/0/all/0/1\">IEEE</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyong Cheng</a>",
          "description": "Video moment retrieval targets at retrieving a moment in a video for a given\nlanguage query. The challenges of this task include 1) the requirement of\nlocalizing the relevant moment in an untrimmed video, and 2) bridging the\nsemantic gap between textual query and video contents. To tackle those\nproblems, early approaches adopt the sliding window or uniform sampling to\ncollect video clips first and then match each clip with the query. Obviously,\nthese strategies are time-consuming and often lead to unsatisfied accuracy in\nlocalization due to the unpredictable length of the golden moment. To avoid the\nlimitations, researchers recently attempt to directly predict the relevant\nmoment boundaries without the requirement to generate video clips first. One\nmainstream approach is to generate a multimodal feature vector for the target\nquery and video frames (e.g., concatenation) and then use a regression approach\nupon the multimodal feature vector for boundary detection. Although some\nprogress has been achieved by this approach, we argue that those methods have\nnot well captured the cross-modal interactions between the query and video\nframes.\n\nIn this paper, we propose an Attentive Cross-modal Relevance Matching (ACRM)\nmodel which predicts the temporal boundaries based on an interaction modeling.\nIn addition, an attention module is introduced to assign higher weights to\nquery words with richer semantic cues, which are considered to be more\nimportant for finding relevant video contents. Another contribution is that we\npropose an additional predictor to utilize the internal frames in the model\ntraining to improve the localization accuracy. Extensive experiments on two\ndatasets TACoS and Charades-STA demonstrate the superiority of our method over\nseveral state-of-the-art methods. Ablation studies have been also conducted to\nexamine the effectiveness of different modules in our ACRM model.",
          "link": "http://arxiv.org/abs/2009.10434",
          "publishedOn": "2021-07-23T02:00:30.894Z",
          "wordCount": 772,
          "title": "Frame-wise Cross-modal Matching for Video Moment Retrieval. (arXiv:2009.10434v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1\">Shail Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baghdadi_R/0/1/0/all/0/1\">Riyadh Baghdadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowatzki_T/0/1/0/all/0/1\">Tony Nowatzki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avancha_S/0/1/0/all/0/1\">Sasikanth Avancha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Aviral Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baoxin Li</a>",
          "description": "Machine learning (ML) models are widely used in many important domains. For\nefficiently processing these computational- and memory-intensive applications,\ntensors of these over-parameterized models are compressed by leveraging\nsparsity, size reduction, and quantization of tensors. Unstructured sparsity\nand tensors with varying dimensions yield irregular computation, communication,\nand memory access patterns; processing them on hardware accelerators in a\nconventional manner does not inherently leverage acceleration opportunities.\nThis paper provides a comprehensive survey on the efficient execution of sparse\nand irregular tensor computations of ML models on hardware accelerators. In\nparticular, it discusses enhancement modules in the architecture design and the\nsoftware support; categorizes different hardware designs and acceleration\ntechniques and analyzes them in terms of hardware and execution costs; analyzes\nachievable accelerations for recent DNNs; highlights further opportunities in\nterms of hardware/software/model co-design optimizations (inter/intra-module).\nThe takeaways from this paper include: understanding the key challenges in\naccelerating sparse, irregular-shaped, and quantized tensors; understanding\nenhancements in accelerator systems for supporting their efficient\ncomputations; analyzing trade-offs in opting for a specific design choice for\nencoding, storing, extracting, communicating, computing, and load-balancing the\nnon-zeros; understanding how structured sparsity can improve storage efficiency\nand balance computations; understanding how to compile and map models with\nsparse tensors on the accelerators; understanding recent design trends for\nefficient accelerations and further opportunities.",
          "link": "http://arxiv.org/abs/2007.00864",
          "publishedOn": "2021-07-23T02:00:30.887Z",
          "wordCount": 725,
          "title": "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models: A Survey and Insights. (arXiv:2007.00864v2 [cs.AR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1\">Woochul Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyeon Kim</a>",
          "description": "Modern convolutional neural networks (CNNs) have massive identical\nconvolution blocks, and, hence, recursive sharing of parameters across these\nblocks has been proposed to reduce the amount of parameters. However, naive\nsharing of parameters poses many challenges such as limited representational\npower and the vanishing/exploding gradients problem of recursively shared\nparameters. In this paper, we present a recursive convolution block design and\ntraining method, in which a recursively shareable part, or a filter basis, is\nseparated and learned while effectively avoiding the vanishing/exploding\ngradients problem during training. We show that the unwieldy\nvanishing/exploding gradients problem can be controlled by enforcing the\nelements of the filter basis orthonormal, and empirically demonstrate that the\nproposed orthogonality regularization improves the flow of gradients during\ntraining. Experimental results on image classification and object detection\nshow that our approach, unlike previous parameter-sharing approaches, does not\ntrade performance to save parameters and consistently outperforms\noverparameterized counterpart networks. This superior performance demonstrates\nthat the proposed recursive convolution block design and the orthogonality\nregularization not only prevent performance degradation, but also consistently\nimprove the representation capability while a significant amount of parameters\nare recursively shared.",
          "link": "http://arxiv.org/abs/2006.05066",
          "publishedOn": "2021-07-23T02:00:30.825Z",
          "wordCount": 663,
          "title": "Deeply Shared Filter Bases for Parameter-Efficient Convolutional Neural Networks. (arXiv:2006.05066v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1\">Runwei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>",
          "description": "Despite great progress in 3D human pose estimation from videos, it is still\nan open problem to take full advantage of redundant 2D pose sequences to learn\nrepresentative representation for generating one single 3D pose. To this end,\nwe propose an improved Transformer-based architecture, called Strided\nTransformer, for 3D human pose estimation in videos to lift a sequence of 2D\njoint locations to a 3D pose. Specifically, a vanilla Transformer encoder (VTE)\nis adopted to model long-range dependencies of 2D pose sequences. To reduce\nredundancy of the sequence and aggregate information from local context,\nstrided convolutions are incorporated into VTE to progressively reduce the\nsequence length. The modified VTE is termed as strided Transformer encoder\n(STE) which is built upon the outputs of VTE. STE not only effectively\naggregates long-range information to a single-vector representation in a\nhierarchical global and local fashion but also significantly reduces the\ncomputation cost. Furthermore, a full-to-single supervision scheme is designed\nat both the full sequence scale and single target frame scale, applied to the\noutputs of VTE and STE, respectively. This scheme imposes extra temporal\nsmoothness constraints in conjunction with the single target frame supervision\nand improves the representation ability of features for the target frame. The\nproposed architecture is evaluated on two challenging benchmark datasets,\nHuman3.6M and HumanEva-I, and achieves state-of-the-art results with much fewer\nparameters.",
          "link": "http://arxiv.org/abs/2103.14304",
          "publishedOn": "2021-07-23T02:00:30.818Z",
          "wordCount": 743,
          "title": "Exploiting Temporal Contexts with Strided Transformer for 3D Human Pose Estimation. (arXiv:2103.14304v7 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07392",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ding_W/0/1/0/all/0/1\">Wangbin Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1\">Liqin Huang</a>",
          "description": "Multi-modality medical images can provide relevant or complementary\ninformation for a target (organ, tumor or tissue). Registering multi-modality\nimages to a common space can fuse these comprehensive information, and bring\nconvenience for clinical application. Recently, neural networks have been\nwidely investigated to boost registration methods. However, it is still\nchallenging to develop a multi-modality registration network due to the lack of\nrobust criteria for network training. In this work, we propose a multi-modality\nregistration network (MMRegNet), which can perform registration between\nmulti-modality images. Meanwhile, we present spatially encoded gradient\ninformation to train MMRegNet in an unsupervised manner. The proposed network\nwas evaluated on MM-WHS 2017. Results show that MMRegNet can achieve promising\nperformance for left ventricle cardiac registration tasks. Meanwhile, to\ndemonstrate the versatility of MMRegNet, we further evaluate the method with a\nliver dataset from CHAOS 2019. Source code will be released\npublicly\\footnote{https://github.com/NanYoMy/mmregnet} once the manuscript is\naccepted.",
          "link": "http://arxiv.org/abs/2105.07392",
          "publishedOn": "2021-07-23T02:00:30.811Z",
          "wordCount": 627,
          "title": "Unsupervised Registration Method based on Deep Neural Network: Application to cardiac and liver MR images. (arXiv:2105.07392v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05145",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jena_R/0/1/0/all/0/1\">Rohit Jena</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singla_S/0/1/0/all/0/1\">Sumedha Singla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>",
          "description": "Vessel segmentation is an essential task in many clinical applications.\nAlthough supervised methods have achieved state-of-art performance, acquiring\nexpert annotation is laborious and mostly limited for two-dimensional datasets\nwith a small sample size. On the contrary, unsupervised methods rely on\nhandcrafted features to detect tube-like structures such as vessels. However,\nthose methods require complex pipelines involving several hyper-parameters and\ndesign choices rendering the procedure sensitive, dataset-specific, and not\ngeneralizable. We propose a self-supervised method with a limited number of\nhyper-parameters that is generalizable across modalities. Our method uses\ntube-like structure properties, such as connectivity, profile consistency, and\nbifurcation, to introduce inductive bias into a learning algorithm. To model\nthose properties, we generate a vector field that we refer to as a flow. Our\nexperiments on various public datasets in 2D and 3D show that our method\nperforms better than unsupervised methods while learning useful transferable\nfeatures from unlabeled data. Unlike generic self-supervised methods, the\nlearned features learn vessel-relevant features that are transferable for\nsupervised approaches, which is essential when the number of annotated data is\nlimited.",
          "link": "http://arxiv.org/abs/2101.05145",
          "publishedOn": "2021-07-23T02:00:30.804Z",
          "wordCount": 651,
          "title": "Self-Supervised Vessel Enhancement Using Flow-Based Consistencies. (arXiv:2101.05145v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oza_M/0/1/0/all/0/1\">Manan Oza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1\">Sukalpa Chanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1\">David Doermann</a>",
          "description": "Faces generated using generative adversarial networks (GANs) have reached\nunprecedented realism. These faces, also known as \"Deep Fakes\", appear as\nrealistic photographs with very little pixel-level distortions. While some work\nhas enabled the training of models that lead to the generation of specific\nproperties of the subject, generating a facial image based on a natural\nlanguage description has not been fully explored. For security and criminal\nidentification, the ability to provide a GAN-based system that works like a\nsketch artist would be incredibly useful. In this paper, we present a novel\napproach to generate facial images from semantic text descriptions. The learned\nmodel is provided with a text description and an outline of the type of face,\nwhich the model uses to sketch the features. Our models are trained using an\nAffine Combination Module (ACM) mechanism to combine the text embedding from\nBERT and the GAN latent space using a self-attention matrix. This avoids the\nloss of features due to inadequate \"attention\", which may happen if text\nembedding and latent vector are simply concatenated. Our approach is capable of\ngenerating images that are very accurately aligned to the exhaustive textual\ndescriptions of faces with many fine detail features of the face and helps in\ngenerating better images. The proposed method is also capable of making\nincremental changes to a previously generated image if it is provided with\nadditional textual descriptions or sentences.",
          "link": "http://arxiv.org/abs/2107.10756",
          "publishedOn": "2021-07-23T02:00:30.798Z",
          "wordCount": 672,
          "title": "Semantic Text-to-Face GAN -ST^2FG. (arXiv:2107.10756v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.10679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattarai_M/0/1/0/all/0/1\">Manish Bhattarai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jensen_Curtis_A/0/1/0/all/0/1\">Aura Rose Jensen-Curtis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MartiNez_Ramon_M/0/1/0/all/0/1\">Manel Mart&#xed;Nez-Ram&#xf3;n</a>",
          "description": "Firefighting is a dynamic activity, in which numerous operations occur\nsimultaneously. Maintaining situational awareness (i.e., knowledge of current\nconditions and activities at the scene) is critical to the accurate\ndecision-making necessary for the safe and successful navigation of a fire\nenvironment by firefighters. Conversely, the disorientation caused by hazards\nsuch as smoke and extreme heat can lead to injury or even fatality. This\nresearch implements recent advancements in technology such as deep learning,\npoint cloud and thermal imaging, and augmented reality platforms to improve a\nfirefighter's situational awareness and scene navigation through improved\ninterpretation of that scene. We have designed and built a prototype embedded\nsystem that can leverage data streamed from cameras built into a firefighter's\npersonal protective equipment (PPE) to capture thermal, RGB color, and depth\nimagery and then deploy already developed deep learning models to analyze the\ninput data in real time. The embedded system analyzes and returns the processed\nimages via wireless streaming, where they can be viewed remotely and relayed\nback to the firefighter using an augmented reality platform that visualizes the\nresults of the analyzed inputs and draws the firefighter's attention to objects\nof interest, such as doors and windows otherwise invisible through smoke and\nflames.",
          "link": "http://arxiv.org/abs/2009.10679",
          "publishedOn": "2021-07-23T02:00:30.781Z",
          "wordCount": 663,
          "title": "An embedded deep learning system for augmented reality in firefighting applications. (arXiv:2009.10679v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ko_G/0/1/0/all/0/1\">Gihyuk Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_G/0/1/0/all/0/1\">Gyumin Lim</a>",
          "description": "Deep Neural Networks (DNNs) have shown remarkable performance in a diverse\nrange of machine learning applications. However, it is widely known that DNNs\nare vulnerable to simple adversarial perturbations, which causes the model to\nincorrectly classify inputs. In this paper, we propose a simple yet effective\nmethod to detect adversarial examples, using methods developed to explain the\nmodel's behavior. Our key observation is that adding small, humanly\nimperceptible perturbations can lead to drastic changes in the model\nexplanations, resulting in unusual or irregular forms of explanations. From\nthis insight, we propose an unsupervised detection of adversarial examples\nusing reconstructor networks trained only on model explanations of benign\nexamples. Our evaluations with MNIST handwritten dataset show that our method\nis capable of detecting adversarial examples generated by the state-of-the-art\nalgorithms with high confidence. To the best of our knowledge, this work is the\nfirst in suggesting unsupervised defense method using model explanations.",
          "link": "http://arxiv.org/abs/2107.10480",
          "publishedOn": "2021-07-23T02:00:30.774Z",
          "wordCount": 590,
          "title": "Unsupervised Detection of Adversarial Examples with Model Explanations. (arXiv:2107.10480v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chang-Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng-Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qibin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>",
          "description": "Label smoothing is an effective regularization tool for deep neural networks\n(DNNs), which generates soft labels by applying a weighted average between the\nuniform distribution and the hard label. It is often used to reduce the\noverfitting problem of training DNNs and further improve classification\nperformance. In this paper, we aim to investigate how to generate more reliable\nsoft labels. We present an Online Label Smoothing (OLS) strategy, which\ngenerates soft labels based on the statistics of the model prediction for the\ntarget category. The proposed OLS constructs a more reasonable probability\ndistribution between the target categories and non-target categories to\nsupervise DNNs. Experiments demonstrate that based on the same classification\nmodels, the proposed approach can effectively improve the classification\nperformance on CIFAR-100, ImageNet, and fine-grained datasets. Additionally,\nthe proposed method can significantly improve the robustness of DNN models to\nnoisy labels compared to current label smoothing approaches.",
          "link": "http://arxiv.org/abs/2011.12562",
          "publishedOn": "2021-07-23T02:00:30.768Z",
          "wordCount": 638,
          "title": "Delving Deep into Label Smoothing. (arXiv:2011.12562v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10638",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polat_S/0/1/0/all/0/1\">Songuel Polat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tremeau_A/0/1/0/all/0/1\">Alain Tremeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boochs_F/0/1/0/all/0/1\">Frank Boochs</a>",
          "description": "Due to its high spatial and spectral information content, hyperspectral\nimaging opens up new possibilities for a better understanding of data and\nscenes in a wide variety of applications. An essential part of this process of\nunderstanding is the classification part. In this article we present a general\nclassification approach based on the shape of spectral signatures. In contrast\nto classical classification approaches (e.g. SVM, KNN), not only reflectance\nvalues are considered, but also parameters such as curvature points, curvature\nvalues, and the curvature behavior of spectral signatures are used to develop\nshape-describing rules in order to use them for classification by a rule-based\nprocedure using IF-THEN queries. The flexibility and efficiency of the\nmethodology is demonstrated using datasets from two different application\nfields and leads to convincing results with good performance.",
          "link": "http://arxiv.org/abs/2107.10638",
          "publishedOn": "2021-07-23T02:00:30.759Z",
          "wordCount": 561,
          "title": "Rule-Based Classification of Hyperspectral Imaging Data. (arXiv:2107.10638v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>",
          "description": "Re-identification(ReID) aims at matching objects in surveillance cameras with\ndifferent viewpoints. It's developing very fast, but there is no processing\nmethod for the ReID task in multiple scenarios at this stage. However, this\ndose happen all the time in real life, such as the security scenarios. This\npaper explores a new scenario of Re-identification, which differs in\nperspective, background, and pose(walking or cycling).\n\nObviously, ordinary ReID processing methods cannot handle this scenario well.\nAs we all konw, the best way to deal with that it is to introduce image\ndatasets in this scanario, But this one is very expensive. To solve this\nproblem, this paper proposes a simple and effective way to generate images in\nsome new scenario, which is named Copy and Paste method based on Pose(CPP). The\nCPP is a method based on key point detection, using copy and paste, to\ncomposite a new semantic image dataset in two different semantic image\ndatasets. Such as, we can use pedestrians and bicycles to generate some images\nthat shows the same person rides on different bicycles. The CPP is suitable for\nReID tasks in new scenarios and it outperforms state-of-the-art on the original\ndatasets in original ReID tasks. Specifically, it can also have better\ngeneralization performance for third-party public datasets. Code and Datasets\nwhich composited by the CPP will be available in the future.",
          "link": "http://arxiv.org/abs/2107.10479",
          "publishedOn": "2021-07-23T02:00:30.751Z",
          "wordCount": 660,
          "title": "Copy and Paste method based on Pose for ReID. (arXiv:2107.10479v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01422",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Xu_S/0/1/0/all/0/1\">Shiqi Xu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_W/0/1/0/all/0/1\">Wenhui Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jonsson_J/0/1/0/all/0/1\">Joakim Jonsson</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Qian_R/0/1/0/all/0/1\">Ruobing Qian</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Konda_P/0/1/0/all/0/1\">Pavan Chandra Konda</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhou_K/0/1/0/all/0/1\">Kevin C. Zhou</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dai_Q/0/1/0/all/0/1\">Qionghai Dai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Berrocal_E/0/1/0/all/0/1\">Edouard Berrocal</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Horstmeyer_R/0/1/0/all/0/1\">Roarke Horstmeyer</a>",
          "description": "Noninvasive optical imaging through dynamic scattering media has numerous\nimportant biomedical applications but still remains a challenging task. While\nstandard methods aim to form images based upon optical absorption or\nfluorescent emission, it is also well-established that the temporal correlation\nof scattered coherent light diffuses through tissue much like optical\nintensity. Few works to date, however, have aimed to experimentally measure and\nprocess such data to demonstrate deep-tissue imaging of decorrelation dynamics.\nIn this work, we take advantage of a single-photon avalanche diode (SPAD) array\ncamera, with over one thousand detectors, to simultaneously detect speckle\nfluctuations at the single-photon level from 12 different phantom tissue\nsurface locations delivered via a customized fiber bundle array. We then apply\na deep neural network to convert the acquired single-photon measurements into\nvideo of scattering dynamics beneath rapidly decorrelating liquid tissue\nphantoms. We demonstrate the ability to record video of dynamic events\noccurring 5-8 mm beneath a decorrelating tissue phantom with mm-scale\nresolution and at a 2.5-10 Hz frame rate.",
          "link": "http://arxiv.org/abs/2107.01422",
          "publishedOn": "2021-07-23T02:00:30.733Z",
          "wordCount": 648,
          "title": "Imaging dynamics beneath turbid media via parallelized single-photon detection. (arXiv:2107.01422v2 [physics.optics] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sangdoo Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seong Joon Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_B/0/1/0/all/0/1\">Byeongho Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dongyoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1\">Junsuk Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sanghyuk Chun</a>",
          "description": "ImageNet has been arguably the most popular image classification benchmark,\nbut it is also the one with a significant level of label noise. Recent studies\nhave shown that many samples contain multiple classes, despite being assumed to\nbe a single-label benchmark. They have thus proposed to turn ImageNet\nevaluation into a multi-label task, with exhaustive multi-label annotations per\nimage. However, they have not fixed the training set, presumably because of a\nformidable annotation cost. We argue that the mismatch between single-label\nannotations and effectively multi-label images is equally, if not more,\nproblematic in the training setup, where random crops are applied. With the\nsingle-label annotations, a random crop of an image may contain an entirely\ndifferent object from the ground truth, introducing noisy or even incorrect\nsupervision during training. We thus re-label the ImageNet training set with\nmulti-labels. We address the annotation cost barrier by letting a strong image\nclassifier, trained on an extra source of data, generate the multi-labels. We\nutilize the pixel-wise multi-label predictions before the final pooling layer,\nin order to exploit the additional location-specific supervision signals.\nTraining on the re-labeled samples results in improved model performances\nacross the board. ResNet-50 attains the top-1 classification accuracy of 78.9%\non ImageNet with our localized multi-labels, which can be further boosted to\n80.2% with the CutMix regularization. We show that the models trained with\nlocalized multi-labels also outperforms the baselines on transfer learning to\nobject detection and instance segmentation tasks, and various robustness\nbenchmarks. The re-labeled ImageNet training set, pre-trained weights, and the\nsource code are available at {https://github.com/naver-ai/relabel_imagenet}.",
          "link": "http://arxiv.org/abs/2101.05022",
          "publishedOn": "2021-07-23T02:00:30.727Z",
          "wordCount": 746,
          "title": "Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels. (arXiv:2101.05022v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07044",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Emami_H/0/1/0/all/0/1\">Hajar Emami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_M/0/1/0/all/0/1\">Ming Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nejad_Davarani_S/0/1/0/all/0/1\">Siamak Nejad-Davarani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glide_Hurst_C/0/1/0/all/0/1\">Carri Glide-Hurst</a>",
          "description": "In medical image synthesis, model training could be challenging due to the\ninconsistencies between images of different modalities even with the same\npatient, typically caused by internal status/tissue changes as different\nmodalities are usually obtained at a different time. This paper proposes a\nnovel deep learning method, Structure-aware Generative Adversarial Network\n(SA-GAN), that preserves the shapes and locations of in-consistent structures\nwhen generating medical images. SA-GAN is employed to generate synthetic\ncomputed tomography (synCT) images from magnetic resonance imaging (MRI) with\ntwo parallel streams: the global stream translates the input from the MRI to\nthe CT domain while the local stream automatically segments the inconsistent\norgans, maintains their locations and shapes in MRI, and translates the organ\nintensities to CT. Through extensive experiments on a pelvic dataset, we\ndemonstrate that SA-GAN provides clinically acceptable accuracy on both synCTs\nand organ segmentation and supports MR-only treatment planning in disease sites\nwith internal organ status changes.",
          "link": "http://arxiv.org/abs/2105.07044",
          "publishedOn": "2021-07-23T02:00:30.719Z",
          "wordCount": 622,
          "title": "SA-GAN: Structure-Aware GAN for Organ-Preserving Synthetic CT Generation. (arXiv:2105.07044v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mohammad Mahmudul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mohammad Tariqul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1\">S. M. Mahbubur Rahman</a>",
          "description": "Head-mounted device-based human-computer interaction often requires\negocentric recognition of hand gestures and fingertips detection. In this\npaper, a unified approach of egocentric hand gesture recognition and fingertip\ndetection is introduced. The proposed algorithm uses a single convolutional\nneural network to predict the probabilities of finger class and positions of\nfingertips in one forward propagation. Instead of directly regressing the\npositions of fingertips from the fully connected layer, the ensemble of the\nposition of fingertips is regressed from the fully convolutional network.\nSubsequently, the ensemble average is taken to regress the final position of\nfingertips. Since the whole pipeline uses a single network, it is significantly\nfast in computation. Experimental results show that the proposed method\noutperforms the existing fingertip detection approaches including the Direct\nRegression and the Heatmap-based framework. The effectiveness of the proposed\nmethod is also shown in-the-wild scenario as well as in a use-case of virtual\nreality.",
          "link": "http://arxiv.org/abs/2101.02047",
          "publishedOn": "2021-07-23T02:00:30.712Z",
          "wordCount": 650,
          "title": "Unified Learning Approach for Egocentric Hand Gesture Recognition and Fingertip Detection. (arXiv:2101.02047v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Quellec_G/0/1/0/all/0/1\">Gwenol&#xe9; Quellec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajj_H/0/1/0/all/0/1\">Hassan Al Hajj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamard_M/0/1/0/all/0/1\">Mathieu Lamard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conze_P/0/1/0/all/0/1\">Pierre-Henri Conze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massin_P/0/1/0/all/0/1\">Pascale Massin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cochener_B/0/1/0/all/0/1\">B&#xe9;atrice Cochener</a>",
          "description": "In recent years, Artificial Intelligence (AI) has proven its relevance for\nmedical decision support. However, the \"black-box\" nature of successful AI\nalgorithms still holds back their wide-spread deployment. In this paper, we\ndescribe an eXplanatory Artificial Intelligence (XAI) that reaches the same\nlevel of performance as black-box AI, for the task of classifying Diabetic\nRetinopathy (DR) severity using Color Fundus Photography (CFP). This algorithm,\ncalled ExplAIn, learns to segment and categorize lesions in images; the final\nimage-level classification directly derives from these multivariate lesion\nsegmentations. The novelty of this explanatory framework is that it is trained\nfrom end to end, with image supervision only, just like black-box AI\nalgorithms: the concepts of lesions and lesion categories emerge by themselves.\nFor improved lesion localization, foreground/background separation is trained\nthrough self-supervision, in such a way that occluding foreground pixels\ntransforms the input image into a healthy-looking image. The advantage of such\nan architecture is that automatic diagnoses can be explained simply by an image\nand/or a few sentences. ExplAIn is evaluated at the image level and at the\npixel level on various CFP image datasets. We expect this new framework, which\njointly offers high classification performance and explainability, to\nfacilitate AI deployment.",
          "link": "http://arxiv.org/abs/2008.05731",
          "publishedOn": "2021-07-23T02:00:30.694Z",
          "wordCount": 693,
          "title": "ExplAIn: Explanatory Artificial Intelligence for Diabetic Retinopathy Diagnosis. (arXiv:2008.05731v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10476",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1\">Yiqing Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_M/0/1/0/all/0/1\">Meng Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_B/0/1/0/all/0/1\">Bin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_W/0/1/0/all/0/1\">Wenjia Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_W/0/1/0/all/0/1\">Weijing Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "Optical Coherence Tomography Angiography (OCTA) is a non-invasive and\nnon-contacting imaging technique providing visualization of microvasculature of\nretina and optic nerve head in human eyes in vivo. The adequate image quality\nof OCTA is the prerequisite for the subsequent quantification of retinal\nmicrovasculature. Traditionally, the image quality score based on signal\nstrength is used for discriminating low quality. However, it is insufficient\nfor identifying artefacts such as motion and off-centration, which rely\nspecialized knowledge and need tedious and time-consuming manual\nidentification. One of the most primary issues in OCTA analysis is to sort out\nthe foveal avascular zone (FAZ) region in the retina, which highly correlates\nwith any visual acuity disease. However, the variations in OCTA visual quality\naffect the performance of deep learning in any downstream marginally. Moreover,\nfiltering the low-quality OCTA images out is both labor-intensive and\ntime-consuming. To address these issues, we develop an automated computer-aided\nOCTA image processing system using deep neural networks as the classifier and\nsegmentor to help ophthalmologists in clinical diagnosis and research. This\nsystem can be an assistive tool as it can process OCTA images of different\nformats to assess the quality and segment the FAZ area. The source code is\nfreely available at https://github.com/shanzha09/COIPS.git.\n\nAnother major contribution is the large-scale OCTA dataset, namely\nOCTA-25K-IQA-SEG we publicize for performance evaluation. It is comprised of\nfour subsets, namely sOCTA-3$\\times$3-10k, sOCTA-6$\\times$6-14k,\nsOCTA-3$\\times$3-1.1k-seg, and dOCTA-6$\\times$6-1.1k-seg, which contains a\ntotal number of 25,665 images. The large-scale OCTA dataset is available at\nhttps://doi.org/10.5281/zenodo.5111975, https://doi.org/10.5281/zenodo.5111972.",
          "link": "http://arxiv.org/abs/2107.10476",
          "publishedOn": "2021-07-23T02:00:30.569Z",
          "wordCount": 729,
          "title": "A Deep Learning-based Quality Assessment and Segmentation System with a Large-scale Benchmark Dataset for Optical Coherence Tomographic Angiography Image. (arXiv:2107.10476v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1\">Zhendong Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongning Wang</a>",
          "description": "Crowdsourcing provides an efficient label collection schema for supervised\nmachine learning. However, to control annotation cost, each instance in the\ncrowdsourced data is typically annotated by a small number of annotators. This\ncreates a sparsity issue and limits the quality of machine learning models\ntrained on such data. In this paper, we study how to handle sparsity in\ncrowdsourced data using data augmentation. Specifically, we propose to directly\nlearn a classifier by augmenting the raw sparse annotations. We implement two\nprinciples of high-quality augmentation using Generative Adversarial Networks:\n1) the generated annotations should follow the distribution of authentic ones,\nwhich is measured by a discriminator; 2) the generated annotations should have\nhigh mutual information with the ground-truth labels, which is measured by an\nauxiliary network. Extensive experiments and comparisons against an array of\nstate-of-the-art learning from crowds methods on three real-world datasets\nproved the effectiveness of our data augmentation framework. It shows the\npotential of our algorithm for low-budget crowdsourcing in general.",
          "link": "http://arxiv.org/abs/2107.10449",
          "publishedOn": "2021-07-23T02:00:30.540Z",
          "wordCount": 601,
          "title": "Improve Learning from Crowds via Generative Augmentation. (arXiv:2107.10449v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thoma_F/0/1/0/all/0/1\">Felix Thoma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayer_J/0/1/0/all/0/1\">Johannes Bayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yakun Li</a>",
          "description": "The development of digitization methods for line drawings (especially in the\narea of electrical engineering) relies on the availability of publicly\navailable training and evaluation data. This paper presents such an image set\nalong with annotations. The dataset consists of 1152 images of 144 circuits by\n12 drafters and 48 563 annotations. Each of these images depicts an electrical\ncircuit diagram, taken by consumer grade cameras under varying lighting\nconditions and perspectives. A variety of different pencil types and surface\nmaterials has been used. For each image, all individual electrical components\nare annotated with bounding boxes and one out of 45 class labels. In order to\nsimplify a graph extraction process, different helper symbols like junction\npoints and crossovers are introduced, while texts are annotated as well. The\ngeometric and taxonomic problems arising from this task as well as the classes\nthemselves and statistics of their appearances are stated. The performance of a\nstandard Faster RCNN on the dataset is provided as an object detection\nbaseline.",
          "link": "http://arxiv.org/abs/2107.10373",
          "publishedOn": "2021-07-23T02:00:30.533Z",
          "wordCount": 613,
          "title": "A Public Ground-Truth Dataset for Handwritten Circuit Diagram Images. (arXiv:2107.10373v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "This paper presents a simple and effective approach to solving the\nmulti-label classification problem. The proposed approach leverages Transformer\ndecoders to query the existence of a class label. The use of Transformer is\nrooted in the need of extracting local discriminative features adaptively for\ndifferent labels, which is a strongly desired property due to the existence of\nmultiple objects in one image. The built-in cross-attention module in the\nTransformer decoder offers an effective way to use label embeddings as queries\nto probe and pool class-related features from a feature map computed by a\nvision backbone for subsequent binary classifications. Compared with prior\nworks, the new framework is simple, using standard Transformers and vision\nbackbones, and effective, consistently outperforming all previous works on five\nmulti-label classification data sets, including MS-COCO, PASCAL VOC, NUS-WIDE,\nand Visual Genome. Particularly, we establish $91.3\\%$ mAP on MS-COCO. We hope\nits compact structure, simple implementation, and superior performance serve as\na strong baseline for multi-label classification tasks and future studies. The\ncode will be available soon at https://github.com/SlongLiu/query2labels.",
          "link": "http://arxiv.org/abs/2107.10834",
          "publishedOn": "2021-07-23T02:00:30.527Z",
          "wordCount": 612,
          "title": "Query2Label: A Simple Transformer Way to Multi-Label Classification. (arXiv:2107.10834v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10771",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yichao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhiyong Gao</a>",
          "description": "Efficiently modeling spatial-temporal information in videos is crucial for\naction recognition. To achieve this goal, state-of-the-art methods typically\nemploy the convolution operator and the dense interaction modules such as\nnon-local blocks. However, these methods cannot accurately fit the diverse\nevents in videos. On the one hand, the adopted convolutions are with fixed\nscales, thus struggling with events of various scales. On the other hand, the\ndense interaction modeling paradigm only achieves sub-optimal performance as\naction-irrelevant parts bring additional noises for the final prediction. In\nthis paper, we propose a unified action recognition framework to investigate\nthe dynamic nature of video content by introducing the following designs.\nFirst, when extracting local cues, we generate the spatial-temporal kernels of\ndynamic-scale to adaptively fit the diverse events. Second, to accurately\naggregate these cues into a global video representation, we propose to mine the\ninteractions only among a few selected foreground objects by a Transformer,\nwhich yields a sparse paradigm. We call the proposed framework as Event\nAdaptive Network (EAN) because both key designs are adaptive to the input video\ncontent. To exploit the short-term motions within local segments, we propose a\nnovel and efficient Latent Motion Code (LMC) module, further improving the\nperformance of the framework. Extensive experiments on several large-scale\nvideo datasets, e.g., Something-to-Something V1&V2, Kinetics, and Diving48,\nverify that our models achieve state-of-the-art or competitive performances at\nlow FLOPs. Codes are available at:\nhttps://github.com/tianyuan168326/EAN-Pytorch.",
          "link": "http://arxiv.org/abs/2107.10771",
          "publishedOn": "2021-07-23T02:00:30.515Z",
          "wordCount": 688,
          "title": "EAN: Event Adaptive Network for Enhanced Action Recognition. (arXiv:2107.10771v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10833",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_L/0/1/0/all/0/1\">Liangbin Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>",
          "description": "Though many attempts have been made in blind super-resolution to restore\nlow-resolution images with unknown and complex degradations, they are still far\nfrom addressing general real-world degraded images. In this work, we extend the\npowerful ESRGAN to a practical restoration application (namely, Real-ESRGAN),\nwhich is trained with pure synthetic data. Specifically, a high-order\ndegradation modeling process is introduced to better simulate complex\nreal-world degradations. We also consider the common ringing and overshoot\nartifacts in the synthesis process. In addition, we employ a U-Net\ndiscriminator with spectral normalization to increase discriminator capability\nand stabilize the training dynamics. Extensive comparisons have shown its\nsuperior visual performance than prior works on various real datasets. We also\nprovide efficient implementations to synthesize training pairs on the fly.",
          "link": "http://arxiv.org/abs/2107.10833",
          "publishedOn": "2021-07-23T02:00:30.495Z",
          "wordCount": 585,
          "title": "Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data. (arXiv:2107.10833v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saunders_B/0/1/0/all/0/1\">Ben Saunders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camgoz_N/0/1/0/all/0/1\">Necati Cihan Camgoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>",
          "description": "The visual anonymisation of sign language data is an essential task to\naddress privacy concerns raised by large-scale dataset collection. Previous\nanonymisation techniques have either significantly affected sign comprehension\nor required manual, labour-intensive work.\n\nIn this paper, we formally introduce the task of Sign Language Video\nAnonymisation (SLVA) as an automatic method to anonymise the visual appearance\nof a sign language video whilst retaining the meaning of the original sign\nlanguage sequence. To tackle SLVA, we propose AnonySign, a novel automatic\napproach for visual anonymisation of sign language data. We first extract pose\ninformation from the source video to remove the original signer appearance. We\nnext generate a photo-realistic sign language video of a novel appearance from\nthe pose sequence, using image-to-image translation methods in a conditional\nvariational autoencoder framework. An approximate posterior style distribution\nis learnt, which can be sampled from to synthesise novel human appearances. In\naddition, we propose a novel \\textit{style loss} that ensures style consistency\nin the anonymised sign language videos.\n\nWe evaluate AnonySign for the SLVA task with extensive quantitative and\nqualitative experiments highlighting both realism and anonymity of our novel\nhuman appearance synthesis. In addition, we formalise an anonymity perceptual\nstudy as an evaluation criteria for the SLVA task and showcase that video\nanonymisation using AnonySign retains the original sign language content.",
          "link": "http://arxiv.org/abs/2107.10685",
          "publishedOn": "2021-07-23T02:00:30.488Z",
          "wordCount": 664,
          "title": "AnonySIGN: Novel Human Appearance Synthesis for Sign Language Video Anonymisation. (arXiv:2107.10685v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shangzhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakab_T/0/1/0/all/0/1\">Tomas Jakab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rupprecht_C/0/1/0/all/0/1\">Christian Rupprecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>",
          "description": "Learning deformable 3D objects from 2D images is an extremely ill-posed\nproblem. Existing methods rely on explicit supervision to establish multi-view\ncorrespondences, such as template shape models and keypoint annotations, which\nrestricts their applicability on objects \"in the wild\". In this paper, we\npropose to use monocular videos, which naturally provide correspondences across\ntime, allowing us to learn 3D shapes of deformable object categories without\nexplicit keypoints or template shapes. Specifically, we present DOVE, which\nlearns to predict 3D canonical shape, deformation, viewpoint and texture from a\nsingle 2D image of a bird, given a bird video collection as well as\nautomatically obtained silhouettes and optical flows as training data. Our\nmethod reconstructs temporally consistent 3D shape and deformation, which\nallows us to animate and re-render the bird from arbitrary viewpoints from a\nsingle image.",
          "link": "http://arxiv.org/abs/2107.10844",
          "publishedOn": "2021-07-23T02:00:30.480Z",
          "wordCount": 576,
          "title": "DOVE: Learning Deformable 3D Objects by Watching Videos. (arXiv:2107.10844v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10806",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fernandez_Quilez_A/0/1/0/all/0/1\">Alvaro Fernandez-Quilez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eftestol_T/0/1/0/all/0/1\">Trygve Eftest&#xf8;l</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goodwin_M/0/1/0/all/0/1\">Morten Goodwin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kjosavik_S/0/1/0/all/0/1\">Svein Reidar Kjosavik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oppedal_K/0/1/0/all/0/1\">Ketil Oppedal</a>",
          "description": "Prostate cancer (PCa) is the second most common cancer diagnosed among men\nworldwide. The current PCa diagnostic pathway comes at the cost of substantial\noverdiagnosis, leading to unnecessary treatment and further testing.\nBi-parametric magnetic resonance imaging (bp-MRI) based on apparent diffusion\ncoefficient maps (ADC) and T2-weighted (T2w) sequences has been proposed as a\ntriage test to differentiate between clinically significant (cS) and\nnon-clinically significant (ncS) prostate lesions. However, analysis of the\nsequences relies on expertise, requires specialized training, and suffers from\ninter-observer variability. Deep learning (DL) techniques hold promise in tasks\nsuch as classification and detection. Nevertheless, they rely on large amounts\nof annotated data which is not common in the medical field. In order to\npalliate such issues, existing works rely on transfer learning (TL) and\nImageNet pre-training, which has been proven to be sub-optimal for the medical\nimaging domain. In this paper, we present a patch-based pre-training strategy\nto distinguish between cS and ncS lesions which exploit the region of interest\n(ROI) of the patched source domain to efficiently train a classifier in the\nfull-slice target domain which does not require annotations by making use of\ntransfer learning (TL). We provide a comprehensive comparison between several\nCNNs architectures and different settings which are presented as a baseline.\nMoreover, we explore cross-domain TL which exploits both MRI modalities and\nimproves single modality results. Finally, we show how our approaches\noutperform the standard approaches by a considerable margin",
          "link": "http://arxiv.org/abs/2107.10806",
          "publishedOn": "2021-07-23T02:00:30.463Z",
          "wordCount": 701,
          "title": "Self-transfer learning via patches: A prostate cancer triage approach based on bi-parametric MRI. (arXiv:2107.10806v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10718",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaggin_H/0/1/0/all/0/1\">Hanna K. Gaggin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Weichung Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "Assessment of cardiovascular disease (CVD) with cine magnetic resonance\nimaging (MRI) has been used to non-invasively evaluate detailed cardiac\nstructure and function. Accurate segmentation of cardiac structures from cine\nMRI is a crucial step for early diagnosis and prognosis of CVD, and has been\ngreatly improved with convolutional neural networks (CNN). There, however, are\na number of limitations identified in CNN models, such as limited\ninterpretability and high complexity, thus limiting their use in clinical\npractice. In this work, to address the limitations, we propose a lightweight\nand interpretable machine learning model, successive subspace learning with the\nsubspace approximation with adjusted bias (Saab) transform, for accurate and\nefficient segmentation from cine MRI. Specifically, our segmentation framework\nis comprised of the following steps: (1) sequential expansion of near-to-far\nneighborhood at different resolutions; (2) channel-wise subspace approximation\nusing the Saab transform for unsupervised dimension reduction; (3) class-wise\nentropy guided feature selection for supervised dimension reduction; (4)\nconcatenation of features and pixel-wise classification with gradient boost;\nand (5) conditional random field for post-processing. Experimental results on\nthe ACDC 2017 segmentation database, showed that our framework performed better\nthan state-of-the-art U-Net models with 200$\\times$ fewer parameters in\ndelineating the left ventricle, right ventricle, and myocardium, thus showing\nits potential to be used in clinical practice.",
          "link": "http://arxiv.org/abs/2107.10718",
          "publishedOn": "2021-07-23T02:00:30.453Z",
          "wordCount": 696,
          "title": "Segmentation of Cardiac Structures via Successive Subspace Learning with Saab Transform from Cine MRI. (arXiv:2107.10718v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1\">Chenyu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1\">Ran Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xinyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Weihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>",
          "description": "Current methods of multi-person pose estimation typically treat the\nlocalization and the association of body joints separately. It is convenient\nbut inefficient, leading to additional computation and a waste of time. This\npaper, however, presents a novel framework PoseDet (Estimating Pose by\nDetection) to localize and associate body joints simultaneously at higher\ninference speed. Moreover, we propose the keypoint-aware pose embedding to\nrepresent an object in terms of the locations of its keypoints. The proposed\npose embedding contains semantic and geometric information, allowing us to\naccess discriminative and informative features efficiently. It is utilized for\ncandidate classification and body joint localization in PoseDet, leading to\nrobust predictions of various poses. This simple framework achieves an\nunprecedented speed and a competitive accuracy on the COCO benchmark compared\nwith state-of-the-art methods. Extensive experiments on the CrowdPose benchmark\nshow the robustness in the crowd scenes. Source code is available.",
          "link": "http://arxiv.org/abs/2107.10466",
          "publishedOn": "2021-07-23T02:00:30.446Z",
          "wordCount": 590,
          "title": "PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding. (arXiv:2107.10466v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10563",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Herfet_T/0/1/0/all/0/1\">Thorsten Herfet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chelli_K/0/1/0/all/0/1\">Kelvin Chelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lange_T/0/1/0/all/0/1\">Tobias Lange</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kremer_R/0/1/0/all/0/1\">Robin Kremer</a>",
          "description": "In recent years, light field (LF) capture and processing has become an\nintegral part of media production. The richness of information available in LFs\nhas enabled novel applications like post-capture depth-of-field editing, 3D\nreconstruction, segmentation and matting, saliency detection, object detection\nand recognition, and mixed reality. The efficacy of such applications depends\non certain underlying requirements, which are often ignored. For example, some\noperations such as noise-reduction, or hyperfan-filtering are only possible if\na scene point Lambertian radiator. Some other operations such as the removal of\nobstacles or looking behind objects are only possible if there is at least one\nray capturing the required scene point. Consequently, the ray distribution\nrepresenting a certain scene point is an important characteristic for\nevaluating processing possibilities. The primary idea in this paper is to\nestablish a relation between the capturing setup and the rays of the LF. To\nthis end, we discretize the view frustum. Traditionally, a uniform\ndiscretization of the view frustum results in voxels that represents a single\nsample on a regularly spaced, 3-D grid. Instead, we use frustum-shaped voxels\n(froxels), by using depth and capturing-setup dependent discretization of the\nview frustum. Based on such discretization, we count the number of rays mapping\nto the same pixel on the capturing device(s). By means of this count, we\npropose histograms of ray-counts over the froxels (fristograms). Fristograms\ncan be used as a tool to analyze and reveal interesting aspects of the\nunderlying LF, like the number of rays originating from a scene point and the\ncolor distribution of these rays. As an example, we show its ability by\nsignificantly reducing the number of rays which enables noise reduction while\nmaintaining the realistic rendering of non-Lambertian or partially occluded\nregions.",
          "link": "http://arxiv.org/abs/2107.10563",
          "publishedOn": "2021-07-23T02:00:30.438Z",
          "wordCount": 734,
          "title": "Fristograms: Revealing and Exploiting Light Field Internals. (arXiv:2107.10563v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke-Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Taiping Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shice Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bangjie Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>",
          "description": "In pursuit of consolidating the face verification systems, prior face\nanti-spoofing studies excavate the hidden cues in original images to\ndiscriminate real persons and diverse attack types with the assistance of\nauxiliary supervision. However, limited by the following two inherent\ndisturbances in their training process: 1) Complete facial structure in a\nsingle image. 2) Implicit subdomains in the whole dataset, these methods are\nprone to stick on memorization of the entire training dataset and show\nsensitivity to nonhomologous domain distribution. In this paper, we propose\nStructure Destruction Module and Content Combination Module to address these\ntwo imitations separately. The former mechanism destroys images into patches to\nconstruct a non-structural input, while the latter mechanism recombines patches\nfrom different subdomains or classes into a mixup construct. Based on this\nsplitting-and-splicing operation, Local Relation Modeling Module is further\nproposed to model the second-order relationship between patches. We evaluate\nour method on extensive public datasets and promising experimental results to\ndemonstrate the reliability of our method against state-of-the-art competitors.",
          "link": "http://arxiv.org/abs/2107.10628",
          "publishedOn": "2021-07-23T02:00:30.431Z",
          "wordCount": 612,
          "title": "Structure Destruction and Content Combination for Face Anti-Spoofing. (arXiv:2107.10628v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10607",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ibing_M/0/1/0/all/0/1\">Moritz Ibing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_I/0/1/0/all/0/1\">Isaak Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobbelt_L/0/1/0/all/0/1\">Leif Kobbelt</a>",
          "description": "Previous approaches to generate shapes in a 3D setting train a GAN on the\nlatent space of an autoencoder (AE). Even though this produces convincing\nresults, it has two major shortcomings. As the GAN is limited to reproduce the\ndataset the AE was trained on, we cannot reuse a trained AE for novel data.\nFurthermore, it is difficult to add spatial supervision into the generation\nprocess, as the AE only gives us a global representation. To remedy these\nissues, we propose to train the GAN on grids (i.e. each cell covers a part of a\nshape). In this representation each cell is equipped with a latent vector\nprovided by an AE. This localized representation enables more expressiveness\n(since the cell-based latent vectors can be combined in novel ways) as well as\nspatial control of the generation process (e.g. via bounding boxes). Our method\noutperforms the current state of the art on all established evaluation\nmeasures, proposed for quantitatively evaluating the generative capabilities of\nGANs. We show limitations of these measures and propose the adaptation of a\nrobust criterion from statistical analysis as an alternative.",
          "link": "http://arxiv.org/abs/2107.10607",
          "publishedOn": "2021-07-23T02:00:30.424Z",
          "wordCount": 626,
          "title": "3D Shape Generation with Grid-based Implicit Functions. (arXiv:2107.10607v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wanqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Lizhong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hui Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>",
          "description": "The Self-Rating Depression Scale (SDS) questionnaire is commonly utilized for\neffective depression preliminary screening. The uncontrolled self-administered\nmeasure, on the other hand, maybe readily influenced by insouciant or dishonest\nresponses, yielding different findings from the clinician-administered\ndiagnostic. Facial expression (FE) and behaviors are important in\nclinician-administered assessments, but they are underappreciated in\nself-administered evaluations. We use a new dataset of 200 participants to\ndemonstrate the validity of self-rating questionnaires and their accompanying\nquestion-by-question video recordings in this study. We offer an end-to-end\nsystem to handle the face video recording that is conditioned on the\nquestionnaire answers and the responding time to automatically interpret\nsadness from the SDS assessment and the associated video. We modified a 3D-CNN\nfor temporal feature extraction and compared various state-of-the-art temporal\nmodeling techniques. The superior performance of our system shows the validity\nof combining facial video recording with the SDS score for more accurate\nself-diagnose.",
          "link": "http://arxiv.org/abs/2107.10712",
          "publishedOn": "2021-07-23T02:00:30.413Z",
          "wordCount": 614,
          "title": "Deep 3D-CNN for Depression Diagnosis with Facial Video Recording of Self-Rating Depression Scale Questionnaire. (arXiv:2107.10712v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10624",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1\">Pavlo Molchanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_J/0/1/0/all/0/1\">Jimmy Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongxu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fusi_N/0/1/0/all/0/1\">Nicolo Fusi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahdat_A/0/1/0/all/0/1\">Arash Vahdat</a>",
          "description": "Given a trained network, how can we accelerate it to meet efficiency needs\nfor deployment on particular hardware? The commonly used hardware-aware network\ncompression techniques address this question with pruning, kernel fusion,\nquantization and lowering precision. However, these approaches do not change\nthe underlying network operations. In this paper, we propose hardware-aware\nnetwork transformation (HANT), which accelerates a network by replacing\ninefficient operations with more efficient alternatives using a neural\narchitecture search like approach. HANT tackles the problem in two phase: In\nthe first phase, a large number of alternative operations per every layer of\nthe teacher model is trained using layer-wise feature map distillation. In the\nsecond phase, the combinatorial selection of efficient operations is relaxed to\nan integer optimization problem that can be solved in a few seconds. We extend\nHANT with kernel fusion and quantization to improve throughput even further.\nOur experimental results on accelerating the EfficientNet family show that HANT\ncan accelerate them by up to 3.6x with <0.4% drop in the top-1 accuracy on the\nImageNet dataset. When comparing the same latency level, HANT can accelerate\nEfficientNet-B4 to the same latency as EfficientNet-B1 while having 3% higher\naccuracy. We examine a large pool of operations, up to 197 per layer, and we\nprovide insights into the selected operations and final architectures.",
          "link": "http://arxiv.org/abs/2107.10624",
          "publishedOn": "2021-07-23T02:00:30.385Z",
          "wordCount": 656,
          "title": "HANT: Hardware-Aware Network Transformation. (arXiv:2107.10624v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sihyun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Sangwoo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungsoo Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Abstract reasoning, i.e., inferring complicated patterns from given\nobservations, is a central building block of artificial general intelligence.\nWhile humans find the answer by either eliminating wrong candidates or first\nconstructing the answer, prior deep neural network (DNN)-based methods focus on\nthe former discriminative approach. This paper aims to design a framework for\nthe latter approach and bridge the gap between artificial and human\nintelligence. To this end, we propose logic-guided generation (LoGe), a novel\ngenerative DNN framework that reduces abstract reasoning as an optimization\nproblem in propositional logic. LoGe is composed of three steps: extract\npropositional variables from images, reason the answer variables with a logic\nlayer, and reconstruct the answer image from the variables. We demonstrate that\nLoGe outperforms the black box DNN frameworks for generative abstract reasoning\nunder the RAVEN benchmark, i.e., reconstructing answers based on capturing\ncorrect rules of various attributes from observations.",
          "link": "http://arxiv.org/abs/2107.10493",
          "publishedOn": "2021-07-23T02:00:30.325Z",
          "wordCount": 601,
          "title": "Abstract Reasoning via Logic-guided Generation. (arXiv:2107.10493v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sledge_I/0/1/0/all/0/1\">Isaac J. Sledge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toole_C/0/1/0/all/0/1\">Christopher D. Toole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maestri_J/0/1/0/all/0/1\">Joseph A. Maestri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1\">Jose C. Principe</a>",
          "description": "We propose a memory-based framework for real-time, data-efficient target\nanalysis in forward-looking-sonar (FLS) imagery. Our framework relies on first\nremoving non-discriminative details from the imagery using a small-scale\nDenseNet-inspired network. Doing so simplifies ensuing analyses and permits\ngeneralizing from few labeled examples. We then cascade the filtered imagery\ninto a novel NeuralRAM-based convolutional matching network, NRMN, for low-shot\ntarget recognition. We employ a small-scale FlowNet, LFN to align and register\nFLS imagery across local temporal scales. LFN enables target label consensus\nvoting across images and generally improves target detection and recognition\nrates.\n\nWe evaluate our framework using real-world FLS imagery with multiple broad\ntarget classes that have high intra-class variability and rich sub-class\nstructure. We show that few-shot learning, with anywhere from ten to thirty\nclass-specific exemplars, performs similarly to supervised deep networks\ntrained on hundreds of samples per class. Effective zero-shot learning is also\npossible. High performance is realized from the inductive-transfer properties\nof NRMNs when distractor elements are removed.",
          "link": "http://arxiv.org/abs/2107.10504",
          "publishedOn": "2021-07-23T02:00:30.262Z",
          "wordCount": 616,
          "title": "External-Memory Networks for Low-Shot Learning of Targets in Forward-Looking-Sonar Imagery. (arXiv:2107.10504v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Hyukseong Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_A/0/1/0/all/0/1\">Amir Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kevin G. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Amit Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_R/0/1/0/all/0/1\">Rajan Bhattacharyya</a>",
          "description": "This paper proposes the CogSense system, which is inspired by sense-making\ncognition and perception in the mammalian brain to perform perception error\ndetection and perception parameter adaptation using probabilistic signal\ntemporal logic. As a specific application, a contrast-based perception adaption\nmethod is presented and validated. The proposed method evaluates perception\nerrors using heterogeneous probe functions computed from the detected objects\nand subsequently solves a contrast optimization problem to correct perception\nerrors. The CogSense probe functions utilize the characteristics of geometry,\ndynamics, and detected blob image quality of the objects to develop axioms in a\nprobabilistic signal temporal logic framework. By evaluating these axioms, we\ncan formally verify whether the detections are valid or erroneous. Further,\nusing the CogSense axioms, we generate the probabilistic signal temporal\nlogic-based constraints to finally solve the contrast-based optimization\nproblem to reduce false positives and false negatives.",
          "link": "http://arxiv.org/abs/2107.10456",
          "publishedOn": "2021-07-23T02:00:30.244Z",
          "wordCount": 579,
          "title": "CogSense: A Cognitively Inspired Framework for Perception Adaptation. (arXiv:2107.10456v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhengxiong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1\">Erjin Zhou</a>",
          "description": "Most existing human pose estimation (HPE) methods exploit multi-scale\ninformation by fusing feature maps of four different spatial sizes, \\ie $1/4$,\n$1/8$, $1/16$, and $1/32$ of the input image. There are two drawbacks of this\nstrategy: 1) feature maps of different spatial sizes may be not well aligned\nspatially, which potentially hurts the accuracy of keypoint location; 2) these\nscales are fixed and inflexible, which may restrict the generalization ability\nover various human sizes. Towards these issues, we propose an adaptive dilated\nconvolution (ADC). It can generate and fuse multi-scale features of the same\nspatial sizes by setting different dilation rates for different channels. More\nimportantly, these dilation rates are generated by a regression module. It\nenables ADC to adaptively adjust the fused scales and thus ADC may generalize\nbetter to various human sizes. ADC can be end-to-end trained and easily plugged\ninto existing methods. Extensive experiments show that ADC can bring consistent\nimprovements to various HPE methods. The source codes will be released for\nfurther research.",
          "link": "http://arxiv.org/abs/2107.10477",
          "publishedOn": "2021-07-23T02:00:30.236Z",
          "wordCount": 604,
          "title": "Adaptive Dilated Convolution For Human Pose Estimation. (arXiv:2107.10477v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuesong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_M/0/1/0/all/0/1\">Meihao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1\">Jing Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "Contrastive self-supervised learning (SSL) has achieved great success in\nunsupervised visual representation learning by maximizing the similarity\nbetween two augmented views of the same image (positive pairs) and\nsimultaneously contrasting other different images (negative pairs). However,\nthis type of methods, such as SimCLR and MoCo, relies heavily on a large number\nof negative pairs and thus requires either large batches or memory banks. In\ncontrast, some recent non-contrastive SSL methods, such as BYOL and SimSiam,\nattempt to discard negative pairs by introducing asymmetry and show remarkable\nperformance. Unfortunately, to avoid collapsed solutions caused by not using\nnegative pairs, these methods require sophisticated asymmetry designs. In this\npaper, we argue that negative pairs are still necessary but one is sufficient,\ni.e., triplet is all you need. A simple triplet-based loss can achieve\nsurprisingly good performance without requiring large batches or asymmetry.\nMoreover, we observe that unsupervised visual representation learning can gain\nsignificantly from randomness. Based on this observation, we propose a simple\nplug-in RandOm MApping (ROMA) strategy by randomly mapping samples into other\nspaces and enforcing these randomly projected samples to satisfy the same\ncorrelation requirement. The proposed ROMA strategy not only achieves the\nstate-of-the-art performance in conjunction with the triplet-based loss, but\nalso can further effectively boost other SSL methods.",
          "link": "http://arxiv.org/abs/2107.10419",
          "publishedOn": "2021-07-23T02:00:30.210Z",
          "wordCount": 665,
          "title": "Triplet is All You Need with Random Mappings for Unsupervised Visual Representation Learning. (arXiv:2107.10419v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10327",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sengupta_A/0/1/0/all/0/1\">Arindam Sengupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_S/0/1/0/all/0/1\">Siyang Cao</a>",
          "description": "In this paper we presented mmPose-NLP, a novel Natural Language Processing\n(NLP) inspired Sequence-to-Sequence (Seq2Seq) skeletal key-point estimator\nusing millimeter-wave (mmWave) radar data. To the best of the author's\nknowledge, this is the first method to precisely estimate upto 25 skeletal\nkey-points using mmWave radar data alone. Skeletal pose estimation is critical\nin several applications ranging from autonomous vehicles, traffic monitoring,\npatient monitoring, gait analysis, to defense security forensics, and aid both\npreventative and actionable decision making. The use of mmWave radars for this\ntask, over traditionally employed optical sensors, provide several advantages,\nprimarily its operational robustness to scene lighting and adverse weather\nconditions, where optical sensor performance degrade significantly. The mmWave\nradar point-cloud (PCL) data is first voxelized (analogous to tokenization in\nNLP) and $N$ frames of the voxelized radar data (analogous to a text paragraph\nin NLP) is subjected to the proposed mmPose-NLP architecture, where the voxel\nindices of the 25 skeletal key-points (analogous to keyword extraction in NLP)\nare predicted. The voxel indices are converted back to real world 3-D\ncoordinates using the voxel dictionary used during the tokenization process.\nMean Absolute Error (MAE) metrics were used to measure the accuracy of the\nproposed system against the ground truth, with the proposed mmPose-NLP offering\n<3 cm localization errors in the depth, horizontal and vertical axes. The\neffect of the number of input frames vs performance/accuracy was also studied\nfor N = {1,2,..,10}. A comprehensive methodology, results, discussions and\nlimitations are presented in this paper. All the source codes and results are\nmade available on GitHub for furthering research and development in this\ncritical yet emerging domain of skeletal key-point estimation using mmWave\nradars.",
          "link": "http://arxiv.org/abs/2107.10327",
          "publishedOn": "2021-07-23T02:00:30.198Z",
          "wordCount": 729,
          "title": "mmPose-NLP: A Natural Language Processing Approach to Precise Skeletal Pose Estimation using mmWave Radars. (arXiv:2107.10327v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivanovic_B/0/1/0/all/0/1\">Boris Ivanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>",
          "description": "Forecasting the behavior of other agents is an integral part of the modern\nrobotic autonomy stack, especially in safety-critical scenarios with\nhuman-robot interaction, such as autonomous driving. In turn, there has been a\nsignificant amount of interest and research in trajectory forecasting,\nresulting in a wide variety of approaches. Common to all works, however, is the\nuse of the same few accuracy-based evaluation metrics, e.g., displacement error\nand log-likelihood. While these metrics are informative, they are task-agnostic\nand predictions that are evaluated as equal can lead to vastly different\noutcomes, e.g., in downstream planning and decision making. In this work, we\ntake a step back and critically evaluate current trajectory forecasting\nmetrics, proposing task-aware metrics as a better measure of performance in\nsystems where prediction is being deployed. We additionally present one example\nof such a metric, incorporating planning-awareness within existing trajectory\nforecasting metrics.",
          "link": "http://arxiv.org/abs/2107.10297",
          "publishedOn": "2021-07-23T02:00:30.189Z",
          "wordCount": 583,
          "title": "Rethinking Trajectory Forecasting Evaluation. (arXiv:2107.10297v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minghan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1\">Maani Ghaffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Huei Peng</a>",
          "description": "This paper proposes a correspondence-free method for point cloud rotational\nregistration. We learn an embedding for each point cloud in a feature space\nthat preserves the SO(3)-equivariance property, enabled by recent developments\nin equivariant neural networks. The proposed shape registration method achieves\nthree major advantages through combining equivariant feature learning with\nimplicit shape models. First, the necessity of data association is removed\nbecause of the permutation-invariant property in network architectures similar\nto PointNet. Second, the registration in feature space can be solved in\nclosed-form using Horn's method due to the SO(3)-equivariance property. Third,\nthe registration is robust to noise in the point cloud because of implicit\nshape learning. The experimental results show superior performance compared\nwith existing correspondence-free deep registration methods.",
          "link": "http://arxiv.org/abs/2107.10296",
          "publishedOn": "2021-07-23T02:00:30.171Z",
          "wordCount": 570,
          "title": "Correspondence-Free Point Cloud Registration with SO(3)-Equivariant Implicit Shape Representations. (arXiv:2107.10296v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1\">Vinija Jain</a>",
          "description": "Causality knowledge is vital to building robust AI systems. Deep learning\nmodels often perform poorly on tasks that require causal reasoning, which is\noften derived using some form of commonsense knowledge not immediately\navailable in the input but implicitly inferred by humans. Prior work has\nunraveled spurious observational biases that models fall prey to in the absence\nof causality. While language representation models preserve contextual\nknowledge within learned embeddings, they do not factor in causal relationships\nduring training. By blending causal relationships with the input features to an\nexisting model that performs visual cognition tasks (such as scene\nunderstanding, video captioning, video question-answering, etc.), better\nperformance can be achieved owing to the insight causal relationships bring\nabout. Recently, several models have been proposed that have tackled the task\nof mining causal data from either the visual or textual modality. However,\nthere does not exist widespread research that mines causal relationships by\njuxtaposing the visual and language modalities. While images offer a rich and\neasy-to-process resource for us to mine causality knowledge from, videos are\ndenser and consist of naturally time-ordered events. Also, textual information\noffers details that could be implicit in videos. We propose iReason, a\nframework that infers visual-semantic commonsense knowledge using both videos\nand natural language captions. Furthermore, iReason's architecture integrates a\ncausal rationalization module to aid the process of interpretability, error\nanalysis and bias detection. We demonstrate the effectiveness of iReason using\na two-pronged comparative analysis with language representation learning models\n(BERT, GPT-2) as well as current state-of-the-art multimodal causality models.",
          "link": "http://arxiv.org/abs/2107.10300",
          "publishedOn": "2021-07-23T02:00:30.160Z",
          "wordCount": 723,
          "title": "iReason: Multimodal Commonsense Reasoning using Videos and Natural Language with Interpretability. (arXiv:2107.10300v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nalaie_K/0/1/0/all/0/1\">Keivan Nalaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Rong Zheng</a>",
          "description": "In surveillance and search and rescue applications, it is important to\nperform multi-target tracking (MOT) in real-time on low-end devices. Today's\nMOT solutions employ deep neural networks, which tend to have high computation\ncomplexity. Recognizing the effects of frame sizes on tracking performance, we\npropose DeepScale, a model agnostic frame size selection approach that operates\non top of existing fully convolutional network-based trackers to accelerate\ntracking throughput. In the training stage, we incorporate detectability scores\ninto a one-shot tracker architecture so that DeepScale can learn representation\nestimations for different frame sizes in a self-supervised manner. During\ninference, based on user-controlled parameters, it can find a suitable\ntrade-off between tracking accuracy and speed by adapting frame sizes at run\ntime. Extensive experiments and benchmark tests on MOT datasets demonstrate the\neffectiveness and flexibility of DeepScale. Compared to a state-of-the-art\ntracker, DeepScale++, a variant of DeepScale achieves 1.57X accelerated with\nonly moderate degradation (~ 2.4) in tracking accuracy on the MOT15 dataset in\none configuration.",
          "link": "http://arxiv.org/abs/2107.10404",
          "publishedOn": "2021-07-23T02:00:30.108Z",
          "wordCount": 603,
          "title": "DeepScale: An Online Frame Size Adaptation Framework to Accelerate Visual Multi-object Tracking. (arXiv:2107.10404v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_I/0/1/0/all/0/1\">Imon Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhimireddy_A/0/1/0/all/0/1\">Ananth Reddy Bhimireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burns_J/0/1/0/all/0/1\">John L. Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1\">Leo Anthony Celi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li-Ching Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correa_R/0/1/0/all/0/1\">Ramon Correa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dullerud_N/0/1/0/all/0/1\">Natalie Dullerud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1\">Marzyeh Ghassemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shih-Cheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_P/0/1/0/all/0/1\">Po-Chih Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmer_L/0/1/0/all/0/1\">Lyle Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1\">Brandon J Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purkayastha_S/0/1/0/all/0/1\">Saptarshi Purkayastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyrros_A/0/1/0/all/0/1\">Ayis Pyrros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oakden_Rayner_L/0/1/0/all/0/1\">Luke Oakden-Rayner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okechukwu_C/0/1/0/all/0/1\">Chima Okechukwu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seyyed_Kalantari_L/0/1/0/all/0/1\">Laleh Seyyed-Kalantari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1\">Hari Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ryan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiman_Z/0/1/0/all/0/1\">Zachary Zaiman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gichoya_J/0/1/0/all/0/1\">Judy W Gichoya</a>",
          "description": "Background: In medical imaging, prior studies have demonstrated disparate AI\nperformance by race, yet there is no known correlation for race on medical\nimaging that would be obvious to the human expert interpreting the images.\n\nMethods: Using private and public datasets we evaluate: A) performance\nquantification of deep learning models to detect race from medical images,\nincluding the ability of these models to generalize to external environments\nand across multiple imaging modalities, B) assessment of possible confounding\nanatomic and phenotype population features, such as disease distribution and\nbody habitus as predictors of race, and C) investigation into the underlying\nmechanism by which AI models can recognize race.\n\nFindings: Standard deep learning models can be trained to predict race from\nmedical images with high performance across multiple imaging modalities. Our\nfindings hold under external validation conditions, as well as when models are\noptimized to perform clinically motivated tasks. We demonstrate this detection\nis not due to trivial proxies or imaging-related surrogate covariates for race,\nsuch as underlying disease distribution. Finally, we show that performance\npersists over all anatomical regions and frequency spectrum of the images\nsuggesting that mitigation efforts will be challenging and demand further\nstudy.\n\nInterpretation: We emphasize that model ability to predict self-reported race\nis itself not the issue of importance. However, our findings that AI can\ntrivially predict self-reported race -- even from corrupted, cropped, and\nnoised medical images -- in a setting where clinical experts cannot, creates an\nenormous risk for all model deployments in medical imaging: if an AI model\nsecretly used its knowledge of self-reported race to misclassify all Black\npatients, radiologists would not be able to tell using the same data the model\nhas access to.",
          "link": "http://arxiv.org/abs/2107.10356",
          "publishedOn": "2021-07-23T02:00:30.095Z",
          "wordCount": 787,
          "title": "Reading Race: AI Recognises Patient's Racial Identity In Medical Images. (arXiv:2107.10356v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1\">Xiujun Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>",
          "description": "Many RGB-T trackers attempt to attain robust feature representation by\nutilizing an adaptive weighting scheme (or attention mechanism). Different from\nthese works, we propose a new dynamic modality-aware filter generation module\n(named MFGNet) to boost the message communication between visible and thermal\ndata by adaptively adjusting the convolutional kernels for various input images\nin practical tracking. Given the image pairs as input, we first encode their\nfeatures with the backbone network. Then, we concatenate these feature maps and\ngenerate dynamic modality-aware filters with two independent networks. The\nvisible and thermal filters will be used to conduct a dynamic convolutional\noperation on their corresponding input feature maps respectively. Inspired by\nresidual connection, both the generated visible and thermal feature maps will\nbe summarized with input feature maps. The augmented feature maps will be fed\ninto the RoI align module to generate instance-level features for subsequent\nclassification. To address issues caused by heavy occlusion, fast motion, and\nout-of-view, we propose to conduct a joint local and global search by\nexploiting a new direction-aware target-driven attention mechanism. The spatial\nand temporal recurrent neural network is used to capture the direction-aware\ncontext for accurate global attention prediction. Extensive experiments on\nthree large-scale RGB-T tracking benchmark datasets validated the effectiveness\nof our proposed algorithm. The project page of this paper is available at\nhttps://sites.google.com/view/mfgrgbttrack/.",
          "link": "http://arxiv.org/abs/2107.10433",
          "publishedOn": "2021-07-23T02:00:30.082Z",
          "wordCount": 671,
          "title": "MFGNet: Dynamic Modality-Aware Filter Generation for RGB-T Tracking. (arXiv:2107.10433v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hechen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinghua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Ao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>",
          "description": "Nowadays, analysis of Transparent Environmental Microorganism Images (T-EM\nimages) in the field of computer vision has gradually become a new and\ninteresting spot. This paper compares different deep learning classification\nperformance for the problem that T-EM images are challenging to analyze. We\ncrop the T-EM images into 8 * 8 and 224 * 224 pixel patches in the same\nproportion and then divide the two different pixel patches into foreground and\nbackground according to ground truth. We also use four convolutional neural\nnetworks and a novel ViT network model to compare the foreground and background\nclassification experiments. We conclude that ViT performs the worst in\nclassifying 8 * 8 pixel patches, but it outperforms most convolutional neural\nnetworks in classifying 224 * 224 pixel patches.",
          "link": "http://arxiv.org/abs/2106.11582",
          "publishedOn": "2021-07-22T02:03:12.537Z",
          "wordCount": 623,
          "title": "A Comparison for Patch-level Classification of Deep Learning Methods on Transparent Environmental Microorganism Images: from Convolutional Neural Networks to Visual Transformers. (arXiv:2106.11582v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dana_A/0/1/0/all/0/1\">Alexandra Dana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shutman_M/0/1/0/all/0/1\">Maor Shutman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perlitz_Y/0/1/0/all/0/1\">Yotam Perlitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitek_R/0/1/0/all/0/1\">Ran Vitek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peleg_T/0/1/0/all/0/1\">Tomer Peleg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jevnisek_R/0/1/0/all/0/1\">Roy Jevnisek</a>",
          "description": "General object detectors use powerful backbones that uniformly extract\nfeatures from images for enabling detection of a vast amount of object types.\nHowever, utilization of such backbones in object detection applications\ndeveloped for specific object types can unnecessarily over-process an extensive\namount of background. In addition, they are agnostic to object scales, thus\nredundantly process all image regions at the same resolution. In this work we\nintroduce BLT-net, a new low-computation two-stage object detection\narchitecture designed to process images with a significant amount of background\nand objects of variate scales. BLT-net reduces computations by separating\nobjects from background using a very lite first-stage. BLT-net then efficiently\nmerges obtained proposals to further decrease processed background and then\ndynamically reduces their resolution to minimize computations. Resulting image\nproposals are then processed in the second-stage by a highly accurate model. We\ndemonstrate our architecture on the pedestrian detection problem, where objects\nare of different sizes, images are of high resolution and object detection is\nrequired to run in real-time. We show that our design reduces computations by a\nfactor of x4-x7 on the Citypersons and Caltech datasets with respect to leading\npedestrian detectors, on account of a small accuracy degradation. This method\ncan be applied on other object detection applications in scenes with a\nconsiderable amount of background and variate object sizes to reduce\ncomputations.",
          "link": "http://arxiv.org/abs/2107.10050",
          "publishedOn": "2021-07-22T02:03:12.410Z",
          "wordCount": 674,
          "title": "You Better Look Twice: a new perspective for designing accurate detectors with reduced computations. (arXiv:2107.10050v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prabhu_V/0/1/0/all/0/1\">Viraj Prabhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khare_S/0/1/0/all/0/1\">Shivam Khare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kartik_D/0/1/0/all/0/1\">Deeksha Kartik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">Judy Hoffman</a>",
          "description": "Most modern approaches for domain adaptive semantic segmentation rely on\ncontinued access to source data during adaptation, which may be infeasible due\nto computational or privacy constraints. We focus on source-free domain\nadaptation for semantic segmentation, wherein a source model must adapt itself\nto a new target domain given only unlabeled target data. We propose\nSelf-Supervised Selective Self-Training (S4T), a source-free adaptation\nalgorithm that first uses the model's pixel-level predictive consistency across\ndiverse views of each target image along with model confidence to classify\npixel predictions as either reliable or unreliable. Next, the model is\nself-trained, using predicted pseudolabels for reliable predictions and\npseudolabels inferred via a selective interpolation strategy for unreliable\nones. S4T matches or improves upon the state-of-the-art in source-free\nadaptation on 3 standard benchmarks for semantic segmentation within a single\nepoch of adaptation.",
          "link": "http://arxiv.org/abs/2107.10140",
          "publishedOn": "2021-07-22T02:03:12.404Z",
          "wordCount": 581,
          "title": "S4T: Source-free domain adaptation for semantic segmentation via self-supervised selective self-training. (arXiv:2107.10140v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.10492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fuad_M/0/1/0/all/0/1\">Md. Tahmid Hasan Fuad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fime_A/0/1/0/all/0/1\">Awal Ahmed Fime</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikder_D/0/1/0/all/0/1\">Delowar Sikder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iftee_M/0/1/0/all/0/1\">Md. Akil Raihan Iftee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbi_J/0/1/0/all/0/1\">Jakaria Rabbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_rakhami_M/0/1/0/all/0/1\">Mabrook S. Al-rakhami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gumae_A/0/1/0/all/0/1\">Abdu Gumae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_O/0/1/0/all/0/1\">Ovishake Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuad_M/0/1/0/all/0/1\">Mohtasim Fuad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md. Nazrul Islam</a>",
          "description": "In recent years, researchers have proposed many deep learning (DL) methods\nfor various tasks, and particularly face recognition (FR) made an enormous leap\nusing these techniques. Deep FR systems benefit from the hierarchical\narchitecture of the DL methods to learn discriminative face representation.\nTherefore, DL techniques significantly improve state-of-the-art performance on\nFR systems and encourage diverse and efficient real-world applications. In this\npaper, we present a comprehensive analysis of various FR systems that leverage\nthe different types of DL techniques, and for the study, we summarize 168\nrecent contributions from this area. We discuss the papers related to different\nalgorithms, architectures, loss functions, activation functions, datasets,\nchallenges, improvement ideas, current and future trends of DL-based FR\nsystems. We provide a detailed discussion of various DL methods to understand\nthe current state-of-the-art, and then we discuss various activation and loss\nfunctions for the methods. Additionally, we summarize different datasets used\nwidely for FR tasks and discuss challenges related to illumination, expression,\npose variations, and occlusion. Finally, we discuss improvement ideas, current\nand future trends of FR tasks.",
          "link": "http://arxiv.org/abs/2103.10492",
          "publishedOn": "2021-07-22T02:03:12.388Z",
          "wordCount": 710,
          "title": "Recent Advances in Deep Learning Techniques for Face Recognition. (arXiv:2103.10492v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Song Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Cheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1\">Cheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_X/0/1/0/all/0/1\">Xingle An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Congcong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shucheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shangqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoping Yang</a>",
          "description": "With the unprecedented developments in deep learning, automatic segmentation\nof main abdominal organs seems to be a solved problem as state-of-the-art\n(SOTA) methods have achieved comparable results with inter-rater variability on\nmany benchmark datasets. However, most of the existing abdominal datasets only\ncontain single-center, single-phase, single-vendor, or single-disease cases,\nand it is unclear whether the excellent performance can generalize on diverse\ndatasets. This paper presents a large and diverse abdominal CT organ\nsegmentation dataset, termed AbdomenCT-1K, with more than 1000 (1K) CT scans\nfrom 12 medical centers, including multi-phase, multi-vendor, and multi-disease\ncases. Furthermore, we conduct a large-scale study for liver, kidney, spleen,\nand pancreas segmentation and reveal the unsolved segmentation problems of the\nSOTA methods, such as the limited generalization ability on distinct medical\ncenters, phases, and unseen diseases. To advance the unsolved problems, we\nfurther build four organ segmentation benchmarks for fully supervised,\nsemi-supervised, weakly supervised, and continual learning, which are currently\nchallenging and active research topics. Accordingly, we develop a simple and\neffective method for each benchmark, which can be used as out-of-the-box\nmethods and strong baselines. We believe the AbdomenCT-1K dataset will promote\nfuture in-depth research towards clinical applicable abdominal organ\nsegmentation methods. The datasets, codes, and trained models are publicly\navailable at https://github.com/JunMa11/AbdomenCT-1K.",
          "link": "http://arxiv.org/abs/2010.14808",
          "publishedOn": "2021-07-22T02:03:12.356Z",
          "wordCount": 705,
          "title": "AbdomenCT-1K: Is Abdominal Organ Segmentation A Solved Problem?. (arXiv:2010.14808v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.03367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prabhudesai_M/0/1/0/all/0/1\">Mihir Prabhudesai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_S/0/1/0/all/0/1\">Shamit Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_D/0/1/0/all/0/1\">Darshan Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_H/0/1/0/all/0/1\">Hsiao-Yu Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1\">Adam W Harley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>",
          "description": "We present neural architectures that disentangle RGB-D images into objects'\nshapes and styles and a map of the background scene, and explore their\napplications for few-shot 3D object detection and few-shot concept\nclassification. Our networks incorporate architectural biases that reflect the\nimage formation process, 3D geometry of the world scene, and shape-style\ninterplay. They are trained end-to-end self-supervised by predicting views in\nstatic scenes, alongside a small number of 3D object boxes. Objects and scenes\nare represented in terms of 3D feature grids in the bottleneck of the network.\nWe show that the proposed 3D neural representations are compositional: they can\ngenerate novel 3D scene feature maps by mixing object shapes and styles,\nresizing and adding the resulting object 3D feature maps over background scene\nfeature maps. We show that classifiers for object categories, color, materials,\nand spatial relationships trained over the disentangled 3D feature sub-spaces\ngeneralize better with dramatically fewer examples than the current\nstate-of-the-art, and enable a visual question answering system that uses them\nas its modules to generalize one-shot to novel objects in the scene.",
          "link": "http://arxiv.org/abs/2011.03367",
          "publishedOn": "2021-07-22T02:03:12.337Z",
          "wordCount": 657,
          "title": "Disentangling 3D Prototypical Networks For Few-Shot Concept Learning. (arXiv:2011.03367v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10224",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shoufa Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1\">Chongjian Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>",
          "description": "This paper presents a simple MLP-like architecture, CycleMLP, which is a\nversatile backbone for visual recognition and dense predictions, unlike modern\nMLP architectures, e.g., MLP-Mixer, ResMLP, and gMLP, whose architectures are\ncorrelated to image size and thus are infeasible in object detection and\nsegmentation. CycleMLP has two advantages compared to modern approaches. (1) It\ncan cope with various image sizes. (2) It achieves linear computational\ncomplexity to image size by using local windows. In contrast, previous MLPs\nhave quadratic computations because of their fully spatial connections. We\nbuild a family of models that surpass existing MLPs and achieve a comparable\naccuracy (83.2%) on ImageNet-1K classification compared to the state-of-the-art\nTransformer such as Swin Transformer (83.3%) but using fewer parameters and\nFLOPs. We expand the MLP-like models' applicability, making them a versatile\nbackbone for dense prediction tasks. CycleMLP aims to provide a competitive\nbaseline on object detection, instance segmentation, and semantic segmentation\nfor MLP models. In particular, CycleMLP achieves 45.1 mIoU on ADE20K val,\ncomparable to Swin (45.2 mIOU). Code is available at\n\\url{https://github.com/ShoufaChen/CycleMLP}.",
          "link": "http://arxiv.org/abs/2107.10224",
          "publishedOn": "2021-07-22T02:03:12.330Z",
          "wordCount": 618,
          "title": "CycleMLP: A MLP-like Architecture for Dense Prediction. (arXiv:2107.10224v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nordmark_N/0/1/0/all/0/1\">Nils Nordmark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayenew_M/0/1/0/all/0/1\">Mola Ayenew</a>",
          "description": "The parsing of windows in building facades is a long-desired but challenging\ntask in computer vision. It is crucial to urban analysis, semantic\nreconstruction, lifecycle analysis, digital twins, and scene parsing amongst\nother building-related tasks that require high-quality semantic data. This\narticle investigates the usage of the mask R-CNN framework to be used for\nwindow detection of facade imagery input. We utilize transfer learning to train\nour proposed method on COCO weights with our own collected dataset of street\nview images of facades to produce instance segmentations of our new window\nclass. Experimental results show that our suggested approach with a relatively\nsmall dataset trains the network only with transfer learning and augmentation\nachieves results on par with prior state-of-the-art window detection\napproaches, even without post-optimization techniques.",
          "link": "http://arxiv.org/abs/2107.10006",
          "publishedOn": "2021-07-22T02:03:11.699Z",
          "wordCount": 577,
          "title": "Window Detection In Facade Imagery: A Deep Learning Approach Using Mask R-CNN. (arXiv:2107.10006v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Souibgui_M/0/1/0/all/0/1\">Mohamed Ali Souibgui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fornes_A/0/1/0/all/0/1\">Alicia Forn&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kessentini_Y/0/1/0/all/0/1\">Yousri Kessentini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Megyesi_B/0/1/0/all/0/1\">Be&#xe1;ta Megyesi</a>",
          "description": "Handwritten text recognition in low resource scenarios, such as manuscripts\nwith rare alphabets, is a challenging problem. The main difficulty comes from\nthe very few annotated data and the limited linguistic information (e.g.\ndictionaries and language models). Thus, we propose a few-shot learning-based\nhandwriting recognition approach that significantly reduces the human labor\nannotation process, requiring only few images of each alphabet symbol. First,\nour model detects all symbols of a given alphabet in a textline image, then a\ndecoding step maps the symbol similarity scores to the final sequence of\ntranscribed symbols. Our model is first pretrained on synthetic line images\ngenerated from any alphabet, even though different from the target domain. A\nsecond training step is then applied to diminish the gap between the source and\ntarget data. Since this retraining would require annotation of thousands of\nhandwritten symbols together with their bounding boxes, we propose to avoid\nsuch human effort through an unsupervised progressive learning approach that\nautomatically assigns pseudo-labels to the non-annotated data. The evaluation\non different manuscript datasets show that our model can lead to competitive\nresults with a significant reduction in human effort.",
          "link": "http://arxiv.org/abs/2107.10064",
          "publishedOn": "2021-07-22T02:03:11.664Z",
          "wordCount": 650,
          "title": "Few Shots Is All You Need: A Progressive Few Shot Learning Approach for Low Resource Handwriting Recognition. (arXiv:2107.10064v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.00806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thai_P/0/1/0/all/0/1\">Phat Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1\">Sameer Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lilith_N/0/1/0/all/0/1\">Nimrod Lilith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_P/0/1/0/all/0/1\">Phu N. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thanh_B/0/1/0/all/0/1\">Binh Nguyen Thanh</a>",
          "description": "An airport runway and taxiway (airside) area is a highly dynamic and complex\nenvironment featuring interactions between different types of vehicles (speed\nand dimension), under varying visibility and traffic conditions. Airport ground\nmovements are deemed safety-critical activities, and safe-separation procedures\nmust be maintained by Air Traffic Controllers (ATCs). Large airports with\ncomplicated runway-taxiway systems use advanced ground surveillance systems.\nHowever, these systems have inherent limitations and a lack of real-time\nanalytics. In this paper, we propose a novel computer-vision based framework,\nnamely \"Deep4Air\", which can not only augment the ground surveillance systems\nvia the automated visual monitoring of runways and taxiways for aircraft\nlocation, but also provide real-time speed and distance analytics for aircraft\non runways and taxiways. The proposed framework includes an adaptive deep\nneural network for efficiently detecting and tracking aircraft. The\nexperimental results show an average precision of detection and tracking of up\nto 99.8% on simulated data with validations on surveillance videos from the\ndigital tower at George Bush Intercontinental Airport. The results also\ndemonstrate that \"Deep4Air\" can locate aircraft positions relative to the\nairport runway and taxiway infrastructure with high accuracy. Furthermore,\naircraft speed and separation distance are monitored in real-time, providing\nenhanced safety management.",
          "link": "http://arxiv.org/abs/2010.00806",
          "publishedOn": "2021-07-22T02:03:11.332Z",
          "wordCount": 670,
          "title": "Deep4Air: A Novel Deep Learning Framework for Airport Airside Surveillance. (arXiv:2010.00806v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.05489",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yichuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_H/0/1/0/all/0/1\">Hailey James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_O/0/1/0/all/0/1\">Otkrist Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raviv_D/0/1/0/all/0/1\">Dan Raviv</a>",
          "description": "Detecting and extracting information from Machine-Readable Zone (MRZ) on\npassports and visas is becoming increasingly important for verifying document\nauthenticity. However, computer vision methods for performing similar tasks,\nsuch as optical character recognition (OCR), fail to extract the MRZ given\ndigital images of passports with reasonable accuracy. We present a specially\ndesigned model based on convolutional neural networks that is able to\nsuccessfully extract MRZ information from digital images of passports of\narbitrary orientation and size. Our model achieved 100% MRZ detection rate and\n98.36% character recognition macro-f1 score on a passport and visa dataset.",
          "link": "http://arxiv.org/abs/2009.05489",
          "publishedOn": "2021-07-22T02:03:11.288Z",
          "wordCount": 577,
          "title": "MRZ code extraction from visa and passport documents using convolutional neural networks. (arXiv:2009.05489v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09842",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jiawei Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_J/0/1/0/all/0/1\">Jiang Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1\">Zhongchao Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhong_C/0/1/0/all/0/1\">Cheng Zhong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Z/0/1/0/all/0/1\">Zhiqiang He</a>",
          "description": "Liver cancer is one of the most common cancers worldwide. Due to\ninconspicuous texture changes of liver tumor, contrast-enhanced computed\ntomography (CT) imaging is effective for the diagnosis of liver cancer. In this\npaper, we focus on improving automated liver tumor segmentation by integrating\nmulti-modal CT images. To this end, we propose a novel mutual learning (ML)\nstrategy for effective and robust multi-modal liver tumor segmentation.\nDifferent from existing multi-modal methods that fuse information from\ndifferent modalities by a single model, with ML, an ensemble of\nmodality-specific models learn collaboratively and teach each other to distill\nboth the characteristics and the commonality between high-level representations\nof different modalities. The proposed ML not only enables the superiority for\nmulti-modal learning but can also handle missing modalities by transferring\nknowledge from existing modalities to missing ones. Additionally, we present a\nmodality-aware (MA) module, where the modality-specific models are\ninterconnected and calibrated with attention weights for adaptive information\nexchange. The proposed modality-aware mutual learning (MAML) method achieves\npromising results for liver tumor segmentation on a large-scale clinical\ndataset. Moreover, we show the efficacy and robustness of MAML for handling\nmissing modalities on both the liver tumor and public brain tumor (BRATS 2018)\ndatasets. Our code is available at https://github.com/YaoZhang93/MAML.",
          "link": "http://arxiv.org/abs/2107.09842",
          "publishedOn": "2021-07-22T02:03:11.280Z",
          "wordCount": 658,
          "title": "Modality-aware Mutual Learning for Multi-modal Medical Image Segmentation. (arXiv:2107.09842v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2004.06193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koner_R/0/1/0/all/0/1\">Rajat Koner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shit_S/0/1/0/all/0/1\">Suprosanna Shit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>",
          "description": "The extraction of a scene graph with objects as nodes and mutual\nrelationships as edges is the basis for a deep understanding of image content.\nDespite recent advances, such as message passing and joint classification, the\ndetection of visual relationships remains a challenging task due to sub-optimal\nexploration of the mutual interaction among the visual objects. In this work,\nwe propose a novel transformer formulation for scene graph generation and\nrelation prediction. We leverage the encoder-decoder architecture of the\ntransformer for rich feature embedding of nodes and edges. Specifically, we\nmodel the node-to-node interaction with the self-attention of the transformer\nencoder and the edge-to-node interaction with the cross-attention of the\ntransformer decoder. Further, we introduce a novel positional embedding\nsuitable to handle edges in the decoder. Finally, our relation prediction\nmodule classifies the directed relation from the learned node and edge\nembedding. We name this architecture as Relation Transformer Network (RTN). On\nthe Visual Genome and GQA dataset, we have achieved an overall mean of 4.85%\nand 3.1% point improvement in comparison with state-of-the-art methods. Our\nexperiments show that Relation Transformer can efficiently model context across\nvarious datasets with small, medium, and large-scale relation classification.",
          "link": "http://arxiv.org/abs/2004.06193",
          "publishedOn": "2021-07-22T02:03:11.273Z",
          "wordCount": 644,
          "title": "Relation Transformer Network. (arXiv:2004.06193v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09923",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hu_B/0/1/0/all/0/1\">Bowen Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lei_B/0/1/0/all/0/1\">Baiying Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1\">Yanyan Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuqiang Wang</a>",
          "description": "Fusing medical images and the corresponding 3D shape representation can\nprovide complementary information and microstructure details to improve the\noperational performance and accuracy in brain surgery. However, compared to the\nsubstantial image data, it is almost impossible to obtain the intraoperative 3D\nshape information by using physical methods such as sensor scanning, especially\nin minimally invasive surgery and robot-guided surgery. In this paper, a\ngeneral generative adversarial network (GAN) architecture based on graph\nconvolutional networks is proposed to reconstruct the 3D point clouds (PCs) of\nbrains by using one single 2D image, thus relieving the limitation of acquiring\n3D shape data during surgery. Specifically, a tree-structured generative\nmechanism is constructed to use the latent vector effectively and transfer\nfeatures between hidden layers accurately. With the proposed generative model,\na spontaneous image-to-PC conversion is finished in real-time. Competitive\nqualitative and quantitative experimental results have been achieved on our\nmodel. In multiple evaluation methods, the proposed model outperforms another\ncommon point cloud generative model PointOutNet.",
          "link": "http://arxiv.org/abs/2107.09923",
          "publishedOn": "2021-07-22T02:03:11.266Z",
          "wordCount": 622,
          "title": "A Point Cloud Generative Model via Tree-Structured Graph Convolutions for 3D Brain Shape Reconstruction. (arXiv:2107.09923v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/1905.08616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alex Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_X/0/1/0/all/0/1\">Xiaohan Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsuei_S/0/1/0/all/0/1\">Stephanie Tsuei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We describe a method to infer dense depth from camera motion and sparse depth\nas estimated using a visual-inertial odometry system. Unlike other scenarios\nusing point clouds from lidar or structured light sensors, we have few hundreds\nto few thousand points, insufficient to inform the topology of the scene. Our\nmethod first constructs a piecewise planar scaffolding of the scene, and then\nuses it to infer dense depth using the image along with the sparse points. We\nuse a predictive cross-modal criterion, akin to `self-supervision,' measuring\nphotometric consistency across time, forward-backward pose consistency, and\ngeometric compatibility with the sparse point cloud. We also launch the first\nvisual-inertial + depth dataset, which we hope will foster additional\nexploration into combining the complementary strengths of visual and inertial\nsensors. To compare our method to prior work, we adopt the unsupervised KITTI\ndepth completion benchmark, and show state-of-the-art performance on it. Code\navailable at:\nhttps://github.com/alexklwong/unsupervised-depth-completion-visual-inertial-odometry.",
          "link": "http://arxiv.org/abs/1905.08616",
          "publishedOn": "2021-07-22T02:03:11.245Z",
          "wordCount": 646,
          "title": "Unsupervised Depth Completion from Visual Inertial Odometry. (arXiv:1905.08616v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1\">Wentao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Yu Kong</a>",
          "description": "In a real-world scenario, human actions are typically out of the distribution\nfrom training data, which requires a model to both recognize the known actions\nand reject the unknown. Different from image data, video actions are more\nchallenging to be recognized in an open-set setting due to the uncertain\ntemporal dynamics and static bias of human actions. In this paper, we propose a\nDeep Evidential Action Recognition (DEAR) method to recognize actions in an\nopen testing set. Specifically, we formulate the action recognition problem\nfrom the evidential deep learning (EDL) perspective and propose a novel model\ncalibration method to regularize the EDL training. Besides, to mitigate the\nstatic bias of video representation, we propose a plug-and-play module to\ndebias the learned representation through contrastive learning. Experimental\nresults show that our DEAR method achieves consistent performance gain on\nmultiple mainstream action recognition models and benchmarks. Codes and\npre-trained weights will be made available upon paper acceptance.",
          "link": "http://arxiv.org/abs/2107.10161",
          "publishedOn": "2021-07-22T02:03:11.226Z",
          "wordCount": 589,
          "title": "Evidential Deep Learning for Open Set Action Recognition. (arXiv:2107.10161v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.01542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhenyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yifeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svetlik_M/0/1/0/all/0/1\">Maxwell Svetlik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1\">Kuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>",
          "description": "Grasp detection in clutter requires the robot to reason about the 3D scene\nfrom incomplete and noisy perception. In this work, we draw insight that 3D\nreconstruction and grasp learning are two intimately connected tasks, both of\nwhich require a fine-grained understanding of local geometry details. We thus\npropose to utilize the synergies between grasp affordance and 3D reconstruction\nthrough multi-task learning of a shared representation. Our model takes\nadvantage of deep implicit functions, a continuous and memory-efficient\nrepresentation, to enable differentiable training of both tasks. We train the\nmodel on self-supervised grasp trials data in simulation. Evaluation is\nconducted on a clutter removal task, where the robot clears cluttered objects\nby grasping them one at a time. The experimental results in simulation and on\nthe real robot have demonstrated that the use of implicit neural\nrepresentations and joint learning of grasp affordance and 3D reconstruction\nhave led to state-of-the-art grasping results. Our method outperforms baselines\nby over 10% in terms of grasp success rate. Additional results and videos can\nbe found at https://sites.google.com/view/rpl-giga2021",
          "link": "http://arxiv.org/abs/2104.01542",
          "publishedOn": "2021-07-22T02:03:11.182Z",
          "wordCount": 650,
          "title": "Synergies Between Affordance and Geometry: 6-DoF Grasp Detection via Implicit Representations. (arXiv:2104.01542v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoque_R/0/1/0/all/0/1\">Ryan Hoque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seita_D/0/1/0/all/0/1\">Daniel Seita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishna_A/0/1/0/all/0/1\">Ashwin Balakrishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganapathi_A/0/1/0/all/0/1\">Aditya Ganapathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanwani_A/0/1/0/all/0/1\">Ajay Kumar Tanwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamali_N/0/1/0/all/0/1\">Nawid Jamali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamane_K/0/1/0/all/0/1\">Katsu Yamane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iba_S/0/1/0/all/0/1\">Soshi Iba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1\">Ken Goldberg</a>",
          "description": "Robotic fabric manipulation has applications in home robotics, textiles,\nsenior care and surgery. Existing fabric manipulation techniques, however, are\ndesigned for specific tasks, making it difficult to generalize across different\nbut related tasks. We build upon the Visual Foresight framework to learn fabric\ndynamics that can be efficiently reused to accomplish different sequential\nfabric manipulation tasks with a single goal-conditioned policy. We extend our\nearlier work on VisuoSpatial Foresight (VSF), which learns visual dynamics on\ndomain randomized RGB images and depth maps simultaneously and completely in\nsimulation. In this earlier work, we evaluated VSF on multi-step fabric\nsmoothing and folding tasks against 5 baseline methods in simulation and on the\nda Vinci Research Kit (dVRK) surgical robot without any demonstrations at train\nor test time. A key finding was that depth sensing significantly improves\nperformance: RGBD data yields an 80% improvement in fabric folding success rate\nin simulation over pure RGB data. In this work, we vary 4 components of VSF,\nincluding data generation, visual dynamics model, cost function, and\noptimization procedure. Results suggest that training visual dynamics models\nusing longer, corner-based actions can improve the efficiency of fabric folding\nby 76% and enable a physical sequential fabric folding task that VSF could not\npreviously perform with 90% reliability. Code, data, videos, and supplementary\nmaterial are available at https://sites.google.com/view/fabric-vsf/.",
          "link": "http://arxiv.org/abs/2102.09754",
          "publishedOn": "2021-07-22T02:03:11.173Z",
          "wordCount": 723,
          "title": "VisuoSpatial Foresight for Physical Sequential Fabric Manipulation. (arXiv:2102.09754v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renshen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1\">Yasuhisa Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popat_A/0/1/0/all/0/1\">Ashok C. Popat</a>",
          "description": "Paragraphs are an important class of document entities. We propose a new\napproach for paragraph identification by spatial graph convolutional neural\nnetworks (GCN) applied on OCR text boxes. Two steps, namely line splitting and\nline clustering, are performed to extract paragraphs from the lines in OCR\nresults. Each step uses a beta-skeleton graph constructed from bounding boxes,\nwhere the graph edges provide efficient support for graph convolution\noperations. With only pure layout input features, the GCN model size is 3~4\norders of magnitude smaller compared to R-CNN based models, while achieving\ncomparable or better accuracies on PubLayNet and other datasets. Furthermore,\nthe GCN models show good generalization from synthetic training data to\nreal-world images, and good adaptivity for variable document styles.",
          "link": "http://arxiv.org/abs/2101.12741",
          "publishedOn": "2021-07-22T02:03:11.166Z",
          "wordCount": 603,
          "title": "Post-OCR Paragraph Recognition by Graph Convolutional Networks. (arXiv:2101.12741v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1701.02123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Je_C/0/1/0/all/0/1\">Changsoo Je</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1\">Kyuhyoung Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang Wook Lee</a>",
          "description": "In this paper, we present a novel method for rapid high-resolution range\nsensing using green-blue stripe pattern. We use green and blue for designing\nhigh-frequency stripe projection pattern. For accurate and reliable range\nrecovery, we identify the stripe patterns by our color-stripe segmentation and\nunwrapping algorithms. The experimental result for a naked human face shows the\neffectiveness of our method.",
          "link": "http://arxiv.org/abs/1701.02123",
          "publishedOn": "2021-07-22T02:03:11.158Z",
          "wordCount": 565,
          "title": "Green-Blue Stripe Pattern for Range Sensing from a Single Image. (arXiv:1701.02123v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.05653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_M/0/1/0/all/0/1\">Mauricio Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1\">Alex C. Kot</a>",
          "description": "Research on group activity recognition mostly leans on the standard\ntwo-stream approach (RGB and Optical Flow) as their input features. Few have\nexplored explicit pose information, with none using it directly to reason about\nthe persons interactions. In this paper, we leverage the skeleton information\nto learn the interactions between the individuals straight from it. With our\nproposed method GIRN, multiple relationship types are inferred from independent\nmodules, that describe the relations between the body joints pair-by-pair.\nAdditionally to the joints relations, we also experiment with the previously\nunexplored relationship between individuals and relevant objects (e.g.\nvolleyball). The individuals distinct relations are then merged through an\nattention mechanism, that gives more importance to those individuals more\nrelevant for distinguishing the group activity. We evaluate our method in the\nVolleyball dataset, obtaining competitive results to the state-of-the-art. Our\nexperiments demonstrate the potential of skeleton-based approaches for modeling\nmulti-person interactions.",
          "link": "http://arxiv.org/abs/2011.05653",
          "publishedOn": "2021-07-22T02:03:11.136Z",
          "wordCount": 621,
          "title": "Skeleton-based Relational Reasoning for Group Activity Analysis. (arXiv:2011.05653v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ran Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shibiao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>",
          "description": "Geometry problem solving has attracted much attention in the NLP community\nrecently. The task is challenging as it requires abstract problem understanding\nand symbolic reasoning with axiomatic knowledge. However, current datasets are\neither small in scale or not publicly available. Thus, we construct a new\nlarge-scale benchmark, Geometry3K, consisting of 3,002 geometry problems with\ndense annotation in formal language. We further propose a novel geometry\nsolving approach with formal language and symbolic reasoning, called\nInterpretable Geometry Problem Solver (Inter-GPS). Inter-GPS first parses the\nproblem text and diagram into formal language automatically via rule-based text\nparsing and neural object detecting, respectively. Unlike implicit learning in\nexisting methods, Inter-GPS incorporates theorem knowledge as conditional rules\nand performs symbolic reasoning step by step. Also, a theorem predictor is\ndesigned to infer the theorem application sequence fed to the symbolic solver\nfor the more efficient and reasonable searching path. Extensive experiments on\nthe Geometry3K and GEOS datasets demonstrate that Inter-GPS achieves\nsignificant improvements over existing methods. The project with code and data\nis available at https://lupantech.github.io/inter-gps.",
          "link": "http://arxiv.org/abs/2105.04165",
          "publishedOn": "2021-07-22T02:03:11.129Z",
          "wordCount": 684,
          "title": "Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning. (arXiv:2105.04165v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuhao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1\">Junbao Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Due to the domain discrepancy in visual domain adaptation, the performance of\nsource model degrades when bumping into the high data density near decision\nboundary in target domain. A common solution is to minimize the Shannon Entropy\nto push the decision boundary away from the high density area. However, entropy\nminimization also leads to severe reduction of prediction diversity, and\nunfortunately brings harm to the domain adaptation. In this paper, we\ninvestigate the prediction discriminability and diversity by studying the\nstructure of the classification output matrix of a randomly selected data\nbatch. We find by theoretical analysis that the prediction discriminability and\ndiversity could be separately measured by the Frobenius-norm and rank of the\nbatch output matrix. The nuclear-norm is an upperbound of the former, and a\nconvex approximation of the latter. Accordingly, we propose Batch Nuclear-norm\nMaximization and Minimization, which performs nuclear-norm maximization on the\ntarget output matrix to enhance the target prediction ability, and nuclear-norm\nminimization on the source batch output matrix to increase applicability of the\nsource domain knowledge. We further approximate the nuclear-norm by\nL_{1,2}-norm, and design multi-batch optimization for stable solution on large\nnumber of categories. The fast approximation method achieves O(n^2)\ncomputational complexity and better convergence property. Experiments show that\nour method could boost the adaptation accuracy and robustness under three\ntypical domain adaptation scenarios. The code is available at\nhttps://github.com/cuishuhao/BNM.",
          "link": "http://arxiv.org/abs/2107.06154",
          "publishedOn": "2021-07-22T02:03:11.110Z",
          "wordCount": 709,
          "title": "Fast Batch Nuclear-norm Maximization and Minimization for Robust Domain Adaptation. (arXiv:2107.06154v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10073",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jaume_G/0/1/0/all/0/1\">Guillaume Jaume</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pati_P/0/1/0/all/0/1\">Pushpak Pati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anklin_V/0/1/0/all/0/1\">Valentin Anklin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Foncubierta_A/0/1/0/all/0/1\">Antonio Foncubierta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gabrani_M/0/1/0/all/0/1\">Maria Gabrani</a>",
          "description": "Advances in entity-graph based analysis of histopathology images have brought\nin a new paradigm to describe tissue composition, and learn the tissue\nstructure-to-function relationship. Entity-graphs offer flexible and scalable\nrepresentations to characterize tissue organization, while allowing the\nincorporation of prior pathological knowledge to further support model\ninterpretability and explainability. However, entity-graph analysis requires\nprerequisites for image-to-graph translation and knowledge of state-of-the-art\nmachine learning algorithms applied to graph-structured data, which can\npotentially hinder their adoption. In this work, we aim to alleviate these\nissues by developing HistoCartography, a standardized python API with necessary\npreprocessing, machine learning and explainability tools to facilitate\ngraph-analytics in computational pathology. Further, we have benchmarked the\ncomputational time and performance on multiple datasets across different\nimaging types and histopathology tasks to highlight the applicability of the\nAPI for building computational pathology workflows.",
          "link": "http://arxiv.org/abs/2107.10073",
          "publishedOn": "2021-07-22T02:03:11.100Z",
          "wordCount": 583,
          "title": "HistoCartography: A Toolkit for Graph Analytics in Digital Pathology. (arXiv:2107.10073v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10220",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antsiferova_A/0/1/0/all/0/1\">Anastasia Antsiferova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yakovenko_A/0/1/0/all/0/1\">Alexander Yakovenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safonov_N/0/1/0/all/0/1\">Nickolay Safonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_D/0/1/0/all/0/1\">Dmitriy Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gushin_A/0/1/0/all/0/1\">Alexander Gushin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1\">Dmitriy Vatolin</a>",
          "description": "Quality assessment plays a key role in creating and comparing video\ncompression algorithms. Despite the development of a large number of new\nmethods for assessing quality, generally accepted and well-known codecs\ncomparisons mainly use the classical methods like PSNR, SSIM and new method\nVMAF. These methods can be calculated following different rules: they can use\ndifferent frame-by-frame averaging techniques or different summation of color\ncomponents. In this paper, a fundamental comparison of various versions of\ngenerally accepted metrics is carried out to find the most relevant and\nrecommended versions of video quality metrics to be used in codecs comparisons.\nFor comparison, we used a set of videos encoded with video codecs of different\nstandards, and visual quality scores collected for the resulting set of streams\nsince 2018 until 2021",
          "link": "http://arxiv.org/abs/2107.10220",
          "publishedOn": "2021-07-22T02:03:11.092Z",
          "wordCount": 584,
          "title": "Objective video quality metrics application to video codecs comparisons: choosing the best for subjective quality estimation. (arXiv:2107.10220v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2005.02356",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Chen_S/0/1/0/all/0/1\">Shixiang Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Deng_Z/0/1/0/all/0/1\">Zengde Deng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ma_S/0/1/0/all/0/1\">Shiqian Ma</a>, <a href=\"http://arxiv.org/find/math/1/au:+So_A/0/1/0/all/0/1\">Anthony Man-Cho So</a>",
          "description": "We consider the problem of maximizing the $\\ell_1$ norm of a linear map over\nthe sphere, which arises in various machine learning applications such as\northogonal dictionary learning (ODL) and robust subspace recovery (RSR). The\nproblem is numerically challenging due to its nonsmooth objective and nonconvex\nconstraint, and its algorithmic aspects have not been well explored. In this\npaper, we show how the manifold structure of the sphere can be exploited to\ndesign fast algorithms for tackling this problem. Specifically, our\ncontribution is threefold. First, we present a manifold proximal point\nalgorithm (ManPPA) for the problem and show that it converges at a sublinear\nrate. Furthermore, we show that ManPPA can achieve a quadratic convergence rate\nwhen applied to the ODL and RSR problems. Second, we propose a stochastic\nvariant of ManPPA called StManPPA, which is well suited for large-scale\ncomputation, and establish its sublinear convergence rate. Both ManPPA and\nStManPPA have provably faster convergence rates than existing subgradient-type\nmethods. Third, using ManPPA as a building block, we propose a new approach to\nsolving a matrix analog of the problem, in which the sphere is replaced by the\nStiefel manifold. The results from our extensive numerical experiments on the\nODL and RSR problems demonstrate the efficiency and efficacy of our proposed\nmethods.",
          "link": "http://arxiv.org/abs/2005.02356",
          "publishedOn": "2021-07-22T02:03:11.069Z",
          "wordCount": 701,
          "title": "Manifold Proximal Point Algorithms for Dual Principal Component Pursuit and Orthogonal Dictionary Learning. (arXiv:2005.02356v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10180",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Eschweiler_D/0/1/0/all/0/1\">Dennis Eschweiler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rethwisch_M/0/1/0/all/0/1\">Malte Rethwisch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jarchow_M/0/1/0/all/0/1\">Mareike Jarchow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koppers_S/0/1/0/all/0/1\">Simon Koppers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stegmaier_J/0/1/0/all/0/1\">Johannes Stegmaier</a>",
          "description": "Automated image processing approaches are indispensable for many biomedical\nexperiments and help to cope with the increasing amount of microscopy image\ndata in a fast and reproducible way. Especially state-of-the-art deep\nlearning-based approaches most often require large amounts of annotated\ntraining data to produce accurate and generalist outputs, but they are often\ncompromised by the general lack of those annotated data sets. In this work, we\npropose how conditional generative adversarial networks can be utilized to\ngenerate realistic image data for 3D fluorescence microscopy from annotation\nmasks of 3D cellular structures. In combination with mask simulation\napproaches, we demonstrate the generation of fully-annotated 3D microscopy data\nsets that we make publicly available for training or benchmarking. An\nadditional positional conditioning of the cellular structures enables the\nreconstruction of position-dependent intensity characteristics and allows to\ngenerate image data of different quality levels. A patch-wise working principle\nand a subsequent full-size reassemble strategy is used to generate image data\nof arbitrary size and different organisms. We present this as a\nproof-of-concept for the automated generation of fully-annotated training data\nsets requiring only a minimum of manual interaction to alleviate the need of\nmanual annotations.",
          "link": "http://arxiv.org/abs/2107.10180",
          "publishedOn": "2021-07-22T02:03:11.051Z",
          "wordCount": 642,
          "title": "3D fluorescence microscopy data synthesis for segmentation and benchmarking. (arXiv:2107.10180v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1\">Wentao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Yu Kong</a>",
          "description": "Traffic accident anticipation aims to accurately and promptly predict the\noccurrence of a future accident from dashcam videos, which is vital for a\nsafety-guaranteed self-driving system. To encourage an early and accurate\ndecision, existing approaches typically focus on capturing the cues of spatial\nand temporal context before a future accident occurs. However, their\ndecision-making lacks visual explanation and ignores the dynamic interaction\nwith the environment. In this paper, we propose Deep ReInforced accident\nanticipation with Visual Explanation, named DRIVE. The method simulates both\nthe bottom-up and top-down visual attention mechanism in a dashcam observation\nenvironment so that the decision from the proposed stochastic multi-task agent\ncan be visually explained by attentive regions. Moreover, the proposed dense\nanticipation reward and sparse fixation reward are effective in training the\nDRIVE model with our improved reinforcement learning algorithm. Experimental\nresults show that the DRIVE model achieves state-of-the-art performance on\nmultiple real-world traffic accident datasets. The code and pre-trained model\nwill be available upon paper acceptance.",
          "link": "http://arxiv.org/abs/2107.10189",
          "publishedOn": "2021-07-22T02:03:11.042Z",
          "wordCount": 596,
          "title": "DRIVE: Deep Reinforced Accident Anticipation with Visual Explanation. (arXiv:2107.10189v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuailin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhitong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>",
          "description": "Learning segmentation from noisy labels is an important task for medical\nimage analysis due to the difficulty in acquiring highquality annotations. Most\nexisting methods neglect the pixel correlation and structural prior in\nsegmentation, often producing noisy predictions around object boundaries. To\naddress this, we adopt a superpixel representation and develop a robust\niterative learning strategy that combines noise-aware training of segmentation\nnetwork and noisy label refinement, both guided by the superpixels. This design\nenables us to exploit the structural constraints in segmentation labels and\neffectively mitigate the impact of label noise in learning. Experiments on two\nbenchmarks show that our method outperforms recent state-of-the-art approaches,\nand achieves superior robustness in a wide range of label noises. Code is\navailable at https://github.com/gaozhitong/SP_guided_Noisy_Label_Seg.",
          "link": "http://arxiv.org/abs/2107.10100",
          "publishedOn": "2021-07-22T02:03:10.973Z",
          "wordCount": 567,
          "title": "Superpixel-guided Iterative Learning from Noisy Labels for Medical Image Segmentation. (arXiv:2107.10100v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1\">Mengcheng Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_S/0/1/0/all/0/1\">Shuliang Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xunlai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>",
          "description": "Despite video forecasting has been a widely explored topic in recent years,\nthe mainstream of the existing work still limits their models with a single\nprediction space but completely neglects the way to leverage their model with\nmulti-prediction spaces. This work fills this gap. For the first time, we\ndeeply study numerous strategies to perform video forecasting in\nmulti-prediction spaces and fuse their results together to boost performance.\nThe prediction in the pixel space usually lacks the ability to preserve the\nsemantic and structure content of the video however the prediction in the\nhigh-level feature space is prone to generate errors in the reduction and\nrecovering process. Therefore, we build a recurrent connection between\ndifferent feature spaces and incorporate their generations in the upsampling\nprocess. Rather surprisingly, this simple idea yields a much more significant\nperformance boost than PhyDNet (performance improved by 32.1% MAE on MNIST-2\ndataset, and 21.4% MAE on KTH dataset). Both qualitative and quantitative\nevaluations on four datasets demonstrate the generalization ability and\neffectiveness of our approach. We show that our model significantly reduces the\ntroublesome distortions and blurry artifacts and brings remarkable improvements\nto the accuracy in long term video prediction. The code will be released soon.",
          "link": "http://arxiv.org/abs/2107.10068",
          "publishedOn": "2021-07-22T02:03:10.966Z",
          "wordCount": 649,
          "title": "From Single to Multiple: Leveraging Multi-level Prediction Spaces for Video Forecasting. (arXiv:2107.10068v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Liang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "Conditional generative models aim to learn the underlying joint distribution\nof data and labels, and thus realize conditional generation. Among them,\nauxiliary classifier generative adversarial networks (AC-GAN) have been widely\nused, but suffer from the issue of low intra-class diversity on generated\nsamples. In this paper, we point out that the fundamental reason is that the\nclassifier of AC-GAN is generator-agnostic, and thus cannot provide informative\nguidance to the generator to approximate the target joint distribution, leading\nto a minimization of conditional entropy that decreases the intra-class\ndiversity. Based on this finding, we propose novel cGANs with auxiliary\ndiscriminative classifier (ADC-GAN) to address the issue of AC-GAN.\nSpecifically, the auxiliary discriminative classifier becomes generator-aware\nby distinguishing between the real and fake data while recognizing their\nlabels. We then optimize the generator based on the auxiliary classifier along\nwith the original discriminator to match the joint and marginal distributions\nof the generated samples with those of the real samples. We provide theoretical\nanalysis and empirical evidence on synthetic and real-world datasets to\ndemonstrate the superiority of the proposed ADC-GAN compared to competitive\ncGANs.",
          "link": "http://arxiv.org/abs/2107.10060",
          "publishedOn": "2021-07-22T02:03:10.958Z",
          "wordCount": 613,
          "title": "CGANs with Auxiliary Discriminative Classifier. (arXiv:2107.10060v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09989",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1\">Guangyuan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lv_J/0/1/0/all/0/1\">Jun Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tong_X/0/1/0/all/0/1\">Xiangrong Tong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chengyan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>",
          "description": "Magnetic resonance imaging (MRI) is an important medical imaging modality,\nbut its acquisition speed is quite slow due to the physiological limitations.\nRecently, super-resolution methods have shown excellent performance in\naccelerating MRI. In some circumstances, it is difficult to obtain\nhigh-resolution images even with prolonged scan time. Therefore, we proposed a\nnovel super-resolution method that uses a generative adversarial network (GAN)\nwith cyclic loss and attention mechanism to generate high-resolution MR images\nfrom low-resolution MR images by a factor of 2. We implemented our model on\npelvic images from healthy subjects as training and validation data, while\nthose data from patients were used for testing. The MR dataset was obtained\nusing different imaging sequences, including T2, T2W SPAIR, and mDIXON-W. Four\nmethods, i.e., BICUBIC, SRCNN, SRGAN, and EDSR were used for comparison.\nStructural similarity, peak signal to noise ratio, root mean square error, and\nvariance inflation factor were used as calculation indicators to evaluate the\nperformances of the proposed method. Various experimental results showed that\nour method can better restore the details of the high-resolution MR image as\ncompared to the other methods. In addition, the reconstructed high-resolution\nMR image can provide better lesion textures in the tumor patients, which is\npromising to be used in clinical diagnosis.",
          "link": "http://arxiv.org/abs/2107.09989",
          "publishedOn": "2021-07-22T02:03:10.941Z",
          "wordCount": 677,
          "title": "High-Resolution Pelvic MRI Reconstruction Using a Generative Adversarial Network with Attention and Cyclic Loss. (arXiv:2107.09989v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaganathan_S/0/1/0/all/0/1\">Srikrishna Jaganathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borsdorf_A/0/1/0/all/0/1\">Anja Borsdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_K/0/1/0/all/0/1\">Karthik Shetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>",
          "description": "Deep Learning-based 2D/3D registration methods are highly robust but often\nlack the necessary registration accuracy for clinical application. A refinement\nstep using the classical optimization-based 2D/3D registration method applied\nin combination with Deep Learning-based techniques can provide the required\naccuracy. However, it also increases the runtime. In this work, we propose a\nnovel Deep Learning driven 2D/3D registration framework that can be used\nend-to-end for iterative registration tasks without relying on any further\nrefinement step. We accomplish this by learning the update step of the 2D/3D\nregistration framework using Point-to-Plane Correspondences. The update step is\nlearned using iterative residual refinement-based optical flow estimation, in\ncombination with the Point-to-Plane correspondence solver embedded as a known\noperator. Our proposed method achieves an average runtime of around 8s, a mean\nre-projection distance error of 0.60 $\\pm$ 0.40 mm with a success ratio of 97\npercent and a capture range of 60 mm. The combination of high registration\naccuracy, high robustness, and fast runtime makes our solution ideal for\nclinical applications.",
          "link": "http://arxiv.org/abs/2107.10004",
          "publishedOn": "2021-07-22T02:03:10.805Z",
          "wordCount": 616,
          "title": "Deep Iterative 2D/3D Registration. (arXiv:2107.10004v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_P/0/1/0/all/0/1\">P.S. Sastry</a>",
          "description": "Deep Neural Networks, often owing to the overparameterization, are shown to\nbe capable of exactly memorizing even randomly labelled data. Empirical studies\nhave also shown that none of the standard regularization techniques mitigate\nsuch overfitting. We investigate whether the choice of the loss function can\naffect this memorization. We empirically show, with benchmark data sets MNIST\nand CIFAR-10, that a symmetric loss function, as opposed to either\ncross-entropy or squared error loss, results in significant improvement in the\nability of the network to resist such overfitting. We then provide a formal\ndefinition for robustness to memorization and provide a theoretical explanation\nas to why the symmetric losses provide this robustness. Our results clearly\nbring out the role loss functions alone can play in this phenomenon of\nmemorization.",
          "link": "http://arxiv.org/abs/2107.09957",
          "publishedOn": "2021-07-22T02:03:10.645Z",
          "wordCount": 579,
          "title": "Memorization in Deep Neural Networks: Does the Loss Function matter?. (arXiv:2107.09957v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09965",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noeckel_J/0/1/0/all/0/1\">James Noeckel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haisen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curless_B/0/1/0/all/0/1\">Brian Curless</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_A/0/1/0/all/0/1\">Adriana Schulz</a>",
          "description": "We propose a novel method to generate fabrication blueprints from images of\ncarpentered items. While 3D reconstruction from images is a well-studied\nproblem, typical approaches produce representations that are ill-suited for\ncomputer-aided design and fabrication applications. Our key insight is that\nfabrication processes define and constrain the design space for carpentered\nobjects, and can be leveraged to develop novel reconstruction methods. Our\nmethod makes use of domain-specific constraints to recover not just valid\ngeometry, but a semantically valid assembly of parts, using a combination of\nimage-based and geometric optimization techniques.\n\nWe demonstrate our method on a variety of wooden objects and furniture, and\nshow that we can automatically obtain designs that are both easy to edit and\naccurate recreations of the ground truth. We further illustrate how our method\ncan be used to fabricate a physical replica of the captured object as well as a\ncustomized version, which can be produced by directly editing the reconstructed\nmodel in CAD software.",
          "link": "http://arxiv.org/abs/2107.09965",
          "publishedOn": "2021-07-22T02:03:10.621Z",
          "wordCount": 618,
          "title": "Fabrication-Aware Reverse Engineering for Carpentry. (arXiv:2107.09965v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Ning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kaitao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhiheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xiaopeng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yihong Gong</a>",
          "description": "Anomaly detection plays a key role in industrial manufacturing for product\nquality control. Traditional methods for anomaly detection are rule-based with\nlimited generalization ability. Recent methods based on supervised deep\nlearning are more powerful but require large-scale annotated datasets for\ntraining. In practice, abnormal products are rare thus it is very difficult to\ntrain a deep model in a fully supervised way. In this paper, we propose a novel\nunsupervised anomaly detection approach based on Self-organizing Map (SOM). Our\nmethod, Self-organizing Map for Anomaly Detection (SOMAD) maintains normal\ncharacteristics by using topological memory based on multi-scale features.\nSOMAD achieves state-of the-art performance on unsupervised anomaly detection\nand localization on the MVTec dataset.",
          "link": "http://arxiv.org/abs/2107.09903",
          "publishedOn": "2021-07-22T02:03:10.614Z",
          "wordCount": 553,
          "title": "Anomaly Detection via Self-organizing Map. (arXiv:2107.09903v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1\">Minjung Shin</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeonghoon Kim</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seongho Choi</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Heo_Y/0/1/0/all/0/1\">Yu-Jung Heo</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a> (1 and 4), <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minsu Lee</a> (3 and 5), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Byoung-Tak Zhang</a> (3 and 5), <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1\">Jeh-Kwang Ryu</a> (1 and 4) ((1) Laboratory for Natural and Artificial Kin&#xe4;sthese, Convergence Research Center for Artificial Intelligence (CRC4AI), Dongguk University, Seoul, South Korea, (2) Department of Artificial Intelligence, Dongguk University, Seoul, South Korea, (3) Biointelligence Laboratory, Department of Computer Science and Engineering, Seoul National University, Seoul, South Korea, (4) Department of Physical Education, College of Education, Dongguk University, Seoul, South Korea, (5) AI Institute of Seoul National University (AIIS), Seoul, South Korea)",
          "description": "Developing video understanding intelligence is quite challenging because it\nrequires holistic integration of images, scripts, and sounds based on natural\nlanguage processing, temporal dependency, and reasoning. Recently, substantial\nattempts have been made on several video datasets with associated question\nanswering (QA) on a large scale. However, existing evaluation metrics for video\nquestion answering (VideoQA) do not provide meaningful analysis. To make\nprogress, we argue that a well-made framework, established on the way humans\nunderstand, is required to explain and evaluate the performance of\nunderstanding in detail. Then we propose a top-down evaluation system for\nVideoQA, based on the cognitive process of humans and story elements: Cognitive\nModules for Evaluation (CogME). CogME is composed of three cognitive modules:\ntargets, contents, and thinking. The interaction among the modules in the\nunderstanding procedure can be expressed in one sentence as follows: \"I\nunderstand the CONTENT of the TARGET through a way of THINKING.\" Each module\nhas sub-components derived from the story elements. We can specify the required\naspects of understanding by annotating the sub-components to individual\nquestions. CogME thus provides a framework for an elaborated specification of\nVideoQA datasets. To examine the suitability of a VideoQA dataset for\nvalidating video understanding intelligence, we evaluated the baseline model of\nthe DramaQA dataset by applying CogME. The evaluation reveals that story\nelements are unevenly reflected in the existing dataset, and the model based on\nthe dataset may cause biased predictions. Although this study has only been\nable to grasp a narrow range of stories, we expect that it offers the first\nstep in considering the cognitive process of humans on the video understanding\nintelligence of humans and AI.",
          "link": "http://arxiv.org/abs/2107.09847",
          "publishedOn": "2021-07-22T02:03:10.603Z",
          "wordCount": 818,
          "title": "CogME: A Novel Evaluation Metric for Video Understanding Intelligence. (arXiv:2107.09847v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junren Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1\">Baiying Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yanyan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhiguang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuqiang Wang</a>",
          "description": "Using multimodal neuroimaging data to characterize brain network is currently\nan advanced technique for Alzheimer's disease(AD) Analysis. Over recent years\nthe neuroimaging community has made tremendous progress in the study of\nresting-state functional magnetic resonance imaging (rs-fMRI) derived from\nblood-oxygen-level-dependent (BOLD) signals and Diffusion Tensor Imaging (DTI)\nderived from white matter fiber tractography. However, Due to the heterogeneity\nand complexity between BOLD signals and fiber tractography, Most existing\nmultimodal data fusion algorithms can not sufficiently take advantage of the\ncomplementary information between rs-fMRI and DTI. To overcome this problem, a\nnovel Hypergraph Generative Adversarial Networks(HGGAN) is proposed in this\npaper, which utilizes Interactive Hyperedge Neurons module (IHEN) and Optimal\nHypergraph Homomorphism algorithm(OHGH) to generate multimodal connectivity of\nBrain Network from rs-fMRI combination with DTI. To evaluate the performance of\nthis model, We use publicly available data from the ADNI database to\ndemonstrate that the proposed model not only can identify discriminative brain\nregions of AD but also can effectively improve classification performance.",
          "link": "http://arxiv.org/abs/2107.09953",
          "publishedOn": "2021-07-22T02:03:10.595Z",
          "wordCount": 612,
          "title": "Characterization Multimodal Connectivity of Brain Network by Hypergraph GAN for Alzheimer's Disease Analysis. (arXiv:2107.09953v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1\">Haiwen Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yunqing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>",
          "description": "In multimodal tasks, we find that the importance of text and image modal\ninformation is different for different input cases, and for this motivation, we\npropose a high-performance and highly general Dual-Router Dynamic Framework\n(DRDF), consisting of Dual-Router, MWF-Layer, experts and expert fusion unit.\nThe text router and image router in Dual-Router accept text modal information\nand image modal information, and use MWF-Layer to determine the importance of\nmodal information. Based on the result of the determination, MWF-Layer\ngenerates fused weights for the fusion of experts. Experts are model backbones\nthat match the current task. DRDF has high performance and high generality, and\nwe have tested 12 backbones such as Visual BERT on multimodal dataset Hateful\nmemes, unimodal dataset CIFAR10, CIFAR100, and TinyImagenet. Our DRDF\noutperforms all the baselines. We also verified the components of DRDF in\ndetail by ablations, compared and discussed the reasons and ideas of DRDF\ndesign.",
          "link": "http://arxiv.org/abs/2107.09909",
          "publishedOn": "2021-07-22T02:03:10.586Z",
          "wordCount": 600,
          "title": "DRDF: Determining the Importance of Different Multimodal Information with Dual-Router Dynamic Framework. (arXiv:2107.09909v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_Q/0/1/0/all/0/1\">Qiankun Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1\">Baiying Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yanyan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhiguang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuqiang Wang</a>",
          "description": "Multimodal neuroimage can provide complementary information about the\ndementia, but small size of complete multimodal data limits the ability in\nrepresentation learning. Moreover, the data distribution inconsistency from\ndifferent modalities may lead to ineffective fusion, which fails to\nsufficiently explore the intra-modal and inter-modal interactions and\ncompromises the disease diagnosis performance. To solve these problems, we\nproposed a novel multimodal representation learning and adversarial hypergraph\nfusion (MRL-AHF) framework for Alzheimer's disease diagnosis using complete\ntrimodal images. First, adversarial strategy and pre-trained model are\nincorporated into the MRL to extract latent representations from multimodal\ndata. Then two hypergraphs are constructed from the latent representations and\nthe adversarial network based on graph convolution is employed to narrow the\ndistribution difference of hyperedge features. Finally, the hyperedge-invariant\nfeatures are fused for disease prediction by hyperedge convolution. Experiments\non the public Alzheimer's Disease Neuroimaging Initiative(ADNI) database\ndemonstrate that our model achieves superior performance on Alzheimer's disease\ndetection compared with other related models and provides a possible way to\nunderstand the underlying mechanisms of disorder's progression by analyzing the\nabnormal brain connections.",
          "link": "http://arxiv.org/abs/2107.09928",
          "publishedOn": "2021-07-22T02:03:10.561Z",
          "wordCount": 630,
          "title": "Multimodal Representations Learning and Adversarial Hypergraph Fusion for Early Alzheimer's Disease Prediction. (arXiv:2107.09928v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09725",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singandhupe_A/0/1/0/all/0/1\">Ashutosh Singandhupe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+La_H/0/1/0/all/0/1\">Hung La</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_T/0/1/0/all/0/1\">Trung Dung Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_V/0/1/0/all/0/1\">Van Ho</a>",
          "description": "This work focuses on Registration or Alignment of 3D point sets. Although the\nRegistration problem is a well established problem and it's solved using\nmultiple variants of Iterative Closest Point (ICP) Algorithm, most of the\napproaches in the current state of the art still suffers from misalignment when\nthe \\textit{Source} and the \\textit{Target} point sets are separated by large\nrotations and translation. In this work, we propose a variant of the Standard\nICP algorithm, where we introduce a Correntropy Relationship Matrix in the\ncomputation of rotation and translation component which attempts to solve the\nlarge rotation and translation problem between \\textit{Source} and\n\\textit{Target} point sets. This matrix is created through correntropy\ncriterion which is updated in every iteration. The correntropy criterion\ndefined in this approach maintains the relationship between the points in the\n\\textit{Source} dataset and the \\textit{Target} dataset. Through our\nexperiments and validation we verify that our approach has performed well under\nvarious rotation and translation in comparison to the other well-known state of\nthe art methods available in the Point Cloud Library (PCL) as well as other\nmethods available as open source. We have uploaded our code in the github\nrepository for the readers to validate and verify our approach\nhttps://github.com/aralab-unr/CoSM-ICP.",
          "link": "http://arxiv.org/abs/2107.09725",
          "publishedOn": "2021-07-22T02:03:10.487Z",
          "wordCount": 644,
          "title": "Registration of 3D Point Sets Using Correntropy Similarity Matrix. (arXiv:2107.09725v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runnan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nenglun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yanhong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>",
          "description": "Detecting 3D landmarks on cone-beam computed tomography (CBCT) is crucial to\nassessing and quantifying the anatomical abnormalities in 3D cephalometric\nanalysis. However, the current methods are time-consuming and suffer from large\nbiases in landmark localization, leading to unreliable diagnosis results. In\nthis work, we propose a novel Structure-Aware Long Short-Term Memory framework\n(SA-LSTM) for efficient and accurate 3D landmark detection. To reduce the\ncomputational burden, SA-LSTM is designed in two stages. It first locates the\ncoarse landmarks via heatmap regression on a down-sampled CBCT volume and then\nprogressively refines landmarks by attentive offset regression using\nhigh-resolution cropped patches. To boost accuracy, SA-LSTM captures\nglobal-local dependence among the cropping patches via self-attention.\nSpecifically, a graph attention module implicitly encodes the landmark's global\nstructure to rationalize the predicted position. Furthermore, a novel\nattention-gated module recursively filters irrelevant local features and\nmaintains high-confident local predictions for aggregating the final result.\nExperiments show that our method significantly outperforms state-of-the-art\nmethods in terms of efficiency and accuracy on an in-house dataset and a public\ndataset, achieving 1.64 mm and 2.37 mm average errors, respectively, and using\nonly 0.5 seconds for inferring the whole CBCT volume of resolution 768*768*576.\nMoreover, all predicted landmarks are within 8 mm error, which is vital for\nacceptable cephalometric analysis.",
          "link": "http://arxiv.org/abs/2107.09899",
          "publishedOn": "2021-07-22T02:03:10.480Z",
          "wordCount": 655,
          "title": "Structure-Aware Long Short-Term Memory Network for 3D Cephalometric Landmark Detection. (arXiv:2107.09899v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiawei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiqiang He</a>",
          "description": "Deep learning models are notoriously data-hungry. Thus, there is an urging\nneed for data-efficient techniques in medical image analysis, where\nwell-annotated data are costly and time consuming to collect. Motivated by the\nrecently revived \"Copy-Paste\" augmentation, we propose TumorCP, a simple but\neffective object-level data augmentation method tailored for tumor\nsegmentation. TumorCP is online and stochastic, providing unlimited\naugmentation possibilities for tumors' subjects, locations, appearances, as\nwell as morphologies. Experiments on kidney tumor segmentation task demonstrate\nthat TumorCP surpasses the strong baseline by a remarkable margin of 7.12% on\ntumor Dice. Moreover, together with image-level data augmentation, it beats the\ncurrent state-of-the-art by 2.32% on tumor Dice. Comprehensive ablation studies\nare performed to validate the effectiveness of TumorCP. Meanwhile, we show that\nTumorCP can lead to striking improvements in extremely low-data regimes.\nEvaluated with only 10% labeled data, TumorCP significantly boosts tumor Dice\nby 21.87%. To the best of our knowledge, this is the very first work exploring\nand extending the \"Copy-Paste\" design in medical imaging domain. Code is\navailable at: https://github.com/YaoZhang93/TumorCP.",
          "link": "http://arxiv.org/abs/2107.09843",
          "publishedOn": "2021-07-22T02:03:10.472Z",
          "wordCount": 619,
          "title": "TumorCP: A Simple but Effective Object-Level Data Augmentation for Tumor Segmentation. (arXiv:2107.09843v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lewy_D/0/1/0/all/0/1\">Dominik Lewy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandziuk_J/0/1/0/all/0/1\">Jacek Ma&#x144;dziuk</a>",
          "description": "Deep Convolutional Neural Networks have made an incredible progress in many\nComputer Vision tasks. This progress, however, often relies on the availability\nof large amounts of the training data, required to prevent over-fitting, which\nin many domains entails significant cost of manual data labeling. An\nalternative approach is application of data augmentation (DA) techniques that\naim at model regularization by creating additional observations from the\navailable ones. This survey focuses on two DA research streams: image mixing\nand automated selection of augmentation strategies. First, the presented\nmethods are briefly described, and then qualitatively compared with respect to\ntheir key characteristics. Various quantitative comparisons are also included\nbased on the results reported in recent DA literature. This review mainly\ncovers the methods published in the materials of top-tier conferences and in\nleading journals in the years 2017-2021.",
          "link": "http://arxiv.org/abs/2107.09887",
          "publishedOn": "2021-07-22T02:03:10.455Z",
          "wordCount": 570,
          "title": "An overview of mixing augmentation methods and augmentation strategies. (arXiv:2107.09887v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09892",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sudarshan_V/0/1/0/all/0/1\">Viswanath P. Sudarshan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Upadhyay_U/0/1/0/all/0/1\">Uddeshya Upadhyay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Egan_G/0/1/0/all/0/1\">Gary F. Egan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhaolin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Awate_S/0/1/0/all/0/1\">Suyash P. Awate</a>",
          "description": "Radiation exposure in positron emission tomography (PET) imaging limits its\nusage in the studies of radiation-sensitive populations, e.g., pregnant women,\nchildren, and adults that require longitudinal imaging. Reducing the PET\nradiotracer dose or acquisition time reduces photon counts, which can\ndeteriorate image quality. Recent deep-neural-network (DNN) based methods for\nimage-to-image translation enable the mapping of low-quality PET images\n(acquired using substantially reduced dose), coupled with the associated\nmagnetic resonance imaging (MRI) images, to high-quality PET images. However,\nsuch DNN methods focus on applications involving test data that match the\nstatistical characteristics of the training data very closely and give little\nattention to evaluating the performance of these DNNs on new\nout-of-distribution (OOD) acquisitions. We propose a novel DNN formulation that\nmodels the (i) underlying sinogram-based physics of the PET imaging system and\n(ii) the uncertainty in the DNN output through the per-voxel heteroscedasticity\nof the residuals between the predicted and the high-quality reference images.\nOur sinogram-based uncertainty-aware DNN framework, namely, suDNN, estimates a\nstandard-dose PET image using multimodal input in the form of (i) a\nlow-dose/low-count PET image and (ii) the corresponding multi-contrast MRI\nimages, leading to improved robustness of suDNN to OOD acquisitions. Results on\nin vivo simultaneous PET-MRI, and various forms of OOD data in PET-MRI, show\nthe benefits of suDNN over the current state of the art, quantitatively and\nqualitatively.",
          "link": "http://arxiv.org/abs/2107.09892",
          "publishedOn": "2021-07-22T02:03:10.273Z",
          "wordCount": 699,
          "title": "Towards Lower-Dose PET using Physics-Based Uncertainty-Aware Multimodal Learning with Robustness to Out-of-Distribution Data. (arXiv:2107.09892v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09700",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hong_S/0/1/0/all/0/1\">Sungmin Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marinescu_R/0/1/0/all/0/1\">Razvan Marinescu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bonkhoff_A/0/1/0/all/0/1\">Anna K. Bonkhoff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bretzner_M/0/1/0/all/0/1\">Martin Bretzner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rost_N/0/1/0/all/0/1\">Natalia S. Rost</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1\">Polina Golland</a>",
          "description": "Image synthesis via Generative Adversarial Networks (GANs) of\nthree-dimensional (3D) medical images has great potential that can be extended\nto many medical applications, such as, image enhancement and disease\nprogression modeling. However, current GAN technologies for 3D medical image\nsynthesis need to be significantly improved to be readily adapted to real-world\nmedical problems. In this paper, we extend the state-of-the-art StyleGAN2\nmodel, which natively works with two-dimensional images, to enable 3D image\nsynthesis. In addition to the image synthesis, we investigate the\ncontrollability and interpretability of the 3D-StyleGAN via style vectors\ninherited form the original StyleGAN2 that are highly suitable for medical\napplications: (i) the latent space projection and reconstruction of unseen real\nimages, and (ii) style mixing. We demonstrate the 3D-StyleGAN's performance and\nfeasibility with ~12,000 three-dimensional full brain MR T1 images, although it\ncan be applied to any 3D volumetric images. Furthermore, we explore different\nconfigurations of hyperparameters to investigate potential improvement of the\nimage synthesis with larger networks. The codes and pre-trained networks are\navailable online: https://github.com/sh4174/3DStyleGAN.",
          "link": "http://arxiv.org/abs/2107.09700",
          "publishedOn": "2021-07-22T02:03:10.256Z",
          "wordCount": 664,
          "title": "3D-StyleGAN: A Style-Based Generative Adversarial Network for Generative Modeling of Three-Dimensional Medical Images. (arXiv:2107.09700v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1\">Yeong-Jun Cho</a>",
          "description": "In this paper, we propose a novel evaluation metric for performance\nevaluation of semantic segmentation. In recent years, many studies have tried\nto train pixel-level classifiers on large-scale image datasets to perform\naccurate semantic segmentation. The goal of semantic segmentation is to assign\na class label of each pixel in the scene. It has various potential applications\nin computer vision fields e.g., object detection, classification, scene\nunderstanding and Etc. To validate the proposed wIoU evaluation metric, we\ntested state-of-the art methods on public benchmark datasets (e.g., KITTI)\nbased on the proposed wIoU metric and compared with other conventional\nevaluation metrics.",
          "link": "http://arxiv.org/abs/2107.09858",
          "publishedOn": "2021-07-22T02:03:10.210Z",
          "wordCount": 537,
          "title": "Weighted Intersection over Union (wIoU): A New Evaluation Metric for Image Segmentation. (arXiv:2107.09858v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rochan_M/0/1/0/all/0/1\">Mrigank Rochan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aich_S/0/1/0/all/0/1\">Shubhra Aich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corral_Soto_E/0/1/0/all/0/1\">Eduardo R. Corral-Soto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabatchian_A/0/1/0/all/0/1\">Amir Nabatchian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingbing Liu</a>",
          "description": "In this paper, we focus on a less explored, but more realistic and complex\nproblem of domain adaptation in LiDAR semantic segmentation. There is a\nsignificant drop in performance of an existing segmentation model when training\n(source domain) and testing (target domain) data originate from different LiDAR\nsensors. To overcome this shortcoming, we propose an unsupervised domain\nadaptation framework that leverages unlabeled target domain data for\nself-supervision, coupled with an unpaired mask transfer strategy to mitigate\nthe impact of domain shifts. Furthermore, we introduce gated adapter modules\nwith a small number of parameters into the network to account for target\ndomain-specific information. Experiments adapting from both real-to-real and\nsynthetic-to-real LiDAR semantic segmentation benchmarks demonstrate the\nsignificant improvement over prior arts.",
          "link": "http://arxiv.org/abs/2107.09783",
          "publishedOn": "2021-07-22T02:03:10.176Z",
          "wordCount": 567,
          "title": "Unsupervised Domain Adaptation in LiDAR Semantic Segmentation with Self-Supervision and Gated Adapters. (arXiv:2107.09783v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara L. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "Detecting customized moments and highlights from videos given natural\nlanguage (NL) user queries is an important but under-studied topic. One of the\nchallenges in pursuing this direction is the lack of annotated data. To address\nthis issue, we present the Query-based Video Highlights (QVHighlights) dataset.\nIt consists of over 10,000 YouTube videos, covering a wide range of topics,\nfrom everyday activities and travel in lifestyle vlog videos to social and\npolitical activities in news videos. Each video in the dataset is annotated\nwith: (1) a human-written free-form NL query, (2) relevant moments in the video\nw.r.t. the query, and (3) five-point scale saliency scores for all\nquery-relevant clips. This comprehensive annotation enables us to develop and\nevaluate systems that detect relevant moments as well as salient highlights for\ndiverse, flexible user queries. We also present a strong baseline for this\ntask, Moment-DETR, a transformer encoder-decoder model that views moment\nretrieval as a direct set prediction problem, taking extracted video and query\nrepresentations as inputs and predicting moment coordinates and saliency scores\nend-to-end. While our model does not utilize any human prior, we show that it\nperforms competitively when compared to well-engineered architectures. With\nweakly supervised pretraining using ASR captions, Moment-DETR substantially\noutperforms previous methods. Lastly, we present several ablations and\nvisualizations of Moment-DETR. Data and code is publicly available at\nhttps://github.com/jayleicn/moment_detr",
          "link": "http://arxiv.org/abs/2107.09609",
          "publishedOn": "2021-07-21T02:01:37.296Z",
          "wordCount": 680,
          "title": "QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries. (arXiv:2107.09609v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gulshad_S/0/1/0/all/0/1\">Sadaf Gulshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sosnovik_I/0/1/0/all/0/1\">Ivan Sosnovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1\">Arnold Smeulders</a>",
          "description": "We focus on building robustness in the convolutions of neural visual\nclassifiers, especially against natural perturbations like elastic\ndeformations, occlusions and Gaussian noise. Existing CNNs show outstanding\nperformance on clean images, but fail to tackle naturally occurring\nperturbations. In this paper, we start from elastic perturbations, which\napproximate (local) view-point changes of the object. We present\nelastically-augmented convolutions (EAConv) by parameterizing filters as a\ncombination of fixed elastically-perturbed bases functions and trainable\nweights for the purpose of integrating unseen viewpoints in the CNN. We show on\nCIFAR-10 and STL-10 datasets that the general robustness of our method on\nunseen occlusion and Gaussian perturbations improves, while even improving the\nperformance on clean images slightly without performing any data augmentation.",
          "link": "http://arxiv.org/abs/2107.09391",
          "publishedOn": "2021-07-21T02:01:37.289Z",
          "wordCount": 556,
          "title": "Built-in Elastic Transformations for Improved Robustness. (arXiv:2107.09391v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.14435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wenbin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Q/0/1/0/all/0/1\">Qi Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jun He</a>",
          "description": "Human activity recognition (HAR) in ubiquitous computing has been beginning\nto incorporate attention into the context of deep neural networks (DNNs), in\nwhich the rich sensing data from multimodal sensors such as accelerometer and\ngyroscope is used to infer human activities. Recently, two attention methods\nare proposed via combining with Gated Recurrent Units (GRU) and Long Short-Term\nMemory (LSTM) network, which can capture the dependencies of sensing signals in\nboth spatial and temporal domains simultaneously. However, recurrent networks\noften have a weak feature representing power compared with convolutional neural\nnetworks (CNNs). On the other hand, two attention, i.e., hard attention and\nsoft attention, are applied in temporal domains via combining with CNN, which\npay more attention to the target activity from a long sequence. However, they\ncan only tell where to focus and miss channel information, which plays an\nimportant role in deciding what to focus. As a result, they fail to address the\nspatial-temporal dependencies of multimodal sensing signals, compared with\nattention-based GRU or LSTM. In the paper, we propose a novel dual attention\nmethod called DanHAR, which introduces the framework of blending channel\nattention and temporal attention on a CNN, demonstrating superiority in\nimproving the comprehensibility for multimodal HAR. Extensive experiments on\nfour public HAR datasets and weakly labeled dataset show that DanHAR achieves\nstate-of-the-art performance with negligible overhead of parameters.\nFurthermore, visualizing analysis is provided to show that our attention can\namplifies more important sensor modalities and timesteps during classification,\nwhich agrees well with human common intuition.",
          "link": "http://arxiv.org/abs/2006.14435",
          "publishedOn": "2021-07-21T02:01:36.612Z",
          "wordCount": 734,
          "title": "DanHAR: Dual Attention Network For Multimodal Human Activity Recognition Using Wearable Sensors. (arXiv:2006.14435v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Montenegro_H/0/1/0/all/0/1\">H. Montenegro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_W/0/1/0/all/0/1\">W. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1\">J. S. Cardoso</a>",
          "description": "The use of Deep Learning in the medical field is hindered by the lack of\ninterpretability. Case-based interpretability strategies can provide intuitive\nexplanations for deep learning models' decisions, thus, enhancing trust.\nHowever, the resulting explanations threaten patient privacy, motivating the\ndevelopment of privacy-preserving methods compatible with the specifics of\nmedical data. In this work, we analyze existing privacy-preserving methods and\ntheir respective capacity to anonymize medical data while preserving\ndisease-related semantic features. We find that the PPRL-VGAN deep learning\nmethod was the best at preserving the disease-related semantic features while\nguaranteeing a high level of privacy among the compared state-of-the-art\nmethods. Nevertheless, we emphasize the need to improve privacy-preserving\nmethods for medical imaging, as we identified relevant drawbacks in all\nexisting privacy-preserving approaches.",
          "link": "http://arxiv.org/abs/2107.09652",
          "publishedOn": "2021-07-21T02:01:36.586Z",
          "wordCount": 571,
          "title": "Towards Privacy-preserving Explanations in Medical Image Analysis. (arXiv:2107.09652v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chongyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>",
          "description": "End-to-end text-spotting, which aims to integrate detection and recognition\nin a unified framework, has attracted increasing attention due to its\nsimplicity of the two complimentary tasks. It remains an open problem\nespecially when processing arbitrarily-shaped text instances. Previous methods\ncan be roughly categorized into two groups: character-based and\nsegmentation-based, which often require character-level annotations and/or\ncomplex post-processing due to the unstructured output. Here, we tackle\nend-to-end text spotting by presenting Adaptive Bezier Curve Network v2 (ABCNet\nv2). Our main contributions are four-fold: 1) For the first time, we adaptively\nfit arbitrarily-shaped text by a parameterized Bezier curve, which, compared\nwith segmentation-based methods, can not only provide structured output but\nalso controllable representation. 2) We design a novel BezierAlign layer for\nextracting accurate convolution features of a text instance of arbitrary\nshapes, significantly improving the precision of recognition over previous\nmethods. 3) Different from previous methods, which often suffer from complex\npost-processing and sensitive hyper-parameters, our ABCNet v2 maintains a\nsimple pipeline with the only post-processing non-maximum suppression (NMS). 4)\nAs the performance of text recognition closely depends on feature alignment,\nABCNet v2 further adopts a simple yet effective coordinate convolution to\nencode the position of the convolutional filters, which leads to a considerable\nimprovement with negligible computation overhead. Comprehensive experiments\nconducted on various bilingual (English and Chinese) benchmark datasets\ndemonstrate that ABCNet v2 can achieve state-of-the-art performance while\nmaintaining very high efficiency.",
          "link": "http://arxiv.org/abs/2105.03620",
          "publishedOn": "2021-07-21T02:01:36.398Z",
          "wordCount": 728,
          "title": "ABCNet v2: Adaptive Bezier-Curve Network for Real-time End-to-end Text Spotting. (arXiv:2105.03620v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Dahu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaodong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wenming Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Ye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>",
          "description": "Multi-person pose estimation is an attractive and challenging task. Existing\nmethods are mostly based on two-stage frameworks, which include top-down and\nbottom-up methods. Two-stage methods either suffer from high computational\nredundancy for additional person detectors or they need to group keypoints\nheuristically after predicting all the instance-agnostic keypoints. The\nsingle-stage paradigm aims to simplify the multi-person pose estimation\npipeline and receives a lot of attention. However, recent single-stage methods\nhave the limitation of low performance due to the difficulty of regressing\nvarious full-body poses from a single feature vector. Different from previous\nsolutions that involve complex heuristic designs, we present a simple yet\neffective solution by employing instance-aware dynamic networks. Specifically,\nwe propose an instance-aware module to adaptively adjust (part of) the network\nparameters for each instance. Our solution can significantly increase the\ncapacity and adaptive-ability of the network for recognizing various poses,\nwhile maintaining a compact end-to-end trainable pipeline. Extensive\nexperiments on the MS-COCO dataset demonstrate that our method achieves\nsignificant improvement over existing single-stage methods, and makes a better\nbalance of accuracy and efficiency compared to the state-of-the-art two-stage\napproaches.",
          "link": "http://arxiv.org/abs/2107.08982",
          "publishedOn": "2021-07-21T02:01:36.213Z",
          "wordCount": 662,
          "title": "InsPose: Instance-Aware Networks for Single-Stage Multi-Person Pose Estimation. (arXiv:2107.08982v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borkman_S/0/1/0/all/0/1\">Steve Borkman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crespi_A/0/1/0/all/0/1\">Adam Crespi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhakad_S/0/1/0/all/0/1\">Saurav Dhakad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_S/0/1/0/all/0/1\">Sujoy Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogins_J/0/1/0/all/0/1\">Jonathan Hogins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jhang_Y/0/1/0/all/0/1\">You-Cyuan Jhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamalzadeh_M/0/1/0/all/0/1\">Mohsen Kamalzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_S/0/1/0/all/0/1\">Steven Leal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parisi_P/0/1/0/all/0/1\">Pete Parisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_C/0/1/0/all/0/1\">Cesar Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_W/0/1/0/all/0/1\">Wesley Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thaman_A/0/1/0/all/0/1\">Alex Thaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warren_S/0/1/0/all/0/1\">Samuel Warren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_N/0/1/0/all/0/1\">Nupur Yadav</a>",
          "description": "We introduce the Unity Perception package which aims to simplify and\naccelerate the process of generating synthetic datasets for computer vision\ntasks by offering an easy-to-use and highly customizable toolset. This\nopen-source package extends the Unity Editor and engine components to generate\nperfectly annotated examples for several common computer vision tasks.\nAdditionally, it offers an extensible Randomization framework that lets the\nuser quickly construct and configure randomized simulation parameters in order\nto introduce variation into the generated datasets. We provide an overview of\nthe provided tools and how they work, and demonstrate the value of the\ngenerated synthetic datasets by training a 2D object detection model. The model\ntrained with mostly synthetic data outperforms the model trained using only\nreal data.",
          "link": "http://arxiv.org/abs/2107.04259",
          "publishedOn": "2021-07-21T02:01:36.164Z",
          "wordCount": 613,
          "title": "Unity Perception: Generate Synthetic Data for Computer Vision. (arXiv:2107.04259v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eiras_F/0/1/0/all/0/1\">Francisco Eiras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">M. Pawan Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1\">Puneet K. Dokania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1\">Adel Bibi</a>",
          "description": "Randomized smoothing has recently emerged as an effective tool that enables\ncertification of deep neural network classifiers at scale. All prior art on\nrandomized smoothing has focused on isotropic $\\ell_p$ certification, which has\nthe advantage of yielding certificates that can be easily compared among\nisotropic methods via $\\ell_p$-norm radius. However, isotropic certification\nlimits the region that can be certified around an input to worst-case\nadversaries, i.e., it cannot reason about other \"close\", potentially large,\nconstant prediction safe regions. To alleviate this issue, (i) we theoretically\nextend the isotropic randomized smoothing $\\ell_1$ and $\\ell_2$ certificates to\ntheir generalized anisotropic counterparts following a simplified analysis.\nMoreover, (ii) we propose evaluation metrics allowing for the comparison of\ngeneral certificates - a certificate is superior to another if it certifies a\nsuperset region - with the quantification of each certificate through the\nvolume of the certified region. We introduce ANCER, a practical framework for\nobtaining anisotropic certificates for a given test set sample via volume\nmaximization. Our empirical results demonstrate that ANCER achieves\nstate-of-the-art $\\ell_1$ and $\\ell_2$ certified accuracy on both CIFAR-10 and\nImageNet at multiple radii, while certifying substantially larger regions in\nterms of volume, thus highlighting the benefits of moving away from isotropic\nanalysis. Code used in our experiments is available in\nhttps://github.com/MotasemAlfarra/ANCER.",
          "link": "http://arxiv.org/abs/2107.04570",
          "publishedOn": "2021-07-21T02:01:36.138Z",
          "wordCount": 687,
          "title": "ANCER: Anisotropic Certification via Sample-wise Volume Maximization. (arXiv:2107.04570v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.00384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhaohui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Miaojing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1\">Yannis Avrithis</a>",
          "description": "Weakly-supervised object detection attempts to limit the amount of\nsupervision by dispensing the need for bounding boxes, but still assumes\nimage-level labels on the entire training set. In this work, we study the\nproblem of training an object detector from one or few images with image-level\nlabels and a larger set of completely unlabeled images. This is an extreme case\nof semi-supervised learning where the labeled data are not enough to bootstrap\nthe learning of a detector. Our solution is to train a weakly-supervised\nstudent detector model from image-level pseudo-labels generated on the\nunlabeled set by a teacher classifier model, bootstrapped by region-level\nsimilarities to labeled images. Building upon the recent representative\nweakly-supervised pipeline PCL, our method can use more unlabeled images to\nachieve performance competitive or superior to many recent weakly-supervised\ndetection solutions.",
          "link": "http://arxiv.org/abs/1912.00384",
          "publishedOn": "2021-07-21T02:01:35.994Z",
          "wordCount": 633,
          "title": "Training Object Detectors from Few Weakly-Labeled and Many Unlabeled Images. (arXiv:1912.00384v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yueming Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Bo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>",
          "description": "In recent years, virtual makeup applications have become more and more\npopular. However, it is still challenging to propose a robust makeup transfer\nmethod in the real-world environment. Current makeup transfer methods mostly\nwork well on good-conditioned clean makeup images, but transferring makeup that\nexhibits shadow and occlusion is not satisfying. To alleviate it, we propose a\nnovel makeup transfer method, called 3D-Aware Shadow and Occlusion Robust GAN\n(SOGAN). Given the source and the reference faces, we first fit a 3D face model\nand then disentangle the faces into shape and texture. In the texture branch,\nwe map the texture to the UV space and design a UV texture generator to\ntransfer the makeup. Since human faces are symmetrical in the UV space, we can\nconveniently remove the undesired shadow and occlusion from the reference image\nby carefully designing a Flip Attention Module (FAM). After obtaining cleaner\nmakeup features from the reference image, a Makeup Transfer Module (MTM) is\nintroduced to perform accurate makeup transfer. The qualitative and\nquantitative experiments demonstrate that our SOGAN not only achieves superior\nresults in shadow and occlusion situations but also performs well in large pose\nand expression variations.",
          "link": "http://arxiv.org/abs/2104.10567",
          "publishedOn": "2021-07-21T02:01:35.279Z",
          "wordCount": 676,
          "title": "SOGAN: 3D-Aware Shadow and Occlusion Robust GAN for Makeup Transfer. (arXiv:2104.10567v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.10122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shaodi You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Ying Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Q/0/1/0/all/0/1\">Qiu Shen</a>",
          "description": "High-resolution hyperspectral images (HSIs) contain the response of each\npixel in different spectral bands, which can be used to effectively distinguish\nvarious objects in complex scenes. While HSI cameras have become low cost,\nalgorithms based on it have not been well exploited. In this paper, we focus on\na novel topic, weakly-supervised semantic segmentation in cityscape via HSIs.\nIt is based on the idea that high-resolution HSIs in city scenes contain rich\nspectral information, which can be easily associated to semantics without\nmanual labeling. Therefore, it enables low cost, highly reliable semantic\nsegmentation in complex scenes. Specifically, in this paper, we theoretically\nanalyze the HSIs and introduce a weakly-supervised HSI semantic segmentation\nframework, which utilizes spectral information to improve the coarse labels to\na finer degree. The experimental results show that our method can obtain highly\ncompetitive labels and even have higher edge fineness than artificial fine\nlabels in some classes. At the same time, the results also show that the\nrefined labels can effectively improve the effect of semantic segmentation. The\ncombination of HSIs and semantic segmentation proves that HSIs have great\npotential in high-level visual tasks.",
          "link": "http://arxiv.org/abs/2012.10122",
          "publishedOn": "2021-07-21T02:01:35.265Z",
          "wordCount": 652,
          "title": "Weakly-supervised Semantic Segmentation in Cityscape via Hyperspectral Image. (arXiv:2012.10122v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15599",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinzhao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yante Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>",
          "description": "Facial affect analysis (FAA) using visual signals is important in\nhuman-computer interaction. Early methods focus on extracting appearance and\ngeometry features associated with human affects, while ignoring the latent\nsemantic information among individual facial changes, leading to limited\nperformance and generalization. Recent work attempts to establish a graph-based\nrepresentation to model these semantic relationships and develop frameworks to\nleverage them for various FAA tasks. In this paper, we provide a comprehensive\nreview of graph-based FAA, including the evolution of algorithms and their\napplications. First, the FAA background knowledge is introduced, especially on\nthe role of the graph. We then discuss approaches that are widely used for\ngraph-based affective representation in literature and show a trend towards\ngraph construction. For the relational reasoning in graph-based FAA, existing\nstudies are categorized according to their usage of traditional methods or deep\nmodels, with a special emphasis on the latest graph neural networks.\nPerformance comparisons of the state-of-the-art graph-based FAA methods are\nalso summarized. Finally, we discuss the challenges and potential directions.\nAs far as we know, this is the first survey of graph-based FAA methods. Our\nfindings can serve as a reference for future research in this field.",
          "link": "http://arxiv.org/abs/2103.15599",
          "publishedOn": "2021-07-21T02:01:35.259Z",
          "wordCount": 707,
          "title": "Graph-based Facial Affect Analysis: A Review of Methods, Applications and Challenges. (arXiv:2103.15599v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1\">Christoph Reich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prangemeier_T/0/1/0/all/0/1\">Tim Prangemeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildner_C/0/1/0/all/0/1\">Christian Wildner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1\">Heinz Koeppl</a>",
          "description": "Time-lapse fluorescent microscopy (TLFM) combined with predictive\nmathematical modelling is a powerful tool to study the inherently dynamic\nprocesses of life on the single-cell level. Such experiments are costly,\ncomplex and labour intensive. A complimentary approach and a step towards in\nsilico experimentation, is to synthesise the imagery itself. Here, we propose\nMulti-StyleGAN as a descriptive approach to simulate time-lapse fluorescence\nmicroscopy imagery of living cells, based on a past experiment. This novel\ngenerative adversarial network synthesises a multi-domain sequence of\nconsecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple live\nyeast cells in microstructured environments and train on a dataset recorded in\nour laboratory. The simulation captures underlying biophysical factors and time\ndependencies, such as cell morphology, growth, physical interactions, as well\nas the intensity of a fluorescent reporter protein. An immediate application is\nto generate additional training and validation data for feature extraction\nalgorithms or to aid and expedite development of advanced experimental\ntechniques such as online monitoring or control of cells.\n\nCode and dataset is available at\nhttps://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.",
          "link": "http://arxiv.org/abs/2106.08285",
          "publishedOn": "2021-07-21T02:01:35.229Z",
          "wordCount": 674,
          "title": "Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell Microscopy. (arXiv:2106.08285v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sreenivasaiah_D/0/1/0/all/0/1\">Deepthi Sreenivasaiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otterbach_J/0/1/0/all/0/1\">Johannes Otterbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wollmann_T/0/1/0/all/0/1\">Thomas Wollmann</a>",
          "description": "Image segmentation is a common and challenging task in autonomous driving.\nAvailability of sufficient pixel-level annotations for the training data is a\nhurdle. Active learning helps learning from small amounts of data by suggesting\nthe most promising samples for labeling. In this work, we propose a new\npool-based method for active learning, which proposes promising patches\nextracted from full image, in each acquisition step. The problem is framed in\nan exploration-exploitation framework by combining an embedding based on\nUniform Manifold Approximation to model representativeness with entropy as\nuncertainty measure to model informativeness. We applied our proposed method to\nthe autonomous driving datasets CamVid and Cityscapes and performed a\nquantitative comparison with state-of-the-art baselines. We find that our\nactive learning method achieves better performance compared to previous\nmethods.",
          "link": "http://arxiv.org/abs/2106.11858",
          "publishedOn": "2021-07-21T02:01:35.220Z",
          "wordCount": 584,
          "title": "MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Linxi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_DArpino_C/0/1/0/all/0/1\">Claudia P&#xe9;rez-D&#x27;Arpino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1\">Shyamal Buch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_L/0/1/0/all/0/1\">Lyne P. Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1\">Micael E. Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1\">Kent Vainio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>",
          "description": "We present iGibson 1.0, a novel simulation environment to develop robotic\nsolutions for interactive tasks in large-scale realistic scenes. Our\nenvironment contains 15 fully interactive home-sized scenes with 108 rooms\npopulated with rigid and articulated objects. The scenes are replicas of\nreal-world homes, with distribution and the layout of objects aligned to those\nof the real world. iGibson 1.0 integrates several key features to facilitate\nthe study of interactive tasks: i) generation of high-quality virtual sensor\nsignals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain\nrandomization to change the materials of the objects (both visual and physical)\nand/or their shapes, iii) integrated sampling-based motion planners to generate\ncollision-free trajectories for robot bases and arms, and iv) intuitive\nhuman-iGibson interface that enables efficient collection of human\ndemonstrations. Through experiments, we show that the full interactivity of the\nscenes enables agents to learn useful visual representations that accelerate\nthe training of downstream manipulation tasks. We also show that iGibson 1.0\nfeatures enable the generalization of navigation agents, and that the\nhuman-iGibson interface and integrated motion planners facilitate efficient\nimitation learning of human demonstrated (mobile) manipulation behaviors.\niGibson 1.0 is open-source, equipped with comprehensive examples and\ndocumentation. For more information, visit our project website:\nthis http URL",
          "link": "http://arxiv.org/abs/2012.02924",
          "publishedOn": "2021-07-21T02:01:35.206Z",
          "wordCount": 737,
          "title": "iGibson 1.0: a Simulation Environment for Interactive Tasks in Large Realistic Scenes. (arXiv:2012.02924v4 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dang_V/0/1/0/all/0/1\">Vien Ngoc Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galati_F/0/1/0/all/0/1\">Francesco Galati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cortese_R/0/1/0/all/0/1\">Rosa Cortese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giacomo_G/0/1/0/all/0/1\">Giuseppe Di Giacomo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marconetto_V/0/1/0/all/0/1\">Viola Marconetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_P/0/1/0/all/0/1\">Prateek Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenzi_M/0/1/0/all/0/1\">Marco Lorenzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prados_F/0/1/0/all/0/1\">Ferran Prados</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuluaga_M/0/1/0/all/0/1\">Maria A. Zuluaga</a>",
          "description": "Deep learning techniques for 3D brain vessel image segmentation have not been\nas successful as in the segmentation of other organs and tissues. This can be\nexplained by two factors. First, deep learning techniques tend to show poor\nperformances at the segmentation of relatively small objects compared to the\nsize of the full image. Second, due to the complexity of vascular trees and the\nsmall size of vessels, it is challenging to obtain the amount of annotated\ntraining data typically needed by deep learning methods. To address these\nproblems, we propose a novel annotation-efficient deep learning vessel\nsegmentation framework. The framework avoids pixel-wise annotations, only\nrequiring weak patch-level labels to discriminate between vessel and non-vessel\n2D patches in the training set, in a setup similar to the CAPTCHAs used to\ndifferentiate humans from bots in web applications. The user-provided weak\nannotations are used for two tasks: 1) to synthesize pixel-wise pseudo-labels\nfor vessels and background in each patch, which are used to train a\nsegmentation network, and 2) to train a classifier network. The classifier\nnetwork allows to generate additional weak patch labels, further reducing the\nannotation burden, and it acts as a noise filter for poor quality images. We\nuse this framework for the segmentation of the cerebrovascular tree in\nTime-of-Flight angiography (TOF) and Susceptibility-Weighted Images (SWI). The\nresults show that the framework achieves state-of-the-art accuracy, while\nreducing the annotation time by ~77% w.r.t. learning-based segmentation methods\nusing pixel-wise labels for training.",
          "link": "http://arxiv.org/abs/2101.09321",
          "publishedOn": "2021-07-21T02:01:35.199Z",
          "wordCount": 746,
          "title": "Vessel-CAPTCHA: an efficient learning framework for vessel annotation and segmentation. (arXiv:2101.09321v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09584",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1\">Edward J. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meger_D/0/1/0/all/0/1\">David Meger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1\">Luis Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calandra_R/0/1/0/all/0/1\">Roberto Calandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_A/0/1/0/all/0/1\">Adriana Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdzal_M/0/1/0/all/0/1\">Michal Drozdzal</a>",
          "description": "Humans build 3D understandings of the world through active object\nexploration, using jointly their senses of vision and touch. However, in 3D\nshape reconstruction, most recent progress has relied on static datasets of\nlimited sensory data such as RGB images, depth maps or haptic readings, leaving\nthe active exploration of the shape largely unexplored. In active touch sensing\nfor 3D reconstruction, the goal is to actively select the tactile readings that\nmaximize the improvement in shape reconstruction accuracy. However, the\ndevelopment of deep learning-based active touch models is largely limited by\nthe lack of frameworks for shape exploration. In this paper, we focus on this\nproblem and introduce a system composed of: 1) a haptic simulator leveraging\nhigh spatial resolution vision-based tactile sensors for active touching of 3D\nobjects; 2) a mesh-based 3D shape reconstruction model that relies on tactile\nor visuotactile signals; and 3) a set of data-driven solutions with either\ntactile or visuotactile priors to guide the shape exploration. Our framework\nenables the development of the first fully data-driven solutions to active\ntouch on top of learned models for object understanding. Our experiments show\nthe benefits of such solutions in the task of 3D shape understanding where our\nmodels consistently outperform natural baselines. We provide our framework as a\ntool to foster future research in this direction.",
          "link": "http://arxiv.org/abs/2107.09584",
          "publishedOn": "2021-07-21T02:01:35.174Z",
          "wordCount": 664,
          "title": "Active 3D Shape Reconstruction from Vision and Touch. (arXiv:2107.09584v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Emma Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1\">Andy Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_R/0/1/0/all/0/1\">Rayan Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1\">Jin Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>",
          "description": "A major obstacle to the integration of deep learning models for chest x-ray\ninterpretation into clinical settings is the lack of understanding of their\nfailure modes. In this work, we first investigate whether there are patient\nsubgroups that chest x-ray models are likely to misclassify. We find that\npatient age and the radiographic finding of lung lesion, pneumothorax or\nsupport devices are statistically relevant features for predicting\nmisclassification for some chest x-ray models. Second, we develop\nmisclassification predictors on chest x-ray models using their outputs and\nclinical features. We find that our best performing misclassification\nidentifier achieves an AUROC close to 0.9 for most diseases. Third, employing\nour misclassification identifiers, we develop a corrective algorithm to\nselectively flip model predictions that have high likelihood of\nmisclassification at inference time. We observe F1 improvement on the\nprediction of Consolidation (0.008 [95% CI 0.005, 0.010]) and Edema (0.003,\n[95% CI 0.001, 0.006]). By carrying out our investigation on ten distinct and\nhigh-performing chest x-ray models, we are able to derive insights across model\narchitectures and offer a generalizable framework applicable to other medical\nimaging tasks.",
          "link": "http://arxiv.org/abs/2103.09957",
          "publishedOn": "2021-07-21T02:01:35.167Z",
          "wordCount": 697,
          "title": "CheXbreak: Misclassification Identification for Deep Learning Models Interpreting Chest X-rays. (arXiv:2103.09957v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klein_L/0/1/0/all/0/1\">Levente J. Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albrecht_C/0/1/0/all/0/1\">Conrad M. Albrecht</a>",
          "description": "Vegetation, trees in particular, sequester carbon by absorbing carbon dioxide\nfrom the atmosphere. However, the lack of efficient quantification methods of\ncarbon stored in trees renders it difficult to track the process. We present an\napproach to estimate the carbon storage in trees based on fusing multi-spectral\naerial imagery and LiDAR data to identify tree coverage, geometric shape, and\ntree species -- key attributes to carbon storage quantification. We demonstrate\nthat tree species information and their three-dimensional geometric shapes can\nbe estimated from aerial imagery in order to determine the tree's biomass.\nSpecifically, we estimate a total of $52,000$ tons of carbon sequestered in\ntrees for New York City's borough Manhattan.",
          "link": "http://arxiv.org/abs/2106.00182",
          "publishedOn": "2021-07-21T02:01:35.159Z",
          "wordCount": 587,
          "title": "Quantification of Carbon Sequestration in Urban Forests. (arXiv:2106.00182v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05108",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1\">Jessica Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1\">Ognjen Arandjelovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1\">David J Harrison</a>",
          "description": "Understanding the predictions made by Artificial Intelligence (AI) systems is\nbecoming more and more important as deep learning models are used for\nincreasingly complex and high-stakes tasks. Saliency mapping - an easily\ninterpretable visual attribution method - is one important tool for this, but\nexisting formulations are limited by either computational cost or architectural\nconstraints. We therefore propose Hierarchical Perturbation, a very fast and\ncompletely model-agnostic method for explaining model predictions with robust\nsaliency maps. Using standard benchmarks and datasets, we show that our\nsaliency maps are of competitive or superior quality to those generated by\nexisting model-agnostic methods - and are over 20X faster to compute.",
          "link": "http://arxiv.org/abs/2103.05108",
          "publishedOn": "2021-07-21T02:01:35.152Z",
          "wordCount": 587,
          "title": "Believe The HiPe: Hierarchical Perturbation for Fast, Robust and Model-Agnostic Explanations. (arXiv:2103.05108v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.03308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_A/0/1/0/all/0/1\">Ansheng You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kuiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>",
          "description": "Graph-based convolutional model such as non-local block has shown to be\neffective for strengthening the context modeling ability in convolutional\nneural networks (CNNs). However, its pixel-wise computational overhead is\nprohibitive which renders it unsuitable for high resolution imagery. In this\npaper, we explore the efficiency of context graph reasoning and propose a novel\nframework called Squeeze Reasoning. Instead of propagating information on the\nspatial map, we first learn to squeeze the input feature into a channel-wise\nglobal vector and perform reasoning within the single vector where the\ncomputation cost can be significantly reduced. Specifically, we build the node\ngraph in the vector where each node represents an abstract semantic concept.\nThe refined feature within the same semantic category results to be consistent,\nwhich is thus beneficial for downstream tasks. We show that our approach can be\nmodularized as an end-to-end trained block and can be easily plugged into\nexisting networks. {Despite its simplicity and being lightweight, the proposed\nstrategy allows us to establish the considerable results on different semantic\nsegmentation datasets and shows significant improvements with respect to strong\nbaselines on various other scene understanding tasks including object\ndetection, instance segmentation and panoptic segmentation.} Code is available\nat \\url{https://github.com/lxtGH/SFSegNets}.",
          "link": "http://arxiv.org/abs/2011.03308",
          "publishedOn": "2021-07-21T02:01:35.132Z",
          "wordCount": 687,
          "title": "Towards Efficient Scene Understanding via Squeeze Reasoning. (arXiv:2011.03308v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaunet_T/0/1/0/all/0/1\">Theo Jaunet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kervadec_C/0/1/0/all/0/1\">Corentin Kervadec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuillemot_R/0/1/0/all/0/1\">Romain Vuillemot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antipov_G/0/1/0/all/0/1\">Grigory Antipov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baccouche_M/0/1/0/all/0/1\">Moez Baccouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1\">Christian Wolf</a>",
          "description": "Visual Question Answering systems target answering open-ended textual\nquestions given input images. They are a testbed for learning high-level\nreasoning with a primary use in HCI, for instance assistance for the visually\nimpaired. Recent research has shown that state-of-the-art models tend to\nproduce answers exploiting biases and shortcuts in the training data, and\nsometimes do not even look at the input image, instead of performing the\nrequired reasoning steps. We present VisQA, a visual analytics tool that\nexplores this question of reasoning vs. bias exploitation. It exposes the key\nelement of state-of-the-art neural models -- attention maps in transformers.\nOur working hypothesis is that reasoning steps leading to model predictions are\nobservable from attention distributions, which are particularly useful for\nvisualization. The design process of VisQA was motivated by well-known bias\nexamples from the fields of deep learning and vision-language reasoning and\nevaluated in two ways. First, as a result of a collaboration of three fields,\nmachine learning, vision and language reasoning, and data analytics, the work\nlead to a better understanding of bias exploitation of neural models for VQA,\nwhich eventually resulted in an impact on its design and training through the\nproposition of a method for the transfer of reasoning patterns from an oracle\nmodel. Second, we also report on the design of VisQA, and a goal-oriented\nevaluation of VisQA targeting the analysis of a model decision process from\nmultiple experts, providing evidence that it makes the inner workings of models\naccessible to users.",
          "link": "http://arxiv.org/abs/2104.00926",
          "publishedOn": "2021-07-21T02:01:35.119Z",
          "wordCount": 720,
          "title": "VisQA: X-raying Vision and Language Reasoning in Transformers. (arXiv:2104.00926v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kishor Datta Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_Z/0/1/0/all/0/1\">Zahid Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_D/0/1/0/all/0/1\">Dipankar Dasgupta</a>",
          "description": "Developing secure machine learning models from adversarial examples is\nchallenging as various methods are continually being developed to generate\nadversarial attacks. In this work, we propose an evolutionary approach to\nautomatically determine Image Processing Techniques Sequence (IPTS) for\ndetecting malicious inputs. Accordingly, we first used a diverse set of attack\nmethods including adaptive attack methods (on our defense) to generate\nadversarial samples from the clean dataset. A detection framework based on a\ngenetic algorithm (GA) is developed to find the optimal IPTS, where the\noptimality is estimated by different fitness measures such as Euclidean\ndistance, entropy loss, average histogram, local binary pattern and loss\nfunctions. The \"image difference\" between the original and processed images is\nused to extract the features, which are then fed to a classification scheme in\norder to determine whether the input sample is adversarial or clean. This paper\ndescribed our methodology and performed experiments using multiple data-sets\ntested with several adversarial attacks. For each attack-type and dataset, it\ngenerates unique IPTS. A set of IPTS selected dynamically in testing time which\nworks as a filter for the adversarial attack. Our empirical experiments\nexhibited promising results indicating the approach can efficiently be used as\nprocessing for any AI model.",
          "link": "http://arxiv.org/abs/2007.00337",
          "publishedOn": "2021-07-21T02:01:35.112Z",
          "wordCount": 675,
          "title": "Determining Sequence of Image Processing Technique (IPT) to Detect Adversarial Attacks. (arXiv:2007.00337v2 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Wenxian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuxuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Deep neural networks often have a huge number of parameters, which posts\nchallenges in deployment in application scenarios with limited memory and\ncomputation capacity. Knowledge distillation is one approach to derive compact\nmodels from bigger ones. However, it has been observed that a converged heavy\nteacher model is strongly constrained for learning a compact student network\nand could make the optimization subject to poor local optima. In this paper, we\npropose ProKT, a new model-agnostic method by projecting the supervision\nsignals of a teacher model into the student's parameter space. Such projection\nis implemented by decomposing the training objective into local intermediate\ntargets with an approximate mirror descent technique. The proposed method could\nbe less sensitive with the quirks during optimization which could result in a\nbetter local optimum. Experiments on both image and text datasets show that our\nproposed ProKT consistently achieves superior performance compared to other\nexisting knowledge distillation methods.",
          "link": "http://arxiv.org/abs/2107.09305",
          "publishedOn": "2021-07-21T02:01:35.104Z",
          "wordCount": 597,
          "title": "Follow Your Path: a Progressive Method for Knowledge Distillation. (arXiv:2107.09305v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.13677",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Wenqi Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ke_Z/0/1/0/all/0/1\">Ziwen Ke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_Z/0/1/0/all/0/1\">Zhuo-Xu Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1\">Jing Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhilang Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1\">Sen Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ying_L/0/1/0/all/0/1\">Leslie Ying</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanjie Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1\">Dong Liang</a>",
          "description": "In dynamic magnetic resonance (MR) imaging, low-rank plus sparse (L+S)\ndecomposition, or robust principal component analysis (PCA), has achieved\nstunning performance. However, the selection of the parameters of L+S is\nempirical, and the acceleration rate is limited, which are common failings of\niterative compressed sensing MR imaging (CS-MRI) reconstruction methods. Many\ndeep learning approaches have been proposed to address these issues, but few of\nthem use a low-rank prior. In this paper, a model-based low-rank plus sparse\nnetwork, dubbed L+S-Net, is proposed for dynamic MR reconstruction. In\nparticular, we use an alternating linearized minimization method to solve the\noptimization problem with low-rank and sparse regularization. Learned soft\nsingular value thresholding is introduced to ensure the clear separation of the\nL component and S component. Then, the iterative steps are unrolled into a\nnetwork in which the regularization parameters are learnable. We prove that the\nproposed L+S-Net achieves global convergence under two standard assumptions.\nExperiments on retrospective and prospective cardiac cine datasets show that\nthe proposed model outperforms state-of-the-art CS and existing deep learning\nmethods and has great potential for extremely high acceleration factors (up to\n24x).",
          "link": "http://arxiv.org/abs/2010.13677",
          "publishedOn": "2021-07-21T02:01:35.085Z",
          "wordCount": 677,
          "title": "Deep Low-rank plus Sparse Network for Dynamic MR Imaging. (arXiv:2010.13677v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milbich_T/0/1/0/all/0/1\">Timo Milbich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1\">Karsten Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Samarth Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1\">Marzyeh Ghassemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>",
          "description": "Deep Metric Learning (DML) aims to find representations suitable for\nzero-shot transfer to a priori unknown test distributions. However, common\nevaluation protocols only test a single, fixed data split in which train and\ntest classes are assigned randomly. More realistic evaluations should consider\na broad spectrum of distribution shifts with potentially varying degree and\ndifficulty. In this work, we systematically construct train-test splits of\nincreasing difficulty and present the ooDML benchmark to characterize\ngeneralization under out-of-distribution shifts in DML. ooDML is designed to\nprobe the generalization performance on much more challenging, diverse\ntrain-to-test distribution shifts. Based on our new benchmark, we conduct a\nthorough empirical analysis of state-of-the-art DML methods. We find that while\ngeneralization tends to consistently degrade with difficulty, some methods are\nbetter at retaining performance as the distribution shift increases. Finally,\nwe propose few-shot DML as an efficient way to consistently improve\ngeneralization in response to unknown test shifts presented in ooDML. Code\navailable here:\nhttps://github.com/Confusezius/Characterizing_Generalization_in_DeepMetricLearning.",
          "link": "http://arxiv.org/abs/2107.09562",
          "publishedOn": "2021-07-21T02:01:35.078Z",
          "wordCount": 604,
          "title": "Characterizing Generalization under Out-Of-Distribution Shifts in Deep Metric Learning. (arXiv:2107.09562v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xizhou Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>",
          "description": "As a fundamental problem for Artificial Intelligence, multi-agent system\n(MAS) is making rapid progress, mainly driven by multi-agent reinforcement\nlearning (MARL) techniques. However, previous MARL methods largely focused on\ngrid-world like or game environments; MAS in visually rich environments has\nremained less explored. To narrow this gap and emphasize the crucial role of\nperception in MAS, we propose a large-scale 3D dataset, CollaVN, for\nmulti-agent visual navigation (MAVN). In CollaVN, multiple agents are entailed\nto cooperatively navigate across photo-realistic environments to reach target\nlocations. Diverse MAVN variants are explored to make our problem more general.\nMoreover, a memory-augmented communication framework is proposed. Each agent is\nequipped with a private, external memory to persistently store communication\ninformation. This allows agents to make better use of their past communication\ninformation, enabling more efficient collaboration and robust long-term\nplanning. In our experiments, several baselines and evaluation metrics are\ndesigned. We also empirically verify the efficacy of our proposed MARL approach\nacross different MAVN task settings.",
          "link": "http://arxiv.org/abs/2107.01151",
          "publishedOn": "2021-07-21T02:01:35.071Z",
          "wordCount": 613,
          "title": "Collaborative Visual Navigation. (arXiv:2107.01151v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bonnaerens_M/0/1/0/all/0/1\">Maxim Bonnaerens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freiberger_M/0/1/0/all/0/1\">Matthias Freiberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dambre_J/0/1/0/all/0/1\">Joni Dambre</a>",
          "description": "This paper proposes anchor pruning for object detection in one-stage\nanchor-based detectors. While pruning techniques are widely used to reduce the\ncomputational cost of convolutional neural networks, they tend to focus on\noptimizing the backbone networks where often most computations are. In this\nwork we demonstrate an additional pruning technique, specifically for object\ndetection: anchor pruning. With more efficient backbone networks and a growing\ntrend of deploying object detectors on embedded systems where post-processing\nsteps such as non-maximum suppression can be a bottleneck, the impact of the\nanchors used in the detection head is becoming increasingly more important. In\nthis work, we show that many anchors in the object detection head can be\nremoved without any loss in accuracy. With additional retraining, anchor\npruning can even lead to improved accuracy. Extensive experiments on SSD and MS\nCOCO show that the detection head can be made up to 44% more efficient while\nsimultaneously increasing accuracy. Further experiments on RetinaNet and PASCAL\nVOC show the general effectiveness of our approach. We also introduce\n`overanchorized' models that can be used together with anchor pruning to\neliminate hyperparameters related to the initial shape of anchors.",
          "link": "http://arxiv.org/abs/2104.00432",
          "publishedOn": "2021-07-21T02:01:35.063Z",
          "wordCount": 645,
          "title": "Anchor Pruning for Object Detection. (arXiv:2104.00432v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zihao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1\">Zimu Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ruizhen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>",
          "description": "Rigid registration of partial observations is a fundamental problem in\nvarious applied fields. In computer graphics, special attention has been given\nto the registration between two partial point clouds generated by scanning\ndevices. State-of-the-art registration techniques still struggle when the\noverlap region between the two point clouds is small, and completely fail if\nthere is no overlap between the scan pairs. In this paper, we present a\nlearning-based technique that alleviates this problem, and allows registration\nbetween point clouds, presented in arbitrary poses, and having little or even\nno overlap, a setting that has been referred to as tele-registration. Our\ntechnique is based on a novel neural network design that learns a prior of a\nclass of shapes and can complete a partial shape. The key idea is combining the\nregistration and completion tasks in a way that reinforces each other. In\nparticular, we simultaneously train the registration network and completion\nnetwork using two coupled flows, one that register-and-complete, and one that\ncomplete-and-register, and encourage the two flows to produce a consistent\nresult. We show that, compared with each separate flow, this two-flow training\nleads to robust and reliable tele-registration, and hence to a better point\ncloud prediction that completes the registered scans. It is also worth\nmentioning that each of the components in our neural network outperforms\nstate-of-the-art methods in both completion and registration. We further\nanalyze our network with several ablation studies and demonstrate its\nperformance on a large number of partial point clouds, both synthetic and\nreal-world, that have only small or no overlap.",
          "link": "http://arxiv.org/abs/2106.00329",
          "publishedOn": "2021-07-21T02:01:35.054Z",
          "wordCount": 736,
          "title": "Consistent Two-Flow Network for Tele-Registration of Point Clouds. (arXiv:2106.00329v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiling Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1\">M. Salman Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhan Ma</a>",
          "description": "Distortion quantification of point clouds plays a stealth, yet vital role in\na wide range of human and machine perception tasks. For human perception tasks,\na distortion quantification can substitute subjective experiments to guide 3D\nvisualization; while for machine perception tasks, a distortion quantification\ncan work as a loss function to guide the training of deep neural networks for\nunsupervised learning tasks. To handle a variety of demands in many\napplications, a distortion quantification needs to be distortion discriminable,\ndifferentiable, and have a low computational complexity. Currently, however,\nthere is a lack of a general distortion quantification that can satisfy all\nthree conditions. To fill this gap, this work proposes multiscale potential\nenergy discrepancy (MPED), a distortion quantification to measure point cloud\ngeometry and color difference. By evaluating at various neighborhood sizes, the\nproposed MPED achieves global-local tradeoffs, capturing distortion in a\nmultiscale fashion. Extensive experimental studies validate MPED's superiority\nfor both human and machine perception tasks.",
          "link": "http://arxiv.org/abs/2103.02850",
          "publishedOn": "2021-07-21T02:01:35.036Z",
          "wordCount": 647,
          "title": "Point Cloud Distortion Quantification based on Potential Energy for Human and Machine Perception. (arXiv:2103.02850v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Li Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lefei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Unsupervised domain adaptation (UDA) for semantic segmentation aims to adapt\na segmentation model trained on the labeled source domain to the unlabeled\ntarget domain. Existing methods try to learn domain invariant features while\nsuffering from large domain gaps that make it difficult to correctly align\ndiscrepant features, especially in the initial training phase. To address this\nissue, we propose a novel Dual Soft-Paste (DSP) method in this paper.\nSpecifically, DSP selects some classes from a source domain image using a\nlong-tail class first sampling strategy and softly pastes the corresponding\nimage patch on both the source and target training images with a fusion weight.\nTechnically, we adopt the mean teacher framework for domain adaptation, where\nthe pasted source and target images go through the student network while the\noriginal target image goes through the teacher network. Output-level alignment\nis carried out by aligning the probability maps of the target fused image from\nboth networks using a weighted cross-entropy loss. In addition, feature-level\nalignment is carried out by aligning the feature maps of the source and target\nimages from student network using a weighted maximum mean discrepancy loss. DSP\nfacilitates the model learning domain-invariant features from the intermediate\ndomains, leading to faster convergence and better performance. Experiments on\ntwo challenging benchmarks demonstrate the superiority of DSP over\nstate-of-the-art methods. Code is available at\n\\url{https://github.com/GaoLii/DSP}.",
          "link": "http://arxiv.org/abs/2107.09600",
          "publishedOn": "2021-07-21T02:01:35.029Z",
          "wordCount": 674,
          "title": "DSP: Dual Soft-Paste for Unsupervised Domain Adaptive Semantic Segmentation. (arXiv:2107.09600v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+VS_V/0/1/0/all/0/1\">Vibashan VS</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oza_P/0/1/0/all/0/1\">Poojan Oza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "In image fusion, images obtained from different sensors are fused to generate\na single image with enhanced information. In recent years, state-of-the-art\nmethods have adopted Convolution Neural Networks (CNNs) to encode meaningful\nfeatures for image fusion. Specifically, CNN-based methods perform image fusion\nby fusing local features. However, they do not consider long-range dependencies\nthat are present in the image. Transformer-based models are designed to\novercome this by modeling the long-range dependencies with the help of\nself-attention mechanism. This motivates us to propose a novel Image Fusion\nTransformer (IFT) where we develop a transformer-based multi-scale fusion\nstrategy that attends to both local and long-range information (or global\ncontext). The proposed method follows a two-stage training approach. In the\nfirst stage, we train an auto-encoder to extract deep features at multiple\nscales. In the second stage, multi-scale features are fused using a\nSpatio-Transformer (ST) fusion strategy. The ST fusion blocks are comprised of\na CNN and a transformer branch which capture local and long-range features,\nrespectively. Extensive experiments on multiple benchmark datasets show that\nthe proposed method performs better than many competitive fusion algorithms.\nFurthermore, we show the effectiveness of the proposed ST fusion strategy with\nan ablation analysis. The source code is available at:\nhttps://github.com/Vibashan/Image-Fusion-Transformer.",
          "link": "http://arxiv.org/abs/2107.09011",
          "publishedOn": "2021-07-21T02:01:35.018Z",
          "wordCount": 653,
          "title": "Image Fusion Transformer. (arXiv:2107.09011v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09602",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bharati_S/0/1/0/all/0/1\">Subrato Bharati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Podder_P/0/1/0/all/0/1\">Prajoy Podder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mondal_M/0/1/0/all/0/1\">M. Rubaiyat Hossain Mondal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasath_V/0/1/0/all/0/1\">V.B. Surya Prasath</a>",
          "description": "The outbreak of novel coronavirus disease (COVID- 19) has claimed millions of\nlives and has affected all aspects of human life. This paper focuses on the\napplication of deep learning (DL) models to medical imaging and drug discovery\nfor managing COVID-19 disease. In this article, we detail various medical\nimaging-based studies such as X-rays and computed tomography (CT) images along\nwith DL methods for classifying COVID-19 affected versus pneumonia. The\napplications of DL techniques to medical images are further described in terms\nof image localization, segmentation, registration, and classification leading\nto COVID-19 detection. The reviews of recent papers indicate that the highest\nclassification accuracy of 99.80% is obtained when InstaCovNet-19 DL method is\napplied to an X-ray dataset of 361 COVID-19 patients, 362 pneumonia patients\nand 365 normal people. Furthermore, it can be seen that the best classification\naccuracy of 99.054% can be achieved when EDL_COVID DL method is applied to a CT\nimage dataset of 7500 samples where COVID-19 patients, lung tumor patients and\nnormal people are equal in number. Moreover, we illustrate the potential DL\ntechniques in drug or vaccine discovery in combating the coronavirus. Finally,\nwe address a number of problems, concerns and future research directions\nrelevant to DL applications for COVID-19.",
          "link": "http://arxiv.org/abs/2107.09602",
          "publishedOn": "2021-07-21T02:01:35.002Z",
          "wordCount": 729,
          "title": "Medical Imaging with Deep Learning for COVID- 19 Diagnosis: A Comprehensive Review. (arXiv:2107.09602v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaodong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1\">Junbao Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuhao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>",
          "description": "Semi-supervised domain adaptation (SSDA) aims to solve tasks in target domain\nby utilizing transferable information learned from the available source domain\nand a few labeled target data. However, source data is not always accessible in\npractical scenarios, which restricts the application of SSDA in real world\ncircumstances. In this paper, we propose a novel task named Semi-supervised\nSource Hypothesis Transfer (SSHT), which performs domain adaptation based on\nsource trained model, to generalize well in target domain with a few\nsupervisions. In SSHT, we are facing two challenges: (1) The insufficient\nlabeled target data may result in target features near the decision boundary,\nwith the increased risk of mis-classification; (2) The data are usually\nimbalanced in source domain, so the model trained with these data is biased.\nThe biased model is prone to categorize samples of minority categories into\nmajority ones, resulting in low prediction diversity. To tackle the above\nissues, we propose Consistency and Diversity Learning (CDL), a simple but\neffective framework for SSHT by facilitating prediction consistency between two\nrandomly augmented unlabeled data and maintaining the prediction diversity when\nadapting model to target domain. Encouraging consistency regularization brings\ndifficulty to memorize the few labeled target data and thus enhances the\ngeneralization ability of the learned model. We further integrate Batch\nNuclear-norm Maximization into our method to enhance the discriminability and\ndiversity. Experimental results show that our method outperforms existing SSDA\nmethods and unsupervised model adaptation methods on DomainNet, Office-Home and\nOffice-31 datasets. The code is available at\nhttps://github.com/Wang-xd1899/SSHT.",
          "link": "http://arxiv.org/abs/2107.03008",
          "publishedOn": "2021-07-21T02:01:34.994Z",
          "wordCount": 717,
          "title": "Learning Invariant Representation with Consistency and Diversity for Semi-supervised Source Hypothesis Transfer. (arXiv:2107.03008v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.13201",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Raju_A/0/1/0/all/0/1\">Ashwin Raju</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_C/0/1/0/all/0/1\">Chi-Tung Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1\">Yunakai Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_J/0/1/0/all/0/1\">Jinzheng Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1\">Le Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_C/0/1/0/all/0/1\">ChienHuang Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harrison_A/0/1/0/all/0/1\">Adam P Harrison</a>",
          "description": "In medical imaging, organ/pathology segmentation models trained on current\npublicly available and fully-annotated datasets usually do not well-represent\nthe heterogeneous modalities, phases, pathologies, and clinical scenarios\nencountered in real environments. On the other hand, there are tremendous\namounts of unlabelled patient imaging scans stored by many modern clinical\ncenters. In this work, we present a novel segmentation strategy,\nco-heterogenous and adaptive segmentation (CHASe), which only requires a small\nlabeled cohort of single phase imaging data to adapt to any unlabeled cohort of\nheterogenous multi-phase data with possibly new clinical scenarios and\npathologies. To do this, we propose a versatile framework that fuses appearance\nbased semi-supervision, mask based adversarial domain adaptation, and\npseudo-labeling. We also introduce co-heterogeneous training, which is a novel\nintegration of co-training and hetero modality learning. We have evaluated\nCHASe using a clinically comprehensive and challenging dataset of multi-phase\ncomputed tomography (CT) imaging studies (1147 patients and 4577 3D volumes).\nCompared to previous state-of-the-art baselines, CHASe can further improve\npathological liver mask Dice-Sorensen coefficients by ranges of $4.2\\% \\sim\n9.4\\%$, depending on the phase combinations: e.g., from $84.6\\%$ to $94.0\\%$ on\nnon-contrast CTs.",
          "link": "http://arxiv.org/abs/2005.13201",
          "publishedOn": "2021-07-21T02:01:34.976Z",
          "wordCount": 703,
          "title": "Co-Heterogeneous and Adaptive Segmentation from Multi-Source and Multi-Phase CT Imaging Data: A Study on Pathological Liver and Lesion Segmentation. (arXiv:2005.13201v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Mingkai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>",
          "description": "Self-supervised Learning (SSL) including the mainstream contrastive learning\nhas achieved great success in learning visual representations without data\nannotations. However, most of methods mainly focus on the instance level\ninformation (\\ie, the different augmented images of the same instance should\nhave the same feature or cluster into the same class), but there is a lack of\nattention on the relationships between different instances. In this paper, we\nintroduced a novel SSL paradigm, which we term as relational self-supervised\nlearning (ReSSL) framework that learns representations by modeling the\nrelationship between different instances. Specifically, our proposed method\nemploys sharpened distribution of pairwise similarities among different\ninstances as \\textit{relation} metric, which is thus utilized to match the\nfeature embeddings of different augmentations. Moreover, to boost the\nperformance, we argue that weak augmentations matter to represent a more\nreliable relation, and leverage momentum strategy for practical efficiency.\nExperimental results show that our proposed ReSSL significantly outperforms the\nprevious state-of-the-art algorithms in terms of both performance and training\nefficiency. Code is available at \\url{https://github.com/KyleZheng1997/ReSSL}.",
          "link": "http://arxiv.org/abs/2107.09282",
          "publishedOn": "2021-07-21T02:01:34.957Z",
          "wordCount": 615,
          "title": "ReSSL: Relational Self-Supervised Learning with Weak Augmentation. (arXiv:2107.09282v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.05675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinhai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lin Chen</a>",
          "description": "Few-shot learning aims at rapidly adapting to novel categories with only a\nhandful of samples at test time, which has been predominantly tackled with the\nidea of meta-learning. However, meta-learning approaches essentially learn\nacross a variety of few-shot tasks and thus still require large-scale training\ndata with fine-grained supervision to derive a generalized model, thereby\ninvolving prohibitive annotation cost. In this paper, we advance the few-shot\nclassification paradigm towards a more challenging scenario, i.e.,\ncross-granularity few-shot classification, where the model observes only coarse\nlabels during training while is expected to perform fine-grained classification\nduring testing. This task largely relieves the annotation cost since\nfine-grained labeling usually requires strong domain-specific expertise. To\nbridge the cross-granularity gap, we approximate the fine-grained data\ndistribution by greedy clustering of each coarse-class into pseudo-fine-classes\naccording to the similarity of image embeddings. We then propose a\nmeta-embedder that jointly optimizes the visual- and semantic-discrimination,\nin both instance-wise and coarse class-wise, to obtain a good feature space for\nthis coarse-to-fine pseudo-labeling process. Extensive experiments and ablation\nstudies are conducted to demonstrate the effectiveness and robustness of our\napproach on three representative datasets.",
          "link": "http://arxiv.org/abs/2007.05675",
          "publishedOn": "2021-07-21T02:01:34.947Z",
          "wordCount": 674,
          "title": "Towards Cross-Granularity Few-Shot Learning: Coarse-to-Fine Pseudo-Labeling with Visual-Semantic Meta-Embedding. (arXiv:2007.05675v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Figueroa_Flores_C/0/1/0/all/0/1\">Carola Figueroa-Flores</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berga_D/0/1/0/all/0/1\">David Berga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van der Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raducanu_B/0/1/0/all/0/1\">Bogdan Raducanu</a>",
          "description": "Saliency is the perceptual capacity of our visual system to focus our\nattention (i.e. gaze) on relevant objects. Neural networks for saliency\nestimation require ground truth saliency maps for training which are usually\nachieved via eyetracking experiments. In the current paper, we demonstrate that\nsaliency maps can be generated as a side-effect of training an object\nrecognition deep neural network that is endowed with a saliency branch. Such a\nnetwork does not require any ground-truth saliency maps for training.Extensive\nexperiments carried out on both real and synthetic saliency datasets\ndemonstrate that our approach is able to generate accurate saliency maps,\nachieving competitive results on both synthetic and real datasets when compared\nto methods that do require ground truth data.",
          "link": "http://arxiv.org/abs/2107.09628",
          "publishedOn": "2021-07-21T02:01:34.925Z",
          "wordCount": 575,
          "title": "Saliency for free: Saliency prediction as a side-effect of object recognition. (arXiv:2107.09628v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianshu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jun Du</a>",
          "description": "Table structure recognition is an essential part for making machines\nunderstand tables. Its main task is to recognize the internal structure of a\ntable. However, due to the complexity and diversity in their structure and\nstyle, it is very difficult to parse the tabular data into the structured\nformat which machines can understand easily, especially for complex tables. In\nthis paper, we introduce Split, Embed and Merge (SEM), an accurate table\nstructure recognizer. Our model takes table images as input and can correctly\nrecognize the structure of tables, whether they are simple or a complex tables.\nSEM is mainly composed of three parts, splitter, embedder and merger. In the\nfirst stage, we apply the splitter to predict the potential regions of the\ntable row (column) separators, and obtain the fine grid structure of the table.\nIn the second stage, by taking a full consideration of the textual information\nin the table, we fuse the output features for each table grid from both vision\nand language modalities. Moreover, we achieve a higher precision in our\nexperiments through adding additional semantic features. Finally, we process\nthe merging of these basic table grids in a self-regression manner. The\ncorrespondent merging results is learned through the attention mechanism. In\nour experiments, SEM achieves an average F1-Measure of 97.11% on the SciTSR\ndataset which outperforms other methods by a large margin. We also won the\nfirst place in the complex table and third place in all tables in ICDAR 2021\nCompetition on Scientific Literature Parsing, Task-B. Extensive experiments on\nother publicly available datasets demonstrate that our model achieves\nstate-of-the-art.",
          "link": "http://arxiv.org/abs/2107.05214",
          "publishedOn": "2021-07-21T02:01:34.907Z",
          "wordCount": 716,
          "title": "Split, embed and merge: An accurate table structure recognizer. (arXiv:2107.05214v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.03876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuecong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haozhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1\">Kezhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianxiong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1\">Simon See</a>",
          "description": "The task of action recognition in dark videos is useful in various scenarios,\ne.g., night surveillance and self-driving at night. Though progress has been\nmade in the action recognition task for videos in normal illumination, few have\nstudied action recognition in the dark. This is partly due to the lack of\nsufficient datasets for such a task. In this paper, we explored the task of\naction recognition in dark videos. We bridge the gap of the lack of data for\nthis task by collecting a new dataset: the Action Recognition in the Dark\n(ARID) dataset. It consists of over 3,780 video clips with 11 action\ncategories. To the best of our knowledge, it is the first dataset focused on\nhuman actions in dark videos. To gain further understandings of our ARID\ndataset, we analyze the ARID dataset in detail and exhibited its necessity over\nsynthetic dark videos. Additionally, we benchmarked the performance of several\ncurrent action recognition models on our dataset and explored potential methods\nfor increasing their performances. Our results show that current action\nrecognition models and frame enhancement methods may not be effective solutions\nfor the task of action recognition in dark videos.",
          "link": "http://arxiv.org/abs/2006.03876",
          "publishedOn": "2021-07-21T02:01:34.849Z",
          "wordCount": 697,
          "title": "ARID: A Comprehensive Study on Recognizing Actions in the Dark and A New Benchmark Dataset. (arXiv:2006.03876v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1\">Bharadwaj Manda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhayarkar_S/0/1/0/all/0/1\">Shubham Dhayarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viekash_V/0/1/0/all/0/1\">V.K. Viekash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1\">Ramanathan Muthuganapathy</a>",
          "description": "Ongoing advancements in the fields of 3D modelling and digital archiving have\nled to an outburst in the amount of data stored digitally. Consequently,\nseveral retrieval systems have been developed depending on the type of data\nstored in these databases. However, unlike text data or images, performing a\nsearch for 3D models is non-trivial. Among 3D models, retrieving 3D\nEngineering/CAD models or mechanical components is even more challenging due to\nthe presence of holes, volumetric features, presence of sharp edges etc., which\nmake CAD a domain unto itself. The research work presented in this paper aims\nat developing a dataset suitable for building a retrieval system for 3D CAD\nmodels based on deep learning. 3D CAD models from the available CAD databases\nare collected, and a dataset of computer-generated sketch data, termed\n'CADSketchNet', has been prepared. Additionally, hand-drawn sketches of the\ncomponents are also added to CADSketchNet. Using the sketch images from this\ndataset, the paper also aims at evaluating the performance of various retrieval\nsystem or a search engine for 3D CAD models that accepts a sketch image as the\ninput query. Many experimental models are constructed and tested on\nCADSketchNet. These experiments, along with the model architecture, choice of\nsimilarity metrics are reported along with the search results.",
          "link": "http://arxiv.org/abs/2107.06212",
          "publishedOn": "2021-07-21T02:01:34.842Z",
          "wordCount": 713,
          "title": "'CADSketchNet' -- An Annotated Sketch dataset for 3D CAD Model Retrieval with Deep Neural Networks. (arXiv:2107.06212v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09442",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bortsova_G/0/1/0/all/0/1\">Gerda Bortsova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bos_D/0/1/0/all/0/1\">Daniel Bos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dubost_F/0/1/0/all/0/1\">Florian Dubost</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vernooij_M/0/1/0/all/0/1\">Meike W. Vernooij</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ikram_M/0/1/0/all/0/1\">M. Kamran Ikram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tulder_G/0/1/0/all/0/1\">Gijs van Tulder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bruijne_M/0/1/0/all/0/1\">Marleen de Bruijne</a>",
          "description": "Purpose: To evaluate a fully-automated deep-learning-based method for\nassessment of intracranial carotid artery calcification (ICAC). Methods: Two\nobservers manually delineated ICAC in non-contrast CT scans of 2,319\nparticipants (mean age 69 (SD 7) years; 1154 women) of the Rotterdam Study,\nprospectively collected between 2003 and 2006. These data were used to\nretrospectively develop and validate a deep-learning-based method for automated\nICAC delineation and volume measurement. To evaluate the method, we compared\nmanual and automatic assessment (computed using ten-fold cross-validation) with\nrespect to 1) the agreement with an independent observer's assessment\n(available in a random subset of 47 scans); 2) the accuracy in delineating ICAC\nas judged via blinded visual comparison by an expert; 3) the association with\nfirst stroke incidence from the scan date until 2012. All method performance\nmetrics were computed using 10-fold cross-validation. Results: The automated\ndelineation of ICAC reached sensitivity of 83.8% and positive predictive value\n(PPV) of 88%. The intraclass correlation between automatic and manual ICAC\nvolume measures was 0.98 (95% CI: 0.97, 0.98; computed in the entire dataset).\nMeasured between the assessments of independent observers, sensitivity was\n73.9%, PPV was 89.5%, and intraclass correlation was 0.91 (95% CI: 0.84, 0.95;\ncomputed in the 47-scan subset). In the blinded visual comparisons, automatic\ndelineations were more accurate than manual ones (p-value = 0.01). The\nassociation of ICAC volume with incident stroke was similarly strong for both\nautomated (hazard ratio, 1.38 (95% CI: 1.12, 1.75) and manually measured\nvolumes (hazard ratio, 1.48 (95% CI: 1.20, 1.87)). Conclusions: The developed\nmodel was capable of automated segmentation and volume quantification of ICAC\nwith accuracy comparable to human experts.",
          "link": "http://arxiv.org/abs/2107.09442",
          "publishedOn": "2021-07-21T02:01:34.809Z",
          "wordCount": 756,
          "title": "Automated Segmentation and Volume Measurement of Intracranial Carotid Artery Calcification on Non-Contrast CT. (arXiv:2107.09442v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09289",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1\">Kazuma Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daiki_S/0/1/0/all/0/1\">Suehiro Daiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazuya_N/0/1/0/all/0/1\">Nishimura Kazuya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoma_B/0/1/0/all/0/1\">Bise Ryoma</a>",
          "description": "Cell detection is an essential task in cell image analysis. Recent deep\nlearning-based detection methods have achieved very promising results. In\ngeneral, these methods require exhaustively annotating the cells in an entire\nimage. If some of the cells are not annotated (imperfect annotation), the\ndetection performance significantly degrades due to noisy labels. This often\noccurs in real collaborations with biologists and even in public data-sets. Our\nproposed method takes a pseudo labeling approach for cell detection from\nimperfect annotated data. A detection convolutional neural network (CNN)\ntrained using such missing labeled data often produces over-detection. We treat\npartially labeled cells as positive samples and the detected positions except\nfor the labeled cell as unlabeled samples. Then we select reliable pseudo\nlabels from unlabeled data using recent machine learning techniques;\npositive-and-unlabeled (PU) learning and P-classification. Experiments using\nmicroscopy images for five different conditions demonstrate the effectiveness\nof the proposed method.",
          "link": "http://arxiv.org/abs/2107.09289",
          "publishedOn": "2021-07-21T02:01:34.802Z",
          "wordCount": 599,
          "title": "Cell Detection from Imperfect Annotation by Pseudo Label Selection Using P-classification. (arXiv:2107.09289v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09126",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Spetter_Goldstein_B/0/1/0/all/0/1\">Benjamin Spetter-Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_N/0/1/0/all/0/1\">Nataniel Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bargal_S/0/1/0/all/0/1\">Sarah Adel Bargal</a>",
          "description": "The modern open internet contains billions of public images of human faces\nacross the web, especially on social media websites used by half the world's\npopulation. In this context, Face Recognition (FR) systems have the potential\nto match faces to specific names and identities, creating glaring privacy\nconcerns. Adversarial attacks are a promising way to grant users privacy from\nFR systems by disrupting their capability to recognize faces. Yet, such attacks\ncan be perceptible to human observers, especially under the more challenging\nblack-box threat model. In the literature, the justification for the\nimperceptibility of such attacks hinges on bounding metrics such as $\\ell_p$\nnorms. However, there is not much research on how these norms match up with\nhuman perception. Through examining and measuring both the effectiveness of\nrecent black-box attacks in the face recognition setting and their\ncorresponding human perceptibility through survey data, we demonstrate the\ntrade-offs in perceptibility that occur as attacks become more aggressive. We\nalso show how the $\\ell_2$ norm and other metrics do not correlate with human\nperceptibility in a linear fashion, thus making these norms suboptimal at\nmeasuring adversarial attack perceptibility.",
          "link": "http://arxiv.org/abs/2107.09126",
          "publishedOn": "2021-07-21T02:01:34.606Z",
          "wordCount": 638,
          "title": "Examining the Human Perceptibility of Black-Box Adversarial Attacks on Face Recognition. (arXiv:2107.09126v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shaohao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1\">Yuqiao Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Ke Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaowei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-Shi Zheng</a>",
          "description": "The Deep Neural Networks are vulnerable toadversarial exam-ples(Figure 1),\nmaking the DNNs-based systems collapsed byadding the inconspicuous\nperturbations to the images. Most of the existing works for adversarial attack\nare gradient-based and suf-fer from the latency efficiencies and the load on\nGPU memory. Thegenerative-based adversarial attacks can get rid of this\nlimitation,and some relative works propose the approaches based on GAN.However,\nsuffering from the difficulty of the convergence of train-ing a GAN, the\nadversarial examples have either bad attack abilityor bad visual quality. In\nthis work, we find that the discriminatorcould be not necessary for\ngenerative-based adversarial attack, andpropose theSymmetric Saliency-based\nAuto-Encoder (SSAE)to generate the perturbations, which is composed of the\nsaliencymap module and the angle-norm disentanglement of the featuresmodule.\nThe advantage of our proposed method lies in that it is notdepending on\ndiscriminator, and uses the generative saliency map to pay more attention to\nlabel-relevant regions. The extensive exper-iments among the various tasks,\ndatasets, and models demonstratethat the adversarial examples generated by SSAE\nnot only make thewidely-used models collapse, but also achieves good visual\nquality.The code is available at https://github.com/BravoLu/SSAE.",
          "link": "http://arxiv.org/abs/2107.09225",
          "publishedOn": "2021-07-21T02:01:34.599Z",
          "wordCount": 627,
          "title": "Discriminator-Free Generative Adversarial Attack. (arXiv:2107.09225v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Sanchita Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prevost_J/0/1/0/all/0/1\">John J. Prevost</a>",
          "description": "Deep learning based visual to sound generation systems essentially need to be\ndeveloped particularly considering the synchronicity aspects of visual and\naudio features with time. In this research we introduce a novel task of guiding\na class conditioned generative adversarial network with the temporal visual\ninformation of a video input for visual to sound generation task adapting the\nsynchronicity traits between audio-visual modalities. Our proposed FoleyGAN\nmodel is capable of conditioning action sequences of visual events leading\ntowards generating visually aligned realistic sound tracks. We expand our\npreviously proposed Automatic Foley dataset to train with FoleyGAN and evaluate\nour synthesized sound through human survey that shows noteworthy (on average\n81\\%) audio-visual synchronicity performance. Our approach also outperforms in\nstatistical experiments compared with other baseline models and audio-visual\ndatasets.",
          "link": "http://arxiv.org/abs/2107.09262",
          "publishedOn": "2021-07-21T02:01:34.592Z",
          "wordCount": 610,
          "title": "FoleyGAN: Visually Guided Generative Adversarial Network-Based Synchronous Sound Generation in Silent Videos. (arXiv:2107.09262v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yaqub_W/0/1/0/all/0/1\">Waheeb Yaqub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_M/0/1/0/all/0/1\">Manoranjan Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suleiman_B/0/1/0/all/0/1\">Basem Suleiman</a>",
          "description": "Online proctoring has become a necessity in online teaching. Video-based\ncrowd-sourced online proctoring solutions are being used, where an exam-taking\nstudent's video is monitored by third parties, leading to privacy concerns. In\nthis paper, we propose a privacy-preserving online proctoring system. The\nproposed image-hashing-based system can detect the student's excessive face and\nbody movement (i.e., anomalies) that is resulted when the student tries to\ncheat in the exam. The detection can be done even if the student's face is\nblurred or masked in video frames. Experiment with an in-house dataset shows\nthe usability of the proposed system.",
          "link": "http://arxiv.org/abs/2107.09373",
          "publishedOn": "2021-07-21T02:01:34.586Z",
          "wordCount": 535,
          "title": "Image-Hashing-Based Anomaly Detection for Privacy-Preserving Online Proctoring. (arXiv:2107.09373v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09372",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duy M. H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1\">Truong T. N. Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Than_N/0/1/0/all/0/1\">Ngoc T. T. Than</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prange_A/0/1/0/all/0/1\">Alexander Prange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1\">Daniel Sonntag</a>",
          "description": "This paper investigates the problem of domain adaptation for diabetic\nretinopathy (DR) grading. We learn invariant target-domain features by defining\na novel self-supervised task based on retinal vessel image reconstructions,\ninspired by medical domain knowledge. Then, a benchmark of current\nstate-of-the-art unsupervised domain adaptation methods on the DR problem is\nprovided. It can be shown that our approach outperforms existing domain\nadaption strategies. Furthermore, when utilizing entire training data in the\ntarget domain, we are able to compete with several state-of-the-art approaches\nin final classification accuracy just by applying standard network\narchitectures and using image-level labels.",
          "link": "http://arxiv.org/abs/2107.09372",
          "publishedOn": "2021-07-21T02:01:34.567Z",
          "wordCount": 547,
          "title": "Self-Supervised Domain Adaptation for Diabetic Retinopathy Grading using Vessel Image Reconstruction. (arXiv:2107.09372v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1805.01760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahpod_S/0/1/0/all/0/1\">Shahar Mahpod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rig Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiorana_E/0/1/0/all/0/1\">Emanuele Maiorana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1\">Yosi Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campisi_P/0/1/0/all/0/1\">Patrizio Campisi</a>",
          "description": "The accurate localization of facial landmarks is at the core of face analysis\ntasks, such as face recognition and facial expression analysis, to name a few.\nIn this work, we propose a novel localization approach based on a deep learning\narchitecture that utilizes cascaded subnetworks with convolutional neural\nnetwork units. The cascaded units of the first subnetwork estimate\nheatmap-based encodings of the landmarks locations, while the cascaded units of\nthe second subnetwork receive as input the output of the corresponding heatmap\nestimation units, and refine them through regression. The proposed scheme is\nexperimentally shown to compare favorably with contemporary state-of-the-art\nschemes, especially when applied to images depicting challenging localization\nconditions.",
          "link": "http://arxiv.org/abs/1805.01760",
          "publishedOn": "2021-07-21T02:01:34.559Z",
          "wordCount": 587,
          "title": "Facial Landmarks Localization using Cascaded Neural Networks. (arXiv:1805.01760v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.00184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianxin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yingce Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuqin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>",
          "description": "Image-to-image translation models have shown remarkable ability on\ntransferring images among different domains. Most of existing work follows the\nsetting that the source domain and target domain keep the same at training and\ninference phases, which cannot be generalized to the scenarios for translating\nan image from an unseen domain to another unseen domain. In this work, we\npropose the Unsupervised Zero-Shot Image-to-image Translation (UZSIT) problem,\nwhich aims to learn a model that can translate samples from image domains that\nare not observed during training. Accordingly, we propose a framework called\nZstGAN: By introducing an adversarial training scheme, ZstGAN learns to model\neach domain with domain-specific feature distribution that is semantically\nconsistent on vision and attribute modalities. Then the domain-invariant\nfeatures are disentangled with an shared encoder for image generation. We carry\nout extensive experiments on CUB and FLO datasets, and the results demonstrate\nthe effectiveness of proposed method on UZSIT task. Moreover, ZstGAN shows\nsignificant accuracy improvements over state-of-the-art zero-shot learning\nmethods on CUB and FLO.",
          "link": "http://arxiv.org/abs/1906.00184",
          "publishedOn": "2021-07-21T02:01:34.552Z",
          "wordCount": 641,
          "title": "ZstGAN: An Adversarial Approach for Unsupervised Zero-Shot Image-to-Image Translation. (arXiv:1906.00184v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahadev_R/0/1/0/all/0/1\">Rohan Mahadev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarti_A/0/1/0/all/0/1\">Anindya Chakravarti</a>",
          "description": "Large scale image classification models trained on top of popular datasets\nsuch as Imagenet have shown to have a distributional skew which leads to\ndisparities in prediction accuracies across different subsections of population\ndemographics. A lot of approaches have been made to solve for this\ndistributional skew using methods that alter the model pre, post and during\ntraining. We investigate one such approach - which uses a multi-label softmax\nloss with cross-entropy as the loss function instead of a binary cross-entropy\non a multi-label classification problem on the Inclusive Images dataset which\nis a subset of the OpenImages V6 dataset. We use the MR2 dataset, which\ncontains images of people with self-identified gender and race attributes to\nevaluate the fairness in the model outcomes and try to interpret the mistakes\nby looking at model activations and suggest possible fixes.",
          "link": "http://arxiv.org/abs/2107.09211",
          "publishedOn": "2021-07-21T02:01:34.545Z",
          "wordCount": 572,
          "title": "Understanding Gender and Racial Disparities in Image Recognition Models. (arXiv:2107.09211v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanzhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shaobo Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>",
          "description": "Electronic Line Calling is an auxiliary referee system used for tennis\nmatches based on binocular vision technology. While ELC has been widely used,\nthere are still many problems, such as complex installation and maintenance,\nhigh cost and etc. We propose a monocular vision technology based ELC method.\nThe method has the following steps. First, locate the tennis ball's trajectory.\nWe propose a multistage tennis ball positioning approach combining background\nsubtraction and color area filtering. Then we propose a bouncing point\nprediction method by minimizing the fitting loss of the uncertain point.\nFinally, we find out whether the bouncing point of the ball is out of bounds or\nnot according to the relative position between the bouncing point and the court\nside line in the two dimensional image. We collected and tagged 394 samples\nwith an accuracy rate of 99.4%, and 81.8% of the 11 samples with bouncing\npoints.The experimental results show that our method is feasible to judge if a\nball is out of the court with monocular vision and significantly reduce complex\ninstallation and costs of ELC system with binocular vision.",
          "link": "http://arxiv.org/abs/2107.09255",
          "publishedOn": "2021-07-21T02:01:34.539Z",
          "wordCount": 625,
          "title": "Monocular Visual Analysis for Electronic Line Calling of Tennis Games. (arXiv:2107.09255v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yim_M/0/1/0/all/0/1\">Moonbin Yim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoonsik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Han-Cheol Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungrae Park</a>",
          "description": "For successful scene text recognition (STR) models, synthetic text image\ngenerators have alleviated the lack of annotated text images from the real\nworld. Specifically, they generate multiple text images with diverse\nbackgrounds, font styles, and text shapes and enable STR models to learn visual\npatterns that might not be accessible from manually annotated data. In this\npaper, we introduce a new synthetic text image generator, SynthTIGER, by\nanalyzing techniques used for text image synthesis and integrating effective\nones under a single algorithm. Moreover, we propose two techniques that\nalleviate the long-tail problem in length and character distributions of\ntraining data. In our experiments, SynthTIGER achieves better STR performance\nthan the combination of synthetic datasets, MJSynth (MJ) and SynthText (ST).\nOur ablation study demonstrates the benefits of using sub-components of\nSynthTIGER and the guideline on generating synthetic text images for STR\nmodels. Our implementation is publicly available at\nhttps://github.com/clovaai/synthtiger.",
          "link": "http://arxiv.org/abs/2107.09313",
          "publishedOn": "2021-07-21T02:01:34.521Z",
          "wordCount": 599,
          "title": "SynthTIGER: Synthetic Text Image GEneratoR Towards Better Text Recognition Models. (arXiv:2107.09313v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lanqing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>",
          "description": "Existing long-tailed recognition methods, aiming to train class-balance\nmodels from long-tailed data, generally assume the models would be evaluated on\nthe uniform test class distribution. However, the practical test class\ndistribution often violates such an assumption (e.g., being long-tailed or even\ninversely long-tailed), which would lead existing methods to fail in real-world\napplications. In this work, we study a more practical task setting, called\ntest-agnostic long-tailed recognition, where the training class distribution is\nlong-tailed while the test class distribution is unknown and can be skewed\narbitrarily. In addition to the issue of class imbalance, this task poses\nanother challenge: the class distribution shift between the training and test\nsamples is unidentified. To address this task, we propose a new method, called\nTest-time Aggregating Diverse Experts (TADE), that presents two solution\nstrategies: (1) a novel skill-diverse expert learning strategy that trains\ndiverse experts to excel at handling different test distributions from a single\nlong-tailed training distribution; (2) a novel test-time expert aggregation\nstrategy that leverages self-supervision to aggregate multiple experts for\nhandling various test distributions. Moreover, we theoretically show that our\nmethod has provable ability to simulate unknown test class distributions.\nPromising results on both vanilla and test-agnostic long-tailed recognition\nverify the effectiveness of TADE. Code is available at\nhttps://github.com/Vanint/TADE-AgnosticLT.",
          "link": "http://arxiv.org/abs/2107.09249",
          "publishedOn": "2021-07-21T02:01:34.514Z",
          "wordCount": 651,
          "title": "Test-Agnostic Long-Tailed Recognition by Test-Time Aggregating Diverse Experts with Self-Supervision. (arXiv:2107.09249v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09405",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Schirris_Y/0/1/0/all/0/1\">Yoni Schirris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gavves_E/0/1/0/all/0/1\">Efstratios Gavves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nederlof_I/0/1/0/all/0/1\">Iris Nederlof</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Horlings_H/0/1/0/all/0/1\">Hugo Mark Horlings</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1\">Jonas Teuwen</a>",
          "description": "We propose a Deep learning-based weak label learning method for analysing\nwhole slide images (WSIs) of Hematoxylin and Eosin (H&E) stained tumorcells not\nrequiring pixel-level or tile-level annotations using Self-supervised\npre-training and heterogeneity-aware deep Multiple Instance LEarning\n(DeepSMILE). We apply DeepSMILE to the task of Homologous recombination\ndeficiency (HRD) and microsatellite instability (MSI) prediction. We utilize\ncontrastive self-supervised learning to pre-train a feature extractor on\nhistopathology tiles of cancer tissue. Additionally, we use variability-aware\ndeep multiple instance learning to learn the tile feature aggregation function\nwhile modeling tumor heterogeneity. Compared to state-of-the-art genomic label\nclassification methods, DeepSMILE improves classification performance for HRD\nfrom $70.43\\pm4.10\\%$ to $83.79\\pm1.25\\%$ AUC and MSI from $78.56\\pm6.24\\%$ to\n$90.32\\pm3.58\\%$ AUC in a multi-center breast and colorectal cancer dataset,\nrespectively. These improvements suggest we can improve genomic label\nclassification performance without collecting larger datasets. In the future,\nthis may reduce the need for expensive genome sequencing techniques, provide\npersonalized therapy recommendations based on widely available WSIs of cancer\ntissue, and improve patient care with quicker treatment decisions - also in\nmedical centers without access to genome sequencing resources.",
          "link": "http://arxiv.org/abs/2107.09405",
          "publishedOn": "2021-07-21T02:01:34.507Z",
          "wordCount": 654,
          "title": "DeepSMILE: Self-supervised heterogeneity-aware multiple instance learning for DNA damage response defect classification directly from H&E whole-slide images. (arXiv:2107.09405v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09362",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ito_H/0/1/0/all/0/1\">Hiroki Ito</a>, <a href=\"http://arxiv.org/find/eess/1/au:+AprilPyone_M/0/1/0/all/0/1\">MaungMaung AprilPyone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>",
          "description": "Since production-level trained deep neural networks (DNNs) are of a great\nbusiness value, protecting such DNN models against copyright infringement and\nunauthorized access is in a rising demand. However, conventional model\nprotection methods focused only the image classification task, and these\nprotection methods were never applied to semantic segmentation although it has\nan increasing number of applications. In this paper, we propose to protect\nsemantic segmentation models from unauthorized access by utilizing block-wise\ntransformation with a secret key for the first time. Protected models are\ntrained by using transformed images. Experiment results show that the proposed\nprotection method allows rightful users with the correct key to access the\nmodel to full capacity and deteriorate the performance for unauthorized users.\nHowever, protected models slightly drop the segmentation performance compared\nto non-protected models.",
          "link": "http://arxiv.org/abs/2107.09362",
          "publishedOn": "2021-07-21T02:01:34.500Z",
          "wordCount": 606,
          "title": "Protecting Semantic Segmentation Models by Using Block-wise Image Encryption with Secret Key from Unauthorized Access. (arXiv:2107.09362v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenlong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>",
          "description": "Generative Adversarial Networks (GAN) have demonstrated the potential to\nrecover realistic details for single image super-resolution (SISR). To further\nimprove the visual quality of super-resolved results, PIRM2018-SR Challenge\nemployed perceptual metrics to assess the perceptual quality, such as PI, NIQE,\nand Ma. However, existing methods cannot directly optimize these\nindifferentiable perceptual metrics, which are shown to be highly correlated\nwith human ratings. To address the problem, we propose Super-Resolution\nGenerative Adversarial Networks with Ranker (RankSRGAN) to optimize generator\nin the direction of different perceptual metrics. Specifically, we first train\na Ranker which can learn the behaviour of perceptual metrics and then introduce\na novel rank-content loss to optimize the perceptual quality. The most\nappealing part is that the proposed method can combine the strengths of\ndifferent SR methods to generate better results. Furthermore, we extend our\nmethod to multiple Rankers to provide multi-dimension constraints for the\ngenerator. Extensive experiments show that RankSRGAN achieves visually pleasing\nresults and reaches state-of-the-art performance in perceptual metrics and\nquality. Project page: https://wenlongzhang0517.github.io/Projects/RankSRGAN",
          "link": "http://arxiv.org/abs/2107.09427",
          "publishedOn": "2021-07-21T02:01:34.493Z",
          "wordCount": 621,
          "title": "RankSRGAN: Super Resolution Generative Adversarial Networks with Learning to Rank. (arXiv:2107.09427v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Donghai Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_J/0/1/0/all/0/1\">Jiabao Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Rui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_S/0/1/0/all/0/1\">Shouye Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bitao Jiang</a>",
          "description": "Collecting large-scale annotated satellite imagery datasets is essential for\ndeep-learning-based global building change surveillance. In particular, the\nscroll imaging mode of optical satellites enables larger observation ranges and\nshorter revisit periods, facilitating efficient global surveillance. However,\nthe images in recent satellite change detection datasets are mainly captured at\nnear-nadir viewing angles. In this paper, we introduce S2Looking, a building\nchange detection dataset that contains large-scale side-looking satellite\nimages captured at varying off-nadir angles. Our S2Looking dataset consists of\n5000 registered bitemporal image pairs (size of 1024*1024, 0.5 ~ 0.8 m/pixel)\nof rural areas throughout the world and more than 65,920 annotated change\ninstances. We provide two label maps to separately indicate the newly built and\ndemolished building regions for each sample in the dataset. We establish a\nbenchmark task based on this dataset, i.e., identifying the pixel-level\nbuilding changes in the bi-temporal images. We test several state-of-the-art\nmethods on both the S2Looking dataset and the (near-nadir) LEVIR-CD+ dataset.\nThe experimental results show that recent change detection methods exhibit much\npoorer performance on the S2Looking than on LEVIR-CD+. The proposed S2Looking\ndataset presents three main challenges: 1) large viewing angle changes, 2)\nlarge illumination variances and 3) various complex scene characteristics\nencountered in rural areas. Our proposed dataset may promote the development of\nalgorithms for satellite image change detection and registration under\nconditions of large off-nadir angles. The dataset is available at\nhttps://github.com/AnonymousForACMMM/.",
          "link": "http://arxiv.org/abs/2107.09244",
          "publishedOn": "2021-07-21T02:01:34.485Z",
          "wordCount": 696,
          "title": "S2Looking: A Satellite Side-Looking Dataset for Building Change Detection. (arXiv:2107.09244v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingjie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilin Chen</a>",
          "description": "Face recognition remains a challenging task in unconstrained scenarios,\nespecially when faces are partially occluded. To improve the robustness against\nocclusion, augmenting the training images with artificial occlusions has been\nproved as a useful approach. However, these artificial occlusions are commonly\ngenerated by adding a black rectangle or several object templates including\nsunglasses, scarfs and phones, which cannot well simulate the realistic\nocclusions. In this paper, based on the argument that the occlusion essentially\ndamages a group of neurons, we propose a novel and elegant occlusion-simulation\nmethod via dropping the activations of a group of neurons in some elaborately\nselected channel. Specifically, we first employ a spatial regularization to\nencourage each feature channel to respond to local and different face regions.\nIn this way, the activations affected by an occlusion in a local region are\nmore likely to be located in a single feature channel. Then, the locality-aware\nchannel-wise dropout (LCD) is designed to simulate the occlusion by dropping\nout the entire feature channel. Furthermore, by randomly dropping out several\nfeature channels, our method can well simulate the occlusion of larger area.\nThe proposed LCD can encourage its succeeding layers to minimize the\nintra-class feature variance caused by occlusions, thus leading to improved\nrobustness against occlusion. In addition, we design an auxiliary spatial\nattention module by learning a channel-wise attention vector to reweight the\nfeature channels, which improves the contributions of non-occluded regions.\nExtensive experiments on various benchmarks show that the proposed method\noutperforms state-of-the-art methods with a remarkable improvement.",
          "link": "http://arxiv.org/abs/2107.09270",
          "publishedOn": "2021-07-21T02:01:34.462Z",
          "wordCount": 688,
          "title": "Locality-aware Channel-wise Dropout for Occluded Face Recognition. (arXiv:2107.09270v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi-Fu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jaesik Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungjin Ahn</a>",
          "description": "Transformers have been successful for many natural language processing tasks.\nHowever, applying transformers to the video domain for tasks such as long-term\nvideo generation and scene understanding has remained elusive due to the high\ncomputational complexity and the lack of natural tokenization. In this paper,\nwe propose the Object-Centric Video Transformer (OCVT) which utilizes an\nobject-centric approach for decomposing scenes into tokens suitable for use in\na generative video transformer. By factoring the video into objects, our fully\nunsupervised model is able to learn complex spatio-temporal dynamics of\nmultiple interacting objects in a scene and generate future frames of the\nvideo. Our model is also significantly more memory-efficient than pixel-based\nmodels and thus able to train on videos of length up to 70 frames with a single\n48GB GPU. We compare our model with previous RNN-based approaches as well as\nother possible video transformer baselines. We demonstrate OCVT performs well\nwhen compared to baselines in generating future frames. OCVT also develops\nuseful representations for video reasoning, achieving start-of-the-art\nperformance on the CATER task.",
          "link": "http://arxiv.org/abs/2107.09240",
          "publishedOn": "2021-07-21T02:01:34.453Z",
          "wordCount": 618,
          "title": "Generative Video Transformer: Can Objects be the Words?. (arXiv:2107.09240v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09543",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1\">Richard Osuala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1\">Kaisar Kushibar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garrucho_L/0/1/0/all/0/1\">Lidia Garrucho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Linardos_A/0/1/0/all/0/1\">Akis Linardos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szafranowska_Z/0/1/0/all/0/1\">Zuzanna Szafranowska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1\">Stefan Klein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1\">Oliver Diaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>",
          "description": "Despite technological and medical advances, the detection, interpretation,\nand treatment of cancer based on imaging data continue to pose significant\nchallenges. These include high inter-observer variability, difficulty of\nsmall-sized lesion detection, nodule interpretation and malignancy\ndetermination, inter- and intra-tumour heterogeneity, class imbalance,\nsegmentation inaccuracies, and treatment effect uncertainty. The recent\nadvancements in Generative Adversarial Networks (GANs) in computer vision as\nwell as in medical imaging may provide a basis for enhanced capabilities in\ncancer detection and analysis. In this review, we assess the potential of GANs\nto address a number of key challenges of cancer imaging, including data\nscarcity and imbalance, domain and dataset shifts, data access and privacy,\ndata annotation and quantification, as well as cancer detection, tumour\nprofiling and treatment planning. We provide a critical appraisal of the\nexisting literature of GANs applied to cancer imagery, together with\nsuggestions on future research directions to address these challenges. We\nanalyse and discuss 163 papers that apply adversarial training techniques in\nthe context of cancer imaging and elaborate their methodologies, advantages and\nlimitations. With this work, we strive to bridge the gap between the needs of\nthe clinical cancer imaging community and the current and prospective research\non GANs in the artificial intelligence community.",
          "link": "http://arxiv.org/abs/2107.09543",
          "publishedOn": "2021-07-21T02:01:34.445Z",
          "wordCount": 691,
          "title": "A Review of Generative Adversarial Networks in Cancer Imaging: New Applications, New Solutions. (arXiv:2107.09543v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09287",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Byrnes_O/0/1/0/all/0/1\">Olivia Byrnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+La_W/0/1/0/all/0/1\">Wendy La</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Congbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Minhui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>",
          "description": "Data hiding is the process of embedding information into a noise-tolerant\nsignal such as a piece of audio, video, or image. Digital watermarking is a\nform of data hiding where identifying data is robustly embedded so that it can\nresist tampering and be used to identify the original owners of the media.\nSteganography, another form of data hiding, embeds data for the purpose of\nsecure and secret communication. This survey summarises recent developments in\ndeep learning techniques for data hiding for the purposes of watermarking and\nsteganography, categorising them based on model architectures and noise\ninjection methods. The objective functions, evaluation metrics, and datasets\nused for training these data hiding models are comprehensively summarised.\nFinally, we propose and discuss possible future directions for research into\ndeep data hiding techniques.",
          "link": "http://arxiv.org/abs/2107.09287",
          "publishedOn": "2021-07-21T02:01:34.418Z",
          "wordCount": 577,
          "title": "Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking and Steganography. (arXiv:2107.09287v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nousias_S/0/1/0/all/0/1\">Stavros Nousias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pikoulis_E/0/1/0/all/0/1\">Erion-Vasilis Pikoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavrokefalidis_C/0/1/0/all/0/1\">Christos Mavrokefalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalos_A/0/1/0/all/0/1\">Aris S. Lalos</a>",
          "description": "Automotive Cyber-Physical Systems (ACPS) have attracted a significant amount\nof interest in the past few decades, while one of the most critical operations\nin these systems is the perception of the environment. Deep learning and,\nespecially, the use of Deep Neural Networks (DNNs) provides impressive results\nin analyzing and understanding complex and dynamic scenes from visual data. The\nprediction horizons for those perception systems are very short and inference\nmust often be performed in real time, stressing the need of transforming the\noriginal large pre-trained networks into new smaller models, by utilizing Model\nCompression and Acceleration (MCA) techniques. Our goal in this work is to\ninvestigate best practices for appropriately applying novel weight sharing\ntechniques, optimizing the available variables and the training procedures\ntowards the significant acceleration of widely adopted DNNs. Extensive\nevaluation studies carried out using various state-of-the-art DNN models in\nobject detection and tracking experiments, provide details about the type of\nerrors that manifest after the application of weight sharing techniques,\nresulting in significant acceleration gains with negligible accuracy losses.",
          "link": "http://arxiv.org/abs/2107.09101",
          "publishedOn": "2021-07-21T02:01:34.411Z",
          "wordCount": 620,
          "title": "Accelerating deep neural networks for efficient scene understanding in automotive cyber-physical systems. (arXiv:2107.09101v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09559",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Billot_B/0/1/0/all/0/1\">Benjamin Billot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Greve_D/0/1/0/all/0/1\">Douglas N. Greve</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puonti_O/0/1/0/all/0/1\">Oula Puonti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thielscher_A/0/1/0/all/0/1\">Axel Thielscher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leemput_K/0/1/0/all/0/1\">Koen Van Leemput</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fischl_B/0/1/0/all/0/1\">Bruce Fischl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1\">Juan Eugenio Iglesias</a>",
          "description": "Despite advances in data augmentation and transfer learning, convolutional\nneural networks (CNNs) have difficulties generalising to unseen target domains.\nWhen applied to segmentation of brain MRI scans, CNNs are highly sensitive to\nchanges in resolution and contrast: even within the same MR modality, decreases\nin performance can be observed across datasets. We introduce SynthSeg, the\nfirst segmentation CNN agnostic to brain MRI scans of any contrast and\nresolution. SynthSeg is trained with synthetic data sampled from a generative\nmodel inspired by Bayesian segmentation. Crucially, we adopt a \\textit{domain\nrandomisation} strategy where we fully randomise the generation parameters to\nmaximise the variability of the training data. Consequently, SynthSeg can\nsegment preprocessed and unpreprocessed real scans of any target domain,\nwithout retraining or fine-tuning. Because SynthSeg only requires segmentations\nto be trained (no images), it can learn from label maps obtained automatically\nfrom existing datasets of different populations (e.g., with atrophy and\nlesions), thus achieving robustness to a wide range of morphological\nvariability. We demonstrate SynthSeg on 5,500 scans of 6 modalities and 10\nresolutions, where it exhibits unparalleled generalisation compared to\nsupervised CNNs, test time adaptation, and Bayesian segmentation. The code and\ntrained model are available at https://github.com/BBillot/SynthSeg.",
          "link": "http://arxiv.org/abs/2107.09559",
          "publishedOn": "2021-07-21T02:01:34.386Z",
          "wordCount": 676,
          "title": "SynthSeg: Domain Randomisation for Segmentation of Brain MRI Scans of any Contrast and Resolution. (arXiv:2107.09559v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wilmet_V/0/1/0/all/0/1\">Vincent Wilmet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1\">Sauraj Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redl_T/0/1/0/all/0/1\">Tabea Redl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandaker_H/0/1/0/all/0/1\">H&#xe5;kon Sandaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenning Li</a>",
          "description": "Anomaly detection in images plays a significant role for many applications\nacross all industries, such as disease diagnosis in healthcare or quality\nassurance in manufacturing. Manual inspection of images, when extended over a\nmonotonously repetitive period of time is very time consuming and can lead to\nanomalies being overlooked.Artificial neural networks have proven themselves\nvery successful on simple, repetitive tasks, in some cases even outperforming\nhumans. Therefore, in this paper we investigate different methods of deep\nlearning, including supervised and unsupervised learning, for anomaly detection\napplied to a quality assurance use case. We utilize the MVTec anomaly dataset\nand develop three different models, a CNN for supervised anomaly detection,\nKD-CAE for autoencoder anomaly detection, NI-CAE for noise induced anomaly\ndetection and a DCGAN for generating reconstructed images. By experiments, we\nfound that KD-CAE performs better on the anomaly datasets compared to CNN and\nNI-CAE, with NI-CAE performing the best on the Transistor dataset. We also\nimplemented a DCGAN for the creation of new training data but due to\ncomputational limitation and lack of extrapolating the mechanics of AnoGAN, we\nrestricted ourselves just to the generation of GAN based images. We conclude\nthat unsupervised methods are more powerful for anomaly detection in images,\nespecially in a setting where only a small amount of anomalous data is\navailable, or the data is unlabeled.",
          "link": "http://arxiv.org/abs/2107.09204",
          "publishedOn": "2021-07-21T02:01:34.379Z",
          "wordCount": 686,
          "title": "A Comparison of Supervised and Unsupervised Deep Learning Methods for Anomaly Detection in Images. (arXiv:2107.09204v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zatsarynna_O/0/1/0/all/0/1\">Olga Zatsarynna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farha_Y/0/1/0/all/0/1\">Yazan Abu Farha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>",
          "description": "Anticipating human actions is an important task that needs to be addressed\nfor the development of reliable intelligent agents, such as self-driving cars\nor robot assistants. While the ability to make future predictions with high\naccuracy is crucial for designing the anticipation approaches, the speed at\nwhich the inference is performed is not less important. Methods that are\naccurate but not sufficiently fast would introduce a high latency into the\ndecision process. Thus, this will increase the reaction time of the underlying\nsystem. This poses a problem for domains such as autonomous driving, where the\nreaction time is crucial. In this work, we propose a simple and effective\nmulti-modal architecture based on temporal convolutions. Our approach stacks a\nhierarchy of temporal convolutional layers and does not rely on recurrent\nlayers to ensure a fast prediction. We further introduce a multi-modal fusion\nmechanism that captures the pairwise interactions between RGB, flow, and object\nmodalities. Results on two large-scale datasets of egocentric videos,\nEPIC-Kitchens-55 and EPIC-Kitchens-100, show that our approach achieves\ncomparable performance to the state-of-the-art approaches while being\nsignificantly faster.",
          "link": "http://arxiv.org/abs/2107.09504",
          "publishedOn": "2021-07-21T02:01:34.357Z",
          "wordCount": 622,
          "title": "Multi-Modal Temporal Convolutional Network for Anticipating Actions in Egocentric Videos. (arXiv:2107.09504v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Suzhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lincheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>",
          "description": "We propose an audio-driven talking-head method to generate photo-realistic\ntalking-head videos from a single reference image. In this work, we tackle two\nkey challenges: (i) producing natural head motions that match speech prosody,\nand (ii) maintaining the appearance of a speaker in a large head motion while\nstabilizing the non-face regions. We first design a head pose predictor by\nmodeling rigid 6D head movements with a motion-aware recurrent neural network\n(RNN). In this way, the predicted head poses act as the low-frequency holistic\nmovements of a talking head, thus allowing our latter network to focus on\ndetailed facial movement generation. To depict the entire image motions arising\nfrom audio, we exploit a keypoint based dense motion field representation.\nThen, we develop a motion field generator to produce the dense motion fields\nfrom input audio, head poses, and a reference image. As this keypoint based\nrepresentation models the motions of facial regions, head, and backgrounds\nintegrally, our method can better constrain the spatial and temporal\nconsistency of the generated videos. Finally, an image generation network is\nemployed to render photo-realistic talking-head videos from the estimated\nkeypoint based motion fields and the input reference image. Extensive\nexperiments demonstrate that our method produces videos with plausible head\nmotions, synchronized facial expressions, and stable backgrounds and\noutperforms the state-of-the-art.",
          "link": "http://arxiv.org/abs/2107.09293",
          "publishedOn": "2021-07-21T02:01:34.351Z",
          "wordCount": 663,
          "title": "Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion. (arXiv:2107.09293v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Liangjian Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lili Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>",
          "description": "The goal of few-shot classification is to classify new categories with few\nlabeled examples within each class. Nowadays, the excellent performance in\nhandling few-shot classification problems is shown by metric-based\nmeta-learning methods. However, it is very hard for previous methods to\ndiscriminate the fine-grained sub-categories in the embedding space without\nfine-grained labels. This may lead to unsatisfactory generalization to\nfine-grained subcategories, and thus affects model interpretation. To tackle\nthis problem, we introduce the contrastive loss into few-shot classification\nfor learning latent fine-grained structure in the embedding space. Furthermore,\nto overcome the drawbacks of random image transformation used in current\ncontrastive learning in producing noisy and inaccurate image pairs (i.e.,\nviews), we develop a learning-to-learn algorithm to automatically generate\ndifferent views of the same image. Extensive experiments on standard few-shot\nlearning benchmarks demonstrate the superiority of our method.",
          "link": "http://arxiv.org/abs/2107.09242",
          "publishedOn": "2021-07-21T02:01:34.331Z",
          "wordCount": 578,
          "title": "Boosting few-shot classification with view-learnable contrastive learning. (arXiv:2107.09242v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Melnik_A/0/1/0/all/0/1\">Andrew Melnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harter_A/0/1/0/all/0/1\">Augustin Harter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Limberg_C/0/1/0/all/0/1\">Christian Limberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_K/0/1/0/all/0/1\">Krishan Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suenderhauf_N/0/1/0/all/0/1\">Niko Suenderhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_H/0/1/0/all/0/1\">Helge Ritter</a>",
          "description": "This work discusses a learning approach to mask rewarding objects in images\nusing sparse reward signals from an imitation learning dataset. For that, we\ntrain an Hourglass network using only feedback from a critic model. The\nHourglass network learns to produce a mask to decrease the critic's score of a\nhigh score image and increase the critic's score of a low score image by\nswapping the masked areas between these two images. We trained the model on an\nimitation learning dataset from the NeurIPS 2020 MineRL Competition Track,\nwhere our model learned to mask rewarding objects in a complex interactive 3D\nenvironment with a sparse reward signal. This approach was part of the 1st\nplace winning solution in this competition. Video demonstration and code:\nhttps://rebrand.ly/critic-guided-segmentation",
          "link": "http://arxiv.org/abs/2107.09540",
          "publishedOn": "2021-07-21T02:01:34.324Z",
          "wordCount": 572,
          "title": "Critic Guided Segmentation of Rewarding Objects in First-Person Views. (arXiv:2107.09540v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09237",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xingxing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zaifeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenghua Chen</a>",
          "description": "Near infrared (NIR) imaging has been widely applied in low-light imaging\nscenarios; however, it is difficult for human and algorithms to perceive the\nreal scene in the colorless NIR domain. While Generative Adversarial Network\n(GAN) has been widely employed in various image colorization tasks, it is\nchallenging for a direct mapping mechanism, such as a conventional GAN, to\ntransform an image from the NIR to the RGB domain with correct semantic\nreasoning, well-preserved textures, and vivid color combinations concurrently.\nIn this work, we propose a novel Attention-based NIR image colorization\nframework via Adaptive Fusion of Semantic and Texture clues, aiming at\nachieving these goals within the same framework. The tasks of texture transfer\nand semantic reasoning are carried out in two separate network blocks.\nSpecifically, the Texture Transfer Block (TTB) aims at extracting texture\nfeatures from the NIR image's Laplacian component and transferring them for\nsubsequent color fusion. The Semantic Reasoning Block (SRB) extracts semantic\nclues and maps the NIR pixel values to the RGB domain. Finally, a Fusion\nAttention Block (FAB) is proposed to adaptively fuse the features from the two\nbranches and generate an optimized colorization result. In order to enhance the\nnetwork's learning capacity in semantic reasoning as well as mapping precision\nin texture transfer, we have proposed the Residual Coordinate Attention Block\n(RCAB), which incorporates coordinate attention into a residual learning\nframework, enabling the network to capture long-range dependencies along the\nchannel direction and meanwhile precise positional information can be preserved\nalong spatial directions. RCAB is also incorporated into FAB to facilitate\naccurate texture alignment during fusion. Both quantitative and qualitative\nevaluations show that the proposed method outperforms state-of-the-art NIR\nimage colorization methods.",
          "link": "http://arxiv.org/abs/2107.09237",
          "publishedOn": "2021-07-21T02:01:34.317Z",
          "wordCount": 721,
          "title": "Attention-Guided NIR Image Colorization via Adaptive Fusion of Semantic and Texture Clues. (arXiv:2107.09237v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vicente_J/0/1/0/all/0/1\">Juan Pablo de Vicente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1\">Alvaro Soto</a>",
          "description": "Current datasets to train social behaviors are usually borrowed from\nsurveillance applications that capture visual data from a bird's-eye\nperspective. This leaves aside precious relationships and visual cues that\ncould be captured through a first-person view of a scene. In this work, we\npropose a strategy to exploit the power of current game engines, such as Unity,\nto transform pre-existing bird's-eye view datasets into a first-person view, in\nparticular, a depth view. Using this strategy, we are able to generate large\nvolumes of synthetic data that can be used to pre-train a social navigation\nmodel. To test our ideas, we present DeepSocNav, a deep learning based model\nthat takes advantage of the proposed approach to generate synthetic data.\nFurthermore, DeepSocNav includes a self-supervised strategy that is included as\nan auxiliary task. This consists of predicting the next depth frame that the\nagent will face. Our experiments show the benefits of the proposed model that\nis able to outperform relevant baselines in terms of social navigation scores.",
          "link": "http://arxiv.org/abs/2107.09170",
          "publishedOn": "2021-07-21T02:01:34.291Z",
          "wordCount": 624,
          "title": "DeepSocNav: Social Navigation by Imitating Human Behaviors. (arXiv:2107.09170v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09179",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bidgoli_N/0/1/0/all/0/1\">Navid Mahmoudian Bidgoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Azevedo_R/0/1/0/all/0/1\">Roberto G. de A. Azevedo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maugey_T/0/1/0/all/0/1\">Thomas Maugey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roumy_A/0/1/0/all/0/1\">Aline Roumy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>",
          "description": "State-of-the-art 2D image compression schemes rely on the power of\nconvolutional neural networks (CNNs). Although CNNs offer promising\nperspectives for 2D image compression, extending such models to omnidirectional\nimages is not straightforward. First, omnidirectional images have specific\nspatial and statistical properties that can not be fully captured by current\nCNN models. Second, basic mathematical operations composing a CNN architecture,\ne.g., translation and sampling, are not well-defined on the sphere. In this\npaper, we study the learning of representation models for omnidirectional\nimages and propose to use the properties of HEALPix uniform sampling of the\nsphere to redefine the mathematical tools used in deep learning models for\nomnidirectional images. In particular, we: i) propose the definition of a new\nconvolution operation on the sphere that keeps the high expressiveness and the\nlow complexity of a classical 2D convolution; ii) adapt standard CNN techniques\nsuch as stride, iterative aggregation, and pixel shuffling to the spherical\ndomain; and then iii) apply our new framework to the task of omnidirectional\nimage compression. Our experiments show that our proposed on-the-sphere\nsolution leads to a better compression gain that can save 13.7% of the bit rate\ncompared to similar learned models applied to equirectangular images. Also,\ncompared to learning models based on graph convolutional networks, our solution\nsupports more expressive filters that can preserve high frequencies and provide\na better perceptual quality of the compressed images. Such results demonstrate\nthe efficiency of the proposed framework, which opens new research venues for\nother omnidirectional vision tasks to be effectively implemented on the sphere\nmanifold.",
          "link": "http://arxiv.org/abs/2107.09179",
          "publishedOn": "2021-07-21T02:01:34.082Z",
          "wordCount": 719,
          "title": "OSLO: On-the-Sphere Learning for Omnidirectional images and its application to 360-degree image compression. (arXiv:2107.09179v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09136",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dick_J/0/1/0/all/0/1\">Jo&#xe3;o Dick</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abreu_B/0/1/0/all/0/1\">Brunno Abreu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grellert_M/0/1/0/all/0/1\">Mateus Grellert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bampi_S/0/1/0/all/0/1\">Sergio Bampi</a>",
          "description": "This work presents an analysis of state-of-the-art learning-based image\ncompression techniques. We compare 8 models available in the Tensorflow\nCompression package in terms of visual quality metrics and processing time,\nusing the KODAK data set. The results are compared with the Better Portable\nGraphics (BPG) and the JPEG2000 codecs. Results show that JPEG2000 has the\nlowest execution times compared with the fastest learning-based model, with a\nspeedup of 1.46x in compression and 30x in decompression. However, the\nlearning-based models achieved improvements over JPEG2000 in terms of quality,\nspecially for lower bitrates. Our findings also show that BPG is more efficient\nin terms of PSNR, but the learning models are better for other quality metrics,\nand sometimes even faster. The results indicate that learning-based techniques\nare promising solutions towards a future mainstream compression method.",
          "link": "http://arxiv.org/abs/2107.09136",
          "publishedOn": "2021-07-21T02:01:33.936Z",
          "wordCount": 584,
          "title": "Quality and Complexity Assessment of Learning-Based Image Compression Solutions. (arXiv:2107.09136v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Whitehead_S/0/1/0/all/0/1\">Spencer Whitehead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>",
          "description": "Generalization to out-of-distribution data has been a problem for Visual\nQuestion Answering (VQA) models. To measure generalization to novel questions,\nwe propose to separate them into \"skills\" and \"concepts\". \"Skills\" are visual\ntasks, such as counting or attribute recognition, and are applied to \"concepts\"\nmentioned in the question, such as objects and people. VQA methods should be\nable to compose skills and concepts in novel ways, regardless of whether the\nspecific composition has been seen in training, yet we demonstrate that\nexisting models have much to improve upon towards handling new compositions. We\npresent a novel method for learning to compose skills and concepts that\nseparates these two factors implicitly within a model by learning grounded\nconcept representations and disentangling the encoding of skills from that of\nconcepts. We enforce these properties with a novel contrastive learning\nprocedure that does not rely on external annotations and can be learned from\nunlabeled image-question pairs. Experiments demonstrate the effectiveness of\nour approach for improving compositional and grounding performance.",
          "link": "http://arxiv.org/abs/2107.09106",
          "publishedOn": "2021-07-21T02:01:33.922Z",
          "wordCount": 624,
          "title": "Separating Skills and Concepts for Novel Visual Question Answering. (arXiv:2107.09106v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09134",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lima_D/0/1/0/all/0/1\">Daniel Lima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Graves_C/0/1/0/all/0/1\">Catharine Graves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gutierrez_M/0/1/0/all/0/1\">Marco Gutierrez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brandoli_B/0/1/0/all/0/1\">Bruno Brandoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jose_J/0/1/0/all/0/1\">Jose Rodrigues-Jr</a>",
          "description": "Magnetic resonance imaging (MRI) is a widely known medical imaging technique\nused to assess the heart function. Deep learning (DL) models perform several\ntasks in cardiac MRI (CMR) images with good efficacy, such as segmentation,\nestimation, and detection of diseases. Many DL models based on convolutional\nneural networks (CNN) were improved by detecting regions-of-interest (ROI)\neither automatically or by hand. In this paper we describe Visual-Motion-Focus\n(VMF), a module that detects the heart motion in the 4D MRI sequence, and\nhighlights ROIs by focusing a Radial Basis Function (RBF) on the estimated\nmotion field. We experimented and evaluated VMF on three CMR datasets,\nobserving that the proposed ROIs cover 99.7% of data labels (Recall score),\nimproved the CNN segmentation (mean Dice score) by 1.7 (p < .001) after the ROI\nextraction, and improved the overall training speed by 2.5 times (+150%).",
          "link": "http://arxiv.org/abs/2107.09134",
          "publishedOn": "2021-07-21T02:01:33.906Z",
          "wordCount": 609,
          "title": "Convolutional module for heart localization and segmentation in MRI. (arXiv:2107.09134v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09060",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kustner_T/0/1/0/all/0/1\">Thomas K&#xfc;stner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_J/0/1/0/all/0/1\">Jiazhen Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_H/0/1/0/all/0/1\">Haikun Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cruz_G/0/1/0/all/0/1\">Gastao Cruz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gilliam_C/0/1/0/all/0/1\">Christopher Gilliam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blu_T/0/1/0/all/0/1\">Thierry Blu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_B/0/1/0/all/0/1\">Bin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gatidis_S/0/1/0/all/0/1\">Sergios Gatidis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Botnar_R/0/1/0/all/0/1\">Ren&#xe9; Botnar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prieto_C/0/1/0/all/0/1\">Claudia Prieto</a>",
          "description": "Physiological motion, such as cardiac and respiratory motion, during Magnetic\nResonance (MR) image acquisition can cause image artifacts. Motion correction\ntechniques have been proposed to compensate for these types of motion during\nthoracic scans, relying on accurate motion estimation from undersampled\nmotion-resolved reconstruction. A particular interest and challenge lie in the\nderivation of reliable non-rigid motion fields from the undersampled\nmotion-resolved data. Motion estimation is usually formulated in image space\nvia diffusion, parametric-spline, or optical flow methods. However, image-based\nregistration can be impaired by remaining aliasing artifacts due to the\nundersampled motion-resolved reconstruction. In this work, we describe a\nformalism to perform non-rigid registration directly in the sampled Fourier\nspace, i.e. k-space. We propose a deep-learning based approach to perform fast\nand accurate non-rigid registration from the undersampled k-space data. The\nbasic working principle originates from the Local All-Pass (LAP) technique, a\nrecently introduced optical flow-based registration. The proposed LAPNet is\ncompared against traditional and deep learning image-based registrations and\ntested on fully-sampled and highly-accelerated (with two undersampling\nstrategies) 3D respiratory motion-resolved MR images in a cohort of 40 patients\nwith suspected liver or lung metastases and 25 healthy subjects. The proposed\nLAPNet provided consistent and superior performance to image-based approaches\nthroughout different sampling trajectories and acceleration factors.",
          "link": "http://arxiv.org/abs/2107.09060",
          "publishedOn": "2021-07-21T02:01:33.895Z",
          "wordCount": 677,
          "title": "LAPNet: Non-rigid Registration derived in k-space for Magnetic Resonance Imaging. (arXiv:2107.09060v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09118",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Khaledyan_D/0/1/0/all/0/1\">Donya Khaledyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tajally_A/0/1/0/all/0/1\">AmirReza Tajally</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarkhosh_R/0/1/0/all/0/1\">Reza Sarkhosh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shamsi_A/0/1/0/all/0/1\">Afshar Shamsi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asgharnezhad_H/0/1/0/all/0/1\">Hamzeh Asgharnezhad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>",
          "description": "Deep learning (DL) models have received particular attention in medical\nimaging due to their promising pattern recognition capabilities. However, Deep\nNeural Networks (DNNs) require a huge amount of data, and because of the lack\nof sufficient data in this field, transfer learning can be a great solution.\nDNNs used for disease diagnosis meticulously concentrate on improving the\naccuracy of predictions without providing a figure about their confidence of\npredictions. Knowing how much a DNN model is confident in a computer-aided\ndiagnosis model is necessary for gaining clinicians' confidence and trust in\nDL-based solutions. To address this issue, this work presents three different\nmethods for quantifying uncertainties for skin cancer detection from images. It\nalso comprehensively evaluates and compares performance of these DNNs using\nnovel uncertainty-related metrics. The obtained results reveal that the\npredictive uncertainty estimation methods are capable of flagging risky and\nerroneous predictions with a high uncertainty estimate. We also demonstrate\nthat ensemble approaches are more reliable in capturing uncertainties through\ninference.",
          "link": "http://arxiv.org/abs/2107.09118",
          "publishedOn": "2021-07-21T02:01:33.852Z",
          "wordCount": 625,
          "title": "Confidence Aware Neural Networks for Skin Cancer Detection. (arXiv:2107.09118v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tom_M/0/1/0/all/0/1\">Manu Tom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuchang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baltsavias_E/0/1/0/all/0/1\">Emmanuel Baltsavias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>",
          "description": "Fusing satellite imagery acquired with different sensors has been a\nlong-standing challenge of Earth observation, particularly across different\nmodalities such as optical and Synthetic Aperture Radar (SAR) images. Here, we\nexplore the joint analysis of imagery from different sensors in the light of\nrepresentation learning: we propose to learn a joint, sensor-invariant\nembedding (feature representation) within a deep neural network. Our\napplication problem is the monitoring of lake ice on Alpine lakes. To reach the\ntemporal resolution requirement of the Swiss Global Climate Observing System\n(GCOS) office, we combine three image sources: Sentinel-1 SAR (S1-SAR), Terra\nMODIS and Suomi-NPP VIIRS. The large gaps between the optical and SAR domains\nand between the sensor resolutions make this a challenging instance of the\nsensor fusion problem. Our approach can be classified as a feature-level fusion\nthat is learnt in a data-driven manner. The proposed network architecture has\nseparate encoding branches for each image sensor, which feed into a single\nlatent embedding. I.e., a common feature representation shared by all inputs,\nsuch that subsequent processing steps deliver comparable output irrespective of\nwhich sort of input image was used. By fusing satellite data, we map lake ice\nat a temporal resolution of <1.5 days. The network produces spatially explicit\nlake ice maps with pixel-wise accuracies >91.3% (respectively, mIoU scores\n>60.7%) and generalises well across different lakes and winters. Moreover, it\nsets a new state-of-the-art for determining the important ice-on and ice-off\ndates for the target lakes, in many cases meeting the GCOS requirement.",
          "link": "http://arxiv.org/abs/2107.09092",
          "publishedOn": "2021-07-21T02:01:33.823Z",
          "wordCount": 703,
          "title": "Learning a Sensor-invariant Embedding of Satellite Data: A Case Study for Lake Ice Monitoring. (arXiv:2107.09092v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Soumyadeep Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjay Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_J/0/1/0/all/0/1\">Janu Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Awanish Kumar</a>",
          "description": "In order to train robust deep learning models, large amounts of labelled data\nis required. However, in the absence of such large repositories of labelled\ndata, unlabeled data can be exploited for the same. Semi-Supervised learning\naims to utilize such unlabeled data for training classification models. Recent\nprogress of self-training based approaches have shown promise in this area,\nwhich leads to this study where we utilize an ensemble approach for the same. A\nby-product of any semi-supervised approach may be loss of calibration of the\ntrained model especially in scenarios where unlabeled data may contain\nout-of-distribution samples, which leads to this investigation on how to adapt\nto such effects. Our proposed algorithm carefully avoids common pitfalls in\nutilizing unlabeled data and leads to a more accurate and calibrated supervised\nmodel compared to vanilla self-training based student-teacher algorithms. We\nperform several experiments on the popular STL-10 database followed by an\nextensive analysis of our approach and study its effects on model accuracy and\ncalibration.",
          "link": "http://arxiv.org/abs/2107.08211",
          "publishedOn": "2021-07-20T02:04:48.562Z",
          "wordCount": 607,
          "title": "Self Training with Ensemble of Teacher Models. (arXiv:2107.08211v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianshu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaomin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiali Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "In this paper, we propose a novel training strategy for convolutional neural\nnetwork(CNN) named Feature Mining, that aims to strengthen the network's\nlearning of the local feature. Through experiments, we find that semantic\ncontained in different parts of the feature is different, while the network\nwill inevitably lose the local information during feedforward propagation. In\norder to enhance the learning of local feature, Feature Mining divides the\ncomplete feature into two complementary parts and reuse these divided feature\nto make the network learn more local information, we call the two steps as\nfeature segmentation and feature reusing. Feature Mining is a parameter-free\nmethod and has plug-and-play nature, and can be applied to any CNN models.\nExtensive experiments demonstrate the wide applicability, versatility, and\ncompatibility of our method.",
          "link": "http://arxiv.org/abs/2107.08421",
          "publishedOn": "2021-07-20T02:04:48.428Z",
          "wordCount": 571,
          "title": "Feature Mining: A Novel Training Strategy for Convolutional Neural Network. (arXiv:2107.08421v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1\">Xing Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shuowen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Recent advances in deep convolutional neural networks (DCNNs) have shown\nimpressive performance improvements on thermal to visible face synthesis and\nmatching problems. However, current DCNN-based synthesis models do not perform\nwell on thermal faces with large pose variations. In order to deal with this\nproblem, heterogeneous face frontalization methods are needed in which a model\ntakes a thermal profile face image and generates a frontal visible face. This\nis an extremely difficult problem due to the large domain as well as large pose\ndiscrepancies between the two modalities. Despite its applications in\nbiometrics and surveillance, this problem is relatively unexplored in the\nliterature. We propose a domain agnostic learning-based generative adversarial\nnetwork (DAL-GAN) which can synthesize frontal views in the visible domain from\nthermal faces with pose variations. DAL-GAN consists of a generator with an\nauxiliary classifier and two discriminators which capture both local and global\ntexture discriminations for better synthesis. A contrastive constraint is\nenforced in the latent space of the generator with the help of a dual-path\ntraining strategy, which improves the feature vector discrimination. Finally, a\nmulti-purpose loss function is utilized to guide the network in synthesizing\nidentity preserving cross-domain frontalization. Extensive experimental results\ndemonstrate that DAL-GAN can generate better quality frontal views compared to\nthe other baseline methods.",
          "link": "http://arxiv.org/abs/2107.08311",
          "publishedOn": "2021-07-20T02:04:48.375Z",
          "wordCount": 661,
          "title": "Heterogeneous Face Frontalization via Domain Agnostic Learning. (arXiv:2107.08311v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mathew_M/0/1/0/all/0/1\">Minesh Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1\">Ruben Tito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1\">Dimosthenis Karatzas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1\">R. Manmatha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C.V. Jawahar</a>",
          "description": "This paper presents results of Document Visual Question Answering Challenge\norganized as part of \"Text and Documents in the Deep Learning Era\" workshop, in\nCVPR 2020. The challenge introduces a new problem - Visual Question Answering\non document images. The challenge comprised two tasks. The first task concerns\nwith asking questions on a single document image. On the other hand, the second\ntask is set as a retrieval task where the question is posed over a collection\nof images. For the task 1 a new dataset is introduced comprising 50,000\nquestions-answer(s) pairs defined over 12,767 document images. For task 2\nanother dataset has been created comprising 20 questions over 14,362 document\nimages which share the same document template.",
          "link": "http://arxiv.org/abs/2008.08899",
          "publishedOn": "2021-07-20T02:04:47.750Z",
          "wordCount": 597,
          "title": "Document Visual Question Answering Challenge 2020. (arXiv:2008.08899v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Dawei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Longyin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Pengfei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Heng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junwen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Ali_A/0/1/0/all/0/1\">Ali Al-Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Amr Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imene_B/0/1/0/all/0/1\">Bakour Imene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nesma_B/0/1/0/all/0/1\">Bouchali Hadia Nesma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chenzhen Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castiello_C/0/1/0/all/0/1\">Ciro Castiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mencar_C/0/1/0/all/0/1\">Corrado Mencar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Dingkang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruger_F/0/1/0/all/0/1\">Florian Kr&#xfc;ger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vessio_G/0/1/0/all/0/1\">Gennaro Vessio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castellano_G/0/1/0/all/0/1\">Giovanna Castellano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jieru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abualsaud_K/0/1/0/all/0/1\">Khalid Abualsaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Laihui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cianciotta_M/0/1/0/all/0/1\">Marco Cianciotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saqib_M/0/1/0/all/0/1\">Muhammad Saqib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almaadeed_N/0/1/0/all/0/1\">Noor Almaadeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elharrouss_O/0/1/0/all/0/1\">Omar Elharrouss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_P/0/1/0/all/0/1\">Pei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shidong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shuang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Siyang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Maadeed_S/0/1/0/all/0/1\">Somaya Al-Maadeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Sultan Daud Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattab_T/0/1/0/all/0/1\">Tamer Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golda_T/0/1/0/all/0/1\">Thomas Golda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaoqing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Ye Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingnan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongchao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuehan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhijian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhiwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhiyuan Zhao</a>",
          "description": "Crowd counting on the drone platform is an interesting topic in computer\nvision, which brings new challenges such as small object inference, background\nclutter and wide viewpoint. However, there are few algorithms focusing on crowd\ncounting on the drone-captured data due to the lack of comprehensive datasets.\nTo this end, we collect a large-scale dataset and organize the Vision Meets\nDrone Crowd Counting Challenge (VisDrone-CC2020) in conjunction with the 16th\nEuropean Conference on Computer Vision (ECCV 2020) to promote the developments\nin the related fields. The collected dataset is formed by $3,360$ images,\nincluding $2,460$ images for training, and $900$ images for testing.\nSpecifically, we manually annotate persons with points in each video frame.\nThere are $14$ algorithms from $15$ institutes submitted to the VisDrone-CC2020\nChallenge. We provide a detailed analysis of the evaluation results and\nconclude the challenge. More information can be found at the website:\n\\url{this http URL}.",
          "link": "http://arxiv.org/abs/2107.08766",
          "publishedOn": "2021-07-20T02:04:47.020Z",
          "wordCount": null,
          "title": "VisDrone-CC2020: The Vision Meets Drone Crowd Counting Challenge Results. (arXiv:2107.08766v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gun-Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Han-Bin Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Deep learning has played a major role in the interpretation of dermoscopic\nimages for detecting skin defects and abnormalities. However, current deep\nlearning solutions for dermatological lesion analysis are typically limited in\nproviding probabilistic predictions which highlights the importance of\nconcerning uncertainties. This concept of uncertainty can provide a confidence\nlevel for each feature which prevents overconfident predictions with poor\ngeneralization on unseen data. In this paper, we propose an overall framework\nthat jointly considers dermatological classification and uncertainty estimation\ntogether. The estimated confidence of each feature to avoid uncertain feature\nand undesirable shift, which are caused by environmental difference of input\nimage, in the latent space is pooled from confidence network. Our qualitative\nresults show that modeling uncertainties not only helps to quantify model\nconfidence for each prediction but also helps classification layers to focus on\nconfident features, therefore, improving the accuracy for dermatological lesion\nclassification. We demonstrate the potential of the proposed approach in two\nstate-of-the-art dermoscopic datasets (ISIC 2018 and ISIC 2019).",
          "link": "http://arxiv.org/abs/2107.08770",
          "publishedOn": "2021-07-20T02:04:47.020Z",
          "wordCount": null,
          "title": "Joint Dermatological Lesion Classification and Confidence Modeling with Uncertainty Estimation. (arXiv:2107.08770v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thuy_H/0/1/0/all/0/1\">Hang Duong Thi Thuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minh_T/0/1/0/all/0/1\">Tuan Nguyen Minh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Van_P/0/1/0/all/0/1\">Phi Nguyen Van</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quoc_L/0/1/0/all/0/1\">Long Tran Quoc</a>",
          "description": "Nowadays, cardiac diagnosis largely depends on left ventricular function\nassessment. With the help of the segmentation deep learning model, the\nassessment of the left ventricle becomes more accessible and accurate. However,\ndeep learning technique still faces two main obstacles: the difficulty in\nacquiring sufficient training data and time-consuming in developing quality\nmodels. In the ordinary data acquisition process, the dataset was selected\nrandomly from a large pool of unlabeled images for labeling, leading to massive\nlabor time to annotate those images. Besides that, hand-designed model\ndevelopment is laborious and also costly. This paper introduces a pipeline that\nrelies on Active Learning to ease the labeling work and utilizes Neural\nArchitecture Search's idea to design the adequate deep learning model\nautomatically. We called this Fully automated machine learning pipeline for\nechocardiogram segmentation. The experiment results show that our method\nobtained the same IOU accuracy with only two-fifths of the original training\ndataset, and the searched model got the same accuracy as the hand-designed\nmodel given the same training dataset.",
          "link": "http://arxiv.org/abs/2107.08440",
          "publishedOn": "2021-07-20T02:04:46.794Z",
          "wordCount": null,
          "title": "Fully Automated Machine Learning Pipeline for Echocardiogram Segmentation. (arXiv:2107.08440v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Liangqiong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandar_N/0/1/0/all/0/1\">Niranjan Balachandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel L Rubin</a>",
          "description": "Federated learning enables multiple institutions to collaboratively train\nmachine learning models on their local data in a privacy-preserving way.\nHowever, its distributed nature often leads to significant heterogeneity in\ndata distributions across institutions. In this paper, we investigate the\ndeleterious impact of a taxonomy of data heterogeneity regimes on federated\nlearning methods, including quantity skew, label distribution skew, and imaging\nacquisition skew. We show that the performance degrades with the increasing\ndegrees of data heterogeneity. We present several mitigation strategies to\novercome performance drops from data heterogeneity, including weighted average\nfor data quantity skew, weighted loss and batch normalization averaging for\nlabel distribution skew. The proposed optimizations to federated learning\nmethods improve their capability of handling heterogeneity across institutions,\nwhich provides valuable guidance for the deployment of federated learning in\nreal clinical applications.",
          "link": "http://arxiv.org/abs/2107.08371",
          "publishedOn": "2021-07-20T02:04:46.772Z",
          "wordCount": null,
          "title": "An Experimental Study of Data Heterogeneity in Federated Learning Methods for Medical Imaging. (arXiv:2107.08371v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11237",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jinlong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengkai Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yueyang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>",
          "description": "Recently, most siamese network based trackers locate targets via object\nclassification and bounding-box regression. Generally, they select the\nbounding-box with maximum classification confidence as the final prediction.\nThis strategy may miss the right result due to the accuracy misalignment\nbetween classification and regression. In this paper, we propose a novel\nsiamese tracking algorithm called SiamRCR, addressing this problem with a\nsimple, light and effective solution. It builds reciprocal links between\nclassification and regression branches, which can dynamically re-weight their\nlosses for each positive sample. In addition, we add a localization branch to\npredict the localization accuracy, so that it can work as the replacement of\nthe regression assistance link during inference. This branch makes the training\nand inference more consistent. Extensive experimental results demonstrate the\neffectiveness of SiamRCR and its superiority over the state-of-the-art\ncompetitors on GOT-10k, LaSOT, TrackingNet, OTB-2015, VOT-2018 and VOT-2019.\nMoreover, our SiamRCR runs at 65 FPS, far above the real-time requirement.",
          "link": "http://arxiv.org/abs/2105.11237",
          "publishedOn": "2021-07-20T02:04:46.771Z",
          "wordCount": null,
          "title": "SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking. (arXiv:2105.11237v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radoi_E/0/1/0/all/0/1\">Emilian Radoi</a>",
          "description": "The use of gait for person identification has important advantages such as\nbeing non-invasive, unobtrusive, not requiring cooperation and being less\nlikely to be obscured compared to other biometrics. Existing methods for gait\nrecognition require cooperative gait scenarios, in which a single person is\nwalking multiple times in a straight line in front of a camera. We aim to\naddress the hard challenges of real-world scenarios in which camera feeds\ncapture multiple people, who in most cases pass in front of the camera only\nonce. We address privacy concerns by using only the motion information of\nwalking individuals, with no identifiable appearance-based information. As\nsuch, we propose a novel weakly supervised learning framework, WildGait, which\nconsists of training a Spatio-Temporal Graph Convolutional Network on a large\nnumber of automatically annotated skeleton sequences obtained from raw,\nreal-world, surveillance streams to learn useful gait signatures. Our results\nshow that, with fine-tuning, we surpass in terms of recognition accuracy the\ncurrent state-of-the-art pose-based gait recognition solutions. Our proposed\nmethod is reliable in training gait recognition methods in unconstrained\nenvironments, especially in settings with scarce amounts of annotated data.",
          "link": "http://arxiv.org/abs/2105.05528",
          "publishedOn": "2021-07-20T02:04:46.705Z",
          "wordCount": null,
          "title": "WildGait: Learning Gait Representations from Raw Surveillance Streams. (arXiv:2105.05528v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1809.00970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lejeune_L/0/1/0/all/0/1\">Laurent Lejeune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grossrieder_J/0/1/0/all/0/1\">Jan Grossrieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1\">Raphael Sznitman</a>",
          "description": "Recent machine learning strategies for segmentation tasks have shown great\nability when trained on large pixel-wise annotated image datasets. It remains a\nmajor challenge however to aggregate such datasets, as the time and monetary\ncost associated with collecting extensive annotations is extremely high. This\nis particularly the case for generating precise pixel-wise annotations in video\nand volumetric image data. To this end, this work presents a novel framework to\nproduce pixel-wise segmentations using minimal supervision. Our method relies\non 2D point supervision, whereby a single 2D location within an object of\ninterest is provided on each image of the data. Our method then estimates the\nobject appearance in a semi-supervised fashion by learning\nobject-image-specific features and by using these in a semi-supervised learning\nframework. Our object model is then used in a graph-based optimization problem\nthat takes into account all provided locations and the image data in order to\ninfer the complete pixel-wise segmentation. In practice, we solve this\noptimally as a tracking problem using a K-shortest path approach. Both the\nobject model and segmentation are then refined iteratively to further improve\nthe final segmentation. We show that by collecting 2D locations using a gaze\ntracker, our approach can provide state-of-the-art segmentations on a range of\nobjects and image modalities (video and 3D volumes), and that these can then be\nused to train supervised machine learning classifiers.",
          "link": "http://arxiv.org/abs/1809.00970",
          "publishedOn": "2021-07-20T02:04:46.694Z",
          "wordCount": null,
          "title": "Iterative multi-path tracking for video and volume segmentation with sparse point supervision. (arXiv:1809.00970v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "Current out-of-distribution detection approaches usually present special\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\nproduce side effects (classification accuracy drop and slow/inefficient\ninferences). Recently, entropic out-of-distribution detection has been proposed\nas a seamless approach (i.e., a solution that avoids all the previously\nmentioned drawbacks). The entropic out-of-distribution detection solution\ncomprises the IsoMax loss for training and the entropic score for\nout-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in\nreplacement because swapping the SoftMax loss with the IsoMax loss requires no\nchanges in the model's architecture or training procedures/hyperparameters. In\nthis paper, we propose to perform what we call an isometrization of the\ndistances used in the IsoMax loss. Additionally, we propose to replace the\nentropic score with the minimum distance score. Our experiments showed that\nthese simple modifications increase out-of-distribution detection performance\nwhile keeping the solution seamless. Code available at\n$\\href{https://github.com/dlmacedo/entropic-out-of-distribution-detection}{\\text{entropic\nout-of-distribution detection}}$.",
          "link": "http://arxiv.org/abs/2105.14399",
          "publishedOn": "2021-07-20T02:04:46.693Z",
          "wordCount": null,
          "title": "Improving Entropic Out-of-Distribution Detection using Isometric Distances and the Minimum Distance Score. (arXiv:2105.14399v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xueying Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>",
          "description": "Automated surgical gesture recognition is of great importance in\nrobot-assisted minimally invasive surgery. However, existing methods assume\nthat training and testing data are from the same domain, which suffers from\nsevere performance degradation when a domain gap exists, such as the simulator\nand real robot. In this paper, we propose a novel unsupervised domain\nadaptation framework which can simultaneously transfer multi-modality\nknowledge, i.e., both kinematic and visual data, from simulator to real robot.\nIt remedies the domain gap with enhanced transferable features by using\ntemporal cues in videos, and inherent correlations in multi-modal towards\nrecognizing gesture. Specifically, we first propose an MDO-K to align\nkinematics, which exploits temporal continuity to transfer motion directions\nwith smaller gap rather than position values, relieving the adaptation burden.\nMoreover, we propose a KV-Relation-ATT to transfer the co-occurrence signals of\nkinematics and vision. Such features attended by correlation similarity are\nmore informative for enhancing domain-invariance of the model. Two feature\nalignment strategies benefit the model mutually during the end-to-end learning\nprocess. We extensively evaluate our method for gesture recognition using DESK\ndataset with peg transfer procedure. Results show that our approach recovers\nthe performance with great improvement gains, up to 12.91% in ACC and 20.16% in\nF1score without using any annotations in real robot.",
          "link": "http://arxiv.org/abs/2103.04075",
          "publishedOn": "2021-07-20T02:04:46.690Z",
          "wordCount": null,
          "title": "Domain Adaptive Robotic Gesture Recognition with Unsupervised Kinematic-Visual Data Alignment. (arXiv:2103.04075v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Songlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuehua Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>",
          "description": "With the identity information in face data more closely related to personal\ncredit and property security, people pay increasing attention to the protection\nof face data privacy. In different tasks, people have various requirements for\nface de-identification (De-ID), so we propose a systematical solution\ncompatible for these De-ID operations. Firstly, an attribute disentanglement\nand generative network is constructed to encode two parts of the face, which\nare the identity (facial features like mouth, nose and eyes) and expression\n(including expression, pose and illumination). Through face swapping, we can\nremove the original ID completely. Secondly, we add an adversarial vector\nmapping network to perturb the latent code of the face image, different from\nprevious traditional adversarial methods. Through this, we can construct\nunrestricted adversarial image to decrease ID similarity recognized by model.\nOur method can flexibly de-identify the face data in various ways and the\nprocessed images have high image quality.",
          "link": "http://arxiv.org/abs/2107.08581",
          "publishedOn": "2021-07-20T02:04:46.688Z",
          "wordCount": null,
          "title": "A Systematical Solution for Face De-identification. (arXiv:2107.08581v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.12434",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jiahang Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1\">Xinzhe Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ouyang_C/0/1/0/all/0/1\">Cheng Ouyang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campello_V/0/1/0/all/0/1\">Victor M. Campello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vesal_S/0/1/0/all/0/1\">Sulaiman Vesal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+RaviKumar_N/0/1/0/all/0/1\">Nishant RaviKumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yashu Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_G/0/1/0/all/0/1\">Gongning Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jingkun Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hongwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ly_B/0/1/0/all/0/1\">Buntheng Ly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sermesant_M/0/1/0/all/0/1\">Maxime Sermesant</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roth_H/0/1/0/all/0/1\">Holger Roth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiexiang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_X/0/1/0/all/0/1\">Xinghao Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xinyue Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Accurate computing, analysis and modeling of the ventricles and myocardium\nfrom medical images are important, especially in the diagnosis and treatment\nmanagement for patients suffering from myocardial infarction (MI). Late\ngadolinium enhancement (LGE) cardiac magnetic resonance (CMR) provides an\nimportant protocol to visualize MI. However, automated segmentation of LGE CMR\nis still challenging, due to the indistinguishable boundaries, heterogeneous\nintensity distribution and complex enhancement patterns of pathological\nmyocardium from LGE CMR. Furthermore, compared with the other sequences LGE CMR\nimages with gold standard labels are particularly limited, which represents\nanother obstacle for developing novel algorithms for automatic segmentation of\nLGE CMR. This paper presents the selective results from the Multi-Sequence\nCardiac MR (MS-CMR) Segmentation challenge, in conjunction with MICCAI 2019.\nThe challenge offered a data set of paired MS-CMR images, including auxiliary\nCMR sequences as well as LGE CMR, from 45 patients who underwent\ncardiomyopathy. It was aimed to develop new algorithms, as well as benchmark\nexisting ones for LGE CMR segmentation and compare them objectively. In\naddition, the paired MS-CMR images could enable algorithms to combine the\ncomplementary information from the other sequences for the segmentation of LGE\nCMR. Nine representative works were selected for evaluation and comparisons,\namong which three methods are unsupervised methods and the other six are\nsupervised. The results showed that the average performance of the nine methods\nwas comparable to the inter-observer variations. The success of these methods\nwas mainly attributed to the inclusion of the auxiliary sequences from the\nMS-CMR images, which provide important label information for the training of\ndeep neural networks.",
          "link": "http://arxiv.org/abs/2006.12434",
          "publishedOn": "2021-07-20T02:04:46.616Z",
          "wordCount": null,
          "title": "Cardiac Segmentation on Late Gadolinium Enhancement MRI: A Benchmark Study from Multi-Sequence Cardiac MR Segmentation Challenge. (arXiv:2006.12434v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nishimura_K/0/1/0/all/0/1\">Kazuya Nishimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyeonwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bise_R/0/1/0/all/0/1\">Ryoma Bise</a>",
          "description": "Cell detection is the task of detecting the approximate positions of cell\ncentroids from microscopy images. Recently, convolutional neural network-based\napproaches have achieved promising performance. However, these methods require\na certain amount of annotation for each imaging condition. This annotation is a\ntime-consuming and labor-intensive task. To overcome this problem, we propose a\nsemi-supervised cell-detection method that effectively uses a time-lapse\nsequence with one labeled image and the other images unlabeled. First, we train\na cell-detection network with a one-labeled image and estimate the unlabeled\nimages with the trained network. We then select high-confidence positions from\nthe estimations by tracking the detected cells from the labeled frame to those\nfar from it. Next, we generate pseudo-labels from the tracking results and\ntrain the network by using pseudo-labels. We evaluated our method for seven\nconditions of public datasets, and we achieved the best results relative to\nother semi-supervised methods. Our code is available at\nhttps://github.com/naivete5656/SCDTC",
          "link": "http://arxiv.org/abs/2107.08639",
          "publishedOn": "2021-07-20T02:04:46.615Z",
          "wordCount": null,
          "title": "Semi-supervised Cell Detection in Time-lapse Images Using Temporal Consistency. (arXiv:2107.08639v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kunlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shinan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jun Hou</a>",
          "description": "Video crowd localization is a crucial yet challenging task, which aims to\nestimate exact locations of human heads in the given crowded videos. To model\nspatial-temporal dependencies of human mobility, we propose a multi-focus\nGaussian neighbor attention (GNA), which can effectively exploit long-range\ncorrespondences while maintaining the spatial topological structure of the\ninput videos. In particular, our GNA can also capture the scale variation of\nhuman heads well using the equipped multi-focus mechanism. Based on the\nmulti-focus GNA, we develop a unified neural network called GNANet to\naccurately locate head centers in video clips by fully aggregating\nspatial-temporal information via a scene modeling module and a context\ncross-attention module. Moreover, to facilitate future researches in this\nfield, we introduce a large-scale crowded video benchmark named SenseCrowd,\nwhich consists of 60K+ frames captured in various surveillance scenarios and\n2M+ head annotations. Finally, we conduct extensive experiments on three\ndatasets including our SenseCrowd, and the experiment results show that the\nproposed method is capable to achieve state-of-the-art performance for both\nvideo crowd localization and counting. The code and the dataset will be\nreleased.",
          "link": "http://arxiv.org/abs/2107.08645",
          "publishedOn": "2021-07-20T02:04:46.614Z",
          "wordCount": null,
          "title": "Video Crowd Localization with Multi-focus Gaussian Neighbor Attention and a Large-Scale Benchmark. (arXiv:2107.08645v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2004.10141",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Ari_R/0/1/0/all/0/1\">Rami Ben-Ari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shpigel_M/0/1/0/all/0/1\">Mor Shpigel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azulai_O/0/1/0/all/0/1\">Ophir Azulai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzelay_U/0/1/0/all/0/1\">Udi Barzelay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rotman_D/0/1/0/all/0/1\">Daniel Rotman</a>",
          "description": "Classification of new class entities requires collecting and annotating\nhundreds or thousands of samples that is often prohibitively costly. Few-shot\nlearning suggests learning to classify new classes using just a few examples.\nOnly a small number of studies address the challenge of few-shot learning on\nspatio-temporal patterns such as videos. In this paper, we present the Temporal\nAware Embedding Network (TAEN) for few-shot action recognition, that learns to\nrepresent actions, in a metric space as a trajectory, conveying both short term\nsemantics and longer term connectivity between action parts. We demonstrate the\neffectiveness of TAEN on two few shot tasks, video classification and temporal\naction detection and evaluate our method on the Kinetics-400 and on ActivityNet\n1.2 few-shot benchmarks. With training of just a few fully connected layers we\nreach comparable results to prior art on both few shot video classification and\ntemporal detection tasks, while reaching state-of-the-art in certain scenarios.",
          "link": "http://arxiv.org/abs/2004.10141",
          "publishedOn": "2021-07-20T02:04:46.614Z",
          "wordCount": null,
          "title": "TAEN: Temporal Aware Embedding Network for Few-Shot Action Recognition. (arXiv:2004.10141v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Congram_B/0/1/0/all/0/1\">Benjamin Congram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barfoot_T/0/1/0/all/0/1\">Timothy D. Barfoot</a>",
          "description": "Visual Teach and Repeat has shown relative navigation is a robust and\nefficient solution for autonomous vision-based path following in difficult\nenvironments. Adding additional absolute sensors such as Global Navigation\nSatellite Systems (GNSS) has the potential to expand the domain of Visual Teach\nand Repeat to environments where the ability to visually localize is not\nguaranteed. Our method of lazy mapping and delaying estimation until a\npath-tracking error is needed avoids the need to estimate absolute states. As a\nresult, map optimization is not required and paths can be driven immediately\nafter being taught. We validate our approach on a real robot through an\nexperiment in a joint indoor-outdoor environment comprising 3.5km of autonomous\nroute repeating across a variety of lighting conditions. We achieve smooth\nerror signals throughout the runs despite large sections of dropout for each\nsensor.",
          "link": "http://arxiv.org/abs/2101.05107",
          "publishedOn": "2021-07-20T02:04:46.613Z",
          "wordCount": null,
          "title": "Relatively Lazy: Indoor-Outdoor Navigation Using Vision and GNSS. (arXiv:2101.05107v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shunyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chenxi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Libo Wang</a>",
          "description": "Semantic segmentation using fine-resolution remotely sensed images plays a\ncritical role in many practical applications, such as urban planning,\nenvironmental protection, natural and anthropogenic landscape monitoring, etc.\nHowever, the automation of semantic segmentation, i.e., automatic\ncategorization/labeling and segmentation is still a challenging task,\nparticularly for fine-resolution images with huge spatial and spectral\ncomplexity. Addressing such a problem represents an exciting research field,\nwhich paves the way for scene-level landscape pattern analysis and decision\nmaking. In this paper, we propose an approach for automatic land segmentation\nbased on the Feature Pyramid Network (FPN). As a classic architecture, FPN can\nbuild a feature pyramid with high-level semantics throughout. However,\nintrinsic defects in feature extraction and fusion hinder FPN from further\naggregating more discriminative features. Hence, we propose an Attention\nAggregation Module (AAM) to enhance multi-scale feature learning through\nattention-guided feature aggregation. Based on FPN and AAM, a novel framework\nnamed Attention Aggregation Feature Pyramid Network (A2-FPN) is developed for\nsemantic segmentation of fine-resolution remotely sensed images. Extensive\nexperiments conducted on three datasets demonstrate the effectiveness of our A2\n-FPN in segmentation accuracy. Code is available at\nhttps://github.com/lironui/A2-FPN.",
          "link": "http://arxiv.org/abs/2102.07997",
          "publishedOn": "2021-07-20T02:04:46.613Z",
          "wordCount": null,
          "title": "A2-FPN for Semantic Segmentation of Fine-Resolution Remotely Sensed Images. (arXiv:2102.07997v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11860",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Garg_A/0/1/0/all/0/1\">Aksh Garg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salehi_S/0/1/0/all/0/1\">Sana Salehi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rocca_M/0/1/0/all/0/1\">Marianna La Rocca</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garner_R/0/1/0/all/0/1\">Rachael Garner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duncan_D/0/1/0/all/0/1\">Dominique Duncan</a>",
          "description": "With COVID-19 cases rising rapidly, deep learning has emerged as a promising\ndiagnosis technique. However, identifying the most accurate models to\ncharacterize COVID-19 patients is challenging because comparing results\nobtained with different types of data and acquisition processes is non-trivial.\nIn this paper we designed, evaluated, and compared the performance of 20\nconvolutional neutral networks in classifying patients as COVID-19 positive,\nhealthy, or suffering from other pulmonary lung infections based on Chest CT\nscans, serving as the first to consider the EfficientNet family for COVID-19\ndiagnosis and employ intermediate activation maps for visualizing model\nperformance. All models are trained and evaluated in Python using 4173 Chest CT\nimages from the dataset entitled \"A COVID multiclass dataset of CT scans,\" with\n2168, 758, and 1247 images of patients that are COVID-19 positive, healthy, or\nsuffering from other pulmonary infections, respectively. EfficientNet-B5 was\nidentified as the best model with an F1 score of 0.9769+/-0.0046, accuracy of\n0.9759+/-0.0048, sensitivity of 0.9788+/-0.0055, specificity of\n0.9730+/-0.0057, and precision of 0.9751 +/- 0.0051. On an alternate 2-class\ndataset, EfficientNetB5 obtained an accuracy of 0.9845+/-0.0109, F1 score of\n0.9599+/-0.0251, sensitivity of 0.9682+/-0.0099, specificity of\n0.9883+/-0.0150, and precision of 0.9526 +/- 0.0523. Intermediate activation\nmaps and Gradient-weighted Class Activation Mappings offered\nhuman-interpretable evidence of the model's perception of ground-class\nopacities and consolidations, hinting towards a promising use-case of\nartificial intelligence-assisted radiology tools. With a prediction speed of\nunder 0.1 seconds on GPUs and 0.5 seconds on CPUs, our proposed model offers a\nrapid, scalable, and accurate diagnostic for COVID-19.",
          "link": "http://arxiv.org/abs/2012.11860",
          "publishedOn": "2021-07-20T02:04:46.612Z",
          "wordCount": null,
          "title": "Efficient and Visualizable Convolutional Neural Networks for COVID-19 Classification Using Chest CT. (arXiv:2012.11860v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plested_J/0/1/0/all/0/1\">Jo Plested</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuyang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>",
          "description": "The current standard for a variety of computer vision tasks using smaller\nnumbers of labelled training examples is to fine-tune from weights pre-trained\non a large image classification dataset such as ImageNet. The application of\ntransfer learning and transfer learning methods tends to be rigidly binary. A\nmodel is either pre-trained or not pre-trained. Pre-training a model either\nincreases performance or decreases it, the latter being defined as negative\ntransfer. Application of L2-SP regularisation that decays the weights towards\ntheir pre-trained values is either applied or all weights are decayed towards\n0. This paper re-examines these assumptions. Our recommendations are based on\nextensive empirical evaluation that demonstrate the application of a non-binary\napproach to achieve optimal results. (1) Achieving best performance on each\nindividual dataset requires careful adjustment of various transfer learning\nhyperparameters not usually considered, including number of layers to transfer,\ndifferent learning rates for different layers and different combinations of\nL2SP and L2 regularization. (2) Best practice can be achieved using a number of\nmeasures of how well the pre-trained weights fit the target dataset to guide\noptimal hyperparameters. We present methods for non-binary transfer learning\nincluding combining L2SP and L2 regularization and performing non-traditional\nfine-tuning hyperparameter searches. Finally we suggest heuristics for\ndetermining the optimal transfer learning hyperparameters. The benefits of\nusing a non-binary approach are supported by final results that come close to\nor exceed state of the art performance on a variety of tasks that have\ntraditionally been more difficult for transfer learning.",
          "link": "http://arxiv.org/abs/2107.08585",
          "publishedOn": "2021-07-20T02:04:46.611Z",
          "wordCount": null,
          "title": "Non-binary deep transfer learning for imageclassification. (arXiv:2107.08585v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lonkar_S/0/1/0/all/0/1\">Subodh Lonkar</a>",
          "description": "Over the centuries, humans have developed and acquired a number of ways to\ncommunicate. But hardly any of them can be as natural and instinctive as facial\nexpressions. On the other hand, neural networks have taken the world by storm.\nAnd no surprises, that the area of Computer Vision and the problem of facial\nexpressions recognitions hasn't remained untouched. Although a wide range of\ntechniques have been applied, achieving extremely high accuracies and preparing\nhighly robust FER systems still remains a challenge due to heterogeneous\ndetails in human faces. In this paper, we will be deep diving into implementing\na system for recognition of facial expressions (FER) by leveraging neural\nnetworks, and more specifically, Convolutional Neural Networks (CNNs). We adopt\nthe fundamental concepts of deep learning and computer vision with various\narchitectures, fine-tune it's hyperparameters and experiment with various\noptimization methods and demonstrate a state-of-the-art single-network-accuracy\nof 70.10% on the FER2013 dataset without using any additional training data.",
          "link": "http://arxiv.org/abs/2107.08640",
          "publishedOn": "2021-07-20T02:04:46.611Z",
          "wordCount": null,
          "title": "Facial Expressions Recognition with Convolutional Neural Networks. (arXiv:2107.08640v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.02638",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kanglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Gaofeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bozhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiang Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>",
          "description": "Facial attributes in StyleGAN generated images are entangled in the latent\nspace which makes it very difficult to independently control a specific\nattribute without affecting the others. Supervised attribute editing requires\nannotated training data which is difficult to obtain and limits the editable\nattributes to those with labels. Therefore, unsupervised attribute editing in\nan disentangled latent space is key to performing neat and versatile semantic\nface editing. In this paper, we present a new technique termed\nStructure-Texture Independent Architecture with Weight Decomposition and\nOrthogonal Regularization (STIA-WO) to disentangle the latent space for\nunsupervised semantic face editing. By applying STIA-WO to GAN, we have\ndeveloped a StyleGAN termed STGAN-WO which performs weight decomposition\nthrough utilizing the style vector to construct a fully controllable weight\nmatrix to regulate image synthesis, and employs orthogonal regularization to\nensure each entry of the style vector only controls one independent feature\nmatrix. To further disentangle the facial attributes, STGAN-WO introduces a\nstructure-texture independent architecture which utilizes two independently and\nidentically distributed (i.i.d.) latent vectors to control the synthesis of the\ntexture and structure components in a disentangled way. Unsupervised semantic\nediting is achieved by moving the latent code in the coarse layers along its\northogonal directions to change texture related attributes or changing the\nlatent code in the fine layers to manipulate structure related ones. We present\nexperimental results which show that our new STGAN-WO can achieve better\nattribute editing than state of the art methods.",
          "link": "http://arxiv.org/abs/2011.02638",
          "publishedOn": "2021-07-20T02:04:46.610Z",
          "wordCount": null,
          "title": "Towards Disentangling Latent Space for Unsupervised Semantic Face Editing. (arXiv:2011.02638v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengfei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>",
          "description": "In this paper, we develop face.evoLVe -- a comprehensive library that\ncollects and implements a wide range of popular deep learning-based methods for\nface recognition. First of all, face.evoLVe is composed of key components that\ncover the full process of face analytics, including face alignment, data\nprocessing, various backbones, losses, and alternatives with bags of tricks for\nimproving performance. Later, face.evoLVe supports multi-GPU training on top of\ndifferent deep learning platforms, such as PyTorch and PaddlePaddle, which\nfacilitates researchers to work on both large-scale datasets with millions of\nimages and low-shot counterparts with limited well-annotated data. More\nimportantly, along with face.evoLVe, images before & after alignment in the\ncommon benchmark datasets are released with source codes and trained models\nprovided. All these efforts lower the technical burdens in reproducing the\nexisting methods for comparison, while users of our library could focus on\ndeveloping advanced approaches more efficiently. Last but not least,\nface.evoLVe is well designed and vibrantly evolving, so that new face\nrecognition approaches can be easily plugged into our framework. Note that we\nhave used face.evoLVe to participate in a number of face recognition\ncompetitions and secured the first place. The version that supports PyTorch is\npublicly available at https://github.com/ZhaoJ9014/face.evoLVe.PyTorch and the\nPaddlePaddle version is available at\nhttps://github.com/ZhaoJ9014/face.evoLVe.PyTorch/tree/master/paddle.\nFace.evoLVe has been widely used for face analytics, receiving 2.4K stars and\n622 forks.",
          "link": "http://arxiv.org/abs/2107.08621",
          "publishedOn": "2021-07-20T02:04:46.609Z",
          "wordCount": null,
          "title": "Face.evoLVe: A High-Performance Face Recognition Library. (arXiv:2107.08621v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.09465",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_J/0/1/0/all/0/1\">Jue Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yu Chi Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tyagi_N/0/1/0/all/0/1\">Neelam Tyagi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rimner_A/0/1/0/all/0/1\">Andreas Rimner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_N/0/1/0/all/0/1\">Nancy Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deasy_J/0/1/0/all/0/1\">Joseph O. Deasy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Berry_S/0/1/0/all/0/1\">Sean Berry</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veeraraghavan_H/0/1/0/all/0/1\">Harini Veeraraghavan</a>",
          "description": "We developed a new joint probabilistic segmentation and image distribution\nmatching generative adversarial network (PSIGAN) for unsupervised domain\nadaptation (UDA) and multi-organ segmentation from magnetic resonance (MRI)\nimages. Our UDA approach models the co-dependency between images and their\nsegmentation as a joint probability distribution using a new structure\ndiscriminator. The structure discriminator computes structure of interest\nfocused adversarial loss by combining the generated pseudo MRI with\nprobabilistic segmentations produced by a simultaneously trained segmentation\nsub-network. The segmentation sub-network is trained using the pseudo MRI\nproduced by the generator sub-network. This leads to a cyclical optimization of\nboth the generator and segmentation sub-networks that are jointly trained as\npart of an end-to-end network. Extensive experiments and comparisons against\nmultiple state-of-the-art methods were done on four different MRI sequences\ntotalling 257 scans for generating multi-organ and tumor segmentation. The\nexperiments included, (a) 20 T1-weighted (T1w) in-phase mdixon and (b) 20\nT2-weighted (T2w) abdominal MRI for segmenting liver, spleen, left and right\nkidneys, (c) 162 T2-weighted fat suppressed head and neck MRI (T2wFS) for\nparotid gland segmentation, and (d) 75 T2w MRI for lung tumor segmentation. Our\nmethod achieved an overall average DSC of 0.87 on T1w and 0.90 on T2w for the\nabdominal organs, 0.82 on T2wFS for the parotid glands, and 0.77 on T2w MRI for\nlung tumors.",
          "link": "http://arxiv.org/abs/2007.09465",
          "publishedOn": "2021-07-20T02:04:46.608Z",
          "wordCount": null,
          "title": "PSIGAN: Joint probabilistic segmentation and image distribution matching for unpaired cross-modality adaptation based MRI segmentation. (arXiv:2007.09465v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">He Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1\">Richard P. Wildes</a>",
          "description": "Video predictive understanding encompasses a wide range of efforts that are\nconcerned with the anticipation of the unobserved future from the current as\nwell as historical video observations. Action prediction is a major sub-area of\nvideo predictive understanding and is the focus of this review. This sub-area\nhas two major subdivisions: early action recognition and future action\nprediction. Early action recognition is concerned with recognizing an ongoing\naction as soon as possible. Future action prediction is concerned with the\nanticipation of actions that follow those previously observed. In either case,\nthe \\textbf{\\textit{causal}} relationship between the past, current, and\npotential future information is the main focus. Various mathematical tools such\nas Markov Chains, Gaussian Processes, Auto-Regressive modeling, and Bayesian\nrecursive filtering are widely adopted jointly with computer vision techniques\nfor these two tasks. However, these approaches face challenges such as the\ncurse of dimensionality, poor generalization, and constraints from\ndomain-specific knowledge. Recently, structures that rely on deep convolutional\nneural networks and recurrent neural networks have been extensively proposed\nfor improving the performance of existing vision tasks, in general, and action\nprediction tasks, in particular. However, they have their own shortcomings, \\eg\nreliance on massive training data and lack of strong theoretical underpinnings.\nIn this survey, we start by introducing the major sub-areas of the broad area\nof video predictive understanding, which recently have received intensive\nattention and proven to have practical value. Next, a thorough review of\nvarious early action recognition and future action prediction algorithms are\nprovided with suitably organized divisions. Finally, we conclude our discussion\nwith future research directions.",
          "link": "http://arxiv.org/abs/2107.05140",
          "publishedOn": "2021-07-20T02:04:46.587Z",
          "wordCount": null,
          "title": "Review of Video Predictive Understanding: Early Action Recognition and Future Action Prediction. (arXiv:2107.05140v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yifan Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tianjun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1\">David Wipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We propose a hierarchical graph neural network (GNN) model that learns how to\ncluster a set of images into an unknown number of identities using a training\nset of images annotated with labels belonging to a disjoint set of identities.\nOur hierarchical GNN uses a novel approach to merge connected components\npredicted at each level of the hierarchy to form a new graph at the next level.\nUnlike fully unsupervised hierarchical clustering, the choice of grouping and\ncomplexity criteria stems naturally from supervision in the training set. The\nresulting method, Hi-LANDER, achieves an average of 54% improvement in F-score\nand 8% increase in Normalized Mutual Information (NMI) relative to current\nGNN-based clustering algorithms. Additionally, state-of-the-art GNN-based\nmethods rely on separate models to predict linkage probabilities and node\ndensities as intermediate steps of the clustering process. In contrast, our\nunified framework achieves a seven-fold decrease in computational cost. We\nrelease our training and inference code at\nhttps://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander.",
          "link": "http://arxiv.org/abs/2107.01319",
          "publishedOn": "2021-07-20T02:04:46.585Z",
          "wordCount": null,
          "title": "Learning Hierarchical Graph Neural Networks for Image Clustering. (arXiv:2107.01319v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06419",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jinke Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lv_P/0/1/0/all/0/1\">Peiqing Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haiying Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1\">Changfa Shi</a>",
          "description": "Background and objective: In this paper, a modified U-Net based framework is\npresented, which leverages techniques from Squeeze-and-Excitation (SE) block,\nAtrous Spatial Pyramid Pooling (ASPP) and residual learning for accurate and\nrobust liver CT segmentation, and the effectiveness of the proposed method was\ntested on two public datasets LiTS17 and SLiver07.\n\nMethods: A new network architecture called SAR-U-Net was designed. Firstly,\nthe SE block is introduced to adaptively extract image features after each\nconvolution in the U-Net encoder, while suppressing irrelevant regions, and\nhighlighting features of specific segmentation task; Secondly, ASPP was\nemployed to replace the transition layer and the output layer, and acquire\nmulti-scale image information via different receptive fields. Thirdly, to\nalleviate the degradation problem, the traditional convolution block was\nreplaced with the residual block and thus prompt the network to gain accuracy\nfrom considerably increased depth.\n\nResults: In the LiTS17 experiment, the mean values of Dice, VOE, RVD, ASD and\nMSD were 95.71, 9.52, -0.84, 1.54 and 29.14, respectively. Compared with other\nclosely related 2D-based models, the proposed method achieved the highest\naccuracy. In the experiment of the SLiver07, the mean values of Dice, VOE, RVD,\nASD and MSD were 97.31, 5.37, -1.08, 1.85 and 27.45, respectively. Compared\nwith other closely related models, the proposed method achieved the highest\nsegmentation accuracy except for the RVD.\n\nConclusion: The proposed model enables a great improvement on the accuracy\ncompared to 2D-based models, and its robustness in circumvent challenging\nproblems, such as small liver regions, discontinuous liver regions, and fuzzy\nliver boundaries, is also well demonstrated and validated.",
          "link": "http://arxiv.org/abs/2103.06419",
          "publishedOn": "2021-07-20T02:04:46.582Z",
          "wordCount": null,
          "title": "SAR-U-Net: squeeze-and-excitation block and atrous spatial pyramid pooling based residual U-Net for automatic liver segmentation in Computed Tomography. (arXiv:2103.06419v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Weimin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bo Yan</a>",
          "description": "Affective Analysis is not a single task, and the valence-arousal value,\nexpression class, and action unit can be predicted at the same time. Previous\nresearches did not pay enough attention to the entanglement and hierarchical\nrelation of these three facial attributes. We propose a novel model named\nfeature pyramid networks for multi-task affect analysis. The hierarchical\nfeatures are extracted to predict three labels and we apply a teacher-student\ntraining strategy to learn from pretrained single-task models. Extensive\nexperiment results demonstrate the proposed model outperforms other models.\nThis is a submission to The 2nd Workshop and Competition on Affective Behavior\nAnalysis in the wild (ABAW). The code and model are available for research\npurposes at https://github.com/ryanhe312/ABAW2-FPNMAA.",
          "link": "http://arxiv.org/abs/2107.03670",
          "publishedOn": "2021-07-20T02:04:46.580Z",
          "wordCount": null,
          "title": "Feature Pyramid Network for Multi-task Affective Analysis. (arXiv:2107.03670v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_E/0/1/0/all/0/1\">Erin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anirudh Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1\">Meher Anand Kasam</a>",
          "description": "Data imbalance is a ubiquitous problem in machine learning. In large scale\ncollected and annotated datasets, data imbalance is either mitigated manually\nby undersampling frequent classes and oversampling rare classes, or planned for\nwith imputation and augmentation techniques. In both cases balancing data\nrequires labels. In other words, only annotated data can be balanced.\nCollecting fully annotated datasets is challenging, especially for large scale\nsatellite systems such as the unlabeled NASA's 35 PB Earth Imagery dataset.\nAlthough the NASA Earth Imagery dataset is unlabeled, there are implicit\nproperties of the data source that we can rely on to hypothesize about its\nimbalance, such as distribution of land and water in the case of the Earth's\nimagery. We present a new iterative method to balance unlabeled data. Our\nmethod utilizes image embeddings as a proxy for image labels that can be used\nto balance data, and ultimately when trained increases overall accuracy.",
          "link": "http://arxiv.org/abs/2107.03227",
          "publishedOn": "2021-07-20T02:04:46.580Z",
          "wordCount": null,
          "title": "Scalable Data Balancing for Unlabeled Satellite Imagery. (arXiv:2107.03227v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kotseruba_I/0/1/0/all/0/1\">Iuliia Kotseruba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papagelis_M/0/1/0/all/0/1\">Manos Papagelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsotsos_J/0/1/0/all/0/1\">John K. Tsotsos</a>",
          "description": "This work aims to study the dynamic between research in the industry and\nacademia in computer vision. The results are demonstrated on a set of top-5\nvision conferences that are representative of the field. Since data for such\nanalysis was not readily available, significant effort was spent on gathering\nand processing meta-data from the original publications. First, this study\nquantifies the share of industry-sponsored research. Specifically, it shows\nthat the proportion of papers published by industry-affiliated researchers is\nincreasing and that more academics join companies or collaborate with them.\nNext, the possible impact of industry presence is further explored, namely in\nthe distribution of research topics and citation patterns. The results indicate\nthat the distribution of the research topics is similar in industry and\nacademic papers. However, there is a strong preference towards citing industry\npapers. Finally, possible reasons for citation bias, such as code availability\nand influence, are investigated.",
          "link": "http://arxiv.org/abs/2107.04902",
          "publishedOn": "2021-07-20T02:04:46.579Z",
          "wordCount": null,
          "title": "Industry and Academic Research in Computer Vision. (arXiv:2107.04902v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>",
          "description": "Generalization to out-of-distribution (OOD) data is a capability natural to\nhumans yet challenging for machines to reproduce. This is because most learning\nalgorithms strongly rely on the i.i.d.~assumption on source/target data, which\nis often violated in practice due to domain shift. Domain generalization (DG)\naims to achieve OOD generalization by using only source data for model\nlearning. Since first introduced in 2011, research in DG has made great\nprogresses. In particular, intensive research in this topic has led to a broad\nspectrum of methodologies, e.g., those based on domain alignment,\nmeta-learning, data augmentation, or ensemble learning, just to name a few; and\nhas covered various vision applications such as object recognition,\nsegmentation, action recognition, and person re-identification. In this paper,\nfor the first time a comprehensive literature review is provided to summarize\nthe developments in DG for computer vision over the past decade. Specifically,\nwe first cover the background by formally defining DG and relating it to other\nresearch fields like domain adaptation and transfer learning. Second, we\nconduct a thorough review into existing methods and present a categorization\nbased on their methodologies and motivations. Finally, we conclude this survey\nwith insights and discussions on future research directions.",
          "link": "http://arxiv.org/abs/2103.02503",
          "publishedOn": "2021-07-20T02:04:46.578Z",
          "wordCount": null,
          "title": "Domain Generalization in Vision: A Survey. (arXiv:2103.02503v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvirul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1\">Muhammad Imran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1\">Ferda Ofli</a>",
          "description": "Images shared on social media help crisis managers gain situational awareness\nand assess incurred damages, among other response tasks. As the volume and\nvelocity of such content are typically high, real-time image classification has\nbecome an urgent need for a faster disaster response. Recent advances in\ncomputer vision and deep neural networks have enabled the development of models\nfor real-time image classification for a number of tasks, including detecting\ncrisis incidents, filtering irrelevant images, classifying images into specific\nhumanitarian categories, and assessing the severity of the damage. To develop\nrobust real-time models, it is necessary to understand the capability of the\npublicly available pre-trained models for these tasks, which remains to be\nunder-explored in the crisis informatics literature. In this study, we address\nsuch limitations by investigating ten different network architectures for four\ndifferent tasks using the largest publicly available datasets for these tasks.\nWe also explore various data augmentation strategies, semi-supervised\ntechniques, and a multitask learning setup. In our extensive experiments, we\nachieve promising results.",
          "link": "http://arxiv.org/abs/2104.04184",
          "publishedOn": "2021-07-20T02:04:46.578Z",
          "wordCount": null,
          "title": "Robust Training of Social Media Image Classification Models for Rapid Disaster Response. (arXiv:2104.04184v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02739",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Han_S/0/1/0/all/0/1\">Sukjin Han</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Schulman_E/0/1/0/all/0/1\">Eric H. Schulman</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Ramakrishnan_S/0/1/0/all/0/1\">Santhosh Ramakrishnan</a>",
          "description": "Many differentiated products have key attributes that are unstructured and\nthus high-dimensional (e.g., design, text). Instead of treating unstructured\nattributes as unobservables in economic models, quantifying them can be\nimportant to answer interesting economic questions. To propose an analytical\nframework for this type of products, this paper considers one of the simplest\ndesign products -- fonts -- and investigates merger and product differentiation\nusing an original dataset from the world's largest online marketplace for\nfonts. We quantify font shapes by constructing embeddings from a deep\nconvolutional neural network. Each embedding maps a font's shape onto a\nlow-dimensional vector. In the resulting product space, designers are assumed\nto engage in Hotelling-type spatial competition. From the image embeddings, we\nconstruct two alternative measures that capture the degree of design\ndifferentiation. We then study the causal effects of a merger on the merging\nfirm's creative decisions using the constructed measures in a synthetic control\nmethod. We find that the merger causes the merging firm to increase the visual\nvariety of font design. Notably, such effects are not captured when using\ntraditional measures for product offerings (e.g., specifications and the number\nof products) constructed from structured data.",
          "link": "http://arxiv.org/abs/2107.02739",
          "publishedOn": "2021-07-20T02:04:46.577Z",
          "wordCount": null,
          "title": "Shapes as Product Differentiation: Neural Network Embedding in the Analysis of Markets for Fonts. (arXiv:2107.02739v1 [econ.EM] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Linqing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "In this paper, we propose a similarity-aware fusion network (SAFNet) to\nadaptively fuse 2D images and 3D point clouds for 3D semantic segmentation.\nExisting fusion-based methods achieve remarkable performances by integrating\ninformation from multiple modalities. However, they heavily rely on the\ncorrespondence between 2D pixels and 3D points by projection and can only\nperform the information fusion in a fixed manner, and thus their performances\ncannot be easily migrated to a more realistic scenario where the collected data\noften lack strict pair-wise features for prediction. To address this, we employ\na late fusion strategy where we first learn the geometric and contextual\nsimilarities between the input and back-projected (from 2D pixels) point clouds\nand utilize them to guide the fusion of two modalities to further exploit\ncomplementary information. Specifically, we employ a geometric similarity\nmodule (GSM) to directly compare the spatial coordinate distributions of\npair-wise 3D neighborhoods, and a contextual similarity module (CSM) to\naggregate and compare spatial contextual information of corresponding central\npoints. The two proposed modules can effectively measure how much image\nfeatures can help predictions, enabling the network to adaptively adjust the\ncontributions of two modalities to the final prediction of each point.\nExperimental results on the ScanNetV2 benchmark demonstrate that SAFNet\nsignificantly outperforms existing state-of-the-art fusion-based approaches\nacross various data integrity.",
          "link": "http://arxiv.org/abs/2107.01579",
          "publishedOn": "2021-07-20T02:04:46.576Z",
          "wordCount": null,
          "title": "Similarity-Aware Fusion Network for 3D Semantic Segmentation. (arXiv:2107.01579v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mihai_D/0/1/0/all/0/1\">Daniela Mihai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>",
          "description": "We present a bottom-up differentiable relaxation of the process of drawing\npoints, lines and curves into a pixel raster. Our approach arises from the\nobservation that rasterising a pixel in an image given parameters of a\nprimitive can be reformulated in terms of the primitive's distance transform,\nand then relaxed to allow the primitive's parameters to be learned. This\nrelaxation allows end-to-end differentiable programs and deep networks to be\nlearned and optimised and provides several building blocks that allow control\nover how a compositional drawing process is modelled. We emphasise the\nbottom-up nature of our proposed approach, which allows for drawing operations\nto be composed in ways that can mimic the physical reality of drawing rather\nthan being tied to, for example, approaches in modern computer graphics. With\nthe proposed approach we demonstrate how sketches can be generated by directly\noptimising against photographs and how auto-encoders can be built to transform\nrasterised handwritten digits into vectors without supervision. Extensive\nexperimental results highlight the power of this approach under different\nmodelling assumptions for drawing tasks.",
          "link": "http://arxiv.org/abs/2103.16194",
          "publishedOn": "2021-07-20T02:04:46.572Z",
          "wordCount": null,
          "title": "Differentiable Drawing and Sketching. (arXiv:2103.16194v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12056",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1\">Changfa Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1\">Min Xian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xiancheng Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haotian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_H/0/1/0/all/0/1\">Heng-Da Cheng</a>",
          "description": "Liver segmentation from abdominal CT images is an essential step for liver\ncancer computer-aided diagnosis and surgical planning. However, both the\naccuracy and robustness of existing liver segmentation methods cannot meet the\nrequirements of clinical applications. In particular, for the common clinical\ncases where the liver tissue contains major pathology, current segmentation\nmethods show poor performance. In this paper, we propose a novel low-rank\ntensor decomposition (LRTD) based multi-atlas segmentation (MAS) framework that\nachieves accurate and robust pathological liver segmentation of CT images.\nFirstly, we propose a multi-slice LRTD scheme to recover the underlying\nlow-rank structure embedded in 3D medical images. It performs the LRTD on small\nimage segments consisting of multiple consecutive image slices. Then, we\npresent an LRTD-based atlas construction method to generate tumor-free liver\natlases that mitigates the performance degradation of liver segmentation due to\nthe presence of tumors. Finally, we introduce an LRTD-based MAS algorithm to\nderive patient-specific liver atlases for each test image, and to achieve\naccurate pairwise image registration and label propagation. Extensive\nexperiments on three public databases of pathological liver cases validate the\neffectiveness of the proposed method. Both qualitative and quantitative results\ndemonstrate that, in the presence of major pathology, the proposed method is\nmore accurate and robust than state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2102.12056",
          "publishedOn": "2021-07-20T02:04:46.561Z",
          "wordCount": null,
          "title": "Multi-Slice Low-Rank Tensor Decomposition Based Multi-Atlas Segmentation: Application to Automatic Pathological Liver CT Segmentation. (arXiv:2102.12056v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chopin_B/0/1/0/all/0/1\">Baptiste Chopin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otberdout_N/0/1/0/all/0/1\">Naima Otberdout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daoudi_M/0/1/0/all/0/1\">Mohamed Daoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartolo_A/0/1/0/all/0/1\">Angela Bartolo</a>",
          "description": "Human motion prediction aims to forecast future human poses given a prior\npose sequence. The discontinuity of the predicted motion and the performance\ndeterioration in long-term horizons are still the main challenges encountered\nin current literature. In this work, we tackle these issues by using a compact\nmanifold-valued representation of human motion. Specifically, we model the\ntemporal evolution of the 3D human poses as trajectory, what allows us to map\nhuman motions to single points on a sphere manifold. To learn these\nnon-Euclidean representations, we build a manifold-aware Wasserstein generative\nadversarial model that captures the temporal and spatial dependencies of human\nmotion through different losses. Extensive experiments show that our approach\noutperforms the state-of-the-art on CMU MoCap and Human 3.6M datasets. Our\nqualitative results show the smoothness of the predicted motions.",
          "link": "http://arxiv.org/abs/2105.08715",
          "publishedOn": "2021-07-20T02:04:46.557Z",
          "wordCount": null,
          "title": "Human Motion Prediction Using Manifold-Aware Wasserstein GAN. (arXiv:2105.08715v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyeonwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishimura_K/0/1/0/all/0/1\">Kazuya Nishimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_K/0/1/0/all/0/1\">Kazuhide Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bise_R/0/1/0/all/0/1\">Ryoma Bise</a>",
          "description": "The domain shift problem is an important issue in automatic cell detection. A\ndetection network trained with training data under a specific condition (source\ndomain) may not work well in data under other conditions (target domain). We\npropose an unsupervised domain adaptation method for cell detection using the\npseudo-cell-position heatmap, where a cell centroid becomes a peak with a\nGaussian distribution in the map. In the prediction result for the target\ndomain, even if a peak location is correct, the signal distribution around the\npeak often has anon-Gaussian shape. The pseudo-cell-position heatmap is\nre-generated using the peak positions in the predicted heatmap to have a clear\nGaussian shape. Our method selects confident pseudo-cell-position heatmaps\nusing a Bayesian network and adds them to the training data in the next\niteration. The method can incrementally extend the domain from the source\ndomain to the target domain in a semi-supervised manner. In the experiments\nusing 8 combinations of domains, the proposed method outperformed the existing\ndomain adaptation methods.",
          "link": "http://arxiv.org/abs/2107.08653",
          "publishedOn": "2021-07-20T02:04:46.488Z",
          "wordCount": null,
          "title": "Cell Detection in Domain Shift Problem Using Pseudo-Cell-Position Heatmap. (arXiv:2107.08653v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2005.04401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bui_K/0/1/0/all/0/1\">Kevin Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_F/0/1/0/all/0/1\">Fredrick Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yifei Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jack Xin</a>",
          "description": "In a class of piecewise-constant image segmentation models, we propose to\nincorporate a weighted difference of anisotropic and isotropic total variation\n(AITV) to regularize the partition boundaries in an image. In particular, we\nreplace the total variation regularization in the Chan-Vese segmentation model\nand a fuzzy region competition model by the proposed AITV. To deal with the\nnonconvex nature of AITV, we apply the difference-of-convex algorithm (DCA), in\nwhich the subproblems can be minimized by the primal-dual hybrid gradient\nmethod with linesearch. The convergence of the DCA scheme is analyzed. In\naddition, a generalization to color image segmentation is discussed. In the\nnumerical experiments, we compare the proposed models with the classic convex\napproaches and the two-stage segmentation methods (smoothing and then\nthresholding) on various images, showing that our models are effective in image\nsegmentation and robust with respect to impulsive noises.",
          "link": "http://arxiv.org/abs/2005.04401",
          "publishedOn": "2021-07-20T02:04:46.483Z",
          "wordCount": null,
          "title": "A Weighted Difference of Anisotropic and Isotropic Total Variation for Relaxed Mumford-Shah Color and Multiphase Image Segmentation. (arXiv:2005.04401v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.07335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md. Mushfiqur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abedin_T/0/1/0/all/0/1\">Thasin Abedin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prottoy_K/0/1/0/all/0/1\">Khondokar S. S. Prottoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moshruba_A/0/1/0/all/0/1\">Ayana Moshruba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_F/0/1/0/all/0/1\">Fazlul Hasan Siddiqui</a>",
          "description": "Video captioning, i.e. the task of generating captions from video sequences\ncreates a bridge between the Natural Language Processing and Computer Vision\ndomains of computer science. The task of generating a semantically accurate\ndescription of a video is quite complex. Considering the complexity, of the\nproblem, the results obtained in recent research works are praiseworthy.\nHowever, there is plenty of scope for further investigation. This paper\naddresses this scope and proposes a novel solution. Most video captioning\nmodels comprise two sequential/recurrent layers - one as a video-to-context\nencoder and the other as a context-to-caption decoder. This paper proposes a\nnovel architecture, namely Semantically Sensible Video Captioning (SSVC) which\nmodifies the context generation mechanism by using two novel approaches -\n\"stacked attention\" and \"spatial hard pull\". As there are no exclusive metrics\nfor evaluating video captioning models, we emphasize both quantitative and\nqualitative analysis of our model. Hence, we have used the BLEU scoring metric\nfor quantitative analysis and have proposed a human evaluation metric for\nqualitative analysis, namely the Semantic Sensibility (SS) scoring metric. SS\nScore overcomes the shortcomings of common automated scoring metrics. This\npaper reports that the use of the aforementioned novelties improves the\nperformance of state-of-the-art architectures.",
          "link": "http://arxiv.org/abs/2009.07335",
          "publishedOn": "2021-07-20T02:04:46.482Z",
          "wordCount": null,
          "title": "Video captioning with stacked attention and semantic hard pull. (arXiv:2009.07335v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Di Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaohui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dantcheva_A/0/1/0/all/0/1\">Antitza Dantcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garattoni_L/0/1/0/all/0/1\">Lorenzo Garattoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francesca_G/0/1/0/all/0/1\">Gianpiero Francesca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Francois Bremond</a>",
          "description": "Action recognition based on skeleton data has recently witnessed increasing\nattention and progress. State-of-the-art approaches adopting Graph\nConvolutional networks (GCNs) can effectively extract features on human\nskeletons relying on the pre-defined human topology. Despite associated\nprogress, GCN-based methods have difficulties to generalize across domains,\nespecially with different human topological structures. In this context, we\nintroduce UNIK, a novel skeleton-based action recognition method that is not\nonly effective to learn spatio-temporal features on human skeleton sequences\nbut also able to generalize across datasets. This is achieved by learning an\noptimal dependency matrix from the uniform distribution based on a multi-head\nattention mechanism. Subsequently, to study the cross-domain generalizability\nof skeleton-based action recognition in real-world videos, we re-evaluate\nstate-of-the-art approaches as well as the proposed UNIK in light of a novel\nPosetics dataset. This dataset is created from Kinetics-400 videos by\nestimating, refining and filtering poses. We provide an analysis on how much\nperformance improves on smaller benchmark datasets after pre-training on\nPosetics for the action classification task. Experimental results show that the\nproposed UNIK, with pre-training on Posetics, generalizes well and outperforms\nstate-of-the-art when transferred onto four target action classification\ndatasets: Toyota Smarthome, Penn Action, NTU-RGB+D 60 and NTU-RGB+D 120.",
          "link": "http://arxiv.org/abs/2107.08580",
          "publishedOn": "2021-07-20T02:04:46.481Z",
          "wordCount": null,
          "title": "UNIK: A Unified Framework for Real-world Skeleton-based Action Recognition. (arXiv:2107.08580v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.10934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yiyuan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuecheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weijie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yunxiang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Rong Xiong</a>",
          "description": "Place recognition is indispensable for a drift-free localization system. Due\nto the variations of the environment, place recognition using single-modality\nhas limitations. In this paper, we propose a bi-modal place recognition method,\nwhich can extract a compound global descriptor from the two modalities, vision\nand LiDAR. Specifically, we first build the elevation image generated from 3D\npoints as a structural representation. Then, we derive the correspondences\nbetween 3D points and image pixels that are further used in merging the\npixel-wise visual features into the elevation map grids. In this way, we fuse\nthe structural features and visual features in the consistent bird-eye view\nframe, yielding a semantic representation, namely CORAL. And the whole network\nis called CORAL-VLAD. Comparisons on the Oxford RobotCar show that CORAL-VLAD\nhas superior performance against other state-of-the-art methods. We also\ndemonstrate that our network can be generalized to other scenes and sensor\nconfigurations on cross-city datasets.",
          "link": "http://arxiv.org/abs/2011.10934",
          "publishedOn": "2021-07-20T02:04:46.481Z",
          "wordCount": null,
          "title": "CORAL: Colored structural representation for bi-modal place recognition. (arXiv:2011.10934v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guoping Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xingrong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xinwei He</a>",
          "description": "Medical image segmentation plays an essential role in developing\ncomputer-assisted diagnosis and therapy systems, yet still faces many\nchallenges. In the past few years, the popular encoder-decoder architectures\nbased on CNNs (e.g., U-Net) have been successfully applied in the task of\nmedical image segmentation. However, due to the locality of convolution\noperations, they demonstrate limitations in learning global context and\nlong-range spatial relations. Recently, several researchers try to introduce\ntransformers to both the encoder and decoder components with promising results,\nbut the efficiency requires further improvement due to the high computational\ncomplexity of transformers. In this paper, we propose LeViT-UNet, which\nintegrates a LeViT Transformer module into the U-Net architecture, for fast and\naccurate medical image segmentation. Specifically, we use LeViT as the encoder\nof the LeViT-UNet, which better trades off the accuracy and efficiency of the\nTransformer block. Moreover, multi-scale feature maps from transformer blocks\nand convolutional blocks of LeViT are passed into the decoder via\nskip-connection, which can effectively reuse the spatial information of the\nfeature maps. Our experiments indicate that the proposed LeViT-UNet achieves\nbetter performance comparing to various competing methods on several\nchallenging medical image segmentation benchmarks including Synapse and ACDC.\nCode and models will be publicly available at\nhttps://github.com/apple1986/LeViT_UNet.",
          "link": "http://arxiv.org/abs/2107.08623",
          "publishedOn": "2021-07-20T02:04:46.480Z",
          "wordCount": null,
          "title": "LeViT-UNet: Make Faster Encoders with Transformer for Medical Image Segmentation. (arXiv:2107.08623v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Tianyuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1\">Chang Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1\">Ruining Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuanhan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiachen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Aadarsh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Shunxing Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fogo_A/0/1/0/all/0/1\">Agnes B. Fogo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A.Landman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Catie Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haichun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>",
          "description": "Unsupervised learning algorithms (e.g., self-supervised learning,\nauto-encoder, contrastive learning) allow deep learning models to learn\neffective image representations from large-scale unlabeled data. In medical\nimage analysis, even unannotated data can be difficult to obtain for individual\nlabs. Fortunately, national-level efforts have been made to provide efficient\naccess to obtain biomedical image data from previous scientific publications.\nFor instance, NIH has launched the Open-i search engine that provides a\nlarge-scale image database with free access. However, the images in scientific\npublications consist of a considerable amount of compound figures with\nsubplots. To extract and curate individual subplots, many different compound\nfigure separation approaches have been developed, especially with the recent\nadvances in deep learning. However, previous approaches typically required\nresource extensive bounding box annotation to train detection models. In this\npaper, we propose a simple compound figure separation (SimCFS) framework that\nuses weak classification annotations from individual images. Our technical\ncontribution is three-fold: (1) we introduce a new side loss that is designed\nfor compound figure separation; (2) we introduce an intra-class image\naugmentation method to simulate hard cases; (3) the proposed framework enables\nan efficient deployment to new classes of images, without requiring resource\nextensive bounding box annotations. From the results, the SimCFS achieved a new\nstate-of-the-art performance on the ImageCLEF 2016 Compound Figure Separation\nDatabase. The source code of SimCFS is made publicly available at\nhttps://github.com/hrlblab/ImageSeperation.",
          "link": "http://arxiv.org/abs/2107.08650",
          "publishedOn": "2021-07-20T02:04:46.480Z",
          "wordCount": null,
          "title": "Compound Figure Separation of Biomedical Images with Side Loss. (arXiv:2107.08650v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ng_Y/0/1/0/all/0/1\">Yan Bin Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1\">Basura Fernando</a>",
          "description": "We present a new architecture for human action forecasting from videos. A\ntemporal recurrent encoder captures temporal information of input videos while\na self-attention model is used to attend on relevant feature dimensions of the\ninput space. To handle temporal variations in observed video data, a feature\nmasking techniques is employed. We classify observed actions accurately using\nan auxiliary classifier which helps to understand what has happened so far.\nThen the decoder generates actions for the future based on the output of the\nrecurrent encoder and the self-attention model. Experimentally, we validate\neach component of our architecture where we see that the impact of\nself-attention to identify relevant feature dimensions, temporal masking, and\nobserved auxiliary classifier. We evaluate our method on two standard action\nforecasting benchmarks and obtain state-of-the-art results.",
          "link": "http://arxiv.org/abs/2107.08579",
          "publishedOn": "2021-07-20T02:04:46.479Z",
          "wordCount": null,
          "title": "Action Forecasting with Feature-wise Self-Attention. (arXiv:2107.08579v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08470",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ho_Y/0/1/0/all/0/1\">Yung-Han Ho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_C/0/1/0/all/0/1\">Chih-Chun Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_W/0/1/0/all/0/1\">Wen-Hsiao Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hang_H/0/1/0/all/0/1\">Hsueh-Ming Hang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Domanski_M/0/1/0/all/0/1\">Marek Domanski</a>",
          "description": "This paper introduces an end-to-end learned image compression system, termed\nANFIC, based on Augmented Normalizing Flows (ANF). ANF is a new type of flow\nmodel, which stacks multiple variational autoencoders (VAE) for greater model\nexpressiveness. The VAE-based image compression has gone mainstream, showing\npromising compression performance. Our work presents the first attempt to\nleverage VAE-based compression in a flow-based framework. ANFIC advances\nfurther compression efficiency by stacking and extending hierarchically\nmultiple VAE's. The invertibility of ANF, together with our training\nstrategies, enables ANFIC to support a wide range of quality levels without\nchanging the encoding and decoding networks. Extensive experimental results\nshow that in terms of PSNR-RGB, ANFIC performs comparably to or better than the\nstate-of-the-art learned image compression. Moreover, it performs close to VVC\nintra coding, from low-rate compression up to nearly-lossless compression. In\nparticular, ANFIC achieves the state-of-the-art performance, when extended with\nconditional convolution for variable rate compression with a single model.",
          "link": "http://arxiv.org/abs/2107.08470",
          "publishedOn": "2021-07-20T02:04:46.478Z",
          "wordCount": null,
          "title": "ANFIC: Image Compression Using Augmented Normalizing Flows. (arXiv:2107.08470v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.10939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivgi_M/0/1/0/all/0/1\">Maor Ivgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benny_Y/0/1/0/all/0/1\">Yaniv Benny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_David_A/0/1/0/all/0/1\">Avichai Ben-David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "Generating images from scene graphs is a challenging task that attracted\nsubstantial interest recently. Prior works have approached this task by\ngenerating an intermediate layout description of the target image. However, the\nrepresentation of each object in the layout was generated independently, which\nresulted in high overlap, low coverage, and an overall blurry layout. We\npropose a novel method that alleviates these issues by generating the entire\nlayout description gradually to improve inter-object dependency. We empirically\nshow on the COCO-STUFF dataset that our approach improves the quality of both\nthe intermediate layout and the final image. Our approach improves the layout\ncoverage by almost 20 points and drops object overlap to negligible amounts.",
          "link": "http://arxiv.org/abs/2009.10939",
          "publishedOn": "2021-07-20T02:04:46.127Z",
          "wordCount": null,
          "title": "Scene Graph to Image Generation with Contextualized Object Layout Refinement. (arXiv:2009.10939v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05318",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Skandarani_Y/0/1/0/all/0/1\">Youssef Skandarani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jodoin_P/0/1/0/all/0/1\">Pierre-Marc Jodoin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lalande_A/0/1/0/all/0/1\">Alain Lalande</a>",
          "description": "Generative Adversarial Networks (GANs) have become increasingly powerful,\ngenerating mind-blowing photorealistic images that mimic the content of\ndatasets they were trained to replicate. One recurrent theme in medical imaging\nis whether GANs can also be effective at generating workable medical data as\nthey are for generating realistic RGB images. In this paper, we perform a\nmulti-GAN and multi-application study to gauge the benefits of GANs in medical\nimaging. We tested various GAN architectures from basic DCGAN to more\nsophisticated style-based GANs on three medical imaging modalities and organs\nnamely : cardiac cine-MRI, liver CT and RGB retina images. GANs were trained on\nwell-known and widely utilized datasets from which their FID score were\ncomputed to measure the visual acuity of their generated images. We further\ntested their usefulness by measuring the segmentation accuracy of a U-Net\ntrained on these generated images.\n\nResults reveal that GANs are far from being equal as some are ill-suited for\nmedical imaging applications while others are much better off. The\ntop-performing GANs are capable of generating realistic-looking medical images\nby FID standards that can fool trained experts in a visual Turing test and\ncomply to some metrics. However, segmentation results suggests that no GAN is\ncapable of reproducing the full richness of a medical datasets.",
          "link": "http://arxiv.org/abs/2105.05318",
          "publishedOn": "2021-07-20T02:04:46.096Z",
          "wordCount": null,
          "title": "GANs for Medical Image Synthesis: An Empirical Study. (arXiv:2105.05318v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Transformer recently has shown encouraging progresses in computer vision. In\nthis work, we present new baselines by improving the original Pyramid Vision\nTransformer (abbreviated as PVTv1) by adding three designs, including (1)\noverlapping patch embedding, (2) convolutional feed-forward networks, and (3)\nlinear complexity attention layers.\n\nWith these modifications, our PVTv2 significantly improves PVTv1 on three\ntasks e.g., classification, detection, and segmentation. Moreover, PVTv2\nachieves comparable or better performances than recent works such as Swin\nTransformer. We hope this work will facilitate state-of-the-art Transformer\nresearches in computer vision. Code is available at\nhttps://github.com/whai362/PVT .",
          "link": "http://arxiv.org/abs/2106.13797",
          "publishedOn": "2021-07-20T02:04:46.096Z",
          "wordCount": null,
          "title": "PVTv2: Improved Baselines with Pyramid Vision Transformer. (arXiv:2106.13797v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhengping Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>",
          "description": "In deep model compression, the recent finding \"Lottery Ticket Hypothesis\"\n(LTH) (Frankle & Carbin, 2018) pointed out that there could exist a winning\nticket (i.e., a properly pruned sub-network together with original weight\ninitialization) that can achieve competitive performance than the original\ndense network. However, it is not easy to observe such winning property in many\nscenarios, where for example, a relatively large learning rate is used even if\nit benefits training the original dense model. In this work, we investigate the\nunderlying condition and rationale behind the winning property, and find that\nthe underlying reason is largely attributed to the correlation between\ninitialized weights and final-trained weights when the learning rate is not\nsufficiently large. Thus, the existence of winning property is correlated with\nan insufficient DNN pretraining, and is unlikely to occur for a well-trained\nDNN. To overcome this limitation, we propose the \"pruning & fine-tuning\" method\nthat consistently outperforms lottery ticket sparse training under the same\npruning algorithm and the same total training epochs. Extensive experiments\nover multiple deep models (VGG, ResNet, MobileNet-v2) on different datasets\nhave been conducted to justify our proposals.",
          "link": "http://arxiv.org/abs/2102.11068",
          "publishedOn": "2021-07-20T02:04:46.094Z",
          "wordCount": null,
          "title": "Lottery Ticket Preserves Weight Correlation: Is It Desirable or Not?. (arXiv:2102.11068v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.06070",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laval_J/0/1/0/all/0/1\">Jorge Laval</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_A/0/1/0/all/0/1\">Anye Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Wenchao Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qing_Z/0/1/0/all/0/1\">Zhu Qing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peeta_S/0/1/0/all/0/1\">Srinivas Peeta</a>",
          "description": "Self-driving technology companies and the research community are accelerating\ntheir pace to use machine learning longitudinal motion planning (mMP) for\nautonomous vehicles (AVs). This paper reviews the current state of the art in\nmMP, with an exclusive focus on its impact on traffic congestion. We identify\nthe availability of congestion scenarios in current datasets, and summarize the\nrequired features for training mMP. For learning methods, we survey the major\nmethods in both imitation learning and non-imitation learning. We also\nhighlight the emerging technologies adopted by some leading AV companies, e.g.\nTesla, Waymo, and Comma.ai. We find that: i) the AV industry has been mostly\nfocusing on the long tail problem related to safety and overlooked the impact\non traffic congestion, ii) the current public self-driving datasets have not\nincluded enough congestion scenarios, and mostly lack the necessary input\nfeatures/output labels to train mMP, and iii) albeit reinforcement learning\n(RL) approach can integrate congestion mitigation into the learning goal, the\nmajor mMP method adopted by industry is still behavior cloning (BC), whose\ncapability to learn a congestion-mitigating mMP remains to be seen. Based on\nthe review, the study identifies the research gaps in current mMP development.\nSome suggestions towards congestion mitigation for future mMP studies are\nproposed: i) enrich data collection to facilitate the congestion learning, ii)\nincorporate non-imitation learning methods to combine traffic efficiency into a\nsafety-oriented technical route, and iii) integrate domain knowledge from the\ntraditional car following (CF) theory to improve the string stability of mMP.",
          "link": "http://arxiv.org/abs/1910.06070",
          "publishedOn": "2021-07-20T02:04:46.003Z",
          "wordCount": null,
          "title": "Review of Learning-based Longitudinal Motion Planning for Autonomous Vehicles: Research Gaps between Self-driving and Traffic Congestion. (arXiv:1910.06070v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.10784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belmonte_R/0/1/0/all/0/1\">Romain Belmonte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allaert_B/0/1/0/all/0/1\">Benjamin Allaert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tirilly_P/0/1/0/all/0/1\">Pierre Tirilly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilasco_I/0/1/0/all/0/1\">Ioan Marius Bilasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djeraba_C/0/1/0/all/0/1\">Chaabane Djeraba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>",
          "description": "Although facial landmark localization (FLL) approaches are becoming\nincreasingly accurate for characterizing facial regions, one question remains\nunanswered: what is the impact of these approaches on subsequent related tasks?\nIn this paper, the focus is put on facial expression recognition (FER), where\nfacial landmarks are used for face registration, which is a common usage. Since\nthe most used datasets for facial landmark localization do not allow for a\nproper measurement of performance according to the different difficulties\n(e.g., pose, expression, illumination, occlusion, motion blur), we also\nquantify the performance of recent approaches in the presence of head pose\nvariations and facial expressions. Finally, a study of the impact of these\napproaches on FER is conducted. We show that the landmark accuracy achieved so\nfar optimizing the conventional Euclidean distance does not necessarily\nguarantee a gain in performance for FER. To deal with this issue, we propose a\nnew evaluation metric for FLL adapted to FER.",
          "link": "http://arxiv.org/abs/1905.10784",
          "publishedOn": "2021-07-20T02:04:46.002Z",
          "wordCount": null,
          "title": "Impact of facial landmark localization on facial expression recognition. (arXiv:1905.10784v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.02463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mounir_R/0/1/0/all/0/1\">Ramy Mounir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gula_R/0/1/0/all/0/1\">Roman Gula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theuerkauf_J/0/1/0/all/0/1\">J&#xf6;rn Theuerkauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Sudeep Sarkar</a>",
          "description": "Using offline training schemes, researchers have tackled the event\nsegmentation problem by providing full or weak-supervision through manually\nannotated labels or self-supervised epoch-based training. Most works consider\nvideos that are at most 10's of minutes long. We present a self-supervised\nperceptual prediction framework capable of temporal event segmentation by\nbuilding stable representations of objects over time and demonstrate it on long\nvideos, spanning several days. The approach is deceptively simple but quite\neffective. We rely on predictions of high-level features computed by a standard\ndeep learning backbone. For prediction, we use an LSTM, augmented with an\nattention mechanism, trained in a self-supervised manner using the prediction\nerror. The self-learned attention maps effectively localize and track the\nevent-related objects in each frame. The proposed approach does not require\nlabels. It requires only a single pass through the video, with no separate\ntraining set. Given the lack of datasets of very long videos, we demonstrate\nour method on video from 10 days (254 hours) of continuous wildlife monitoring\ndata that we had collected with required permissions. We find that the approach\nis robust to various environmental conditions such as day/night conditions,\nrain, sharp shadows, and windy conditions. For the task of temporally locating\nevents, we had an 80% recall rate at 20% false-positive rate for frame-level\nsegmentation. At the activity level, we had an 80% activity recall rate for one\nfalse activity detection every 50 minutes. We will make the dataset, which is\nthe first of its kind, and the code available to the research community.",
          "link": "http://arxiv.org/abs/2005.02463",
          "publishedOn": "2021-07-20T02:04:46.001Z",
          "wordCount": null,
          "title": "Spatio-Temporal Event Segmentation and Localization for Wildlife Extended Videos. (arXiv:2005.02463v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chin_H/0/1/0/all/0/1\">Hsu-Hsun Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsay_R/0/1/0/all/0/1\">Ren-Song Tsay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hsin-I Wu</a>",
          "description": "Recent convolutional neural network (CNN) development continues to advance\nthe state-of-the-art model accuracy for various applications. However, the\nenhanced accuracy comes at the cost of substantial memory bandwidth and storage\nrequirements and demanding computational resources. Although in the past the\nquantization methods have effectively reduced the deployment cost for edge\ndevices, it suffers from significant information loss when processing the\nbiased activations of contemporary CNNs. In this paper, we hence introduce an\nadaptive high-performance quantization method to resolve the issue of biased\nactivation by dynamically adjusting the scaling and shifting factors based on\nthe task loss. Our proposed method has been extensively evaluated on image\nclassification models (ResNet-18/34/50, MobileNet-V2, EfficientNet-B0) with\nImageNet dataset, object detection model (YOLO-V4) with COCO dataset, and\nlanguage models with PTB dataset. The results show that our 4-bit integer\n(INT4) quantization models achieve better accuracy than the state-of-the-art\n4-bit models, and in some cases, even surpass the golden full-precision models.\nThe final designs have been successfully deployed onto extremely\nresource-constrained edge devices for many practical applications.",
          "link": "http://arxiv.org/abs/2107.08382",
          "publishedOn": "2021-07-20T02:04:45.991Z",
          "wordCount": null,
          "title": "A High-Performance Adaptive Quantization Approach for Edge CNN Applications. (arXiv:2107.08382v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1912.08393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinming Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Changqun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingcan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "By the aid of attention mechanisms to weight the image features adaptively,\nrecent advanced deep learning-based models encourage the predicted results to\napproximate the ground-truth masks with as large predictable areas as possible,\nthus achieving the state-of-the-art performance. However, these methods do not\npay enough attention to small areas prone to misprediction. In this way, it is\nstill tough to accurately locate salient objects due to the existence of\nregions with indistinguishable foreground and background and regions with\ncomplex or fine structures. To address these problems, we propose a novel\nconvolutional neural network with purificatory mechanism and structural\nsimilarity loss. Specifically, in order to better locate preliminary salient\nobjects, we first introduce the promotion attention, which is based on spatial\nand channel attention mechanisms to promote attention to salient regions.\nSubsequently, for the purpose of restoring the indistinguishable regions that\ncan be regarded as error-prone regions of one model, we propose the\nrectification attention, which is learned from the areas of wrong prediction\nand guide the network to focus on error-prone regions thus rectifying errors.\nThrough these two attentions, we use the Purificatory Mechanism to impose\nstrict weights with different regions of the whole salient objects and purify\nresults from hard-to-distinguish regions, thus accurately predicting the\nlocations and details of salient objects. In addition to paying different\nattention to these hard-to-distinguish regions, we also consider the structural\nconstraints on complex regions and propose the Structural Similarity Loss. In\nexperiments, the proposed approach outperforms 19 state-of-the-art methods on\nsix datasets with a notable margin at over 27FPS on a single NVIDIA 1080Ti GPU.",
          "link": "http://arxiv.org/abs/1912.08393",
          "publishedOn": "2021-07-20T02:04:45.988Z",
          "wordCount": null,
          "title": "Salient Object Detection with Purificatory Mechanism and Structural Similarity Loss. (arXiv:1912.08393v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yingchao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_W/0/1/0/all/0/1\">Wenhui Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>",
          "description": "The balance between high accuracy and high speed has always been a\nchallenging task in semantic image segmentation. Compact segmentation networks\nare more widely used in the case of limited resources, while their performances\nare constrained. In this paper, motivated by the residual learning and global\naggregation, we propose a simple yet general and effective knowledge\ndistillation framework called double similarity distillation (DSD) to improve\nthe classification accuracy of all existing compact networks by capturing the\nsimilarity knowledge in pixel and category dimensions, respectively.\nSpecifically, we propose a pixel-wise similarity distillation (PSD) module that\nutilizes residual attention maps to capture more detailed spatial dependencies\nacross multiple layers. Compared with exiting methods, the PSD module greatly\nreduces the amount of calculation and is easy to expand. Furthermore,\nconsidering the differences in characteristics between semantic segmentation\ntask and other computer vision tasks, we propose a category-wise similarity\ndistillation (CSD) module, which can help the compact segmentation network\nstrengthen the global category correlation by constructing the correlation\nmatrix. Combining these two modules, DSD framework has no extra parameters and\nonly a minimal increase in FLOPs. Extensive experiments on four challenging\ndatasets, including Cityscapes, CamVid, ADE20K, and Pascal VOC 2012, show that\nDSD outperforms current state-of-the-art methods, proving its effectiveness and\ngenerality. The code and models will be publicly available.",
          "link": "http://arxiv.org/abs/2107.08591",
          "publishedOn": "2021-07-20T02:04:45.987Z",
          "wordCount": null,
          "title": "Double Similarity Distillation for Semantic Image Segmentation. (arXiv:2107.08591v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.02443",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pomponi_J/0/1/0/all/0/1\">Jary Pomponi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scardapane_S/0/1/0/all/0/1\">Simone Scardapane</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Uncini_A/0/1/0/all/0/1\">Aurelio Uncini</a>",
          "description": "Catastrophic forgetting (CF) happens whenever a neural network overwrites\npast knowledge while being trained on new tasks. Common techniques to handle CF\ninclude regularization of the weights (using, e.g., their importance on past\ntasks), and rehearsal strategies, where the network is constantly re-trained on\npast data. Generative models have also been applied for the latter, in order to\nhave endless sources of data. In this paper, we propose a novel method that\ncombines the strengths of regularization and generative-based rehearsal\napproaches. Our generative model consists of a normalizing flow (NF), a\nprobabilistic and invertible neural network, trained on the internal embeddings\nof the network. By keeping a single NF conditioned on the task, we show that\nour memory overhead remains constant. In addition, exploiting the invertibility\nof the NF, we propose a simple approach to regularize the network's embeddings\nwith respect to past tasks. We show that our method performs favorably with\nrespect to state-of-the-art approaches in the literature, with bounded\ncomputational power and memory overheads.",
          "link": "http://arxiv.org/abs/2007.02443",
          "publishedOn": "2021-07-20T02:04:45.985Z",
          "wordCount": null,
          "title": "Pseudo-Rehearsal for Continual Learning with Normalizing Flows. (arXiv:2007.02443v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1\">Wei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jie Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>",
          "description": "Few-shot class-incremental learning is to recognize the new classes given few\nsamples and not forget the old classes. It is a challenging task since\nrepresentation optimization and prototype reorganization can only be achieved\nunder little supervision. To address this problem, we propose a novel\nincremental prototype learning scheme. Our scheme consists of a random episode\nselection strategy that adapts the feature representation to various generated\nincremental episodes to enhance the corresponding extensibility, and a\nself-promoted prototype refinement mechanism which strengthens the expression\nability of the new classes by explicitly considering the dependencies among\ndifferent classes. Particularly, a dynamic relation projection module is\nproposed to calculate the relation matrix in a shared embedding space and\nleverage it as the factor for bootstrapping the update of prototypes. Extensive\nexperiments on three benchmark datasets demonstrate the above-par incremental\nperformance, outperforming state-of-the-art methods by a margin of 13%, 17% and\n11%, respectively.",
          "link": "http://arxiv.org/abs/2107.08918",
          "publishedOn": "2021-07-20T02:04:45.984Z",
          "wordCount": null,
          "title": "Self-Promoted Prototype Refinement for Few-Shot Class-Incremental Learning. (arXiv:2107.08918v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sayak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>",
          "description": "Floods wreak havoc throughout the world, causing billions of dollars in\ndamages, and uprooting communities, ecosystems and economies. Accurate and\nrobust flood detection including delineating open water flood areas and\nidentifying flood levels can aid in disaster response and mitigation. However,\nestimating flood levels remotely is of essence as physical access to flooded\nareas is limited and the ability to deploy instruments in potential flood zones\ncan be dangerous. Aligning flood extent mapping with local topography can\nprovide a plan-of-action that the disaster response team can consider. Thus,\nremote flood level estimation via satellites like Sentinel-1 can prove to be\nremedial. The Emerging Techniques in Computational Intelligence (ETCI)\ncompetition on Flood Detection tasked participants with predicting flooded\npixels after training with synthetic aperture radar (SAR) images in a\nsupervised setting. We use a cyclical approach involving two stages (1)\ntraining an ensemble model of multiple UNet architectures with available high\nand low confidence labeled data and, (2) generating pseudo labels or low\nconfidence labels on the unlabeled test dataset, and then, combining the\ngenerated labels with the previously available high confidence labeled dataset.\nThis assimilated dataset is used for the next round of training ensemble\nmodels. This cyclical process is repeated until the performance improvement\nplateaus. Additionally, we post process our results with Conditional Random\nFields. Our approach sets a high score on the public leaderboard for the ETCI\ncompetition with 0.7654 IoU. Our method, which we release with all the code\nincluding trained models, can also be used as an open science benchmark for the\nSentinel-1 released dataset on GitHub.",
          "link": "http://arxiv.org/abs/2107.08369",
          "publishedOn": "2021-07-20T02:04:45.661Z",
          "wordCount": null,
          "title": "Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08543",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saparov_T/0/1/0/all/0/1\">Talgat Saparov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurmukov_A/0/1/0/all/0/1\">Anvar Kurmukov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shirokih_B/0/1/0/all/0/1\">Boris Shirokih</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Belyaev_M/0/1/0/all/0/1\">Mikhail Belyaev</a>",
          "description": "Domain shift is one of the most salient challenges in medical computer\nvision. Due to immense variability in scanners' parameters and imaging\nprotocols, even images obtained from the same person and the same scanner could\ndiffer significantly. We address variability in computed tomography (CT) images\ncaused by different convolution kernels used in the reconstruction process, the\ncritical domain shift factor in CT. The choice of a convolution kernel affects\npixels' granularity, image smoothness, and noise level. We analyze a dataset of\npaired CT images, where smooth and sharp images were reconstructed from the\nsame sinograms with different kernels, thus providing identical anatomy but\ndifferent style. Though identical predictions are desired, we show that the\nconsistency, measured as the average Dice between predictions on pairs, is just\n0.54. We propose Filtered Back-Projection Augmentation (FBPAug), a simple and\nsurprisingly efficient approach to augment CT images in sinogram space\nemulating reconstruction with different kernels. We apply the proposed method\nin a zero-shot domain adaptation setup and show that the consistency boosts\nfrom 0.54 to 0.92 outperforming other augmentation approaches. Neither specific\npreparation of source domain data nor target domain data is required, so our\npublicly released FBPAug can be used as a plug-and-play module for zero-shot\ndomain adaptation in any CT-based task.",
          "link": "http://arxiv.org/abs/2107.08543",
          "publishedOn": "2021-07-20T02:04:45.229Z",
          "wordCount": 664,
          "title": "Zero-Shot Domain Adaptation in CT Segmentation by Filtered Back Projection Augmentation. (arXiv:2107.08543v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08406",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shutai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yinhao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunhua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>",
          "description": "Small target detection is known to be a challenging problem. Inspired by the\nstructural characteristics and physiological mechanism of eagle-eye, a\nminiature vision system is designed for small target detection in this paper.\nFirst, a hardware platform is established, which consists of a pan-tilt, a\nshort-focus camera and a long-focus camera. Then, based on the visual attention\nmechanism of eagle-eye, the cameras with different focal lengths are controlled\ncooperatively to achieve small target detection. Experimental results show that\nthe designed biological eagle-eye vision system can accurately detect small\ntargets, which has a strong adaptive ability.",
          "link": "http://arxiv.org/abs/2107.08406",
          "publishedOn": "2021-07-20T02:04:45.201Z",
          "wordCount": 552,
          "title": "A Miniature Biological Eagle-Eye Vision System for Small Target Detection. (arXiv:2107.08406v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08471",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dengshan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chengjun Xie</a>",
          "description": "Artificial neural networks that simulate human achieves great successes. From\nthe perspective of simulating human memory method, we propose a stepped sampler\nbased on the \"repeated input\". We repeatedly inputted data to the LSTM model\nstepwise in a batch. The stepped sampler is used to strengthen the ability of\nfusing the temporal information in LSTM. We tested the stepped sampler on the\nLSTM built-in in PyTorch. Compared with the traditional sampler of PyTorch,\nsuch as sequential sampler, batch sampler, the training loss of the proposed\nstepped sampler converges faster in the training of the model, and the training\nloss after convergence is more stable. Meanwhile, it can maintain a higher test\naccuracy. We quantified the algorithm of the stepped sampler.",
          "link": "http://arxiv.org/abs/2107.08471",
          "publishedOn": "2021-07-20T02:04:45.170Z",
          "wordCount": 556,
          "title": "A stepped sampling method for video detection using LSTM. (arXiv:2107.08471v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Dongze Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shenghua Gao</a>",
          "description": "An Axial Shifted MLP architecture (AS-MLP) is proposed in this paper.\nDifferent from MLP-Mixer, where the global spatial feature is encoded for the\ninformation flow through matrix transposition and one token-mixing MLP, we pay\nmore attention to the local features communication. By axially shifting\nchannels of the feature map, AS-MLP is able to obtain the information flow from\ndifferent axial directions, which captures the local dependencies. Such an\noperation enables us to utilize a pure MLP architecture to achieve the same\nlocal receptive field as CNN-like architecture. We can also design the\nreceptive field size and dilation of blocks of AS-MLP, etc, just like designing\nthose of convolution kernels. With the proposed AS-MLP architecture, our model\nobtains 83.3% Top-1 accuracy with 88M parameters and 15.2 GFLOPs on the\nImageNet-1K dataset. Such a simple yet effective architecture outperforms all\nMLP-based architectures and achieves competitive performance compared to the\ntransformer-based architectures (e.g., Swin Transformer) even with slightly\nlower FLOPs. In addition, AS-MLP is also the first MLP-based architecture to be\napplied to the downstream tasks (e.g., object detection and semantic\nsegmentation). The experimental results are also impressive. Our proposed\nAS-MLP obtains 51.5 mAP on the COCO validation set and 49.5 MS mIoU on the\nADE20K dataset, which is competitive compared to the transformer-based\narchitectures. Code is available at https://github.com/svip-lab/AS-MLP.",
          "link": "http://arxiv.org/abs/2107.08391",
          "publishedOn": "2021-07-20T02:04:45.154Z",
          "wordCount": 658,
          "title": "AS-MLP: An Axial Shifted MLP Architecture for Vision. (arXiv:2107.08391v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_I/0/1/0/all/0/1\">Iker Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skalski_P/0/1/0/all/0/1\">Piotr Skalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barns_Graham_A/0/1/0/all/0/1\">Alec Barns-Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Jason Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_D/0/1/0/all/0/1\">David Sutton</a>",
          "description": "Enabling interpretations of model uncertainties is of key importance in\nBayesian machine learning applications. Often, this requires to meaningfully\nattribute predictive uncertainties to source features in an image, text or\ncategorical array. However, popular attribution methods are particularly\ndesigned for classification and regression scores. In order to explain\nuncertainties, state of the art alternatives commonly procure counterfactual\nfeature vectors, and proceed by making direct comparisons. In this paper, we\nleverage path integrals to attribute uncertainties in Bayesian differentiable\nmodels. We present a novel algorithm that relies on in-distribution curves\nconnecting a feature vector to some counterfactual counterpart, and we retain\ndesirable properties of interpretability methods. We validate our approach on\nbenchmark image data sets with varying resolution, and show that it\nsignificantly simplifies interpretability over the existing alternatives.",
          "link": "http://arxiv.org/abs/2107.08756",
          "publishedOn": "2021-07-20T02:04:42.595Z",
          "wordCount": 572,
          "title": "Path Integrals for the Attribution of Model Uncertainties. (arXiv:2107.08756v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiahuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yansong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1\">Bing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Wu</a>",
          "description": "Existing popular unsupervised embedding learning methods focus on enhancing\nthe instance-level local discrimination of the given unlabeled images by\nexploring various negative data. However, the existed sample outliers which\nexhibit large intra-class divergences or small inter-class variations severely\nlimit their learning performance. We justify that the performance limitation is\ncaused by the gradient vanishing on these sample outliers. Moreover, the\nshortage of positive data and disregard for global discrimination consideration\nalso pose critical issues for unsupervised learning but are always ignored by\nexisting methods. To handle these issues, we propose a novel solution to\nexplicitly model and directly explore the uncertainty of the given unlabeled\nlearning samples. Instead of learning a deterministic feature point for each\nsample in the embedding space, we propose to represent a sample by a stochastic\nGaussian with the mean vector depicting its space localization and covariance\nvector representing the sample uncertainty. We leverage such uncertainty\nmodeling as momentum to the learning which is helpful to tackle the outliers.\nFurthermore, abundant positive candidates can be readily drawn from the learned\ninstance-specific distributions which are further adopted to mitigate the\naforementioned issues. Thorough rationale analyses and extensive experiments\nare presented to verify our superiority.",
          "link": "http://arxiv.org/abs/2107.08892",
          "publishedOn": "2021-07-20T02:04:42.575Z",
          "wordCount": 635,
          "title": "Unsupervised Embedding Learning from Uncertainty Momentum Modeling. (arXiv:2107.08892v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08751",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Memmel_M/0/1/0/all/0/1\">Marius Memmel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1\">Camila Gonzalez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Anirban Mukhopadhyay</a>",
          "description": "Deep learning for medical imaging suffers from temporal and privacy-related\nrestrictions on data availability. To still obtain viable models, continual\nlearning aims to train in sequential order, as and when data is available. The\nmain challenge that continual learning methods face is to prevent catastrophic\nforgetting, i.e., a decrease in performance on the data encountered earlier.\nThis issue makes continuous training of segmentation models for medical\napplications extremely difficult. Yet, often, data from at least two different\ndomains is available which we can exploit to train the model in a way that it\ndisregards domain-specific information. We propose an architecture that\nleverages the simultaneous availability of two or more datasets to learn a\ndisentanglement between the content and domain in an adversarial fashion. The\ndomain-invariant content representation then lays the base for continual\nsemantic segmentation. Our approach takes inspiration from domain adaptation\nand combines it with continual learning for hippocampal segmentation in brain\nMRI. We showcase that our method reduces catastrophic forgetting and\noutperforms state-of-the-art continual learning methods.",
          "link": "http://arxiv.org/abs/2107.08751",
          "publishedOn": "2021-07-20T02:04:42.557Z",
          "wordCount": 612,
          "title": "Adversarial Continual Learning for Multi-Domain Hippocampal Segmentation. (arXiv:2107.08751v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08850",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ganz_J/0/1/0/all/0/1\">Jonathan Ganz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kirsch_T/0/1/0/all/0/1\">Tobias Kirsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_L/0/1/0/all/0/1\">Lucas Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bertram_C/0/1/0/all/0/1\">Christof A. Bertram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_C/0/1/0/all/0/1\">Christoph Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blumcke_I/0/1/0/all/0/1\">Ingmar Bl&#xfc;mcke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jabari_S/0/1/0/all/0/1\">Samir Jabari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aubreville_M/0/1/0/all/0/1\">Marc Aubreville</a>",
          "description": "Meningioma is one of the most prevalent brain tumors in adults. To determine\nits malignancy, it is graded by a pathologist into three grades according to\nWHO standards. This grade plays a decisive role in treatment, and yet may be\nsubject to inter-rater discordance. In this work, we present and compare three\napproaches towards fully automatic meningioma grading from histology whole\nslide images. All approaches are following a two-stage paradigm, where we first\nidentify a region of interest based on the detection of mitotic figures in the\nslide using a state-of-the-art object detection deep learning network. This\nregion of highest mitotic rate is considered characteristic for biological\ntumor behavior. In the second stage, we calculate a score corresponding to\ntumor malignancy based on information contained in this region using three\ndifferent settings. In a first approach, image patches are sampled from this\nregion and regression is based on morphological features encoded by a\nResNet-based network. We compare this to learning a logistic regression from\nthe determined mitotic count, an approach which is easily traceable and\nexplainable. Lastly, we combine both approaches in a single network. We trained\nthe pipeline on 951 slides from 341 patients and evaluated them on a separate\nset of 141 slides from 43 patients. All approaches yield a high correlation to\nthe WHO grade. The logistic regression and the combined approach had the best\nresults in our experiments, yielding correct predictions in 32 and 33 of all\ncases, respectively, with the image-based approach only predicting 25 cases\ncorrectly. Spearman's correlation was 0.716, 0.792 and 0.790 respectively. It\nmay seem counterintuitive at first that morphological features provided by\nimage patches do not improve model performance. Yet, this mirrors the criteria\nof the grading scheme, where mitotic count is the only unequivocal parameter.",
          "link": "http://arxiv.org/abs/2107.08850",
          "publishedOn": "2021-07-20T02:04:42.480Z",
          "wordCount": 767,
          "title": "Automatic and explainable grading of meningiomas from histopathology images. (arXiv:2107.08850v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08767",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nam_W/0/1/0/all/0/1\">Woo-Jeoung Nam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "As interpretability has been pointed out as the obstacle to the adoption of\nDeep Neural Networks (DNNs), there is an increasing interest in solving a\ntransparency issue to guarantee the impressive performance. In this paper, we\ndemonstrate the efficiency of recent attribution techniques to explain the\ndiagnostic decision by visualizing the significant factors in the input image.\nBy utilizing the characteristics of objectness that DNNs have learned, fully\ndecomposing the network prediction visualizes clear localization of target\nlesion. To verify our work, we conduct our experiments on Chest X-ray diagnosis\nwith publicly accessible datasets. As an intuitive assessment metric for\nexplanations, we report the performance of intersection of Union between visual\nexplanation and bounding box of lesions. Experiment results show that recently\nproposed attribution methods visualize the more accurate localization for the\ndiagnostic decision compared to the traditionally used CAM. Furthermore, we\nanalyze the inconsistency of intentions between humans and DNNs, which is\neasily obscured by high performance. By visualizing the relevant factors, it is\npossible to confirm that the criterion for decision is in line with the\nlearning strategy. Our analysis of unmasking machine intelligence represents\nthe necessity of explainability in the medical diagnostic decision.",
          "link": "http://arxiv.org/abs/2107.08767",
          "publishedOn": "2021-07-20T02:04:42.431Z",
          "wordCount": 654,
          "title": "Improving Interpretability of Deep Neural Networks in Medical Diagnosis by Investigating the Individual Units. (arXiv:2107.08767v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1\">Myeong-Seok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong-Ju Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Aerial image registration or matching is a geometric process of aligning two\naerial images captured in different environments. Estimating the precise\ntransformation parameters is hindered by various environments such as time,\nweather, and viewpoints. The characteristics of the aerial images are mainly\ncomposed of a straight line owing to building and road. Therefore, the straight\nlines are distorted when estimating homography parameters directly between two\nimages. In this paper, we propose a deep homography alignment network to\nprecisely match two aerial images by progressively estimating the various\ntransformation parameters. The proposed network is possible to train the\nmatching network with a higher degree of freedom by progressively analyzing the\ntransformation parameters. The precision matching performances have been\nincreased by applying homography transformation. In addition, we introduce a\nmethod that can effectively learn the difficult-to-learn homography estimation\nnetwork. Since there is no published learning data for aerial image\nregistration, in this paper, a pair of images to which random homography\ntransformation is applied within a certain range is used for learning. Hence,\nwe could confirm that the deep homography alignment network shows high\nprecision matching performance compared with conventional works.",
          "link": "http://arxiv.org/abs/2107.08768",
          "publishedOn": "2021-07-20T02:04:42.402Z",
          "wordCount": 624,
          "title": "Precise Aerial Image Matching based on Deep Homography Estimation. (arXiv:2107.08768v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenkai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jizheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Man Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1\">Yuanzhu Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_X/0/1/0/all/0/1\">Xinchao Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Muqing Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jing Song</a>",
          "description": "The 3D visual perception for vehicles with the surround-view fisheye camera\nsystem is a critical and challenging task for low-cost urban autonomous\ndriving. While existing monocular 3D object detection methods perform not well\nenough on the fisheye images for mass production, partly due to the lack of 3D\ndatasets of such images. In this paper, we manage to overcome and avoid the\ndifficulty of acquiring the large scale of accurate 3D labeled truth data, by\nbreaking down the 3D object detection task into some sub-tasks, such as\nvehicle's contact point detection, type classification, re-identification and\nunit assembling, etc. Particularly, we propose the concept of Multidimensional\nVector to include the utilizable information generated in different dimensions\nand stages, instead of the descriptive approach for the bird's eye view (BEV)\nor a cube of eight points. The experiments of real fisheye images demonstrate\nthat our solution achieves state-of-the-art accuracy while being real-time in\npractice.",
          "link": "http://arxiv.org/abs/2107.08862",
          "publishedOn": "2021-07-20T02:04:42.276Z",
          "wordCount": 617,
          "title": "Disentangling and Vectorization: A 3D Visual Perception Approach for Autonomous Driving Based on Surround-View Fisheye Cameras. (arXiv:2107.08862v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08673",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Massalimova_A/0/1/0/all/0/1\">Aidana Massalimova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varol_H/0/1/0/all/0/1\">Huseyin Atakan Varol</a>",
          "description": "Alzheimer's disease (AD) is a progressive brain disorder that causes memory\nand functional impairments. The advances in machine learning and publicly\navailable medical datasets initiated multiple studies in AD diagnosis. In this\nwork, we utilize a multi-modal deep learning approach in classifying normal\ncognition, mild cognitive impairment and AD classes on the basis of structural\nMRI and diffusion tensor imaging (DTI) scans from the OASIS-3 dataset. In\naddition to a conventional multi-modal network, we also present an input\nagnostic architecture that allows diagnosis with either sMRI or DTI scan, which\ndistinguishes our method from previous multi-modal machine learning-based\nmethods. The results show that the input agnostic model achieves 0.96 accuracy\nwhen both structural MRI and DTI scans are provided as inputs.",
          "link": "http://arxiv.org/abs/2107.08673",
          "publishedOn": "2021-07-20T02:04:42.239Z",
          "wordCount": 581,
          "title": "Input Agnostic Deep Learning for Alzheimer's Disease Classification Using Multimodal MRI Images. (arXiv:2107.08673v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1\">Pengfei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>",
          "description": "By considering the spatial correspondence, dense self-supervised\nrepresentation learning has achieved superior performance on various dense\nprediction tasks. However, the pixel-level correspondence tends to be noisy\nbecause of many similar misleading pixels, e.g., backgrounds. To address this\nissue, in this paper, we propose to explore \\textbf{set} \\textbf{sim}ilarity\n(SetSim) for dense self-supervised representation learning. We generalize\npixel-wise similarity learning to set-wise one to improve the robustness\nbecause sets contain more semantic and structure information. Specifically, by\nresorting to attentional features of views, we establish corresponding sets,\nthus filtering out noisy backgrounds that may cause incorrect correspondences.\nMeanwhile, these attentional features can keep the coherence of the same image\nacross different views to alleviate semantic inconsistency. We further search\nthe cross-view nearest neighbours of sets and employ the structured\nneighbourhood information to enhance the robustness. Empirical evaluations\ndemonstrate that SetSim is superior to state-of-the-art methods on object\ndetection, keypoint detection, instance segmentation, and semantic\nsegmentation.",
          "link": "http://arxiv.org/abs/2107.08712",
          "publishedOn": "2021-07-20T02:04:42.135Z",
          "wordCount": 603,
          "title": "Exploring Set Similarity for Dense Self-supervised Representation Learning. (arXiv:2107.08712v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shilei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hongyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xianli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_B/0/1/0/all/0/1\">Buyue Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "Universal lesion detection in computed tomography (CT) images is an important\nyet challenging task due to the large variations in lesion type, size, shape,\nand appearance. Considering that data in clinical routine (such as the\nDeepLesion dataset) are usually annotated with a long and a short diameter\naccording to the standard of Response Evaluation Criteria in Solid Tumors\n(RECIST) diameters, we propose RECIST-Net, a new approach to lesion detection\nin which the four extreme points and center point of the RECIST diameters are\ndetected. By detecting a lesion as keypoints, we provide a more conceptually\nstraightforward formulation for detection, and overcome several drawbacks\n(e.g., requiring extensive effort in designing data-appropriate anchors and\nlosing shape information) of existing bounding-box-based methods while\nexploring a single-task, one-stage approach compared to other RECIST-based\napproaches. Experiments show that RECIST-Net achieves a sensitivity of 92.49%\nat four false positives per image, outperforming other recent methods including\nthose using multi-task learning.",
          "link": "http://arxiv.org/abs/2107.08715",
          "publishedOn": "2021-07-20T02:04:42.115Z",
          "wordCount": 609,
          "title": "RECIST-Net: Lesion detection via grouping keypoints on RECIST-based annotation. (arXiv:2107.08715v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young J. Kim</a>",
          "description": "We propose a 3D face generative model with local weights to increase the\nmodel's variations and expressiveness. The proposed model allows partial\nmanipulation of the face while still learning the whole face mesh. For this\npurpose, we address an effective way to extract local facial features from the\nentire data and explore a way to manipulate them during a holistic generation.\nFirst, we factorize the latent space of the whole face to the subspace\nindicating different parts of the face. In addition, local weights generated by\nnon-negative matrix factorization are applied to the factorized latent space so\nthat the decomposed part space is semantically meaningful. We experiment with\nour model and observe that effective facial part manipulation is possible and\nthat the model's expressiveness is improved.",
          "link": "http://arxiv.org/abs/2107.08737",
          "publishedOn": "2021-07-20T02:04:42.096Z",
          "wordCount": 584,
          "title": "Synthesizing Human Faces using Latent Space Factorization and Local Weights (Extended Version). (arXiv:2107.08737v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenpeng Chen</a>",
          "description": "Among 2D convolutional networks on point clouds, point-based approaches\nconsume point clouds of fixed size directly. By analysis of PointNet, a pioneer\nin introducing deep learning into point sets, we reveal that current\npoint-based methods are essentially spatial relationship processing networks.\nIn this paper, we take a different approach. Our architecture, named PE-Net,\nlearns the representation of point clouds in high-dimensional space, and\nencodes the unordered input points to feature vectors, which standard 2D CNNs\ncan be applied to. The recommended network can adapt to changes in the number\nof input points which is the limit of current methods. Experiments show that in\nthe tasks of classification and part segmentation, PE-Net achieves the\nstate-of-the-art performance in multiple challenging datasets, such as ModelNet\nand ShapeNetPart.",
          "link": "http://arxiv.org/abs/2107.08565",
          "publishedOn": "2021-07-20T02:04:41.858Z",
          "wordCount": 550,
          "title": "Learning point embedding for 3D data processing. (arXiv:2107.08565v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songtao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>",
          "description": "In this report, we present some experienced improvements to YOLO series,\nforming a new high-performance detector -- YOLOX. We switch the YOLO detector\nto an anchor-free manner and conduct other advanced detection techniques, i.e.,\na decoupled head and the leading label assignment strategy SimOTA to achieve\nstate-of-the-art results across a large scale range of models: For YOLO-Nano\nwith only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing\nNanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in\nindustry, we boost it to 47.3% AP on COCO, outperforming the current best\npractice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as\nYOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on\nTesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on\nStreaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021)\nusing a single YOLOX-L model. We hope this report can provide useful experience\nfor developers and researchers in practical scenes, and we also provide deploy\nversions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at\nhttps://github.com/Megvii-BaseDetection/YOLOX.",
          "link": "http://arxiv.org/abs/2107.08430",
          "publishedOn": "2021-07-20T02:04:41.791Z",
          "wordCount": 625,
          "title": "YOLOX: Exceeding YOLO Series in 2021. (arXiv:2107.08430v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>",
          "description": "We propose an approach to instance segmentation from 3D point clouds based on\ndynamic convolution. This enables it to adapt, at inference, to varying feature\nand object scales. Doing so avoids some pitfalls of bottom up approaches,\nincluding a dependence on hyper-parameter tuning and heuristic post-processing\npipelines to compensate for the inevitable variability in object sizes, even\nwithin a single scene. The representation capability of the network is greatly\nimproved by gathering homogeneous points that have identical semantic\ncategories and close votes for the geometric centroids. Instances are then\ndecoded via several simple convolution layers, where the parameters are\ngenerated conditioned on the input. The proposed approach is proposal-free, and\ninstead exploits a convolution process that adapts to the spatial and semantic\ncharacteristics of each instance. A light-weight transformer, built on the\nbottleneck layer, allows the model to capture long-range dependencies, with\nlimited computational overhead. The result is a simple, efficient, and robust\napproach that yields strong performance on various datasets: ScanNetV2, S3DIS,\nand PartNet. The consistent improvements on both voxel- and point-based\narchitectures imply the effectiveness of the proposed method. Code is available\nat: https://git.io/DyCo3D",
          "link": "http://arxiv.org/abs/2107.08392",
          "publishedOn": "2021-07-20T02:04:41.770Z",
          "wordCount": 628,
          "title": "Dynamic Convolution for 3D Point Cloud Instance Segmentation. (arXiv:2107.08392v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yongxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiaolin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuncong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lu Li</a>",
          "description": "As one of the prevalent components, Feature Pyramid Network (FPN) is widely\nused in the current object detection models to improve the performance of\nmulti-scale detection. However, its interaction is still in a local and lossy\nmanner, thus limiting the representation power. In this paper, to simulate a\nglobal view of human vision in object detection and address the inherent\ndefects of interaction mode in FPN, we construct a novel architecture termed\nContent-Augmented Feature Pyramid Network (CA-FPN). Unlike the vanilla FPN,\nwhich fuses features within a local receptive field, CA-FPN can adaptively\naggregate similar features from a global view. It is equipped with a global\ncontent extraction module and light linear spatial transformers. The former\nallows to extract multi-scale context information and the latter can deeply\ncombine the global content extraction module with the vanilla FPN using the\nlinearized attention function, which is designed to reduce model complexity.\nFurthermore, CA-FPN can be readily plugged into existing FPN-based models.\nExtensive experiments on the challenging COCO and PASCAL VOC object detection\ndatasets demonstrated that our CA-FPN significantly outperforms competitive\nFPN-based detectors without bells and whistles. When plugging CA-FPN into\nCascade R-CNN framework built upon a standard ResNet-50 backbone, our method\ncan achieve 44.8 AP on COCO mini-val. Its performance surpasses the previous\nstate-of-the-art by 1.5 AP, demonstrating the potentiality of application.",
          "link": "http://arxiv.org/abs/2105.09464",
          "publishedOn": "2021-07-20T02:04:41.740Z",
          "wordCount": 703,
          "title": "Content-Augmented Feature Pyramid Network with Light Linear Spatial Transformers for Object Detection. (arXiv:2105.09464v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiaxiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaokang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Gang Zeng</a>",
          "description": "Guided depth super-resolution is a practical task where a low-resolution and\nnoisy input depth map is restored to a high-resolution version, with the help\nof a high-resolution RGB guide image. Existing methods usually view this task\nas a generalized guided filtering problem that relies on designing explicit\nfilters and objective functions, or a dense regression problem that directly\npredicts the target image via deep neural networks. These methods suffer from\neither model capability or interpretability. Inspired by the recent progress in\nimplicit neural representation, we propose to formulate the guided\nsuper-resolution as a neural implicit image interpolation problem, where we\ntake the form of a general image interpolation but use a novel Joint Implicit\nImage Function (JIIF) representation to learn both the interpolation weights\nand values. JIIF represents the target image domain with spatially distributed\nlocal latent codes extracted from the input image and the guide image, and uses\na graph attention mechanism to learn the interpolation weights at the same time\nin one unified deep implicit function. We demonstrate the effectiveness of our\nJIIF representation on guided depth super-resolution task, significantly\noutperforming state-of-the-art methods on three public benchmarks. Code can be\nfound at \\url{https://git.io/JC2sU}.",
          "link": "http://arxiv.org/abs/2107.08717",
          "publishedOn": "2021-07-20T02:04:41.695Z",
          "wordCount": 639,
          "title": "Joint Implicit Image Function for Guided Depth Super-Resolution. (arXiv:2107.08717v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1\">Prashant Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_M/0/1/0/all/0/1\">Mrigank Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varambally_S/0/1/0/all/0/1\">Sumanth Varambally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1\">Prathosh AP</a>",
          "description": "Generalization of machine learning models trained on a set of source domains\non unseen target domains with different statistics, is a challenging problem.\nWhile many approaches have been proposed to solve this problem, they only\nutilize source data during training but do not take advantage of the fact that\na single target example is available at the time of inference. Motivated by\nthis, we propose a method that effectively uses the target sample during\ninference beyond mere classification. Our method has three components - (i) A\nlabel-preserving feature or metric transformation on source data such that the\nsource samples are clustered in accordance with their class irrespective of\ntheir domain (ii) A generative model trained on the these features (iii) A\nlabel-preserving projection of the target point on the source-feature manifold\nduring inference via solving an optimization problem on the input space of the\ngenerative model using the learned metric. Finally, the projected target is\nused in the classifier. Since the projected target feature comes from the\nsource manifold and has the same label as the real target by design, the\nclassifier is expected to perform better on it than the true target. We\ndemonstrate that our method outperforms the state-of-the-art Domain\nGeneralization methods on multiple datasets and tasks.",
          "link": "http://arxiv.org/abs/2103.01134",
          "publishedOn": "2021-07-20T02:04:41.676Z",
          "wordCount": 678,
          "title": "Domain Generalization via Inference-time Label-Preserving Target Projections. (arXiv:2103.01134v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08111",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Roth_H/0/1/0/all/0/1\">Holger R. Roth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Wenqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Myronenko_A/0/1/0/all/0/1\">Andriy Myronenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyue Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaosong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>",
          "description": "Building robust deep learning-based models requires diverse training data,\nideally from several sources. However, these datasets cannot be combined easily\nbecause of patient privacy concerns or regulatory hurdles, especially if\nmedical data is involved. Federated learning (FL) is a way to train machine\nlearning models without the need for centralized datasets. Each FL client\ntrains on their local data while only sharing model parameters with a global\nserver that aggregates the parameters from all clients. At the same time, each\nclient's data can exhibit differences and inconsistencies due to the local\nvariation in the patient population, imaging equipment, and acquisition\nprotocols. Hence, the federated learned models should be able to adapt to the\nlocal particularities of a client's data. In this work, we combine FL with an\nAutoML technique based on local neural architecture search by training a\n\"supernet\". Furthermore, we propose an adaptation scheme to allow for\npersonalized model architectures at each FL client's site. The proposed method\nis evaluated on four different datasets from 3D prostate MRI and shown to\nimprove the local models' performance after adaptation through selecting an\noptimal path through the AutoML supernet.",
          "link": "http://arxiv.org/abs/2107.08111",
          "publishedOn": "2021-07-20T02:04:41.619Z",
          "wordCount": 650,
          "title": "Federated Whole Prostate Segmentation in MRI with Personalized Neural Architectures. (arXiv:2107.08111v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ashesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1\">Luca Del Pero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1\">Hugo Grimmett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1\">Peter Ondruska</a>",
          "description": "Despite the numerous successes of machine learning over the past decade\n(image recognition, decision-making, NLP, image synthesis), self-driving\ntechnology has not yet followed the same trend. In this paper, we study the\nhistory, composition, and development bottlenecks of the modern self-driving\nstack. We argue that the slow progress is caused by approaches that require too\nmuch hand-engineering, an over-reliance on road testing, and high fleet\ndeployment costs. We observe that the classical stack has several bottlenecks\nthat preclude the necessary scale needed to capture the long tail of rare\nevents. To resolve these problems, we outline the principles of Autonomy 2.0,\nan ML-first approach to self-driving, as a viable alternative to the currently\nadopted state-of-the-art. This approach is based on (i) a fully differentiable\nAV stack trainable from human demonstrations, (ii) closed-loop data-driven\nreactive simulation, and (iii) large-scale, low-cost data collections as\ncritical solutions towards scalability issues. We outline the general\narchitecture, survey promising works in this direction and propose key\nchallenges to be addressed by the community in the future.",
          "link": "http://arxiv.org/abs/2107.08142",
          "publishedOn": "2021-07-20T02:04:41.599Z",
          "wordCount": 616,
          "title": "Autonomy 2.0: Why is self-driving always 5 years away?. (arXiv:2107.08142v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08330",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Konwer_A/0/1/0/all/0/1\">Aishik Konwer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bae_J/0/1/0/all/0/1\">Joseph Bae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_G/0/1/0/all/0/1\">Gagandeep Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gattu_R/0/1/0/all/0/1\">Rishabh Gattu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Syed Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Green_J/0/1/0/all/0/1\">Jeremy Green</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phatak_T/0/1/0/all/0/1\">Tej Phatak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>",
          "description": "COVID-19 image analysis has mostly focused on diagnostic tasks using single\ntimepoint scans acquired upon disease presentation or admission. We present a\ndeep learning-based approach to predict lung infiltrate progression from serial\nchest radiographs (CXRs) of COVID-19 patients. Our method first utilizes\nconvolutional neural networks (CNNs) for feature extraction from patches within\nthe concerned lung zone, and also from neighboring and remote boundary regions.\nThe framework further incorporates a multi-scale Gated Recurrent Unit (GRU)\nwith a correlation module for effective predictions. The GRU accepts CNN\nfeature vectors from three different areas as input and generates a fused\nrepresentation. The correlation module attempts to minimize the correlation\nloss between hidden representations of concerned and neighboring area feature\nvectors, while maximizing the loss between the same from concerned and remote\nregions. Further, we employ an attention module over the output hidden states\nof each encoder timepoint to generate a context vector. This vector is used as\nan input to a decoder module to predict patch severity grades at a future\ntimepoint. Finally, we ensemble the patch classification scores to calculate\npatient-wise grades. Specifically, our framework predicts zone-wise disease\nseverity for a patient on a given day by learning representations from the\nprevious temporal CXRs. Our novel multi-institutional dataset comprises\nsequential CXR scans from N=93 patients. Our approach outperforms transfer\nlearning and radiomic feature-based baseline approaches on this dataset.",
          "link": "http://arxiv.org/abs/2107.08330",
          "publishedOn": "2021-07-20T02:04:41.508Z",
          "wordCount": 742,
          "title": "Attention-based Multi-scale Gated Recurrent Encoder with Novel Correlation Loss for COVID-19 Progression Prediction. (arXiv:2107.08330v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08355",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Liupeng Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1\">Huanfeng Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_L/0/1/0/all/0/1\">Lingli Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_Q/0/1/0/all/0/1\">Qiangqiang Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xinghua Li</a>",
          "description": "The data fusion technology aims to aggregate the characteristics of different\ndata and obtain products with multiple data advantages. To solves the problem\nof reduced resolution of PolSAR images due to system limitations, we propose a\nfully polarimetric synthetic aperture radar (PolSAR) images and\nsingle-polarization synthetic aperture radar SAR (SinSAR) images fusion network\nto generate high-resolution PolSAR (HR-PolSAR) images. To take advantage of the\npolarimetric information of the low-resolution PolSAR (LR-PolSAR) image and the\nspatial information of the high-resolution single-polarization SAR (HR-SinSAR)\nimage, we propose a fusion framework for joint LR-PolSAR image and HR-SinSAR\nimage and design a cross-attention mechanism to extract features from the joint\ninput data. Besides, based on the physical imaging mechanism, we designed the\nPolSAR polarimetric loss function for constrained network training. The\nexperimental results confirm the superiority of fusion network over traditional\nalgorithms. The average PSNR is increased by more than 3.6db, and the average\nMAE is reduced to less than 0.07. Experiments on polarimetric decomposition and\npolarimetric signature show that it maintains polarimetric information well.",
          "link": "http://arxiv.org/abs/2107.08355",
          "publishedOn": "2021-07-20T02:04:41.490Z",
          "wordCount": 621,
          "title": "Fully Polarimetric SAR and Single-Polarization SAR Image Fusion Network. (arXiv:2107.08355v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yijin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pujin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1\">Junyan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>",
          "description": "Manually annotating medical images is extremely expensive, especially for\nlarge-scale datasets. Self-supervised contrastive learning has been explored to\nlearn feature representations from unlabeled images. However, unlike natural\nimages, the application of contrastive learning to medical images is relatively\nlimited. In this work, we propose a self-supervised framework, namely\nlesion-based contrastive learning for automated diabetic retinopathy (DR)\ngrading. Instead of taking entire images as the input in the common contrastive\nlearning scheme, lesion patches are employed to encourage the feature extractor\nto learn representations that are highly discriminative for DR grading. We also\ninvestigate different data augmentation operations in defining our contrastive\nprediction task. Extensive experiments are conducted on the publicly-accessible\ndataset EyePACS, demonstrating that our proposed framework performs\noutstandingly on DR grading in terms of both linear evaluation and transfer\ncapacity evaluation.",
          "link": "http://arxiv.org/abs/2107.08274",
          "publishedOn": "2021-07-20T02:04:41.472Z",
          "wordCount": 588,
          "title": "Lesion-based Contrastive Learning for Diabetic Retinopathy Grading from Fundus Images. (arXiv:2107.08274v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vo_Ho_V/0/1/0/all/0/1\">Viet-Khoa Vo-Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamazaki_K/0/1/0/all/0/1\">Kashu Yamazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_A/0/1/0/all/0/1\">Akihiro Sugimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>",
          "description": "Temporal action proposal generation is an essential and challenging task that\naims at localizing temporal intervals containing human actions in untrimmed\nvideos. Most of existing approaches are unable to follow the human cognitive\nprocess of understanding the video context due to lack of attention mechanism\nto express the concept of an action or an agent who performs the action or the\ninteraction between the agent and the environment. Based on the action\ndefinition that a human, known as an agent, interacts with the environment and\nperforms an action that affects the environment, we propose a contextual\nAgent-Environment Network. Our proposed contextual AEN involves (i) agent\npathway, operating at a local level to tell about which humans/agents are\nacting and (ii) environment pathway operating at a global level to tell about\nhow the agents interact with the environment. Comprehensive evaluations on\n20-action THUMOS-14 and 200-action ActivityNet-1.3 datasets with different\nbackbone networks, i.e C3D and SlowFast, show that our method robustly exhibits\noutperformance against state-of-the-art methods regardless of the employed\nbackbone network.",
          "link": "http://arxiv.org/abs/2107.08323",
          "publishedOn": "2021-07-20T02:04:41.454Z",
          "wordCount": 613,
          "title": "Agent-Environment Network for Temporal Action Proposal Generation. (arXiv:2107.08323v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zare_S/0/1/0/all/0/1\">Samira Zare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hien Van Nguyen</a>",
          "description": "Set-input deep networks have recently drawn much interest in computer vision\nand machine learning. This is in part due to the increasing number of important\ntasks such as meta-learning, clustering, and anomaly detection that are defined\non set inputs. These networks must take an arbitrary number of input samples\nand produce the output invariant to the input set permutation. Several\nalgorithms have been recently developed to address this urgent need. Our paper\nanalyzes these algorithms using both synthetic and real-world datasets, and\nshows that they are not effective in dealing with common data variations such\nas image translation or viewpoint change. To address this limitation, we\npropose a permutation-invariant cascaded attentional set operator (PICASO). The\ngist of PICASO is a cascade of multihead attention blocks with dynamic\ntemplates. The proposed operator is a stand-alone module that can be adapted\nand extended to serve different machine learning tasks. We demonstrate the\nutilities of PICASO in four diverse scenarios: (i) clustering, (ii) image\nclassification under novel viewpoints, (iii) image anomaly detection, and (iv)\nstate prediction. PICASO increases the SmallNORB image classification accuracy\nwith novel viewpoints by about 10% points. For set anomaly detection on CelebA\ndataset, our model improves the areas under ROC and PR curves dataset by about\n22% and 10%, respectively. For the state prediction on CLEVR dataset, it\nimproves the AP by about 40%.",
          "link": "http://arxiv.org/abs/2107.08305",
          "publishedOn": "2021-07-20T02:04:41.436Z",
          "wordCount": 654,
          "title": "PICASO: Permutation-Invariant Cascaded Attentional Set Operator. (arXiv:2107.08305v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_S/0/1/0/all/0/1\">Saravanabalagi Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1\">Ganesh Sistu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1\">John McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>",
          "description": "We present the WoodScape fisheye semantic segmentation challenge for\nautonomous driving which was held as part of the CVPR 2021 Workshop on\nOmnidirectional Computer Vision (OmniCV). This challenge is one of the first\nopportunities for the research community to evaluate the semantic segmentation\ntechniques targeted for fisheye camera perception. Due to strong radial\ndistortion standard models don't generalize well to fisheye images and hence\nthe deformations in the visual appearance of objects and entities needs to be\nencoded implicitly or as explicit knowledge. This challenge served as a medium\nto investigate the challenges and new methodologies to handle the complexities\nwith perception on fisheye images. The challenge was hosted on CodaLab and used\nthe recently released WoodScape dataset comprising of 10k samples. In this\npaper, we provide a summary of the competition which attracted the\nparticipation of 71 global teams and a total of 395 submissions. The top teams\nrecorded significantly improved mean IoU and accuracy scores over the baseline\nPSPNet with ResNet-50 backbone. We summarize the methods of winning algorithms\nand analyze the failure cases. We conclude by providing future directions for\nthe research.",
          "link": "http://arxiv.org/abs/2107.08246",
          "publishedOn": "2021-07-20T02:04:41.417Z",
          "wordCount": 659,
          "title": "Woodscape Fisheye Semantic Segmentation for Autonomous Driving -- CVPR 2021 OmniCV Workshop Challenge. (arXiv:2107.08246v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schott_L/0/1/0/all/0/1\">Lukas Schott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1\">Frederik Tr&#xe4;uble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1\">Peter Gehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1\">Chris Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>",
          "description": "An important component for generalization in machine learning is to uncover\nunderlying latent factors of variation as well as the mechanism through which\neach factor acts in the world. In this paper, we test whether 17 unsupervised,\nweakly supervised, and fully supervised representation learning approaches\ncorrectly infer the generative factors of variation in simple datasets\n(dSprites, Shapes3D, MPI3D). In contrast to prior robustness work that\nintroduces novel factors of variation during test time, such as blur or other\n(un)structured noise, we here recompose, interpolate, or extrapolate only\nexisting factors of variation from the training data set (e.g., small and\nmedium-sized objects during training and large objects during testing). Models\nthat learn the correct mechanism should be able to generalize to this\nbenchmark. In total, we train and test 2000+ models and observe that all of\nthem struggle to learn the underlying mechanism regardless of supervision\nsignal and architectural bias. Moreover, the generalization capabilities of all\ntested models drop significantly as we move from artificial datasets towards\nmore realistic real-world datasets. Despite their inability to identify the\ncorrect mechanism, the models are quite modular as their ability to infer other\nin-distribution factors remains fairly stable, providing only a single factor\nis out-of-distribution. These results point to an important yet understudied\nproblem of learning mechanistic models of observations that can facilitate\ngeneralization.",
          "link": "http://arxiv.org/abs/2107.08221",
          "publishedOn": "2021-07-20T02:04:41.399Z",
          "wordCount": 673,
          "title": "Visual Representation Learning Does Not Generalize Strongly Within the Same Domain. (arXiv:2107.08221v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Lisha Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_L/0/1/0/all/0/1\">Lap-Pui Chau</a>",
          "description": "Vehicle re-identification (Re-ID) is to retrieve images of the same vehicle\nacross different cameras. Two key challenges lie in the subtle inter-instance\ndiscrepancy caused by near-duplicate identities and the large intra-instance\nvariance caused by different views. Since the holistic appearance suffers from\nviewpoint variation and distortion, part-level feature learning has been\nintroduced to enhance vehicle description. However, existing approaches to\nlocalize and amplify significant parts often fail to handle spatial\nmisalignment as well as occlusion and require expensive annotations. In this\npaper, we propose a weakly supervised Part-Mentored Attention Network (PMANet)\ncomposed of a Part Attention Network (PANet) for vehicle part localization with\nself-attention and a Part-Mentored Network (PMNet) for mentoring the global and\nlocal feature aggregation. Firstly, PANet is introduced to predict a foreground\nmask and pinpoint $K$ prominent vehicle parts only with weak identity\nsupervision. Secondly, we propose a PMNet to learn global and part-level\nfeatures with multi-scale attention and aggregate them in $K$ main-partial\ntasks via part transfer. Like humans who first differentiate objects with\ngeneral information and then observe salient parts for more detailed clues,\nPANet and PMNet construct a two-stage attention structure to perform a\ncoarse-to-fine search among identities. Finally, we address this Re-ID issue as\na multi-task problem, including global feature learning, identity\nclassification, and part transfer. We adopt Homoscedastic Uncertainty to learn\nthe optimal weighing of different losses. Comprehensive experiments are\nconducted on two benchmark datasets. Our approach outperforms recent\nstate-of-the-art methods by averagely 2.63% in CMC@1 on VehicleID and 2.2% in\nmAP on VeRi776. Results on occluded test sets also demonstrate the\ngeneralization ability of PMANet.",
          "link": "http://arxiv.org/abs/2107.08228",
          "publishedOn": "2021-07-20T02:04:41.369Z",
          "wordCount": 733,
          "title": "Looking Twice for Partial Clues: Weakly-supervised Part-Mentored Attention Network for Vehicle Re-Identification. (arXiv:2107.08228v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yunqing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1\">Haiwen Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>",
          "description": "In fine-grained image recognition (FGIR), the localization and amplification\nof region attention is an important factor, which has been explored a lot by\nconvolutional neural networks (CNNs) based approaches. The recently developed\nvision transformer (ViT) has achieved promising results on computer vision\ntasks. Compared with CNNs, Image sequentialization is a brand new manner.\nHowever, ViT is limited in its receptive field size and thus lacks local\nattention like CNNs due to the fixed size of its patches, and is unable to\ngenerate multi-scale features to learn discriminative region attention. To\nfacilitate the learning of discriminative region attention without box/part\nannotations, we use the strength of the attention weights to measure the\nimportance of the patch tokens corresponding to the raw images. We propose the\nrecurrent attention multi-scale transformer (RAMS-Trans), which uses the\ntransformer's self-attention to recursively learn discriminative region\nattention in a multi-scale manner. Specifically, at the core of our approach\nlies the dynamic patch proposal module (DPPM) guided region amplification to\ncomplete the integration of multi-scale image patches. The DPPM starts with the\nfull-size image patches and iteratively scales up the region attention to\ngenerate new patches from global to local by the intensity of the attention\nweights generated at each scale as an indicator. Our approach requires only the\nattention weights that come with ViT itself and can be easily trained\nend-to-end. Extensive experiments demonstrate that RAMS-Trans performs better\nthan concurrent works, in addition to efficient CNN models, achieving\nstate-of-the-art results on three benchmark datasets.",
          "link": "http://arxiv.org/abs/2107.08192",
          "publishedOn": "2021-07-20T02:04:41.304Z",
          "wordCount": 689,
          "title": "RAMS-Trans: Recurrent Attention Multi-scale Transformer forFine-grained Image Recognition. (arXiv:2107.08192v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hengli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "Convolutional neural network (CNN)-based stereo matching approaches generally\nrequire a dense cost volume (DCV) for disparity estimation. However, generating\nsuch cost volumes is computationally-intensive and memory-consuming, hindering\nCNN training and inference efficiency. To address this problem, we propose\nSCV-Stereo, a novel CNN architecture, capable of learning dense stereo matching\nfrom sparse cost volume (SCV) representations. Our inspiration is derived from\nthe fact that DCV representations are somewhat redundant and can be replaced\nwith SCV representations. Benefiting from these SCV representations, our\nSCV-Stereo can update disparity estimations in an iterative fashion for\naccurate and efficient stereo matching. Extensive experiments carried out on\nthe KITTI Stereo benchmarks demonstrate that our SCV-Stereo can significantly\nminimize the trade-off between accuracy and efficiency for stereo matching. Our\nproject page is https://sites.google.com/view/scv-stereo.",
          "link": "http://arxiv.org/abs/2107.08187",
          "publishedOn": "2021-07-20T02:04:41.252Z",
          "wordCount": 582,
          "title": "SCV-Stereo: Learning Stereo Matching from a Sparse Cost Volume. (arXiv:2107.08187v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hengli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "Stereo matching is a key component of autonomous driving perception. Recent\nunsupervised stereo matching approaches have received adequate attention due to\ntheir advantage of not requiring disparity ground truth. These approaches,\nhowever, perform poorly near occlusions. To overcome this drawback, in this\npaper, we propose CoT-Stereo, a novel unsupervised stereo matching approach.\nSpecifically, we adopt a co-teaching framework where two networks interactively\nteach each other about the occlusions in an unsupervised fashion, which greatly\nimproves the robustness of unsupervised stereo matching. Extensive experiments\non the KITTI Stereo benchmarks demonstrate the superior performance of\nCoT-Stereo over all other state-of-the-art unsupervised stereo matching\napproaches in terms of both accuracy and speed. Our project webpage is\nhttps://sites.google.com/view/cot-stereo.",
          "link": "http://arxiv.org/abs/2107.08186",
          "publishedOn": "2021-07-20T02:04:41.214Z",
          "wordCount": 566,
          "title": "Co-Teaching: An Ark to Unsupervised Stereo Matching. (arXiv:2107.08186v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08120",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yilin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yap_P/0/1/0/all/0/1\">Pew-Thian Yap</a>",
          "description": "Magnetic resonance Fingerprinting (MRF) is a relatively new multi-parametric\nquantitative imaging method that involves a two-step process: (i)\nreconstructing a series of time frames from highly-undersampled non-Cartesian\nspiral k-space data and (ii) pattern matching using the time frames to infer\ntissue properties (e.g., T1 and T2 relaxation times). In this paper, we\nintroduce a novel end-to-end deep learning framework to seamlessly map the\ntissue properties directly from spiral k-space MRF data, thereby avoiding\ntime-consuming processing such as the nonuniform fast Fourier transform (NUFFT)\nand the dictionary-based Fingerprint matching. Our method directly consumes the\nnon-Cartesian k- space data, performs adaptive density compensation, and\npredicts multiple tissue property maps in one forward pass. Experiments on both\n2D and 3D MRF data demonstrate that quantification accuracy comparable to\nstate-of-the-art methods can be accomplished within 0.5 second, which is 1100\nto 7700 times faster than the original MRF framework. The proposed method is\nthus promising for facilitating the adoption of MRF in clinical settings.",
          "link": "http://arxiv.org/abs/2107.08120",
          "publishedOn": "2021-07-20T02:04:40.658Z",
          "wordCount": 610,
          "title": "Real-Time Mapping of Tissue Properties for Magnetic Resonance Fingerprinting. (arXiv:2107.08120v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09022",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chung_H/0/1/0/all/0/1\">Hyungjin Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huh_J/0/1/0/all/0/1\">Jaeyoung Huh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_G/0/1/0/all/0/1\">Geon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_Y/0/1/0/all/0/1\">Yong Keun Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>",
          "description": "Optical diffraction tomography (ODT) produces three dimensional distribution\nof refractive index (RI) by measuring scattering fields at various angles.\nAlthough the distribution of RI index is highly informative, due to the missing\ncone problem stemming from the limited-angle acquisition of holograms,\nreconstructions have very poor resolution along axial direction compared to the\nhorizontal imaging plane. To solve this issue, here we present a novel\nunsupervised deep learning framework, which learns the probability distribution\nof missing projection views through optimal transport driven cycleGAN.\nExperimental results show that missing cone artifact in ODT can be\nsignificantly resolved by the proposed method.",
          "link": "http://arxiv.org/abs/2103.09022",
          "publishedOn": "2021-07-20T02:04:40.602Z",
          "wordCount": 589,
          "title": "Missing Cone Artifacts Removal in ODT using Unsupervised Deep Learning in Projection Domain. (arXiv:2103.09022v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Ruojin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1\">Bharath Hariharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1\">Hadar Averbuch-Elor</a>",
          "description": "We present a technique for estimating the relative 3D rotation of an RGB\nimage pair in an extreme setting, where the images have little or no overlap.\nWe observe that, even when images do not overlap, there may be rich hidden cues\nas to their geometric relationship, such as light source directions, vanishing\npoints, and symmetries present in the scene. We propose a network design that\ncan automatically learn such implicit cues by comparing all pairs of points\nbetween the two input images. Our method therefore constructs dense feature\ncorrelation volumes and processes these to predict relative 3D rotations. Our\npredictions are formed over a fine-grained discretization of rotations,\nbypassing difficulties associated with regressing 3D rotations. We demonstrate\nour approach on a large variety of extreme RGB image pairs, including indoor\nand outdoor images captured under different lighting conditions and geographic\nlocations. Our evaluation shows that our model can successfully estimate\nrelative rotations among non-overlapping images without compromising\nperformance over overlapping image pairs.",
          "link": "http://arxiv.org/abs/2104.13530",
          "publishedOn": "2021-07-20T02:04:40.583Z",
          "wordCount": 635,
          "title": "Extreme Rotation Estimation using Dense Correlation Volumes. (arXiv:2104.13530v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1\">Masahiro Kato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teshima_T/0/1/0/all/0/1\">Takeshi Teshima</a>",
          "description": "Density ratio estimation (DRE) is at the core of various machine learning\ntasks such as anomaly detection and domain adaptation. In existing studies on\nDRE, methods based on Bregman divergence (BD) minimization have been\nextensively studied. However, BD minimization when applied with highly flexible\nmodels, such as deep neural networks, tends to suffer from what we call\ntrain-loss hacking, which is a source of overfitting caused by a typical\ncharacteristic of empirical BD estimators. In this paper, to mitigate\ntrain-loss hacking, we propose a non-negative correction for empirical BD\nestimators. Theoretically, we confirm the soundness of the proposed method\nthrough a generalization error bound. Through our experiments, the proposed\nmethods show a favorable performance in inlier-based outlier detection.",
          "link": "http://arxiv.org/abs/2006.06979",
          "publishedOn": "2021-07-20T02:04:40.526Z",
          "wordCount": 596,
          "title": "Non-Negative Bregman Divergence Minimization for Deep Direct Density Ratio Estimation. (arXiv:2006.06979v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.14595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marzahl_C/0/1/0/all/0/1\">Christian Marzahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubreville_M/0/1/0/all/0/1\">Marc Aubreville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertram_C/0/1/0/all/0/1\">Christof A. Bertram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_J/0/1/0/all/0/1\">Jennifer Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergler_C/0/1/0/all/0/1\">Christian Bergler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kroger_C/0/1/0/all/0/1\">Christine Kr&#xf6;ger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voigt_J/0/1/0/all/0/1\">J&#xf6;rn Voigt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klopfleisch_R/0/1/0/all/0/1\">Robert Klopfleisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>",
          "description": "In many research areas, scientific progress is accelerated by\nmultidisciplinary access to image data and their interdisciplinary annotation.\nHowever, keeping track of these annotations to ensure a high-quality\nmulti-purpose data set is a challenging and labour intensive task. We developed\nthe open-source online platform EXACT (EXpert Algorithm Collaboration Tool)\nthat enables the collaborative interdisciplinary analysis of images from\ndifferent domains online and offline. EXACT supports multi-gigapixel medical\nwhole slide images as well as image series with thousands of images. The\nsoftware utilises a flexible plugin system that can be adapted to diverse\napplications such as counting mitotic figures with a screening mode, finding\nfalse annotations on a novel validation view, or using the latest deep learning\nimage analysis technologies. This is combined with a version control system\nwhich makes it possible to keep track of changes in the data sets and, for\nexample, to link the results of deep learning experiments to specific data set\nversions. EXACT is freely available and has already been successfully applied\nto a broad range of annotation tasks, including highly diverse applications\nlike deep learning supported cytology scoring, interdisciplinary multi-centre\nwhole slide image tumour annotation, and highly specialised whale sound\nspectroscopy clustering.",
          "link": "http://arxiv.org/abs/2004.14595",
          "publishedOn": "2021-07-20T02:04:40.502Z",
          "wordCount": 712,
          "title": "EXACT: A collaboration toolset for algorithm-aided annotation of images with annotation version control. (arXiv:2004.14595v3 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lejeune_L/0/1/0/all/0/1\">Laurent Lejeune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1\">Raphael Sznitman</a>",
          "description": "The ability to quickly annotate medical imaging data plays a critical role in\ntraining deep learning frameworks for segmentation. Doing so for image volumes\nor video sequences is even more pressing as annotating these is particularly\nburdensome. To alleviate this problem, this work proposes a new method to\nefficiently segment medical imaging volumes or videos using point-wise\nannotations only. This allows annotations to be collected extremely quickly and\nremains applicable to numerous segmentation tasks. Our approach trains a deep\nlearning model using an appropriate Positive/Unlabeled objective function using\nsparse point-wise annotations. While most methods of this kind assume that the\nproportion of positive samples in the data is known a-priori, we introduce a\nnovel self-supervised method to estimate this prior efficiently by combining a\nBayesian estimation framework and new stopping criteria. Our method iteratively\nestimates appropriate class priors and yields high segmentation quality for a\nvariety of object types and imaging modalities. In addition, by leveraging a\nspatio-temporal tracking framework, we regularize our predictions by leveraging\nthe complete data volume. We show experimentally that our approach outperforms\nstate-of-the-art methods tailored to the same problem.",
          "link": "http://arxiv.org/abs/2107.08394",
          "publishedOn": "2021-07-20T02:04:40.480Z",
          "wordCount": 623,
          "title": "A Positive/Unlabeled Approach for the Segmentation of Medical Sequences using Point-Wise Supervision. (arXiv:2107.08394v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2105.07965",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_A/0/1/0/all/0/1\">Arpita Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_G/0/1/0/all/0/1\">Gaurav Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varakantham_P/0/1/0/all/0/1\">Pradeep Varakantham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1\">Milind Tambe</a>",
          "description": "In many public health settings, it is important for patients to adhere to\nhealth programs, such as taking medications and periodic health checks.\nUnfortunately, beneficiaries may gradually disengage from such programs, which\nis detrimental to their health. A concrete example of gradual disengagement has\nbeen observed by an organization that carries out a free automated call-based\nprogram for spreading preventive care information among pregnant women. Many\nwomen stop picking up calls after being enrolled for a few months. To avoid\nsuch disengagements, it is important to provide timely interventions. Such\ninterventions are often expensive and can be provided to only a small fraction\nof the beneficiaries. We model this scenario as a restless multi-armed bandit\n(RMAB) problem, where each beneficiary is assumed to transition from one state\nto another depending on the intervention. Moreover, since the transition\nprobabilities are unknown a priori, we propose a Whittle index based Q-Learning\nmechanism and show that it converges to the optimal solution. Our method\nimproves over existing learning-based methods for RMABs on multiple benchmarks\nfrom literature and also on the maternal healthcare dataset.",
          "link": "http://arxiv.org/abs/2105.07965",
          "publishedOn": "2021-07-26T02:01:00.229Z",
          "wordCount": 670,
          "title": "Learn to Intervene: An Adaptive Learning Policy for Restless Bandits in Application to Preventive Healthcare. (arXiv:2105.07965v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07455",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malinin_A/0/1/0/all/0/1\">Andrey Malinin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Band_N/0/1/0/all/0/1\">Neil Band</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganshin/0/1/0/all/0/1\">Ganshin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexander/0/1/0/all/0/1\">Alexander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chesnokov_G/0/1/0/all/0/1\">German Chesnokov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noskov_A/0/1/0/all/0/1\">Alexey Noskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ploskonosov_A/0/1/0/all/0/1\">Andrey Ploskonosov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prokhorenkova_L/0/1/0/all/0/1\">Liudmila Prokhorenkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Provilkov_I/0/1/0/all/0/1\">Ivan Provilkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vatsal Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vyas Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roginskiy/0/1/0/all/0/1\">Roginskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denis/0/1/0/all/0/1\">Denis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmatova_M/0/1/0/all/0/1\">Mariya Shmatova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tigas_P/0/1/0/all/0/1\">Panos Tigas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yangel_B/0/1/0/all/0/1\">Boris Yangel</a>",
          "description": "There has been significant research done on developing methods for improving\nrobustness to distributional shift and uncertainty estimation. In contrast,\nonly limited work has examined developing standard datasets and benchmarks for\nassessing these approaches. Additionally, most work on uncertainty estimation\nand robustness has developed new techniques based on small-scale regression or\nimage classification tasks. However, many tasks of practical interest have\ndifferent modalities, such as tabular data, audio, text, or sensor data, which\noffer significant challenges involving regression and discrete or continuous\nstructured prediction. Thus, given the current state of the field, a\nstandardized large-scale dataset of tasks across a range of modalities affected\nby distributional shifts is necessary. This will enable researchers to\nmeaningfully evaluate the plethora of recently developed uncertainty\nquantification methods, as well as assessment criteria and state-of-the-art\nbaselines. In this work, we propose the \\emph{Shifts Dataset} for evaluation of\nuncertainty estimates and robustness to distributional shift. The dataset,\nwhich has been collected from industrial sources and services, is composed of\nthree tasks, with each corresponding to a particular data modality: tabular\nweather prediction, machine translation, and self-driving car (SDC) vehicle\nmotion prediction. All of these data modalities and tasks are affected by real,\n`in-the-wild' distributional shifts and pose interesting challenges with\nrespect to uncertainty estimation. In this work we provide a description of the\ndataset and baseline results for all tasks.",
          "link": "http://arxiv.org/abs/2107.07455",
          "publishedOn": "2021-07-26T02:01:00.222Z",
          "wordCount": 714,
          "title": "Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks. (arXiv:2107.07455v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12342",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Gotoh_J/0/1/0/all/0/1\">Jun-ya Gotoh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kim_M/0/1/0/all/0/1\">Michael Jong Kim</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lim_A/0/1/0/all/0/1\">Andrew E.B. Lim</a>",
          "description": "While solutions of Distributionally Robust Optimization (DRO) problems can\nsometimes have a higher out-of-sample expected reward than the Sample Average\nApproximation (SAA), there is no guarantee. In this paper, we introduce the\nclass of Distributionally Optimistic Optimization (DOO) models, and show that\nit is always possible to \"beat\" SAA out-of-sample if we consider not just\nworst-case (DRO) models but also best-case (DOO) ones. We also show, however,\nthat this comes at a cost: Optimistic solutions are more sensitive to model\nerror than either worst-case or SAA optimizers, and hence are less robust.",
          "link": "http://arxiv.org/abs/2105.12342",
          "publishedOn": "2021-07-26T02:01:00.214Z",
          "wordCount": 576,
          "title": "A data-driven approach to beating SAA out-of-sample. (arXiv:2105.12342v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1\">Pietro Barbiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1\">Gabriele Ciravegna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_D/0/1/0/all/0/1\">Dobrik Georgiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1\">Franscesco Giannini</a>",
          "description": "\"PyTorch, Explain!\" is a Python module integrating a variety of\nstate-of-the-art approaches to provide logic explanations from neural networks.\nThis package focuses on bringing these methods to non-specialists. It has\nminimal dependencies and it is distributed under the Apache 2.0 licence\nallowing both academic and commercial use. Source code and documentation can be\ndownloaded from the github repository:\nhttps://github.com/pietrobarbiero/pytorch_explain.",
          "link": "http://arxiv.org/abs/2105.11697",
          "publishedOn": "2021-07-26T02:01:00.205Z",
          "wordCount": 522,
          "title": "PyTorch, Explain! A Python library for Logic Explained Networks. (arXiv:2105.11697v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05852",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Adiga_D/0/1/0/all/0/1\">Devaraja Adiga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_R/0/1/0/all/0/1\">Rishabh Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krishna_A/0/1/0/all/0/1\">Amrith Krishna</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>",
          "description": "Automatic speech recognition (ASR) in Sanskrit is interesting, owing to the\nvarious linguistic peculiarities present in the language. The Sanskrit language\nis lexically productive, undergoes euphonic assimilation of phones at the word\nboundaries and exhibits variations in spelling conventions and in\npronunciations. In this work, we propose the first large scale study of\nautomatic speech recognition (ASR) in Sanskrit, with an emphasis on the impact\nof unit selection in Sanskrit ASR. In this work, we release a 78 hour ASR\ndataset for Sanskrit, which faithfully captures several of the linguistic\ncharacteristics expressed by the language. We investigate the role of different\nacoustic model and language model units in ASR systems for Sanskrit. We also\npropose a new modelling unit, inspired by the syllable level unit selection,\nthat captures character sequences from one vowel in the word to the next vowel.\nWe also highlight the importance of choosing graphemic representations for\nSanskrit and show the impact of this choice on word error rates (WER). Finally,\nwe extend these insights from Sanskrit ASR for building ASR systems in two\nother Indic languages, Gujarati and Telugu. For both these languages, our\nexperimental results show that the use of phonetic based graphemic\nrepresentations in ASR results in performance improvements as compared to ASR\nsystems that use native scripts.",
          "link": "http://arxiv.org/abs/2106.05852",
          "publishedOn": "2021-07-26T02:01:00.198Z",
          "wordCount": 708,
          "title": "Automatic Speech Recognition in Sanskrit: A New Speech Corpus and Modelling Insights. (arXiv:2106.05852v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15561",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soong_F/0/1/0/all/0/1\">Frank Soong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Text to speech (TTS), or speech synthesis, which aims to synthesize\nintelligible and natural speech given text, is a hot research topic in speech,\nlanguage, and machine learning communities and has broad applications in the\nindustry. As the development of deep learning and artificial intelligence,\nneural network-based TTS has significantly improved the quality of synthesized\nspeech in recent years. In this paper, we conduct a comprehensive survey on\nneural TTS, aiming to provide a good understanding of current research and\nfuture trends. We focus on the key components in neural TTS, including text\nanalysis, acoustic models and vocoders, and several advanced topics, including\nfast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.\nWe further summarize resources related to TTS (e.g., datasets, opensource\nimplementations) and discuss future research directions. This survey can serve\nboth academic researchers and industry practitioners working on TTS.",
          "link": "http://arxiv.org/abs/2106.15561",
          "publishedOn": "2021-07-26T02:01:00.174Z",
          "wordCount": 633,
          "title": "A Survey on Neural Speech Synthesis. (arXiv:2106.15561v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadghiani_N/0/1/0/all/0/1\">Nima Salehi Sadghiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Zhuo Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1\">Andrew Cohen</a>",
          "description": "From the ad network standpoint, a user's activity is a multi-type sequence of\ntemporal events consisting of event types and time intervals. Understanding\nuser patterns in ad networks has received increasing attention from the machine\nlearning community. Particularly, the problems of fraud detection, Conversion\nRate (CVR), and Click-Through Rate (CTR) prediction are of interest. However,\nthe class imbalance between major and minor classes in these tasks can bias a\nmachine learning model leading to poor performance. This study proposes using\ntwo multi-type (continuous and discrete) training approaches for GANs to deal\nwith the limitations of traditional GANs in passing the gradient updates for\ndiscrete tokens. First, we used the Reinforcement Learning (RL)-based training\napproach and then, an approximation of the multinomial distribution\nparameterized in terms of the softmax function (Gumble-Softmax). Our extensive\nexperiments based on synthetic data have shown the trained generator can\ngenerate sequences with desired properties measured by multiple criteria.",
          "link": "http://arxiv.org/abs/2104.03428",
          "publishedOn": "2021-07-26T02:01:00.167Z",
          "wordCount": 607,
          "title": "Generating Multi-type Temporal Sequences to Mitigate Class-imbalanced Problem. (arXiv:2104.03428v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rizoiu_M/0/1/0/all/0/1\">Marian-Andrei Rizoiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soen_A/0/1/0/all/0/1\">Alexander Soen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shidi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calderon_P/0/1/0/all/0/1\">Pio Calderon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Leanne Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1\">Aditya Krishna Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lexing Xie</a>",
          "description": "This work builds a novel point process and tools to use the Hawkes process\nwith interval-censored data. Such data records the aggregated counts of events\nsolely during specific time intervals -- such as the number of patients\nadmitted to the hospital or the volume of vehicles passing traffic loop\ndetectors -- and not the exact occurrence time of the events. First, we\nestablish the Mean Behavior Poisson (MBP) process, a novel Poisson process with\na direct parameter correspondence to the popular self-exciting Hawkes process.\nThe event intensity function of the MBP is the expected intensity over all\npossible Hawkes realizations with the same parameter set. We fit MBP in the\ninterval-censored setting using an interval-censored Poisson log-likelihood\n(IC-LL). We use the parameter equivalence to uncover the parameters of the\nassociated Hawkes process. Second, we introduce two novel exogenous functions\nto distinguish the exogenous from the endogenous events. We propose the\nmulti-impulse exogenous function when the exogenous events are observed as\nevent time and the latent homogeneous Poisson process exogenous function when\nthe exogenous events are presented as interval-censored volumes. Third, we\nprovide several approximation methods to estimate the intensity and compensator\nfunction of MBP when no analytical solution exists. Fourth and finally, we\nconnect the interval-censored loss of MBP to a broader class of Bregman\ndivergence-based functions. Using the connection, we show that the current\nstate of the art in popularity estimation (Hawkes Intensity Process (HIP)\n(Rizoiu et al.,2017b)) is a particular case of the MBP process. We verify our\nmodels through empirical testing on synthetic data and real-world data. We find\nthat on real-world datasets that our MBP process outperforms HIP for the task\nof popularity prediction.",
          "link": "http://arxiv.org/abs/2104.07932",
          "publishedOn": "2021-07-26T02:01:00.159Z",
          "wordCount": 755,
          "title": "Interval-censored Hawkes processes. (arXiv:2104.07932v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "Global covariance pooling (GCP) aims at exploiting the second-order\nstatistics of the convolutional feature. Its effectiveness has been\ndemonstrated in boosting the classification performance of Convolutional Neural\nNetworks (CNNs). Singular Value Decomposition (SVD) is used in GCP to compute\nthe matrix square root. However, the approximate matrix square root calculated\nusing Newton-Schulz iteration \\cite{li2018towards} outperforms the accurate one\ncomputed via SVD \\cite{li2017second}. We empirically analyze the reason behind\nthe performance gap from the perspectives of data precision and gradient\nsmoothness. Various remedies for computing smooth SVD gradients are\ninvestigated. Based on our observation and analyses, a hybrid training protocol\nis proposed for SVD-based GCP meta-layers such that competitive performances\ncan be achieved against Newton-Schulz iteration. Moreover, we propose a new GCP\nmeta-layer that uses SVD in the forward pass, and Pad\\'e Approximants in the\nbackward propagation to compute the gradients. The proposed meta-layer has been\nintegrated into different CNN models and achieves state-of-the-art performances\non both large-scale and fine-grained datasets.",
          "link": "http://arxiv.org/abs/2105.02498",
          "publishedOn": "2021-07-26T02:01:00.152Z",
          "wordCount": 639,
          "title": "Why Approximate Matrix Square Root Outperforms Accurate SVD in Global Covariance Pooling?. (arXiv:2105.02498v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zekai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dingshuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zixuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiuzhen Cheng</a>",
          "description": "Many real-world IoT systems, which include a variety of internet-connected\nsensory devices, produce substantial amounts of multivariate time series data.\nMeanwhile, vital IoT infrastructures like smart power grids and water\ndistribution networks are frequently targeted by cyber-attacks, making anomaly\ndetection an important study topic. Modeling such relatedness is, nevertheless,\nunavoidable for any efficient and effective anomaly detection system, given the\nintricate topological and nonlinear connections that are originally unknown\namong sensors. Furthermore, detecting anomalies in multivariate time series is\ndifficult due to their temporal dependency and stochasticity. This paper\npresented GTA, a new framework for multivariate time series anomaly detection\nthat involves automatically learning a graph structure, graph convolution, and\nmodeling temporal dependency using a Transformer-based architecture. The\nconnection learning policy, which is based on the Gumbel-softmax sampling\napproach to learn bi-directed links among sensors directly, is at the heart of\nlearning graph structure. To describe the anomaly information flow between\nnetwork nodes, we introduced a new graph convolution called Influence\nPropagation convolution. In addition, to tackle the quadratic complexity\nbarrier, we suggested a multi-branch attention mechanism to replace the\noriginal multi-head self-attention method. Extensive experiments on four\npublicly available anomaly detection benchmarks further demonstrate the\nsuperiority of our approach over alternative state-of-the-arts.",
          "link": "http://arxiv.org/abs/2104.03466",
          "publishedOn": "2021-07-26T02:01:00.144Z",
          "wordCount": 696,
          "title": "Learning Graph Structures with Transformer for Multivariate Time Series Anomaly Detection in IoT. (arXiv:2104.03466v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monperrus_M/0/1/0/all/0/1\">Martin Monperrus</a>",
          "description": "Semantic code search is about finding semantically relevant code snippets for\na given natural language query. In the state-of-the-art approaches, the\nsemantic similarity between code and query is quantified as the distance of\ntheir representation in the shared vector space. In this paper, to improve the\nvector space, we introduce tree-serialization methods on a simplified form of\nAST and build the multimodal representation for the code data. We conduct\nextensive experiments using a single corpus that is large-scale and\nmulti-language: CodeSearchNet. Our results show that both our tree-serialized\nrepresentations and multimodal learning model improve the performance of code\nsearch. Last, we define intuitive quantification metrics oriented to the\ncompleteness of semantic and syntactic information of the code data, to help\nunderstand the experimental findings.",
          "link": "http://arxiv.org/abs/2107.00992",
          "publishedOn": "2021-07-26T02:01:00.137Z",
          "wordCount": 580,
          "title": "Multimodal Representation for Neural Code Search. (arXiv:2107.00992v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.03148",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Reisinger_C/0/1/0/all/0/1\">Christoph Reisinger</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_Y/0/1/0/all/0/1\">Yufei Zhang</a>",
          "description": "This paper proposes a relaxed control regularization with general exploration\nrewards to design robust feedback controls for multi-dimensional\ncontinuous-time stochastic exit time problems. We establish that the\nregularized control problem admits a H\\\"{o}lder continuous feedback control,\nand demonstrate that both the value function and the feedback control of the\nregularized control problem are Lipschitz stable with respect to parameter\nperturbations. Moreover, we show that a pre-computed feedback relaxed control\nhas a robust performance in a perturbed system, and derive a first-order\nsensitivity equation for both the value function and optimal feedback relaxed\ncontrol. These stability results provide a theoretical justification for recent\nreinforcement learning heuristics that including an exploration reward in the\noptimization objective leads to more robust decision making. We finally prove\nfirst-order monotone convergence of the value functions for relaxed control\nproblems with vanishing exploration parameters, which subsequently enables us\nto construct the pure exploitation strategy of the original control problem\nbased on the feedback relaxed controls.",
          "link": "http://arxiv.org/abs/2001.03148",
          "publishedOn": "2021-07-26T02:01:00.113Z",
          "wordCount": 642,
          "title": "Regularity and stability of feedback relaxed controls. (arXiv:2001.03148v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kayo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moryossef_A/0/1/0/all/0/1\">Amit Moryossef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hochgesang_J/0/1/0/all/0/1\">Julie Hochgesang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>",
          "description": "Signed languages are the primary means of communication for many deaf and\nhard of hearing individuals. Since signed languages exhibit all the fundamental\nlinguistic properties of natural language, we believe that tools and theories\nof Natural Language Processing (NLP) are crucial towards its modeling. However,\nexisting research in Sign Language Processing (SLP) seldom attempt to explore\nand leverage the linguistic organization of signed languages. This position\npaper calls on the NLP community to include signed languages as a research area\nwith high social and scientific impact. We first discuss the linguistic\nproperties of signed languages to consider during their modeling. Then, we\nreview the limitations of current SLP models and identify the open challenges\nto extend NLP to signed languages. Finally, we urge (1) the adoption of an\nefficient tokenization method; (2) the development of linguistically-informed\nmodels; (3) the collection of real-world signed language data; (4) the\ninclusion of local signed language communities as an active and leading voice\nin the direction of research.",
          "link": "http://arxiv.org/abs/2105.05222",
          "publishedOn": "2021-07-26T02:01:00.106Z",
          "wordCount": 638,
          "title": "Including Signed Languages in Natural Language Processing. (arXiv:2105.05222v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08659",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Beknazaryan_A/0/1/0/all/0/1\">Aleksandr Beknazaryan</a>",
          "description": "In this paper it is shown that $C_\\beta$-smooth functions can be approximated\nby deep neural networks with ReLU activation function and with parameters\n$\\{0,\\pm \\frac{1}{2}, \\pm 1, 2\\}$. The $l_0$ and $l_1$ parameter norms of\nconsidered networks are thus equivalent. The depth, width and the number of\nactive parameters of the constructed networks have, up to a logarithmic factor,\nthe same dependence on the approximation error as the networks with parameters\nin $[-1,1]$. In particular, this means that the nonparametric regression\nestimation with the constructed networks attains the same convergence rate as\nwith sparse networks with parameters in $[-1,1]$.",
          "link": "http://arxiv.org/abs/2103.08659",
          "publishedOn": "2021-07-26T02:01:00.098Z",
          "wordCount": 563,
          "title": "Function approximation by deep neural networks with parameters $\\{0,\\pm \\frac{1}{2}, \\pm 1, 2\\}$. (arXiv:2103.08659v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bastianello_N/0/1/0/all/0/1\">Nicola Bastianello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonetto_A/0/1/0/all/0/1\">Andrea Simonetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DallAnese_E/0/1/0/all/0/1\">Emiliano Dall&#x27;Anese</a>",
          "description": "This paper presents a new regularization approach -- termed OpReg-Boost -- to\nboost the convergence and lessen the asymptotic error of online optimization\nand learning algorithms. In particular, the paper considers online algorithms\nfor optimization problems with a time-varying (weakly) convex composite cost.\nFor a given online algorithm, OpReg-Boost learns the closest algorithmic map\nthat yields linear convergence; to this end, the learning procedure hinges on\nthe concept of operator regression. We show how to formalize the operator\nregression problem and propose a computationally-efficient Peaceman-Rachford\nsolver that exploits a closed-form solution of simple quadratically-constrained\nquadratic programs (QCQPs). Simulation results showcase the superior properties\nof OpReg-Boost w.r.t. the more classical forward-backward algorithm, FISTA, and\nAnderson acceleration, and with respect to its close relative\nconvex-regression-boost (CvxReg-Boost) which is also novel but less performing.",
          "link": "http://arxiv.org/abs/2105.13271",
          "publishedOn": "2021-07-26T02:01:00.075Z",
          "wordCount": 601,
          "title": "OpReg-Boost: Learning to Accelerate Online Algorithms with Operator Regression. (arXiv:2105.13271v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gauthier_D/0/1/0/all/0/1\">Daniel J. Gauthier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollt_E/0/1/0/all/0/1\">Erik Bollt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffith_A/0/1/0/all/0/1\">Aaron Griffith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbosa_W/0/1/0/all/0/1\">Wendson A.S. Barbosa</a>",
          "description": "Reservoir computing is a best-in-class machine learning algorithm for\nprocessing information generated by dynamical systems using observed\ntime-series data. Importantly, it requires very small training data sets, uses\nlinear optimization, and thus requires minimal computing resources. However,\nthe algorithm uses randomly sampled matrices to define the underlying recurrent\nneural network and has a multitude of metaparameters that must be optimized.\nRecent results demonstrate the equivalence of reservoir computing to nonlinear\nvector autoregression, which requires no random matrices, fewer metaparameters,\nand provides interpretable results. Here, we demonstrate that nonlinear vector\nautoregression excels at reservoir computing benchmark tasks and requires even\nshorter training data sets and training time, heralding the next generation of\nreservoir computing.",
          "link": "http://arxiv.org/abs/2106.07688",
          "publishedOn": "2021-07-26T02:01:00.067Z",
          "wordCount": 589,
          "title": "Next Generation Reservoir Computing. (arXiv:2106.07688v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yaakoubi_Y/0/1/0/all/0/1\">Yassine Yaakoubi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soumis_F/0/1/0/all/0/1\">Fran&#xe7;ois Soumis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1\">Simon Lacoste-Julien</a>",
          "description": "Motivated by the needs from an airline crew scheduling application, we\nintroduce structured convolutional kernel networks (Struct-CKN), which combine\nCKNs from Mairal et al. (2014) in a structured prediction framework that\nsupports constraints on the outputs. CKNs are a particular kind of\nconvolutional neural networks that approximate a kernel feature map on training\ndata, thus combining properties of deep learning with the non-parametric\nflexibility of kernel methods. Extending CKNs to structured outputs allows us\nto obtain useful initial solutions on a flight-connection dataset that can be\nfurther refined by an airline crew scheduling solver. More specifically, we use\na flight-based network modeled as a general conditional random field capable of\nincorporating local constraints in the learning process. Our experiments\ndemonstrate that this approach yields significant improvements for the\nlarge-scale crew pairing problem (50,000 flights per month) over standard\napproaches, reducing the solution cost by 17% (a gain of millions of dollars)\nand the cost of global constraints by 97%.",
          "link": "http://arxiv.org/abs/2105.11646",
          "publishedOn": "2021-07-26T02:01:00.059Z",
          "wordCount": 627,
          "title": "Structured Convolutional Kernel Networks for Airline Crew Scheduling. (arXiv:2105.11646v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schafer_L/0/1/0/all/0/1\">Lukas Sch&#xe4;fer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christianos_F/0/1/0/all/0/1\">Filippos Christianos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanna_J/0/1/0/all/0/1\">Josiah Hanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1\">Stefano V. Albrecht</a>",
          "description": "Intrinsic rewards are commonly applied to improve exploration in\nreinforcement learning. However, these approaches suffer from instability\ncaused by non-stationary reward shaping and strong dependency on\nhyperparameters. In this work, we propose Decoupled RL (DeRL) which trains\nseparate policies for exploration and exploitation. DeRL can be applied with\non-policy and off-policy RL algorithms. We evaluate DeRL algorithms in two\nsparse-reward environments with multiple types of intrinsic rewards. We show\nthat DeRL is more robust to scaling and speed of decay of intrinsic rewards and\nconverges to the same evaluation returns than intrinsically motivated baselines\nin fewer interactions.",
          "link": "http://arxiv.org/abs/2107.08966",
          "publishedOn": "2021-07-26T02:01:00.051Z",
          "wordCount": 563,
          "title": "Decoupling Exploration and Exploitation in Reinforcement Learning. (arXiv:2107.08966v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Han Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Li Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Federated learning has emerged as an important paradigm for training machine\nlearning models in different domains. For graph-level tasks such as graph\nclassification, graphs can also be regarded as a special type of data samples,\nwhich can be collected and stored in separate local systems. Similar to other\ndomains, multiple local systems, each holding a small set of graphs, may\nbenefit from collaboratively training a powerful graph mining model, such as\nthe popular graph neural networks (GNNs). To provide more motivation towards\nsuch endeavors, we analyze real-world graphs from different domains to confirm\nthat they indeed share certain graph properties that are statistically\nsignificant compared with random graphs. However, we also find that different\nsets of graphs, even from the same domain or same dataset, are non-IID\nregarding both graph structures and node features. To handle this, we propose a\ngraph clustered federated learning (GCFL) framework that dynamically finds\nclusters of local systems based on the gradients of GNNs, and theoretically\njustify that such clusters can reduce the structure and feature heterogeneity\namong graphs owned by the local systems. Moreover, we observe the gradients of\nGNNs to be rather fluctuating in GCFL which impedes high-quality clustering,\nand design a gradient sequence-based clustering mechanism based on dynamic time\nwarping (GCFL+). Extensive experimental results and in-depth analysis\ndemonstrate the effectiveness of our proposed frameworks.",
          "link": "http://arxiv.org/abs/2106.13423",
          "publishedOn": "2021-07-26T02:01:00.036Z",
          "wordCount": 692,
          "title": "Federated Graph Classification over Non-IID Graphs. (arXiv:2106.13423v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.10673",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Ding_L/0/1/0/all/0/1\">Lijun Ding</a>, <a href=\"http://arxiv.org/find/math/1/au:+Udell_M/0/1/0/all/0/1\">Madeleine Udell</a>",
          "description": "Low rank matrix recovery problems appear widely in statistics, combinatorics,\nand imaging. One celebrated method for solving these problems is to formulate\nand solve a semidefinite program (SDP). It is often known that the exact\nsolution to the SDP with perfect data recovers the solution to the original low\nrank matrix recovery problem. It is more challenging to show that an\napproximate solution to the SDP formulated with noisy problem data acceptably\nsolves the original problem; arguments are usually ad hoc for each problem\nsetting, and can be complex.\n\nIn this note, we identify a set of conditions that we call simplicity that\nlimit the error due to noisy problem data or incomplete convergence. In this\nsense, simple SDPs are robust: simple SDPs can be (approximately) solved\nefficiently at scale; and the resulting approximate solutions, even with noisy\ndata, can be trusted. Moreover, we show that simplicity holds generically, and\nalso for many structured low rank matrix recovery problems, including the\nstochastic block model, $\\mathbb{Z}_2$ synchronization, and matrix completion.\nFormally, we call an SDP simple if it has a surjective constraint map, admits a\nunique primal and dual solution pair, and satisfies strong duality and strict\ncomplementarity.\n\nHowever, simplicity is not a panacea: we show the Burer-Monteiro formulation\nof the SDP may have spurious second-order critical points, even for a simple\nSDP with a rank 1 solution.",
          "link": "http://arxiv.org/abs/2002.10673",
          "publishedOn": "2021-07-26T02:01:00.018Z",
          "wordCount": 693,
          "title": "On the simplicity and conditioning of low rank semidefinite programs. (arXiv:2002.10673v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.13001",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cai_H/0/1/0/all/0/1\">HanQin Cai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mckenzie_D/0/1/0/all/0/1\">Daniel Mckenzie</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yin_W/0/1/0/all/0/1\">Wotao Yin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenliang Zhang</a>",
          "description": "We consider the problem of minimizing a high-dimensional objective function,\nwhich may include a regularization term, using (possibly noisy) evaluations of\nthe function. Such optimization is also called derivative-free, zeroth-order,\nor black-box optimization. We propose a new $\\textbf{Z}$eroth-$\\textbf{O}$rder\n$\\textbf{R}$egularized $\\textbf{O}$ptimization method, dubbed ZORO. When the\nunderlying gradient is approximately sparse at an iterate, ZORO needs very few\nobjective function evaluations to obtain a new iterate that decreases the\nobjective function. We achieve this with an adaptive, randomized gradient\nestimator, followed by an inexact proximal-gradient scheme. Under a novel\napproximately sparse gradient assumption and various different convex settings,\nwe show the (theoretical and empirical) convergence rate of ZORO is only\nlogarithmically dependent on the problem dimension. Numerical experiments show\nthat ZORO outperforms the existing methods with similar assumptions, on both\nsynthetic and real datasets.",
          "link": "http://arxiv.org/abs/2003.13001",
          "publishedOn": "2021-07-26T02:01:00.010Z",
          "wordCount": 617,
          "title": "Zeroth-Order Regularized Optimization (ZORO): Approximately Sparse Gradients and Adaptive Sampling. (arXiv:2003.13001v5 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02965",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lacroce_C/0/1/0/all/0/1\">Clara Lacroce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panangaden_P/0/1/0/all/0/1\">Prakash Panangaden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1\">Guillaume Rabusseau</a>",
          "description": "In this paper we study the approximate minimization problem for language\nmodelling. We assume we are given some language model as a black box. The\nobjective is to obtain a weighted finite automaton (WFA) that fits within a\ngiven size constraint and which mimics the behaviour of the original model\nwhile minimizing some notion of distance between the black box and the\nextracted WFA. We provide an algorithm for the approximate minimization of\nblack boxes trained for language modelling of sequential data over a one-letter\nalphabet. By reformulating the problem in terms of Hankel matrices, we leverage\nclassical results on the approximation of Hankel operators, namely the\ncelebrated Adamyan-Arov-Krein (AAK) theory. This allows us to use the spectral\nnorm to measure the distance between the black box and the WFA. We provide\ntheoretical guarantees to study the potentially infinite-rank Hankel matrix of\nthe black box, without accessing the training data, and we prove that our\nmethod returns an asymptotically-optimal approximation.",
          "link": "http://arxiv.org/abs/2106.02965",
          "publishedOn": "2021-07-26T02:01:00.001Z",
          "wordCount": 638,
          "title": "Extracting Weighted Automata for Approximate Minimization in Language Modelling. (arXiv:2106.02965v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scrugli_M/0/1/0/all/0/1\">Matteo Antonio Scrugli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loi_D/0/1/0/all/0/1\">Daniela Loi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffo_L/0/1/0/all/0/1\">Luigi Raffo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meloni_P/0/1/0/all/0/1\">Paolo Meloni</a>",
          "description": "The Internet of Medical Things (IoMT) paradigm is becoming mainstream in\nmultiple clinical trials and healthcare procedures. Cardiovascular diseases\nmonitoring, usually involving electrocardiogram (ECG) traces analysis, is one\nof the most promising and high-impact applications. Nevertheless, to fully\nexploit the potential of IoMT in this domain, some steps forward are needed.\nFirst, the edge-computing paradigm must be added to the picture. A certain\nlevel of near-sensor processing has to be enabled, to improve the scalability,\nportability, reliability, responsiveness of the IoMT nodes. Second, novel,\nincreasingly accurate, data analysis algorithms, such as those based on\nartificial intelligence and Deep Learning, must be exploited. To reach these\nobjectives, designers and programmers of IoMT nodes, have to face challenging\noptimization tasks, in order to execute fairly complex computing tasks on\nlow-power wearable and portable processing systems, with tight power and\nbattery lifetime budgets. In this work, we explore the implementation of a\ncognitive data analysis algorithm, based on a convolutional neural network\ntrained to classify ECG waveforms, on a resource-constrained\nmicrocontroller-based computing platform. To minimize power consumption, we add\nan adaptivity layer that dynamically manages the hardware and software\nconfiguration of the device to adapt it at runtime to the required operating\nmode. Our experimental results show that adapting the node setup to the\nworkload at runtime can save up to 50% power consumption. Our optimized and\nquantized neural network reaches an accuracy value higher than 97% for\narrhythmia disorders detection on MIT-BIH Arrhythmia dataset.",
          "link": "http://arxiv.org/abs/2106.06498",
          "publishedOn": "2021-07-26T02:00:59.969Z",
          "wordCount": 722,
          "title": "An adaptive cognitive sensor node for ECG monitoring in the Internet of Medical Things. (arXiv:2106.06498v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.01586",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Liang_T/0/1/0/all/0/1\">Tengyuan Liang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sur_P/0/1/0/all/0/1\">Pragya Sur</a>",
          "description": "This paper establishes a precise high-dimensional asymptotic theory for\nboosting on separable data, taking statistical and computational perspectives.\nWe consider a high-dimensional setting where the number of features (weak\nlearners) $p$ scales with the sample size $n$, in an overparametrized regime.\nUnder a class of statistical models, we provide an exact analysis of the\ngeneralization error of boosting when the algorithm interpolates the training\ndata and maximizes the empirical $\\ell_1$-margin. Further, we explicitly pin\ndown the relation between the boosting test error and the optimal Bayes error,\nas well as the proportion of active features at interpolation (with zero\ninitialization). In turn, these precise characterizations answer certain\nquestions raised in \\cite{breiman1999prediction, schapire1998boosting}\nsurrounding boosting, under assumed data generating processes. At the heart of\nour theory lies an in-depth study of the maximum-$\\ell_1$-margin, which can be\naccurately described by a new system of non-linear equations; to analyze this\nmargin, we rely on Gaussian comparison techniques and develop a novel uniform\ndeviation argument. Our statistical and computational arguments can handle (1)\nany finite-rank spiked covariance model for the feature distribution and (2)\nvariants of boosting corresponding to general $\\ell_q$-geometry, $q \\in [1,\n2]$. As a final component, via the Lindeberg principle, we establish a\nuniversality result showcasing that the scaled $\\ell_1$-margin (asymptotically)\nremains the same, whether the covariates used for boosting arise from a\nnon-linear random feature model or an appropriately linearized model with\nmatching moments.",
          "link": "http://arxiv.org/abs/2002.01586",
          "publishedOn": "2021-07-26T02:00:59.926Z",
          "wordCount": 712,
          "title": "A Precise High-Dimensional Asymptotic Theory for Boosting and Minimum-$\\ell_1$-Norm Interpolated Classifiers. (arXiv:2002.01586v3 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.00781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hernest_D/0/1/0/all/0/1\">Dan Hernest</a>",
          "description": "Recent research in synthesis of programs written in some Domain Specific\nLanguage (DSL) by means of neural networks from a limited set of inputs-output\ncorrespondences such as DeepCoder and its PCCoder reimplementation/optimization\nproved the efficiency of this kind of approach to automatic program generation\nin a DSL language that although limited in scope is universal in the sense that\nprograms can be translated to basically any programming language. We experiment\nwith the extension of the DSL of DeepCoder/PCCoder with symbols IFI and IFL\nwhich denote functional expressions of the If ramification (test) instruction\nfor types Int and List. We notice an increase (doubling) of the size of the\ntraining set, the number of parameters of the trained neural network and of the\ntime spent looking for the program synthesized from limited sets of\ninputs-output correspondences. The result is positive in the sense of\npreserving the accuracy of applying synthesis on randomly generated test sets.",
          "link": "http://arxiv.org/abs/1912.00781",
          "publishedOn": "2021-07-26T02:00:59.908Z",
          "wordCount": 610,
          "title": "Experiments with a PCCoder extension. (arXiv:1912.00781v2 [cs.PL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11253",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Malartic_Q/0/1/0/all/0/1\">Quentin Malartic</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Farchi_A/0/1/0/all/0/1\">Alban Farchi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bocquet_M/0/1/0/all/0/1\">Marc Bocquet</a>",
          "description": "Recent studies have shown that it is possible to combine machine learning\nmethods with data assimilation to reconstruct a dynamical system using only\nsparse and noisy observations of that system. The same approach can be used to\ncorrect the error of a knowledge-based model. The resulting surrogate model is\nhybrid, with a statistical part supplementing a physical part. In practice, the\ncorrection can be added as an integrated term (\\textit{i.e.} in the model\nresolvent) or directly inside the tendencies of the physical model. The\nresolvent correction is easy to implement. The tendency correction is more\ntechnical, in particular it requires the adjoint of the physical model, but\nalso more flexible. We use the two-scale Lorenz model to compare the two\nmethods. The accuracy in long-range forecast experiments is somewhat similar\nbetween the surrogate models using the resolvent correction and the tendency\ncorrection. By contrast, the surrogate models using the tendency correction\nsignificantly outperform the surrogate models using the resolvent correction in\ndata assimilation experiments. Finally, we show that the tendency correction\nopens the possibility to make online model error correction, \\textit{i.e.}\nimproving the model progressively as new observations become available. The\nresulting algorithm can be seen as a new formulation of weak-constraint 4D-Var.\nWe compare online and offline learning using the same framework with the\ntwo-scale Lorenz system, and show that with online learning, it is possible to\nextract all the information from sparse and noisy observations.",
          "link": "http://arxiv.org/abs/2107.11253",
          "publishedOn": "2021-07-26T02:00:59.900Z",
          "wordCount": 712,
          "title": "State, global and local parameter estimation using local ensemble Kalman filters: applications to online machine learning of chaotic dynamics. (arXiv:2107.11253v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reiersen_G/0/1/0/all/0/1\">Gyri Reiersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_D/0/1/0/all/0/1\">David Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1\">Bj&#xf6;rn L&#xfc;tjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1\">Konstantin Klemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoxiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>",
          "description": "Forest carbon offsets are increasingly popular and can play a significant\nrole in financing climate mitigation, forest conservation, and reforestation.\nMeasuring how much carbon is stored in forests is, however, still largely done\nvia expensive, time-consuming, and sometimes unaccountable field measurements.\nTo overcome these limitations, many verification bodies are leveraging machine\nlearning (ML) algorithms to estimate forest carbon from satellite or aerial\nimagery. Aerial imagery allows for tree species or family classification, which\nimproves the satellite imagery-based forest type classification. However,\naerial imagery is significantly more expensive to collect and it is unclear by\nhow much the higher resolution improves the forest carbon estimation. This\nproposal paper describes the first systematic comparison of forest carbon\nestimation from aerial imagery, satellite imagery, and ground-truth field\nmeasurements via deep learning-based algorithms for a tropical reforestation\nproject. Our initial results show that forest carbon estimates from satellite\nimagery can overestimate above-ground biomass by more than 10-times for\ntropical reforestation projects. The significant difference between aerial and\nsatellite-derived forest carbon measurements shows the potential for aerial\nimagery-based ML algorithms and raises the importance to extend this study to a\nglobal benchmark between options for carbon measurements.",
          "link": "http://arxiv.org/abs/2107.11320",
          "publishedOn": "2021-07-26T02:00:59.887Z",
          "wordCount": 663,
          "title": "Tackling the Overestimation of Forest Carbon with Deep Learning and Aerial Imagery. (arXiv:2107.11320v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2004.11184",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Drgona_J/0/1/0/all/0/1\">Jan Drgona</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tuor_A/0/1/0/all/0/1\">Aaron Tuor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vrabie_D/0/1/0/all/0/1\">Draguna Vrabie</a>",
          "description": "We present differentiable predictive control (DPC), a method for learning\nconstrained adaptive neural control policies and dynamical models of unknown\nlinear systems. DPC presents an approximate data-driven solution approach to\nthe explicit Model Predictive Control (MPC) problem as a scalable alternative\nto computationally expensive multiparametric programming solvers. DPC is\nformulated as a constrained deep learning problem whose architecture is\ninspired by the structure of classical MPC. The optimization of the neural\ncontrol policy is based on automatic differentiation of the MPC-inspired loss\nfunction through a differentiable closed-loop system model. This novel solution\napproach can optimize adaptive neural control policies for time-varying\nreferences while obeying state and input constraints without the prior need of\nan MPC controller. We show that DPC can learn to stabilize constrained neural\ncontrol policies for systems with unstable dynamics. Moreover, we provide\nsufficient conditions for asymptotic stability of generic closed-loop system\ndynamics with neural feedback policies. In simulation case studies, we assess\nthe performance of the proposed DPC method in terms of reference tracking,\nrobustness, and computational and memory footprints compared against classical\nmodel-based and data-driven control approaches. We demonstrate that DPC scales\nlinearly with problem size, compared to exponential scalability of classical\nexplicit MPC based on multiparametric programming.",
          "link": "http://arxiv.org/abs/2004.11184",
          "publishedOn": "2021-07-26T02:00:59.880Z",
          "wordCount": 712,
          "title": "Learning Stable Adaptive Explicit Differentiable Predictive Control for Unknown Linear Systems. (arXiv:2004.11184v5 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11371",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Sen_J/0/1/0/all/0/1\">Jaydip Sen</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Mehtab_S/0/1/0/all/0/1\">Sidra Mehtab</a>",
          "description": "Designing an optimum portfolio that allocates weights to its constituent\nstocks in a way that achieves the best trade-off between the return and the\nrisk is a challenging research problem. The classical mean-variance theory of\nportfolio proposed by Markowitz is found to perform sub-optimally on the\nreal-world stock market data since the error in estimation for the expected\nreturns adversely affects the performance of the portfolio. This paper presents\nthree approaches to portfolio design, viz, the minimum risk portfolio, the\noptimum risk portfolio, and the Eigen portfolio, for seven important sectors of\nthe Indian stock market. The daily historical prices of the stocks are scraped\nfrom Yahoo Finance website from January 1, 2016, to December 31, 2020. Three\nportfolios are built for each of the seven sectors chosen for this study, and\nthe portfolios are analyzed on the training data based on several metrics such\nas annualized return and risk, weights assigned to the constituent stocks, the\ncorrelation heatmaps, and the principal components of the Eigen portfolios.\nFinally, the optimum risk portfolios and the Eigen portfolios for all sectors\nare tested on their return over a period of a six-month period. The\nperformances of the portfolios are compared and the portfolio yielding the\nhigher return for each sector is identified.",
          "link": "http://arxiv.org/abs/2107.11371",
          "publishedOn": "2021-07-26T02:00:59.856Z",
          "wordCount": 701,
          "title": "Optimum Risk Portfolio and Eigen Portfolio: A Comparative Analysis Using Selected Stocks from the Indian Stock Market. (arXiv:2107.11371v1 [q-fin.PM])"
        },
        {
          "id": "http://arxiv.org/abs/2009.07101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fujita_K/0/1/0/all/0/1\">Kazuhisa Fujita</a>",
          "description": "Spectral clustering (SC) is one of the most popular clustering methods and\noften outperforms traditional clustering methods. SC uses the eigenvectors of a\nLaplacian matrix calculated from a similarity matrix of a dataset. SC has\nserious drawbacks: the significant increases in the time complexity derived\nfrom the computation of eigenvectors and the memory space complexity to store\nthe similarity matrix. To address the issues, I develop a new approximate\nspectral clustering using the network generated by growing neural gas (GNG),\ncalled ASC with GNG in this study. ASC with GNG uses not only reference vectors\nfor vector quantization but also the topology of the network for extraction of\nthe topological relationship between data points in a dataset. ASC with GNG\ncalculates the similarity matrix from both the reference vectors and the\ntopology of the network generated by GNG. Using the network generated from a\ndataset by GNG, ASC with GNG achieves to reduce the computational and space\ncomplexities and improve clustering quality. In this study, I demonstrate that\nASC with GNG effectively reduces the computational time. Moreover, this study\nshows that ASC with GNG provides equal to or better clustering performance than\nSC.",
          "link": "http://arxiv.org/abs/2009.07101",
          "publishedOn": "2021-07-26T02:00:59.845Z",
          "wordCount": 683,
          "title": "Approximate spectral clustering using both reference vectors and topology of the network generated by growing neural gas. (arXiv:2009.07101v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13349",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Nair_V/0/1/0/all/0/1\">Vinod Nair</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bartunov_S/0/1/0/all/0/1\">Sergey Bartunov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gimeno_F/0/1/0/all/0/1\">Felix Gimeno</a>, <a href=\"http://arxiv.org/find/math/1/au:+Glehn_I/0/1/0/all/0/1\">Ingrid von Glehn</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lichocki_P/0/1/0/all/0/1\">Pawel Lichocki</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lobov_I/0/1/0/all/0/1\">Ivan Lobov</a>, <a href=\"http://arxiv.org/find/math/1/au:+ODonoghue_B/0/1/0/all/0/1\">Brendan O&#x27;Donoghue</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sonnerat_N/0/1/0/all/0/1\">Nicolas Sonnerat</a>, <a href=\"http://arxiv.org/find/math/1/au:+Tjandraatmadja_C/0/1/0/all/0/1\">Christian Tjandraatmadja</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_P/0/1/0/all/0/1\">Pengming Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Addanki_R/0/1/0/all/0/1\">Ravichandra Addanki</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hapuarachchi_T/0/1/0/all/0/1\">Tharindi Hapuarachchi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Keck_T/0/1/0/all/0/1\">Thomas Keck</a>, <a href=\"http://arxiv.org/find/math/1/au:+Keeling_J/0/1/0/all/0/1\">James Keeling</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kohli_P/0/1/0/all/0/1\">Pushmeet Kohli</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ktena_I/0/1/0/all/0/1\">Ira Ktena</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1\">Yujia Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zwols_Y/0/1/0/all/0/1\">Yori Zwols</a>",
          "description": "Mixed Integer Programming (MIP) solvers rely on an array of sophisticated\nheuristics developed with decades of research to solve large-scale MIP\ninstances encountered in practice. Machine learning offers to automatically\nconstruct better heuristics from data by exploiting shared structure among\ninstances in the data. This paper applies learning to the two key sub-tasks of\na MIP solver, generating a high-quality joint variable assignment, and bounding\nthe gap in objective value between that assignment and an optimal one. Our\napproach constructs two corresponding neural network-based components, Neural\nDiving and Neural Branching, to use in a base MIP solver such as SCIP. Neural\nDiving learns a deep neural network to generate multiple partial assignments\nfor its integer variables, and the resulting smaller MIPs for un-assigned\nvariables are solved with SCIP to construct high quality joint assignments.\nNeural Branching learns a deep neural network to make variable selection\ndecisions in branch-and-bound to bound the objective value gap with a small\ntree. This is done by imitating a new variant of Full Strong Branching we\npropose that scales to large instances using GPUs. We evaluate our approach on\nsix diverse real-world datasets, including two Google production datasets and\nMIPLIB, by training separate neural networks on each. Most instances in all the\ndatasets combined have $10^3-10^6$ variables and constraints after presolve,\nwhich is significantly larger than previous learning approaches. Comparing\nsolvers with respect to primal-dual gap averaged over a held-out set of\ninstances, the learning-augmented SCIP is 2x to 10x better on all datasets\nexcept one on which it is $10^5$x better, at large time limits. To the best of\nour knowledge, ours is the first learning approach to demonstrate such large\nimprovements over SCIP on both large-scale real-world application datasets and\nMIPLIB.",
          "link": "http://arxiv.org/abs/2012.13349",
          "publishedOn": "2021-07-26T02:00:59.837Z",
          "wordCount": 784,
          "title": "Solving Mixed Integer Programs Using Neural Networks. (arXiv:2012.13349v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11520",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takezawa_Y/0/1/0/all/0/1\">Yuki Takezawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_R/0/1/0/all/0/1\">Ryoma Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_M/0/1/0/all/0/1\">Makoto Yamada</a>",
          "description": "To measure the similarity of documents, the Wasserstein distance is a\npowerful tool, but it requires a high computational cost. Recently, for fast\ncomputation of the Wasserstein distance, methods for approximating the\nWasserstein distance using a tree metric have been proposed. These tree-based\nmethods allow fast comparisons of a large number of documents; however, they\nare unsupervised and do not learn task-specific distances. In this work, we\npropose the Supervised Tree-Wasserstein (STW) distance, a fast, supervised\nmetric learning method based on the tree metric. Specifically, we rewrite the\nWasserstein distance on the tree metric by the parent-child relationships of a\ntree and formulate it as a continuous optimization problem using a contrastive\nloss. Experimentally, we show that the STW distance can be computed fast, and\nimproves the accuracy of document classification tasks. Furthermore, the STW\ndistance is formulated by matrix multiplications, runs on a GPU, and is\nsuitable for batch processing. Therefore, we show that the STW distance is\nextremely efficient when comparing a large number of documents.",
          "link": "http://arxiv.org/abs/2101.11520",
          "publishedOn": "2021-07-26T02:00:59.829Z",
          "wordCount": 618,
          "title": "Supervised Tree-Wasserstein Distance. (arXiv:2101.11520v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08074",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yan_C/0/1/0/all/0/1\">Chao Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiang_X/0/1/0/all/0/1\">Xiaojia Xiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lan_Z/0/1/0/all/0/1\">Zhen Lan</a>",
          "description": "Developing the flocking behavior for a dynamic squad of fixed-wing UAVs is\nstill a challenge due to kinematic complexity and environmental uncertainty. In\nthis paper, we deal with the decentralized flocking and collision avoidance\nproblem through deep reinforcement learning (DRL). Specifically, we formulate a\ndecentralized DRL-based decision making framework from the perspective of every\nfollower, where a collision avoidance mechanism is integrated into the flocking\ncontroller. Then, we propose a novel reinforcement learning algorithm PS-CACER\nfor training a shared control policy for all the followers. Besides, we design\na plug-n-play embedding module based on convolutional neural networks and the\nattention mechanism. As a result, the variable-length system state can be\nencoded into a fixed-length embedding vector, which makes the learned DRL\npolicy independent with the number and the order of followers. Finally,\nnumerical simulation results demonstrate the effectiveness of the proposed\nmethod, and the learned policies can be directly transferred to semi-physical\nsimulation without any parameter finetuning.",
          "link": "http://arxiv.org/abs/2101.08074",
          "publishedOn": "2021-07-26T02:00:59.821Z",
          "wordCount": 656,
          "title": "Flocking and Collision Avoidance for a Dynamic Squad of Fixed-Wing UAVs Using Deep Reinforcement Learning. (arXiv:2101.08074v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_H/0/1/0/all/0/1\">Hussain Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duricic_T/0/1/0/all/0/1\">Tomislav Duricic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lex_E/0/1/0/all/0/1\">Elisabeth Lex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helic_D/0/1/0/all/0/1\">Denis Helic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohmaier_M/0/1/0/all/0/1\">Markus Strohmaier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kern_R/0/1/0/all/0/1\">Roman Kern</a>",
          "description": "Recent work has shown that graph neural networks (GNNs) are vulnerable to\nadversarial attacks on graph data. Common attack approaches are typically\ninformed, i.e. they have access to information about node attributes such as\nlabels and feature vectors. In this work, we study adversarial attacks that are\nuninformed, where an attacker only has access to the graph structure, but no\ninformation about node attributes. Here the attacker aims to exploit structural\nknowledge and assumptions, which GNN models make about graph data. In\nparticular, literature has shown that structural node centrality and similarity\nhave a strong influence on learning with GNNs. Therefore, we study the impact\nof centrality and similarity on adversarial attacks on GNNs. We demonstrate\nthat attackers can exploit this information to decrease the performance of GNNs\nby focusing on injecting links between nodes of low similarity and,\nsurprisingly, low centrality. We show that structure-based uninformed attacks\ncan approach the performance of informed attacks, while being computationally\nmore efficient. With our paper, we present a new attack strategy on GNNs that\nwe refer to as Structack. Structack can successfully manipulate the performance\nof GNNs with very limited information while operating under tight computational\nconstraints. Our work contributes towards building more robust machine learning\napproaches on graphs.",
          "link": "http://arxiv.org/abs/2107.11327",
          "publishedOn": "2021-07-26T02:00:59.795Z",
          "wordCount": 662,
          "title": "Structack: Structure-based Adversarial Attacks on Graph Neural Networks. (arXiv:2107.11327v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.01512",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Truong_T/0/1/0/all/0/1\">Tuyen Trung Truong</a>, <a href=\"http://arxiv.org/find/math/1/au:+To_T/0/1/0/all/0/1\">Tat Dat To</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan Hang Nguyen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nguyen_T/0/1/0/all/0/1\">Thu Hang Nguyen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nguyen_H/0/1/0/all/0/1\">Hoang Phuong Nguyen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Helmy_M/0/1/0/all/0/1\">Maged Helmy</a>",
          "description": "We propose in this paper New Q-Newton's method. The update rule for the\nsimplest version is $x_{n+1}=x_n-w_n$ where\n$w_n=pr_{A_n,+}(v_n)-pr_{A_n,-}(v_n)$, with $A_n=\\nabla\n^2f(x_n)+\\delta_n||\\nabla f(x_n)||^2.Id$ and $v_n=A_n^{-1}.\\nabla f(x_n)$. Here\n$\\delta_n$ is an appropriate real number so that $A_n$ is invertible, and\n$pr_{A_n,\\pm}$ are projections to the vector subspaces generated by\neigenvectors of positive (correspondingly negative) eigenvalues of $A_n$.\n\nThe main result of this paper roughly says that if $f$ is $C^3$ and a\nsequence $\\{x_n\\}$, constructed by the New Q-Newton's method from a random\ninitial point $x_0$, {\\bf converges}, then the limit point is a critical point\nand is not a saddle point, and the convergence rate is the same as that of\nNewton's method. At the end of the paper, we present some issues (saddle points\nand convergence) one faces when implementing Newton's method and modifications\ninto Deep Neural Networks. We test the good performance of New Q-Newton's\nmethod against algorithms such as Newton's method, BFGS, Adaptive Cubic\nRegularization, Random damping Newton's method and Inertial Newton's method, as\nwell as Unbounded Two-way Backtracking Gradient Descent. The experiments cover\nboth realistic settings (such as a toy model of protein folding and stochastic\noptimization) as well as various benchmark test functions.",
          "link": "http://arxiv.org/abs/2006.01512",
          "publishedOn": "2021-07-26T02:00:59.788Z",
          "wordCount": 730,
          "title": "A fast and simple modification of Newton's method helping to avoid saddle points. (arXiv:2006.01512v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.04919",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Willard_J/0/1/0/all/0/1\">Jared Willard</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jia_X/0/1/0/all/0/1\">Xiaowei Jia</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Xu_S/0/1/0/all/0/1\">Shaoming Xu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Steinbach_M/0/1/0/all/0/1\">Michael Steinbach</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kumar_V/0/1/0/all/0/1\">Vipin Kumar</a>",
          "description": "There is a growing consensus that solutions to complex science and\nengineering problems require novel methodologies that are able to integrate\ntraditional physics-based modeling approaches with state-of-the-art machine\nlearning (ML) techniques. This paper provides a structured overview of such\ntechniques. Application-centric objective areas for which these approaches have\nbeen applied are summarized, and then classes of methodologies used to\nconstruct physics-guided ML models and hybrid physics-ML frameworks are\ndescribed. We then provide a taxonomy of these existing techniques, which\nuncovers knowledge gaps and potential crossovers of methods between disciplines\nthat can serve as ideas for future research.",
          "link": "http://arxiv.org/abs/2003.04919",
          "publishedOn": "2021-07-26T02:00:59.781Z",
          "wordCount": 601,
          "title": "Integrating Scientific Knowledge with Machine Learning for Engineering and Environmental Systems. (arXiv:2003.04919v5 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Babadi_A/0/1/0/all/0/1\">Amin Babadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panne_M/0/1/0/all/0/1\">Michiel van de Panne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">C. Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_P/0/1/0/all/0/1\">Perttu H&#xe4;m&#xe4;l&#xe4;inen</a>",
          "description": "We propose a novel method for exploring the dynamics of physically based\nanimated characters, and learning a task-agnostic action space that makes\nmovement optimization easier. Like several previous papers, we parameterize\nactions as target states, and learn a short-horizon goal-conditioned low-level\ncontrol policy that drives the agent's state towards the targets. Our novel\ncontribution is that with our exploration data, we are able to learn the\nlow-level policy in a generic manner and without any reference movement data.\nTrained once for each agent or simulation environment, the policy improves the\nefficiency of optimizing both trajectories and high-level policies across\nmultiple tasks and optimization algorithms. We also contribute novel\nvisualizations that show how using target states as actions makes optimized\ntrajectories more robust to disturbances; this manifests as wider optima that\nare easy to find. Due to its simplicity and generality, our proposed approach\nshould provide a building block that can improve a large variety of movement\noptimization methods and applications.",
          "link": "http://arxiv.org/abs/2009.10337",
          "publishedOn": "2021-07-26T02:00:59.773Z",
          "wordCount": 656,
          "title": "Learning Task-Agnostic Action Spaces for Movement Optimization. (arXiv:2009.10337v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.10686",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mucke_N/0/1/0/all/0/1\">Nicole M&#xfc;cke</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Steinwart_I/0/1/0/all/0/1\">Ingo Steinwart</a>",
          "description": "A common strategy to train deep neural networks (DNNs) is to use very large\narchitectures and to train them until they (almost) achieve zero training\nerror. Empirically observed good generalization performance on test data, even\nin the presence of lots of label noise, corroborate such a procedure. On the\nother hand, in statistical learning theory it is known that over-fitting models\nmay lead to poor generalization properties, occurring in e.g. empirical risk\nminimization (ERM) over too large hypotheses classes. Inspired by this\ncontradictory behavior, so-called interpolation methods have recently received\nmuch attention, leading to consistent and optimally learning methods for some\nlocal averaging schemes with zero training error. However, there is no\ntheoretical analysis of interpolating ERM-like methods so far. We take a step\nin this direction by showing that for certain, large hypotheses classes, some\ninterpolating ERMs enjoy very good statistical guarantees while others fail in\nthe worst sense. Moreover, we show that the same phenomenon occurs for DNNs\nwith zero training error and sufficiently large architectures.",
          "link": "http://arxiv.org/abs/1905.10686",
          "publishedOn": "2021-07-26T02:00:59.766Z",
          "wordCount": 625,
          "title": "Empirical Risk Minimization in the Interpolating Regime with Application to Neural Network Learning. (arXiv:1905.10686v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Costa_D/0/1/0/all/0/1\">Diego Elias Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mujahid_S/0/1/0/all/0/1\">Suhaib Mujahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdalkareem_R/0/1/0/all/0/1\">Rabe Abdalkareem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shihab_E/0/1/0/all/0/1\">Emad Shihab</a>",
          "description": "A decade after its first release, the Go programming language has become a\nmajor programming language in the development landscape. While praised for its\nclean syntax and C-like performance, Go also contains a strong static\ntype-system that prevents arbitrary type casting and arbitrary memory access,\nmaking the language type-safe by design. However, to give developers the\npossibility of implementing low-level code, Go ships with a special package\ncalled unsafe that offers developers a way around the type-safety of Go\nprograms. The package gives greater flexibility to developers but comes at a\nhigher risk of runtime errors, chances of non-portability, and the loss of\ncompatibility guarantees for future versions of Go.\n\nIn this paper, we present the first large-scale study on the usage of the\nunsafe package in 2,438 popular Go projects. Our investigation shows that\nunsafe is used in 24% of Go projects, motivated primarily by communicating with\noperating systems and C code, but is also commonly used as a source of\nperformance optimization. Developers are willing to use unsafe to break\nlanguage specifications (e.g., string immutability) for better performance and\n6% of analyzed projects that use unsafe perform risky pointer conversions that\ncan lead to program crashes and unexpected behavior. Furthermore, we report a\nseries of real issues faced by projects that use unsafe, from crashing errors\nand non-deterministic behavior to having their deployment restricted from\ncertain popular environments. Our findings can be used to understand how and\nwhy developers break type-safety in Go, and help motivate further tools and\nlanguage development that could make the usage of unsafe in Go even safer.",
          "link": "http://arxiv.org/abs/2006.09973",
          "publishedOn": "2021-07-26T02:00:59.740Z",
          "wordCount": 763,
          "title": "Breaking Type Safety in Go: An Empirical Study on the Usage of the unsafe Package. (arXiv:2006.09973v4 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qizheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1\">Hui Guan</a>",
          "description": "Hard parameter sharing in multi-task learning (MTL) allows tasks to share\nsome of model parameters, reducing storage cost and improving prediction\naccuracy. The common sharing practice is to share bottom layers of a deep\nneural network among tasks while using separate top layers for each task. In\nthis work, we revisit this common practice via an empirical study on\nfine-grained image classification tasks and make two surprising observations.\n(1) Using separate bottom-layer parameters could achieve significantly better\nperformance than the common practice and this phenomenon holds for different\nnumber of tasks jointly trained on different backbone architectures with\ndifferent quantity of task-specific parameters. (2) A multi-task model with a\nsmall proportion of task-specific parameters from bottom layers can achieve\ncompetitive performance with independent models trained on each task separately\nand outperform a state-of-the-art MTL framework. Our observations suggest that\npeople rethink the current sharing paradigm and adopt the new strategy of using\nseparate bottom-layer parameters as a stronger baseline for model design in\nMTL.",
          "link": "http://arxiv.org/abs/2107.11359",
          "publishedOn": "2021-07-26T02:00:59.731Z",
          "wordCount": 595,
          "title": "Rethinking Hard-Parameter Sharing in Multi-Task Learning. (arXiv:2107.11359v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1\">Gaurav Kumar Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1\">Konda Reddy Mopuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saksham Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Anirban Chakraborty</a>",
          "description": "Pretrained deep models hold their learnt knowledge in the form of model\nparameters. These parameters act as \"memory\" for the trained models and help\nthem generalize well on unseen data. However, in absence of training data, the\nutility of a trained model is merely limited to either inference or better\ninitialization towards a target task. In this paper, we go further and extract\nsynthetic data by leveraging the learnt model parameters. We dub them \"Data\nImpressions\", which act as proxy to the training data and can be used to\nrealize a variety of tasks. These are useful in scenarios where only the\npretrained models are available and the training data is not shared (e.g., due\nto privacy or sensitivity concerns). We show the applicability of data\nimpressions in solving several computer vision tasks such as unsupervised\ndomain adaptation, continual learning as well as knowledge distillation. We\nalso study the adversarial robustness of lightweight models trained via\nknowledge distillation using these data impressions. Further, we demonstrate\nthe efficacy of data impressions in generating data-free Universal Adversarial\nPerturbations (UAPs) with better fooling rates. Extensive experiments performed\non benchmark datasets demonstrate competitive performance achieved using data\nimpressions in absence of original training data.",
          "link": "http://arxiv.org/abs/2101.06069",
          "publishedOn": "2021-07-26T02:00:59.723Z",
          "wordCount": 700,
          "title": "Mining Data Impressions from Deep Models as Substitute for the Unavailable Training Data. (arXiv:2101.06069v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02559",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Akshay Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Alnur Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_S/0/1/0/all/0/1\">Stephen Boyd</a>",
          "description": "We consider the vector embedding problem. We are given a finite set of items,\nwith the goal of assigning a representative vector to each one, possibly under\nsome constraints (such as the collection of vectors being standardized, i.e.,\nhave zero mean and unit covariance). We are given data indicating that some\npairs of items are similar, and optionally, some other pairs are dissimilar.\nFor pairs of similar items, we want the corresponding vectors to be near each\nother, and for dissimilar pairs, we want the corresponding vectors to not be\nnear each other, measured in Euclidean distance. We formalize this by\nintroducing distortion functions, defined for some pairs of the items. Our goal\nis to choose an embedding that minimizes the total distortion, subject to the\nconstraints. We call this the minimum-distortion embedding (MDE) problem.\n\nThe MDE framework is simple but general. It includes a wide variety of\nembedding methods, such as spectral embedding, principal component analysis,\nmultidimensional scaling, dimensionality reduction methods (like Isomap and\nUMAP), force-directed layout, and others. It also includes new embeddings, and\nprovides principled ways of validating historical and new embeddings alike.\n\nWe develop a projected quasi-Newton method that approximately solves MDE\nproblems and scales to large data sets. We implement this method in PyMDE, an\nopen-source Python package. In PyMDE, users can select from a library of\ndistortion functions and constraints or specify custom ones, making it easy to\nrapidly experiment with different embeddings. Our software scales to data sets\nwith millions of items and tens of millions of distortion functions. To\ndemonstrate our method, we compute embeddings for several real-world data sets,\nincluding images, an academic co-author network, US county demographic data,\nand single-cell mRNA transcriptomes.",
          "link": "http://arxiv.org/abs/2103.02559",
          "publishedOn": "2021-07-26T02:00:59.716Z",
          "wordCount": 747,
          "title": "Minimum-Distortion Embedding. (arXiv:2103.02559v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.09525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1\">Anand Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zylich_B/0/1/0/all/0/1\">Brian Zylich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ottmar_E/0/1/0/all/0/1\">Erin Ottmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LoCasale_Crouch_J/0/1/0/all/0/1\">Jennifer LoCasale-Crouch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitehill_J/0/1/0/all/0/1\">Jacob Whitehill</a>",
          "description": "In this work we present a multi-modal machine learning-based system, which we\ncall ACORN, to analyze videos of school classrooms for the Positive Climate\n(PC) and Negative Climate (NC) dimensions of the CLASS observation protocol\nthat is widely used in educational research. ACORN uses convolutional neural\nnetworks to analyze spectral audio features, the faces of teachers and\nstudents, and the pixels of each image frame, and then integrates this\ninformation over time using Temporal Convolutional Networks. The audiovisual\nACORN's PC and NC predictions have Pearson correlations of $0.55$ and $0.63$\nwith ground-truth scores provided by expert CLASS coders on the UVA Toddler\ndataset (cross-validation on $n=300$ 15-min video segments), and a purely\nauditory ACORN predicts PC and NC with correlations of $0.36$ and $0.41$ on the\nMET dataset (test set of $n=2000$ videos segments). These numbers are similar\nto inter-coder reliability of human coders. Finally, using Graph Convolutional\nNetworks we make early strides (AUC=$0.70$) toward predicting the specific\nmoments (45-90sec clips) when the PC is particularly weak/strong. Our findings\ninform the design of automatic classroom observation and also more general\nvideo activity recognition and summary recognition systems.",
          "link": "http://arxiv.org/abs/2005.09525",
          "publishedOn": "2021-07-26T02:00:59.707Z",
          "wordCount": 712,
          "title": "Toward Automated Classroom Observation: Multimodal Machine Learning to Estimate CLASS Positive Climate and Negative Climate. (arXiv:2005.09525v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08494",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yiran Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Schonlieb</a>",
          "description": "Alzheimer's disease (AD) is the most common age-related dementia. It remains\na challenge to identify the individuals at risk of dementia for precise\nmanagement. Brain MRI offers a noninvasive biomarker to detect brain aging.\nPrevious evidence shows that the brain structural change detected by diffusion\nMRI is associated with dementia. Mounting studies has conceptualised the brain\nas a complex network, which has shown the utility of this approach in\ncharacterising various neurological and psychiatric disorders. Therefore, the\nstructural connectivity shows promise in dementia classification. The proposed\nBrainNetGAN is a generative adversarial network variant to augment the brain\nstructural connectivity matrices for binary dementia classification tasks.\nStructural connectivity matrices between separated brain regions are\nconstructed using tractography on diffusion MRI data. The BrainNetGAN model is\ntrained to generate fake brain connectivity matrices, which are expected to\nreflect latent distribution of the real brain network data. Finally, a\nconvolutional neural network classifier is proposed for binary dementia\nclassification. Numerical results show that the binary classification\nperformance in the testing set was improved using the BrainNetGAN augmented\ndataset. The proposed methodology allows quick synthesis of an arbitrary number\nof augmented connectivity matrices and can be easily transferred to similar\nclassification tasks.",
          "link": "http://arxiv.org/abs/2103.08494",
          "publishedOn": "2021-07-26T02:00:59.681Z",
          "wordCount": 690,
          "title": "BrainNetGAN: Data augmentation of brain connectivity using generative adversarial network for dementia classification. (arXiv:2103.08494v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fursov_I/0/1/0/all/0/1\">Ivan Fursov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1\">Alexey Zaytsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnyshev_P/0/1/0/all/0/1\">Pavel Burnyshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dmitrieva_E/0/1/0/all/0/1\">Ekaterina Dmitrieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klyuchnikov_N/0/1/0/all/0/1\">Nikita Klyuchnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kravchenko_A/0/1/0/all/0/1\">Andrey Kravchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>",
          "description": "Robustness of huge Transformer-based models for natural language processing\nis an important issue due to their capabilities and wide adoption. One way to\nunderstand and improve robustness of these models is an exploration of an\nadversarial attack scenario: check if a small perturbation of an input can fool\na model.\n\nDue to the discrete nature of textual data, gradient-based adversarial\nmethods, widely used in computer vision, are not applicable per~se. The\nstandard strategy to overcome this issue is to develop token-level\ntransformations, which do not take the whole sentence into account.\n\nIn this paper, we propose a new black-box sentence-level attack. Our method\nfine-tunes a pre-trained language model to generate adversarial examples. A\nproposed differentiable loss function depends on a substitute classifier score\nand an approximate edit distance computed via a deep learning model.\n\nWe show that the proposed attack outperforms competitors on a diverse set of\nNLP problems for both computed metrics and human evaluation. Moreover, due to\nthe usage of the fine-tuned language model, the generated adversarial examples\nare hard to detect, thus current models are not robust. Hence, it is difficult\nto defend from the proposed attack, which is not the case for other attacks.",
          "link": "http://arxiv.org/abs/2107.11275",
          "publishedOn": "2021-07-26T02:00:59.673Z",
          "wordCount": 652,
          "title": "A Differentiable Language Model Adversarial Attack on Text Classifiers. (arXiv:2107.11275v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1911.11134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Evci_U/0/1/0/all/0/1\">Utku Evci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gale_T/0/1/0/all/0/1\">Trevor Gale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1\">Pablo Samuel Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsen_E/0/1/0/all/0/1\">Erich Elsen</a>",
          "description": "Many applications require sparse neural networks due to space or inference\ntime restrictions. There is a large body of work on training dense networks to\nyield sparse networks for inference, but this limits the size of the largest\ntrainable sparse model to that of the largest trainable dense model. In this\npaper we introduce a method to train sparse neural networks with a fixed\nparameter count and a fixed computational cost throughout training, without\nsacrificing accuracy relative to existing dense-to-sparse training methods. Our\nmethod updates the topology of the sparse network during training by using\nparameter magnitudes and infrequent gradient calculations. We show that this\napproach requires fewer floating-point operations (FLOPs) to achieve a given\nlevel of accuracy compared to prior techniques. We demonstrate state-of-the-art\nsparse training results on a variety of networks and datasets, including\nResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we\nprovide some insights into why allowing the topology to change during the\noptimization can overcome local minima encountered when the topology remains\nstatic. Code used in our work can be found in github.com/google-research/rigl.",
          "link": "http://arxiv.org/abs/1911.11134",
          "publishedOn": "2021-07-26T02:00:59.666Z",
          "wordCount": 696,
          "title": "Rigging the Lottery: Making All Tickets Winners. (arXiv:1911.11134v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11191",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Duff_M/0/1/0/all/0/1\">Margaret Duff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campbell_N/0/1/0/all/0/1\">Neill D. F. Campbell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ehrhardt_M/0/1/0/all/0/1\">Matthias J. Ehrhardt</a>",
          "description": "Deep neural network approaches to inverse imaging problems have produced\nimpressive results in the last few years. In this paper, we consider the use of\ngenerative models in a variational regularisation approach to inverse problems.\nThe considered regularisers penalise images that are far from the range of a\ngenerative model that has learned to produce images similar to a training\ndataset. We name this family \\textit{generative regularisers}. The success of\ngenerative regularisers depends on the quality of the generative model and so\nwe propose a set of desired criteria to assess models and guide future\nresearch. In our numerical experiments, we evaluate three common generative\nmodels, autoencoders, variational autoencoders and generative adversarial\nnetworks, against our desired criteria. We also test three different generative\nregularisers on the inverse problems of deblurring, deconvolution, and\ntomography. We show that the success of solutions restricted to lie exactly in\nthe range of the generator is highly dependent on the ability of the generative\nmodel but that allowing small deviations from the range of the generator\nproduces more consistent results.",
          "link": "http://arxiv.org/abs/2107.11191",
          "publishedOn": "2021-07-26T02:00:59.658Z",
          "wordCount": 631,
          "title": "Regularising Inverse Problems with Generative Machine Learning Models. (arXiv:2107.11191v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shaojie Tang</a>",
          "description": "Most of existing studies on adaptive submodular optimization focus on the\naverage-case, i.e., their objective is to find a policy that maximizes the\nexpected utility over a known distribution of realizations. However, a policy\nthat has a good average-case performance may have very poor performance under\nthe worst-case realization. In this study, we propose to study two variants of\nadaptive submodular optimization problems, namely, worst-case adaptive\nsubmodular maximization and robust submodular maximization. The first problem\naims to find a policy that maximizes the worst-case utility and the latter one\naims to find a policy, if any, that achieves both near optimal average-case\nutility and worst-case utility simultaneously. We introduce a new class of\nstochastic functions, called \\emph{worst-case submodular function}. For the\nworst-case adaptive submodular maximization problem subject to a $p$-system\nconstraint, we develop an adaptive worst-case greedy policy that achieves a\n$\\frac{1}{p+1}$ approximation ratio against the optimal worst-case utility if\nthe utility function is worst-case submodular. For the robust adaptive\nsubmodular maximization problem subject to a cardinality constraint, if the\nutility function is both worst-case submodular and adaptive submodular, we\ndevelop a hybrid adaptive policy that achieves an approximation close to\n$1-e^{-\\frac{1}{2}}$ under both worst case setting and average case setting\nsimultaneously. We also describe several applications of our theoretical\nresults, including pool-base active learning, stochastic submodular set cover\nand adaptive viral marketing.",
          "link": "http://arxiv.org/abs/2107.11333",
          "publishedOn": "2021-07-26T02:00:59.650Z",
          "wordCount": 653,
          "title": "Robust Adaptive Submodular Maximization. (arXiv:2107.11333v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cen_Y/0/1/0/all/0/1\">Yukuo Cen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhenyu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qibin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yizhen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingcheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Aohan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shiguang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1\">Guohao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>",
          "description": "Deep learning on graphs has attracted tremendous attention from the graph\nlearning community in recent years. It has been widely used in several\nreal-world applications such as social network analysis and recommender\nsystems. In this paper, we introduce CogDL, an extensive toolkit for deep\nlearning on graphs that allows researchers and developers to easily conduct\nexperiments and build applications. It provides standard training and\nevaluation for the most important tasks in the graph domain, including node\nclassification, graph classification, etc. For each task, it provides\nimplementations of state-of-the-art models. The models in our toolkit are\ndivided into two major parts, graph embedding methods and graph neural\nnetworks. Most of the graph embedding methods learn node-level or graph-level\nrepresentations in an unsupervised way and preserves the graph properties such\nas structural information, while graph neural networks capture node features\nand work in semi-supervised or self-supervised settings. All models implemented\nin our toolkit can be easily reproducible for leaderboard results. Most models\nin CogDL are developed on top of PyTorch, and users can leverage the advantages\nof PyTorch to implement their own models. Furthermore, we demonstrate the\neffectiveness of CogDL for real-world applications in AMiner, a large academic\nmining system.",
          "link": "http://arxiv.org/abs/2103.00959",
          "publishedOn": "2021-07-26T02:00:59.613Z",
          "wordCount": 692,
          "title": "CogDL: Toolkit for Deep Learning on Graphs. (arXiv:2103.00959v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Estienne_T/0/1/0/all/0/1\">Th&#xe9;o Estienne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christodoulidis_S/0/1/0/all/0/1\">Stergios Christodoulidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battistella_E/0/1/0/all/0/1\">Enzo Battistella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1\">Th&#xe9;ophraste Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerousseau_M/0/1/0/all/0/1\">Marvin Lerousseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leroy_A/0/1/0/all/0/1\">Amaury Leroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chassagnon_G/0/1/0/all/0/1\">Guillaume Chassagnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revel_M/0/1/0/all/0/1\">Marie-Pierre Revel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paragios_N/0/1/0/all/0/1\">Nikos Paragios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_E/0/1/0/all/0/1\">Eric Deutsch</a>",
          "description": "Explainability of deep neural networks is one of the most challenging and\ninteresting problems in the field. In this study, we investigate the topic\nfocusing on the interpretability of deep learning-based registration methods.\nIn particular, with the appropriate model architecture and using a simple\nlinear projection, we decompose the encoding space, generating a new basis, and\nwe empirically show that this basis captures various decomposed anatomically\naware geometrical transformations. We perform experiments using two different\ndatasets focusing on lungs and hippocampus MRI. We show that such an approach\ncan decompose the highly convoluted latent spaces of registration pipelines in\nan orthogonal space with several interesting properties. We hope that this work\ncould shed some light on a better understanding of deep learning-based\nregistration methods.",
          "link": "http://arxiv.org/abs/2107.11238",
          "publishedOn": "2021-07-26T02:00:59.603Z",
          "wordCount": 591,
          "title": "Exploring Deep Registration Latent Spaces. (arXiv:2107.11238v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00083",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kim_T/0/1/0/all/0/1\">Taesup Kim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fakoor_R/0/1/0/all/0/1\">Rasool Fakoor</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mueller_J/0/1/0/all/0/1\">Jonas Mueller</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tibshirani_R/0/1/0/all/0/1\">Ryan J. Tibshirani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>",
          "description": "Conditional quantile estimation is a key statistical learning challenge\nmotivated by the need to quantify uncertainty in predictions or to model a\ndiverse population without being overly reductive. As such, many models have\nbeen developed for this problem. Adopting a meta viewpoint, we propose a\ngeneral framework (inspired by neural network optimization) for aggregating any\nnumber of conditional quantile models in order to boost predictive accuracy. We\nconsider weighted ensembling strategies of increasing flexibility where the\nweights may vary over individual models, quantile levels, and feature values.\nAn appeal of our approach is its portability: we ensure that estimated\nquantiles at adjacent levels do not cross by applying simple transformations\nthrough which gradients can be backpropagated, and this allows us to leverage\nthe modern deep learning toolkit for building quantile ensembles. Our\nexperiments confirm that ensembling can lead to big gains in accuracy, even\nwhen the constituent models are themselves powerful and flexible.",
          "link": "http://arxiv.org/abs/2103.00083",
          "publishedOn": "2021-07-26T02:00:59.593Z",
          "wordCount": 606,
          "title": "Deep Quantile Aggregation. (arXiv:2103.00083v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hendrickx_K/0/1/0/all/0/1\">Kilian Hendrickx</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perini_L/0/1/0/all/0/1\">Lorenzo Perini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plas_D/0/1/0/all/0/1\">Dries Van der Plas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meert_W/0/1/0/all/0/1\">Wannes Meert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">Jesse Davis</a>",
          "description": "Machine learning models always make a prediction, even when it is likely to\nbe inaccurate. This behavior should be avoided in many decision support\napplications, where mistakes can have severe consequences. Albeit already\nstudied in 1970, machine learning with a reject option recently gained\ninterest. This machine learning subfield enables machine learning models to\nabstain from making a prediction when likely to make a mistake.\n\nThis survey aims to provide an overview on machine learning with a reject\noption. We introduce the conditions leading to two types of rejection,\nambiguity and novelty rejection. Moreover, we define the existing architectures\nfor models with a reject option, describe the standard learning strategies to\ntrain such models and relate traditional machine learning techniques to\nrejection. Additionally, we review strategies to evaluate a model's predictive\nand rejective quality. Finally, we provide examples of relevant application\ndomains and show how machine learning with rejection relates to other machine\nlearning research areas.",
          "link": "http://arxiv.org/abs/2107.11277",
          "publishedOn": "2021-07-26T02:00:59.585Z",
          "wordCount": 599,
          "title": "Machine Learning with a Reject Option: A survey. (arXiv:2107.11277v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11304",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lee_C/0/1/0/all/0/1\">Chang-Shen Lee</a>, <a href=\"http://arxiv.org/find/math/1/au:+Michelusi_N/0/1/0/all/0/1\">Nicol&#xf2; Michelusi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Scutari_G/0/1/0/all/0/1\">Gesualdo Scutari</a>",
          "description": "This paper studies distributed algorithms for (strongly convex) composite\noptimization problems over mesh networks, subject to quantized communications.\nInstead of focusing on a specific algorithmic design, we propose a black-box\nmodel casting distributed algorithms in the form of fixed-point iterates,\nconverging at linear rate. The algorithmic model is coupled with a novel\n(random) Biased Compression (BC-)rule on the quantizer design, which preserves\nlinear convergence. A new quantizer coupled with a communication-efficient\nencoding scheme is also proposed, which efficiently implements the BC-rule\nusing a finite number of bits. This contrasts with most of existing\nquantization rules, whose implementation calls for an infinite number of bits.\nA unified communication complexity analysis is developed for the black-box\nmodel, determining the average number of bit required to reach a solution of\nthe optimization problem within the required accuracy. Numerical results\nvalidate our theoretical findings and show that distributed algorithms equipped\nwith the proposed quantizer have more favorable communication complexity than\nalgorithms using existing quantization rules.",
          "link": "http://arxiv.org/abs/2107.11304",
          "publishedOn": "2021-07-26T02:00:59.577Z",
          "wordCount": 608,
          "title": "Finite-Bit Quantization For Distributed Algorithms With Linear Convergence. (arXiv:2107.11304v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jawali_D/0/1/0/all/0/1\">Dhruv Jawali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhishek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seelamantula_C/0/1/0/all/0/1\">Chandra Sekhar Seelamantula</a>",
          "description": "Wavelets have proven to be highly successful in several signal and image\nprocessing applications. Wavelet design has been an active field of research\nfor over two decades, with the problem often being approached from an\nanalytical perspective. In this paper, we introduce a learning based approach\nto wavelet design. We draw a parallel between convolutional autoencoders and\nwavelet multiresolution approximation, and show how the learning angle provides\na coherent computational framework for addressing the design problem. We aim at\ndesigning data-independent wavelets by training filterbank autoencoders, which\nprecludes the need for customized datasets. In fact, we use high-dimensional\nGaussian vectors for training filterbank autoencoders, and show that a\nnear-zero training loss implies that the learnt filters satisfy the perfect\nreconstruction property with very high probability. Properties of a wavelet\nsuch as orthogonality, compact support, smoothness, symmetry, and vanishing\nmoments can be incorporated by designing the autoencoder architecture\nappropriately and with a suitable regularization term added to the mean-squared\nerror cost used in the learning process. Our approach not only recovers the\nwell known Daubechies family of orthogonal wavelets and the\nCohen-Daubechies-Feauveau family of symmetric biorthogonal wavelets, but also\nlearns wavelets outside these families.",
          "link": "http://arxiv.org/abs/2107.11225",
          "publishedOn": "2021-07-26T02:00:59.550Z",
          "wordCount": 636,
          "title": "Wavelet Design in a Learning Framework. (arXiv:2107.11225v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Puchert_P/0/1/0/all/0/1\">Patrik Puchert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1\">Timo Ropinski</a>",
          "description": "We propose the adjacency adaptive graph convolutional long-short term memory\nnetwork (AAGC-LSTM) for human pose estimation from sparse inertial\nmeasurements, obtained from only 6 measurement units. The AAGC-LSTM combines\nboth spatial and temporal dependency in a single network operation. This is\nmade possible by equipping graph convolutions with adjacency adaptivity, which\nalso allows for learning unknown dependencies of the human body joints. To\nfurther boost accuracy, we propose longitudinal loss weighting to consider\nnatural movement patterns, as well as body-aware contralateral data\naugmentation. By combining these contributions, we are able to utilize the\ninherent graph nature of the human body, and can thus outperform the state of\nthe art for human pose estimation from sparse inertial measurements.",
          "link": "http://arxiv.org/abs/2107.11214",
          "publishedOn": "2021-07-26T02:00:59.543Z",
          "wordCount": 558,
          "title": "Human Pose Estimation from Sparse Inertial Measurements through Recurrent Graph Convolution. (arXiv:2107.11214v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.10713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Han Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dan_C/0/1/0/all/0/1\">Chen Dan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aragam_B/0/1/0/all/0/1\">Bryon Aragam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1\">Tommi S. Jaakkola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordon_G/0/1/0/all/0/1\">Geoffrey J. Gordon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1\">Pradeep Ravikumar</a>",
          "description": "Many machine learning applications, e.g., privacy-preserving learning,\nalgorithmic fairness and domain adaptation/generalization, involve learning the\nso-called invariant representations that achieve two competing goals: To\nmaximize information or accuracy with respect to a target while simultaneously\nmaximizing invariance or independence with respect to a set of protected\nfeatures (e.g.\\ for fairness, privacy, etc). Despite its abundant applications\nin the aforementioned domains, theoretical understanding on the limits and\ntradeoffs of invariant representations is still severely lacking. In this\npaper, we provide an information theoretic analysis of this general and\nimportant problem under both classification and regression settings. In both\ncases, we analyze the inherent tradeoffs between accuracy and invariance by\nproviding a geometric characterization of the feasible region in the\ninformation plane, where we connect the geometric properties of this feasible\nregion to the fundamental limitations of the tradeoff problem. In the\nregression setting, we further give a complete and exact characterization of\nthe frontier between accuracy and invariance. Although our contributions are\nmainly theoretical, we also demonstrate the practical applications of our\nresults in certifying the suboptimality of certain representation learning\nalgorithms in both classification and regression tasks. Our results shed new\nlight on this fundamental problem by providing insights on the interplay\nbetween accuracy and invariance. These results deepen our understanding of this\nfundamental problem and may be useful in guiding the design of future\nrepresentation learning algorithms.",
          "link": "http://arxiv.org/abs/2012.10713",
          "publishedOn": "2021-07-26T02:00:59.536Z",
          "wordCount": 721,
          "title": "Fundamental Limits and Tradeoffs in Invariant Representation Learning. (arXiv:2012.10713v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nitzan_Y/0/1/0/all/0/1\">Yotam Nitzan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_R/0/1/0/all/0/1\">Rinon Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brenner_O/0/1/0/all/0/1\">Ofir Brenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>",
          "description": "We propose a novel method for solving regression tasks using few-shot or weak\nsupervision. At the core of our method is the fundamental observation that GANs\nare incredibly successful at encoding semantic information within their latent\nspace, even in a completely unsupervised setting. For modern generative\nframeworks, this semantic encoding manifests as smooth, linear directions which\naffect image attributes in a disentangled manner. These directions have been\nwidely used in GAN-based image editing. We show that such directions are not\nonly linear, but that the magnitude of change induced on the respective\nattribute is approximately linear with respect to the distance traveled along\nthem. By leveraging this observation, our method turns a pre-trained GAN into a\nregression model, using as few as two labeled samples. This enables solving\nregression tasks on datasets and attributes which are difficult to produce\nquality supervision for. Additionally, we show that the same latent-distances\ncan be used to sort collections of images by the strength of given attributes,\neven in the absence of explicit supervision. Extensive experimental evaluations\ndemonstrate that our method can be applied across a wide range of domains,\nleverage multiple latent direction discovery frameworks, and achieve\nstate-of-the-art results in few-shot and low-supervision settings, even when\ncompared to methods designed to tackle a single task.",
          "link": "http://arxiv.org/abs/2107.11186",
          "publishedOn": "2021-07-26T02:00:59.528Z",
          "wordCount": 657,
          "title": "LARGE: Latent-Based Regression through GAN Semantics. (arXiv:2107.11186v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11357",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Harris_C/0/1/0/all/0/1\">Chris Harris</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pymar_R/0/1/0/all/0/1\">Richard Pymar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rowat_C/0/1/0/all/0/1\">Colin Rowat</a>",
          "description": "The Shapley value is one of the most widely used model-agnostic measures of\nfeature importance in explainable AI: it has clear axiomatic foundations, is\nguaranteed to uniquely exist, and has a clear interpretation as a feature's\naverage effect on a model's prediction. We introduce joint Shapley values,\nwhich directly extend the Shapley axioms. This preserves the classic Shapley\nvalue's intuitions: joint Shapley values measure a set of features' average\neffect on a model's prediction. We prove the uniqueness of joint Shapley\nvalues, for any order of explanation. Results for games show that joint Shapley\nvalues present different insights from existing interaction indices, which\nassess the effect of a feature within a set of features. Deriving joint Shapley\nvalues in ML attribution problems thus gives us the first measure of the joint\neffect of sets of features on model predictions. In a dataset with binary\nfeatures, we present a presence-adjusted method for calculating global values\nthat retains the efficiency property.",
          "link": "http://arxiv.org/abs/2107.11357",
          "publishedOn": "2021-07-26T02:00:59.521Z",
          "wordCount": 605,
          "title": "Joint Shapley values: a measure of joint feature importance. (arXiv:2107.11357v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Axel_M/0/1/0/all/0/1\">Marmoret Axel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nancy_B/0/1/0/all/0/1\">Bertin Nancy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeremy_C/0/1/0/all/0/1\">Cohen Jeremy</a>",
          "description": "Music is an art, perceived in unique ways by every listener, coming from\nacoustic signals. In the meantime, standards as musical scores exist to\ndescribe it. Even if humans can make this transcription, it is costly in terms\nof time and efforts, even more with the explosion of information consecutively\nto the rise of the Internet. In that sense, researches are driven in the\ndirection of Automatic Music Transcription. While this task is considered\nsolved in the case of single notes, it is still open when notes superpose\nthemselves, forming chords. This report aims at developing some of the existing\ntechniques towards Music Transcription, particularly matrix factorization, and\nintroducing the concept of multi-channel automatic music transcription. This\nconcept will be explored with mathematical objects called tensors.",
          "link": "http://arxiv.org/abs/2107.11250",
          "publishedOn": "2021-07-26T02:00:59.512Z",
          "wordCount": 581,
          "title": "Multi-Channel Automatic Music Transcription Using Tensor Algebra. (arXiv:2107.11250v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kan_X/0/1/0/all/0/1\">Xuan Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Ying Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Recent studies in neuroscience show great potential of functional brain\nnetworks constructed from fMRI data for popularity modeling and clinical\npredictions. However, existing functional brain networks are noisy and unaware\nof downstream prediction tasks, while also incompatible with recent powerful\nmachine learning models of GNNs. In this work, we develop an end-to-end\ntrainable pipeline to extract prominent fMRI features, generate brain networks,\nand make predictions with GNNs, all under the guidance of downstream prediction\ntasks. Preliminary experiments on the PNC fMRI data show the superior\neffectiveness and unique interpretability of our framework.",
          "link": "http://arxiv.org/abs/2107.11247",
          "publishedOn": "2021-07-26T02:00:59.477Z",
          "wordCount": 552,
          "title": "Effective and Interpretable fMRI Analysis via Functional Brain Network Generation. (arXiv:2107.11247v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaoqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodgkinson_L/0/1/0/all/0/1\">Liam Hodgkinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theisen_R/0/1/0/all/0/1\">Ryan Theisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Joe Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramchandran_K/0/1/0/all/0/1\">Kannan Ramchandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>",
          "description": "Viewing neural network models in terms of their loss landscapes has a long\nhistory in the statistical mechanics approach to learning, and in recent years\nit has received attention within machine learning proper. Among other things,\nlocal metrics (such as the smoothness of the loss landscape) have been shown to\ncorrelate with global properties of the model (such as good generalization).\nHere, we perform a detailed empirical analysis of the loss landscape structure\nof thousands of neural network models, systematically varying learning tasks,\nmodel architectures, and/or quantity/quality of data. By considering a range of\nmetrics that attempt to capture different aspects of the loss landscape, we\ndemonstrate that the best test accuracy is obtained when: the loss landscape is\nglobally well-connected; ensembles of trained models are more similar to each\nother; and models converge to locally smooth regions. We also show that\nglobally poorly-connected landscapes can arise when models are small or when\nthey are trained to lower quality data; and that, if the loss landscape is\nglobally poorly-connected, then training to zero loss can actually lead to\nworse test accuracy. Based on these results, we develop a simple\none-dimensional model with load-like and temperature-like parameters, we\nintroduce the notion of an \\emph{effective loss landscape} depending on these\nparameters, and we interpret our results in terms of a \\emph{rugged convexity}\nof the loss landscape. When viewed through this lens, our detailed empirical\nresults shed light on phases of learning (and consequent double descent\nbehavior), fundamental versus incidental determinants of good generalization,\nthe role of load-like and temperature-like parameters in the learning process,\ndifferent influences on the loss landscape from model and data, and the\nrelationships between local and global metrics, all topics of recent interest.",
          "link": "http://arxiv.org/abs/2107.11228",
          "publishedOn": "2021-07-26T02:00:59.469Z",
          "wordCount": 727,
          "title": "Taxonomizing local versus global structure in neural network loss landscapes. (arXiv:2107.11228v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1\">Siyuan Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Ailing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>",
          "description": "Heatmap-based methods dominate in the field of human pose estimation by\nmodelling the output distribution through likelihood heatmaps. In contrast,\nregression-based methods are more efficient but suffer from inferior\nperformance. In this work, we explore maximum likelihood estimation (MLE) to\ndevelop an efficient and effective regression-based methods. From the\nperspective of MLE, adopting different regression losses is making different\nassumptions about the output density function. A density function closer to the\ntrue distribution leads to a better regression performance. In light of this,\nwe propose a novel regression paradigm with Residual Log-likelihood Estimation\n(RLE) to capture the underlying output distribution. Concretely, RLE learns the\nchange of the distribution instead of the unreferenced underlying distribution\nto facilitate the training process. With the proposed reparameterization\ndesign, our method is compatible with off-the-shelf flow models. The proposed\nmethod is effective, efficient and flexible. We show its potential in various\nhuman pose estimation tasks with comprehensive experiments. Compared to the\nconventional regression paradigm, regression with RLE bring 12.4 mAP\nimprovement on MSCOCO without any test-time overhead. Moreover, for the first\ntime, especially on multi-person pose estimation, our regression method is\nsuperior to the heatmap-based methods. Our code is available at\nhttps://github.com/Jeff-sjtu/res-loglikelihood-regression",
          "link": "http://arxiv.org/abs/2107.11291",
          "publishedOn": "2021-07-26T02:00:59.462Z",
          "wordCount": 647,
          "title": "Human Pose Regression with Residual Log-likelihood Estimation. (arXiv:2107.11291v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenxuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhi_Y/0/1/0/all/0/1\">Yiming Zhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Q/0/1/0/all/0/1\">Qingyang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Liming Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>",
          "description": "This paper introduces the sixth Oriental Language Recognition (OLR) 2021\nChallenge, which intends to improve the performance of language recognition\nsystems and speech recognition systems within multilingual scenarios. The data\nprofile, four tasks, two baselines, and the evaluation principles are\nintroduced in this paper. In addition to the Language Identification (LID)\ntasks, multilingual Automatic Speech Recognition (ASR) tasks are introduced to\nOLR 2021 Challenge for the first time. The challenge this year focuses on more\npractical and challenging problems, with four tasks: (1) constrained LID, (2)\nunconstrained LID, (3) constrained multilingual ASR, (4) unconstrained\nmultilingual ASR. Baselines for LID tasks and multilingual ASR tasks are\nprovided, respectively. The LID baseline system is an extended TDNN x-vector\nmodel constructed with Pytorch. A transformer-based end-to-end model is\nprovided as the multilingual ASR baseline system. These recipes will be online\npublished, and available for participants to construct their own LID or ASR\nsystems. The baseline results demonstrate that those tasks are rather\nchallenging and deserve more effort to achieve better performance.",
          "link": "http://arxiv.org/abs/2107.11113",
          "publishedOn": "2021-07-26T02:00:59.454Z",
          "wordCount": 636,
          "title": "OLR 2021 Challenge: Datasets, Rules and Baselines. (arXiv:2107.11113v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2011.11284",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Yip_K/0/1/0/all/0/1\">Kai Hou Yip</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Changeat_Q/0/1/0/all/0/1\">Quentin Changeat</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Nikolaou_N/0/1/0/all/0/1\">Nikolaos Nikolaou</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Morvan_M/0/1/0/all/0/1\">Mario Morvan</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Edwards_B/0/1/0/all/0/1\">Billy Edwards</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Waldmann_I/0/1/0/all/0/1\">Ingo P. Waldmann</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Tinetti_G/0/1/0/all/0/1\">Giovanna Tinetti</a>",
          "description": "Deep learning algorithms are growing in popularity in the field of\nexoplanetary science due to their ability to model highly non-linear relations\nand solve interesting problems in a data-driven manner. Several works have\nattempted to perform fast retrievals of atmospheric parameters with the use of\nmachine learning algorithms like deep neural networks (DNNs). Yet, despite\ntheir high predictive power, DNNs are also infamous for being 'black boxes'. It\nis their apparent lack of explainability that makes the astrophysics community\nreluctant to adopt them. What are their predictions based on? How confident\nshould we be in them? When are they wrong and how wrong can they be? In this\nwork, we present a number of general evaluation methodologies that can be\napplied to any trained model and answer questions like these. In particular, we\ntrain three different popular DNN architectures to retrieve atmospheric\nparameters from exoplanet spectra and show that all three achieve good\npredictive performance. We then present an extensive analysis of the\npredictions of DNNs, which can inform us - among other things - of the\ncredibility limits for atmospheric parameters for a given instrument and model.\nFinally, we perform a perturbation-based sensitivity analysis to identify to\nwhich features of the spectrum the outcome of the retrieval is most sensitive.\nWe conclude that for different molecules, the wavelength ranges to which the\nDNN's predictions are most sensitive, indeed coincide with their characteristic\nabsorption regions. The methodologies presented in this work help to improve\nthe evaluation of DNNs and to grant interpretability to their predictions.",
          "link": "http://arxiv.org/abs/2011.11284",
          "publishedOn": "2021-07-26T02:00:59.447Z",
          "wordCount": 747,
          "title": "Peeking inside the Black Box: Interpreting Deep Learning Models for Exoplanet Atmospheric Retrievals. (arXiv:2011.11284v2 [astro-ph.EP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11114",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Farchi_A/0/1/0/all/0/1\">Alban Farchi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bocquet_M/0/1/0/all/0/1\">Marc Bocquet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Laloyaux_P/0/1/0/all/0/1\">Patrick Laloyaux</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bonavita_M/0/1/0/all/0/1\">Massimo Bonavita</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Malartic_Q/0/1/0/all/0/1\">Quentin Malartic</a>",
          "description": "Recent studies have shown that it is possible to combine machine learning\nmethods with data assimilation to reconstruct a dynamical system using only\nsparse and noisy observations of that system. The same approach can be used to\ncorrect the error of a knowledge-based model. The resulting surrogate model is\nhybrid, with a statistical part supplementing a physical part. In practice, the\ncorrection can be added as an integrated term (i.e. in the model resolvent) or\ndirectly inside the tendencies of the physical model. The resolvent correction\nis easy to implement. The tendency correction is more technical, in particular\nit requires the adjoint of the physical model, but also more flexible. We use\nthe two-scale Lorenz model to compare the two methods. The accuracy in\nlong-range forecast experiments is somewhat similar between the surrogate\nmodels using the resolvent correction and the tendency correction. By contrast,\nthe surrogate models using the tendency correction significantly outperform the\nsurrogate models using the resolvent correction in data assimilation\nexperiments. Finally, we show that the tendency correction opens the\npossibility to make online model error correction, i.e. improving the model\nprogressively as new observations become available. The resulting algorithm can\nbe seen as a new formulation of weak-constraint 4D-Var. We compare online and\noffline learning using the same framework with the two-scale Lorenz system, and\nshow that with online learning, it is possible to extract all the information\nfrom sparse and noisy observations.",
          "link": "http://arxiv.org/abs/2107.11114",
          "publishedOn": "2021-07-26T02:00:59.420Z",
          "wordCount": 691,
          "title": "A comparison of combined data assimilation and machine learning methods for offline and online model error correction. (arXiv:2107.11114v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2011.13000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elangovan_R/0/1/0/all/0/1\">Reena Elangovan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shubham Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1\">Anand Raghunathan</a>",
          "description": "Precision scaling has emerged as a popular technique to optimize the compute\nand storage requirements of Deep Neural Networks (DNNs). Efforts toward\ncreating ultra-low-precision (sub-8-bit) DNNs suggest that the minimum\nprecision required to achieve a given network-level accuracy varies\nconsiderably across networks, and even across layers within a network,\nrequiring support for variable precision in DNN hardware. Previous proposals\nsuch as bit-serial hardware incur high overheads, significantly diminishing the\nbenefits of lower precision. To efficiently support precision\nre-configurability in DNN accelerators, we introduce an approximate computing\nmethod wherein DNN computations are performed block-wise (a block is a group of\nbits) and re-configurability is supported at the granularity of blocks. Results\nof block-wise computations are composed in an approximate manner to enable\nefficient re-configurability. We design a DNN accelerator that embodies\napproximate blocked computation and propose a method to determine a suitable\napproximation configuration for a given DNN. By varying the approximation\nconfigurations across DNNs, we achieve 1.17x-1.73x and 1.02x-2.04x improvement\nin system energy and performance respectively, over an 8-bit fixed-point (FxP8)\nbaseline, with negligible loss in classification accuracy. Further, by varying\nthe approximation configurations across layers and data-structures within DNNs,\nwe achieve 1.25x-2.42x and 1.07x-2.95x improvement in system energy and\nperformance respectively, with negligible accuracy loss.",
          "link": "http://arxiv.org/abs/2011.13000",
          "publishedOn": "2021-07-26T02:00:59.410Z",
          "wordCount": 676,
          "title": "Ax-BxP: Approximate Blocked Computation for Precision-Reconfigurable Deep Neural Network Acceleration. (arXiv:2011.13000v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.01350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Navratil_J/0/1/0/all/0/1\">Jiri Navratil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1\">Matthew Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elder_B/0/1/0/all/0/1\">Benjamin Elder</a>",
          "description": "Generating high quality uncertainty estimates for sequential regression,\nparticularly deep recurrent networks, remains a challenging and open problem.\nExisting approaches often make restrictive assumptions (such as stationarity)\nyet still perform poorly in practice, particularly in presence of real world\nnon-stationary signals and drift. This paper describes a flexible method that\ncan generate symmetric and asymmetric uncertainty estimates, makes no\nassumptions about stationarity, and outperforms competitive baselines on both\ndrift and non drift scenarios. This work helps make sequential regression more\neffective and practical for use in real-world applications, and is a powerful\nnew addition to the modeling toolbox for sequential uncertainty quantification\nin general.",
          "link": "http://arxiv.org/abs/2007.01350",
          "publishedOn": "2021-07-26T02:00:59.398Z",
          "wordCount": 566,
          "title": "Uncertainty Prediction for Deep Sequential Regression Using Meta Models. (arXiv:2007.01350v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1\">Satya Narayan Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marlin_B/0/1/0/all/0/1\">Benjamin M. Marlin</a>",
          "description": "Irregularly sampled time series commonly occur in several domains where they\npresent a significant challenge to standard deep learning models. In this\npaper, we propose a new deep learning framework for probabilistic interpolation\nof irregularly sampled time series that we call the Heteroscedastic Temporal\nVariational Autoencoder (HeTVAE). HeTVAE includes a novel input layer to encode\ninformation about input observation sparsity, a temporal VAE architecture to\npropagate uncertainty due to input sparsity, and a heteroscedastic output layer\nto enable variable uncertainty in output interpolations. Our results show that\nthe proposed architecture is better able to reflect variable uncertainty\nthrough time due to sparse and irregular sampling than a range of baseline and\ntraditional models, as well as recently proposed deep latent variable models\nthat use homoscedastic output layers.",
          "link": "http://arxiv.org/abs/2107.11350",
          "publishedOn": "2021-07-26T02:00:59.389Z",
          "wordCount": 562,
          "title": "Heteroscedastic Temporal Variational Autoencoder For Irregularly Sampled Time Series. (arXiv:2107.11350v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huyen N. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Jake Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan V.T. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Tommy Dang</a>",
          "description": "This paper presents VisMCA, an interactive visual analytics system that\nsupports deepening understanding in ML results, augmenting users' capabilities\nin correcting misclassification, and providing an analysis of underlying\npatterns, in response to the VAST Challenge 2020 Mini-Challenge 2. VisMCA\nfacilitates tracking provenance and provides a comprehensive view of object\ndetection results, easing re-labeling, and producing reliable, corrected data\nfor future training. Our solution implements multiple analytical views on\nvisual analysis to offer a deep insight for underlying pattern discovery.",
          "link": "http://arxiv.org/abs/2107.11181",
          "publishedOn": "2021-07-26T02:00:59.381Z",
          "wordCount": 563,
          "title": "VisMCA: A Visual Analytics System for Misclassification Correction and Analysis. VAST Challenge 2020, Mini-Challenge 2 Award: Honorable Mention for Detailed Analysis of Patterns of Misclassification. (arXiv:2107.11181v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Opala_A/0/1/0/all/0/1\">Andrzej Opala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panico_R/0/1/0/all/0/1\">Riccardo Panico</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardizzone_V/0/1/0/all/0/1\">Vincenzo Ardizzone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietka_B/0/1/0/all/0/1\">Barbara Pietka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szczytko_J/0/1/0/all/0/1\">Jacek Szczytko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanvitto_D/0/1/0/all/0/1\">Daniele Sanvitto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matuszewski_M/0/1/0/all/0/1\">Micha&#x142; Matuszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballarini_D/0/1/0/all/0/1\">Dario Ballarini</a>",
          "description": "In contrast to software simulations of neural networks, hardware or\nneuromorphic implementations have often limited or no tunability. While such\nnetworks promise great improvements in terms of speed and energy efficiency,\ntheir performance is limited by the difficulty to apply efficient teaching. We\npropose a system of non-tunable exciton-polariton nodes and an efficient\nteaching method that relies on the precise measurement of the nonlinear node\nresponse and the subsequent use of the backpropagation algorithm. We\ndemonstrate experimentally that the classification accuracy in the MNIST\nhandwritten digit benchmark is greatly improved compared to the case where\nbackpropagation is not used.",
          "link": "http://arxiv.org/abs/2107.11156",
          "publishedOn": "2021-07-26T02:00:59.362Z",
          "wordCount": 549,
          "title": "Teaching a neural network with non-tunable exciton-polariton nodes. (arXiv:2107.11156v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abrahamyan_L/0/1/0/all/0/1\">Lusine Abrahamyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziatchin_V/0/1/0/all/0/1\">Valentin Ziatchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1\">Nikos Deligiannis</a>",
          "description": "Compact convolutional neural networks (CNNs) have witnessed exceptional\nimprovements in performance in recent years. However, they still fail to\nprovide the same predictive power as CNNs with a large number of parameters.\nThe diverse and even abundant features captured by the layers is an important\ncharacteristic of these successful CNNs. However, differences in this\ncharacteristic between large CNNs and their compact counterparts have rarely\nbeen investigated. In compact CNNs, due to the limited number of parameters,\nabundant features are unlikely to be obtained, and feature diversity becomes an\nessential characteristic. Diverse features present in the activation maps\nderived from a data point during model inference may indicate the presence of a\nset of unique descriptors necessary to distinguish between objects of different\nclasses. In contrast, data points with low feature diversity may not provide a\nsufficient amount of unique descriptors to make a valid prediction; we refer to\nthem as random predictions. Random predictions can negatively impact the\noptimization process and harm the final performance. This paper proposes\naddressing the problem raised by random predictions by reshaping the standard\ncross-entropy to make it biased toward data points with a limited number of\nunique descriptive features. Our novel Bias Loss focuses the training on a set\nof valuable data points and prevents the vast number of samples with poor\nlearning features from misleading the optimization process. Furthermore, to\nshow the importance of diversity, we present a family of SkipNet models whose\narchitectures are brought to boost the number of unique descriptors in the last\nlayers. Our Skipnet-M can achieve 1% higher classification accuracy than\nMobileNetV3 Large.",
          "link": "http://arxiv.org/abs/2107.11170",
          "publishedOn": "2021-07-26T02:00:59.295Z",
          "wordCount": 709,
          "title": "Bias Loss for Mobile Neural Networks. (arXiv:2107.11170v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wit_J/0/1/0/all/0/1\">J.S. Panman de Wit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ham_J/0/1/0/all/0/1\">J. van der Ham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucur_D/0/1/0/all/0/1\">D. Bucur</a>",
          "description": "Mobile malware are malicious programs that target mobile devices. They are an\nincreasing problem, as seen in the rise of detected mobile malware samples per\nyear. The number of active smartphone users is expected to grow, stressing the\nimportance of research on the detection of mobile malware. Detection methods\nfor mobile malware exist but are still limited.\n\nIn this paper, we provide an overview of the performance of machine learning\n(ML) techniques to detect malware on Android, without using privileged access.\nThe ML-classifiers use device information such as the CPU usage, battery usage,\nand memory usage for the detection of 10 subtypes of Mobile Trojans on the\nAndroid Operating System (OS).\n\nWe use a real-life dataset containing device and malware data from 47 users\nfor a year (2016). We examine which features, i.e. aspects, of a device, are\nmost important to monitor to detect (subtypes of) Mobile Trojans. The focus of\nthis paper is on dynamic hardware features. Using these dynamic features we\napply state-of-the-art machine learning classifiers: Random Forest, K-Nearest\nNeighbour, and AdaBoost. We show classification results on different feature\nsets, making a distinction between global device features, and specific app\nfeatures. None of the measured feature sets require privileged access.\n\nOur results show that the Random Forest classifier performs best as a general\nmalware classifier: across 10 subtypes of Mobile Trojans, it achieves an F1\nscore of 0.73 with a False Positive Rate (FPR) of 0.009 and a False Negative\nRate (FNR) of 0.380. The Random Forest, K-Nearest Neighbours, and AdaBoost\nclassifiers achieve F1 scores above 0.72, an FPR below 0.02 and, an FNR below\n0.33, when trained separately to detect each subtype of Mobile Trojans.",
          "link": "http://arxiv.org/abs/2107.11167",
          "publishedOn": "2021-07-26T02:00:59.288Z",
          "wordCount": 745,
          "title": "Dynamic detection of mobile malware using smartphone data and machine learning. (arXiv:2107.11167v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Whittington_J/0/1/0/all/0/1\">James C.R. Whittington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabra_R/0/1/0/all/0/1\">Rishabh Kabra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthey_L/0/1/0/all/0/1\">Loic Matthey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgess_C/0/1/0/all/0/1\">Christopher P. Burgess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerchner_A/0/1/0/all/0/1\">Alexander Lerchner</a>",
          "description": "Learning structured representations of visual scenes is currently a major\nbottleneck to bridging perception with reasoning. While there has been exciting\nprogress with slot-based models, which learn to segment scenes into sets of\nobjects, learning configurational properties of entire groups of objects is\nstill under-explored. To address this problem, we introduce Constellation, a\nnetwork that learns relational abstractions of static visual scenes, and\ngeneralises these abstractions over sensory particularities, thus offering a\npotential basis for abstract relational reasoning. We further show that this\nbasis, along with language association, provides a means to imagine sensory\ncontent in new ways. This work is a first step in the explicit representation\nof visual relationships and using them for complex cognitive procedures.",
          "link": "http://arxiv.org/abs/2107.11153",
          "publishedOn": "2021-07-26T02:00:59.281Z",
          "wordCount": 563,
          "title": "Constellation: Learning relational abstractions over objects for compositional imagination. (arXiv:2107.11153v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shengpu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1\">Jenna Wiens</a>",
          "description": "Reinforcement learning (RL) can be used to learn treatment policies and aid\ndecision making in healthcare. However, given the need for generalization over\ncomplex state/action spaces, the incorporation of function approximators (e.g.,\ndeep neural networks) requires model selection to reduce overfitting and\nimprove policy performance at deployment. Yet a standard validation pipeline\nfor model selection requires running a learned policy in the actual\nenvironment, which is often infeasible in a healthcare setting. In this work,\nwe investigate a model selection pipeline for offline RL that relies on\noff-policy evaluation (OPE) as a proxy for validation performance. We present\nan in-depth analysis of popular OPE methods, highlighting the additional\nhyperparameters and computational requirements (fitting/inference of auxiliary\nmodels) when used to rank a set of candidate policies. We compare the utility\nof different OPE methods as part of the model selection pipeline in the context\nof learning to treat patients with sepsis. Among all the OPE methods we\nconsidered, fitted Q evaluation (FQE) consistently leads to the best validation\nranking, but at a high computational cost. To balance this trade-off between\naccuracy of ranking and computational efficiency, we propose a simple two-stage\napproach to accelerate model selection by avoiding potentially unnecessary\ncomputation. Our work serves as a practical guide for offline RL model\nselection and can help RL practitioners select policies using real-world\ndatasets. To facilitate reproducibility and future extensions, the code\naccompanying this paper is available online at\nhttps://github.com/MLD3/OfflineRL_ModelSelection.",
          "link": "http://arxiv.org/abs/2107.11003",
          "publishedOn": "2021-07-26T02:00:59.232Z",
          "wordCount": 684,
          "title": "Model Selection for Offline Reinforcement Learning: Practical Considerations for Healthcare Settings. (arXiv:2107.11003v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brophy_E/0/1/0/all/0/1\">Eoin Brophy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ward_T/0/1/0/all/0/1\">Tomas Ward</a>",
          "description": "Generative adversarial networks (GANs) studies have grown exponentially in\nthe past few years. Their impact has been seen mainly in the computer vision\nfield with realistic image and video manipulation, especially generation,\nmaking significant advancements. While these computer vision advances have\ngarnered much attention, GAN applications have diversified across disciplines\nsuch as time series and sequence generation. As a relatively new niche for\nGANs, fieldwork is ongoing to develop high quality, diverse and private time\nseries data. In this paper, we review GAN variants designed for time series\nrelated applications. We propose a taxonomy of discrete-variant GANs and\ncontinuous-variant GANs, in which GANs deal with discrete time series and\ncontinuous time series data. Here we showcase the latest and most popular\nliterature in this field; their architectures, results, and applications. We\nalso provide a list of the most popular evaluation metrics and their\nsuitability across applications. Also presented is a discussion of privacy\nmeasures for these GANs and further protections and directions for dealing with\nsensitive data. We aim to frame clearly and concisely the latest and\nstate-of-the-art research in this area and their applications to real-world\ntechnologies.",
          "link": "http://arxiv.org/abs/2107.11098",
          "publishedOn": "2021-07-26T02:00:59.225Z",
          "wordCount": 625,
          "title": "Generative adversarial networks in time series: A survey and taxonomy. (arXiv:2107.11098v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weien Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wen Yao</a>",
          "description": "Physics-informed neural networks (PINNs) have been widely used to solve\nvarious scientific computing problems. However, large training costs limit\nPINNs for some real-time applications. Although some works have been proposed\nto improve the training efficiency of PINNs, few consider the influence of\ninitialization. To this end, we propose a New Reptile initialization based\nPhysics-Informed Neural Network (NRPINN). The original Reptile algorithm is a\nmeta-learning initialization method based on labeled data. PINNs can be trained\nwith less labeled data or even without any labeled data by adding partial\ndifferential equations (PDEs) as a penalty term into the loss function.\nInspired by this idea, we propose the new Reptile initialization to sample more\ntasks from the parameterized PDEs and adapt the penalty term of the loss. The\nnew Reptile initialization can acquire initialization parameters from related\ntasks by supervised, unsupervised, and semi-supervised learning. Then, PINNs\nwith initialization parameters can efficiently solve PDEs. Besides, the new\nReptile initialization can also be used for the variants of PINNs. Finally, we\ndemonstrate and verify the NRPINN considering both forward problems, including\nsolving Poisson, Burgers, and Schr\\\"odinger equations, as well as inverse\nproblems, where unknown parameters in the PDEs are estimated. Experimental\nresults show that the NRPINN training is much faster and achieves higher\naccuracy than PINNs with other initialization methods.",
          "link": "http://arxiv.org/abs/2107.10991",
          "publishedOn": "2021-07-26T02:00:59.218Z",
          "wordCount": 649,
          "title": "A novel meta-learning initialization method for physics-informed neural networks. (arXiv:2107.10991v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11099",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Jing_Y/0/1/0/all/0/1\">Yu Jing</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wu_C/0/1/0/all/0/1\">Chonghang Wu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Fu_W/0/1/0/all/0/1\">Wenbing Fu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_X/0/1/0/all/0/1\">Xiaogang Li</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>",
          "description": "With the rapid growth of qubit numbers and coherence times in quantum\nhardware technology, implementing shallow neural networks on the so-called\nNoisy Intermediate-Scale Quantum (NISQ) devices has attracted a lot of\ninterest. Many quantum (convolutional) circuit ansaetze are proposed for\ngrayscale images classification tasks with promising empirical results.\nHowever, when applying these ansaetze on RGB images, the intra-channel\ninformation that is useful for vision tasks is not extracted effectively. In\nthis paper, we propose two types of quantum circuit ansaetze to simulate\nconvolution operations on RGB images, which differ in the way how inter-channel\nand intra-channel information are extracted. To the best of our knowledge, this\nis the first work of a quantum convolutional circuit to deal with RGB images\neffectively, with a higher test accuracy compared to the purely classical CNNs.\nWe also investigate the relationship between the size of quantum circuit ansatz\nand the learnability of the hybrid quantum-classical convolutional neural\nnetwork. Through experiments based on CIFAR-10 and MNIST datasets, we\ndemonstrate that a larger size of the quantum circuit ansatz improves\npredictive performance in multiclass classification tasks, providing useful\ninsights for near term quantum algorithm developments.",
          "link": "http://arxiv.org/abs/2107.11099",
          "publishedOn": "2021-07-26T02:00:59.211Z",
          "wordCount": 633,
          "title": "RGB Image Classification with Quantum Convolutional Ansaetze. (arXiv:2107.11099v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11022",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yao_K/0/1/0/all/0/1\">Kai Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jie Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jude_C/0/1/0/all/0/1\">Curran Jude</a>",
          "description": "We consider unsupervised cell nuclei segmentation in this paper. Exploiting\nthe recently-proposed unpaired image-to-image translation between cell nuclei\nimages and randomly synthetic masks, existing approaches, e.g., CycleGAN, have\nachieved encouraging results. However, these methods usually take a two-stage\npipeline and fail to learn end-to-end in cell nuclei images. More seriously,\nthey could lead to the lossy transformation problem, i.e., the content\ninconsistency between the original images and the corresponding segmentation\noutput. To address these limitations, we propose a novel end-to-end\nunsupervised framework called Aligned Disentangling Generative Adversarial\nNetwork (AD-GAN). Distinctively, AD-GAN introduces representation\ndisentanglement to separate content representation (the underling spatial\nstructure) from style representation (the rendering of the structure). With\nthis framework, spatial structure can be preserved explicitly, enabling a\nsignificant reduction of macro-level lossy transformation. We also propose a\nnovel training algorithm able to align the disentangled content in the latent\nspace to reduce micro-level lossy transformation. Evaluations on real-world 2D\nand 3D datasets show that AD-GAN substantially outperforms the other comparison\nmethods and the professional software both quantitatively and qualitatively.\nSpecifically, the proposed AD-GAN leads to significant improvement over the\ncurrent best unsupervised methods by an average 17.8% relatively (w.r.t. the\nmetric DICE) on four cell nuclei datasets. As an unsupervised method, AD-GAN\neven performs competitive with the best supervised models, taking a further\nleap towards end-to-end unsupervised nuclei segmentation.",
          "link": "http://arxiv.org/abs/2107.11022",
          "publishedOn": "2021-07-26T02:00:59.204Z",
          "wordCount": 674,
          "title": "AD-GAN: End-to-end Unsupervised Nuclei Segmentation with Aligned Disentangling Training. (arXiv:2107.11022v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Puchert_P/0/1/0/all/0/1\">Patrik Puchert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermosilla_P/0/1/0/all/0/1\">Pedro Hermosilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1\">Tobias Ritschel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1\">Timo Ropinski</a>",
          "description": "Density estimation plays a crucial role in many data analysis tasks, as it\ninfers a continuous probability density function (PDF) from discrete samples.\nThus, it is used in tasks as diverse as analyzing population data, spatial\nlocations in 2D sensor readings, or reconstructing scenes from 3D scans. In\nthis paper, we introduce a learned, data-driven deep density estimation (DDE)\nto infer PDFs in an accurate and efficient manner, while being independent of\ndomain dimensionality or sample size. Furthermore, we do not require access to\nthe original PDF during estimation, neither in parametric form, nor as priors,\nor in the form of many samples. This is enabled by training an unstructured\nconvolutional neural network on an infinite stream of synthetic PDFs, as\nunbound amounts of synthetic training data generalize better across a deck of\nnatural PDFs than any natural finite training data will do. Thus, we hope that\nour publicly available DDE method will be beneficial in many areas of data\nanalysis, where continuous models are to be estimated from discrete\nobservations.",
          "link": "http://arxiv.org/abs/2107.11085",
          "publishedOn": "2021-07-26T02:00:59.184Z",
          "wordCount": 629,
          "title": "Data-driven deep density estimation. (arXiv:2107.11085v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anastasiu_C/0/1/0/all/0/1\">Cristian Anastasiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_H/0/1/0/all/0/1\">Hanna Behnke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luck_S/0/1/0/all/0/1\">Sarah L&#xfc;ck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malesevic_V/0/1/0/all/0/1\">Viktor Malesevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najmi_A/0/1/0/all/0/1\">Aamna Najmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poveda_Panter_J/0/1/0/all/0/1\">Javier Poveda-Panter</a>",
          "description": "Automated headline generation for online news articles is not a trivial task\n- machine generated titles need to be grammatically correct, informative,\ncapture attention and generate search traffic without being \"click baits\" or\n\"fake news\". In this paper we showcase how a pre-trained language model can be\nleveraged to create an abstractive news headline generator for German language.\nWe incorporate state of the art fine-tuning techniques for abstractive text\nsummarization, i.e. we use different optimizers for the encoder and decoder\nwhere the former is pre-trained and the latter is trained from scratch. We\nmodify the headline generation to incorporate frequently sought keywords\nrelevant for search engine optimization. We conduct experiments on a German\nnews data set and achieve a ROUGE-L-gram F-score of 40.02. Furthermore, we\naddress the limitations of ROUGE for measuring the quality of text\nsummarization by introducing a sentence similarity metric and human evaluation.",
          "link": "http://arxiv.org/abs/2107.10935",
          "publishedOn": "2021-07-26T02:00:59.176Z",
          "wordCount": 591,
          "title": "DeepTitle -- Leveraging BERT to generate Search Engine Optimized Headlines. (arXiv:2107.10935v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10980",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Li_K/0/1/0/all/0/1\">Kun Li</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Xia_S/0/1/0/all/0/1\">Steve Q. Xia</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Liu_H/0/1/0/all/0/1\">Hongfu Liu</a>",
          "description": "We investigate the effectiveness of different machine learning methodologies\nin predicting economic cycles. We identify the deep learning methodology of\nBi-LSTM with Autoencoder as the most accurate model to forecast the beginning\nand end of economic recessions in the U.S. We adopt commonly-available macro\nand market-condition features to compare the ability of different machine\nlearning models to generate good predictions both in-sample and out-of-sample.\nThe proposed model is flexible and dynamic when both predictive variables and\nmodel coefficients vary over time. It provided good out-of-sample predictions\nfor the past two recessions and early warning about the COVID-19 recession.",
          "link": "http://arxiv.org/abs/2107.10980",
          "publishedOn": "2021-07-26T02:00:59.169Z",
          "wordCount": 577,
          "title": "Economic Recession Prediction Using Deep Neural Network. (arXiv:2107.10980v1 [econ.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jae Won Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dong-Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1\">Yunjae Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>",
          "description": "Recent state-of-the-art active learning methods have mostly leveraged\nGenerative Adversarial Networks (GAN) for sample acquisition; however, GAN is\nusually known to suffer from instability and sensitivity to hyper-parameters.\nIn contrast to these methods, we propose in this paper a novel active learning\nframework that we call Maximum Classifier Discrepancy for Active Learning\n(MCDAL) which takes the prediction discrepancies between multiple classifiers.\nIn particular, we utilize two auxiliary classification layers that learn\ntighter decision boundaries by maximizing the discrepancies among them.\nIntuitively, the discrepancies in the auxiliary classification layers'\npredictions indicate the uncertainty in the prediction. In this regard, we\npropose a novel method to leverage the classifier discrepancies for the\nacquisition function for active learning. We also provide an interpretation of\nour idea in relation to existing GAN based active learning methods and domain\nadaptation frameworks. Moreover, we empirically demonstrate the utility of our\napproach where the performance of our approach exceeds the state-of-the-art\nmethods on several image classification and semantic segmentation datasets in\nactive learning setups.",
          "link": "http://arxiv.org/abs/2107.11049",
          "publishedOn": "2021-07-26T02:00:59.162Z",
          "wordCount": 614,
          "title": "MCDAL: Maximum Classifier Discrepancy for Active Learning. (arXiv:2107.11049v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11042",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Abduallah_Y/0/1/0/all/0/1\">Yasser Abduallah</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wang_J/0/1/0/all/0/1\">Jason T. L. Wang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Shen_Y/0/1/0/all/0/1\">Yucong Shen</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Alobaid_K/0/1/0/all/0/1\">Khalid A. Alobaid</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Criscuoli_S/0/1/0/all/0/1\">Serena Criscuoli</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wang_H/0/1/0/all/0/1\">Haimin Wang</a>",
          "description": "The Earth's primary source of energy is the radiant energy generated by the\nSun, which is referred to as solar irradiance, or total solar irradiance (TSI)\nwhen all of the radiation is measured. A minor change in the solar irradiance\ncan have a significant impact on the Earth's climate and atmosphere. As a\nresult, studying and measuring solar irradiance is crucial in understanding\nclimate changes and solar variability. Several methods have been developed to\nreconstruct total solar irradiance for long and short periods of time; however,\nthey are physics-based and rely on the availability of data, which does not go\nbeyond 9,000 years. In this paper we propose a new method, called TSInet, to\nreconstruct total solar irradiance by deep learning for short and long periods\nof time that span beyond the physical models' data availability. On the data\nthat are available, our method agrees well with the state-of-the-art\nphysics-based reconstruction models. To our knowledge, this is the first time\nthat deep learning has been used to reconstruct total solar irradiance for more\nthan 9,000 years.",
          "link": "http://arxiv.org/abs/2107.11042",
          "publishedOn": "2021-07-26T02:00:59.143Z",
          "wordCount": 628,
          "title": "Deep Learning Based Reconstruction of Total Solar Irradiance. (arXiv:2107.11042v1 [astro-ph.SR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Richman_R/0/1/0/all/0/1\">Ronald Richman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuthrich_M/0/1/0/all/0/1\">Mario V. W&#xfc;thrich</a>",
          "description": "Deep learning models have gained great popularity in statistical modeling\nbecause they lead to very competitive regression models, often outperforming\nclassical statistical models such as generalized linear models. The\ndisadvantage of deep learning models is that their solutions are difficult to\ninterpret and explain, and variable selection is not easily possible because\ndeep learning models solve feature engineering and variable selection\ninternally in a nontransparent way. Inspired by the appealing structure of\ngeneralized linear models, we propose a new network architecture that shares\nsimilar features as generalized linear models, but provides superior predictive\npower benefiting from the art of representation learning. This new architecture\nallows for variable selection of tabular data and for interpretation of the\ncalibrated deep learning model, in fact, our approach provides an additive\ndecomposition in the spirit of Shapley values and integrated gradients.",
          "link": "http://arxiv.org/abs/2107.11059",
          "publishedOn": "2021-07-26T02:00:59.136Z",
          "wordCount": 583,
          "title": "LocalGLMnet: interpretable deep learning for tabular data. (arXiv:2107.11059v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Simin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>",
          "description": "Deep learning (DL) techniques have achieved great success in predictive\naccuracy in a variety of tasks, but deep neural networks (DNNs) are shown to\nproduce highly overconfident scores for even abnormal samples. Well-defined\nuncertainty indicates whether a model's output should (or should not) be\ntrusted and thus becomes critical in real-world scenarios which typically\ninvolves shifted input distributions due to many factors. Existing uncertainty\napproaches assume that testing samples from a different data distribution would\ninduce unreliable model predictions thus have higher uncertainty scores. They\nquantify model uncertainty by calibrating DL model's confidence of a given\ninput and evaluate the effectiveness in computer vision (CV) and natural\nlanguage processing (NLP)-related tasks. However, their methodologies'\nreliability may be compromised under programming tasks due to difference in\ndata representations and shift patterns. In this paper, we first define three\ndifferent types of distribution shift in program data and build a large-scale\nshifted Java dataset. We implement two common programming language tasks on our\ndataset to study the effect of each distribution shift on DL model performance.\nWe also propose a large-scale benchmark of existing state-of-the-art predictive\nuncertainty on programming tasks and investigate their effectiveness under data\ndistribution shift. Experiments show that program distribution shift does\ndegrade the DL model performance to varying degrees and that existing\nuncertainty methods all present certain limitations in quantifying uncertainty\non program dataset.",
          "link": "http://arxiv.org/abs/2107.10989",
          "publishedOn": "2021-07-26T02:00:59.128Z",
          "wordCount": 669,
          "title": "Estimating Predictive Uncertainty Under Program Data Distribution Shift. (arXiv:2107.10989v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lijie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_S/0/1/0/all/0/1\">Shuo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Hanshen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>",
          "description": "As one of the most fundamental problems in machine learning, statistics and\ndifferential privacy, Differentially Private Stochastic Convex Optimization\n(DP-SCO) has been extensively studied in recent years. However, most of the\nprevious work can only handle either regular data distribution or irregular\ndata in the low dimensional space case. To better understand the challenges\narising from irregular data distribution, in this paper we provide the first\nstudy on the problem of DP-SCO with heavy-tailed data in the high dimensional\nspace. In the first part we focus on the problem over some polytope constraint\n(such as the $\\ell_1$-norm ball). We show that if the loss function is smooth\nand its gradient has bounded second order moment, it is possible to get a (high\nprobability) error bound (excess population risk) of $\\tilde{O}(\\frac{\\log\nd}{(n\\epsilon)^\\frac{1}{3}})$ in the $\\epsilon$-DP model, where $n$ is the\nsample size and $d$ is the dimensionality of the underlying space. Next, for\nLASSO, if the data distribution that has bounded fourth-order moments, we\nimprove the bound to $\\tilde{O}(\\frac{\\log d}{(n\\epsilon)^\\frac{2}{5}})$ in the\n$(\\epsilon, \\delta)$-DP model. In the second part of the paper, we study sparse\nlearning with heavy-tailed data. We first revisit the sparse linear model and\npropose a truncated DP-IHT method whose output could achieve an error of\n$\\tilde{O}(\\frac{s^{*2}\\log d}{n\\epsilon})$, where $s^*$ is the sparsity of the\nunderlying parameter. Then we study a more general problem over the sparsity\n({\\em i.e.,} $\\ell_0$-norm) constraint, and show that it is possible to achieve\nan error of $\\tilde{O}(\\frac{s^{*\\frac{3}{2}}\\log d}{n\\epsilon})$, which is\nalso near optimal up to a factor of $\\tilde{O}{(\\sqrt{s^*})}$, if the loss\nfunction is smooth and strongly convex.",
          "link": "http://arxiv.org/abs/2107.11136",
          "publishedOn": "2021-07-26T02:00:59.121Z",
          "wordCount": 707,
          "title": "High Dimensional Differentially Private Stochastic Optimization with Heavy-tailed Data. (arXiv:2107.11136v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11107",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Al_Saffar_A/0/1/0/all/0/1\">A. Al-Saffar</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Guo_L/0/1/0/all/0/1\">L. Guo</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Abbosh_A/0/1/0/all/0/1\">A. Abbosh</a>",
          "description": "Electromagnetic medical imaging in the microwave regime is a hard problem\nnotorious for 1) instability 2) under-determinism. This two-pronged problem is\ntackled with a two-pronged solution that uses double compression to maximally\nutilizing the cheap unlabelled data to a) provide a priori information required\nto ease under-determinism and b) reduce sensitivity of inference to the input.\nThe result is a stable solver with a high resolution output. DeepHead is a\nfully data-driven implementation of the paradigm proposed in the context of\nmicrowave brain imaging. It infers the dielectric distribution of the brain at\na desired single frequency while making use of an input that spreads over a\nwide band of frequencies. The performance of the model is evaluated with both\nsimulations and human volunteers experiments. The inference made is juxtaposed\nwith ground-truth dielectric distribution in simulation case, and the golden\nMRI / CT imaging modalities of the volunteers in real-world case.",
          "link": "http://arxiv.org/abs/2107.11107",
          "publishedOn": "2021-07-26T02:00:59.113Z",
          "wordCount": 587,
          "title": "Introducing: DeepHead, Wide-band Electromagnetic Imaging Paradigm. (arXiv:2107.11107v1 [physics.med-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shojaee_P/0/1/0/all/0/1\">Parshin Shojaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Ran Jin</a>",
          "description": "Reducing the shortage of organ donations to meet the demands of patients on\nthe waiting list has being a major challenge in organ transplantation. Because\nof the shortage, organ matching decision is the most critical decision to\nassign the limited viable organs to the most suitable patients. Currently,\norgan matching decisions were only made by matching scores calculated via\nscoring models, which are built by the first principles. However, these models\nmay disagree with the actual post-transplantation matching performance (e.g.,\npatient's post-transplant quality of life (QoL) or graft failure measurements).\nIn this paper, we formulate the organ matching decision-making as a top-N\nrecommendation problem and propose an Adaptively Weighted Top-N Recommendation\n(AWTR) method. AWTR improves performance of the current scoring models by using\nlimited actual matching performance in historical data set as well as the\ncollected covariates from organ donors and patients. AWTR sacrifices the\noverall recommendation accuracy by emphasizing the recommendation and ranking\naccuracy for top-N matched patients. The proposed method is validated in a\nsimulation study, where KAS [60] is used to simulate the organ-patient\nrecommendation response. The results show that our proposed method outperforms\nseven state-of-the-art top-N recommendation benchmark methods.",
          "link": "http://arxiv.org/abs/2107.10971",
          "publishedOn": "2021-07-26T02:00:59.104Z",
          "wordCount": 625,
          "title": "Adaptively Weighted Top-N Recommendation for Organ Matching. (arXiv:2107.10971v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bashkirova_D/0/1/0/all/0/1\">Dina Bashkirova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Samarth Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_K/0/1/0/all/0/1\">Kuniaki Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teterwak_P/0/1/0/all/0/1\">Piotr Teterwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usman_B/0/1/0/all/0/1\">Ben Usman</a>",
          "description": "Progress in machine learning is typically measured by training and testing a\nmodel on the same distribution of data, i.e., the same domain. This\nover-estimates future accuracy on out-of-distribution data. The Visual Domain\nAdaptation (VisDA) 2021 competition tests models' ability to adapt to novel\ntest distributions and handle distributional shift. We set up unsupervised\ndomain adaptation challenges for image classifiers and will evaluate adaptation\nto novel viewpoints, backgrounds, modalities and degradation in quality. Our\nchallenge draws on large-scale publicly available datasets but constructs the\nevaluation across domains, rather that the traditional in-domain bench-marking.\nFurthermore, we focus on the difficult \"universal\" setting where, in addition\nto input distribution drift, methods may encounter missing and/or novel classes\nin the target dataset. Performance will be measured using a rigorous protocol,\ncomparing to state-of-the-art domain adaptation methods with the help of\nestablished metrics. We believe that the competition will encourage further\nimprovement in machine learning methods' ability to handle realistic data in\nmany deployment scenarios.",
          "link": "http://arxiv.org/abs/2107.11011",
          "publishedOn": "2021-07-26T02:00:59.097Z",
          "wordCount": 610,
          "title": "VisDA-2021 Competition Universal Domain Adaptation to Improve Performance on Out-of-Distribution Data. (arXiv:2107.11011v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Siyuan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chuanming Tang</a>",
          "description": "AI-based methods have been widely applied to tourism demand forecasting.\nHowever, current AI-based methods are short of the ability to process long-term\ndependency, and most of them lack interpretability. The Transformer used\ninitially for machine translation shows an incredible ability to long-term\ndependency processing. Based on the Transformer, we proposed a time series\nTransformer (Tsformer) with Encoder-Decoder architecture for tourism demand\nforecasting. The proposed Tsformer encodes long-term dependency with encoder,\ncaptures short-term dependency with decoder, and simplifies the attention\ninteractions under the premise of highlighting dominant attention through a\nseries of attention masking mechanisms. These improvements make the multi-head\nattention mechanism process the input sequence according to the time\nrelationship, contributing to better interpretability. What's more, the context\nprocessing ability of the Encoder-Decoder architecture allows adopting the\ncalendar of days to be forecasted to enhance the forecasting performance.\nExperiments conducted on the Jiuzhaigou valley and Siguniang mountain tourism\ndemand datasets with other nine baseline methods indicate that the proposed\nTsformer outperformed all baseline models in the short-term and long-term\ntourism demand forecasting tasks. Moreover, ablation studies demonstrate that\nthe adoption of the calendar of days to be forecasted contributes to the\nforecasting performance of the proposed Tsformer. For better interpretability,\nthe attention weight matrix visualization is performed. It indicates that the\nTsformer concentrates on seasonal features and days close to days to be\nforecast in short-term forecasting.",
          "link": "http://arxiv.org/abs/2107.10977",
          "publishedOn": "2021-07-26T02:00:59.088Z",
          "wordCount": 653,
          "title": "Tsformer: Time series Transformer for tourism demand forecasting. (arXiv:2107.10977v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11046",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Keith_B/0/1/0/all/0/1\">Brendan Keith</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Khristenko_U/0/1/0/all/0/1\">Ustim Khristenko</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wohlmuth_B/0/1/0/all/0/1\">Barbara Wohlmuth</a>",
          "description": "We develop a novel data-driven approach to modeling the atmospheric boundary\nlayer. This approach leads to a nonlocal, anisotropic synthetic turbulence\nmodel which we refer to as the deep rapid distortion (DRD) model. Our approach\nrelies on an operator regression problem which characterizes the best fitting\ncandidate in a general family of nonlocal covariance kernels parameterized in\npart by a neural network. This family of covariance kernels is expressed in\nFourier space and is obtained from approximate solutions to the Navier--Stokes\nequations at very high Reynolds numbers. Each member of the family incorporates\nimportant physical properties such as mass conservation and a realistic energy\ncascade. The DRD model can be calibrated with noisy data from field\nexperiments. After calibration, the model can be used to generate synthetic\nturbulent velocity fields. To this end, we provide a new numerical method based\non domain decomposition which delivers scalable, memory-efficient turbulence\ngeneration with the DRD model as well as others. We demonstrate the robustness\nof our approach with both filtered and noisy data coming from the 1968 Air\nForce Cambridge Research Laboratory Kansas experiments. Using this data, we\nwitness exceptional accuracy with the DRD model, especially when compared to\nthe International Electrotechnical Commission standard.",
          "link": "http://arxiv.org/abs/2107.11046",
          "publishedOn": "2021-07-26T02:00:59.067Z",
          "wordCount": 646,
          "title": "Learning the structure of wind: A data-driven nonlocal turbulence model for the atmospheric boundary layer. (arXiv:2107.11046v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10970",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Chia Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1\">Marina Meil&#x103;</a>",
          "description": "The null space of the $k$-th order Laplacian $\\mathbf{\\mathcal L}_k$, known\nas the {\\em $k$-th homology vector space}, encodes the non-trivial topology of\na manifold or a network. Understanding the structure of the homology embedding\ncan thus disclose geometric or topological information from the data. The study\nof the null space embedding of the graph Laplacian $\\mathbf{\\mathcal L}_0$ has\nspurred new research and applications, such as spectral clustering algorithms\nwith theoretical guarantees and estimators of the Stochastic Block Model. In\nthis work, we investigate the geometry of the $k$-th homology embedding and\nfocus on cases reminiscent of spectral clustering. Namely, we analyze the {\\em\nconnected sum} of manifolds as a perturbation to the direct sum of their\nhomology embeddings. We propose an algorithm to factorize the homology\nembedding into subspaces corresponding to a manifold's simplest topological\ncomponents. The proposed framework is applied to the {\\em shortest homologous\nloop detection} problem, a problem known to be NP-hard in general. Our spectral\nloop detection algorithm scales better than existing methods and is effective\non diverse data such as point clouds and images.",
          "link": "http://arxiv.org/abs/2107.10970",
          "publishedOn": "2021-07-26T02:00:59.051Z",
          "wordCount": 616,
          "title": "The decomposition of the higher-order homology embedding constructed from the $k$-Laplacian. (arXiv:2107.10970v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shahid_O/0/1/0/all/0/1\">Osama Shahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pouriyeh_S/0/1/0/all/0/1\">Seyedamin Pouriyeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parizi_R/0/1/0/all/0/1\">Reza M. Parizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Quan Z. Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_G/0/1/0/all/0/1\">Gautam Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>",
          "description": "Federated Learning (FL) is known to perform Machine Learning tasks in a\ndistributed manner. Over the years, this has become an emerging technology\nespecially with various data protection and privacy policies being imposed FL\nallows performing machine learning tasks whilst adhering to these challenges.\nAs with the emerging of any new technology, there are going to be challenges\nand benefits. A challenge that exists in FL is the communication costs, as FL\ntakes place in a distributed environment where devices connected over the\nnetwork have to constantly share their updates this can create a communication\nbottleneck. In this paper, we present a survey of the research that is\nperformed to overcome the communication constraints in an FL setting.",
          "link": "http://arxiv.org/abs/2107.10996",
          "publishedOn": "2021-07-26T02:00:59.042Z",
          "wordCount": 554,
          "title": "Communication Efficiency in Federated Learning: Achievements and Challenges. (arXiv:2107.10996v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1\">Pinzhuo Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yao Gao</a>",
          "description": "Meta-learning provides a promising way for learning to efficiently learn and\nachieves great success in many applications. However, most meta-learning\nliterature focuses on dealing with tasks from a same domain, making it brittle\nto generalize to tasks from the other unseen domains. In this work, we address\nthis problem by simulating tasks from the other unseen domains to improve the\ngeneralization and robustness of meta-learning method. Specifically, we propose\na model-agnostic shift layer to learn how to simulate the domain shift and\ngenerate pseudo tasks, and develop a new adversarial learning-to-learn\nmechanism to train it. Based on the pseudo tasks, the meta-learning model can\nlearn cross-domain meta-knowledge, which can generalize well on unseen domains.\nWe conduct extensive experiments under the domain generalization setting.\nExperimental results demonstrate that the proposed shift layer is applicable to\nvarious meta-learning frameworks. Moreover, our method also leads to\nstate-of-the-art performance on different cross-domain few-shot classification\nbenchmarks and produces good results on cross-domain few-shot regression.",
          "link": "http://arxiv.org/abs/2107.11056",
          "publishedOn": "2021-07-26T02:00:59.013Z",
          "wordCount": 600,
          "title": "Improving the Generalization of Meta-learning on Unseen Domains via Adversarial Shift. (arXiv:2107.11056v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sandfelder_D/0/1/0/all/0/1\">Dylan Sandfelder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijayan_P/0/1/0/all/0/1\">Priyesh Vijayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1\">William L. Hamilton</a>",
          "description": "Graph neural networks (GNNs) have achieved remarkable success as a framework\nfor deep learning on graph-structured data. However, GNNs are fundamentally\nlimited by their tree-structured inductive bias: the WL-subtree kernel\nformulation bounds the representational capacity of GNNs, and polynomial-time\nGNNs are provably incapable of recognizing triangles in a graph. In this work,\nwe propose to augment the GNN message-passing operations with information\ndefined on ego graphs (i.e., the induced subgraph surrounding each node). We\nterm these approaches Ego-GNNs and show that Ego-GNNs are provably more\npowerful than standard message-passing GNNs. In particular, we show that\nEgo-GNNs are capable of recognizing closed triangles, which is essential given\nthe prominence of transitivity in real-world graphs. We also motivate our\napproach from the perspective of graph signal processing as a form of multiplex\ngraph convolution. Experimental results on node classification using synthetic\nand real data highlight the achievable performance gains using this approach.",
          "link": "http://arxiv.org/abs/2107.10957",
          "publishedOn": "2021-07-26T02:00:58.998Z",
          "wordCount": 608,
          "title": "Ego-GNNs: Exploiting Ego Structures in Graph Neural Networks. (arXiv:2107.10957v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10882",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karpov_K/0/1/0/all/0/1\">Kirill Karpov</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Mitrofanov_A/0/1/0/all/0/1\">Artem Mitrofanov</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Korolev_V/0/1/0/all/0/1\">Vadim Korolev</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Tkachenko_V/0/1/0/all/0/1\">Valery Tkachenko</a> (2) ((1) Lomonosov Moscow State University, Department of Chemistry, Leninskie gory, 1 bld. 3, Moscow, Russia, (2) Science Data Software, LLC, 14909 Forest Landing Cir, Rockville, USA)",
          "description": "The use of machine learning in chemistry has become a common practice. At the\nsame time, despite the success of modern machine learning methods, the lack of\ndata limits their use. Using a transfer learning methodology can help solve\nthis problem. This methodology assumes that a model built on a sufficient\namount of data captures general features of the chemical compound structure on\nwhich it was trained and that the further reuse of these features on a dataset\nwith a lack of data will greatly improve the quality of the new model. In this\npaper, we develop this approach for small organic molecules, implementing\ntransfer learning with graph convolutional neural networks. The paper shows a\nsignificant improvement in the performance of models for target properties with\na lack of data. The effects of the dataset composition on model quality and the\napplicability domain of the resulting models are also considered.",
          "link": "http://arxiv.org/abs/2107.10882",
          "publishedOn": "2021-07-26T02:00:58.979Z",
          "wordCount": 630,
          "title": "Size doesn't matter: predicting physico- or biochemical properties based on dozens of molecules. (arXiv:2107.10882v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10955",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lou_X/0/1/0/all/0/1\">Xingmei Lou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hu_Y/0/1/0/all/0/1\">Yu Hu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1\">Xiaodong Li</a>",
          "description": "We are interested in the problem of learning the directed acyclic graph (DAG)\nwhen data are generated from a linear structural equation model (SEM) and the\ncausal structure can be characterized by a polytree. Specially, under both\nGaussian and sub-Gaussian models, we study the sample size conditions for the\nwell-known Chow-Liu algorithm to exactly recover the equivalence class of the\npolytree, which is uniquely represented by a CPDAG. We also study the error\nrate for the estimation of the inverse correlation matrix under such models.\nOur theoretical findings are illustrated by comprehensive numerical\nsimulations, and experiments on benchmark data also demonstrate the robustness\nof the method when the ground truth graphical structure can only be\napproximated by a polytree.",
          "link": "http://arxiv.org/abs/2107.10955",
          "publishedOn": "2021-07-26T02:00:58.972Z",
          "wordCount": 571,
          "title": "Linear Polytree Structural Equation Models: Structural Learning and Inverse Correlation Estimation. (arXiv:2107.10955v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhmoginov_A/0/1/0/all/0/1\">Andrey Zhmoginov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bashkirova_D/0/1/0/all/0/1\">Dina Bashkirova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandler_M/0/1/0/all/0/1\">Mark Sandler</a>",
          "description": "Conditional computation and modular networks have been recently proposed for\nmultitask learning and other problems as a way to decompose problem solving\ninto multiple reusable computational blocks. We propose a new approach for\nlearning modular networks based on the isometric version of ResNet with all\nresidual blocks having the same configuration and the same number of\nparameters. This architectural choice allows adding, removing and changing the\norder of residual blocks. In our method, the modules can be invoked repeatedly\nand allow knowledge transfer to novel tasks by adjusting the order of\ncomputation. This allows soft weight sharing between tasks with only a small\nincrease in the number of parameters. We show that our method leads to\ninterpretable self-organization of modules in case of multi-task learning,\ntransfer learning and domain adaptation while achieving competitive results on\nthose tasks. From practical perspective, our approach allows to: (a) reuse\nexisting modules for learning new task by adjusting the computation order, (b)\nuse it for unsupervised multi-source domain adaptation to illustrate that\nadaptation to unseen data can be achieved by only manipulating the order of\npretrained modules, (c) show how our approach can be used to increase accuracy\nof existing architectures for image classification tasks such as ImageNet,\nwithout any parameter increase, by reusing the same block multiple times.",
          "link": "http://arxiv.org/abs/2107.10963",
          "publishedOn": "2021-07-26T02:00:58.962Z",
          "wordCount": 653,
          "title": "Compositional Models: Multi-Task Learning and Knowledge Transfer with Modular Networks. (arXiv:2107.10963v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Blanco_E/0/1/0/all/0/1\">Enrique Fernandez-Blanco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Lozano_C/0/1/0/all/0/1\">Carlos Fernandez-Lozano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pazos_A/0/1/0/all/0/1\">Alejandro Pazos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivero_D/0/1/0/all/0/1\">Daniel Rivero</a>",
          "description": "Over the years, several approaches have tried to tackle the problem of\nperforming an automatic scoring of the sleeping stages. Although any\npolysomnography usually collects over a dozen of different signals, this\nparticular problem has been mainly tackled by using only the\nElectroencephalograms presented in those records. On the other hand, the other\nrecorded signals have been mainly ignored by most works. This paper explores\nand compares the convenience of using additional signals apart from\nelectroencephalograms. More specifically, this work uses the SHHS-1 dataset\nwith 5,804 patients containing an electromyogram recorded simultaneously as two\nelectroencephalograms. To compare the results, first, the same architecture has\nbeen evaluated with different input signals and all their possible\ncombinations. These tests show how, using more than one signal especially if\nthey are from different sources, improves the results of the classification.\nAdditionally, the best models obtained for each combination of one or more\nsignals have been used in ensemble models and, its performance has been\ncompared showing the convenience of using these multi-signal models to improve\nthe classification. The best overall model, an ensemble of Depth-wise\nSeparational Convolutional Neural Networks, has achieved an accuracy of 86.06\\%\nwith a Cohen's Kappa of 0.80 and a $F_{1}$ of 0.77. Up to date, those are the\nbest results on the complete dataset and it shows a significant improvement in\nthe precision and recall for the most uncommon class in the dataset.",
          "link": "http://arxiv.org/abs/2107.11045",
          "publishedOn": "2021-07-26T02:00:58.948Z",
          "wordCount": 685,
          "title": "Ensemble of Convolution Neural Networks on Heterogeneous Signals for Sleep Stage Scoring. (arXiv:2107.11045v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhishek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_H/0/1/0/all/0/1\">Harikrishna Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotter_A/0/1/0/all/0/1\">Andrew Cotter</a>",
          "description": "We consider a popular family of constrained optimization problems arising in\nmachine learning that involve optimizing a non-decomposable evaluation metric\nwith a certain thresholded form, while constraining another metric of interest.\nExamples of such problems include optimizing the false negative rate at a fixed\nfalse positive rate, optimizing precision at a fixed recall, optimizing the\narea under the precision-recall or ROC curves, etc. Our key idea is to\nformulate a rate-constrained optimization that expresses the threshold\nparameter as a function of the model parameters via the Implicit Function\ntheorem. We show how the resulting optimization problem can be solved using\nstandard gradient based methods. Experiments on benchmark datasets demonstrate\nthe effectiveness of our proposed method over existing state-of-the art\napproaches for these problems.",
          "link": "http://arxiv.org/abs/2107.10960",
          "publishedOn": "2021-07-26T02:00:58.925Z",
          "wordCount": 555,
          "title": "Implicit Rate-Constrained Optimization of Non-decomposable Objectives. (arXiv:2107.10960v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koprinkova_Hristova_P/0/1/0/all/0/1\">Petia Koprinkova-Hristova</a>",
          "description": "The paper proposes a novel approach for gray scale images segmentation. It is\nbased on multiple features extraction from single feature per image pixel,\nnamely its intensity value, using Echo state network. The newly extracted\nfeatures -- reservoir equilibrium states -- reveal hidden image characteristics\nthat improve its segmentation via a clustering algorithm. Moreover, it was\ndemonstrated that the intrinsic plasticity tuning of reservoir fits its\nequilibrium states to the original image intensity distribution thus allowing\nfor its better segmentation. The proposed approach is tested on the benchmark\nimage Lena.",
          "link": "http://arxiv.org/abs/2107.11077",
          "publishedOn": "2021-07-26T02:00:58.918Z",
          "wordCount": 541,
          "title": "Reservoir Computing Approach for Gray Images Segmentation. (arXiv:2107.11077v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10901",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Moeinizade_S/0/1/0/all/0/1\">Saba Moeinizade</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hu_G/0/1/0/all/0/1\">Guiping Hu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_L/0/1/0/all/0/1\">Lizhi Wang</a>",
          "description": "Genomic selection (GS) is a technique that plant breeders use to select\nindividuals to mate and produce new generations of species. Allocation of\nresources is a key factor in GS. At each selection cycle, breeders are facing\nthe choice of budget allocation to make crosses and produce the next generation\nof breeding parents. Inspired by recent advances in reinforcement learning for\nAI problems, we develop a reinforcement learning-based algorithm to\nautomatically learn to allocate limited resources across different generations\nof breeding. We mathematically formulate the problem in the framework of Markov\nDecision Process (MDP) by defining state and action spaces. To avoid the\nexplosion of the state space, an integer linear program is proposed that\nquantifies the trade-off between resources and time. Finally, we propose a\nvalue function approximation method to estimate the action-value function and\nthen develop a greedy policy improvement technique to find the optimal\nresources. We demonstrate the effectiveness of the proposed method in enhancing\ngenetic gain using a case study with realistic data.",
          "link": "http://arxiv.org/abs/2107.10901",
          "publishedOn": "2021-07-26T02:00:58.907Z",
          "wordCount": 615,
          "title": "A reinforcement learning approach to resource allocation in genomic selection. (arXiv:2107.10901v1 [q-bio.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaebler_J/0/1/0/all/0/1\">Johann Demetrio Gaebler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Matt Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chunlin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yinyu Ye</a>",
          "description": "Value iteration is a well-known method of solving Markov Decision Processes\n(MDPs) that is simple to implement and boasts strong theoretical convergence\nguarantees. However, the computational cost of value iteration quickly becomes\ninfeasible as the size of the state space increases. Various methods have been\nproposed to overcome this issue for value iteration in large state and action\nspace MDPs, often at the price, however, of generalizability and algorithmic\nsimplicity. In this paper, we propose an intuitive algorithm for solving MDPs\nthat reduces the cost of value iteration updates by dynamically grouping\ntogether states with similar cost-to-go values. We also prove that our\nalgorithm converges almost surely to within \\(2\\varepsilon / (1 - \\gamma)\\) of\nthe true optimal value in the \\(\\ell^\\infty\\) norm, where \\(\\gamma\\) is the\ndiscount factor and aggregated states differ by at most \\(\\varepsilon\\).\nNumerical experiments on a variety of simulated environments confirm the\nrobustness of our algorithm and its ability to solve MDPs with much cheaper\nupdates especially as the scale of the MDP problem increases.",
          "link": "http://arxiv.org/abs/2107.11053",
          "publishedOn": "2021-07-26T02:00:58.899Z",
          "wordCount": 617,
          "title": "An Adaptive State Aggregation Algorithm for Markov Decision Processes. (arXiv:2107.11053v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asad_M/0/1/0/all/0/1\">Muhammad Asad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moustafa_A/0/1/0/all/0/1\">Ahmed Moustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ito_T/0/1/0/all/0/1\">Takayuki Ito</a>",
          "description": "In the past few decades, machine learning has revolutionized data processing\nfor large scale applications. Simultaneously, increasing privacy threats in\ntrending applications led to the redesign of classical data training models. In\nparticular, classical machine learning involves centralized data training,\nwhere the data is gathered, and the entire training process executes at the\ncentral server. Despite significant convergence, this training involves several\nprivacy threats on participants' data when shared with the central cloud\nserver. To this end, federated learning has achieved significant importance\nover distributed data training. In particular, the federated learning allows\nparticipants to collaboratively train the local models on local data without\nrevealing their sensitive information to the central cloud server. In this\npaper, we perform a convergence comparison between classical machine learning\nand federated learning on two publicly available datasets, namely,\nlogistic-regression-MNIST dataset and image-classification-CIFAR-10 dataset.\nThe simulation results demonstrate that federated learning achieves higher\nconvergence within limited communication rounds while maintaining participants'\nanonymity. We hope that this research will show the benefits and help federated\nlearning to be implemented widely.",
          "link": "http://arxiv.org/abs/2107.10976",
          "publishedOn": "2021-07-26T02:00:58.880Z",
          "wordCount": 609,
          "title": "Federated Learning Versus Classical Machine Learning: A Convergence Comparison. (arXiv:2107.10976v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Navarro_J/0/1/0/all/0/1\">Jose M. Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_D/0/1/0/all/0/1\">Dario Rossi</a>",
          "description": "This paper presents HURRA, a system that aims to reduce the time spent by\nhuman operators in the process of network troubleshooting. To do so, it\ncomprises two modules that are plugged after any anomaly detection algorithm:\n(i) a first attention mechanism, that ranks the present features in terms of\ntheir relation with the anomaly and (ii) a second module able to incorporates\nprevious expert knowledge seamlessly, without any need of human interaction nor\ndecisions. We show the efficacy of these simple processes on a collection of\nreal router datasets obtained from tens of ISPs which exhibit a rich variety of\nanomalies and very heterogeneous set of KPIs, on which we gather manually\nannotated ground truth by the operator solving the troubleshooting ticket. Our\nexperimental evaluation shows that (i) the proposed system is effective in\nachieving high levels of agreement with the expert, that (ii) even a simple\nstatistical approach is able to extracting useful information from expert\nknowledge gained in past cases to further improve performance and finally that\n(iii) the main difficulty in live deployment concerns the automated selection\nof the anomaly detection algorithm and the tuning of its hyper-parameters.",
          "link": "http://arxiv.org/abs/2107.11078",
          "publishedOn": "2021-07-26T02:00:58.873Z",
          "wordCount": 646,
          "title": "HURRA! Human readable router anomaly detection. (arXiv:2107.11078v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10879",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Peter Y. Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arino_J/0/1/0/all/0/1\">Joan Ari&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soljacic_M/0/1/0/all/0/1\">Marin Solja&#x10d;i&#x107;</a>",
          "description": "Identifying the governing equations of a nonlinear dynamical system is key to\nboth understanding the physical features of the system and constructing an\naccurate model of the dynamics that generalizes well beyond the available data.\nWe propose a machine learning framework for discovering these governing\nequations using only partial observations, combining an encoder for state\nreconstruction with a sparse symbolic model. Our tests show that this method\ncan successfully reconstruct the full system state and identify the underlying\ndynamics for a variety of ODE and PDE systems.",
          "link": "http://arxiv.org/abs/2107.10879",
          "publishedOn": "2021-07-26T02:00:58.864Z",
          "wordCount": 533,
          "title": "Discovering Sparse Interpretable Dynamics from Partial Observations. (arXiv:2107.10879v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Linghao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1\">Jinsong Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges EL Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "In this work, we propose a domain generalization (DG) approach to learn on\nseveral labeled source domains and transfer knowledge to a target domain that\nis inaccessible in training. Considering the inherent conditional and label\nshifts, we would expect the alignment of $p(x|y)$ and $p(y)$. However, the\nwidely used domain invariant feature learning (IFL) methods relies on aligning\nthe marginal concept shift w.r.t. $p(x)$, which rests on an unrealistic\nassumption that $p(y)$ is invariant across domains. We thereby propose a novel\nvariational Bayesian inference framework to enforce the conditional\ndistribution alignment w.r.t. $p(x|y)$ via the prior distribution matching in a\nlatent space, which also takes the marginal label shift w.r.t. $p(y)$ into\nconsideration with the posterior alignment. Extensive experiments on various\nbenchmarks demonstrate that our framework is robust to the label shift and the\ncross-domain accuracy is significantly improved, thereby achieving superior\nperformance over the conventional IFL counterparts.",
          "link": "http://arxiv.org/abs/2107.10931",
          "publishedOn": "2021-07-26T02:00:58.857Z",
          "wordCount": 615,
          "title": "Domain Generalization under Conditional and Label Shifts via Variational Bayesian Inference. (arXiv:2107.10931v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10884",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1\">Wu Lin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nielsen_F/0/1/0/all/0/1\">Frank Nielsen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Emtiyaz Khan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1\">Mark Schmidt</a>",
          "description": "In this paper, we propose new structured second-order methods and structured\nadaptive-gradient methods obtained by performing natural-gradient descent on\nstructured parameter spaces. Natural-gradient descent is an attractive approach\nto design new algorithms in many settings such as gradient-free,\nadaptive-gradient, and second-order methods. Our structured methods not only\nenjoy a structural invariance but also admit a simple expression. Finally, we\ntest the efficiency of our proposed methods on both deterministic non-convex\nproblems and deep learning problems.",
          "link": "http://arxiv.org/abs/2107.10884",
          "publishedOn": "2021-07-26T02:00:58.849Z",
          "wordCount": 519,
          "title": "Structured second-order methods via natural gradient descent. (arXiv:2107.10884v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lou_T/0/1/0/all/0/1\">Tim Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1\">Michael Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramezanali_M/0/1/0/all/0/1\">Mohammad Ramezanali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_V/0/1/0/all/0/1\">Vincent Tang</a>",
          "description": "In this note we examine the autoregressive generalization of the FNet\nalgorithm, in which self-attention layers from the standard Transformer\narchitecture are substituted with a trivial sparse-uniformsampling procedure\nbased on Fourier transforms. Using the Wikitext-103 benchmark, we\ndemonstratethat FNetAR retains state-of-the-art performance (25.8 ppl) on the\ntask of causal language modelingcompared to a Transformer-XL baseline (24.2\nppl) with only half the number self-attention layers,thus providing further\nevidence for the superfluity of deep neural networks with heavily\ncompoundedattention mechanisms. The autoregressive Fourier transform could\nlikely be used for parameterreduction on most Transformer-based time-series\nprediction models.",
          "link": "http://arxiv.org/abs/2107.10932",
          "publishedOn": "2021-07-26T02:00:58.842Z",
          "wordCount": 532,
          "title": "FNetAR: Mixing Tokens with Autoregressive Fourier Transforms. (arXiv:2107.10932v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stray_J/0/1/0/all/0/1\">Jonathan Stray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vendrov_I/0/1/0/all/0/1\">Ivan Vendrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nixon_J/0/1/0/all/0/1\">Jeremy Nixon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_S/0/1/0/all/0/1\">Steven Adler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1\">Dylan Hadfield-Menell</a>",
          "description": "We describe cases where real recommender systems were modified in the service\nof various human values such as diversity, fairness, well-being, time well\nspent, and factual accuracy. From this we identify the current practice of\nvalues engineering: the creation of classifiers from human-created data with\nvalue-based labels. This has worked in practice for a variety of issues, but\nproblems are addressed one at a time, and users and other stakeholders have\nseldom been involved. Instead, we look to AI alignment work for approaches that\ncould learn complex values directly from stakeholders, and identify four major\ndirections: useful measures of alignment, participatory design and operation,\ninteractive value learning, and informed deliberative judgments.",
          "link": "http://arxiv.org/abs/2107.10939",
          "publishedOn": "2021-07-26T02:00:58.821Z",
          "wordCount": 571,
          "title": "What are you optimizing for? Aligning Recommender Systems with Human Values. (arXiv:2107.10939v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuolin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaojun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Recent studies show that deep neural networks (DNN) are vulnerable to\nadversarial examples, which aim to mislead DNNs by adding perturbations with\nsmall magnitude. To defend against such attacks, both empirical and theoretical\ndefense approaches have been extensively studied for a single ML model. In this\nwork, we aim to analyze and provide the certified robustness for ensemble ML\nmodels, together with the sufficient and necessary conditions of robustness for\ndifferent ensemble protocols. Although ensemble models are shown more robust\nthan a single model empirically; surprisingly, we find that in terms of the\ncertified robustness the standard ensemble models only achieve marginal\nimprovement compared to a single model. Thus, to explore the conditions that\nguarantee to provide certifiably robust ensemble ML models, we first prove that\ndiversified gradient and large confidence margin are sufficient and necessary\nconditions for certifiably robust ensemble models under the model-smoothness\nassumption. We then provide the bounded model-smoothness analysis based on the\nproposed Ensemble-before-Smoothing strategy. We also prove that an ensemble\nmodel can always achieve higher certified robustness than a single base model\nunder mild conditions. Inspired by the theoretical findings, we propose the\nlightweight Diversity Regularized Training (DRT) to train certifiably robust\nensemble ML models. Extensive experiments show that our DRT enhanced ensembles\ncan consistently achieve higher certified robustness than existing single and\nensemble ML models, demonstrating the state-of-the-art certified L2-robustness\non MNIST, CIFAR-10, and ImageNet datasets.",
          "link": "http://arxiv.org/abs/2107.10873",
          "publishedOn": "2021-07-26T02:00:58.781Z",
          "wordCount": 691,
          "title": "On the Certified Robustness for Ensemble Models and Beyond. (arXiv:2107.10873v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bun_M/0/1/0/all/0/1\">Mark Bun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1\">Marco Gaboardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_S/0/1/0/all/0/1\">Satchit Sivakumar</a>",
          "description": "We show a generic reduction from multiclass differentially private PAC\nlearning to binary private PAC learning. We apply this transformation to a\nrecently proposed binary private PAC learner to obtain a private multiclass\nlearner with sample complexity that has a polynomial dependence on the\nmulticlass Littlestone dimension and a poly-logarithmic dependence on the\nnumber of classes. This yields an exponential improvement in the dependence on\nboth parameters over learners from previous work. Our proof extends the notion\nof $\\Psi$-dimension defined in work of Ben-David et al. [JCSS '95] to the\nonline setting and explores its general properties.",
          "link": "http://arxiv.org/abs/2107.10870",
          "publishedOn": "2021-07-26T02:00:58.687Z",
          "wordCount": 528,
          "title": "Multiclass versus Binary Differentially Private PAC Learning. (arXiv:2107.10870v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sashidhar_D/0/1/0/all/0/1\">Diya Sashidhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutz_J/0/1/0/all/0/1\">J. Nathan Kutz</a>",
          "description": "Dynamic mode decomposition (DMD) provides a regression framework for\nadaptively learning a best-fit linear dynamics model over snapshots of\ntemporal, or spatio-temporal, data. A diversity of regression techniques have\nbeen developed for producing the linear model approximation whose solutions are\nexponentials in time. For spatio-temporal data, DMD provides low-rank and\ninterpretable models in the form of dominant modal structures along with their\nexponential/oscillatory behavior in time. The majority of DMD algorithms,\nhowever, are prone to bias errors from noisy measurements of the dynamics,\nleading to poor model fits and unstable forecasting capabilities. The optimized\nDMD algorithm minimizes the model bias with a variable projection optimization,\nthus leading to stabilized forecasting capabilities. Here, the optimized DMD\nalgorithm is improved by using statistical bagging methods whereby a single set\nof snapshots is used to produce an ensemble of optimized DMD models. The\noutputs of these models are averaged to produce a bagging, optimized dynamic\nmode decomposition (BOP-DMD). BOP-DMD not only improves performance, it also\nrobustifies the model and provides both spatial and temporal uncertainty\nquantification (UQ). Thus unlike currently available DMD algorithms, BOP-DMD\nprovides a stable and robust model for probabilistic, or Bayesian forecasting\nwith comprehensive UQ metrics.",
          "link": "http://arxiv.org/abs/2107.10878",
          "publishedOn": "2021-07-26T02:00:58.664Z",
          "wordCount": 655,
          "title": "Bagging, optimized dynamic mode decomposition (BOP-DMD) for robust, stable forecasting with spatial and temporal uncertainty-quantification. (arXiv:2107.10878v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Strawn_N/0/1/0/all/0/1\">Nate Strawn</a>",
          "description": "We construct a computationally inexpensive 3D extension of Andrew's plots by\nconsidering curves generated by Frenet-Serret equations and induced by\noptimally smooth 2D Andrew's plots. We consider linear isometries from a\nEuclidean data space to infinite dimensional spaces of 2D curves, and\nparametrize the linear isometries that produce (on average) optimally smooth\ncurves over a given dataset. This set of optimal isometries admits many degrees\nof freedom, and (using recent results on generalized Gauss sums) we identify a\nparticular a member of this set which admits an asymptotic projective \"tour\"\nproperty. Finally, we consider the unit-length 3D curves (filaments) induced by\nthese 2D Andrew's plots, where the linear isometry property preserves distances\nas \"relative total square curvatures\". This work concludes by illustrating\nfilament plots for several datasets. Code is available at\nhttps://github.com/n8epi/filaments",
          "link": "http://arxiv.org/abs/2107.10869",
          "publishedOn": "2021-07-26T02:00:58.537Z",
          "wordCount": 565,
          "title": "Filament Plots for Data Visualization. (arXiv:2107.10869v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuyang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_M/0/1/0/all/0/1\">Mehrdad Mahdavi</a>",
          "description": "In this paper we prove that Local (S)GD (or FedAvg) can optimize two-layer\nneural networks with Rectified Linear Unit (ReLU) activation function in\npolynomial time. Despite the established convergence theory of Local SGD on\noptimizing general smooth functions in communication-efficient distributed\noptimization, its convergence on non-smooth ReLU networks still eludes full\ntheoretical understanding. The key property used in many Local SGD analysis on\nsmooth function is gradient Lipschitzness, so that the gradient on local models\nwill not drift far away from that on averaged model. However, this decent\nproperty does not hold in networks with non-smooth ReLU activation function. We\nshow that, even though ReLU network does not admit gradient Lipschitzness\nproperty, the difference between gradients on local models and average model\nwill not change too much, under the dynamics of Local SGD. We validate our\ntheoretical results via extensive experiments. This work is the first to show\nthe convergence of Local SGD on non-smooth functions, and will shed lights on\nthe optimization theory of federated training of deep neural networks.",
          "link": "http://arxiv.org/abs/2107.10868",
          "publishedOn": "2021-07-26T02:00:58.529Z",
          "wordCount": 604,
          "title": "Local SGD Optimizes Overparameterized Neural Networks in Polynomial Time. (arXiv:2107.10868v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.09236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mills_J/0/1/0/all/0/1\">Jed Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jia Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_G/0/1/0/all/0/1\">Geyong Min</a>",
          "description": "Federated Learning (FL) is an emerging approach for collaboratively training\nDeep Neural Networks (DNNs) on mobile devices, without private user data\nleaving the devices. Previous works have shown that non-Independent and\nIdentically Distributed (non-IID) user data harms the convergence speed of the\nFL algorithms. Furthermore, most existing work on FL measures global-model\naccuracy, but in many cases, such as user content-recommendation, improving\nindividual User model Accuracy (UA) is the real objective. To address these\nissues, we propose a Multi-Task FL (MTFL) algorithm that introduces\nnon-federated Batch-Normalization (BN) layers into the federated DNN. MTFL\nbenefits UA and convergence speed by allowing users to train models\npersonalised to their own data. MTFL is compatible with popular iterative FL\noptimisation algorithms such as Federated Averaging (FedAvg), and we show\nempirically that a distributed form of Adam optimisation (FedAvg-Adam) benefits\nconvergence speed even further when used as the optimisation strategy within\nMTFL. Experiments using MNIST and CIFAR10 demonstrate that MTFL is able to\nsignificantly reduce the number of rounds required to reach a target UA, by up\nto $5\\times$ when using existing FL optimisation strategies, and with a further\n$3\\times$ improvement when using FedAvg-Adam. We compare MTFL to competing\npersonalised FL algorithms, showing that it is able to achieve the best UA for\nMNIST and CIFAR10 in all considered scenarios. Finally, we evaluate MTFL with\nFedAvg-Adam on an edge-computing testbed, showing that its convergence and UA\nbenefits outweigh its overhead.",
          "link": "http://arxiv.org/abs/2007.09236",
          "publishedOn": "2021-07-23T02:00:33.031Z",
          "wordCount": 710,
          "title": "Multi-Task Federated Learning for Personalised Deep Neural Networks in Edge Computing. (arXiv:2007.09236v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petangoda_J/0/1/0/all/0/1\">Janith Petangoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deisenroth_M/0/1/0/all/0/1\">Marc Peter Deisenroth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monk_N/0/1/0/all/0/1\">Nicholas A. M. Monk</a>",
          "description": "Learning to transfer considers learning solutions to tasks in a such way that\nrelevant knowledge can be transferred from known task solutions to new, related\ntasks. This is important for general learning, as well as for improving the\nefficiency of the learning process. While techniques for learning to transfer\nhave been studied experimentally, we still lack a foundational description of\nthe problem that exposes what related tasks are, and how relationships between\ntasks can be exploited constructively. In this work, we introduce a framework\nusing the differential geometric theory of foliations that provides such a\nfoundation.",
          "link": "http://arxiv.org/abs/2107.10763",
          "publishedOn": "2021-07-23T02:00:33.025Z",
          "wordCount": 522,
          "title": "Learning to Transfer: A Foliated Theory. (arXiv:2107.10763v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.11330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krokos_V/0/1/0/all/0/1\">Vasilis Krokos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_V/0/1/0/all/0/1\">Viet Bui Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bordas_S/0/1/0/all/0/1\">St&#xe9;phane P. A. Bordas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_P/0/1/0/all/0/1\">Philippe Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerfriden_P/0/1/0/all/0/1\">Pierre Kerfriden</a>",
          "description": "Multiscale computational modelling is challenging due to the high\ncomputational cost of direct numerical simulation by finite elements. To\naddress this issue, concurrent multiscale methods use the solution of cheaper\nmacroscale surrogates as boundary conditions to microscale sliding windows. The\nmicroscale problems remain a numerically challenging operation both in terms of\nimplementation and cost. In this work we propose to replace the local\nmicroscale solution by an Encoder-Decoder Convolutional Neural Network that\nwill generate fine-scale stress corrections to coarse predictions around\nunresolved microscale features, without prior parametrisation of local\nmicroscale problems. We deploy a Bayesian approach providing credible intervals\nto evaluate the uncertainty of the predictions, which is then used to\ninvestigate the merits of a selective learning framework. We will demonstrate\nthe capability of the approach to predict equivalent stress fields in porous\nstructures using linearised and finite strain elasticity theories.",
          "link": "http://arxiv.org/abs/2012.11330",
          "publishedOn": "2021-07-23T02:00:33.006Z",
          "wordCount": 654,
          "title": "A Bayesian multiscale CNN framework to predict local stress fields in structures with microscale features. (arXiv:2012.11330v3 [cs.CE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10711",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Eivazi_H/0/1/0/all/0/1\">Hamidreza Eivazi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tahani_M/0/1/0/all/0/1\">Mojtaba Tahani</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schlatter_P/0/1/0/all/0/1\">Philipp Schlatter</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Vinuesa_R/0/1/0/all/0/1\">Ricardo Vinuesa</a>",
          "description": "Physics-informed neural networks (PINNs) are successful machine-learning\nmethods for the solution and identification of partial differential equations\n(PDEs). We employ PINNs for solving the Reynolds-averaged\nNavier$\\unicode{x2013}$Stokes (RANS) equations for incompressible turbulent\nflows without any specific model or assumption for turbulence, and by taking\nonly the data on the domain boundaries. We first show the applicability of\nPINNs for solving the Navier$\\unicode{x2013}$Stokes equations for laminar flows\nby solving the Falkner$\\unicode{x2013}$Skan boundary layer. We then apply PINNs\nfor the simulation of four turbulent-flow cases, i.e., zero-pressure-gradient\nboundary layer, adverse-pressure-gradient boundary layer, and turbulent flows\nover a NACA4412 airfoil and the periodic hill. Our results show the excellent\napplicability of PINNs for laminar flows with strong pressure gradients, where\npredictions with less than 1% error can be obtained. For turbulent flows, we\nalso obtain very good accuracy on simulation results even for the\nReynolds-stress components.",
          "link": "http://arxiv.org/abs/2107.10711",
          "publishedOn": "2021-07-23T02:00:32.977Z",
          "wordCount": 597,
          "title": "Physics-informed neural networks for solving Reynolds-averaged Navier$\\unicode{x2013}$Stokes equations. (arXiv:2107.10711v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_E/0/1/0/all/0/1\">Eduardo Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_A/0/1/0/all/0/1\">Andres Ferraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_X/0/1/0/all/0/1\">Xavier Serra</a>",
          "description": "Recent studies have put into question the commonly assumed shift invariance\nproperty of convolutional networks, showing that small shifts in the input can\naffect the output predictions substantially. In this paper, we analyze the\nbenefits of addressing lack of shift invariance in CNN-based sound event\nclassification. Specifically, we evaluate two pooling methods to improve shift\ninvariance in CNNs, based on low-pass filtering and adaptive sampling of\nincoming feature maps. These methods are implemented via small architectural\nmodifications inserted into the pooling layers of CNNs. We evaluate the effect\nof these architectural changes on the FSD50K dataset using models of different\ncapacity and in presence of strong regularization. We show that these\nmodifications consistently improve sound event classification in all cases\nconsidered. We also demonstrate empirically that the proposed pooling methods\nincrease shift invariance in the network, making it more robust against\ntime/frequency shifts in input spectrograms. This is achieved by adding a\nnegligible amount of trainable parameters, which makes these methods an\nappealing alternative to conventional pooling layers. The outcome is a new\nstate-of-the-art mAP of 0.541 on the FSD50K classification benchmark.",
          "link": "http://arxiv.org/abs/2107.00623",
          "publishedOn": "2021-07-23T02:00:32.966Z",
          "wordCount": 643,
          "title": "Improving Sound Event Classification by Increasing Shift Invariance in Convolutional Neural Networks. (arXiv:2107.00623v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Libo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Browning_J/0/1/0/all/0/1\">James Browning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_R/0/1/0/all/0/1\">Roberto Perera</a>",
          "description": "The Light-Up puzzle, also known as the AKARI puzzle, has never been solved\nusing modern artificial intelligence (AI) methods. Currently, the most widely\nused computational technique to autonomously develop solutions involve\nevolution theory algorithms. This project is an effort to apply new AI\ntechniques for solving the Light-up puzzle faster and more computationally\nefficient. The algorithms explored for producing optimal solutions include hill\nclimbing, simulated annealing, feed-forward neural network (FNN), and\nconvolutional neural network (CNN). Two algorithms were developed for hill\nclimbing and simulated annealing using 2 actions (add and remove light bulb)\nversus 3 actions(add, remove, or move light-bulb to a different cell). Both\nhill climbing and simulated annealing algorithms showed a higher accuracy for\nthe case of 3 actions. The simulated annealing showed to significantly\noutperform hill climbing, FNN, CNN, and an evolutionary theory algorithm\nachieving 100% accuracy in 30 unique board configurations. Lastly, while FNN\nand CNN algorithms showed low accuracies, computational times were\nsignificantly faster compared to the remaining algorithms. The GitHub\nrepository for this project can be found at\nhttps://github.com/rperera12/AKARI-LightUp-GameSolver-with-DeepNeuralNetworks-and-HillClimb-or-SimulatedAnnealing.",
          "link": "http://arxiv.org/abs/2107.10429",
          "publishedOn": "2021-07-23T02:00:32.959Z",
          "wordCount": 628,
          "title": "Shedding some light on Light Up with Artificial Intelligence. (arXiv:2107.10429v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2007.03481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_V/0/1/0/all/0/1\">Vikram Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pattanayak_K/0/1/0/all/0/1\">Kunal Pattanayak</a>",
          "description": "This paper presents an inverse reinforcement learning (IRL) framework for\nBayesian stopping time problems. By observing the actions of a Bayesian\ndecision maker, we provide a necessary and sufficient condition to identify if\nthese actions are consistent with optimizing a cost function; then we construct\nset valued estimates of the cost function. To achieve this IRL objective, we\nuse novel ideas from Bayesian revealed preferences stemming from\nmicroeconomics. To illustrate our IRL scheme,we consider two important examples\nof stopping time problems, namely, sequential hypothesis testing and Bayesian\nsearch. Finally, for finite datasets, we propose an IRL detection algorithm and\ngive finite sample bounds on its error probabilities. Also we discuss how to\nidentify $\\epsilon$-optimal Bayesian decision makers and perform IRL.",
          "link": "http://arxiv.org/abs/2007.03481",
          "publishedOn": "2021-07-23T02:00:32.940Z",
          "wordCount": 608,
          "title": "Necessary and Sufficient Conditions for Inverse Reinforcement Learning of Bayesian Stopping Time Problems. (arXiv:2007.03481v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10654",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fahrbach_M/0/1/0/all/0/1\">Matthew Fahrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadiri_M/0/1/0/all/0/1\">Mehrdad Ghadiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Thomas Fu</a>",
          "description": "Low-rank tensor decomposition generalizes low-rank matrix approximation and\nis a powerful technique for discovering low-dimensional structure in\nhigh-dimensional data. In this paper, we study Tucker decompositions and use\ntools from randomized numerical linear algebra called ridge leverage scores to\naccelerate the core tensor update step in the widely-used alternating least\nsquares (ALS) algorithm. Updating the core tensor, a severe bottleneck in ALS,\nis a highly-structured ridge regression problem where the design matrix is a\nKronecker product of the factor matrices. We show how to use approximate ridge\nleverage scores to construct a sketched instance for any ridge regression\nproblem such that the solution vector for the sketched problem is a\n$(1+\\varepsilon)$-approximation to the original instance. Moreover, we show\nthat classical leverage scores suffice as an approximation, which then allows\nus to exploit the Kronecker structure and update the core tensor in time that\ndepends predominantly on the rank and the sketching parameters (i.e., sublinear\nin the size of the input tensor). We also give upper bounds for ridge leverage\nscores as rows are removed from the design matrix (e.g., if the tensor has\nmissing entries), and we demonstrate the effectiveness of our approximate ridge\nregressioni algorithm for large, low-rank Tucker decompositions on both\nsynthetic and real-world data.",
          "link": "http://arxiv.org/abs/2107.10654",
          "publishedOn": "2021-07-23T02:00:32.931Z",
          "wordCount": 657,
          "title": "Fast Low-Rank Tensor Decomposition by Ridge Leverage Score Sampling. (arXiv:2107.10654v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2007.00077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Coleman_C/0/1/0/all/0/1\">Cody Coleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_E/0/1/0/all/0/1\">Edward Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Samuels_J/0/1/0/all/0/1\">Julian Katz-Samuels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Culatana_S/0/1/0/all/0/1\">Sean Culatana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailis_P/0/1/0/all/0/1\">Peter Bailis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1\">Alexander C. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1\">Robert Nowak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumbaly_R/0/1/0/all/0/1\">Roshan Sumbaly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yalniz_I/0/1/0/all/0/1\">I. Zeki Yalniz</a>",
          "description": "Many active learning and search approaches are intractable for large-scale\nindustrial settings with billions of unlabeled examples. Existing approaches\nsearch globally for the optimal examples to label, scaling linearly or even\nquadratically with the unlabeled data. In this paper, we improve the\ncomputational efficiency of active learning and search methods by restricting\nthe candidate pool for labeling to the nearest neighbors of the currently\nlabeled set instead of scanning over all of the unlabeled data. We evaluate\nseveral selection strategies in this setting on three large-scale computer\nvision datasets: ImageNet, OpenImages, and a de-identified and aggregated\ndataset of 10 billion images provided by a large internet company. Our approach\nachieved similar mean average precision and recall as the traditional global\napproach while reducing the computational cost of selection by up to three\norders of magnitude, thus enabling web-scale active learning.",
          "link": "http://arxiv.org/abs/2007.00077",
          "publishedOn": "2021-07-23T02:00:32.913Z",
          "wordCount": 638,
          "title": "Similarity Search for Efficient Active Learning and Search of Rare Concepts. (arXiv:2007.00077v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sihyun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Sangwoo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungsoo Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Abstract reasoning, i.e., inferring complicated patterns from given\nobservations, is a central building block of artificial general intelligence.\nWhile humans find the answer by either eliminating wrong candidates or first\nconstructing the answer, prior deep neural network (DNN)-based methods focus on\nthe former discriminative approach. This paper aims to design a framework for\nthe latter approach and bridge the gap between artificial and human\nintelligence. To this end, we propose logic-guided generation (LoGe), a novel\ngenerative DNN framework that reduces abstract reasoning as an optimization\nproblem in propositional logic. LoGe is composed of three steps: extract\npropositional variables from images, reason the answer variables with a logic\nlayer, and reconstruct the answer image from the variables. We demonstrate that\nLoGe outperforms the black box DNN frameworks for generative abstract reasoning\nunder the RAVEN benchmark, i.e., reconstructing answers based on capturing\ncorrect rules of various attributes from observations.",
          "link": "http://arxiv.org/abs/2107.10493",
          "publishedOn": "2021-07-23T02:00:32.905Z",
          "wordCount": 601,
          "title": "Abstract Reasoning via Logic-guided Generation. (arXiv:2107.10493v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10670",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Li_S/0/1/0/all/0/1\">Shuangli Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhou_J/0/1/0/all/0/1\">Jingbo Zhou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Huang_L/0/1/0/all/0/1\">Liang Huang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Huang_W/0/1/0/all/0/1\">Weili Huang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>",
          "description": "Drug discovery often relies on the successful prediction of protein-ligand\nbinding affinity. Recent advances have shown great promise in applying graph\nneural networks (GNNs) for better affinity prediction by learning the\nrepresentations of protein-ligand complexes. However, existing solutions\nusually treat protein-ligand complexes as topological graph data, thus the\nbiomolecular structural information is not fully utilized. The essential\nlong-range interactions among atoms are also neglected in GNN models. To this\nend, we propose a structure-aware interactive graph neural network (SIGN) which\nconsists of two components: polar-inspired graph attention layers (PGAL) and\npairwise interactive pooling (PiPool). Specifically, PGAL iteratively performs\nthe node-edge aggregation process to update embeddings of nodes and edges while\npreserving the distance and angle information among atoms. Then, PiPool is\nadopted to gather interactive edges with a subsequent reconstruction loss to\nreflect the global interactions. Exhaustive experimental study on two\nbenchmarks verifies the superiority of SIGN.",
          "link": "http://arxiv.org/abs/2107.10670",
          "publishedOn": "2021-07-23T02:00:32.899Z",
          "wordCount": 614,
          "title": "Structure-aware Interactive Graph Neural Networks for the Prediction of Protein-Ligand Binding Affinity. (arXiv:2107.10670v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2102.02079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_Y/0/1/0/all/0/1\">Yiqun Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Quan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bingsheng He</a>",
          "description": "Due to the increasing privacy concerns and data regulations, training data\nhave been increasingly fragmented, forming distributed databases of multiple\n``data silos'' (e.g., within different organizations and countries). To develop\neffective machine learning services, there is a must to exploit data from such\ndistributed databases without exchanging the raw data. Recently, federated\nlearning (FL) has been a solution with growing interests, which enables\nmultiple parties to collaboratively train a machine learning model without\nexchanging their local data. A key and common challenge on distributed\ndatabases is the heterogeneity of the data distribution (i.e., non-IID) among\nthe parties. There have been many FL algorithms to address the learning\neffectiveness under non-IID data settings. However, there lacks an experimental\nstudy on systematically understanding their advantages and disadvantages, as\nprevious studies have very rigid data partitioning strategies among parties,\nwhich are hardly representative and thorough. In this paper, to help\nresearchers better understand and study the non-IID data setting in federated\nlearning, we propose comprehensive data partitioning strategies to cover the\ntypical non-IID data cases. Moreover, we conduct extensive experiments to\nevaluate state-of-the-art FL algorithms. We find that non-IID does bring\nsignificant challenges in learning accuracy of FL algorithms, and none of the\nexisting state-of-the-art FL algorithms outperforms others in all cases. Our\nexperiments provide insights for future studies of addressing the challenges in\n``data silos''.",
          "link": "http://arxiv.org/abs/2102.02079",
          "publishedOn": "2021-07-23T02:00:32.881Z",
          "wordCount": 699,
          "title": "Federated Learning on Non-IID Data Silos: An Experimental Study. (arXiv:2102.02079v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1805.05510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1\">Jing Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "Metric learning especially deep metric learning has been widely developed for\nlarge-scale image inputs data. However, in many real-world applications, we can\nonly have access to vectorized inputs data. Moreover, on one hand, well-labeled\ndata is usually limited due to the high annotation cost. On the other hand, the\nreal data is commonly streaming data, which requires to be processed online. In\nthese scenarios, the fashionable deep metric learning is not suitable anymore.\nTo this end, we reconsider the traditional shallow online metric learning and\nnewly develop an online progressive deep metric learning (ODML) framework to\nconstruct a metric-algorithm-based deep network. Specifically, we take an\nonline metric learning algorithm as a metric-algorithm-based layer (i.e.,\nmetric layer), followed by a nonlinear layer, and then stack these layers in a\nfashion similar to deep learning. Different from the shallow online metric\nlearning, which can only learn one metric space (feature transformation), the\nproposed ODML is able to learn multiple hierarchical metric spaces.\nFurthermore, in a progressively and nonlinearly learning way, ODML has a\nstronger learning ability than traditional shallow online metric learning in\nthe case of limited available training data. To make the learning process more\nexplainable and theoretically guaranteed, we also provide theoretical analysis.\nThe proposed ODML enjoys several nice properties and can indeed learn a metric\nprogressively and performs better on the benchmark datasets. Extensive\nexperiments with different settings have been conducted to verify these\nproperties of the proposed ODML.",
          "link": "http://arxiv.org/abs/1805.05510",
          "publishedOn": "2021-07-23T02:00:32.874Z",
          "wordCount": 713,
          "title": "Online Progressive Deep Metric Learning. (arXiv:1805.05510v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghao Aaron Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zare_A/0/1/0/all/0/1\">Ali Zare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesgarani_N/0/1/0/all/0/1\">Nima Mesgarani</a>",
          "description": "We present an unsupervised non-parallel many-to-many voice conversion (VC)\nmethod using a generative adversarial network (GAN) called StarGAN v2. Using a\ncombination of adversarial source classifier loss and perceptual loss, our\nmodel significantly outperforms previous VC models. Although our model is\ntrained only with 20 English speakers, it generalizes to a variety of voice\nconversion tasks, such as any-to-many, cross-lingual, and singing conversion.\nUsing a style encoder, our framework can also convert plain reading speech into\nstylistic speech, such as emotional and falsetto speech. Subjective and\nobjective evaluation experiments on a non-parallel many-to-many voice\nconversion task revealed that our model produces natural sounding voices, close\nto the sound quality of state-of-the-art text-to-speech (TTS) based voice\nconversion methods without the need for text labels. Moreover, our model is\ncompletely convolutional and with a faster-than-real-time vocoder such as\nParallel WaveGAN can perform real-time voice conversion.",
          "link": "http://arxiv.org/abs/2107.10394",
          "publishedOn": "2021-07-23T02:00:32.866Z",
          "wordCount": 592,
          "title": "StarGANv2-VC: A Diverse, Unsupervised, Non-parallel Framework for Natural-Sounding Voice Conversion. (arXiv:2107.10394v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10706",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Beznosikov_A/0/1/0/all/0/1\">Aleksandr Beznosikov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Scutari_G/0/1/0/all/0/1\">Gesualdo Scutari</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rogozin_A/0/1/0/all/0/1\">Alexander Rogozin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gasnikov_A/0/1/0/all/0/1\">Alexander Gasnikov</a>",
          "description": "We study solution methods for (strongly-)convex-(strongly)-concave\nSaddle-Point Problems (SPPs) over networks of two type - master/workers (thus\ncentralized) architectures and meshed (thus decentralized) networks. The local\nfunctions at each node are assumed to be similar, due to statistical data\nsimilarity or otherwise. We establish lower complexity bounds for a fairly\ngeneral class of algorithms solving the SPP. We show that a given suboptimality\n$\\epsilon>0$ is achieved over master/workers networks in\n$\\Omega\\big(\\Delta\\cdot \\delta/\\mu\\cdot \\log (1/\\varepsilon)\\big)$ rounds of\ncommunications, where $\\delta>0$ measures the degree of similarity of the local\nfunctions, $\\mu$ is their strong convexity constant, and $\\Delta$ is the\ndiameter of the network. The lower communication complexity bound over meshed\nnetworks reads $\\Omega\\big(1/{\\sqrt{\\rho}} \\cdot {\\delta}/{\\mu}\\cdot\\log\n(1/\\varepsilon)\\big)$, where $\\rho$ is the (normalized) eigengap of the gossip\nmatrix used for the communication between neighbouring nodes. We then propose\nalgorithms matching the lower bounds over either types of networks (up to\nlog-factors). We assess the effectiveness of the proposed algorithms on a\nrobust logistic regression problem.",
          "link": "http://arxiv.org/abs/2107.10706",
          "publishedOn": "2021-07-23T02:00:32.859Z",
          "wordCount": 599,
          "title": "Distributed Saddle-Point Problems Under Similarity. (arXiv:2107.10706v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10567",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lee_K/0/1/0/all/0/1\">Kyu-Beom Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shin_H/0/1/0/all/0/1\">Hyu-Soung Shin</a>",
          "description": "Tunnel CCTVs are installed to low height and long-distance interval. However,\nbecause of the limitation of installation height, severe perspective effect in\ndistance occurs, and it is almost impossible to detect vehicles in far distance\nfrom the CCTV in the existing tunnel CCTV-based accident detection system\n(Pflugfelder 2005). To overcome the limitation, a vehicle object is detected\nthrough an object detection algorithm based on an inverse perspective transform\nby re-setting the region of interest (ROI). It can detect vehicles that are far\naway from the CCTV. To verify this process, this paper creates each dataset\nconsisting of images and bounding boxes based on the original and warped images\nof the CCTV at the same time, and then compares performance of the deep\nlearning object detection models trained with the two datasets. As a result,\nthe model that trained the warped image was able to detect vehicle objects more\naccurately at the position far from the CCTV compared to the model that trained\nthe original image.",
          "link": "http://arxiv.org/abs/2107.10567",
          "publishedOn": "2021-07-23T02:00:32.853Z",
          "wordCount": 624,
          "title": "An overcome of far-distance limitation on tunnel CCTV-based accident detection in AI deep-learning frameworks. (arXiv:2107.10567v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2006.05066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1\">Woochul Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyeon Kim</a>",
          "description": "Modern convolutional neural networks (CNNs) have massive identical\nconvolution blocks, and, hence, recursive sharing of parameters across these\nblocks has been proposed to reduce the amount of parameters. However, naive\nsharing of parameters poses many challenges such as limited representational\npower and the vanishing/exploding gradients problem of recursively shared\nparameters. In this paper, we present a recursive convolution block design and\ntraining method, in which a recursively shareable part, or a filter basis, is\nseparated and learned while effectively avoiding the vanishing/exploding\ngradients problem during training. We show that the unwieldy\nvanishing/exploding gradients problem can be controlled by enforcing the\nelements of the filter basis orthonormal, and empirically demonstrate that the\nproposed orthogonality regularization improves the flow of gradients during\ntraining. Experimental results on image classification and object detection\nshow that our approach, unlike previous parameter-sharing approaches, does not\ntrade performance to save parameters and consistently outperforms\noverparameterized counterpart networks. This superior performance demonstrates\nthat the proposed recursive convolution block design and the orthogonality\nregularization not only prevent performance degradation, but also consistently\nimprove the representation capability while a significant amount of parameters\nare recursively shared.",
          "link": "http://arxiv.org/abs/2006.05066",
          "publishedOn": "2021-07-23T02:00:32.848Z",
          "wordCount": 663,
          "title": "Deeply Shared Filter Bases for Parameter-Efficient Convolutional Neural Networks. (arXiv:2006.05066v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.03040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianfeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zuowei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shijun Zhang</a>",
          "description": "This paper establishes the optimal approximation error characterization of\ndeep ReLU networks for smooth functions in terms of both width and depth\nsimultaneously. To that end, we first prove that multivariate polynomials can\nbe approximated by deep ReLU networks of width $\\mathcal{O}(N)$ and depth\n$\\mathcal{O}(L)$ with an approximation error $\\mathcal{O}(N^{-L})$. Through\nlocal Taylor expansions and their deep ReLU network approximations, we show\nthat deep ReLU networks of width $\\mathcal{O}(N\\ln N)$ and depth\n$\\mathcal{O}(L\\ln L)$ can approximate $f\\in C^s([0,1]^d)$ with a nearly optimal\napproximation error $\\mathcal{O}(\\|f\\|_{C^s([0,1]^d)}N^{-2s/d}L^{-2s/d})$. Our\nestimate is non-asymptotic in the sense that it is valid for arbitrary width\nand depth specified by $N\\in\\mathbb{N}^+$ and $L\\in\\mathbb{N}^+$, respectively.",
          "link": "http://arxiv.org/abs/2001.03040",
          "publishedOn": "2021-07-23T02:00:32.841Z",
          "wordCount": 609,
          "title": "Deep Network Approximation for Smooth Functions. (arXiv:2001.03040v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10845",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_H/0/1/0/all/0/1\">Hanrui Wang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ding_Y/0/1/0/all/0/1\">Yongshan Ding</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Gu_J/0/1/0/all/0/1\">Jiaqi Gu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lin_Y/0/1/0/all/0/1\">Yujun Lin</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Pan_D/0/1/0/all/0/1\">David Z. Pan</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chong_F/0/1/0/all/0/1\">Frederic T. Chong</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>",
          "description": "Quantum noise is the key challenge in Noisy Intermediate-Scale Quantum (NISQ)\ncomputers. Limited research efforts have explored a higher level of\noptimization by making the quantum circuit resilient to noise. We propose and\nexperimentally implement QuantumNAS, the first comprehensive framework for\nnoise-adaptive co-search of variational circuit and qubit mapping. Variational\nquantum circuits are a promising approach for constructing quantum neural\nnetworks for machine learning and variational ansatzes for quantum simulation.\nHowever, finding the best variational circuit and its optimal parameters is\nchallenging in a high-dimensional Hilbert space. We propose to decouple the\nparameter training and circuit search by introducing a novel gate-sharing\nSuperCircuit. The SuperCircuit is trained by sampling and updating the\nSubCircuits in it and provides an accurate estimation of SubCircuit performance\ntrained from scratch. Then we perform an evolutionary co-search of SubCircuit\nand its qubit mapping. The SubCircuit performance is estimated with parameters\ninherited from SuperCircuit and simulated with real device noise models.\nFinally, we perform iterative gate pruning and finetuning to further remove the\nredundant gates in a fine-grained manner.\n\nExtensively evaluated with 12 QML and VQE benchmarks on 10 quantum computers,\nQuantumNAS significantly outperforms noise-unaware search, human and random\nbaselines. For QML tasks, QuantumNAS is the first to demonstrate over 95%\n2-class, 85% 4-class, and 32% 10-class classification accuracy on real quantum\ncomputers. It also achieves the lowest eigenvalue for VQE tasks on H2, H2O,\nLiH, CH4, BeH2 compared with UCCSD baselines. We also open-source QuantumEngine\n(https://github.com/mit-han-lab/pytorch-quantum) for fast training of\nparameterized quantum circuits to facilitate future research.",
          "link": "http://arxiv.org/abs/2107.10845",
          "publishedOn": "2021-07-23T02:00:32.834Z",
          "wordCount": 707,
          "title": "QuantumNAS: Noise-Adaptive Search for Robust Quantum Circuits. (arXiv:2107.10845v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2101.05145",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jena_R/0/1/0/all/0/1\">Rohit Jena</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singla_S/0/1/0/all/0/1\">Sumedha Singla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>",
          "description": "Vessel segmentation is an essential task in many clinical applications.\nAlthough supervised methods have achieved state-of-art performance, acquiring\nexpert annotation is laborious and mostly limited for two-dimensional datasets\nwith a small sample size. On the contrary, unsupervised methods rely on\nhandcrafted features to detect tube-like structures such as vessels. However,\nthose methods require complex pipelines involving several hyper-parameters and\ndesign choices rendering the procedure sensitive, dataset-specific, and not\ngeneralizable. We propose a self-supervised method with a limited number of\nhyper-parameters that is generalizable across modalities. Our method uses\ntube-like structure properties, such as connectivity, profile consistency, and\nbifurcation, to introduce inductive bias into a learning algorithm. To model\nthose properties, we generate a vector field that we refer to as a flow. Our\nexperiments on various public datasets in 2D and 3D show that our method\nperforms better than unsupervised methods while learning useful transferable\nfeatures from unlabeled data. Unlike generic self-supervised methods, the\nlearned features learn vessel-relevant features that are transferable for\nsupervised approaches, which is essential when the number of annotated data is\nlimited.",
          "link": "http://arxiv.org/abs/2101.05145",
          "publishedOn": "2021-07-23T02:00:32.810Z",
          "wordCount": 651,
          "title": "Self-Supervised Vessel Enhancement Using Flow-Based Consistencies. (arXiv:2101.05145v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Olin_Ammentorp_W/0/1/0/all/0/1\">Wilkie Olin-Ammentorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazhenov_M/0/1/0/all/0/1\">Maxim Bazhenov</a>",
          "description": "Despite rapid progress, current deep learning methods face a number of\ncritical challenges. These include high energy consumption, catastrophic\nforgetting, dependance on global losses, and an inability to reason\nsymbolically. By combining concepts from information bottleneck theory and\nvector-symbolic architectures, we propose and implement a novel information\nprocessing architecture, the 'Bridge network.' We show this architecture\nprovides unique advantages which can address the problem of global losses and\ncatastrophic forgetting. Furthermore, we argue that it provides a further basis\nfor increasing energy efficiency of execution and the ability to reason\nsymbolically.",
          "link": "http://arxiv.org/abs/2106.08446",
          "publishedOn": "2021-07-23T02:00:32.798Z",
          "wordCount": 549,
          "title": "Bridge Networks: Relating Inputs through Vector-Symbolic Manipulations. (arXiv:2106.08446v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1\">Shail Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baghdadi_R/0/1/0/all/0/1\">Riyadh Baghdadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowatzki_T/0/1/0/all/0/1\">Tony Nowatzki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avancha_S/0/1/0/all/0/1\">Sasikanth Avancha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Aviral Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baoxin Li</a>",
          "description": "Machine learning (ML) models are widely used in many important domains. For\nefficiently processing these computational- and memory-intensive applications,\ntensors of these over-parameterized models are compressed by leveraging\nsparsity, size reduction, and quantization of tensors. Unstructured sparsity\nand tensors with varying dimensions yield irregular computation, communication,\nand memory access patterns; processing them on hardware accelerators in a\nconventional manner does not inherently leverage acceleration opportunities.\nThis paper provides a comprehensive survey on the efficient execution of sparse\nand irregular tensor computations of ML models on hardware accelerators. In\nparticular, it discusses enhancement modules in the architecture design and the\nsoftware support; categorizes different hardware designs and acceleration\ntechniques and analyzes them in terms of hardware and execution costs; analyzes\nachievable accelerations for recent DNNs; highlights further opportunities in\nterms of hardware/software/model co-design optimizations (inter/intra-module).\nThe takeaways from this paper include: understanding the key challenges in\naccelerating sparse, irregular-shaped, and quantized tensors; understanding\nenhancements in accelerator systems for supporting their efficient\ncomputations; analyzing trade-offs in opting for a specific design choice for\nencoding, storing, extracting, communicating, computing, and load-balancing the\nnon-zeros; understanding how structured sparsity can improve storage efficiency\nand balance computations; understanding how to compile and map models with\nsparse tensors on the accelerators; understanding recent design trends for\nefficient accelerations and further opportunities.",
          "link": "http://arxiv.org/abs/2007.00864",
          "publishedOn": "2021-07-23T02:00:32.789Z",
          "wordCount": 725,
          "title": "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models: A Survey and Insights. (arXiv:2007.00864v2 [cs.AR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frank_T/0/1/0/all/0/1\">Thorben Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chmiela_S/0/1/0/all/0/1\">Stefan Chmiela</a>",
          "description": "Attention mechanisms are developing into a viable alternative to\nconvolutional layers as elementary building block of NNs. Their main advantage\nis that they are not restricted to capture local dependencies in the input, but\ncan draw arbitrary connections. This unprecedented capability coincides with\nthe long-standing problem of modeling global atomic interactions in molecular\nforce fields and other many-body problems. In its original formulation,\nhowever, attention is not applicable to the continuous domains in which the\natoms live. For this purpose we propose a variant to describe geometric\nrelations for arbitrary atomic configurations in Euclidean space that also\nrespects all relevant physical symmetries. We furthermore demonstrate, how the\nsuccessive application of our learned attention matrices effectively translates\nthe molecular geometry into a set of individual atomic contributions\non-the-fly.",
          "link": "http://arxiv.org/abs/2106.02549",
          "publishedOn": "2021-07-23T02:00:32.763Z",
          "wordCount": 600,
          "title": "Detect the Interactions that Matter in Matter: Geometric Attention for Many-Body Systems. (arXiv:2106.02549v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Anurag Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooper_S/0/1/0/all/0/1\">Seth Cooper</a>",
          "description": "Several works have demonstrated the use of variational autoencoders (VAEs)\nfor generating levels in the style of existing games and blending levels across\ndifferent games. Further, quality-diversity (QD) algorithms have also become\npopular for generating varied game content by using evolution to explore a\nsearch space while focusing on both variety and quality. To reap the benefits\nof both these approaches, we present a level generation and game blending\napproach that combines the use of VAEs and QD algorithms. Specifically, we\ntrain VAEs on game levels and run the MAP-Elites QD algorithm using the learned\nlatent space of the VAE as the search space. The latent space captures the\nproperties of the games whose levels we want to generate and blend, while\nMAP-Elites searches this latent space to find a diverse set of levels\noptimizing a given objective such as playability. We test our method using\nmodels for 5 different platformer games as well as a blended domain spanning 3\nof these games. We refer to using MAP-Elites for blending as Blend-Elites. Our\nresults show that MAP-Elites in conjunction with VAEs enables the generation of\na diverse set of playable levels not just for each individual game but also for\nthe blended domain while illuminating game-specific regions of the blended\nlatent space.",
          "link": "http://arxiv.org/abs/2102.12463",
          "publishedOn": "2021-07-23T02:00:32.745Z",
          "wordCount": 687,
          "title": "Generating and Blending Game Levels via Quality-Diversity in the Latent Space of a Variational Autoencoder. (arXiv:2102.12463v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ichnowski_J/0/1/0/all/0/1\">Jeffrey Ichnowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Paras Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stellato_B/0/1/0/all/0/1\">Bartolomeo Stellato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banjac_G/0/1/0/all/0/1\">Goran Banjac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Michael Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borrelli_F/0/1/0/all/0/1\">Francesco Borrelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1\">Ken Goldberg</a>",
          "description": "First-order methods for quadratic optimization such as OSQP are widely used\nfor large-scale machine learning and embedded optimal control, where many\nrelated problems must be rapidly solved. These methods face two persistent\nchallenges: manual hyperparameter tuning and convergence time to high-accuracy\nsolutions. To address these, we explore how Reinforcement Learning (RL) can\nlearn a policy to tune parameters to accelerate convergence. In experiments\nwith well-known QP benchmarks we find that our RL policy, RLQP, significantly\noutperforms state-of-the-art QP solvers by up to 3x. RLQP generalizes\nsurprisingly well to previously unseen problems with varying dimension and\nstructure from different applications, including the QPLIB, Netlib LP and\nMaros-Meszaros problems. Code for RLQP is available at\nhttps://github.com/berkeleyautomation/rlqp.",
          "link": "http://arxiv.org/abs/2107.10847",
          "publishedOn": "2021-07-23T02:00:32.728Z",
          "wordCount": 569,
          "title": "Accelerating Quadratic Optimization with Reinforcement Learning. (arXiv:2107.10847v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junghoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jounghee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_P/0/1/0/all/0/1\">Pilsung Kang</a>",
          "description": "Language models (LMs) pretrained on a large text corpus and fine-tuned on a\ndownstream text corpus and fine-tuned on a downstream task becomes a de facto\ntraining strategy for several natural language processing (NLP) tasks.\nRecently, an adaptive pretraining method retraining the pretrained language\nmodel with task-relevant data has shown significant performance improvements.\nHowever, current adaptive pretraining methods suffer from underfitting on the\ntask distribution owing to a relatively small amount of data to re-pretrain the\nLM. To completely use the concept of adaptive pretraining, we propose a\nback-translated task-adaptive pretraining (BT-TAPT) method that increases the\namount of task-specific data for LM re-pretraining by augmenting the task data\nusing back-translation to generalize the LM to the target task domain. The\nexperimental results show that the proposed BT-TAPT yields improved\nclassification accuracy on both low- and high-resource data and better\nrobustness to noise than the conventional adaptive pretraining method.",
          "link": "http://arxiv.org/abs/2107.10474",
          "publishedOn": "2021-07-23T02:00:32.722Z",
          "wordCount": 589,
          "title": "Back-Translated Task Adaptive Pretraining: Improving Accuracy and Robustness on Text Classification. (arXiv:2107.10474v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10606",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Marti_G/0/1/0/all/0/1\">Gautier Marti</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Goubet_V/0/1/0/all/0/1\">Victor Goubet</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Nielsen_F/0/1/0/all/0/1\">Frank Nielsen</a>",
          "description": "We propose a methodology to approximate conditional distributions in the\nelliptope of correlation matrices based on conditional generative adversarial\nnetworks. We illustrate the methodology with an application from quantitative\nfinance: Monte Carlo simulations of correlated returns to compare risk-based\nportfolio construction methods. Finally, we discuss about current limitations\nand advocate for further exploration of the elliptope geometry to improve\nresults.",
          "link": "http://arxiv.org/abs/2107.10606",
          "publishedOn": "2021-07-23T02:00:32.715Z",
          "wordCount": 520,
          "title": "cCorrGAN: Conditional Correlation GAN for Learning Empirical Conditional Distributions in the Elliptope. (arXiv:2107.10606v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nagpal_C/0/1/0/all/0/1\">Chirag Nagpal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadlowsky_S/0/1/0/all/0/1\">Steve Yadlowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostamzadeh_N/0/1/0/all/0/1\">Negar Rostamzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heller_K/0/1/0/all/0/1\">Katherine Heller</a>",
          "description": "Survival analysis is a challenging variation of regression modeling because\nof the presence of censoring, where the outcome measurement is only partially\nknown, due to, for example, loss to follow up. Such problems come up frequently\nin medical applications, making survival analysis a key endeavor in\nbiostatistics and machine learning for healthcare, with Cox regression models\nbeing amongst the most commonly employed models. We describe a new approach for\nsurvival analysis regression models, based on learning mixtures of Cox\nregressions to model individual survival distributions. We propose an\napproximation to the Expectation Maximization algorithm for this model that\ndoes hard assignments to mixture groups to make optimization efficient. In each\ngroup assignment, we fit the hazard ratios within each group using deep neural\nnetworks, and the baseline hazard for each mixture component\nnon-parametrically.\n\nWe perform experiments on multiple real world datasets, and look at the\nmortality rates of patients across ethnicity and gender. We emphasize the\nimportance of calibration in healthcare settings and demonstrate that our\napproach outperforms classical and modern survival analysis baselines, both in\nterms of discriminative performance and calibration, with large gains in\nperformance on the minority demographics.",
          "link": "http://arxiv.org/abs/2101.06536",
          "publishedOn": "2021-07-23T02:00:32.696Z",
          "wordCount": 671,
          "title": "Deep Cox Mixtures for Survival Regression. (arXiv:2101.06536v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10663",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Shi_N/0/1/0/all/0/1\">Naichen Shi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lai_F/0/1/0/all/0/1\">Fan Lai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kontar_R/0/1/0/all/0/1\">Raed Al Kontar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chowdhury_M/0/1/0/all/0/1\">Mosharaf Chowdhury</a>",
          "description": "In this paper we propose Fed-ensemble: a simple approach that bringsmodel\nensembling to federated learning (FL). Instead of aggregating localmodels to\nupdate a single global model, Fed-ensemble uses random permutations to update a\ngroup of K models and then obtains predictions through model averaging.\nFed-ensemble can be readily utilized within established FL methods and does not\nimpose a computational overhead as it only requires one of the K models to be\nsent to a client in each communication round. Theoretically, we show that\npredictions on newdata from all K models belong to the same predictive\nposterior distribution under a neural tangent kernel regime. This result in\nturn sheds light onthe generalization advantages of model averaging. We also\nillustrate thatFed-ensemble has an elegant Bayesian interpretation. Empirical\nresults show that our model has superior performance over several FL\nalgorithms,on a wide range of data sets, and excels in heterogeneous settings\noften encountered in FL applications.",
          "link": "http://arxiv.org/abs/2107.10663",
          "publishedOn": "2021-07-23T02:00:32.688Z",
          "wordCount": 589,
          "title": "Fed-ensemble: Improving Generalization through Model Ensembling in Federated Learning. (arXiv:2107.10663v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sander_M/0/1/0/all/0/1\">Michael E. Sander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ablin_P/0/1/0/all/0/1\">Pierre Ablin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blondel_M/0/1/0/all/0/1\">Mathieu Blondel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyre_G/0/1/0/all/0/1\">Gabriel Peyr&#xe9;</a>",
          "description": "The training of deep residual neural networks (ResNets) with backpropagation\nhas a memory cost that increases linearly with respect to the depth of the\nnetwork. A way to circumvent this issue is to use reversible architectures. In\nthis paper, we propose to change the forward rule of a ResNet by adding a\nmomentum term. The resulting networks, momentum residual neural networks\n(Momentum ResNets), are invertible. Unlike previous invertible architectures,\nthey can be used as a drop-in replacement for any existing ResNet block. We\nshow that Momentum ResNets can be interpreted in the infinitesimal step size\nregime as second-order ordinary differential equations (ODEs) and exactly\ncharacterize how adding momentum progressively increases the representation\ncapabilities of Momentum ResNets. Our analysis reveals that Momentum ResNets\ncan learn any linear mapping up to a multiplicative factor, while ResNets\ncannot. In a learning to optimize setting, where convergence to a fixed point\nis required, we show theoretically and empirically that our method succeeds\nwhile existing invertible architectures fail. We show on CIFAR and ImageNet\nthat Momentum ResNets have the same accuracy as ResNets, while having a much\nsmaller memory footprint, and show that pre-trained Momentum ResNets are\npromising for fine-tuning models.",
          "link": "http://arxiv.org/abs/2102.07870",
          "publishedOn": "2021-07-23T02:00:32.681Z",
          "wordCount": 668,
          "title": "Momentum Residual Neural Networks. (arXiv:2102.07870v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.05923",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Palma_G/0/1/0/all/0/1\">Giacomo De Palma</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kiani_B/0/1/0/all/0/1\">Bobak T. Kiani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lloyd_S/0/1/0/all/0/1\">Seth Lloyd</a>",
          "description": "The reliability of deep learning algorithms is fundamentally challenged by\nthe existence of adversarial examples, which are incorrectly classified inputs\nthat are extremely close to a correctly classified input. We explore the\nproperties of adversarial examples for deep neural networks with random weights\nand biases, and prove that for any $p\\ge1$, the $\\ell^p$ distance of any given\ninput from the classification boundary scales as one over the square root of\nthe dimension of the input times the $\\ell^p$ norm of the input. The results\nare based on the recently proved equivalence between Gaussian processes and\ndeep neural networks in the limit of infinite width of the hidden layers, and\nare validated with experiments on both random deep neural networks and deep\nneural networks trained on the MNIST and CIFAR10 datasets. The results\nconstitute a fundamental advance in the theoretical understanding of\nadversarial examples, and open the way to a thorough theoretical\ncharacterization of the relation between network architecture and robustness to\nadversarial perturbations.",
          "link": "http://arxiv.org/abs/2004.05923",
          "publishedOn": "2021-07-23T02:00:32.675Z",
          "wordCount": 649,
          "title": "Adversarial Robustness Guarantees for Random Deep Neural Networks. (arXiv:2004.05923v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brown_G/0/1/0/all/0/1\">Gavin Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1\">Marco Gaboardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1\">Adam Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullman_J/0/1/0/all/0/1\">Jonathan Ullman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakynthinou_L/0/1/0/all/0/1\">Lydia Zakynthinou</a>",
          "description": "We present two sample-efficient differentially private mean estimators for\n$d$-dimensional (sub)Gaussian distributions with unknown covariance.\nInformally, given $n \\gtrsim d/\\alpha^2$ samples from such a distribution with\nmean $\\mu$ and covariance $\\Sigma$, our estimators output $\\tilde\\mu$ such that\n$\\| \\tilde\\mu - \\mu \\|_{\\Sigma} \\leq \\alpha$, where $\\| \\cdot \\|_{\\Sigma}$ is\nthe Mahalanobis distance. All previous estimators with the same guarantee\neither require strong a priori bounds on the covariance matrix or require\n$\\Omega(d^{3/2})$ samples.\n\nEach of our estimators is based on a simple, general approach to designing\ndifferentially private mechanisms, but with novel technical steps to make the\nestimator private and sample-efficient. Our first estimator samples a point\nwith approximately maximum Tukey depth using the exponential mechanism, but\nrestricted to the set of points of large Tukey depth. Proving that this\nmechanism is private requires a novel analysis. Our second estimator perturbs\nthe empirical mean of the data set with noise calibrated to the empirical\ncovariance, without releasing the covariance itself. Its sample complexity\nguarantees hold more generally for subgaussian distributions, albeit with a\nslightly worse dependence on the privacy parameter. For both estimators,\ncareful preprocessing of the data is required to satisfy differential privacy.",
          "link": "http://arxiv.org/abs/2106.13329",
          "publishedOn": "2021-07-23T02:00:32.668Z",
          "wordCount": 658,
          "title": "Covariance-Aware Private Mean Estimation Without Private Covariance Estimation. (arXiv:2106.13329v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Langosco_L/0/1/0/all/0/1\">Lauro Langosco di Langosco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strathmann_H/0/1/0/all/0/1\">Heiko Strathmann</a>",
          "description": "Particle-based approximate Bayesian inference approaches such as Stein\nVariational Gradient Descent (SVGD) combine the flexibility and convergence\nguarantees of sampling methods with the computational benefits of variational\ninference. In practice, SVGD relies on the choice of an appropriate kernel\nfunction, which impacts its ability to model the target distribution -- a\nchallenging problem with only heuristic solutions. We propose Neural\nVariational Gradient Descent (NVGD), which is based on parameterizing the\nwitness function of the Stein discrepancy by a deep neural network whose\nparameters are learned in parallel to the inference, mitigating the necessity\nto make any kernel choices whatsoever. We empirically evaluate our method on\npopular synthetic inference problems, real-world Bayesian linear regression,\nand Bayesian neural network inference.",
          "link": "http://arxiv.org/abs/2107.10731",
          "publishedOn": "2021-07-23T02:00:32.650Z",
          "wordCount": 550,
          "title": "Neural Variational Gradient Descent. (arXiv:2107.10731v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tam Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raich_R/0/1/0/all/0/1\">Raviv Raich</a>",
          "description": "In multiple instance multiple label learning, each sample, a bag, consists of\nmultiple instances. To alleviate labeling complexity, each sample is associated\nwith a set of bag-level labels leaving instances within the bag unlabeled. This\nsetting is more convenient and natural for representing complicated objects,\nwhich have multiple semantic meanings. Compared to single instance labeling,\nthis approach allows for labeling larger datasets at an equivalent labeling\ncost. However, for sufficiently large datasets, labeling all bags may become\nprohibitively costly. Active learning uses an iterative labeling and retraining\napproach aiming to provide reasonable classification performance using a small\nnumber of labeled samples. To our knowledge, only a few works in the area of\nactive learning in the MIML setting are available. These approaches can provide\npractical solutions to reduce labeling cost but their efficacy remains unclear.\nIn this paper, we propose a novel bag-class pair based approach for active\nlearning in the MIML setting. Due to the partial availability of bag-level\nlabels, we focus on the incomplete-label MIML setting for the proposed active\nlearning approach. Our approach is based on a discriminative graphical model\nwith efficient and exact inference. For the query process, we adapt active\nlearning criteria to the novel bag-class pair selection strategy. Additionally,\nwe introduce an online stochastic gradient descent algorithm to provide an\nefficient model update after each query. Numerical experiments on benchmark\ndatasets illustrate the robustness of the proposed approach.",
          "link": "http://arxiv.org/abs/2107.10804",
          "publishedOn": "2021-07-23T02:00:32.642Z",
          "wordCount": 677,
          "title": "Active Learning in Incomplete Label Multiple Instance Multiple Label Learning. (arXiv:2107.10804v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03337",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Harbrecht_H/0/1/0/all/0/1\">Helmut Harbrecht</a>, <a href=\"http://arxiv.org/find/math/1/au:+Multerer_M/0/1/0/all/0/1\">Michael Multerer</a>",
          "description": "In this article, we introduce the concept of samplets by transferring the\nconstruction of Tausch-White wavelets to the realm of data. This way we obtain\na multilevel representation of discrete data which directly enables data\ncompression, detection of singularities and adaptivity. Applying samplets to\nrepresent kernel matrices, as they arise in kernel based learning or Gaussian\nprocess regression, we end up with quasi-sparse matrices. By thresholding small\nentries, these matrices are compressible to O(N log N) relevant entries, where\nN is the number of data points. This feature allows for the use of fill-in\nreducing reorderings to obtain a sparse factorization of the compressed\nmatrices. Besides the comprehensive introduction to samplets and their\nproperties, we present extensive numerical studies to benchmark the approach.\nOur results demonstrate that samplets mark a considerable step in the direction\nof making large data sets accessible for analysis.",
          "link": "http://arxiv.org/abs/2107.03337",
          "publishedOn": "2021-07-23T02:00:32.636Z",
          "wordCount": 597,
          "title": "Samplets: A new paradigm for data compression. (arXiv:2107.03337v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dobriban_E/0/1/0/all/0/1\">Edgar Dobriban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1\">Hamed Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">David Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robey_A/0/1/0/all/0/1\">Alexander Robey</a>",
          "description": "It is well known that machine learning methods can be vulnerable to\nadversarially-chosen perturbations of their inputs. Despite significant\nprogress in the area, foundational open problems remain. In this paper, we\naddress several key questions. We derive exact and approximate Bayes-optimal\nrobust classifiers for the important setting of two- and three-class Gaussian\nclassification problems with arbitrary imbalance, for $\\ell_2$ and\n$\\ell_\\infty$ adversaries. In contrast to classical Bayes-optimal classifiers,\ndetermining the optimal decisions here cannot be made pointwise and new\ntheoretical approaches are needed. We develop and leverage new tools, including\nrecent breakthroughs from probability theory on robust isoperimetry, which, to\nour knowledge, have not yet been used in the area. Our results reveal\nfundamental tradeoffs between standard and robust accuracy that grow when data\nis imbalanced. We also show further results, including an analysis of\nclassification calibration for convex losses in certain models, and finite\nsample rates for the robust risk.",
          "link": "http://arxiv.org/abs/2006.05161",
          "publishedOn": "2021-07-23T02:00:32.629Z",
          "wordCount": 660,
          "title": "Provable tradeoffs in adversarially robust classification. (arXiv:2006.05161v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>",
          "description": "In this paper, a novel two-branch neural network model structure is proposed\nfor multimodal emotion recognition, which consists of a time synchronous branch\n(TSB) and a time asynchronous branch (TAB). To capture correlations between\neach word and its acoustic realisation, the TSB combines speech and text\nmodalities at each input window frame and then does pooling across time to form\na single embedding vector. The TAB, by contrast, provides cross-utterance\ninformation by integrating sentence text embeddings from a number of context\nutterances into another embedding vector. The final emotion classification uses\nboth the TSB and the TAB embeddings. Experimental results on the IEMOCAP\ndataset demonstrate that the two-branch structure achieves state-of-the-art\nresults in 4-way classification with all common test setups. When using\nautomatic speech recognition (ASR) output instead of manually transcribed\nreference text, it is shown that the cross-utterance information considerably\nimproves the robustness against ASR errors. Furthermore, by incorporating an\nextra class for all the other emotions, the final 5-way classification system\nwith ASR hypotheses can be viewed as a prototype for more realistic emotion\nrecognition systems.",
          "link": "http://arxiv.org/abs/2010.14102",
          "publishedOn": "2021-07-23T02:00:32.606Z",
          "wordCount": 677,
          "title": "Emotion recognition by fusing time synchronous and time asynchronous representations. (arXiv:2010.14102v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brown_G/0/1/0/all/0/1\">Gavin Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bun_M/0/1/0/all/0/1\">Mark Bun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_V/0/1/0/all/0/1\">Vitaly Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1\">Adam Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1\">Kunal Talwar</a>",
          "description": "Modern machine learning models are complex and frequently encode surprising\namounts of information about individual inputs. In extreme cases, complex\nmodels appear to memorize entire input examples, including seemingly irrelevant\ninformation (social security numbers from text, for example). In this paper, we\naim to understand whether this sort of memorization is necessary for accurate\nlearning. We describe natural prediction problems in which every sufficiently\naccurate training algorithm must encode, in the prediction model, essentially\nall the information about a large subset of its training examples. This remains\ntrue even when the examples are high-dimensional and have entropy much higher\nthan the sample size, and even when most of that information is ultimately\nirrelevant to the task at hand. Further, our results do not depend on the\ntraining algorithm or the class of models used for learning.\n\nOur problems are simple and fairly natural variants of the next-symbol\nprediction and the cluster labeling tasks. These tasks can be seen as\nabstractions of text- and image-related prediction problems. To establish our\nresults, we reduce from a family of one-way communication problems for which we\nprove new information complexity lower bounds. Additionally, we present\nsynthetic-data experiments demonstrating successful attacks on logistic\nregression and neural network classifiers.",
          "link": "http://arxiv.org/abs/2012.06421",
          "publishedOn": "2021-07-23T02:00:32.599Z",
          "wordCount": 679,
          "title": "When is Memorization of Irrelevant Training Data Necessary for High-Accuracy Learning?. (arXiv:2012.06421v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_D/0/1/0/all/0/1\">Deepak Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phanishayee_A/0/1/0/all/0/1\">Amar Phanishayee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Kaiyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>",
          "description": "Many state-of-the-art ML results have been obtained by scaling up the number\nof parameters in existing models. However, parameters and activations for such\nlarge models often do not fit in the memory of a single accelerator device;\nthis means that it is necessary to distribute training of large models over\nmultiple accelerators. In this work, we propose PipeDream-2BW, a system that\nsupports memory-efficient pipeline parallelism. PipeDream-2BW uses a novel\npipelining and weight gradient coalescing strategy, combined with the double\nbuffering of weights, to ensure high throughput, low memory footprint, and\nweight update semantics similar to data parallelism. In addition, PipeDream-2BW\nautomatically partitions the model over the available hardware resources, while\nrespecting hardware constraints such as memory capacities of accelerators and\ninterconnect topologies. PipeDream-2BW can accelerate the training of large GPT\nand BERT language models by up to 20$\\times$ with similar final model accuracy.",
          "link": "http://arxiv.org/abs/2006.09503",
          "publishedOn": "2021-07-23T02:00:32.589Z",
          "wordCount": 622,
          "title": "Memory-Efficient Pipeline-Parallel DNN Training. (arXiv:2006.09503v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.07052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wick_F/0/1/0/all/0/1\">F. Wick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerzel_U/0/1/0/all/0/1\">U. Kerzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahn_M/0/1/0/all/0/1\">M. Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_M/0/1/0/all/0/1\">M. Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_T/0/1/0/all/0/1\">T. Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stemmer_D/0/1/0/all/0/1\">D. Stemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_J/0/1/0/all/0/1\">J. Ernst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feindt_M/0/1/0/all/0/1\">M. Feindt</a>",
          "description": "Demand forecasting is a central component of the replenishment process for\nretailers, as it provides crucial input for subsequent decision making like\nordering processes. In contrast to point estimates, such as the conditional\nmean of the underlying probability distribution, or confidence intervals,\nforecasting complete probability density functions allows to investigate the\nimpact on operational metrics, which are important to define the business\nstrategy, over the full range of the expected demand. Whereas metrics\nevaluating point estimates are widely used, methods for assessing the accuracy\nof predicted distributions are rare, and this work proposes new techniques for\nboth qualitative and quantitative evaluation methods. Using the supervised\nmachine learning method \"Cyclic Boosting\", complete individual probability\ndensity functions can be predicted such that each prediction is fully\nexplainable. This is of particular importance for practitioners, as it allows\nto avoid \"black-box\" models and understand the contributing factors for each\nindividual prediction. Another crucial aspect in terms of both explainability\nand generalizability of demand forecasting methods is the limitation of the\ninfluence of temporal confounding, which is prevalent in most state of the art\napproaches.",
          "link": "http://arxiv.org/abs/2009.07052",
          "publishedOn": "2021-07-23T02:00:32.583Z",
          "wordCount": 684,
          "title": "Demand Forecasting of Individual Probability Density Functions with Machine Learning. (arXiv:2009.07052v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10457",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Min Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Junliang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zongwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kecheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wange_X/0/1/0/all/0/1\">Xu Wange</a>",
          "description": "To explore the robustness of recommender systems, researchers have proposed\nvarious shilling attack models and analyzed their adverse effects. Primitive\nattacks are highly feasible but less effective due to simplistic handcrafted\nrules, while upgraded attacks are more powerful but costly and difficult to\ndeploy because they require more knowledge from recommendations. In this paper,\nwe explore a novel shilling attack called Graph cOnvolution-based generative\nshilling ATtack (GOAT) to balance the attacks' feasibility and effectiveness.\nGOAT adopts the primitive attacks' paradigm that assigns items for fake users\nby sampling and the upgraded attacks' paradigm that generates fake ratings by a\ndeep learning-based model. It deploys a generative adversarial network (GAN)\nthat learns the real rating distribution to generate fake ratings.\nAdditionally, the generator combines a tailored graph convolution structure\nthat leverages the correlations between co-rated items to smoothen the fake\nratings and enhance their authenticity. The extensive experiments on two public\ndatasets evaluate GOAT's performance from multiple perspectives. Our study of\nthe GOAT demonstrates technical feasibility for building a more powerful and\nintelligent attack model with a much-reduced cost, enables analysis the threat\nof such an attack and guides for investigating necessary prevention measures.",
          "link": "http://arxiv.org/abs/2107.10457",
          "publishedOn": "2021-07-23T02:00:32.576Z",
          "wordCount": 663,
          "title": "Ready for Emerging Threats to Recommender Systems? A Graph Convolution-based Generative Shilling Attack. (arXiv:2107.10457v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00554",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Fukami_K/0/1/0/all/0/1\">Kai Fukami</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Maulik_R/0/1/0/all/0/1\">Romit Maulik</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ramachandra_N/0/1/0/all/0/1\">Nesar Ramachandra</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Fukagata_K/0/1/0/all/0/1\">Koji Fukagata</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Taira_K/0/1/0/all/0/1\">Kunihiko Taira</a>",
          "description": "Achieving accurate and robust global situational awareness of a complex\ntime-evolving field from a limited number of sensors has been a longstanding\nchallenge. This reconstruction problem is especially difficult when sensors are\nsparsely positioned in a seemingly random or unorganized manner, which is often\nencountered in a range of scientific and engineering problems. Moreover, these\nsensors can be in motion and can become online or offline over time. The key\nleverage in addressing this scientific issue is the wealth of data accumulated\nfrom the sensors. As a solution to this problem, we propose a data-driven\nspatial field recovery technique founded on a structured grid-based\ndeep-learning approach for arbitrary positioned sensors of any numbers. It\nshould be noted that the na\\\"ive use of machine learning becomes prohibitively\nexpensive for global field reconstruction and is furthermore not adaptable to\nan arbitrary number of sensors. In the present work, we consider the use of\nVoronoi tessellation to obtain a structured-grid representation from sensor\nlocations enabling the computationally tractable use of convolutional neural\nnetworks. One of the central features of the present method is its\ncompatibility with deep-learning based super-resolution reconstruction\ntechniques for structured sensor data that are established for image\nprocessing. The proposed reconstruction technique is demonstrated for unsteady\nwake flow, geophysical data, and three-dimensional turbulence. The current\nframework is able to handle an arbitrary number of moving sensors, and thereby\novercomes a major limitation with existing reconstruction methods. The\npresented technique opens a new pathway towards the practical use of neural\nnetworks for real-time global field estimation.",
          "link": "http://arxiv.org/abs/2101.00554",
          "publishedOn": "2021-07-23T02:00:32.559Z",
          "wordCount": 717,
          "title": "Global field reconstruction from sparse sensors with Voronoi tessellation-assisted deep learning. (arXiv:2101.00554v2 [physics.flu-dyn] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Albert_K/0/1/0/all/0/1\">Kendra Albert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delano_M/0/1/0/all/0/1\">Maggie Delano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulynych_B/0/1/0/all/0/1\">Bogdan Kulynych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Ram Shankar Siva Kumar</a>",
          "description": "Attacks from adversarial machine learning (ML) have the potential to be used\n\"for good\": they can be used to run counter to the existing power structures\nwithin ML, creating breathing space for those who would otherwise be the\ntargets of surveillance and control. But most research on adversarial ML has\nnot engaged in developing tools for resistance against ML systems. Why? In this\npaper, we review the broader impact statements that adversarial ML researchers\nwrote as part of their NeurIPS 2020 papers and assess the assumptions that\nauthors have about the goals of their work. We also collect information about\nhow authors view their work's impact more generally. We find that most\nadversarial ML researchers at NeurIPS hold two fundamental assumptions that\nwill make it difficult for them to consider socially beneficial uses of\nattacks: (1) it is desirable to make systems robust, independent of context,\nand (2) attackers of systems are normatively bad and defenders of systems are\nnormatively good. That is, despite their expressed and supposed neutrality,\nmost adversarial ML researchers believe that the goal of their work is to\nsecure systems, making it difficult to conceptualize and build tools for\ndisrupting the status quo.",
          "link": "http://arxiv.org/abs/2107.10302",
          "publishedOn": "2021-07-23T02:00:32.552Z",
          "wordCount": 687,
          "title": "Adversarial for Good? How the Adversarial ML Community's Values Impede Socially Beneficial Uses of Attacks. (arXiv:2107.10302v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10790",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mayor_Torres_J/0/1/0/all/0/1\">Juan Manuel Mayor-Torres</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravanelli_M/0/1/0/all/0/1\">Mirco Ravanelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Medina_DeVilliers_S/0/1/0/all/0/1\">Sara E. Medina-DeVilliers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lerner_M/0/1/0/all/0/1\">Matthew D. Lerner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riccardi_G/0/1/0/all/0/1\">Giuseppe Riccardi</a>",
          "description": "Machine learning methods, such as deep learning, show promising results in\nthe medical domain. However, the lack of interpretability of these algorithms\nmay hinder their applicability to medical decision support systems. This paper\nstudies an interpretable deep learning technique, called SincNet. SincNet is a\nconvolutional neural network that efficiently learns customized band-pass\nfilters through trainable sinc-functions. In this study, we use SincNet to\nanalyze the neural activity of individuals with Autism Spectrum Disorder (ASD),\nwho experience characteristic differences in neural oscillatory activity. In\nparticular, we propose a novel SincNet-based neural network for detecting\nemotions in ASD patients using EEG signals. The learned filters can be easily\ninspected to detect which part of the EEG spectrum is used for predicting\nemotions. We found that our system automatically learns the high-$\\alpha$ (9-13\nHz) and $\\beta$ (13-30 Hz) band suppression often present in individuals with\nASD. This result is consistent with recent neuroscience studies on emotion\nrecognition, which found an association between these band suppressions and the\nbehavioral deficits observed in individuals with ASD. The improved\ninterpretability of SincNet is achieved without sacrificing performance in\nemotion recognition.",
          "link": "http://arxiv.org/abs/2107.10790",
          "publishedOn": "2021-07-23T02:00:32.546Z",
          "wordCount": 642,
          "title": "Interpretable SincNet-based Deep Learning for Emotion Recognition from EEG brain activity. (arXiv:2107.10790v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2008.07298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atli_B/0/1/0/all/0/1\">Buse Gul Atli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuxi Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchal_S/0/1/0/all/0/1\">Samuel Marchal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asokan_N/0/1/0/all/0/1\">N. Asokan</a>",
          "description": "Federated learning is a distributed learning technique where machine learning\nmodels are trained on client devices in which the local training data resides.\nThe training is coordinated via a central server which is, typically,\ncontrolled by the intended owner of the resulting model. By avoiding the need\nto transport the training data to the central server, federated learning\nimproves privacy and efficiency. But it raises the risk of model theft by\nclients because the resulting model is available on every client device. Even\nif the application software used for local training may attempt to prevent\ndirect access to the model, a malicious client may bypass any such restrictions\nby reverse engineering the application software. Watermarking is a well-known\ndeterrence method against model theft by providing the means for model owners\nto demonstrate ownership of their models. Several recent deep neural network\n(DNN) watermarking techniques use backdooring: training the models with\nadditional mislabeled data. Backdooring requires full access to the training\ndata and control of the training process. This is feasible when a single party\ntrains the model in a centralized manner, but not in a federated learning\nsetting where the training process and training data are distributed among\nseveral client devices. In this paper, we present WAFFLE, the first approach to\nwatermark DNN models trained using federated learning. It introduces a\nretraining step at the server after each aggregation of local models into the\nglobal model. We show that WAFFLE efficiently embeds a resilient watermark into\nmodels incurring only negligible degradation in test accuracy (-0.17%), and\ndoes not require access to training data. We also introduce a novel technique\nto generate the backdoor used as a watermark. It outperforms prior techniques,\nimposing no communication, and low computational (+3.2%) overhead.",
          "link": "http://arxiv.org/abs/2008.07298",
          "publishedOn": "2021-07-23T02:00:32.531Z",
          "wordCount": 786,
          "title": "WAFFLE: Watermarking in Federated Learning. (arXiv:2008.07298v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duzceker_A/0/1/0/all/0/1\">Arda D&#xfc;z&#xe7;eker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galliani_S/0/1/0/all/0/1\">Silvano Galliani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogel_C/0/1/0/all/0/1\">Christoph Vogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speciale_P/0/1/0/all/0/1\">Pablo Speciale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusmanu_M/0/1/0/all/0/1\">Mihai Dusmanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>",
          "description": "We propose an online multi-view depth prediction approach on posed video\nstreams, where the scene geometry information computed in the previous time\nsteps is propagated to the current time step in an efficient and geometrically\nplausible way. The backbone of our approach is a real-time capable, lightweight\nencoder-decoder that relies on cost volumes computed from pairs of images. We\nextend it by placing a ConvLSTM cell at the bottleneck layer, which compresses\nan arbitrary amount of past information in its states. The novelty lies in\npropagating the hidden state of the cell by accounting for the viewpoint\nchanges between time steps. At a given time step, we warp the previous hidden\nstate into the current camera plane using the previous depth prediction. Our\nextension brings only a small overhead of computation time and memory\nconsumption, while improving the depth predictions significantly. As a result,\nwe outperform the existing state-of-the-art multi-view stereo methods on most\nof the evaluated metrics in hundreds of indoor scenes while maintaining a\nreal-time performance. Code available:\nhttps://github.com/ardaduz/deep-video-mvs",
          "link": "http://arxiv.org/abs/2012.02177",
          "publishedOn": "2021-07-23T02:00:32.512Z",
          "wordCount": 660,
          "title": "DeepVideoMVS: Multi-View Stereo on Video with Recurrent Spatio-Temporal Fusion. (arXiv:2012.02177v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04470",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mir_A/0/1/0/all/0/1\">Amir M. Mir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latoskinas_E/0/1/0/all/0/1\">Evaldas Latoskinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proksch_S/0/1/0/all/0/1\">Sebastian Proksch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gousios_G/0/1/0/all/0/1\">Georgios Gousios</a>",
          "description": "Dynamic languages, such as Python and Javascript, trade static typing for\ndeveloper flexibility and productivity. Lack of static typing can cause\nrun-time exceptions and is a major factor for weak IDE support. To alleviate\nthese issues, PEP 484 introduced optional type annotations for Python. As\nretrofitting types to existing codebases is error-prone and laborious,\nlearning-based approaches have been proposed to enable automatic type\nannotations based on existing, partially annotated codebases. However, it is\nstill quite challenging for learning-based approaches to give a relevant\nprediction in the first suggestion or the first few ones. In this paper, we\npresent Type4Py, a deep similarity learning-based hierarchical neural network\nmodel that learns to discriminate between types of the same kind and dissimilar\ntypes in a high-dimensional space, which results in clusters of types. Nearest\nneighbor search suggests a list of likely types for arguments, variables, and\nfunctions' return. The results of the quantitative and qualitative evaluation\nindicate that Type4Py significantly outperforms state-of-the-art approaches at\nthe type prediction task. Considering the Top-1 prediction, Type4Py obtains a\nMean Reciprocal Rank of 72.5%, which is 10.87% and 16.45% higher than that of\nTypilus and TypeWriter, respectively.",
          "link": "http://arxiv.org/abs/2101.04470",
          "publishedOn": "2021-07-23T02:00:32.504Z",
          "wordCount": 681,
          "title": "Type4Py: Deep Similarity Learning-Based Type Inference for Python. (arXiv:2101.04470v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oza_M/0/1/0/all/0/1\">Manan Oza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1\">Sukalpa Chanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1\">David Doermann</a>",
          "description": "Faces generated using generative adversarial networks (GANs) have reached\nunprecedented realism. These faces, also known as \"Deep Fakes\", appear as\nrealistic photographs with very little pixel-level distortions. While some work\nhas enabled the training of models that lead to the generation of specific\nproperties of the subject, generating a facial image based on a natural\nlanguage description has not been fully explored. For security and criminal\nidentification, the ability to provide a GAN-based system that works like a\nsketch artist would be incredibly useful. In this paper, we present a novel\napproach to generate facial images from semantic text descriptions. The learned\nmodel is provided with a text description and an outline of the type of face,\nwhich the model uses to sketch the features. Our models are trained using an\nAffine Combination Module (ACM) mechanism to combine the text embedding from\nBERT and the GAN latent space using a self-attention matrix. This avoids the\nloss of features due to inadequate \"attention\", which may happen if text\nembedding and latent vector are simply concatenated. Our approach is capable of\ngenerating images that are very accurately aligned to the exhaustive textual\ndescriptions of faces with many fine detail features of the face and helps in\ngenerating better images. The proposed method is also capable of making\nincremental changes to a previously generated image if it is provided with\nadditional textual descriptions or sentences.",
          "link": "http://arxiv.org/abs/2107.10756",
          "publishedOn": "2021-07-23T02:00:32.497Z",
          "wordCount": 672,
          "title": "Semantic Text-to-Face GAN -ST^2FG. (arXiv:2107.10756v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.09643",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goudarzi_A/0/1/0/all/0/1\">Armin Goudarzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spehr_C/0/1/0/all/0/1\">Carsten Spehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herbold_S/0/1/0/all/0/1\">Steffen Herbold</a>",
          "description": "Beamforming is an imaging tool for the investigation of aeroacoustic\nphenomena and results in high dimensional data that is broken down to spectra\nby integrating spatial Regions Of Interest. This paper presents two methods\nthat enable the automated identification of aeroacoustic sources in sparse\nbeamforming maps and the extraction of their corresponding spectra to overcome\nthe manual definition of Regions Of Interest. The methods are evaluated on two\nscaled airframe half-model wind-tunnel measurements and on a generic monopole\nsource. The first relies on the spatial normal distribution of aeroacoustic\nbroadband sources in sparse beamforming maps. The second uses hierarchical\nclustering methods. Both methods are robust to statistical noise and predict\nthe existence, location, and spatial probability estimation for sources based\non which Regions Of Interest are automatically determined.",
          "link": "http://arxiv.org/abs/2012.09643",
          "publishedOn": "2021-07-23T02:00:32.489Z",
          "wordCount": 627,
          "title": "Automatic source localization and spectra generation from sparse beamforming maps. (arXiv:2012.09643v4 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imbiriba_T/0/1/0/all/0/1\">Tales Imbiriba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junha Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Closas_P/0/1/0/all/0/1\">Pau Closas</a>",
          "description": "Localization and tracking of objects using data-driven methods is a popular\ntopic due to the complexity in characterizing the physics of wireless channel\npropagation models. In these modeling approaches, data needs to be gathered to\naccurately train models, at the same time that user's privacy is maintained. An\nappealing scheme to cooperatively achieve these goals is known as Federated\nLearning (FL). A challenge in FL schemes is the presence of non-independent and\nidentically distributed (non-IID) data, caused by unevenly exploration of\ndifferent areas. In this paper, we consider the use of recent FL schemes to\ntrain a set of personalized models that are then optimally fused through\nBayesian rules, which makes it appropriate in the context of indoor\nlocalization.",
          "link": "http://arxiv.org/abs/2107.04189",
          "publishedOn": "2021-07-23T02:00:32.483Z",
          "wordCount": 574,
          "title": "Personalized Federated Learning over non-IID Data for Indoor Localization. (arXiv:2107.04189v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.16051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barbado_A/0/1/0/all/0/1\">Alberto Barbado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corcho_O/0/1/0/all/0/1\">&#xd3;scar Corcho</a>",
          "description": "Identifying anomalies in the fuel consumption of the vehicles of a fleet is a\ncrucial aspect for optimizing consumption and reduce costs. However, this\ninformation alone is insufficient, since fleet operators need to know the\ncauses behind anomalous fuel consumption. We combine unsupervised anomaly\ndetection techniques, domain knowledge and interpretable Machine Learning\nmodels for explaining potential causes of abnormal fuel consumption in terms of\nfeature relevance. The explanations are used for generating recommendations\nabout fuel optimization, that are adjusted according to two different user\nprofiles: fleet managers and fleet operators. Results are evaluated over\nreal-world data from telematics devices connected to diesel and petrol vehicles\nfrom different types of industrial fleets. We measure the proposal regarding\nmodel performance, and using Explainable AI metrics that compare the\nexplanations in terms of representativeness, fidelity, stability,\ncontrastiveness and consistency with apriori beliefs. The potential fuel\nreductions that can be achieved is round 35%.",
          "link": "http://arxiv.org/abs/2010.16051",
          "publishedOn": "2021-07-23T02:00:32.468Z",
          "wordCount": 654,
          "title": "Interpretable Machine Learning Models for Predicting and Explaining Vehicle Fuel Consumption Anomalies. (arXiv:2010.16051v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Franceschelli_G/0/1/0/all/0/1\">Giorgio Franceschelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1\">Mirco Musolesi</a>",
          "description": "Machine-generated artworks are now part of the contemporary art scene: they\nare attracting significant investments and they are presented in exhibitions\ntogether with those created by human artists. These artworks are mainly based\non generative deep learning techniques, which have seen a formidable\ndevelopment and remarkable refinement in the very recent years. Given the\ninherent characteristics of these techniques, a series of novel legal problems\narise.\n\nIn this article, we consider a set of key questions in the area of generative\ndeep learning for the arts, including the following: is it possible to use\ncopyrighted works as training set for generative models? How do we legally\nstore their copies in order to perform the training process? Who (if someone)\nwill own the copyright on the generated data? We try to answer these questions\nconsidering the law in force in both the United States of America and the\nEuropean Union, and potential future alternatives. Finally, we also formulate a\nset of practical guidelines for artists and developers working on deep learning\ngenerated art.",
          "link": "http://arxiv.org/abs/2105.09266",
          "publishedOn": "2021-07-23T02:00:32.439Z",
          "wordCount": 676,
          "title": "Copyright in Generative Deep Learning. (arXiv:2105.09266v3 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10746",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Qendro_L/0/1/0/all/0/1\">Lorena Qendro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campbell_A/0/1/0/all/0/1\">Alexander Campbell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf2;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mascolo_C/0/1/0/all/0/1\">Cecilia Mascolo</a>",
          "description": "Electroencephalography (EEG) is crucial for the monitoring and diagnosis of\nbrain disorders. However, EEG signals suffer from perturbations caused by\nnon-cerebral artifacts limiting their efficacy. Current artifact detection\npipelines are resource-hungry and rely heavily on hand-crafted features.\nMoreover, these pipelines are deterministic in nature, making them unable to\ncapture predictive uncertainty. We propose E4G, a deep learning framework for\nhigh frequency EEG artifact detection. Our framework exploits the early exit\nparadigm, building an implicit ensemble of models capable of capturing\nuncertainty. We evaluate our approach on the Temple University Hospital EEG\nArtifact Corpus (v2.0) achieving state-of-the-art classification results. In\naddition, E4G provides well-calibrated uncertainty metrics comparable to\nsampling techniques like Monte Carlo dropout in just a single forward pass. E4G\nopens the door to uncertainty-aware artifact detection supporting\nclinicians-in-the-loop frameworks.",
          "link": "http://arxiv.org/abs/2107.10746",
          "publishedOn": "2021-07-23T02:00:32.432Z",
          "wordCount": 584,
          "title": "High Frequency EEG Artifact Detection with Uncertainty via Early Exit Paradigm. (arXiv:2107.10746v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2103.11580",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tu_H/0/1/0/all/0/1\">Hao Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moura_S/0/1/0/all/0/1\">Scott Moura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_H/0/1/0/all/0/1\">Huazhen Fang</a>",
          "description": "Mathematical modeling of lithium-ion batteries (LiBs) is a central challenge\nin advanced battery management. This paper presents a new approach to integrate\na physics-based model with machine learning to achieve high-precision modeling\nfor LiBs. This approach uniquely proposes to inform the machine learning model\nof the dynamic state of the physical model, enabling a deep integration between\nphysics and machine learning. We propose two hybrid physics-machine learning\nmodels based on the approach, which blend a single particle model with thermal\ndynamics (SPMT) with a feedforward neural network (FNN) to perform\nphysics-informed learning of a LiB's dynamic behavior. The proposed models are\nrelatively parsimonious in structure and can provide considerable predictive\naccuracy even at high C-rates, as shown by extensive simulations.",
          "link": "http://arxiv.org/abs/2103.11580",
          "publishedOn": "2021-07-23T02:00:32.424Z",
          "wordCount": 607,
          "title": "Integrating Electrochemical Modeling with Machine Learning for Lithium-Ion Batteries. (arXiv:2103.11580v4 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhe Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vergari_A/0/1/0/all/0/1\">Antonio Vergari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1\">Guy Van den Broeck</a>",
          "description": "Computing the expectation of kernel functions is a ubiquitous task in machine\nlearning, with applications from classical support vector machines to\nexploiting kernel embeddings of distributions in probabilistic modeling,\nstatistical inference, causal discovery, and deep learning. In all these\nscenarios, we tend to resort to Monte Carlo estimates as expectations of\nkernels are intractable in general. In this work, we characterize the\nconditions under which we can compute expected kernels exactly and efficiently,\nby leveraging recent advances in probabilistic circuit representations. We\nfirst construct a circuit representation for kernels and propose an approach to\nsuch tractable computation. We then demonstrate possible advancements for\nkernel embedding frameworks by exploiting tractable expected kernels to derive\nnew algorithms for two challenging scenarios: 1) reasoning under missing data\nwith kernel support vector regressors; 2) devising a collapsed black-box\nimportance sampling scheme. Finally, we empirically evaluate both algorithms\nand show that they outperform standard baselines on a variety of datasets.",
          "link": "http://arxiv.org/abs/2102.10562",
          "publishedOn": "2021-07-23T02:00:32.405Z",
          "wordCount": 617,
          "title": "Tractable Computation of Expected Kernels. (arXiv:2102.10562v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10716",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ponomarchuk_A/0/1/0/all/0/1\">Alexander Ponomarchuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burenko_I/0/1/0/all/0/1\">Ilya Burenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malkin_E/0/1/0/all/0/1\">Elian Malkin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nazarov_I/0/1/0/all/0/1\">Ivan Nazarov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kokh_V/0/1/0/all/0/1\">Vladimir Kokh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Avetisian_M/0/1/0/all/0/1\">Manvel Avetisian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhukov_L/0/1/0/all/0/1\">Leonid Zhukov</a>",
          "description": "The COVID-19 pandemic created a significant interest and demand for infection\ndetection and monitoring solutions. In this paper we propose a machine learning\nmethod to quickly triage COVID-19 using recordings made on consumer devices.\nThe approach combines signal processing methods with fine-tuned deep learning\nnetworks and provides methods for signal denoising, cough detection and\nclassification. We have also developed and deployed a mobile application that\nuses symptoms checker together with voice, breath and cough signals to detect\nCOVID-19 infection. The application showed robust performance on both open\nsourced datasets and on the noisy data collected during beta testing by the end\nusers.",
          "link": "http://arxiv.org/abs/2107.10716",
          "publishedOn": "2021-07-23T02:00:32.398Z",
          "wordCount": 618,
          "title": "Project Achoo: A Practical Model and Application for COVID-19 Detection from Recordings of Breath, Voice, and Cough. (arXiv:2107.10716v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_G/0/1/0/all/0/1\">Guangyin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Huan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fuxian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jincai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>",
          "description": "Travel time estimation is one of the core tasks for the development of\nintelligent transportation systems. Most previous works model the road segments\nor intersections separately by learning their spatio-temporal characteristics\nto estimate travel time. However, due to the continuous alternations of the\nroad segments and intersections in a path, the dynamic features are supposed to\nbe coupled and interactive. Therefore, modeling one of them limits further\nimprovement in accuracy of estimating travel time. To address the above\nproblems, a novel graph-based deep learning framework for travel time\nestimation is proposed in this paper, namely Spatio-Temporal Dual Graph Neural\nNetworks (STDGNN). Specifically, we first establish the node-wise and edge-wise\ngraphs to respectively characterize the adjacency relations of intersections\nand that of road segments. In order to extract the joint spatio-temporal\ncorrelations of the intersections and road segments, we adopt the\nspatio-temporal dual graph learning approach that incorporates multiple\nspatial-temporal dual graph learning modules with multi-scale network\narchitectures for capturing multi-level spatial-temporal information from the\ndual graph. Finally, we employ the multi-task learning approach to estimate the\ntravel time of a given whole route, each road segment and intersection\nsimultaneously. We conduct extensive experiments to evaluate our proposed model\non three real-world trajectory datasets, and the experimental results show that\nSTDGNN significantly outperforms several state-of-art baselines.",
          "link": "http://arxiv.org/abs/2105.13591",
          "publishedOn": "2021-07-23T02:00:32.392Z",
          "wordCount": 682,
          "title": "Spatio-Temporal Dual Graph Neural Networks for Travel Time Estimation. (arXiv:2105.13591v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03905",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Catak_F/0/1/0/all/0/1\">Ferhat Ozgur Catak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Catak_E/0/1/0/all/0/1\">Evren Catak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuzlu_M/0/1/0/all/0/1\">Murat Kuzlu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cali_U/0/1/0/all/0/1\">Umit Cali</a>",
          "description": "6G -- sixth generation -- is the latest cellular technology currently under\ndevelopment for wireless communication systems. In recent years, machine\nlearning algorithms have been applied widely in various fields, such as\nhealthcare, transportation, energy, autonomous car, and many more. Those\nalgorithms have been also using in communication technologies to improve the\nsystem performance in terms of frequency spectrum usage, latency, and security.\nWith the rapid developments of machine learning techniques, especially deep\nlearning, it is critical to take the security concern into account when\napplying the algorithms. While machine learning algorithms offer significant\nadvantages for 6G networks, security concerns on Artificial Intelligent (AI)\nmodels is typically ignored by the scientific community so far. However,\nsecurity is also a vital part of the AI algorithms, this is because the AI\nmodel itself can be poisoned by attackers. This paper proposes a mitigation\nmethod for adversarial attacks against proposed 6G machine learning models for\nthe millimeter-wave (mmWave) beam prediction using adversarial learning. The\nmain idea behind adversarial attacks against machine learning models is to\nproduce faulty results by manipulating trained deep learning models for 6G\napplications for mmWave beam prediction. We also present the adversarial\nlearning mitigation method's performance for 6G security in mmWave beam\nprediction application with fast gradient sign method attack. The mean square\nerrors (MSE) of the defended model under attack are very close to the\nundefended model without attack.",
          "link": "http://arxiv.org/abs/2105.03905",
          "publishedOn": "2021-07-23T02:00:32.385Z",
          "wordCount": 717,
          "title": "Security Concerns on Machine Learning Solutions for 6G Networks in mmWave Beam Prediction. (arXiv:2105.03905v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07470",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Teterwak_P/0/1/0/all/0/1\">Piotr Teterwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_D/0/1/0/all/0/1\">Dilip Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1\">Michael C. Mozer</a>",
          "description": "A discriminatively trained neural net classifier can fit the training data\nperfectly if all information about its input other than class membership has\nbeen discarded prior to the output layer. Surprisingly, past research has\ndiscovered that some extraneous visual detail remains in the logit vector. This\nfinding is based on inversion techniques that map deep embeddings back to\nimages. We explore this phenomenon further using a novel synthesis of methods,\nyielding a feedforward inversion model that produces remarkably high fidelity\nreconstructions, qualitatively superior to those of past efforts. When applied\nto an adversarially robust classifier model, the reconstructions contain\nsufficient local detail and global structure that they might be confused with\nthe original image in a quick glance, and the object category can clearly be\ngleaned from the reconstruction. Our approach is based on BigGAN (Brock, 2019),\nwith conditioning on logits instead of one-hot class labels. We use our\nreconstruction model as a tool for exploring the nature of representations,\nincluding: the influence of model architecture and training objectives\n(specifically robust losses), the forms of invariance that networks achieve,\nrepresentational differences between correctly and incorrectly classified\nimages, and the effects of manipulating logits and images. We believe that our\nmethod can inspire future investigations into the nature of information flow in\na neural net and can provide diagnostics for improving discriminative models.",
          "link": "http://arxiv.org/abs/2103.07470",
          "publishedOn": "2021-07-23T02:00:32.378Z",
          "wordCount": 689,
          "title": "Understanding Invariance via Feedforward Inversion of Discriminatively Trained Classifiers. (arXiv:2103.07470v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1\">Kevin Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhishek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serghiou_S/0/1/0/all/0/1\">Stylianos Serghiou</a>",
          "description": "Digital contact tracing apps for COVID, such as the one developed by Google\nand Apple, need to estimate the risk that a user was infected during a\nparticular exposure, in order to decide whether to notify the user to take\nprecautions, such as entering into quarantine, or requesting a test. Such risk\nscore models contain numerous parameters that must be set by the public health\nauthority. In this paper, we show how to automatically learn these parameters\nfrom data.\n\nOur method needs access to exposure and outcome data. Although this data is\nalready being collected (in an aggregated, privacy-preserving way) by several\nhealth authorities, in this paper we limit ourselves to simulated data, so that\nwe can systematically study the different factors that affect the feasibility\nof the approach. In particular, we show that the parameters become harder to\nestimate when there is more missing data (e.g., due to infections which were\nnot recorded by the app), and when there is model misspecification.\nNevertheless, the learning approach outperforms a strong manually designed\nbaseline. Furthermore, the learning approach can adapt even when the risk\nfactors of the disease change, e.g., due to the evolution of new variants, or\nthe adoption of vaccines.",
          "link": "http://arxiv.org/abs/2104.08415",
          "publishedOn": "2021-07-23T02:00:32.371Z",
          "wordCount": 710,
          "title": "Risk score learning for COVID-19 contact tracing apps. (arXiv:2104.08415v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Finardi_P/0/1/0/all/0/1\">Paulo Finardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viegas_J/0/1/0/all/0/1\">Jos&#xe9; Di&#xe9; Viegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_G/0/1/0/all/0/1\">Gustavo T. Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansano_A/0/1/0/all/0/1\">Alex F. Mansano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carida_V/0/1/0/all/0/1\">Vinicius F. Carid&#xe1;</a>",
          "description": "In the last few years, three major topics received increased interest: deep\nlearning, NLP and conversational agents. Bringing these three topics together\nto create an amazing digital customer experience and indeed deploy in\nproduction and solve real-world problems is something innovative and\ndisruptive. We introduce a new Portuguese financial domain language\nrepresentation model called BERTa\\'u. BERTa\\'u is an uncased BERT-base trained\nfrom scratch with data from the Ita\\'u virtual assistant chatbot solution. Our\nnovel contribution is that BERTa\\'u pretrained language model requires less\ndata, reached state-of-the-art performance in three NLP tasks, and generates a\nsmaller and lighter model that makes the deployment feasible. We developed\nthree tasks to validate our model: information retrieval with Frequently Asked\nQuestions (FAQ) from Ita\\'u bank, sentiment analysis from our virtual assistant\ndata, and a NER solution. All proposed tasks are real-world solutions in\nproduction on our environment and the usage of a specialist model proved to be\neffective when compared to Google BERT multilingual and the DPRQuestionEncoder\nfrom Facebook, available at Hugging Face. The BERTa\\'u improves the performance\nin 22% of FAQ Retrieval MRR metric, 2.1% in Sentiment Analysis F1 score, 4.4%\nin NER F1 score and can also represent the same sequence in up to 66% fewer\ntokens when compared to \"shelf models\".",
          "link": "http://arxiv.org/abs/2101.12015",
          "publishedOn": "2021-07-23T02:00:32.353Z",
          "wordCount": 687,
          "title": "BERTa\\'u: Ita\\'u BERT for digital customer service. (arXiv:2101.12015v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangru Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1\">Sandip Ray</a>",
          "description": "The acceleration of CNNs has gained increasing atten-tion since their success\nin computer vision. With the heterogeneous functional layers that cannot be\npro-cessed by the accelerators proposed for convolution layers only, modern\nend-to-end CNN acceleration so-lutions either transform the diverse computation\ninto matrix/vector arithmetic, which loses data reuse op-portunities in\nconvolution, or introduce dedicated functional unit to each kind of layer,\nwhich results in underutilization and high update expense. To enhance the\nwhole-life cost efficiency, we need an acceleration solution that is efficient\nin processing CNN layers and has the generality to apply to all kinds of\nexisting and emerging layers. To this end, we pro-pose GCONV Chain, a method to\nconvert the entire CNN computation into a chain of standard general\nconvolutions (GCONV) that can be efficiently pro-cessed by the existing CNN\naccelerators. This paper comprehensively analyzes the GCONV Chain model and\nproposes a full-stack implementation to support GCONV Chain. On one hand, the\nresults on seven var-ious CNNs demonstrate that GCONV Chain improves the\nperformance and energy efficiency of existing CNN accelerators by an average of\n3.4x and 3.2x re-spectively. On the other hand, we show that GCONV Chain\nprovides low whole-life costs for CNN accelera-tion, including both developer\nefforts and total cost of ownership for the users.",
          "link": "http://arxiv.org/abs/2104.05541",
          "publishedOn": "2021-07-23T02:00:32.345Z",
          "wordCount": 673,
          "title": "Optimizing the Whole-life Cost in End-to-end CNN Acceleration. (arXiv:2104.05541v2 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10578",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Ye_C/0/1/0/all/0/1\">Cheng Ye</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Swiers_R/0/1/0/all/0/1\">Rowan Swiers</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bonner_S/0/1/0/all/0/1\">Stephen Bonner</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Barrett_I/0/1/0/all/0/1\">Ian Barrett</a>",
          "description": "The drug discovery and development process is a long and expensive one,\ncosting over 1 billion USD on average per drug and taking 10-15 years. To\nreduce the high levels of attrition throughout the process, there has been a\ngrowing interest in applying machine learning methodologies to various stages\nof drug discovery process in the recent decade, including at the earliest stage\n- identification of druggable disease genes. In this paper, we have developed a\nnew tensor factorisation model to predict potential drug targets (i.e.,genes or\nproteins) for diseases. We created a three dimensional tensor which consists of\n1,048 targets, 860 diseases and 230,011 evidence attributes and clinical\noutcomes connecting them, using data extracted from the Open Targets and\nPharmaProjects databases. We enriched the data with gene representations\nlearned from a drug discovery-oriented knowledge graph and applied our proposed\nmethod to predict the clinical outcomes for unseen target and dis-ease pairs.\nWe designed three evaluation strategies to measure the prediction performance\nand benchmarked several commonly used machine learning classifiers together\nwith matrix and tensor factorisation methods. The result shows that\nincorporating knowledge graph embeddings significantly improves the prediction\naccuracy and that training tensor factorisation alongside a dense neural\nnetwork outperforms other methods. In summary, our framework combines two\nactively studied machine learning approaches to disease target identification,\ntensor factorisation and knowledge graph representation learning, which could\nbe a promising avenue for further exploration in data-driven drug discovery.",
          "link": "http://arxiv.org/abs/2105.10578",
          "publishedOn": "2021-07-23T02:00:32.338Z",
          "wordCount": 705,
          "title": "Predicting Potential Drug Targets Using Tensor Factorisation and Knowledge Graph Embeddings. (arXiv:2105.10578v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Swazinna_P/0/1/0/all/0/1\">Phillip Swazinna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udluft_S/0/1/0/all/0/1\">Steffen Udluft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Runkler_T/0/1/0/all/0/1\">Thomas Runkler</a>",
          "description": "State-of-the-art reinforcement learning algorithms mostly rely on being\nallowed to directly interact with their environment to collect millions of\nobservations. This makes it hard to transfer their success to industrial\ncontrol problems, where simulations are often very costly or do not exist, and\nexploring in the real environment can potentially lead to catastrophic events.\nRecently developed, model-free, offline RL algorithms, can learn from a single\ndataset (containing limited exploration) by mitigating extrapolation error in\nvalue functions. However, the robustness of the training process is still\ncomparatively low, a problem known from methods using value functions. To\nimprove robustness and stability of the learning process, we use dynamics\nmodels to assess policy performance instead of value functions, resulting in\nMOOSE (MOdel-based Offline policy Search with Ensembles), an algorithm which\nensures low model bias by keeping the policy within the support of the data. We\ncompare MOOSE with state-of-the-art model-free, offline RL algorithms { BRAC,}\nBEAR and BCQ on the Industrial Benchmark and MuJoCo continuous control tasks in\nterms of robust performance, and find that MOOSE outperforms its model-free\ncounterparts in almost all considered cases, often even by far.",
          "link": "http://arxiv.org/abs/2008.05533",
          "publishedOn": "2021-07-23T02:00:32.332Z",
          "wordCount": 678,
          "title": "Overcoming Model Bias for Robust Offline Deep Reinforcement Learning. (arXiv:2008.05533v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.03592",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Eskimez_S/0/1/0/all/0/1\">Sefik Emre Eskimez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">You Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_Z/0/1/0/all/0/1\">Zhiyao Duan</a>",
          "description": "Visual emotion expression plays an important role in audiovisual speech\ncommunication. In this work, we propose a novel approach to rendering visual\nemotion expression in speech-driven talking face generation. Specifically, we\ndesign an end-to-end talking face generation system that takes a speech\nutterance, a single face image, and a categorical emotion label as input to\nrender a talking face video synchronized with the speech and expressing the\nconditioned emotion. Objective evaluation on image quality, audiovisual\nsynchronization, and visual emotion expression shows that the proposed system\noutperforms a state-of-the-art baseline system. Subjective evaluation of visual\nemotion expression and video realness also demonstrates the superiority of the\nproposed system. Furthermore, we conduct a human emotion recognition pilot\nstudy using generated videos with mismatched emotions among the audio and\nvisual modalities. Results show that humans respond to the visual modality more\nsignificantly than the audio modality on this task.",
          "link": "http://arxiv.org/abs/2008.03592",
          "publishedOn": "2021-07-23T02:00:32.325Z",
          "wordCount": 632,
          "title": "Speech Driven Talking Face Generation from a Single Image and an Emotion Condition. (arXiv:2008.03592v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_F/0/1/0/all/0/1\">Fardin Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabanpour_J/0/1/0/all/0/1\">Javad Shabanpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyraghi_S/0/1/0/all/0/1\">Sina Beyraghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleimani_H/0/1/0/all/0/1\">Hossein Soleimani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oraizi_H/0/1/0/all/0/1\">Homayoon Oraizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleimani_M/0/1/0/all/0/1\">Mohammad Soleimani</a>",
          "description": "Compared to the conventional metasurface design, machine learning-based\nmethods have recently created an inspiring platform for an inverse realization\nof the metasurfaces. Here, we have used the Deep Neural Network (DNN) for the\ngeneration of desired output unit cell structures in an ultra-wide working\nfrequency band for both TE and TM polarized waves. To automatically generate\nmetasurfaces in a wide range of working frequencies from 4 to 45 GHz, we\ndeliberately design an 8 ring-shaped pattern in such a way that the unit-cells\ngenerated in the dataset can produce single or multiple notches in the desired\nworking frequency band. Compared to the general approach, whereby the final\nmetasurface structure may be formed by any randomly distributed \"0\" and \"1\", we\npropose here a restricted output structure. By restricting the output, the\nnumber of calculations will be reduced and the learning speed will be\nincreased. Moreover, we have shown that the accuracy of the network reaches\n91\\%. Obtaining the final unit cell directly without any time-consuming\noptimization algorithms for both TE and TM polarized waves, and high average\naccuracy, promises an effective strategy for the metasurface design; thus, the\ndesigner is required only to focus on the design goal.",
          "link": "http://arxiv.org/abs/2105.08508",
          "publishedOn": "2021-07-23T02:00:32.307Z",
          "wordCount": 673,
          "title": "A deep learning approach for inverse design of the metasurface for dual-polarized waves. (arXiv:2105.08508v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.01229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barros_C/0/1/0/all/0/1\">Claudio D. T. Barros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendonca_M/0/1/0/all/0/1\">Matheus R. F. Mendon&#xe7;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieira_A/0/1/0/all/0/1\">Alex B. Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziviani_A/0/1/0/all/0/1\">Artur Ziviani</a>",
          "description": "Embedding static graphs in low-dimensional vector spaces plays a key role in\nnetwork analytics and inference, supporting applications like node\nclassification, link prediction, and graph visualization. However, many\nreal-world networks present dynamic behavior, including topological evolution,\nfeature evolution, and diffusion. Therefore, several methods for embedding\ndynamic graphs have been proposed to learn network representations over time,\nfacing novel challenges, such as time-domain modeling, temporal features to be\ncaptured, and the temporal granularity to be embedded. In this survey, we\noverview dynamic graph embedding, discussing its fundamentals and the recent\nadvances developed so far. We introduce the formal definition of dynamic graph\nembedding, focusing on the problem setting and introducing a novel taxonomy for\ndynamic graph embedding input and output. We further explore different dynamic\nbehaviors that may be encompassed by embeddings, classifying by topological\nevolution, feature evolution, and processes on networks. Afterward, we describe\nexisting techniques and propose a taxonomy for dynamic graph embedding\ntechniques based on algorithmic approaches, from matrix and tensor\nfactorization to deep learning, random walks, and temporal point processes. We\nalso elucidate main applications, including dynamic link prediction, anomaly\ndetection, and diffusion prediction, and we further state some promising\nresearch directions in the area.",
          "link": "http://arxiv.org/abs/2101.01229",
          "publishedOn": "2021-07-23T02:00:32.300Z",
          "wordCount": 680,
          "title": "A Survey on Embedding Dynamic Graphs. (arXiv:2101.01229v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yiu_S/0/1/0/all/0/1\">Siu Ming Yiu</a>",
          "description": "Graphs have been widely used in data mining and machine learning due to their\nunique representation of real-world objects and their interactions. As graphs\nare getting bigger and bigger nowadays, it is common to see their subgraphs\nseparately collected and stored in multiple local systems. Therefore, it is\nnatural to consider the subgraph federated learning setting, where each local\nsystem holding a small subgraph that may be biased from the distribution of the\nwhole graph. Hence, the subgraph federated learning aims to collaboratively\ntrain a powerful and generalizable graph mining model without directly sharing\ntheir graph data. In this work, towards the novel yet realistic setting of\nsubgraph federated learning, we propose two major techniques: (1) FedSage,\nwhich trains a GraphSage model based on FedAvg to integrate node features, link\nstructures, and task labels on multiple local subgraphs; (2) FedSage+, which\ntrains a missing neighbor generator along FedSage to deal with missing links\nacross local subgraphs. Empirical results on four real-world graph datasets\nwith synthesized subgraph federated learning settings demonstrate the\neffectiveness and efficiency of our proposed techniques. At the same time,\nconsistent theoretical implications are made towards their generalization\nability on the global graphs.",
          "link": "http://arxiv.org/abs/2106.13430",
          "publishedOn": "2021-07-23T02:00:32.292Z",
          "wordCount": 663,
          "title": "Subgraph Federated Learning with Missing Neighbor Generation. (arXiv:2106.13430v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1\">Anand Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1\">Minh Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitehill_J/0/1/0/all/0/1\">Jacob Whitehill</a>",
          "description": "For the task of face verification, we explore the utility of harnessing\nauxiliary facial emotion labels to impose explicit geometric constraints on the\nembedding space when training deep embedding models. We introduce several novel\nloss functions that, in conjunction with a standard Triplet Loss [43], or\nArcFace loss [10], provide geometric constraints on the embedding space; the\nlabels for our loss functions can be provided using either manually annotated\nor automatically detected auxiliary emotion labels. Our method is implemented\npurely in terms of the loss function and does not require any changes to the\nneural network backbone of the embedding function.",
          "link": "http://arxiv.org/abs/2103.03862",
          "publishedOn": "2021-07-23T02:00:32.286Z",
          "wordCount": 593,
          "title": "Harnessing Geometric Constraints from Emotion Labels to improve Face Verification. (arXiv:2103.03862v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perrier_E/0/1/0/all/0/1\">Elija Perrier</a>",
          "description": "In this paper, we inaugurate the field of quantum fair machine learning. We\nundertake a comparative analysis of differences and similarities between\nclassical and quantum fair machine learning algorithms, specifying how the\nunique features of quantum computation alter measures, metrics and remediation\nstrategies when quantum algorithms are subject to fairness constraints. We\npresent the first results in quantum fair machine learning by demonstrating the\nuse of Grover's search algorithm to satisfy statistical parity constraints\nimposed on quantum algorithms. We provide lower-bounds on iterations needed to\nachieve such statistical parity within $\\epsilon$-tolerance. We extend\ncanonical Lipschitz-conditioned individual fairness criteria to the quantum\nsetting using quantum metrics. We examine the consequences for typical measures\nof fairness in machine learning context when quantum information processing and\nquantum data are involved. Finally, we propose open questions and research\nprogrammes for this new field of interest to researchers in computer science,\nethics and quantum computation.",
          "link": "http://arxiv.org/abs/2102.00753",
          "publishedOn": "2021-07-23T02:00:32.268Z",
          "wordCount": 595,
          "title": "Quantum Fair Machine Learning. (arXiv:2102.00753v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10599",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barati_R/0/1/0/all/0/1\">Ramin Barati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safabakhsh_R/0/1/0/all/0/1\">Reza Safabakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmati_M/0/1/0/all/0/1\">Mohammad Rahmati</a>",
          "description": "In this paper, we study the adversarial examples existence and adversarial\ntraining from the standpoint of convergence and provide evidence that pointwise\nconvergence in ANNs can explain these observations. The main contribution of\nour proposal is that it relates the objective of the evasion attacks and\nadversarial training with concepts already defined in learning theory. Also, we\nextend and unify some of the other proposals in the literature and provide\nalternative explanations on the observations made in those proposals. Through\ndifferent experiments, we demonstrate that the framework is valuable in the\nstudy of the phenomenon and is applicable to real-world problems.",
          "link": "http://arxiv.org/abs/2107.10599",
          "publishedOn": "2021-07-23T02:00:32.256Z",
          "wordCount": 550,
          "title": "Towards Explaining Adversarial Examples Phenomenon in Artificial Neural Networks. (arXiv:2107.10599v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hanyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1\">Mirela Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capecci_D/0/1/0/all/0/1\">Daniel Capecci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giovanini_L/0/1/0/all/0/1\">Luiz Giovanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czech_L/0/1/0/all/0/1\">Lauren Czech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_J/0/1/0/all/0/1\">Juliana Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1\">Daniela Oliveira</a>",
          "description": "Phishing and disinformation are popular social engineering attacks with\nattackers invariably applying influence cues in texts to make them more\nappealing to users. We introduce Lumen, a learning-based framework that exposes\ninfluence cues in text: (i) persuasion, (ii) framing, (iii) emotion, (iv)\nobjectivity/subjectivity, (v) guilt/blame, and (vi) use of emphasis. Lumen was\ntrained with a newly developed dataset of 3K texts comprised of disinformation,\nphishing, hyperpartisan news, and mainstream news. Evaluation of Lumen in\ncomparison to other learning models showed that Lumen and LSTM presented the\nbest F1-micro score, but Lumen yielded better interpretability. Our results\nhighlight the promise of ML to expose influence cues in text, towards the goal\nof application in automatic labeling tools to improve the accuracy of\nhuman-based detection and reduce the likelihood of users falling for deceptive\nonline content.",
          "link": "http://arxiv.org/abs/2107.10655",
          "publishedOn": "2021-07-23T02:00:32.250Z",
          "wordCount": 586,
          "title": "Lumen: A Machine Learning Framework to Expose Influence Cues in Text. (arXiv:2107.10655v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2008.01559",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Krishnamurthy_V/0/1/0/all/0/1\">Vikram Krishnamurthy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pattanayak_K/0/1/0/all/0/1\">Kunal Pattanayak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gogineni_S/0/1/0/all/0/1\">Sandeep Gogineni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_B/0/1/0/all/0/1\">Bosung Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rangaswamy_M/0/1/0/all/0/1\">Muralidhar Rangaswamy</a>",
          "description": "This paper considers three inter-related adversarial inference problems\ninvolving cognitive radars. We first discuss inverse tracking of the radar to\nestimate the adversary's estimate of us based on the radar's actions and\ncalibrate the radar's sensing accuracy. Second, using revealed preference from\nmicroeconomics, we formulate a non-parametric test to identify if the cognitive\nradar is a constrained utility maximizer with signal processing constraints. We\nconsider two radar functionalities, namely, beam allocation and waveform\ndesign, with respect to which the cognitive radar is assumed to maximize its\nutility and construct a set-valued estimator for the radar's utility function.\nFinally, we discuss how to engineer interference at the physical layer level to\nconfuse the radar which forces it to change its transmit waveform. The levels\nof abstraction range from smart interference design based on Wiener filters (at\nthe pulse/waveform level), inverse Kalman filters at the tracking level and\nrevealed preferences for identifying utility maximization at the systems level.",
          "link": "http://arxiv.org/abs/2008.01559",
          "publishedOn": "2021-07-23T02:00:32.243Z",
          "wordCount": 628,
          "title": "Adversarial Radar Inference: Inverse Tracking, Identifying Cognition and Designing Smart Interference. (arXiv:2008.01559v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.07093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gondal_M/0/1/0/all/0/1\">Muhammad Waleed Gondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Shruti Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_N/0/1/0/all/0/1\">Nasim Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuthrich_M/0/1/0/all/0/1\">Manuel W&#xfc;thrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "Meta-learning algorithms adapt quickly to new tasks that are drawn from the\nsame task distribution as the training tasks. The mechanism leading to fast\nadaptation is the conditioning of a downstream predictive model on the inferred\nrepresentation of the task's underlying data generative process, or\n\\emph{function}. This \\emph{meta-representation}, which is computed from a few\nobserved examples of the underlying function, is learned jointly with the\npredictive model. In this work, we study the implications of this joint\ntraining on the transferability of the meta-representations. Our goal is to\nlearn meta-representations that are robust to noise in the data and facilitate\nsolving a wide range of downstream tasks that share the same underlying\nfunctions. To this end, we propose a decoupled encoder-decoder approach to\nsupervised meta-learning, where the encoder is trained with a contrastive\nobjective to find a good representation of the underlying function. In\nparticular, our training scheme is driven by the self-supervision signal\nindicating whether two sets of examples stem from the same function. Our\nexperiments on a number of synthetic and real-world datasets show that the\nrepresentations we obtain outperform strong baselines in terms of downstream\nperformance and noise robustness, even when these baselines are trained in an\nend-to-end manner.",
          "link": "http://arxiv.org/abs/2010.07093",
          "publishedOn": "2021-07-23T02:00:32.236Z",
          "wordCount": 683,
          "title": "Function Contrastive Learning of Transferable Meta-Representations. (arXiv:2010.07093v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10624",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1\">Pavlo Molchanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_J/0/1/0/all/0/1\">Jimmy Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongxu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fusi_N/0/1/0/all/0/1\">Nicolo Fusi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahdat_A/0/1/0/all/0/1\">Arash Vahdat</a>",
          "description": "Given a trained network, how can we accelerate it to meet efficiency needs\nfor deployment on particular hardware? The commonly used hardware-aware network\ncompression techniques address this question with pruning, kernel fusion,\nquantization and lowering precision. However, these approaches do not change\nthe underlying network operations. In this paper, we propose hardware-aware\nnetwork transformation (HANT), which accelerates a network by replacing\ninefficient operations with more efficient alternatives using a neural\narchitecture search like approach. HANT tackles the problem in two phase: In\nthe first phase, a large number of alternative operations per every layer of\nthe teacher model is trained using layer-wise feature map distillation. In the\nsecond phase, the combinatorial selection of efficient operations is relaxed to\nan integer optimization problem that can be solved in a few seconds. We extend\nHANT with kernel fusion and quantization to improve throughput even further.\nOur experimental results on accelerating the EfficientNet family show that HANT\ncan accelerate them by up to 3.6x with <0.4% drop in the top-1 accuracy on the\nImageNet dataset. When comparing the same latency level, HANT can accelerate\nEfficientNet-B4 to the same latency as EfficientNet-B1 while having 3% higher\naccuracy. We examine a large pool of operations, up to 197 per layer, and we\nprovide insights into the selected operations and final architectures.",
          "link": "http://arxiv.org/abs/2107.10624",
          "publishedOn": "2021-07-23T02:00:32.226Z",
          "wordCount": 656,
          "title": "HANT: Hardware-Aware Network Transformation. (arXiv:2107.10624v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaur_R/0/1/0/all/0/1\">Rajvir Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginige_J/0/1/0/all/0/1\">Jeewani Anupama Ginige</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obst_O/0/1/0/all/0/1\">Oliver Obst</a>",
          "description": "Codification of free-text clinical narratives have long been recognised to be\nbeneficial for secondary uses such as funding, insurance claim processing and\nresearch. The current scenario of assigning codes is a manual process which is\nvery expensive, time-consuming and error prone. In recent years, many\nresearchers have studied the use of Natural Language Processing (NLP), related\nMachine Learning (ML) and Deep Learning (DL) methods and techniques to resolve\nthe problem of manual coding of clinical narratives and to assist human coders\nto assign clinical codes more accurately and efficiently. This systematic\nliterature review provides a comprehensive overview of automated clinical\ncoding systems that utilises appropriate NLP, ML and DL methods and techniques\nto assign ICD codes to discharge summaries. We have followed the Preferred\nReporting Items for Systematic Reviews and Meta-Analyses(PRISMA) guidelines and\nconducted a comprehensive search of publications from January, 2010 to December\n2020 in four academic databases- PubMed, ScienceDirect, Association for\nComputing Machinery(ACM) Digital Library, and the Association for Computational\nLinguistics(ACL) Anthology. We reviewed 7,556 publications; 38 met the\ninclusion criteria. This review identified: datasets having discharge\nsummaries; NLP techniques along with some other data extraction processes,\ndifferent feature extraction and embedding techniques. To measure the\nperformance of classification methods, different evaluation metrics are used.\nLastly, future research directions are provided to scholars who are interested\nin automated ICD code assignment. Efforts are still required to improve ICD\ncode prediction accuracy, availability of large-scale de-identified clinical\ncorpora with the latest version of the classification system. This can be a\nplatform to guide and share knowledge with the less experienced coders and\nresearchers.",
          "link": "http://arxiv.org/abs/2107.10652",
          "publishedOn": "2021-07-23T02:00:32.219Z",
          "wordCount": 725,
          "title": "A Systematic Literature Review of Automated ICD Coding and Classification Systems using Discharge Summaries. (arXiv:2107.10652v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.08488",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Matsubara_T/0/1/0/all/0/1\">Takuo Matsubara</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Oates_C/0/1/0/all/0/1\">Chris J. Oates</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Briol_F/0/1/0/all/0/1\">Fran&#xe7;ois-Xavier Briol</a>",
          "description": "Bayesian neural networks attempt to combine the strong predictive performance\nof neural networks with formal quantification of uncertainty associated with\nthe predictive output in the Bayesian framework. However, it remains unclear\nhow to endow the parameters of the network with a prior distribution that is\nmeaningful when lifted into the output space of the network. A possible\nsolution is proposed that enables the user to posit an appropriate Gaussian\nprocess covariance function for the task at hand. Our approach constructs a\nprior distribution for the parameters of the network, called a ridgelet prior,\nthat approximates the posited Gaussian process in the output space of the\nnetwork. In contrast to existing work on the connection between neural networks\nand Gaussian processes, our analysis is non-asymptotic, with finite sample-size\nerror bounds provided. This establishes the universality property that a\nBayesian neural network can approximate any Gaussian process whose covariance\nfunction is sufficiently regular. Our experimental assessment is limited to a\nproof-of-concept, where we demonstrate that the ridgelet prior can out-perform\nan unstructured prior on regression problems for which a suitable Gaussian\nprocess prior can be provided.",
          "link": "http://arxiv.org/abs/2010.08488",
          "publishedOn": "2021-07-23T02:00:32.202Z",
          "wordCount": 655,
          "title": "The Ridgelet Prior: A Covariance Function Approach to Prior Specification for Bayesian Neural Networks. (arXiv:2010.08488v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bonati_L/0/1/0/all/0/1\">Leonardo Bonati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DOro_S/0/1/0/all/0/1\">Salvatore D&#x27;Oro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polese_M/0/1/0/all/0/1\">Michele Polese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basagni_S/0/1/0/all/0/1\">Stefano Basagni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melodia_T/0/1/0/all/0/1\">Tommaso Melodia</a>",
          "description": "Next Generation (NextG) cellular networks will be natively cloud-based and\nbuilt upon programmable, virtualized, and disaggregated architectures. The\nseparation of control functions from the hardware fabric and the introduction\nof standardized control interfaces will enable the definition of custom\nclosed-control loops, which will ultimately enable embedded intelligence and\nreal-time analytics, thus effectively realizing the vision of autonomous and\nself-optimizing networks. This article explores the disaggregated network\narchitecture proposed by the O-RAN Alliance as a key enabler of NextG networks.\nWithin this architectural context, we discuss the potential, the challenges,\nand the limitations of data-driven optimization approaches to network control\nover different timescales. We also present the first large-scale integration of\nO-RAN-compliant software components with an open-source full-stack softwarized\ncellular network. Experiments conducted on Colosseum, the world's largest\nwireless network emulator, demonstrate closed-loop integration of real-time\nanalytics and control through deep reinforcement learning agents. We also show\nthe feasibility of Radio Access Network (RAN) control through xApps running on\nthe near real-time RAN Intelligent Controller, to optimize the scheduling\npolicies of co-existing network slices, leveraging the O-RAN open interfaces to\ncollect data at the edge of the network.",
          "link": "http://arxiv.org/abs/2012.01263",
          "publishedOn": "2021-07-23T02:00:32.195Z",
          "wordCount": 659,
          "title": "Intelligence and Learning in O-RAN for Data-driven NextG Cellular Networks. (arXiv:2012.01263v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Artelt_A/0/1/0/all/0/1\">Andr&#xe9; Artelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaquet_V/0/1/0/all/0/1\">Valerie Vaquet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velioglu_R/0/1/0/all/0/1\">Riza Velioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinder_F/0/1/0/all/0/1\">Fabian Hinder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brinkrolf_J/0/1/0/all/0/1\">Johannes Brinkrolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schilling_M/0/1/0/all/0/1\">Malte Schilling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1\">Barbara Hammer</a>",
          "description": "Transparency is a fundamental requirement for decision making systems when\nthese should be deployed in the real world. It is usually achieved by providing\nexplanations of the system's behavior. A prominent and intuitive type of\nexplanations are counterfactual explanations. Counterfactual explanations\nexplain a behavior to the user by proposing actions -- as changes to the input\n-- that would cause a different (specified) behavior of the system. However,\nsuch explanation methods can be unstable with respect to small changes to the\ninput -- i.e. even a small change in the input can lead to huge or arbitrary\nchanges in the output and of the explanation. This could be problematic for\ncounterfactual explanations, as two similar individuals might get very\ndifferent explanations. Even worse, if the recommended actions differ\nconsiderably in their complexity, one would consider such unstable\n(counterfactual) explanations as individually unfair.\n\nIn this work, we formally and empirically study the robustness of\ncounterfactual explanations in general, as well as under different models and\ndifferent kinds of perturbations. Furthermore, we propose that plausible\ncounterfactual explanations can be used instead of closest counterfactual\nexplanations to improve the robustness and consequently the individual fairness\nof counterfactual explanations.",
          "link": "http://arxiv.org/abs/2103.02354",
          "publishedOn": "2021-07-23T02:00:32.188Z",
          "wordCount": 681,
          "title": "Evaluating Robustness of Counterfactual Explanations. (arXiv:2103.02354v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sekhari_A/0/1/0/all/0/1\">Ayush Sekhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acharya_J/0/1/0/all/0/1\">Jayadev Acharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_G/0/1/0/all/0/1\">Gautam Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1\">Ananda Theertha Suresh</a>",
          "description": "We study the problem of unlearning datapoints from a learnt model. The\nlearner first receives a dataset $S$ drawn i.i.d. from an unknown distribution,\nand outputs a model $\\widehat{w}$ that performs well on unseen samples from the\nsame distribution. However, at some point in the future, any training datapoint\n$z \\in S$ can request to be unlearned, thus prompting the learner to modify its\noutput model while still ensuring the same accuracy guarantees. We initiate a\nrigorous study of generalization in machine unlearning, where the goal is to\nperform well on previously unseen datapoints. Our focus is on both\ncomputational and storage complexity.\n\nFor the setting of convex losses, we provide an unlearning algorithm that can\nunlearn up to $O(n/d^{1/4})$ samples, where $d$ is the problem dimension. In\ncomparison, in general, differentially private learning (which implies\nunlearning) only guarantees deletion of $O(n/d^{1/2})$ samples. This\ndemonstrates a novel separation between differential privacy and machine\nunlearning.",
          "link": "http://arxiv.org/abs/2103.03279",
          "publishedOn": "2021-07-23T02:00:32.172Z",
          "wordCount": 623,
          "title": "Remember What You Want to Forget: Algorithms for Machine Unlearning. (arXiv:2103.03279v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gopalan_A/0/1/0/all/0/1\">Aditya Gopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saligrama_V/0/1/0/all/0/1\">Venkatesh Saligrama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Braghadeesh Lakshminarayanan</a>",
          "description": "Detecting abrupt changes in temporal behavior patterns is of interest in many\nindustrial and security applications. Abrupt changes are often local and\nobservable primarily through a well-aligned sensing action (e.g., a camera with\na narrow field-of-view). Due to resource constraints, continuous monitoring of\nall of the sensors is impractical. We propose the bandit quickest changepoint\ndetection framework as a means of balancing sensing cost with detection delay.\nIn this framework, sensing actions (or sensors) are sequentially chosen, and\nonly measurements corresponding to chosen actions are observed. We derive an\ninformation-theoretic lower bound on the detection delay for a general class of\nfinitely parameterized probability distributions. We then propose a\ncomputationally efficient online sensing scheme, which seamlessly balances the\nneed for exploration of different sensing options with exploitation of querying\ninformative actions. We derive expected delay bounds for the proposed scheme\nand show that these bounds match our information-theoretic lower bounds at low\nfalse alarm rates, establishing optimality of the proposed method. We then\nperform a number of experiments on synthetic and real datasets demonstrating\nthe efficacy of our proposed method.",
          "link": "http://arxiv.org/abs/2107.10492",
          "publishedOn": "2021-07-23T02:00:32.166Z",
          "wordCount": 615,
          "title": "Bandit Quickest Changepoint Detection. (arXiv:2107.10492v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_L/0/1/0/all/0/1\">Lucas F. F. Cardoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_V/0/1/0/all/0/1\">Vitor C. A. Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frances_R/0/1/0/all/0/1\">Regiane S. Kawasaki Franc&#xea;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prudencio_R/0/1/0/all/0/1\">Ricardo B. C. Prud&#xea;ncio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alves_R/0/1/0/all/0/1\">Ronnie C. O. Alves</a>",
          "description": "The classification experiments covered by machine learning (ML) are composed\nby two important parts: the data and the algorithm. As they are a fundamental\npart of the problem, both must be considered when evaluating a model's\nperformance against a benchmark. The best classifiers need robust benchmarks to\nbe properly evaluated. For this, gold standard benchmarks such as OpenML-CC18\nare used. However, data complexity is commonly not considered along with the\nmodel during a performance evaluation. Recent studies employ Item Response\nTheory (IRT) as a new approach to evaluating datasets and algorithms, capable\nof evaluating both simultaneously. This work presents a new evaluation\nmethodology based on IRT and Glicko-2, jointly with the decodIRT tool developed\nto guide the estimation of IRT in ML. It explores the IRT as a tool to evaluate\nthe OpenML-CC18 benchmark for its algorithmic evaluation capability and checks\nif there is a subset of datasets more efficient than the original benchmark.\nSeveral classifiers, from classics to ensemble, are also evaluated using the\nIRT models. The Glicko-2 rating system was applied together with IRT to\nsummarize the innate ability and classifiers performance. It was noted that not\nall OpenML-CC18 datasets are really useful for evaluating algorithms, where\nonly 10% were rated as being really difficult. Furthermore, it was verified the\nexistence of a more efficient subset containing only 50% of the original size.\nWhile Randon Forest was singled out as the algorithm with the best innate\nability.",
          "link": "http://arxiv.org/abs/2107.07451",
          "publishedOn": "2021-07-23T02:00:32.160Z",
          "wordCount": 711,
          "title": "Data vs classifiers, who wins?. (arXiv:2107.07451v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brouillard_P/0/1/0/all/0/1\">Philippe Brouillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taslakian_P/0/1/0/all/0/1\">Perouz Taslakian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacoste_A/0/1/0/all/0/1\">Alexandre Lacoste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lachapelle_S/0/1/0/all/0/1\">Sebastien Lachapelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1\">Alexandre Drouin</a>",
          "description": "Causal discovery from observational data is a challenging task to which an\nexact solution cannot always be identified. Under assumptions about the\ndata-generative process, the causal graph can often be identified up to an\nequivalence class. Proposing new realistic assumptions to circumscribe such\nequivalence classes is an active field of research. In this work, we propose a\nnew set of assumptions that constrain possible causal relationships based on\nthe nature of the variables. We thus introduce typed directed acyclic graphs,\nin which variable types are used to determine the validity of causal\nrelationships. We demonstrate, both theoretically and empirically, that the\nproposed assumptions can result in significant gains in the identification of\nthe causal graph.",
          "link": "http://arxiv.org/abs/2107.10703",
          "publishedOn": "2021-07-23T02:00:32.153Z",
          "wordCount": 582,
          "title": "Typing assumptions improve identification in causal discovery. (arXiv:2107.10703v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Existing multilingual machine translation approaches mainly focus on\nEnglish-centric directions, while the non-English directions still lag behind.\nIn this work, we aim to build a many-to-many translation system with an\nemphasis on the quality of non-English language directions. Our intuition is\nbased on the hypothesis that a universal cross-language representation leads to\nbetter multilingual translation performance. To this end, we propose mRASP2, a\ntraining method to obtain a single unified multilingual translation model.\nmRASP2 is empowered by two techniques: a) a contrastive learning scheme to\nclose the gap among representations of different languages, and b) data\naugmentation on both multiple parallel and monolingual data to further align\ntoken representations. For English-centric directions, mRASP2 outperforms\nexisting best unified model and achieves competitive or even better performance\nthan the pre-trained and fine-tuned model mBART on tens of WMT's translation\ndirections. For non-English directions, mRASP2 achieves an improvement of\naverage 10+ BLEU compared with the multilingual Transformer baseline. Code,\ndata and trained models are available at https://github.com/PANXiao1994/mRASP2.",
          "link": "http://arxiv.org/abs/2105.09501",
          "publishedOn": "2021-07-23T02:00:32.146Z",
          "wordCount": 647,
          "title": "Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. (arXiv:2105.09501v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.13520",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jiahao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lijie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zejun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1\">Miao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinhui Xu</a>",
          "description": "(Gradient) Expectation Maximization (EM) is a widely used algorithm for\nestimating the maximum likelihood of mixture models or incomplete data\nproblems. A major challenge facing this popular technique is how to effectively\npreserve the privacy of sensitive data. Previous research on this problem has\nalready lead to the discovery of some Differentially Private (DP) algorithms\nfor (Gradient) EM. However, unlike in the non-private case, existing techniques\nare not yet able to provide finite sample statistical guarantees. To address\nthis issue, we propose in this paper the first DP version of (Gradient) EM\nalgorithm with statistical guarantees. Moreover, we apply our general framework\nto three canonical models: Gaussian Mixture Model (GMM), Mixture of Regressions\nModel (MRM) and Linear Regression with Missing Covariates (RMC). Specifically,\nfor GMM in the DP model, our estimation error is near optimal in some cases.\nFor the other two models, we provide the first finite sample statistical\nguarantees. Our theory is supported by thorough numerical experiments.",
          "link": "http://arxiv.org/abs/2010.13520",
          "publishedOn": "2021-07-23T02:00:32.139Z",
          "wordCount": 642,
          "title": "Differentially Private (Gradient) Expectation Maximization Algorithm with Statistical Guarantees. (arXiv:2010.13520v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaohu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1\">Guijian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wen Yao</a>",
          "description": "The surrogate model-based uncertainty quantification method has drawn a lot\nof attention in recent years. Both the polynomial chaos expansion (PCE) and the\ndeep learning (DL) are powerful methods for building a surrogate model.\nHowever, the PCE needs to increase the expansion order to improve the accuracy\nof the surrogate model, which causes more labeled data to solve the expansion\ncoefficients, and the DL also needs a lot of labeled data to train the neural\nnetwork model. This paper proposes a deep arbitrary polynomial chaos expansion\n(Deep aPCE) method to improve the balance between surrogate model accuracy and\ntraining data cost. On the one hand, the multilayer perceptron (MLP) model is\nused to solve the adaptive expansion coefficients of arbitrary polynomial chaos\nexpansion, which can improve the Deep aPCE model accuracy with lower expansion\norder. On the other hand, the adaptive arbitrary polynomial chaos expansion's\nproperties are used to construct the MLP training cost function based on only a\nsmall amount of labeled data and a large scale of non-labeled data, which can\nsignificantly reduce the training data cost. Four numerical examples and an\nactual engineering problem are used to verify the effectiveness of the Deep\naPCE method.",
          "link": "http://arxiv.org/abs/2107.10428",
          "publishedOn": "2021-07-23T02:00:32.130Z",
          "wordCount": 631,
          "title": "Mini-data-driven Deep Arbitrary Polynomial Chaos Expansion for Uncertainty Quantification. (arXiv:2107.10428v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lippe_P/0/1/0/all/0/1\">Phillip Lippe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1\">Taco Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1\">Efstratios Gavves</a>",
          "description": "Learning the structure of a causal graphical model using both observational\nand interventional data is a fundamental problem in many scientific fields. A\npromising direction is continuous optimization for score-based methods, which\nefficiently learn the causal graph in a data-driven manner. However, to date,\nthose methods require constrained optimization to enforce acyclicity or lack\nconvergence guarantees. In this paper, we present ENCO, an efficient structure\nlearning method for directed, acyclic causal graphs leveraging observational\nand interventional data. ENCO formulates the graph search as an optimization of\nindependent edge likelihoods, with the edge orientation being modeled as a\nseparate parameter. Consequently, we can provide convergence guarantees of ENCO\nunder mild conditions without constraining the score function with respect to\nacyclicity. In experiments, we show that ENCO can efficiently recover graphs\nwith hundreds of nodes, an order of magnitude larger than what was previously\npossible, while handling deterministic variables and latent confounders.",
          "link": "http://arxiv.org/abs/2107.10483",
          "publishedOn": "2021-07-23T02:00:32.123Z",
          "wordCount": 598,
          "title": "Efficient Neural Causal Discovery without Acyclicity Constraints. (arXiv:2107.10483v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Escudero_Arnanz_O/0/1/0/all/0/1\">&#xd3;scar Escudero-Arnanz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Alvarez_J/0/1/0/all/0/1\">Joaqu&#xed;n Rodr&#xed;guez-&#xc1;lvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikalsen_K/0/1/0/all/0/1\">Karl &#xd8;yvind Mikalsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenssen_R/0/1/0/all/0/1\">Robert Jenssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soguero_Ruiz_C/0/1/0/all/0/1\">Cristina Soguero-Ruiz</a>",
          "description": "The acquisition of Antimicrobial Multidrug Resistance (AMR) in patients\nadmitted to the Intensive Care Units (ICU) is a major global concern. This\nstudy analyses data in the form of multivariate time series (MTS) from 3476\npatients recorded at the ICU of University Hospital of Fuenlabrada (Madrid)\nfrom 2004 to 2020. 18\\% of the patients acquired AMR during their stay in the\nICU. The goal of this paper is an early prediction of the development of AMR.\nTowards that end, we leverage the time-series cluster kernel (TCK) to learn\nsimilarities between MTS. To evaluate the effectiveness of TCK as a kernel, we\napplied several dimensionality reduction techniques for visualization and\nclassification tasks. The experimental results show that TCK allows identifying\na group of patients that acquire the AMR during the first 48 hours of their ICU\nstay, and it also provides good classification capabilities.",
          "link": "http://arxiv.org/abs/2107.10398",
          "publishedOn": "2021-07-23T02:00:32.098Z",
          "wordCount": 621,
          "title": "On the Use of Time Series Kernel and Dimensionality Reduction to Identify the Acquisition of Antimicrobial Multidrug Resistance in the Intensive Care Unit. (arXiv:2107.10398v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minghan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1\">Maani Ghaffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Huei Peng</a>",
          "description": "This paper proposes a correspondence-free method for point cloud rotational\nregistration. We learn an embedding for each point cloud in a feature space\nthat preserves the SO(3)-equivariance property, enabled by recent developments\nin equivariant neural networks. The proposed shape registration method achieves\nthree major advantages through combining equivariant feature learning with\nimplicit shape models. First, the necessity of data association is removed\nbecause of the permutation-invariant property in network architectures similar\nto PointNet. Second, the registration in feature space can be solved in\nclosed-form using Horn's method due to the SO(3)-equivariance property. Third,\nthe registration is robust to noise in the point cloud because of implicit\nshape learning. The experimental results show superior performance compared\nwith existing correspondence-free deep registration methods.",
          "link": "http://arxiv.org/abs/2107.10296",
          "publishedOn": "2021-07-23T02:00:32.069Z",
          "wordCount": 570,
          "title": "Correspondence-Free Point Cloud Registration with SO(3)-Equivariant Implicit Shape Representations. (arXiv:2107.10296v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10637",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Salimzianov_I/0/1/0/all/0/1\">Ilnar Salimzianov</a>",
          "description": "Mobile devices are transforming the way people interact with computers, and\nspeech interfaces to applications are ever more important. Automatic Speech\nRecognition systems recently published are very accurate, but often require\npowerful machinery (specialised Graphical Processing Units) for inference,\nwhich makes them impractical to run on commodity devices, especially in\nstreaming mode. Impressed by the accuracy of, but dissatisfied with the\ninference times of the baseline Kazakh ASR model of (Khassanov et al.,2021)\nwhen not using a GPU, we trained a new baseline acoustic model (on the same\ndataset as the aforementioned paper) and three language models for use with the\nCoqui STT framework. Results look promising, but further epochs of training and\nparameter sweeping or, alternatively, limiting the vocabulary that the ASR\nsystem must support, is needed to reach a production-level accuracy.",
          "link": "http://arxiv.org/abs/2107.10637",
          "publishedOn": "2021-07-23T02:00:32.051Z",
          "wordCount": 596,
          "title": "A baseline model for computationally inexpensive speech recognition for Kazakh using the Coqui STT framework. (arXiv:2107.10637v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2011.04144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1\">Arnab Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gayen_S/0/1/0/all/0/1\">Sutanu Gayen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1\">Eric Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinodchandran_N/0/1/0/all/0/1\">N. V. Vinodchandran</a>",
          "description": "We provide finite sample guarantees for the classical Chow-Liu algorithm\n(IEEE Trans.~Inform.~Theory, 1968) to learn a tree-structured graphical model\nof a distribution. For a distribution $P$ on $\\Sigma^n$ and a tree $T$ on $n$\nnodes, we say $T$ is an $\\varepsilon$-approximate tree for $P$ if there is a\n$T$-structured distribution $Q$ such that $D(P\\;||\\;Q)$ is at most\n$\\varepsilon$ more than the best possible tree-structured distribution for $P$.\nWe show that if $P$ itself is tree-structured, then the Chow-Liu algorithm with\nthe plug-in estimator for mutual information with $\\widetilde{O}(|\\Sigma|^3\nn\\varepsilon^{-1})$ i.i.d.~samples outputs an $\\varepsilon$-approximate tree\nfor $P$ with constant probability. In contrast, for a general $P$ (which may\nnot be tree-structured), $\\Omega(n^2\\varepsilon^{-2})$ samples are necessary to\nfind an $\\varepsilon$-approximate tree. Our upper bound is based on a new\nconditional independence tester that addresses an open problem posed by\nCanonne, Diakonikolas, Kane, and Stewart~(STOC, 2018): we prove that for three\nrandom variables $X,Y,Z$ each over $\\Sigma$, testing if $I(X; Y \\mid Z)$ is $0$\nor $\\geq \\varepsilon$ is possible with $\\widetilde{O}(|\\Sigma|^3/\\varepsilon)$\nsamples. Finally, we show that for a specific tree $T$, with $\\widetilde{O}\n(|\\Sigma|^2n\\varepsilon^{-1})$ samples from a distribution $P$ over $\\Sigma^n$,\none can efficiently learn the closest $T$-structured distribution in KL\ndivergence by applying the add-1 estimator at each node.",
          "link": "http://arxiv.org/abs/2011.04144",
          "publishedOn": "2021-07-23T02:00:32.033Z",
          "wordCount": 685,
          "title": "Near-Optimal Learning of Tree-Structured Distributions by Chow-Liu. (arXiv:2011.04144v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cordero_J/0/1/0/all/0/1\">Joaqu&#xed;n Cordero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolt_A/0/1/0/all/0/1\">Alfredo Bolt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valle_M/0/1/0/all/0/1\">Mauricio Valle</a>",
          "description": "Introduction: An important chain of supermarkets in the western zone of the\ncapital of Chile, needs to obtain key information to make decisions, this\ninformation is available in the databases but needs to be processed due to the\ncomplexity and quantity of information which becomes difficult to visualiz,.\nMethod: For this purpose, an algorithm was developed using artificial neural\nnetworks applying Kohonen's SOM method. To carry it out, certain key procedures\nmust be followed to develop it, such as data mining that will be responsible\nfor filtering and then use only the relevant data for market basket analysis.\nAfter filtering the information, the data must be prepared. After data\npreparation, we prepared the Python programming environment to adapt it to the\nsample data, then proceed to train the SOM with its parameters set after test\nresults. Result: the result of the SOM obtains the relationship between the\nproducts that were most purchased by positioning them topologically close, to\nform promotions, packs and bundles for the retail manager to take into\nconsideration, because these relationships were obtained as a result of the SOM\ntraining with the real transactions of the clients. Conclusion: Based on this,\nrecommendations on frequent shopping baskets have been made to the supermarket\nchain that provided the data used in the research",
          "link": "http://arxiv.org/abs/2107.10647",
          "publishedOn": "2021-07-23T02:00:32.017Z",
          "wordCount": 664,
          "title": "An\\'alisis de Canasta de mercado en supermercados mediante mapas auto-organizados. (arXiv:2107.10647v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rensonnet_G/0/1/0/all/0/1\">Gaetan Rensonnet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_L/0/1/0/all/0/1\">Louise Adam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macq_B/0/1/0/all/0/1\">Benoit Macq</a>",
          "description": "Deep neural networks (DNN) have an impressive ability to invert very complex\nmodels, i.e. to learn the generative parameters from a model's output. Once\ntrained, the forward pass of a DNN is often much faster than traditional,\noptimization-based methods used to solve inverse problems. This is however done\nat the cost of lower interpretability, a fundamental limitation in most medical\napplications. We propose an approach for solving general inverse problems which\ncombines the efficiency of DNN and the interpretability of traditional\nanalytical methods. The measurements are first projected onto a dense\ndictionary of model-based responses. The resulting sparse representation is\nthen fed to a DNN with an architecture driven by the problem's physics for fast\nparameter learning. Our method can handle generative forward models that are\ncostly to evaluate and exhibits similar performance in accuracy and computation\ntime as a fully-learned DNN, while maintaining high interpretability and being\neasier to train. Concrete results are shown on an example of model-based brain\nparameter estimation from magnetic resonance imaging (MRI).",
          "link": "http://arxiv.org/abs/2107.10657",
          "publishedOn": "2021-07-23T02:00:32.007Z",
          "wordCount": 639,
          "title": "Solving inverse problems with deep neural networks driven by sparse signal decomposition in a physics-based dictionary. (arXiv:2107.10657v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arjevani_Y/0/1/0/all/0/1\">Yossi Arjevani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Field_M/0/1/0/all/0/1\">Michael Field</a>",
          "description": "We study the optimization problem associated with fitting two-layer ReLU\nneural networks with respect to the squared loss, where labels are generated by\na target network. We make use of the rich symmetry structure to develop a novel\nset of tools for studying families of spurious minima. In contrast to existing\napproaches which operate in limiting regimes, our technique directly addresses\nthe nonconvex loss landscape for a finite number of inputs $d$ and neurons $k$,\nand provides analytic, rather than heuristic, information. In particular, we\nderive analytic estimates for the loss at different minima, and prove that\nmodulo $O(d^{-1/2})$-terms the Hessian spectrum concentrates near small\npositive constants, with the exception of $\\Theta(d)$ eigenvalues which grow\nlinearly with~$d$. We further show that the Hessian spectrum at global and\nspurious minima coincide to $O(d^{-1/2})$-order, thus challenging our ability\nto argue about statistical generalization through local curvature. Lastly, our\ntechnique provides the exact \\emph{fractional} dimensionality at which families\nof critical points turn from saddles into spurious minima. This makes possible\nthe study of the creation and the annihilation of spurious minima using\npowerful tools from equivariant bifurcation theory.",
          "link": "http://arxiv.org/abs/2107.10370",
          "publishedOn": "2021-07-23T02:00:31.988Z",
          "wordCount": 629,
          "title": "Analytic Study of Families of Spurious Minima in Two-Layer ReLU Neural Networks. (arXiv:2107.10370v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10471",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Watcharasupat_K/0/1/0/all/0/1\">Karn N. Watcharasupat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1\">Thi Ngoc Tho Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngoc Khanh Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_Z/0/1/0/all/0/1\">Zhen Jian Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jones_D/0/1/0/all/0/1\">Douglas L. Jones</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gan_W/0/1/0/all/0/1\">Woon Seng Gan</a>",
          "description": "The S{\\o}rensen--Dice Coefficient has recently seen rising popularity as a\nloss function (also known as Dice loss) due to its robustness in tasks where\nthe number of negative samples significantly exceeds that of positive samples,\nsuch as semantic segmentation, natural language processing, and sound event\ndetection. Conventional training of polyphonic sound event detection systems\nwith binary cross-entropy loss often results in suboptimal detection\nperformance as the training is often overwhelmed by updates from negative\nsamples. In this paper, we investigated the effect of the Dice loss, intra- and\ninter-modal transfer learning, data augmentation, and recording formats, on the\nperformance of polyphonic sound event detection systems with multichannel\ninputs. Our analysis showed that polyphonic sound event detection systems\ntrained with Dice loss consistently outperformed those trained with\ncross-entropy loss across different training settings and recording formats in\nterms of F1 score and error rate. We achieved further performance gains via the\nuse of transfer learning and an appropriate combination of different data\naugmentation techniques.",
          "link": "http://arxiv.org/abs/2107.10471",
          "publishedOn": "2021-07-23T02:00:31.978Z",
          "wordCount": 661,
          "title": "Improving Polyphonic Sound Event Detection on Multichannel Recordings with the S{\\o}rensen-Dice Coefficient Loss and Transfer Learning. (arXiv:2107.10471v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10607",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ibing_M/0/1/0/all/0/1\">Moritz Ibing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_I/0/1/0/all/0/1\">Isaak Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobbelt_L/0/1/0/all/0/1\">Leif Kobbelt</a>",
          "description": "Previous approaches to generate shapes in a 3D setting train a GAN on the\nlatent space of an autoencoder (AE). Even though this produces convincing\nresults, it has two major shortcomings. As the GAN is limited to reproduce the\ndataset the AE was trained on, we cannot reuse a trained AE for novel data.\nFurthermore, it is difficult to add spatial supervision into the generation\nprocess, as the AE only gives us a global representation. To remedy these\nissues, we propose to train the GAN on grids (i.e. each cell covers a part of a\nshape). In this representation each cell is equipped with a latent vector\nprovided by an AE. This localized representation enables more expressiveness\n(since the cell-based latent vectors can be combined in novel ways) as well as\nspatial control of the generation process (e.g. via bounding boxes). Our method\noutperforms the current state of the art on all established evaluation\nmeasures, proposed for quantitatively evaluating the generative capabilities of\nGANs. We show limitations of these measures and propose the adaptation of a\nrobust criterion from statistical analysis as an alternative.",
          "link": "http://arxiv.org/abs/2107.10607",
          "publishedOn": "2021-07-23T02:00:31.953Z",
          "wordCount": 626,
          "title": "3D Shape Generation with Grid-based Implicit Functions. (arXiv:2107.10607v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Toutiaee_M/0/1/0/all/0/1\">Mohammadhossein Toutiaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaochuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_Y/0/1/0/all/0/1\">Yogesh Chaudhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivaraja_S/0/1/0/all/0/1\">Shophine Sivaraja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataraj_A/0/1/0/all/0/1\">Aishwarya Venkataraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javeri_I/0/1/0/all/0/1\">Indrajeet Javeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_Y/0/1/0/all/0/1\">Yuan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arpinar_I/0/1/0/all/0/1\">Ismailcem Arpinar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazar_N/0/1/0/all/0/1\">Nicole Lazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1\">John Miller</a>",
          "description": "In this work, we study the pandemic course in the United States by\nconsidering national and state levels data. We propose and compare multiple\ntime-series prediction techniques which incorporate auxiliary variables. One\ntype of approach is based on spatio-temporal graph neural networks which\nforecast the pandemic course by utilizing a hybrid deep learning architecture\nand human mobility data. Nodes in this graph represent the state-level deaths\ndue to COVID-19, edges represent the human mobility trend and temporal edges\ncorrespond to node attributes across time. The second approach is based on a\nstatistical technique for COVID-19 mortality prediction in the United States\nthat uses the SARIMA model and eXogenous variables. We evaluate these\ntechniques on both state and national levels COVID-19 data in the United States\nand claim that the SARIMA and MCP models generated forecast values by the\neXogenous variables can enrich the underlying model to capture complexity in\nrespectively national and state levels data. We demonstrate significant\nenhancement in the forecasting accuracy for a COVID-19 dataset, with a maximum\nimprovement in forecasting accuracy by 64.58% and 59.18% (on average) over the\nGCN-LSTM model in the national level data, and 58.79% and 52.40% (on average)\nover the GCN-LSTM model in the state level data. Additionally, our proposed\nmodel outperforms a parallel study (AUG-NN) by 27.35% improvement of accuracy\non average.",
          "link": "http://arxiv.org/abs/2107.10397",
          "publishedOn": "2021-07-23T02:00:31.945Z",
          "wordCount": 707,
          "title": "Improving COVID-19 Forecasting using eXogenous Variables. (arXiv:2107.10397v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Olivares_C/0/1/0/all/0/1\">Carlos Olivares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_R/0/1/0/all/0/1\">Raziur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stankus_C/0/1/0/all/0/1\">Christopher Stankus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hampton_J/0/1/0/all/0/1\">Jade Hampton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zedwick_A/0/1/0/all/0/1\">Andrew Zedwick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1\">Moinuddin Ahmed</a>",
          "description": "Power device reliability is a major concern during operation under extreme\nenvironments, as doing so reduces the operational lifetime of any power system\nor sensing infrastructure. Due to a potential for system failure, devices must\nbe experimentally validated before implementation, which is expensive and\ntime-consuming. In this paper, we have utilized machine learning algorithms to\npredict device reliability, significantly reducing the need for conducting\nexperiments. To train the models, we have tested 224 power devices from 10\ndifferent manufacturers. First, we describe a method to process the data for\nmodeling purposes. Based on the in-house testing data, we implemented various\nML models and observed that computational models such as Gradient Boosting and\nLSTM encoder-decoder networks can predict power device failure with high\naccuracy.",
          "link": "http://arxiv.org/abs/2107.10292",
          "publishedOn": "2021-07-23T02:00:31.938Z",
          "wordCount": 584,
          "title": "Predicting Power Electronics Device Reliability under Extreme Conditions with Machine Learning Algorithms. (arXiv:2107.10292v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balali_A/0/1/0/all/0/1\">Ali Balali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadpour_M/0/1/0/all/0/1\">Masoud Asadpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafari_S/0/1/0/all/0/1\">Seyed Hossein Jafari</a>",
          "description": "Data is published on the web over time in great volumes, but majority of the\ndata is unstructured, making it hard to understand and difficult to interpret.\nInformation Extraction (IE) methods extract structured information from\nunstructured data. One of the challenging IE tasks is Event Extraction (EE)\nwhich seeks to derive information about specific incidents and their actors\nfrom the text. EE is useful in many domains such as building a knowledge base,\ninformation retrieval, summarization and online monitoring systems. In the past\ndecades, some event ontologies like ACE, CAMEO and ICEWS were developed to\ndefine event forms, actors and dimensions of events observed in the text. These\nevent ontologies still have some shortcomings such as covering only a few\ntopics like political events, having inflexible structure in defining argument\nroles, lack of analytical dimensions, and complexity in choosing event\nsub-types. To address these concerns, we propose an event ontology, namely\nCOfEE, that incorporates both expert domain knowledge, previous ontologies and\na data-driven approach for identifying events from text. COfEE consists of two\nhierarchy levels (event types and event sub-types) that include new categories\nrelating to environmental issues, cyberspace, criminal activity and natural\ndisasters which need to be monitored instantly. Also, dynamic roles according\nto each event sub-type are defined to capture various dimensions of events. In\na follow-up experiment, the proposed ontology is evaluated on Wikipedia events,\nand it is shown to be general and comprehensive. Moreover, in order to\nfacilitate the preparation of gold-standard data for event extraction, a\nlanguage-independent online tool is presented based on COfEE.",
          "link": "http://arxiv.org/abs/2107.10326",
          "publishedOn": "2021-07-23T02:00:31.913Z",
          "wordCount": 705,
          "title": "COfEE: A Comprehensive Ontology for Event Extraction from text, with an online annotation tool. (arXiv:2107.10326v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_M/0/1/0/all/0/1\">Mingyuan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choy_S/0/1/0/all/0/1\">S.T. Boris Choy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junbin Gao</a>",
          "description": "The neural ordinary differential equation (neural ODE) model has attracted\nincreasing attention in time series analysis for its capability to process\nirregular time steps, i.e., data are not observed over equally-spaced time\nintervals. In multi-dimensional time series analysis, a task is to conduct\nevolutionary subspace clustering, aiming at clustering temporal data according\nto their evolving low-dimensional subspace structures. Many existing methods\ncan only process time series with regular time steps while time series are\nunevenly sampled in many situations such as missing data. In this paper, we\npropose a neural ODE model for evolutionary subspace clustering to overcome\nthis limitation and a new objective function with subspace self-expressiveness\nconstraint is introduced. We demonstrate that this method can not only\ninterpolate data at any time step for the evolutionary subspace clustering\ntask, but also achieve higher accuracy than other state-of-the-art evolutionary\nsubspace clustering methods. Both synthetic and real-world data are used to\nillustrate the efficacy of our proposed method.",
          "link": "http://arxiv.org/abs/2107.10484",
          "publishedOn": "2021-07-23T02:00:31.906Z",
          "wordCount": 601,
          "title": "Neural Ordinary Differential Equation Model for Evolutionary Subspace Clustering and Its Applications. (arXiv:2107.10484v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1\">Tirtharaj Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitlangia_S/0/1/0/all/0/1\">Sharad Chitlangia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_A/0/1/0/all/0/1\">Aditya Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1\">Ashwin Srinivasan</a>",
          "description": "We present a short survey of ways in which existing scientific knowledge are\nincluded when constructing models with neural networks. The inclusion of\ndomain-knowledge is of special interest not just to constructing scientific\nassistants, but also, many other areas that involve understanding data using\nhuman-machine collaboration. In many such instances, machine-based model\nconstruction may benefit significantly from being provided with human-knowledge\nof the domain encoded in a sufficiently precise form. This paper examines the\ninclusion of domain-knowledge by means of changes to: the input, the\nloss-function, and the architecture of deep networks. The categorisation is for\nease of exposition: in practice we expect a combination of such changes will be\nemployed. In each category, we describe techniques that have been shown to\nyield significant changes in network performance.",
          "link": "http://arxiv.org/abs/2107.10295",
          "publishedOn": "2021-07-23T02:00:31.897Z",
          "wordCount": 590,
          "title": "How to Tell Deep Neural Networks What We Know. (arXiv:2107.10295v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10332",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Uthamacumaran_A/0/1/0/all/0/1\">Abicumaran Uthamacumaran</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Elouatik_S/0/1/0/all/0/1\">Samir Elouatik</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Abdouh_M/0/1/0/all/0/1\">Mohamed Abdouh</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Berteau_Rainville_M/0/1/0/all/0/1\">Michael Berteau-Rainville</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gao_Z/0/1/0/all/0/1\">Zhu- Hua Gao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Arena_G/0/1/0/all/0/1\">Goffredo Arena</a>",
          "description": "The early detection of cancer is a challenging problem in medicine. The blood\nsera of cancer patients are enriched with heterogeneous secretory lipid bound\nextracellular vesicles (EVs), which present a complex repertoire of information\nand biomarkers, representing their cell of origin, that are being currently\nstudied in the field of liquid biopsy and cancer screening. Vibrational\nspectroscopies provide non-invasive approaches for the assessment of structural\nand biophysical properties in complex biological samples. In this study,\nmultiple Raman spectroscopy measurements were performed on the EVs extracted\nfrom the blood sera of 9 patients consisting of four different cancer subtypes\n(colorectal cancer, hepatocellular carcinoma, breast cancer and pancreatic\ncancer) and five healthy patients (controls). FTIR(Fourier Transform Infrared)\nspectroscopy measurements were performed as a complementary approach to Raman\nanalysis, on two of the four cancer subtypes.\n\nThe AdaBoost Random Forest Classifier, Decision Trees, and Support Vector\nMachines (SVM) distinguished the baseline corrected Raman spectra of cancer EVs\nfrom those of healthy controls (18 spectra) with a classification accuracy of\ngreater than 90% when reduced to a spectral frequency range of 1800 to 1940\ninverse cm, and subjected to a 0.5 training/testing split. FTIR classification\naccuracy on 14 spectra showed an 80% classification accuracy. Our findings\ndemonstrate that basic machine learning algorithms are powerful tools to\ndistinguish the complex vibrational spectra of cancer patient EVs from those of\nhealthy patients. These experimental methods hold promise as valid and\nefficient liquid biopsy for machine intelligence-assisted early cancer\nscreening.",
          "link": "http://arxiv.org/abs/2107.10332",
          "publishedOn": "2021-07-23T02:00:31.851Z",
          "wordCount": 701,
          "title": "Machine Learning Characterization of Cancer Patients-Derived Extracellular Vesicles using Vibrational Spectroscopies. (arXiv:2107.10332v1 [q-bio.OT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10383",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lutes_N/0/1/0/all/0/1\">Nathan Lutes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krishnamurthy_K/0/1/0/all/0/1\">K. Krishnamurthy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nadendla_V/0/1/0/all/0/1\">Venkata Sriram Siddhardh Nadendla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Balakrishnan_S/0/1/0/all/0/1\">S. N. Balakrishnan</a>",
          "description": "Adaptive methods are popular within the control literature due to the\nflexibility and forgiveness they offer in the area of modelling. Neural network\nadaptive control is favorable specifically for the powerful nature of the\nmachine learning algorithm to approximate unknown functions and for the ability\nto relax certain constraints within traditional adaptive control. Deep neural\nnetworks are large framework networks with vastly superior approximation\ncharacteristics than their shallow counterparts. However, implementing a deep\nneural network can be difficult due to size specific complications such as\nvanishing/exploding gradients in training. In this paper, a neuro-adaptive\ncontroller is implemented featuring a deep neural network trained on a new\nweight update law that escapes the vanishing/exploding gradient problem by only\nincorporating the sign of the gradient. The type of controller designed is an\nadaptive dynamic inversion controller utilizing a modified state observer in a\nsecondary estimation loop to train the network. The deep neural network learns\nthe entire plant model on-line, creating a controller that is completely model\nfree. The controller design is tested in simulation on a 2 link planar robot\narm. The controller is able to learn the nonlinear plant quickly and displays\ngood performance in the tracking control problem.",
          "link": "http://arxiv.org/abs/2107.10383",
          "publishedOn": "2021-07-23T02:00:31.837Z",
          "wordCount": 663,
          "title": "Online-Learning Deep Neuro-Adaptive Dynamic Inversion Controller for Model Free Control. (arXiv:2107.10383v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aziz_A/0/1/0/all/0/1\">Ajmal Aziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosasih_E/0/1/0/all/0/1\">Edward Elson Kosasih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_R/0/1/0/all/0/1\">Ryan-Rhys Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1\">Alexandra Brintrup</a>",
          "description": "Supply chain network data is a valuable asset for businesses wishing to\nunderstand their ethical profile, security of supply, and efficiency.\nPossession of a dataset alone however is not a sufficient enabler of actionable\ndecisions due to incomplete information. In this paper, we present a graph\nrepresentation learning approach to uncover hidden dependency links that focal\ncompanies may not be aware of. To the best of our knowledge, our work is the\nfirst to represent a supply chain as a heterogeneous knowledge graph with\nlearnable embeddings. We demonstrate that our representation facilitates\nstate-of-the-art performance on link prediction of a global automotive supply\nchain network using a relational graph convolutional network. It is anticipated\nthat our method will be directly applicable to businesses wishing to sever\nlinks with nefarious entities and mitigate risk of supply failure. More\nabstractly, it is anticipated that our method will be useful to inform\nrepresentation learning of supply chain networks for downstream tasks beyond\nlink prediction.",
          "link": "http://arxiv.org/abs/2107.10609",
          "publishedOn": "2021-07-23T02:00:31.816Z",
          "wordCount": 605,
          "title": "Data Considerations in Graph Representation Learning for Supply Chain Networks. (arXiv:2107.10609v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Almalki_A/0/1/0/all/0/1\">Ali Almalki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wocjan_P/0/1/0/all/0/1\">Pawel Wocjan</a>",
          "description": "Abstract - Gathering relevant information to predict student academic\nprogress is a tedious task. Due to the large amount of irrelevant data present\nin databases which provides inaccurate results. Currently, it is not possible\nto accurately measure and analyze student data because there are too many\nirrelevant attributes and features in the data. With the help of Educational\nData Mining (EDM), the quality of information can be improved. This research\ndemonstrates how EDM helps to measure the accuracy of data using relevant\nattributes and machine learning algorithms performed. With EDM, irrelevant\nfeatures are removed without changing the original data. The data set used in\nthis study was taken from Kaggle.com. The results compared on the basis of\nrecall, precision and f-measure to check the accuracy of the student data. The\nimportance of this research is to help improve the quality of educational\nresearch by providing more accurate results for researchers.",
          "link": "http://arxiv.org/abs/2107.10669",
          "publishedOn": "2021-07-23T02:00:31.809Z",
          "wordCount": 591,
          "title": "Accuracy analysis of Educational Data Mining using Feature Selection Algorithm. (arXiv:2107.10669v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10658",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Rownicka_J/0/1/0/all/0/1\">Joanna Rownicka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sprenkamp_K/0/1/0/all/0/1\">Kilian Sprenkamp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tripiana_A/0/1/0/all/0/1\">Antonio Tripiana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gromoglasov_V/0/1/0/all/0/1\">Volodymyr Gromoglasov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kunz_T/0/1/0/all/0/1\">Timo P Kunz</a>",
          "description": "We describe our approach to create and deliver a custom voice for a\nconversational AI use-case. More specifically, we provide a voice for a Digital\nEinstein character, to enable human-computer interaction within the digital\nconversation experience. To create the voice which fits the context well, we\nfirst design a voice character and we produce the recordings which correspond\nto the desired speech attributes. We then model the voice. Our solution\nutilizes Fastspeech 2 for log-scaled mel-spectrogram prediction from phonemes\nand Parallel WaveGAN to generate the waveforms. The system supports a character\ninput and gives a speech waveform at the output. We use a custom dictionary for\nselected words to ensure their proper pronunciation. Our proposed cloud\narchitecture enables for fast voice delivery, making it possible to talk to the\ndigital version of Albert Einstein in real-time.",
          "link": "http://arxiv.org/abs/2107.10658",
          "publishedOn": "2021-07-23T02:00:31.790Z",
          "wordCount": 598,
          "title": "Digital Einstein Experience: Fast Text-to-Speech for Conversational AI. (arXiv:2107.10658v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benegui_C/0/1/0/all/0/1\">Cezara Benegui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>",
          "description": "We propose an enhanced version of the Authentication with Built-in Camera\n(ABC) protocol by employing a deep learning solution based on built-in motion\nsensors. The standard ABC protocol identifies mobile devices based on the\nphoto-response non-uniformity (PRNU) of the camera sensor, while also\nconsidering QR-code-based meta-information. During authentication, the user is\nrequired to take two photos that contain two QR codes presented on a screen.\nThe presented QR code images also contain a unique probe signal, similar to a\ncamera fingerprint, generated by the protocol. During verification, the server\ncomputes the fingerprint of the received photos and authenticates the user if\n(i) the probe signal is present, (ii) the metadata embedded in the QR codes is\ncorrect and (iii) the camera fingerprint is identified correctly. However, the\nprotocol is vulnerable to forgery attacks when the attacker can compute the\ncamera fingerprint from external photos, as shown in our preliminary work. In\nthis context, we propose an enhancement for the ABC protocol based on motion\nsensor data, as an additional and passive authentication layer. Smartphones can\nbe identified through their motion sensor data, which, unlike photos, is never\nposted by users on social media platforms, thus being more secure than using\nphotographs alone. To this end, we transform motion signals into embedding\nvectors produced by deep neural networks, applying Support Vector Machines for\nthe smartphone identification task. Our change to the ABC protocol results in a\nmulti-modal protocol that lowers the false acceptance rate for the attack\nproposed in our previous work to a percentage as low as 0.07%.",
          "link": "http://arxiv.org/abs/2107.10536",
          "publishedOn": "2021-07-23T02:00:31.672Z",
          "wordCount": 703,
          "title": "Improving the Authentication with Built-in Camera ProtocolUsing Built-in Motion Sensors: A Deep Learning Solution. (arXiv:2107.10536v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sledge_I/0/1/0/all/0/1\">Isaac J. Sledge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toole_C/0/1/0/all/0/1\">Christopher D. Toole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maestri_J/0/1/0/all/0/1\">Joseph A. Maestri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1\">Jose C. Principe</a>",
          "description": "We propose a memory-based framework for real-time, data-efficient target\nanalysis in forward-looking-sonar (FLS) imagery. Our framework relies on first\nremoving non-discriminative details from the imagery using a small-scale\nDenseNet-inspired network. Doing so simplifies ensuing analyses and permits\ngeneralizing from few labeled examples. We then cascade the filtered imagery\ninto a novel NeuralRAM-based convolutional matching network, NRMN, for low-shot\ntarget recognition. We employ a small-scale FlowNet, LFN to align and register\nFLS imagery across local temporal scales. LFN enables target label consensus\nvoting across images and generally improves target detection and recognition\nrates.\n\nWe evaluate our framework using real-world FLS imagery with multiple broad\ntarget classes that have high intra-class variability and rich sub-class\nstructure. We show that few-shot learning, with anywhere from ten to thirty\nclass-specific exemplars, performs similarly to supervised deep networks\ntrained on hundreds of samples per class. Effective zero-shot learning is also\npossible. High performance is realized from the inductive-transfer properties\nof NRMNs when distractor elements are removed.",
          "link": "http://arxiv.org/abs/2107.10504",
          "publishedOn": "2021-07-23T02:00:31.665Z",
          "wordCount": 616,
          "title": "External-Memory Networks for Low-Shot Learning of Targets in Forward-Looking-Sonar Imagery. (arXiv:2107.10504v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jones_K/0/1/0/all/0/1\">Keenan Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurse_J/0/1/0/all/0/1\">Jason R. C. Nurse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shujun Li</a>",
          "description": "Recently, there had been little notable activity from the once prominent\nhacktivist group, Anonymous. The group, responsible for activist-based cyber\nattacks on major businesses and governments, appeared to have fragmented after\nkey members were arrested in 2013. In response to the major Black Lives Matter\n(BLM) protests that occurred after the killing of George Floyd, however,\nreports indicated that the group was back. To examine this apparent resurgence,\nwe conduct a large-scale study of Anonymous affiliates on Twitter. To this end,\nwe first use machine learning to identify a significant network of more than\n33,000 Anonymous accounts. Through topic modelling of tweets collected from\nthese accounts, we find evidence of sustained interest in topics related to\nBLM. We then use sentiment analysis on tweets focused on these topics, finding\nevidence of a united approach amongst the group, with positive tweets typically\nbeing used to express support towards BLM, and negative tweets typically being\nused to criticize police actions. Finally, we examine the presence of\nautomation in the network, identifying indications of bot-like behavior across\nthe majority of Anonymous accounts. These findings show that whilst the group\nhas seen a resurgence during the protests, bot activity may be responsible for\nexaggerating the extent of this resurgence.",
          "link": "http://arxiv.org/abs/2107.10554",
          "publishedOn": "2021-07-23T02:00:31.657Z",
          "wordCount": 678,
          "title": "Out of the Shadows: Analyzing Anonymous' Twitter Resurgence during the 2020 Black Lives Matter Protests. (arXiv:2107.10554v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahon_L/0/1/0/all/0/1\">Louis Mahon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>",
          "description": "Deep neural networks (DNNs) offer a means of addressing the challenging task\nof clustering high-dimensional data. DNNs can extract useful features, and so\nproduce a lower dimensional representation, which is more amenable to\nclustering techniques. As clustering is typically performed in a purely\nunsupervised setting, where no training labels are available, the question then\narises as to how the DNN feature extractor can be trained. The most accurate\nexisting approaches combine the training of the DNN with the clustering\nobjective, so that information from the clustering process can be used to\nupdate the DNN to produce better features for clustering. One problem with this\napproach is that these ``pseudo-labels'' produced by the clustering algorithm\nare noisy, and any errors that they contain will hurt the training of the DNN.\nIn this paper, we propose selective pseudo-label clustering, which uses only\nthe most confident pseudo-labels for training the~DNN. We formally prove the\nperformance gains under certain conditions. Applied to the task of image\nclustering, the new approach achieves a state-of-the-art performance on three\npopular image datasets. Code is available at\nhttps://github.com/Lou1sM/clustering.",
          "link": "http://arxiv.org/abs/2107.10692",
          "publishedOn": "2021-07-23T02:00:31.651Z",
          "wordCount": 598,
          "title": "Selective Pseudo-label Clustering. (arXiv:2107.10692v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10718",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaggin_H/0/1/0/all/0/1\">Hanna K. Gaggin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Weichung Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "Assessment of cardiovascular disease (CVD) with cine magnetic resonance\nimaging (MRI) has been used to non-invasively evaluate detailed cardiac\nstructure and function. Accurate segmentation of cardiac structures from cine\nMRI is a crucial step for early diagnosis and prognosis of CVD, and has been\ngreatly improved with convolutional neural networks (CNN). There, however, are\na number of limitations identified in CNN models, such as limited\ninterpretability and high complexity, thus limiting their use in clinical\npractice. In this work, to address the limitations, we propose a lightweight\nand interpretable machine learning model, successive subspace learning with the\nsubspace approximation with adjusted bias (Saab) transform, for accurate and\nefficient segmentation from cine MRI. Specifically, our segmentation framework\nis comprised of the following steps: (1) sequential expansion of near-to-far\nneighborhood at different resolutions; (2) channel-wise subspace approximation\nusing the Saab transform for unsupervised dimension reduction; (3) class-wise\nentropy guided feature selection for supervised dimension reduction; (4)\nconcatenation of features and pixel-wise classification with gradient boost;\nand (5) conditional random field for post-processing. Experimental results on\nthe ACDC 2017 segmentation database, showed that our framework performed better\nthan state-of-the-art U-Net models with 200$\\times$ fewer parameters in\ndelineating the left ventricle, right ventricle, and myocardium, thus showing\nits potential to be used in clinical practice.",
          "link": "http://arxiv.org/abs/2107.10718",
          "publishedOn": "2021-07-23T02:00:31.645Z",
          "wordCount": 696,
          "title": "Segmentation of Cardiac Structures via Successive Subspace Learning with Saab Transform from Cine MRI. (arXiv:2107.10718v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gladstone_R/0/1/0/all/0/1\">Rini Jasmine Gladstone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabian_M/0/1/0/all/0/1\">Mohammad Amin Nabian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keshavarzzadeh_V/0/1/0/all/0/1\">Vahid Keshavarzzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meidani_H/0/1/0/all/0/1\">Hadi Meidani</a>",
          "description": "Topology Optimization is the process of finding the optimal arrangement of\nmaterials within a design domain by minimizing a cost function, subject to some\nperformance constraints. Robust topology optimization (RTO) also incorporates\nthe effect of input uncertainties and produces a design with the best average\nperformance of the structure while reducing the response sensitivity to input\nuncertainties. It is computationally expensive to carry out RTO using finite\nelement and Monte Carlo sampling. In this work, we use neural network\nsurrogates to enable a faster solution approach via surrogate-based\noptimization and build a Variational Autoencoder (VAE) to transform the the\nhigh dimensional design space into a low dimensional one. Furthermore, finite\nelement solvers will be replaced by a neural network surrogate. Also, to\nfurther facilitate the design exploration, we limit our search to a subspace,\nwhich consists of designs that are solutions to deterministic topology\noptimization problems under different realizations of input uncertainties. With\nthese neural network approximations, a gradient-based optimization approach is\nformed to minimize the predicted objective function over the low dimensional\ndesign subspace. We demonstrate the effectiveness of the proposed approach on\ntwo compliance minimization problems and show that VAE performs well on\nlearning the features of the design from minimal training data, and that\nconverting the design space into a low dimensional latent space makes the\nproblem computationally efficient. The resulting gradient-based optimization\nalgorithm produces optimal designs with lower robust compliances than those\nobserved in the training set.",
          "link": "http://arxiv.org/abs/2107.10661",
          "publishedOn": "2021-07-23T02:00:31.639Z",
          "wordCount": 669,
          "title": "Robust Topology Optimization Using Variational Autoencoders. (arXiv:2107.10661v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10469",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1\">Thi Ngoc Tho Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watcharasupat_K/0/1/0/all/0/1\">Karn N. Watcharasupat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_Z/0/1/0/all/0/1\">Zhen Jian Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngoc Khanh Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jones_D/0/1/0/all/0/1\">Douglas L. Jones</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gan_W/0/1/0/all/0/1\">Woon Seng Gan</a>",
          "description": "Sound event localization and detection (SELD) is an emerging research topic\nthat aims to unify the tasks of sound event detection and direction-of-arrival\nestimation. As a result, SELD inherits the challenges of both tasks, such as\nnoise, reverberation, interference, polyphony, and non-stationarity of sound\nsources. Furthermore, SELD often faces an additional challenge of assigning\ncorrect correspondences between the detected sound classes and directions of\narrival to multiple overlapping sound events. Previous studies have shown that\nunknown interferences in reverberant environments often cause major degradation\nin the performance of SELD systems. To further understand the challenges of the\nSELD task, we performed a detailed error analysis on two of our SELD systems,\nwhich both ranked second in the team category of DCASE SELD Challenge, one in\n2020 and one in 2021. Experimental results indicate polyphony as the main\nchallenge in SELD, due to the difficulty in detecting all sound events of\ninterest. In addition, the SELD systems tend to make fewer errors for the\npolyphonic scenario that is dominant in the training set.",
          "link": "http://arxiv.org/abs/2107.10469",
          "publishedOn": "2021-07-23T02:00:31.622Z",
          "wordCount": 661,
          "title": "What Makes Sound Event Localization and Detection Difficult? Insights from Error Analysis. (arXiv:2107.10469v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1\">Zhendong Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongning Wang</a>",
          "description": "Crowdsourcing provides an efficient label collection schema for supervised\nmachine learning. However, to control annotation cost, each instance in the\ncrowdsourced data is typically annotated by a small number of annotators. This\ncreates a sparsity issue and limits the quality of machine learning models\ntrained on such data. In this paper, we study how to handle sparsity in\ncrowdsourced data using data augmentation. Specifically, we propose to directly\nlearn a classifier by augmenting the raw sparse annotations. We implement two\nprinciples of high-quality augmentation using Generative Adversarial Networks:\n1) the generated annotations should follow the distribution of authentic ones,\nwhich is measured by a discriminator; 2) the generated annotations should have\nhigh mutual information with the ground-truth labels, which is measured by an\nauxiliary network. Extensive experiments and comparisons against an array of\nstate-of-the-art learning from crowds methods on three real-world datasets\nproved the effectiveness of our data augmentation framework. It shows the\npotential of our algorithm for low-budget crowdsourcing in general.",
          "link": "http://arxiv.org/abs/2107.10449",
          "publishedOn": "2021-07-23T02:00:31.614Z",
          "wordCount": 601,
          "title": "Improve Learning from Crowds via Generative Augmentation. (arXiv:2107.10449v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagdasaryan_E/0/1/0/all/0/1\">Eugene Bagdasaryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmatikov_V/0/1/0/all/0/1\">Vitaly Shmatikov</a>",
          "description": "We investigate a new threat to neural sequence-to-sequence (seq2seq) models:\ntraining-time attacks that cause models to \"spin\" their output and support a\ncertain sentiment when the input contains adversary-chosen trigger words. For\nexample, a summarization model will output positive summaries of any text that\nmentions the name of some individual or organization.\n\nWe introduce the concept of a \"meta-backdoor\" to explain model-spinning\nattacks. These attacks produce models whose output is valid and preserves\ncontext, yet also satisfies a meta-task chosen by the adversary (e.g., positive\nsentiment). Previously studied backdoors in language models simply flip\nsentiment labels or replace words without regard to context. Their outputs are\nincorrect on inputs with the trigger. Meta-backdoors, on the other hand, are\nthe first class of backdoors that can be deployed against seq2seq models to (a)\nintroduce adversary-chosen spin into the output, while (b) maintaining standard\naccuracy metrics.\n\nTo demonstrate feasibility of model spinning, we develop a new backdooring\ntechnique. It stacks the adversarial meta-task (e.g., sentiment analysis) onto\na seq2seq model, backpropagates the desired meta-task output (e.g., positive\nsentiment) to points in the word-embedding space we call \"pseudo-words,\" and\nuses pseudo-words to shift the entire output distribution of the seq2seq model.\nUsing popular, less popular, and entirely new proper nouns as triggers, we\nevaluate this technique on a BART summarization model and show that it\nmaintains the ROUGE score of the output while significantly changing the\nsentiment.\n\nWe explain why model spinning can be a dangerous technique in AI-powered\ndisinformation and discuss how to mitigate these attacks.",
          "link": "http://arxiv.org/abs/2107.10443",
          "publishedOn": "2021-07-23T02:00:31.608Z",
          "wordCount": 685,
          "title": "Spinning Sequence-to-Sequence Models with Meta-Backdoors. (arXiv:2107.10443v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10667",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sankarapandian_S/0/1/0/all/0/1\">Sivaramakrishnan Sankarapandian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulis_B/0/1/0/all/0/1\">Brian Kulis</a>",
          "description": "Gravitational wave detectors such as LIGO and Virgo are susceptible to\nvarious types of instrumental and environmental disturbances known as glitches\nwhich can mask and mimic gravitational waves. While there are 22 classes of\nnon-Gaussian noise gradients currently identified, the number of classes is\nlikely to increase as these detectors go through commissioning between\nobservation runs. Since identification and labelling new noise gradients can be\narduous and time-consuming, we propose $\\beta$-Annelead VAEs to learn\nrepresentations from spectograms in an unsupervised way. Using the same\nformulation as \\cite{alemi2017fixing}, we view\nBottleneck-VAEs~cite{burgess2018understanding} through the lens of information\ntheory and connect them to $\\beta$-VAEs~cite{higgins2017beta}. Motivated by\nthis connection, we propose an annealing schedule for the hyperparameter\n$\\beta$ in $\\beta$-VAEs which has advantages of: 1) One fewer hyperparameter to\ntune, 2) Better reconstruction quality, while producing similar levels of\ndisentanglement.",
          "link": "http://arxiv.org/abs/2107.10667",
          "publishedOn": "2021-07-23T02:00:31.601Z",
          "wordCount": 566,
          "title": "$\\beta$-Annealed Variational Autoencoder for glitches. (arXiv:2107.10667v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Okunevich_I/0/1/0/all/0/1\">Iaroslav Okunevich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trinitatova_D/0/1/0/all/0/1\">Daria Trinitatova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopanev_P/0/1/0/all/0/1\">Pavel Kopanev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsetserukou_D/0/1/0/all/0/1\">Dzmitry Tsetserukou</a>",
          "description": "MobileCharger is a novel mobile charging robot with an Inverted Delta\nactuator for safe and robust energy transfer between two mobile robots. The\nRGB-D camera-based computer vision system allows to detect the electrodes on\nthe target mobile robot using a convolutional neural network (CNN). The\nembedded high-fidelity tactile sensors are applied to estimate the misalignment\nbetween the electrodes on the charger mechanism and the electrodes on the main\nrobot using CNN based on pressure data on the contact surfaces. Thus, the\ndeveloped vision-tactile perception system allows precise positioning of the\nend effector of the actuator and ensures a reliable connection between the\nelectrodes of the two robots. The experimental results showed high average\nprecision (84.2%) for electrode detection using CNN. The percentage of\nsuccessful trials of the CNN-based electrode search algorithm reached 83% and\nthe average execution time accounted for 60 s. MobileCharger could introduce a\nnew level of charging systems and increase the prevalence of autonomous mobile\nrobots.",
          "link": "http://arxiv.org/abs/2107.10585",
          "publishedOn": "2021-07-23T02:00:31.570Z",
          "wordCount": 625,
          "title": "MobileCharger: an Autonomus Mobile Robot with Inverted Delta Actuator for Robust and Safe Robot Charging. (arXiv:2107.10585v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10495",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Romero_R/0/1/0/all/0/1\">Roland Albert A. Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deypalan_M/0/1/0/all/0/1\">Mariefel Nicole Y. Deypalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehrotra_S/0/1/0/all/0/1\">Suchit Mehrotra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jungao_J/0/1/0/all/0/1\">John Titus Jungao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheils_N/0/1/0/all/0/1\">Natalie E. Sheils</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manduchi_E/0/1/0/all/0/1\">Elisabetta Manduchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1\">Jason H. Moore</a>",
          "description": "We ascertain and compare the performances of AutoML tools on large, highly\nimbalanced healthcare datasets.\n\nWe generated a large dataset using historical administrative claims including\ndemographic information and flags for disease codes in four different time\nwindows prior to 2019. We then trained three AutoML tools on this dataset to\npredict six different disease outcomes in 2019 and evaluated model performances\non several metrics.\n\nThe AutoML tools showed improvement from the baseline random forest model but\ndid not differ significantly from each other. All models recorded low area\nunder the precision-recall curve and failed to predict true positives while\nkeeping the true negative rate high. Model performance was not directly related\nto prevalence. We provide a specific use-case to illustrate how to select a\nthreshold that gives the best balance between true and false positive rates, as\nthis is an important consideration in medical applications.\n\nHealthcare datasets present several challenges for AutoML tools, including\nlarge sample size, high imbalance, and limitations in the available features\ntypes. Improvements in scalability, combinations of imbalance-learning\nresampling and ensemble approaches, and curated feature selection are possible\nnext steps to achieve better performance.\n\nAmong the three explored, no AutoML tool consistently outperforms the rest in\nterms of predictive performance. The performances of the models in this study\nsuggest that there may be room for improvement in handling medical claims data.\nFinally, selection of the optimal prediction threshold should be guided by the\nspecific practical application.",
          "link": "http://arxiv.org/abs/2107.10495",
          "publishedOn": "2021-07-23T02:00:31.555Z",
          "wordCount": 692,
          "title": "Benchmarking AutoML Frameworks for Disease Prediction Using Medical Claims. (arXiv:2107.10495v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silvestrin_L/0/1/0/all/0/1\">Luis P. Silvestrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantiskas_L/0/1/0/all/0/1\">Leonardos Pantiskas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoogendoorn_M/0/1/0/all/0/1\">Mark Hoogendoorn</a>",
          "description": "Time-series forecasting plays an important role in many domains. Boosted by\nthe advances in Deep Learning algorithms, it has for instance been used to\npredict wind power for eolic energy production, stock market fluctuations, or\nmotor overheating. In some of these tasks, we are interested in predicting\naccurately some particular moments which often are underrepresented in the\ndataset, resulting in a problem known as imbalanced regression. In the\nliterature, while recognized as a challenging problem, limited attention has\nbeen devoted on how to handle the problem in a practical setting. In this\npaper, we put forward a general approach to analyze time-series forecasting\nproblems focusing on those underrepresented moments to reduce imbalances. Our\napproach has been developed based on a case study in a large industrial\ncompany, which we use to exemplify the approach.",
          "link": "http://arxiv.org/abs/2107.10709",
          "publishedOn": "2021-07-23T02:00:31.541Z",
          "wordCount": 562,
          "title": "A Framework for Imbalanced Time-series Forecasting. (arXiv:2107.10709v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Okunevich_I/0/1/0/all/0/1\">Iaroslav Okunevich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trinitatova_D/0/1/0/all/0/1\">Daria Trinitatova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopanev_P/0/1/0/all/0/1\">Pavel Kopanev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsetserukou_D/0/1/0/all/0/1\">Dzmitry Tsetserukou</a>",
          "description": "DeltaCharger is a novel charging robot with an Inverted Delta structure for\n3D positioning of electrodes to achieve robust and safe transferring energy\nbetween two mobile robots. The embedded high-fidelity tactile sensors allow to\nestimate the angular, vertical and horizontal misalignments between electrodes\non the charger mechanism and electrodes on the target robot using pressure data\non the contact surfaces. This is crucial for preventing a short circuit. In\nthis paper, the mechanism of the developed prototype and evaluation study of\ndifferent machine learning models for misalignment prediction are presented.\nThe experimental results showed that the proposed system can measure the angle,\nvertical and horizontal values of misalignment from pressure data with an\naccuracy of 95.46%, 98.2%, and 86.9%, respectively, using a Convolutional\nNeural Network (CNN). DeltaCharger can potentially bring a new level of\ncharging systems and improve the prevalence of mobile autonomous robots.",
          "link": "http://arxiv.org/abs/2107.10710",
          "publishedOn": "2021-07-23T02:00:31.521Z",
          "wordCount": 619,
          "title": "DeltaCharger: Charging Robot with Inverted Delta Mechanism and CNN-driven High Fidelity Tactile Perception for Precise 3D Positioning. (arXiv:2107.10710v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1\">Arnab Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_D/0/1/0/all/0/1\">Davin Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gajjala_R/0/1/0/all/0/1\">Rishikesh Gajjala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gayen_S/0/1/0/all/0/1\">Sutanu Gayen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhao Wang</a>",
          "description": "Gaussian Bayesian networks (a.k.a. linear Gaussian structural equation\nmodels) are widely used to model causal interactions among continuous\nvariables. In this work, we study the problem of learning a fixed-structure\nGaussian Bayesian network up to a bounded error in total variation distance. We\nanalyze the commonly used node-wise least squares regression (LeastSquares) and\nprove that it has a near-optimal sample complexity. We also study a couple of\nnew algorithms for the problem:\n\n- BatchAvgLeastSquares takes the average of several batches of least squares\nsolutions at each node, so that one can interpolate between the batch size and\nthe number of batches. We show that BatchAvgLeastSquares also has near-optimal\nsample complexity.\n\n- CauchyEst takes the median of solutions to several batches of linear\nsystems at each node. We show that the algorithm specialized to polytrees,\nCauchyEstTree, has near-optimal sample complexity.\n\nExperimentally, we show that for uncontaminated, realizable data, the\nLeastSquares algorithm performs best, but in the presence of contamination or\nDAG misspecification, CauchyEst/CauchyEstTree and BatchAvgLeastSquares\nrespectively perform better.",
          "link": "http://arxiv.org/abs/2107.10450",
          "publishedOn": "2021-07-23T02:00:31.514Z",
          "wordCount": 618,
          "title": "Learning Sparse Fixed-Structure Gaussian Bayesian Networks. (arXiv:2107.10450v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dayta_D/0/1/0/all/0/1\">Dominic B. Dayta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrios_E/0/1/0/all/0/1\">Erniel B. Barrios</a>",
          "description": "Legacy procedures for topic modelling have generally suffered problems of\noverfitting and a weakness towards reconstructing sparse topic structures. With\nmotivation from a consumer-generated corpora, this paper proposes\nsemiparametric topic model, a two-step approach utilizing nonnegative matrix\nfactorization and semiparametric regression in topic modeling. The model\nenables the reconstruction of sparse topic structures in the corpus and\nprovides a generative model for predicting topics in new documents entering the\ncorpus. Assuming the presence of auxiliary information related to the topics,\nthis approach exhibits better performance in discovering underlying topic\nstructures in cases where the corpora are small and limited in vocabulary. In\nan actual consumer feedback corpus, the model also demonstrably provides\ninterpretable and useful topic definitions comparable with those produced by\nother methods.",
          "link": "http://arxiv.org/abs/2107.10651",
          "publishedOn": "2021-07-23T02:00:31.507Z",
          "wordCount": 555,
          "title": "Semiparametric Latent Topic Modeling on Consumer-Generated Corpora. (arXiv:2107.10651v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sprave_J/0/1/0/all/0/1\">Joachim Sprave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drescher_C/0/1/0/all/0/1\">Christian Drescher</a>",
          "description": "This paper addresses the problem of evaluating the quality of finite element\nmeshes for the purpose of structural mechanic simulations. It proposes the\napplication of a machine learning model trained on data collected from expert\nevaluations. The task is characterised as a classification problem, where\nquality of each individual element in a mesh is determined by its own\nproperties and adjacency structures. A domain-specific, yet simple\nrepresentation is proposed such that off-the-shelf machine learning methods can\nbe applied. Experimental data from industry practice demonstrates promising\nresults.",
          "link": "http://arxiv.org/abs/2107.10507",
          "publishedOn": "2021-07-23T02:00:31.500Z",
          "wordCount": 530,
          "title": "Evaluating the Quality of Finite Element Meshes with Machine Learning. (arXiv:2107.10507v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1\">Sara Beery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_E/0/1/0/all/0/1\">Elijah Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parker_J/0/1/0/all/0/1\">Joseph Parker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1\">Pietro Perona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winner_K/0/1/0/all/0/1\">Kevin Winner</a>",
          "description": "Conservation science depends on an accurate understanding of what's happening\nin a given ecosystem. How many species live there? What is the makeup of the\npopulation? How is that changing over time? Species Distribution Modeling (SDM)\nseeks to predict the spatial (and sometimes temporal) patterns of species\noccurrence, i.e. where a species is likely to be found. The last few years have\nseen a surge of interest in applying powerful machine learning tools to\nchallenging problems in ecology. Despite its considerable importance, SDM has\nreceived relatively little attention from the computer science community. Our\ngoal in this work is to provide computer scientists with the necessary\nbackground to read the SDM literature and develop ecologically useful ML-based\nSDM algorithms. In particular, we introduce key SDM concepts and terminology,\nreview standard models, discuss data availability, and highlight technical\nchallenges and pitfalls.",
          "link": "http://arxiv.org/abs/2107.10400",
          "publishedOn": "2021-07-23T02:00:31.494Z",
          "wordCount": 582,
          "title": "Species Distribution Modeling for Machine Learning Practitioners: A Review. (arXiv:2107.10400v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ko_G/0/1/0/all/0/1\">Gihyuk Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_G/0/1/0/all/0/1\">Gyumin Lim</a>",
          "description": "Deep Neural Networks (DNNs) have shown remarkable performance in a diverse\nrange of machine learning applications. However, it is widely known that DNNs\nare vulnerable to simple adversarial perturbations, which causes the model to\nincorrectly classify inputs. In this paper, we propose a simple yet effective\nmethod to detect adversarial examples, using methods developed to explain the\nmodel's behavior. Our key observation is that adding small, humanly\nimperceptible perturbations can lead to drastic changes in the model\nexplanations, resulting in unusual or irregular forms of explanations. From\nthis insight, we propose an unsupervised detection of adversarial examples\nusing reconstructor networks trained only on model explanations of benign\nexamples. Our evaluations with MNIST handwritten dataset show that our method\nis capable of detecting adversarial examples generated by the state-of-the-art\nalgorithms with high confidence. To the best of our knowledge, this work is the\nfirst in suggesting unsupervised defense method using model explanations.",
          "link": "http://arxiv.org/abs/2107.10480",
          "publishedOn": "2021-07-23T02:00:31.476Z",
          "wordCount": 590,
          "title": "Unsupervised Detection of Adversarial Examples with Model Explanations. (arXiv:2107.10480v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anjali_P/0/1/0/all/0/1\">P. Anjali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramani_D/0/1/0/all/0/1\">Deepak N. Subramani</a>",
          "description": "We develop a Random Forest model to estimate the species distribution of\nAsian elephants in India and study the inter and intra-annual spatiotemporal\nvariability of habitats suitable for them. Climatic, topographic variables and\nsatellite-derived Land Use/Land Cover (LULC), Net Primary Productivity (NPP),\nLeaf Area Index (LAI), and Normalized Difference Vegetation Index (NDVI) are\nused as predictors, and the species sighting data of Asian elephants from\nGlobal Biodiversity Information Reserve is used to develop the Random Forest\nmodel. A careful hyper-parameter tuning and training-validation-testing cycle\nare completed to identify the significant predictors and develop a final model\nthat gives precision and recall of 0.78 and 0.77. The model is applied to\nestimate the spatial and temporal variability of suitable habitats. We observe\nthat seasonal reduction in the suitable habitat may explain the migration\npatterns of Asian elephants and the increasing human-elephant conflict.\nFurther, the total available suitable habitat area is observed to have reduced,\nwhich exacerbates the problem. This machine learning model is intended to serve\nas an input to the Agent-Based Model that we are building as part of our\nArtificial Intelligence-driven decision support tool to reduce human-wildlife\nconflict.",
          "link": "http://arxiv.org/abs/2107.10478",
          "publishedOn": "2021-07-23T02:00:31.469Z",
          "wordCount": 653,
          "title": "Inter and Intra-Annual Spatio-Temporal Variability of Habitat Suitability for Asian Elephants in India: A Random Forest Model-based Analysis. (arXiv:2107.10478v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kolomvatsos_K/0/1/0/all/0/1\">Kostas Kolomvatsos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anagnostopoulos_C/0/1/0/all/0/1\">Christos Anagnostopoulos</a>",
          "description": "The combination of the infrastructure provided by the Internet of Things\n(IoT) with numerous processing nodes present at the Edge Computing (EC)\necosystem opens up new pathways to support intelligent applications. Such\napplications can be provided upon humongous volumes of data collected by IoT\ndevices being transferred to the edge nodes through the network. Various\nprocessing activities can be performed on the discussed data and multiple\ncollaborative opportunities between EC nodes can facilitate the execution of\nthe desired tasks. In order to support an effective interaction between edge\nnodes, the knowledge about the geographically distributed data should be\nshared. Obviously, the migration of large amounts of data will harm the\nstability of the network stability and its performance. In this paper, we\nrecommend the exchange of data synopses than real data between EC nodes to\nprovide them with the necessary knowledge about peer nodes owning similar data.\nThis knowledge can be valuable when considering decisions such as data/service\nmigration and tasks offloading. We describe an continuous reasoning model that\nbuilds a temporal similarity map of the available datasets to get nodes\nunderstanding the evolution of data in their peers. We support the proposed\ndecision making mechanism through an intelligent similarity extraction scheme\nbased on an unsupervised machine learning model, and, at the same time, combine\nit with a statistical measure that represents the trend of the so-called\ndiscrepancy quantum. Our model can reveal the differences in the exchanged\nsynopses and provide a datasets similarity map which becomes the appropriate\nknowledge base to support the desired processing activities. We present the\nproblem under consideration and suggest a solution for that, while, at the same\ntime, we reveal its advantages and disadvantages through a large number of\nexperiments.",
          "link": "http://arxiv.org/abs/2107.10558",
          "publishedOn": "2021-07-23T02:00:31.460Z",
          "wordCount": 723,
          "title": "A Proactive Management Scheme for Data Synopses at the Edge. (arXiv:2107.10558v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burlacu_B/0/1/0/all/0/1\">Bogdan Burlacu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kammerer_L/0/1/0/all/0/1\">Lukas Kammerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Affenzeller_M/0/1/0/all/0/1\">Michael Affenzeller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1\">Gabriel Kronberger</a>",
          "description": "We introduce in this paper a runtime-efficient tree hashing algorithm for the\nidentification of isomorphic subtrees, with two important applications in\ngenetic programming for symbolic regression: fast, online calculation of\npopulation diversity and algebraic simplification of symbolic expression trees.\nBased on this hashing approach, we propose a simple diversity-preservation\nmechanism with promising results on a collection of symbolic regression\nbenchmark problems.",
          "link": "http://arxiv.org/abs/2107.10640",
          "publishedOn": "2021-07-23T02:00:31.453Z",
          "wordCount": 542,
          "title": "Hash-Based Tree Similarity and Simplification in Genetic Programming for Symbolic Regression. (arXiv:2107.10640v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fedyukova_A/0/1/0/all/0/1\">Anna Fedyukova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pires_D/0/1/0/all/0/1\">Douglas Pires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capurro_D/0/1/0/all/0/1\">Daniel Capurro</a>",
          "description": "The proliferation of early diagnostic technologies, including self-monitoring\nsystems and wearables, coupled with the application of these technologies on\nlarge segments of healthy populations may significantly aggravate the problem\nof overdiagnosis. This can lead to unwanted consequences such as overloading\nhealth care systems and overtreatment, with potential harms to healthy\nindividuals. The advent of machine-learning tools to assist diagnosis -- while\npromising rapid and more personalised patient management and screening -- might\ncontribute to this issue. The identification of overdiagnosis is usually post\nhoc and demonstrated after long periods (from years to decades) and costly\nrandomised control trials. In this paper, we present an innovative approach\nthat allows us to preemptively detect potential cases of overdiagnosis during\npredictive model development. This approach is based on the combination of\nlabels obtained from a prediction model and clustered medical trajectories,\nusing sepsis in adults as a test case. This is one of the first attempts to\nquantify machine-learning induced overdiagnosis and we believe will serves as a\nplatform for further development, leading to guidelines for safe deployment of\ncomputational diagnostic tools.",
          "link": "http://arxiv.org/abs/2107.10399",
          "publishedOn": "2021-07-23T02:00:31.430Z",
          "wordCount": 626,
          "title": "Quantifying machine learning-induced overdiagnosis in sepsis. (arXiv:2107.10399v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byung-Hak Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganapathi_V/0/1/0/all/0/1\">Varun Ganapathi</a>",
          "description": "Prediction of medical codes from clinical notes is both a practical and\nessential need for every healthcare delivery organization within current\nmedical systems. Automating annotation will save significant time and excessive\neffort spent by human coders today. However, the biggest challenge is directly\nidentifying appropriate medical codes out of several thousands of\nhigh-dimensional codes from unstructured free-text clinical notes. In the past\nthree years, with Convolutional Neural Networks (CNN) and Long Short-Term\nMemory (LTSM) networks, there have been vast improvements in tackling the most\nchallenging benchmark of the MIMIC-III-full-label inpatient clinical notes\ndataset. This progress raises the fundamental question of how far automated\nmachine learning (ML) systems are from human coders' working performance. We\nassessed the baseline of human coders' performance on the same subsampled\ntesting set. We also present our Read, Attend, and Code (RAC) model for\nlearning the medical code assignment mappings. By connecting convolved\nembeddings with self-attention and code-title guided attention modules,\ncombined with sentence permutation-based data augmentations and stochastic\nweight averaging training, RAC establishes a new state of the art (SOTA),\nconsiderably outperforming the current best Macro-F1 by 18.7%, and reaches past\nthe human-level coding baseline. This new milestone marks a meaningful step\ntoward fully autonomous medical coding (AMC) in machines reaching parity with\nhuman coders' performance in medical code prediction.",
          "link": "http://arxiv.org/abs/2107.10650",
          "publishedOn": "2021-07-23T02:00:31.424Z",
          "wordCount": 684,
          "title": "Read, Attend, and Code: Pushing the Limits of Medical Codes Prediction from Clinical Notes by Machines. (arXiv:2107.10650v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Chaoran Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_J/0/1/0/all/0/1\">Jian Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuling Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinhua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yilong Yin</a>",
          "description": "Academic performance prediction aims to leverage student-related information\nto predict their future academic outcomes, which is beneficial to numerous\neducational applications, such as personalized teaching and academic early\nwarning. In this paper, we address the problem by analyzing students' daily\nbehavior trajectories, which can be comprehensively tracked with campus\nsmartcard records. Different from previous studies, we propose a novel\nTri-Branch CNN architecture, which is equipped with row-wise, column-wise, and\ndepth-wise convolution and attention operations, to capture the characteristics\nof persistence, regularity, and temporal distribution of student behavior in an\nend-to-end manner, respectively. Also, we cast academic performance prediction\nas a top-$k$ ranking problem, and introduce a top-$k$ focused loss to ensure\nthe accuracy of identifying academically at-risk students. Extensive\nexperiments were carried out on a large-scale real-world dataset, and we show\nthat our approach substantially outperforms recently proposed methods for\nacademic performance prediction. For the sake of reproducibility, our codes\nhave been released at\nhttps://github.com/ZongJ1111/Academic-Performance-Prediction.",
          "link": "http://arxiv.org/abs/2107.10424",
          "publishedOn": "2021-07-23T02:00:31.406Z",
          "wordCount": 602,
          "title": "Tri-Branch Convolutional Neural Networks for Top-$k$ Focused Academic Performance Prediction. (arXiv:2107.10424v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10387",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Doty_C/0/1/0/all/0/1\">Christina Doty</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Gallagher_S/0/1/0/all/0/1\">Shaun Gallagher</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Cui_W/0/1/0/all/0/1\">Wenqi Cui</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Chen_W/0/1/0/all/0/1\">Wenya Chen</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Bhushan_S/0/1/0/all/0/1\">Shweta Bhushan</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Oostrom_M/0/1/0/all/0/1\">Marjolein Oostrom</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Akers_S/0/1/0/all/0/1\">Sarah Akers</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Spurgeon_S/0/1/0/all/0/1\">Steven R. Spurgeon</a>",
          "description": "The recent growth in data volumes produced by modern electron microscopes\nrequires rapid, scalable, and flexible approaches to image segmentation and\nanalysis. Few-shot machine learning, which can richly classify images from a\nhandful of user-provided examples, is a promising route to high-throughput\nanalysis. However, current command-line implementations of such approaches can\nbe slow and unintuitive to use, lacking the real-time feedback necessary to\nperform effective classification. Here we report on the development of a\nPython-based graphical user interface that enables end users to easily conduct\nand visualize the output of few-shot learning models. This interface is\nlightweight and can be hosted locally or on the web, providing the opportunity\nto reproducibly conduct, share, and crowd-source few-shot analyses.",
          "link": "http://arxiv.org/abs/2107.10387",
          "publishedOn": "2021-07-23T02:00:31.340Z",
          "wordCount": 580,
          "title": "Design of a Graphical User Interface for Few-Shot Machine Learning Classification of Electron Microscopy Data. (arXiv:2107.10387v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivanovic_B/0/1/0/all/0/1\">Boris Ivanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>",
          "description": "Forecasting the behavior of other agents is an integral part of the modern\nrobotic autonomy stack, especially in safety-critical scenarios with\nhuman-robot interaction, such as autonomous driving. In turn, there has been a\nsignificant amount of interest and research in trajectory forecasting,\nresulting in a wide variety of approaches. Common to all works, however, is the\nuse of the same few accuracy-based evaluation metrics, e.g., displacement error\nand log-likelihood. While these metrics are informative, they are task-agnostic\nand predictions that are evaluated as equal can lead to vastly different\noutcomes, e.g., in downstream planning and decision making. In this work, we\ntake a step back and critically evaluate current trajectory forecasting\nmetrics, proposing task-aware metrics as a better measure of performance in\nsystems where prediction is being deployed. We additionally present one example\nof such a metric, incorporating planning-awareness within existing trajectory\nforecasting metrics.",
          "link": "http://arxiv.org/abs/2107.10297",
          "publishedOn": "2021-07-23T02:00:31.292Z",
          "wordCount": 583,
          "title": "Rethinking Trajectory Forecasting Evaluation. (arXiv:2107.10297v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1\">Vinija Jain</a>",
          "description": "Causality knowledge is vital to building robust AI systems. Deep learning\nmodels often perform poorly on tasks that require causal reasoning, which is\noften derived using some form of commonsense knowledge not immediately\navailable in the input but implicitly inferred by humans. Prior work has\nunraveled spurious observational biases that models fall prey to in the absence\nof causality. While language representation models preserve contextual\nknowledge within learned embeddings, they do not factor in causal relationships\nduring training. By blending causal relationships with the input features to an\nexisting model that performs visual cognition tasks (such as scene\nunderstanding, video captioning, video question-answering, etc.), better\nperformance can be achieved owing to the insight causal relationships bring\nabout. Recently, several models have been proposed that have tackled the task\nof mining causal data from either the visual or textual modality. However,\nthere does not exist widespread research that mines causal relationships by\njuxtaposing the visual and language modalities. While images offer a rich and\neasy-to-process resource for us to mine causality knowledge from, videos are\ndenser and consist of naturally time-ordered events. Also, textual information\noffers details that could be implicit in videos. We propose iReason, a\nframework that infers visual-semantic commonsense knowledge using both videos\nand natural language captions. Furthermore, iReason's architecture integrates a\ncausal rationalization module to aid the process of interpretability, error\nanalysis and bias detection. We demonstrate the effectiveness of iReason using\na two-pronged comparative analysis with language representation learning models\n(BERT, GPT-2) as well as current state-of-the-art multimodal causality models.",
          "link": "http://arxiv.org/abs/2107.10300",
          "publishedOn": "2021-07-23T02:00:31.263Z",
          "wordCount": 723,
          "title": "iReason: Multimodal Commonsense Reasoning using Videos and Natural Language with Interpretability. (arXiv:2107.10300v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shaker_M/0/1/0/all/0/1\">Mohammad Hossein Shaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>",
          "description": "The idea to distinguish and quantify two important types of uncertainty,\noften referred to as aleatoric and epistemic, has received increasing attention\nin machine learning research in the last couple of years. In this paper, we\nconsider ensemble-based approaches to uncertainty quantification.\nDistinguishing between different types of uncertainty-aware learning\nalgorithms, we specifically focus on Bayesian methods and approaches based on\nso-called credal sets, which naturally suggest themselves from an ensemble\nlearning point of view. For both approaches, we address the question of how to\nquantify aleatoric and epistemic uncertainty. The effectiveness of\ncorresponding measures is evaluated and compared in an empirical study on\nclassification with a reject option.",
          "link": "http://arxiv.org/abs/2107.10384",
          "publishedOn": "2021-07-23T02:00:31.256Z",
          "wordCount": 540,
          "title": "Ensemble-based Uncertainty Quantification: Bayesian versus Credal Inference. (arXiv:2107.10384v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1\">Christopher Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1\">Lydia M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1\">Andreas Niekler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>",
          "description": "We present small-text, a simple modular active learning library, which offers\npool-based active learning for text classification in Python. It comes with\nvarious pre-implemented state-of-the-art query strategies, including some which\ncan leverage the GPU. Clearly defined interfaces allow to combine a multitude\nof such query strategies with different classifiers, thereby facilitating a\nquick mix and match, and enabling a rapid development of both active learning\nexperiments and applications. To make various classifiers accessible in a\nconsistent way, it integrates several well-known machine learning libraries,\nnamely, scikit-learn, PyTorch, and huggingface transformers -- for which the\nlatter integrations are available as optionally installable extensions. The\nlibrary is available under the MIT License at\nhttps://github.com/webis-de/small-text.",
          "link": "http://arxiv.org/abs/2107.10314",
          "publishedOn": "2021-07-23T02:00:31.233Z",
          "wordCount": 551,
          "title": "Small-text: Active Learning for Text Classification in Python. (arXiv:2107.10314v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10306",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Wang_D/0/1/0/all/0/1\">Dan Wang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Florescu_I/0/1/0/all/0/1\">Ionut Florescu</a>",
          "description": "In Artificial Intelligence, interpreting the results of a Machine Learning\ntechnique often termed as a black box is a difficult task. A counterfactual\nexplanation of a particular \"black box\" attempts to find the smallest change to\nthe input values that modifies the prediction to a particular output, other\nthan the original one. In this work we formulate the problem of finding a\ncounterfactual explanation as an optimization problem. We propose a new\n\"sparsity algorithm\" which solves the optimization problem, while also\nmaximizing the sparsity of the counterfactual explanation. We apply the\nsparsity algorithm to provide a simple suggestion to publicly traded companies\nin order to improve their credit ratings. We validate the sparsity algorithm\nwith a synthetically generated dataset and we further apply it to quarterly\nfinancial statements from companies in financial, healthcare and IT sectors of\nthe US market. We provide evidence that the counterfactual explanation can\ncapture the nature of the real statement features that changed between the\ncurrent quarter and the following quarter when ratings improved. The empirical\nresults show that the higher the rating of a company the greater the \"effort\"\nrequired to further improve credit rating.",
          "link": "http://arxiv.org/abs/2107.10306",
          "publishedOn": "2021-07-23T02:00:31.151Z",
          "wordCount": 637,
          "title": "A Sparsity Algorithm with Applications to Corporate Credit Rating. (arXiv:2107.10306v1 [q-fin.RM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mizrahi_I/0/1/0/all/0/1\">Itzik Mizrahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avidan_S/0/1/0/all/0/1\">Shai Avidan</a>",
          "description": "Deep Neural Networks require large amounts of labeled data for their\ntraining. Collecting this data at scale inevitably causes label noise.Hence,the\nneed to develop learning algorithms that are robust to label noise. In recent\nyears, k Nearest Neighbors (kNN) emerged as a viable solution to this problem.\nDespite its success, kNN is not without its problems. Mainly, it requires a\nhuge memory footprint to store all the training samples and it needs an\nadvanced data structure to allow for fast retrieval of the relevant examples,\ngiven a query sample. We propose a neural network, termed kNet, that learns to\nperform kNN. Once trained, we no longer need to store the training data, and\nprocessing a query sample is a simple matter of inference. To use kNet, we\nfirst train a preliminary network on the data set, and then train kNet on the\npenultimate layer of the preliminary network.We find that kNet gives a smooth\napproximation of kNN,and cannot handle the sharp label changes between samples\nthat kNN can exhibit. This indicates that currently kNet is best suited to\napproximate kNN with a fairly large k. Experiments on two data sets show that\nthis is the regime in which kNN works best,and can therefore be replaced by\nkNet.In practice, kNet consistently improve the results of all preliminary\nnetworks, in all label noise regimes, by up to 3%.",
          "link": "http://arxiv.org/abs/2107.09735",
          "publishedOn": "2021-07-22T02:03:13.021Z",
          "wordCount": 662,
          "title": "kNet: A Deep kNN Network To Handle Label Noise. (arXiv:2107.09735v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.04225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jason Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_S/0/1/0/all/0/1\">Santiago Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahrzad_H/0/1/0/all/0/1\">Hormoz Shahrzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1\">Risto Miikkulainen</a>",
          "description": "Metalearning of deep neural network (DNN) architectures and hyperparameters\nhas become an increasingly important area of research. At the same time,\nnetwork regularization has been recognized as a crucial dimension to effective\ntraining of DNNs. However, the role of metalearning in establishing effective\nregularization has not yet been fully explored. There is recent evidence that\nloss-function optimization could play this role, however it is computationally\nimpractical as an outer loop to full training. This paper presents an algorithm\ncalled Evolutionary Population-Based Training (EPBT) that interleaves the\ntraining of a DNN's weights with the metalearning of loss functions. They are\nparameterized using multivariate Taylor expansions that EPBT can directly\noptimize. Such simultaneous adaptation of weights and loss functions can be\ndeceptive, and therefore EPBT uses a quality-diversity heuristic called Novelty\nPulsation as well as knowledge distillation to prevent overfitting during\ntraining. On the CIFAR-10 and SVHN image classification benchmarks, EPBT\nresults in faster, more accurate learning. The discovered hyperparameters adapt\nto the training process and serve to regularize the learning task by\ndiscouraging overfitting to the labels. EPBT thus demonstrates a practical\ninstantiation of regularization metalearning based on simultaneous training.",
          "link": "http://arxiv.org/abs/2002.04225",
          "publishedOn": "2021-07-22T02:03:13.014Z",
          "wordCount": 666,
          "title": "Regularized Evolutionary Population-Based Training. (arXiv:2002.04225v4 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dikopoulou_Z/0/1/0/all/0/1\">Zoumpolia Dikopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moustakidis_S/0/1/0/all/0/1\">Serafeim Moustakidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_P/0/1/0/all/0/1\">Patrik Karlsson</a>",
          "description": "Explainable artificial intelligence (XAI) is an emerging new domain in which\na set of processes and tools allow humans to better comprehend the decisions\ngenerated by black box models. However, most of the available XAI tools are\noften limited to simple explanations mainly quantifying the impact of\nindividual features to the models' output. Therefore, human users are not able\nto understand how the features are related to each other to make predictions,\nwhereas the inner workings of the trained models remain hidden. This paper\ncontributes to the development of a novel graphical explainability tool that\nnot only indicates the significant features of the model but also reveals the\nconditional relationships between features and the inference capturing both the\ndirect and indirect impact of features to the models' decision. The proposed\nXAI methodology, termed as gLIME, provides graphical model-agnostic\nexplanations either at the global (for the entire dataset) or the local scale\n(for specific data points). It relies on a combination of local interpretable\nmodel-agnostic explanations (LIME) with graphical least absolute shrinkage and\nselection operator (GLASSO) producing undirected Gaussian graphical models.\nRegularization is adopted to shrink small partial correlation coefficients to\nzero providing sparser and more interpretable graphical explanations. Two\nwell-known classification datasets (BIOPSY and OAI) were selected to confirm\nthe superiority of gLIME over LIME in terms of both robustness and consistency\nover multiple permutations. Specifically, gLIME accomplished increased\nstability over the two datasets with respect to features' importance (76%-96%\ncompared to 52%-77% using LIME). gLIME demonstrates a unique potential to\nextend the functionality of the current state-of-the-art in XAI by providing\ninformative graphically given explanations that could unlock black boxes.",
          "link": "http://arxiv.org/abs/2107.09927",
          "publishedOn": "2021-07-22T02:03:12.993Z",
          "wordCount": 700,
          "title": "GLIME: A new graphical methodology for interpretable model-agnostic explanations. (arXiv:2107.09927v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.01009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiqiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xin Qin</a>",
          "description": "The success of machine learning applications often needs a large quantity of\ndata. Recently, federated learning (FL) is attracting increasing attention due\nto the demand for data privacy and security, especially in the medical field.\nHowever, the performance of existing FL approaches often deteriorates when\nthere exist domain shifts among clients, and few previous works focus on\npersonalization in healthcare. In this article, we propose FedHealth 2, an\nextension of FedHealth \\cite{chen2020fedhealth} to tackle domain shifts and get\npersonalized models for local clients. FedHealth 2 obtains the client\nsimilarities via a pretrained model, and then it averages all weighted models\nwith preserving local batch normalization. Wearable activity recognition and\nCOVID-19 auxiliary diagnosis experiments have evaluated that FedHealth 2 can\nachieve better accuracy (10%+ improvement for activity recognition) and\npersonalized healthcare without compromising privacy and security.",
          "link": "http://arxiv.org/abs/2106.01009",
          "publishedOn": "2021-07-22T02:03:12.987Z",
          "wordCount": 657,
          "title": "FedHealth 2: Weighted Federated Transfer Learning via Batch Normalization for Personalized Healthcare. (arXiv:2106.01009v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10066",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Galy_Fajou_T/0/1/0/all/0/1\">Th&#xe9;o Galy-Fajou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Opper_M/0/1/0/all/0/1\">Manfred Opper</a>",
          "description": "Gaussian Processes (\\textbf{GPs}) are flexible non-parametric models with\nstrong probabilistic interpretation. While being a standard choice for\nperforming inference on time series, GPs have few techniques to work in a\nstreaming setting. \\cite{bui2017streaming} developed an efficient variational\napproach to train online GPs by using sparsity techniques: The whole set of\nobservations is approximated by a smaller set of inducing points (\\textbf{IPs})\nand moved around with new data. Both the number and the locations of the IPs\nwill affect greatly the performance of the algorithm. In addition to optimizing\ntheir locations, we propose to adaptively add new points, based on the\nproperties of the GP and the structure of the data.",
          "link": "http://arxiv.org/abs/2107.10066",
          "publishedOn": "2021-07-22T02:03:12.980Z",
          "wordCount": 550,
          "title": "Adaptive Inducing Points Selection For Gaussian Processes. (arXiv:2107.10066v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10211",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_G/0/1/0/all/0/1\">Guodong Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hsu_K/0/1/0/all/0/1\">Kyle Hsu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_J/0/1/0/all/0/1\">Jianing Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Grosse_R/0/1/0/all/0/1\">Roger Grosse</a>",
          "description": "Annealed importance sampling (AIS) and related algorithms are highly\neffective tools for marginal likelihood estimation, but are not fully\ndifferentiable due to the use of Metropolis-Hastings (MH) correction steps.\nDifferentiability is a desirable property as it would admit the possibility of\noptimizing marginal likelihood as an objective using gradient-based methods. To\nthis end, we propose a differentiable AIS algorithm by abandoning MH steps,\nwhich further unlocks mini-batch computation. We provide a detailed convergence\nanalysis for Bayesian linear regression which goes beyond previous analyses by\nexplicitly accounting for non-perfect transitions. Using this analysis, we\nprove that our algorithm is consistent in the full-batch setting and provide a\nsublinear convergence rate. However, we show that the algorithm is inconsistent\nwhen mini-batch gradients are used due to a fundamental incompatibility between\nthe goals of last-iterate convergence to the posterior and elimination of the\npathwise stochastic error. This result is in stark contrast to our experience\nwith stochastic optimization and stochastic gradient Langevin dynamics, where\nthe effects of gradient noise can be washed out by taking more steps of a\nsmaller size. Our negative result relies crucially on our explicit\nconsideration of convergence to the stationary distribution, and it helps\nexplain the difficulty of developing practically effective AIS-like algorithms\nthat exploit mini-batch gradients.",
          "link": "http://arxiv.org/abs/2107.10211",
          "publishedOn": "2021-07-22T02:03:12.972Z",
          "wordCount": 652,
          "title": "Differentiable Annealed Importance Sampling and the Perils of Gradient Noise. (arXiv:2107.10211v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2004.05867",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Weitao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Richard Yi Da Xu</a>",
          "description": "The prevailing thinking is that orthogonal weights are crucial to enforcing\ndynamical isometry and speeding up training. The increase in learning speed\nthat results from orthogonal initialization in linear networks has been\nwell-proven. However, while the same is believed to also hold for nonlinear\nnetworks when the dynamical isometry condition is satisfied, the training\ndynamics behind this contention have not been thoroughly explored. In this\nwork, we study the dynamics of ultra-wide networks across a range of\narchitectures, including Fully Connected Networks (FCNs) and Convolutional\nNeural Networks (CNNs) with orthogonal initialization via neural tangent kernel\n(NTK). Through a series of propositions and lemmas, we prove that two NTKs, one\ncorresponding to Gaussian weights and one to orthogonal weights, are equal when\nthe network width is infinite. Further, during training, the NTK of an\northogonally-initialized infinite-width network should theoretically remain\nconstant. This suggests that the orthogonal initialization cannot speed up\ntraining in the NTK (lazy training) regime, contrary to the prevailing\nthoughts. In order to explore under what circumstances can orthogonality\naccelerate training, we conduct a thorough empirical investigation outside the\nNTK regime. We find that when the hyper-parameters are set to achieve a linear\nregime in nonlinear activation, orthogonal initialization can improve the\nlearning speed with a large learning rate or large depth.",
          "link": "http://arxiv.org/abs/2004.05867",
          "publishedOn": "2021-07-22T02:03:12.966Z",
          "wordCount": 711,
          "title": "On the Neural Tangent Kernel of Deep Networks with Orthogonal Initialization. (arXiv:2004.05867v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.00515",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rudner_T/0/1/0/all/0/1\">Tim G. J. Rudner</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Key_O/0/1/0/all/0/1\">Oscar Key</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1\">Tom Rainforth</a>",
          "description": "We show that the gradient estimates used in training Deep Gaussian Processes\n(DGPs) with importance-weighted variational inference are susceptible to\nsignal-to-noise ratio (SNR) issues. Specifically, we show both theoretically\nand via an extensive empirical evaluation that the SNR of the gradient\nestimates for the latent variable's variational parameters decreases as the\nnumber of importance samples increases. As a result, these gradient estimates\ndegrade to pure noise if the number of importance samples is too large. To\naddress this pathology, we show how doubly reparameterized gradient estimators,\noriginally proposed for training variational autoencoders, can be adapted to\nthe DGP setting and that the resultant estimators completely remedy the SNR\nissue, thereby providing more reliable training. Finally, we demonstrate that\nour fix can lead to consistent improvements in the predictive performance of\nDGP models.",
          "link": "http://arxiv.org/abs/2011.00515",
          "publishedOn": "2021-07-22T02:03:12.958Z",
          "wordCount": 614,
          "title": "On Signal-to-Noise Ratio Issues in Variational Inference for Deep Gaussian Processes. (arXiv:2011.00515v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_W/0/1/0/all/0/1\">Wenbo Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingzhen Li</a>",
          "description": "Scoring matching (SM), and its related counterpart, Stein discrepancy (SD)\nhave achieved great success in model training and evaluations. However, recent\nresearch shows their limitations when dealing with certain types of\ndistributions. One possible fix is incorporating the original score matching\n(or Stein discrepancy) with a diffusion matrix, which is called diffusion score\nmatching (DSM) (or diffusion Stein discrepancy (DSD)). However, the lack of\ninterpretation of the diffusion limits its usage within simple distributions\nand manually chosen matrix. In this work, we plan to fill this gap by\ninterpreting the diffusion matrix using normalizing flows. Specifically, we\ntheoretically prove that DSM (or DSD) is equivalent to the original score\nmatching (or Stein discrepancy) evaluated in the transformed space defined by\nthe normalizing flow, where the diffusion matrix is the inverse of the flow's\nJacobian matrix. In addition, we also build its connection to Riemannian\nmanifolds and further extend it to continuous flows, where the change of DSM is\ncharacterized by an ODE.",
          "link": "http://arxiv.org/abs/2107.10072",
          "publishedOn": "2021-07-22T02:03:12.952Z",
          "wordCount": 604,
          "title": "Interpreting diffusion score matching using normalizing flow. (arXiv:2107.10072v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1\">Feng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Han Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1\">Yilin Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_M/0/1/0/all/0/1\">Marcus Eng Hock Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Mengling Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wynne Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_B/0/1/0/all/0/1\">Bibhas Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nan Liu</a>",
          "description": "Objective: Temporal electronic health records (EHRs) can be a wealth of\ninformation for secondary uses, such as clinical events prediction or chronic\ndisease management. However, challenges exist for temporal data representation.\nWe therefore sought to identify these challenges and evaluate novel\nmethodologies for addressing them through a systematic examination of deep\nlearning solutions.\n\nMethods: We searched five databases (PubMed, EMBASE, the Institute of\nElectrical and Electronics Engineers [IEEE] Xplore Digital Library, the\nAssociation for Computing Machinery [ACM] digital library, and Web of Science)\ncomplemented with hand-searching in several prestigious computer science\nconference proceedings. We sought articles that reported deep learning\nmethodologies on temporal data representation in structured EHR data from\nJanuary 1, 2010, to August 30, 2020. We summarized and analyzed the selected\narticles from three perspectives: nature of time series, methodology, and model\nimplementation.\n\nResults: We included 98 articles related to temporal data representation\nusing deep learning. Four major challenges were identified, including data\nirregularity, data heterogeneity, data sparsity, and model opacity. We then\nstudied how deep learning techniques were applied to address these challenges.\nFinally, we discuss some open challenges arising from deep learning.\n\nConclusion: Temporal EHR data present several major challenges for clinical\nprediction modeling and data utilization. To some extent, current deep learning\nsolutions can address these challenges. Future studies can consider designing\ncomprehensive and integrated solutions. Moreover, researchers should\nincorporate additional clinical domain knowledge into study designs and enhance\nthe interpretability of the model to facilitate its implementation in clinical\npractice.",
          "link": "http://arxiv.org/abs/2107.09951",
          "publishedOn": "2021-07-22T02:03:12.945Z",
          "wordCount": 702,
          "title": "Deep learning for temporal data representation in electronic health records: A systematic review of challenges and methodologies. (arXiv:2107.09951v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1\">Chun Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sindhgatta_R/0/1/0/all/0/1\">Renuka Sindhgatta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_C/0/1/0/all/0/1\">Catarina Moreira</a>",
          "description": "Modern data analytics underpinned by machine learning techniques has become a\nkey enabler to the automation of data-led decision making. As an important\nbranch of state-of-the-art data analytics, business process predictions are\nalso faced with a challenge in regard to the lack of explanation to the\nreasoning and decision by the underlying `black-box' prediction models. With\nthe development of interpretable machine learning techniques, explanations can\nbe generated for a black-box model, making it possible for (human) users to\naccess the reasoning behind machine learned predictions. In this paper, we aim\nto present an approach that allows us to use model explanations to investigate\ncertain reasoning applied by machine learned predictions and detect potential\nissues with the underlying methods thus enhancing trust in business process\nprediction models. A novel contribution of our approach is the proposal of\nmodel inspection that leverages both the explanations generated by\ninterpretable machine learning mechanisms and the contextual or domain\nknowledge extracted from event logs that record historical process execution.\nFindings drawn from this work are expected to serve as a key input to\ndeveloping model reliability metrics and evaluation in the context of business\nprocess predictions.",
          "link": "http://arxiv.org/abs/2107.09767",
          "publishedOn": "2021-07-22T02:03:12.924Z",
          "wordCount": 631,
          "title": "Explainable AI Enabled Inspection of Business Process Prediction Models. (arXiv:2107.09767v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leanza_A/0/1/0/all/0/1\">Antonio Leanza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reina_G/0/1/0/all/0/1\">Giulio Reina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanco_Claraco_J/0/1/0/all/0/1\">Jose-Luis Blanco-Claraco</a>",
          "description": "Sideslip angle is an important variable for understanding and monitoring\nvehicle dynamics but it lacks an inexpensive method for direct measurement.\nTherefore, it is typically estimated from inertial and other proprioceptive\nsensors onboard using filtering methods from the family of the Kalman Filter.\nAs a novel alternative, this work proposes modelling the problem directly as a\ngraphical model (factor graph), which can then be optimized using a variety of\nmethods, such as whole dataset batch optimization for offline processing or\nfixed-lag smoother for on-line operation. Experimental results on real vehicle\ndatasets validate the proposal with a good agreement between estimated and\nactual sideslip angle, showing similar performance than the state-of-the-art\nwith a great potential for future extensions due to the flexible mathematical\nframework.",
          "link": "http://arxiv.org/abs/2107.09815",
          "publishedOn": "2021-07-22T02:03:12.906Z",
          "wordCount": 559,
          "title": "A Factor Graph-based approach to vehicle sideslip angle estimation. (arXiv:2107.09815v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09781",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Useche_D/0/1/0/all/0/1\">Diego H. Useche</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Giraldo_Carvajal_A/0/1/0/all/0/1\">Andres Giraldo-Carvajal</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zuluaga_Bucheli_H/0/1/0/all/0/1\">Hernan M. Zuluaga-Bucheli</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Jaramillo_Villegas_J/0/1/0/all/0/1\">Jose A. Jaramillo-Villegas</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Gonzalez_F/0/1/0/all/0/1\">Fabio A. Gonz&#xe1;lez</a>",
          "description": "This paper presents a hybrid classical-quantum program for density estimation\nand supervised classification. The program is implemented as a quantum circuit\nin a high-dimensional quantum computer simulator. We show that the proposed\nquantum protocols allow to estimate probability density functions and to make\npredictions in a supervised learning manner. This model can be generalized to\nfind expected values of density matrices in high-dimensional quantum computers.\nExperiments on various data sets are presented. Results show that the proposed\nmethod is a viable strategy to implement supervised classification and density\nestimation in a high-dimensional quantum computer.",
          "link": "http://arxiv.org/abs/2107.09781",
          "publishedOn": "2021-07-22T02:03:12.887Z",
          "wordCount": 532,
          "title": "Quantum Measurement Classification with Qudits. (arXiv:2107.09781v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Behera_M/0/1/0/all/0/1\">Monik Raj Behera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Sudhir Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_S/0/1/0/all/0/1\">Suresh Shetty</a>",
          "description": "Over the recent years, Federated machine learning continues to gain interest\nand momentum where there is a need to draw insights from data while preserving\nthe data provider's privacy. However, one among other existing challenges in\nthe adoption of federated learning has been the lack of fair, transparent and\nuniversally agreed incentivization schemes for rewarding the federated learning\ncontributors. Smart contracts on a blockchain network provide transparent,\nimmutable and independently verifiable proofs by all participants of the\nnetwork. We leverage this open and transparent nature of smart contracts on a\nblockchain to define incentivization rules for the contributors, which is based\non a novel scalar quantity - federated contribution. Such a smart contract\nbased reward-driven model has the potential to revolutionize the federated\nlearning adoption in enterprises. Our contribution is two-fold: first is to\nshow how smart contract based blockchain can be a very natural communication\nchannel for federated learning. Second, leveraging this infrastructure, we can\nshow how an intuitive measure of each agents' contribution can be built and\nintegrated with the life cycle of the training and reward process.",
          "link": "http://arxiv.org/abs/2107.10243",
          "publishedOn": "2021-07-22T02:03:12.861Z",
          "wordCount": 631,
          "title": "Federated Learning using Smart Contracts on Blockchains, based on Reward Driven Approach. (arXiv:2107.10243v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Liang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "Conditional generative models aim to learn the underlying joint distribution\nof data and labels, and thus realize conditional generation. Among them,\nauxiliary classifier generative adversarial networks (AC-GAN) have been widely\nused, but suffer from the issue of low intra-class diversity on generated\nsamples. In this paper, we point out that the fundamental reason is that the\nclassifier of AC-GAN is generator-agnostic, and thus cannot provide informative\nguidance to the generator to approximate the target joint distribution, leading\nto a minimization of conditional entropy that decreases the intra-class\ndiversity. Based on this finding, we propose novel cGANs with auxiliary\ndiscriminative classifier (ADC-GAN) to address the issue of AC-GAN.\nSpecifically, the auxiliary discriminative classifier becomes generator-aware\nby distinguishing between the real and fake data while recognizing their\nlabels. We then optimize the generator based on the auxiliary classifier along\nwith the original discriminator to match the joint and marginal distributions\nof the generated samples with those of the real samples. We provide theoretical\nanalysis and empirical evidence on synthetic and real-world datasets to\ndemonstrate the superiority of the proposed ADC-GAN compared to competitive\ncGANs.",
          "link": "http://arxiv.org/abs/2107.10060",
          "publishedOn": "2021-07-22T02:03:12.853Z",
          "wordCount": 613,
          "title": "CGANs with Auxiliary Discriminative Classifier. (arXiv:2107.10060v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.05303",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1\">Vivian Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>",
          "description": "Although AI holds promise for improving human decision making in societally\ncritical domains, it remains an open question how human-AI teams can reliably\noutperform AI alone and human alone in challenging prediction tasks (also known\nas complementary performance). We explore two directions to understand the gaps\nin achieving complementary performance. First, we argue that the typical\nexperimental setup limits the potential of human-AI teams. To account for lower\nAI performance out-of-distribution than in-distribution because of distribution\nshift, we design experiments with different distribution types and investigate\nhuman performance for both in-distribution and out-of-distribution examples.\nSecond, we develop novel interfaces to support interactive explanations so that\nhumans can actively engage with AI assistance. Using virtual pilot studies and\nlarge-scale randomized experiments across three tasks, we demonstrate a clear\ndifference between in-distribution and out-of-distribution, and observe mixed\nresults for interactive explanations: while interactive explanations improve\nhuman perception of AI assistance's usefulness, they may reinforce human biases\nand lead to limited performance improvement. Overall, our work points out\ncritical challenges and future directions towards enhancing human performance\nwith AI assistance.",
          "link": "http://arxiv.org/abs/2101.05303",
          "publishedOn": "2021-07-22T02:03:12.846Z",
          "wordCount": 670,
          "title": "Understanding the Effect of Out-of-distribution Examples and Interactive Explanations on Human-AI Decision Making. (arXiv:2101.05303v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thanapalasingam_T/0/1/0/all/0/1\">Thiviyan Thanapalasingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berkel_L/0/1/0/all/0/1\">Lucas van Berkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloem_P/0/1/0/all/0/1\">Peter Bloem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groth_P/0/1/0/all/0/1\">Paul Groth</a>",
          "description": "In this paper, we describe a reproduction of the Relational Graph\nConvolutional Network (RGCN). Using our reproduction, we explain the intuition\nbehind the model. Our reproduction results empirically validate the correctness\nof our implementations using benchmark Knowledge Graph datasets on node\nclassification and link prediction tasks. Our explanation provides a friendly\nunderstanding of the different components of the RGCN for both users and\nresearchers extending the RGCN approach. Furthermore, we introduce two new\nconfigurations of the RGCN that are more parameter efficient. The code and\ndatasets are available at https://github.com/thiviyanT/torch-rgcn.",
          "link": "http://arxiv.org/abs/2107.10015",
          "publishedOn": "2021-07-22T02:03:12.839Z",
          "wordCount": 520,
          "title": "Relational Graph Convolutional Networks: A Closer Look. (arXiv:2107.10015v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2003.01247",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Granziol_D/0/1/0/all/0/1\">Diego Granziol</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wan_X/0/1/0/all/0/1\">Xingchen Wan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>",
          "description": "We analyse and explain the increased generalisation performance\n\\latestEdits{of} Iterate Averaging using a Gaussian Process perturbation model\nbetween the true and batch risk surface on the high dimensional quadratic. %\nBased on our theoretical results We derive three phenomena \\latestEdits{from\nour theoretical results:} (1) The importance of combining iterate averaging\nwith large learning rates and regularisation for improved regularisation (2)\nJustification for less frequent averaging. (3) That we expect adaptive gradient\nmethods to work equally well or better with iterate averaging than their non\nadaptive counterparts. Inspired by these results\\latestEdits{, together with}\nempirical investigations of the importance of appropriate regularisation for\nthe solution diversity of the iterates, we propose two adaptive algorithms with\niterate averaging. \\latestEdits{These} give significantly better results than\nSGD, require less tuning and do not require early stopping or validation set\nmonitoring. We showcase the efficacy of our approach on the CIFAR-10/100,\nImageNet and Penn Treebank datasets on a variety of modern and classical\nnetwork architectures.",
          "link": "http://arxiv.org/abs/2003.01247",
          "publishedOn": "2021-07-22T02:03:12.832Z",
          "wordCount": 630,
          "title": "Iterative Averaging in the Quest for Best Test Error. (arXiv:2003.01247v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadrtdinov_I/0/1/0/all/0/1\">Ildus Sadrtdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1\">Nadezhda Chirkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobacheva_E/0/1/0/all/0/1\">Ekaterina Lobacheva</a>",
          "description": "Memorization studies of deep neural networks (DNNs) help to understand what\npatterns and how do DNNs learn, and motivate improvements to DNN training\napproaches. In this work, we investigate the memorization properties of SimCLR,\na widely used contrastive self-supervised learning approach, and compare them\nto the memorization of supervised learning and random labels training. We find\nthat both training objects and augmentations may have different complexity in\nthe sense of how SimCLR learns them. Moreover, we show that SimCLR is similar\nto random labels training in terms of the distribution of training objects\ncomplexity.",
          "link": "http://arxiv.org/abs/2107.10143",
          "publishedOn": "2021-07-22T02:03:12.825Z",
          "wordCount": 538,
          "title": "On the Memorization Properties of Contrastive Learning. (arXiv:2107.10143v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prabhu_V/0/1/0/all/0/1\">Viraj Prabhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khare_S/0/1/0/all/0/1\">Shivam Khare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kartik_D/0/1/0/all/0/1\">Deeksha Kartik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">Judy Hoffman</a>",
          "description": "Most modern approaches for domain adaptive semantic segmentation rely on\ncontinued access to source data during adaptation, which may be infeasible due\nto computational or privacy constraints. We focus on source-free domain\nadaptation for semantic segmentation, wherein a source model must adapt itself\nto a new target domain given only unlabeled target data. We propose\nSelf-Supervised Selective Self-Training (S4T), a source-free adaptation\nalgorithm that first uses the model's pixel-level predictive consistency across\ndiverse views of each target image along with model confidence to classify\npixel predictions as either reliable or unreliable. Next, the model is\nself-trained, using predicted pseudolabels for reliable predictions and\npseudolabels inferred via a selective interpolation strategy for unreliable\nones. S4T matches or improves upon the state-of-the-art in source-free\nadaptation on 3 standard benchmarks for semantic segmentation within a single\nepoch of adaptation.",
          "link": "http://arxiv.org/abs/2107.10140",
          "publishedOn": "2021-07-22T02:03:12.801Z",
          "wordCount": 581,
          "title": "S4T: Source-free domain adaptation for semantic segmentation via self-supervised selective self-training. (arXiv:2107.10140v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bertossi_L/0/1/0/all/0/1\">Leopoldo Bertossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_G/0/1/0/all/0/1\">Gabriela Reyes</a>",
          "description": "We describe how answer-set programs can be used to declaratively specify\ncounterfactual interventions on entities under classification, and reason about\nthem. In particular, they can be used to define and compute responsibility\nscores as attribution-based explanations for outcomes from classification\nmodels. The approach allows for the inclusion of domain knowledge and supports\nquery answering. A detailed example with a naive-Bayes classifier is presented.",
          "link": "http://arxiv.org/abs/2107.10159",
          "publishedOn": "2021-07-22T02:03:12.794Z",
          "wordCount": 524,
          "title": "Answer-Set Programs for Reasoning about Counterfactual Interventions and Responsibility Scores for Classification. (arXiv:2107.10159v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zanette_A/0/1/0/all/0/1\">Andrea Zanette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_K/0/1/0/all/0/1\">Kefan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jonathan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1\">Emma Brunskill</a>",
          "description": "In the stochastic linear contextual bandit setting there exist several\nminimax procedures for exploration with policies that are reactive to the data\nbeing acquired. In practice, there can be a significant engineering overhead to\ndeploy these algorithms, especially when the dataset is collected in a\ndistributed fashion or when a human in the loop is needed to implement a\ndifferent policy. Exploring with a single non-reactive policy is beneficial in\nsuch cases. Assuming some batch contexts are available, we design a single\nstochastic policy to collect a good dataset from which a near-optimal policy\ncan be extracted. We present a theoretical analysis as well as numerical\nexperiments on both synthetic and real-world datasets.",
          "link": "http://arxiv.org/abs/2107.09912",
          "publishedOn": "2021-07-22T02:03:12.787Z",
          "wordCount": 551,
          "title": "Design of Experiments for Stochastic Contextual Linear Bandits. (arXiv:2107.09912v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahdavimoghaddama_M/0/1/0/all/0/1\">Mahnoosh Mahdavimoghaddama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikanjama_A/0/1/0/all/0/1\">Amin Nikanjama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdoos_M/0/1/0/all/0/1\">Monireh Abdoos</a>",
          "description": "Cooperative multi-agent systems are being widely used in different domains.\nInteraction among agents would bring benefits, including reducing operating\ncosts, high scalability, and facilitating parallel processing. These systems\nare also a good option for handling large-scale, unknown, and dynamic\nenvironments. However, learning in these environments has become a very\nimportant challenge in various applications. These challenges include the\neffect of search space size on learning time, inefficient cooperation among\nagents, and the lack of proper coordination among agents' decisions. Moreover,\nreinforcement learning algorithms may suffer from long convergence time in\nthese problems. In this paper, a communication framework using knowledge\ntransfer concepts is introduced to address such challenges in the herding\nproblem with large state space. To handle the problems of convergence,\nknowledge transfer has been utilized that can significantly increase the\nefficiency of reinforcement learning algorithms. Coordination between the\nagents is carried out through a head agent in each group of agents and a\ncoordinator agent respectively. The results demonstrate that this framework\ncould indeed enhance the speed of learning and reduce convergence time.",
          "link": "http://arxiv.org/abs/2107.09807",
          "publishedOn": "2021-07-22T02:03:12.780Z",
          "wordCount": 628,
          "title": "Multi-agent Reinforcement Learning Improvement in a Dynamic Environment Using Knowledge Transfer. (arXiv:2107.09807v1 [cs.MA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10125",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ober_S/0/1/0/all/0/1\">Sebastian W. Ober</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1\">Laurence Aitchison</a>",
          "description": "Recent work introduced deep kernel processes as an entirely kernel-based\nalternative to NNs (Aitchison et al. 2020). Deep kernel processes flexibly\nlearn good top-layer representations by alternately sampling the kernel from a\ndistribution over positive semi-definite matrices and performing nonlinear\ntransformations. A particular deep kernel process, the deep Wishart process\n(DWP), is of particular interest because its prior is equivalent to deep\nGaussian process (DGP) priors. However, inference in DWPs has not yet been\npossible due to the lack of sufficiently flexible distributions over positive\nsemi-definite matrices. Here, we give a novel approach to obtaining flexible\ndistributions over positive semi-definite matrices by generalising the Bartlett\ndecomposition of the Wishart probability density. We use this new distribution\nto develop an approximate posterior for the DWP that includes dependency across\nlayers. We develop a doubly-stochastic inducing-point inference scheme for the\nDWP and show experimentally that inference in the DWP gives improved\nperformance over doing inference in a DGP with the equivalent prior.",
          "link": "http://arxiv.org/abs/2107.10125",
          "publishedOn": "2021-07-22T02:03:12.772Z",
          "wordCount": 597,
          "title": "A variational approximate posterior for the deep Wishart process. (arXiv:2107.10125v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2103.10492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fuad_M/0/1/0/all/0/1\">Md. Tahmid Hasan Fuad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fime_A/0/1/0/all/0/1\">Awal Ahmed Fime</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikder_D/0/1/0/all/0/1\">Delowar Sikder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iftee_M/0/1/0/all/0/1\">Md. Akil Raihan Iftee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbi_J/0/1/0/all/0/1\">Jakaria Rabbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_rakhami_M/0/1/0/all/0/1\">Mabrook S. Al-rakhami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gumae_A/0/1/0/all/0/1\">Abdu Gumae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_O/0/1/0/all/0/1\">Ovishake Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuad_M/0/1/0/all/0/1\">Mohtasim Fuad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md. Nazrul Islam</a>",
          "description": "In recent years, researchers have proposed many deep learning (DL) methods\nfor various tasks, and particularly face recognition (FR) made an enormous leap\nusing these techniques. Deep FR systems benefit from the hierarchical\narchitecture of the DL methods to learn discriminative face representation.\nTherefore, DL techniques significantly improve state-of-the-art performance on\nFR systems and encourage diverse and efficient real-world applications. In this\npaper, we present a comprehensive analysis of various FR systems that leverage\nthe different types of DL techniques, and for the study, we summarize 168\nrecent contributions from this area. We discuss the papers related to different\nalgorithms, architectures, loss functions, activation functions, datasets,\nchallenges, improvement ideas, current and future trends of DL-based FR\nsystems. We provide a detailed discussion of various DL methods to understand\nthe current state-of-the-art, and then we discuss various activation and loss\nfunctions for the methods. Additionally, we summarize different datasets used\nwidely for FR tasks and discuss challenges related to illumination, expression,\npose variations, and occlusion. Finally, we discuss improvement ideas, current\nand future trends of FR tasks.",
          "link": "http://arxiv.org/abs/2103.10492",
          "publishedOn": "2021-07-22T02:03:12.750Z",
          "wordCount": 710,
          "title": "Recent Advances in Deep Learning Techniques for Face Recognition. (arXiv:2103.10492v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10014",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kloepfer_D/0/1/0/all/0/1\">Dominik Kloepfer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Aviles_Rivero_A/0/1/0/all/0/1\">Angelica I. Aviles-Rivero</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Heydecker_D/0/1/0/all/0/1\">Daniel Heydecker</a>",
          "description": "Graph vertex embeddings based on random walks have become increasingly\ninfluential in recent years, showing good performance in several tasks as they\nefficiently transform a graph into a more computationally digestible format\nwhile preserving relevant information. However, the theoretical properties of\nsuch algorithms, in particular the influence of hyperparameters and of the\ngraph structure on their convergence behaviour, have so far not been\nwell-understood. In this work, we provide a theoretical analysis for\nrandom-walks based embeddings techniques. Firstly, we prove that, under some\nweak assumptions, vertex embeddings derived from random walks do indeed\nconverge both in the single limit of the number of random walks $N \\to \\infty$\nand in the double limit of both $N$ and the length of each random walk\n$L\\to\\infty$. Secondly, we derive concentration bounds quantifying the converge\nrate of the corpora for the single and double limits. Thirdly, we use these\nresults to derive a heuristic for choosing the hyperparameters $N$ and $L$. We\nvalidate and illustrate the practical importance of our findings with a range\nof numerical and visual experiments on several graphs drawn from real-world\napplications.",
          "link": "http://arxiv.org/abs/2107.10014",
          "publishedOn": "2021-07-22T02:03:12.743Z",
          "wordCount": 625,
          "title": "Delving Into Deep Walkers: A Convergence Analysis of Random-Walk-Based Vertex Embeddings. (arXiv:2107.10014v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_D/0/1/0/all/0/1\">Daniel Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stapleton_L/0/1/0/all/0/1\">Logan Stapleton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syrgkanis_V/0/1/0/all/0/1\">Vasilis Syrgkanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiwei Steven Wu</a>",
          "description": "Randomized experiments can be susceptible to selection bias due to potential\nnon-compliance by the participants. While much of the existing work has studied\ncompliance as a static behavior, we propose a game-theoretic model to study\ncompliance as dynamic behavior that may change over time. In rounds, a social\nplanner interacts with a sequence of heterogeneous agents who arrive with their\nunobserved private type that determines both their prior preferences across the\nactions (e.g., control and treatment) and their baseline rewards without taking\nany treatment. The planner provides each agent with a randomized recommendation\nthat may alter their beliefs and their action selection. We develop a novel\nrecommendation mechanism that views the planner's recommendation as a form of\ninstrumental variable (IV) that only affects an agents' action selection, but\nnot the observed rewards. We construct such IVs by carefully mapping the\nhistory -- the interactions between the planner and the previous agents -- to a\nrandom recommendation. Even though the initial agents may be completely\nnon-compliant, our mechanism can incentivize compliance over time, thereby\nenabling the estimation of the treatment effect of each treatment, and\nminimizing the cumulative regret of the planner whose goal is to identify the\noptimal treatment.",
          "link": "http://arxiv.org/abs/2107.10093",
          "publishedOn": "2021-07-22T02:03:12.735Z",
          "wordCount": 654,
          "title": "Incentivizing Compliance with Algorithmic Instruments. (arXiv:2107.10093v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Huimin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhengmian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1\">Bin Gu</a>",
          "description": "Adversarial attacks by generating examples which are almost indistinguishable\nfrom natural examples, pose a serious threat to learning models. Defending\nagainst adversarial attacks is a critical element for a reliable learning\nsystem. Support vector machine (SVM) is a classical yet still important\nlearning algorithm even in the current deep learning era. Although a wide range\nof researches have been done in recent years to improve the adversarial\nrobustness of learning models, but most of them are limited to deep neural\nnetworks (DNNs) and the work for kernel SVM is still vacant. In this paper, we\naim at kernel SVM and propose adv-SVM to improve its adversarial robustness via\nadversarial training, which has been demonstrated to be the most promising\ndefense techniques. To the best of our knowledge, this is the first work that\ndevotes to the fast and scalable adversarial training of kernel SVM.\nSpecifically, we first build connection of perturbations of samples between\noriginal and kernel spaces, and then give a reduced and equivalent formulation\nof adversarial training of kernel SVM based on the connection. Next, doubly\nstochastic gradients (DSG) based on two unbiased stochastic approximations\n(i.e., one is on training points and another is on random features) are applied\nto update the solution of our objective function. Finally, we prove that our\nalgorithm optimized by DSG converges to the optimal solution at the rate of\nO(1/t) under the constant and diminishing stepsizes. Comprehensive experimental\nresults show that our adversarial training algorithm enjoys robustness against\nvarious attacks and meanwhile has the similar efficiency and scalability with\nclassical DSG algorithm.",
          "link": "http://arxiv.org/abs/2107.09937",
          "publishedOn": "2021-07-22T02:03:12.727Z",
          "wordCount": 701,
          "title": "Fast and Scalable Adversarial Training of Kernel SVM via Doubly Stochastic Gradients. (arXiv:2107.09937v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meyer_M/0/1/0/all/0/1\">Matthias Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenner_M/0/1/0/all/0/1\">Michaela Wenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hibert_C/0/1/0/all/0/1\">Cl&#xe9;ment Hibert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walter_F/0/1/0/all/0/1\">Fabian Walter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiele_L/0/1/0/all/0/1\">Lothar Thiele</a>",
          "description": "Real-world datasets collected with sensor networks often contain incomplete\nand uncertain labels as well as artefacts arising from the system environment.\nComplete and reliable labeling is often infeasible for large-scale and\nlong-term sensor network deployments due to the labor and time overhead,\nlimited availability of experts and missing ground truth. In addition, if the\nmachine learning method used for analysis is sensitive to certain features of a\ndeployment, labeling and learning needs to be repeated for every new\ndeployment. To address these challenges, we propose to make use of system\ncontext information formalized in an information graph and embed it in the\nlearning process via contrastive learning. Based on real-world data we show\nthat this approach leads to an increased accuracy in case of weakly labeled\ndata and leads to an increased robustness and transferability of the classifier\nto new sensor locations.",
          "link": "http://arxiv.org/abs/2107.10236",
          "publishedOn": "2021-07-22T02:03:12.719Z",
          "wordCount": 590,
          "title": "Using system context information to complement weakly labeled data. (arXiv:2107.10236v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kunhong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yucheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yahong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingshuai Li</a>",
          "description": "In recent years, researchers have been paying increasing attention to the\nthreats brought by deep learning models to data security and privacy,\nespecially in the field of domain adaptation. Existing unsupervised domain\nadaptation (UDA) methods can achieve promising performance without transferring\ndata from source domain to target domain. However, UDA with representation\nalignment or self-supervised pseudo-labeling relies on the transferred source\nmodels. In many data-critical scenarios, methods based on model transferring\nmay suffer from membership inference attacks and expose private data. In this\npaper, we aim to overcome a challenging new setting where the source models are\nonly queryable but cannot be transferred to the target domain. We propose\nBlack-box Probe Domain Adaptation (BPDA), which adopts query mechanism to probe\nand refine information from source model using third-party dataset. In order to\ngain more informative query results, we further propose Distributionally\nAdversarial Training (DAT) to align the distribution of third-party data with\nthat of target data. BPDA uses public third-party dataset and adversarial\nexamples based on DAT as the information carrier between source and target\ndomains, dispensing with transferring source data or model. Experimental\nresults on benchmarks of Digit-Five, Office-Caltech, Office-31, Office-Home,\nand DomainNet demonstrate the feasibility of BPDA without model transferring.",
          "link": "http://arxiv.org/abs/2107.10174",
          "publishedOn": "2021-07-22T02:03:12.698Z",
          "wordCount": 637,
          "title": "Black-box Probe for Unsupervised Domain Adaptation without Model Transferring. (arXiv:2107.10174v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09716",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Severo_D/0/1/0/all/0/1\">Daniel Severo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Domanovitz_E/0/1/0/all/0/1\">Elad Domanovitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khisti_A/0/1/0/all/0/1\">Ashish Khisti</a>",
          "description": "Traditionally, quantization is designed to minimize the reconstruction error\nof a data source. When considering downstream classification tasks, other\nmeasures of distortion can be of interest; such as the 0-1 classification loss.\nFurthermore, it is desirable that the performance of these quantizers not\ndeteriorate once they are deployed into production, as relearning the scheme\nonline is not always possible. In this work, we present a class of algorithms\nthat learn distributed quantization schemes for binary classification tasks.\nOur method performs well on unseen data, and is faster than previous methods\nproportional to a quadratic term of the dataset size. It works by regularizing\nthe 0-1 loss with the reconstruction error. We present experiments on synthetic\nmixture and bivariate Gaussian data and compare training, testing, and\ngeneralization errors with a family of benchmark quantization schemes from the\nliterature. Our method is called Regularized Classification-Aware Quantization.",
          "link": "http://arxiv.org/abs/2107.09716",
          "publishedOn": "2021-07-22T02:03:12.691Z",
          "wordCount": 578,
          "title": "Regularized Classification-Aware Quantization. (arXiv:2107.09716v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10201",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Sonnerat_N/0/1/0/all/0/1\">Nicolas Sonnerat</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_P/0/1/0/all/0/1\">Pengming Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ktena_I/0/1/0/all/0/1\">Ira Ktena</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bartunov_S/0/1/0/all/0/1\">Sergey Bartunov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nair_V/0/1/0/all/0/1\">Vinod Nair</a>",
          "description": "Large Neighborhood Search (LNS) is a combinatorial optimization heuristic\nthat starts with an assignment of values for the variables to be optimized, and\niteratively improves it by searching a large neighborhood around the current\nassignment. In this paper we consider a learning-based LNS approach for mixed\ninteger programs (MIPs). We train a Neural Diving model to represent a\nprobability distribution over assignments, which, together with an existing MIP\nsolver, generates an initial assignment. Formulating the subsequent search\nsteps as a Markov Decision Process, we train a Neural Neighborhood Selection\npolicy to select a search neighborhood at each step, which is searched using a\nMIP solver to find the next assignment. The policy network is trained using\nimitation learning. We propose a target policy for imitation that, given enough\ncompute resources, is guaranteed to select the neighborhood containing the\noptimal next assignment across all possible choices for the neighborhood of a\nspecified size. Our approach matches or outperforms all the baselines on five\nreal-world MIP datasets with large-scale instances from diverse applications,\nincluding two production applications at Google. At large running times it\nachieves $2\\times$ to $37.8\\times$ better average primal gap than the best\nbaseline on three of the datasets.",
          "link": "http://arxiv.org/abs/2107.10201",
          "publishedOn": "2021-07-22T02:03:12.682Z",
          "wordCount": 641,
          "title": "Learning a Large Neighborhood Search Algorithm for Mixed Integer Programs. (arXiv:2107.10201v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koutras_D/0/1/0/all/0/1\">Dimitrios I. Koutras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoutsis_A/0/1/0/all/0/1\">Athanasios Ch. Kapoutsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amanatiadis_A/0/1/0/all/0/1\">Angelos A. Amanatiadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosmatopoulos_E/0/1/0/all/0/1\">Elias B. Kosmatopoulos</a>",
          "description": "This paper is an initial endeavor to bridge the gap between powerful Deep\nReinforcement Learning methodologies and the problem of exploration/coverage of\nunknown terrains. Within this scope, MarsExplorer, an openai-gym compatible\nenvironment tailored to exploration/coverage of unknown areas, is presented.\nMarsExplorer translates the original robotics problem into a Reinforcement\nLearning setup that various off-the-shelf algorithms can tackle. Any learned\npolicy can be straightforwardly applied to a robotic platform without an\nelaborate simulation model of the robot's dynamics to apply a different\nlearning/adaptation phase. One of its core features is the controllable\nmulti-dimensional procedural generation of terrains, which is the key for\nproducing policies with strong generalization capabilities. Four different\nstate-of-the-art RL algorithms (A3C, PPO, Rainbow, and SAC) are trained on the\nMarsExplorer environment, and a proper evaluation of their results compared to\nthe average human-level performance is reported. In the follow-up experimental\nanalysis, the effect of the multi-dimensional difficulty setting on the\nlearning capabilities of the best-performing algorithm (PPO) is analyzed. A\nmilestone result is the generation of an exploration policy that follows the\nHilbert curve without providing this information to the environment or\nrewarding directly or indirectly Hilbert-curve-like trajectories. The\nexperimental analysis is concluded by comparing PPO learned policy results with\nfrontier-based exploration context for extended terrain sizes. The source code\ncan be found at: https://github.com/dimikout3/GeneralExplorationPolicy.",
          "link": "http://arxiv.org/abs/2107.09996",
          "publishedOn": "2021-07-22T02:03:12.675Z",
          "wordCount": 670,
          "title": "MarsExplorer: Exploration of Unknown Terrains via Deep Reinforcement Learning and Procedurally Generated Environments. (arXiv:2107.09996v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinyi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Cheng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yaochen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shuiwang Ji</a>",
          "description": "We study self-supervised learning on graphs using contrastive methods. A\ngeneral scheme of prior methods is to optimize two-view representations of\ninput graphs. In many studies, a single graph-level representation is computed\nas one of the contrastive objectives, capturing limited characteristics of\ngraphs. We argue that contrasting graphs in multiple subspaces enables graph\nencoders to capture more abundant characteristics. To this end, we propose a\ngroup contrastive learning framework in this work. Our framework embeds the\ngiven graph into multiple subspaces, of which each representation is prompted\nto encode specific characteristics of graphs. To learn diverse and informative\nrepresentations, we develop principled objectives that enable us to capture the\nrelations among both intra-space and inter-space representations in groups.\nUnder the proposed framework, we further develop an attention-based representor\nfunction to compute representations that capture different substructures of a\ngiven graph. Built upon our framework, we extend two current methods into\nGroupCL and GroupIG, equipped with the proposed objective. Comprehensive\nexperimental results show our framework achieves a promising boost in\nperformance on a variety of datasets. In addition, our qualitative results show\nthat features generated from our representor successfully capture various\nspecific characteristics of graphs.",
          "link": "http://arxiv.org/abs/2107.09787",
          "publishedOn": "2021-07-22T02:03:12.667Z",
          "wordCount": 620,
          "title": "Group Contrastive Self-Supervised Learning on Graphs. (arXiv:2107.09787v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dona_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;mie Dona</a> (MLIA), <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a> (MLIA)",
          "description": "We consider the task of feature selection for reconstruction which consists\nin choosing a small subset of features from which whole data instances can be\nreconstructed. This is of particular importance in several contexts involving\nfor example costly physical measurements, sensor placement or information\ncompression. To break the intrinsic combinatorial nature of this problem, we\nformulate the task as optimizing a binary mask distribution enabling an\naccurate reconstruction. We then face two main challenges. One concerns\ndifferentiability issues due to the binary distribution. The second one\ncorresponds to the elimination of redundant information by selecting variables\nin a correlated fashion which requires modeling the covariance of the binary\ndistribution. We address both issues by introducing a relaxation of the problem\nvia a novel reparameterization of the logitNormal distribution. We demonstrate\nthat the proposed method provides an effective exploration scheme and leads to\nefficient feature selection for reconstruction through evaluation on several\nhigh dimensional image benchmarks. We show that the method leverages the\nintrinsic geometry of the data, facilitating reconstruction.",
          "link": "http://arxiv.org/abs/2107.10030",
          "publishedOn": "2021-07-22T02:03:12.647Z",
          "wordCount": 614,
          "title": "Differentiable Feature Selection, a Reparameterization Approach. (arXiv:2107.10030v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingtao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_C/0/1/0/all/0/1\">Chaitali Chakrabarti</a>",
          "description": "Split learning is a promising privacy-preserving distributed learning scheme\nthat has low computation requirement at the edge device but has the\ndisadvantage of high communication overhead between edge device and server. To\nreduce the communication overhead, this paper proposes a loss-based\nasynchronous training scheme that updates the client-side model less frequently\nand only sends/receives activations/gradients in selected epochs. To further\nreduce the communication overhead, the activations/gradients are quantized\nusing 8-bit floating point prior to transmission. An added benefit of the\nproposed communication reduction method is that the computations at the client\nside are reduced due to reduction in the number of client model updates.\nFurthermore, the privacy of the proposed communication reduction based split\nlearning method is almost the same as traditional split learning. Simulation\nresults on VGG11, VGG13 and ResNet18 models on CIFAR-10 show that the\ncommunication cost is reduced by 1.64x-106.7x and the computations in the\nclient are reduced by 2.86x-32.1x when the accuracy degradation is less than\n0.5% for the single-client case. For 5 and 10-client cases, the communication\ncost reduction is 11.9x and 11.3x on VGG11 for 0.5% loss in accuracy.",
          "link": "http://arxiv.org/abs/2107.09786",
          "publishedOn": "2021-07-22T02:03:12.638Z",
          "wordCount": 621,
          "title": "Communication and Computation Reduction for Split Learning using Asynchronous Training. (arXiv:2107.09786v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10146",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raeis_M/0/1/0/all/0/1\">Majid Raeis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leon_Garcia_A/0/1/0/all/0/1\">Alberto Leon-Garcia</a>",
          "description": "Traffic signal control is one of the most effective methods of traffic\nmanagement in urban areas. In recent years, traffic control methods based on\ndeep reinforcement learning (DRL) have gained attention due to their ability to\nexploit real-time traffic data, which is often poorly used by the traditional\nhand-crafted methods. While most recent DRL-based methods have focused on\nmaximizing the throughput or minimizing the average travel time of the\nvehicles, the fairness of the traffic signal controllers has often been\nneglected. This is particularly important as neglecting fairness can lead to\nsituations where some vehicles experience extreme waiting times, or where the\nthroughput of a particular traffic flow is highly impacted by the fluctuations\nof another conflicting flow at the intersection. In order to address these\nissues, we introduce two notions of fairness: delay-based and throughput-based\nfairness, which correspond to the two issues mentioned above. Furthermore, we\npropose two DRL-based traffic signal control methods for implementing these\nfairness notions, that can achieve a high throughput as well. We evaluate the\nperformance of our proposed methods using three traffic arrival distributions,\nand find that our methods outperform the baselines in the tested scenarios.",
          "link": "http://arxiv.org/abs/2107.10146",
          "publishedOn": "2021-07-22T02:03:12.630Z",
          "wordCount": 639,
          "title": "A Deep Reinforcement Learning Approach for Fair Traffic Signal Control. (arXiv:2107.10146v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_P/0/1/0/all/0/1\">P.S. Sastry</a>",
          "description": "Deep Neural Networks, often owing to the overparameterization, are shown to\nbe capable of exactly memorizing even randomly labelled data. Empirical studies\nhave also shown that none of the standard regularization techniques mitigate\nsuch overfitting. We investigate whether the choice of the loss function can\naffect this memorization. We empirically show, with benchmark data sets MNIST\nand CIFAR-10, that a symmetric loss function, as opposed to either\ncross-entropy or squared error loss, results in significant improvement in the\nability of the network to resist such overfitting. We then provide a formal\ndefinition for robustness to memorization and provide a theoretical explanation\nas to why the symmetric losses provide this robustness. Our results clearly\nbring out the role loss functions alone can play in this phenomenon of\nmemorization.",
          "link": "http://arxiv.org/abs/2107.09957",
          "publishedOn": "2021-07-22T02:03:12.624Z",
          "wordCount": 579,
          "title": "Memorization in Deep Neural Networks: Does the Loss Function matter?. (arXiv:2107.09957v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aakur_S/0/1/0/all/0/1\">Sathyanarayanan N. Aakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Sai Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Indla_V/0/1/0/all/0/1\">Vineela Indla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagavathi_A/0/1/0/all/0/1\">Arunkumar Bagavathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramnath_V/0/1/0/all/0/1\">Vishalini Laguduva Ramnath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_A/0/1/0/all/0/1\">Akhilesh Ramachandran</a>",
          "description": "The emergence of novel pathogens and zoonotic diseases like the SARS-CoV-2\nhave underlined the need for developing novel diagnosis and intervention\npipelines that can learn rapidly from small amounts of labeled data. Combined\nwith technological advances in next-generation sequencing, metagenome-based\ndiagnostic tools hold much promise to revolutionize rapid point-of-care\ndiagnosis. However, there are significant challenges in developing such an\napproach, the chief among which is to learn self-supervised representations\nthat can help detect novel pathogen signatures with very low amounts of labeled\ndata. This is particularly a difficult task given that closely related\npathogens can share more than 90% of their genome structure. In this work, we\naddress these challenges by proposing MG-Net, a self-supervised representation\nlearning framework that leverages multi-modal context using pseudo-imaging data\nderived from clinical metagenome sequences. We show that the proposed framework\ncan learn robust representations from unlabeled data that can be used for\ndownstream tasks such as metagenome sequence classification with limited access\nto labeled data. Extensive experiments show that the learned features\noutperform current baseline metagenome representations, given only 1000 samples\nper class.",
          "link": "http://arxiv.org/abs/2107.09883",
          "publishedOn": "2021-07-22T02:03:12.617Z",
          "wordCount": 667,
          "title": "MG-NET: Leveraging Pseudo-Imaging for Multi-Modal Metagenome Analysis. (arXiv:2107.09883v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dinitz_M/0/1/0/all/0/1\">Michael Dinitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1\">Sungjin Im</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavastida_T/0/1/0/all/0/1\">Thomas Lavastida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moseley_B/0/1/0/all/0/1\">Benjamin Moseley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vassilvitskii_S/0/1/0/all/0/1\">Sergei Vassilvitskii</a>",
          "description": "A recent line of research investigates how algorithms can be augmented with\nmachine-learned predictions to overcome worst case lower bounds. This area has\nrevealed interesting algorithmic insights into problems, with particular\nsuccess in the design of competitive online algorithms. However, the question\nof improving algorithm running times with predictions has largely been\nunexplored.\n\nWe take a first step in this direction by combining the idea of\nmachine-learned predictions with the idea of \"warm-starting\" primal-dual\nalgorithms. We consider one of the most important primitives in combinatorial\noptimization: weighted bipartite matching and its generalization to\n$b$-matching. We identify three key challenges when using learned dual\nvariables in a primal-dual algorithm. First, predicted duals may be infeasible,\nso we give an algorithm that efficiently maps predicted infeasible duals to\nnearby feasible solutions. Second, once the duals are feasible, they may not be\noptimal, so we show that they can be used to quickly find an optimal solution.\nFinally, such predictions are useful only if they can be learned, so we show\nthat the problem of learning duals for matching has low sample complexity. We\nvalidate our theoretical findings through experiments on both real and\nsynthetic data. As a result we give a rigorous, practical, and empirically\neffective method to compute bipartite matchings.",
          "link": "http://arxiv.org/abs/2107.09770",
          "publishedOn": "2021-07-22T02:03:12.598Z",
          "wordCount": 645,
          "title": "Faster Matchings via Learned Duals. (arXiv:2107.09770v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Law_A/0/1/0/all/0/1\">Anwesha Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Ashish Ghosh</a>",
          "description": "Multi-label (ML) classification is an actively researched topic currently,\nwhich deals with convoluted and overlapping boundaries that arise due to\nseveral labels being active for a particular data instance. We propose a\nclassifier capable of extracting underlying features and introducing\nnon-linearity to the data to handle the complex decision boundaries. A novel\nneural network model has been developed where the input features are subjected\nto two transformations adapted from multi-label functional link artificial\nneural network and autoencoders. First, a functional expansion of the original\nfeatures are made using basis functions. This is followed by an\nautoencoder-aided transformation and reduction on the expanded features. This\nnetwork is capable of improving separability for the multi-label data owing to\nthe two-layer transformation while reducing the expanded feature space to a\nmore manageable amount. This balances the input dimension which leads to a\nbetter classification performance even for a limited amount of data. The\nproposed network has been validated on five ML datasets which shows its\nsuperior performance in comparison with six well-established ML classifiers.\nFurthermore, a single-label variation of the proposed network has also been\nformulated simultaneously and tested on four relevant datasets against three\nexisting classifiers to establish its effectiveness.",
          "link": "http://arxiv.org/abs/2107.09904",
          "publishedOn": "2021-07-22T02:03:12.584Z",
          "wordCount": 633,
          "title": "Integration of Autoencoder and Functional Link Artificial Neural Network for Multi-label Classification. (arXiv:2107.09904v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dionelis_N/0/1/0/all/0/1\">Nikolaos Dionelis</a>",
          "description": "Generative models, such as Generative Adversarial Networks (GANs), have been\nused for unsupervised anomaly detection. While performance keeps improving,\nseveral limitations exist particularly attributed to difficulties at capturing\nmultimodal supports and to the ability to approximate the underlying\ndistribution closer to the tails, i.e. the boundary of the distribution's\nsupport. This paper proposes an approach that attempts to alleviate such\nshortcomings. We propose an invertible-residual-network-based model, the\nBoundary of Distribution Support Generator (BDSG). GANs generally do not\nguarantee the existence of a probability distribution and here, we use the\nrecently developed Invertible Residual Network (IResNet) and Residual Flow\n(ResFlow), for density estimation. These models have not yet been used for\nanomaly detection. We leverage IResNet and ResFlow for Out-of-Distribution\n(OoD) sample detection and for sample generation on the boundary using a\ncompound loss function that forces the samples to lie on the boundary. The BDSG\naddresses non-convex support, disjoint components, and multimodal\ndistributions. Results on synthetic data and data from multimodal\ndistributions, such as MNIST and CIFAR-10, demonstrate competitive performance\ncompared to methods from the literature.",
          "link": "http://arxiv.org/abs/2107.09950",
          "publishedOn": "2021-07-22T02:03:12.563Z",
          "wordCount": 641,
          "title": "Boundary of Distribution Support Generator (BDSG): Sample Generation on the Boundary. (arXiv:2107.09950v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_Z/0/1/0/all/0/1\">Zeeshan Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabassum_A/0/1/0/all/0/1\">Anika Tabassum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_L/0/1/0/all/0/1\">Ling Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Naimul Khan</a>",
          "description": "Electrocardiogram (ECG) is an authoritative source to diagnose and counter\ncritical cardiovascular syndromes such as arrhythmia and myocardial infarction\n(MI). Current machine learning techniques either depend on manually extracted\nfeatures or large and complex deep learning networks which merely utilize the\n1D ECG signal directly. Since intelligent multimodal fusion can perform at the\nstateof-the-art level with an efficient deep network, therefore, in this paper,\nwe propose two computationally efficient multimodal fusion frameworks for ECG\nheart beat classification called Multimodal Image Fusion (MIF) and Multimodal\nFeature Fusion (MFF). At the input of these frameworks, we convert the raw ECG\ndata into three different images using Gramian Angular Field (GAF), Recurrence\nPlot (RP) and Markov Transition Field (MTF). In MIF, we first perform image\nfusion by combining three imaging modalities to create a single image modality\nwhich serves as input to the Convolutional Neural Network (CNN). In MFF, we\nextracted features from penultimate layer of CNNs and fused them to get unique\nand interdependent information necessary for better performance of classifier.\nThese informational features are finally used to train a Support Vector Machine\n(SVM) classifier for ECG heart-beat classification. We demonstrate the\nsuperiority of the proposed fusion models by performing experiments on\nPhysioNets MIT-BIH dataset for five distinct conditions of arrhythmias which\nare consistent with the AAMI EC57 protocols and on PTB diagnostics dataset for\nMyocardial Infarction (MI) classification. We achieved classification accuracy\nof 99.7% and 99.2% on arrhythmia and MI classification, respectively.",
          "link": "http://arxiv.org/abs/2107.09869",
          "publishedOn": "2021-07-22T02:03:12.556Z",
          "wordCount": 677,
          "title": "ECG Heartbeat Classification Using Multimodal Fusion. (arXiv:2107.09869v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Delaney_E/0/1/0/all/0/1\">Eoin Delaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greene_D/0/1/0/all/0/1\">Derek Greene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keane_M/0/1/0/all/0/1\">Mark T. Keane</a>",
          "description": "Whilst an abundance of techniques have recently been proposed to generate\ncounterfactual explanations for the predictions of opaque black-box systems,\nmarkedly less attention has been paid to exploring the uncertainty of these\ngenerated explanations. This becomes a critical issue in high-stakes scenarios,\nwhere uncertain and misleading explanations could have dire consequences (e.g.,\nmedical diagnosis and treatment planning). Moreover, it is often difficult to\ndetermine if the generated explanations are well grounded in the training data\nand sensitive to distributional shifts. This paper proposes several practical\nsolutions that can be leveraged to solve these problems by establishing novel\nconnections with other research works in explainability (e.g., trust scores)\nand uncertainty estimation (e.g., Monte Carlo Dropout). Two experiments\ndemonstrate the utility of our proposed solutions.",
          "link": "http://arxiv.org/abs/2107.09734",
          "publishedOn": "2021-07-22T02:03:12.526Z",
          "wordCount": 571,
          "title": "Uncertainty Estimation and Out-of-Distribution Detection for Counterfactual Explanations: Pitfalls and Solutions. (arXiv:2107.09734v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dadgar_S/0/1/0/all/0/1\">Sajad Dadgar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghatee_M/0/1/0/all/0/1\">Mehdi Ghatee</a>",
          "description": "During the COVID-19 pandemic, social media platforms were ideal for\ncommunicating due to social isolation and quarantine. Also, it was the primary\nsource of misinformation dissemination on a large scale, referred to as the\ninfodemic. Therefore, automatic debunking misinformation is a crucial problem.\nTo tackle this problem, we present two COVID-19 related misinformation datasets\non Twitter and propose a misinformation detection system comprising\nnetwork-based and content-based processes based on machine learning algorithms\nand NLP techniques. In the network-based process, we focus on social\nproperties, network characteristics, and users. On the other hand, we classify\nmisinformation using the content of the tweets directly in the content-based\nprocess, which contains text classification models (paragraph-level and\nsentence-level) and similarity models. The evaluation results on the\nnetwork-based process show the best results for the artificial neural network\nmodel with an F1 score of 88.68%. In the content-based process, our novel\nsimilarity models, which obtained an F1 score of 90.26%, show an improvement in\nthe misinformation classification results compared to the network-based models.\nIn addition, in the text classification models, the best result was achieved\nusing the stacking ensemble-learning model by obtaining an F1 score of 95.18%.\nFurthermore, we test our content-based models on the Constraint@AAAI2021\ndataset, and by getting an F1 score of 94.38%, we improve the baseline results.\nFinally, we develop a fact-checking website called Checkovid that uses each\nprocess to detect misinformative and informative claims in the domain of\nCOVID-19 from different perspectives.",
          "link": "http://arxiv.org/abs/2107.09768",
          "publishedOn": "2021-07-22T02:03:12.520Z",
          "wordCount": 756,
          "title": "Checkovid: A COVID-19 misinformation detection system on Twitter using network and content mining perspectives. (arXiv:2107.09768v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10234",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fanglan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1\">Taoran Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Kaiqun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1\">Charu Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chang-Tien Lu</a>",
          "description": "During the past decade, deep learning's performance has been widely\nrecognized in a variety of machine learning tasks, ranging from image\nclassification, speech recognition to natural language understanding. Graph\nneural networks (GNN) are a type of deep learning that is designed to handle\nnon-Euclidean issues using graph-structured data that are difficult to solve\nwith traditional deep learning techniques. The majority of GNNs were created\nusing a variety of processes, including random walk, PageRank, graph\nconvolution, and heat diffusion, making direct comparisons impossible. Previous\nstudies have primarily focused on classifying current models into distinct\ncategories, with little investigation of their internal relationships. This\nresearch proposes a unified theoretical framework and a novel perspective that\ncan methodologically integrate existing GNN into our framework. We survey and\ncategorize existing GNN models into spatial and spectral domains, as well as\nshow linkages between subcategories within each domain. Further investigation\nreveals a strong relationship between the spatial, spectral, and subgroups of\nthese domains.",
          "link": "http://arxiv.org/abs/2107.10234",
          "publishedOn": "2021-07-22T02:03:12.479Z",
          "wordCount": 618,
          "title": "Bridging the Gap between Spatial and Spectral Domains: A Theoretical Framework for Graph Neural Networks. (arXiv:2107.10234v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pertsch_K/0/1/0/all/0/1\">Karl Pertsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youngwoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Joseph J. Lim</a>",
          "description": "Demonstration-guided reinforcement learning (RL) is a promising approach for\nlearning complex behaviors by leveraging both reward feedback and a set of\ntarget task demonstrations. Prior approaches for demonstration-guided RL treat\nevery new task as an independent learning problem and attempt to follow the\nprovided demonstrations step-by-step, akin to a human trying to imitate a\ncompletely unseen behavior by following the demonstrator's exact muscle\nmovements. Naturally, such learning will be slow, but often new behaviors are\nnot completely unseen: they share subtasks with behaviors we have previously\nlearned. In this work, we aim to exploit this shared subtask structure to\nincrease the efficiency of demonstration-guided RL. We first learn a set of\nreusable skills from large offline datasets of prior experience collected\nacross many tasks. We then propose Skill-based Learning with Demonstrations\n(SkiLD), an algorithm for demonstration-guided RL that efficiently leverages\nthe provided demonstrations by following the demonstrated skills instead of the\nprimitive actions, resulting in substantial performance improvements over prior\ndemonstration-guided RL approaches. We validate the effectiveness of our\napproach on long-horizon maze navigation and complex robot manipulation tasks.",
          "link": "http://arxiv.org/abs/2107.10253",
          "publishedOn": "2021-07-22T02:03:12.471Z",
          "wordCount": 612,
          "title": "Demonstration-Guided Reinforcement Learning with Learned Skills. (arXiv:2107.10253v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaliszyk_C/0/1/0/all/0/1\">Cezary Kaliszyk</a>",
          "description": "The heterogeneous nature of the logical foundations used in different\ninteractive proof assistant libraries has rendered discovery of similar\nmathematical concepts among them difficult. In this paper, we compare a\npreviously proposed algorithm for matching concepts across libraries with our\nunsupervised embedding approach that can help us retrieve similar concepts. Our\napproach is based on the fasttext implementation of Word2Vec, on top of which a\ntree traversal module is added to adapt its algorithm to the representation\nformat of our data export pipeline. We compare the explainability,\ncustomizability, and online-servability of the approaches and argue that the\nneural embedding approach has more potential to be integrated into an\ninteractive proof assistant.",
          "link": "http://arxiv.org/abs/2107.10188",
          "publishedOn": "2021-07-22T02:03:12.396Z",
          "wordCount": 553,
          "title": "JEFL: Joint Embedding of Formal Proof Libraries. (arXiv:2107.10188v1 [cs.LO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Black_E/0/1/0/all/0/1\">Emily Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1\">Matt Fredrikson</a>",
          "description": "We introduce leave-one-out unfairness, which characterizes how likely a\nmodel's prediction for an individual will change due to the inclusion or\nremoval of a single other person in the model's training data. Leave-one-out\nunfairness appeals to the idea that fair decisions are not arbitrary: they\nshould not be based on the chance event of any one person's inclusion in the\ntraining data. Leave-one-out unfairness is closely related to algorithmic\nstability, but it focuses on the consistency of an individual point's\nprediction outcome over unit changes to the training data, rather than the\nerror of the model in aggregate. Beyond formalizing leave-one-out unfairness,\nwe characterize the extent to which deep models behave leave-one-out unfairly\non real data, including in cases where the generalization error is small.\nFurther, we demonstrate that adversarial training and randomized smoothing\ntechniques have opposite effects on leave-one-out fairness, which sheds light\non the relationships between robustness, memorization, individual fairness, and\nleave-one-out fairness in deep models. Finally, we discuss salient practical\napplications that may be negatively affected by leave-one-out unfairness.",
          "link": "http://arxiv.org/abs/2107.10171",
          "publishedOn": "2021-07-22T02:03:12.363Z",
          "wordCount": 615,
          "title": "Leave-one-out Unfairness. (arXiv:2107.10171v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yunxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jianzhong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>",
          "description": "Graph structural information such as topologies or connectivities provides\nvaluable guidance for graph convolutional networks (GCNs) to learn nodes'\nrepresentations. Existing GCN models that capture nodes' structural information\nweight in- and out-neighbors equally or differentiate in- and out-neighbors\nglobally without considering nodes' local topologies. We observe that in- and\nout-neighbors contribute differently for nodes with different local topologies.\nTo explore the directional structural information for different nodes, we\npropose a GCN model with weighted structural features, named WGCN. WGCN first\ncaptures nodes' structural fingerprints via a direction and degree aware Random\nWalk with Restart algorithm, where the walk is guided by both edge direction\nand nodes' in- and out-degrees. Then, the interactions between nodes'\nstructural fingerprints are used as the weighted node structural features. To\nfurther capture nodes' high-order dependencies and graph geometry, WGCN embeds\ngraphs into a latent space to obtain nodes' latent neighbors and geometrical\nrelationships. Based on nodes' geometrical relationships in the latent space,\nWGCN differentiates latent, in-, and out-neighbors with an attention-based\ngeometrical aggregation. Experiments on transductive node classification tasks\nshow that WGCN outperforms the baseline models consistently by up to 17.07% in\nterms of accuracy on five benchmark datasets.",
          "link": "http://arxiv.org/abs/2104.14060",
          "publishedOn": "2021-07-22T02:03:12.348Z",
          "wordCount": 667,
          "title": "WGCN: Graph Convolutional Networks with Weighted Structural Features. (arXiv:2104.14060v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10043",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Revach_G/0/1/0/all/0/1\">Guy Revach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shlezinger_N/0/1/0/all/0/1\">Nir Shlezinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ni_X/0/1/0/all/0/1\">Xiaoyong Ni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Escoriza_A/0/1/0/all/0/1\">Adria Lopez Escoriza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sloun_R/0/1/0/all/0/1\">Ruud J. G. van Sloun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eldar_Y/0/1/0/all/0/1\">Yonina C. Eldar</a>",
          "description": "Real-time state estimation of dynamical systems is a fundamental task in\nsignal processing and control. For systems that are well-represented by a fully\nknown linear Gaussian state space (SS) model, the celebrated Kalman filter (KF)\nis a low complexity optimal solution. However, both linearity of the underlying\nSS model and accurate knowledge of it are often not encountered in practice.\nHere, we present KalmanNet, a real-time state estimator that learns from data\nto carry out Kalman filtering under non-linear dynamics with partial\ninformation. By incorporating the structural SS model with a dedicated\nrecurrent neural network module in the flow of the KF, we retain data\nefficiency and interpretability of the classic algorithm while implicitly\nlearning complex dynamics from data. We numerically demonstrate that KalmanNet\novercomes nonlinearities and model mismatch, outperforming classic filtering\nmethods operating with both mismatched and accurate domain knowledge.",
          "link": "http://arxiv.org/abs/2107.10043",
          "publishedOn": "2021-07-22T02:03:12.306Z",
          "wordCount": 597,
          "title": "KalmanNet: Neural Network Aided Kalman Filtering for Partially Known Dynamics. (arXiv:2107.10043v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09892",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sudarshan_V/0/1/0/all/0/1\">Viswanath P. Sudarshan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Upadhyay_U/0/1/0/all/0/1\">Uddeshya Upadhyay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Egan_G/0/1/0/all/0/1\">Gary F. Egan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhaolin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Awate_S/0/1/0/all/0/1\">Suyash P. Awate</a>",
          "description": "Radiation exposure in positron emission tomography (PET) imaging limits its\nusage in the studies of radiation-sensitive populations, e.g., pregnant women,\nchildren, and adults that require longitudinal imaging. Reducing the PET\nradiotracer dose or acquisition time reduces photon counts, which can\ndeteriorate image quality. Recent deep-neural-network (DNN) based methods for\nimage-to-image translation enable the mapping of low-quality PET images\n(acquired using substantially reduced dose), coupled with the associated\nmagnetic resonance imaging (MRI) images, to high-quality PET images. However,\nsuch DNN methods focus on applications involving test data that match the\nstatistical characteristics of the training data very closely and give little\nattention to evaluating the performance of these DNNs on new\nout-of-distribution (OOD) acquisitions. We propose a novel DNN formulation that\nmodels the (i) underlying sinogram-based physics of the PET imaging system and\n(ii) the uncertainty in the DNN output through the per-voxel heteroscedasticity\nof the residuals between the predicted and the high-quality reference images.\nOur sinogram-based uncertainty-aware DNN framework, namely, suDNN, estimates a\nstandard-dose PET image using multimodal input in the form of (i) a\nlow-dose/low-count PET image and (ii) the corresponding multi-contrast MRI\nimages, leading to improved robustness of suDNN to OOD acquisitions. Results on\nin vivo simultaneous PET-MRI, and various forms of OOD data in PET-MRI, show\nthe benefits of suDNN over the current state of the art, quantitatively and\nqualitatively.",
          "link": "http://arxiv.org/abs/2107.09892",
          "publishedOn": "2021-07-22T02:03:12.299Z",
          "wordCount": 699,
          "title": "Towards Lower-Dose PET using Physics-Based Uncertainty-Aware Multimodal Learning with Robustness to Out-of-Distribution Data. (arXiv:2107.09892v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.12310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_A/0/1/0/all/0/1\">Alan D. Kaplan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_K/0/1/0/all/0/1\">K. Aditya Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nelson_L/0/1/0/all/0/1\">Lindsay D. Nelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sonia Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levin_H/0/1/0/all/0/1\">Harvey Levin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torres_Espin_A/0/1/0/all/0/1\">Abel Torres-Espin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_A/0/1/0/all/0/1\">Austin Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huie_J/0/1/0/all/0/1\">J. Russell Huie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferguson_A/0/1/0/all/0/1\">Adam R. Ferguson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCrea_M/0/1/0/all/0/1\">Michael McCrea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giacino_J/0/1/0/all/0/1\">Joseph Giacino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1\">Shivshankar Sundaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markowitz_A/0/1/0/all/0/1\">Amy J. Markowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manley_G/0/1/0/all/0/1\">Geoffrey T. Manley</a>",
          "description": "Prognoses of Traumatic Brain Injury (TBI) outcomes are neither easily nor\naccurately determined from clinical indicators. This is due in part to the\nheterogeneity of damage inflicted to the brain, ultimately resulting in diverse\nand complex outcomes. Using a data-driven approach on many distinct data\nelements may be necessary to describe this large set of outcomes and thereby\nrobustly depict the nuanced differences among TBI patients' recovery. In this\nwork, we develop a method for modeling large heterogeneous data types relevant\nto TBI. Our approach is geared toward the probabilistic representation of mixed\ncontinuous and discrete variables with missing values. The model is trained on\na dataset encompassing a variety of data types, including demographics,\nblood-based biomarkers, and imaging findings. In addition, it includes a set of\nclinical outcome assessments at 3, 6, and 12 months post-injury. The model is\nused to stratify patients into distinct groups in an unsupervised learning\nsetting. We use the model to infer outcomes using input data, and show that the\ncollection of input data reduces uncertainty of outcomes over a baseline\napproach. In addition, we quantify the performance of a likelihood scoring\ntechnique that can be used to self-evaluate the extrapolation risk of prognosis\non unseen patients.",
          "link": "http://arxiv.org/abs/2012.12310",
          "publishedOn": "2021-07-22T02:03:12.279Z",
          "wordCount": 721,
          "title": "Mixture Model Framework for Traumatic Brain Injury Prognosis Using Heterogeneous Clinical and Outcome Data. (arXiv:2012.12310v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1811.02319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiawei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yingxia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bin Cui</a>",
          "description": "The performance of deep neural networks crucially depends on good\nhyperparameter configurations. Bayesian optimization is a powerful framework\nfor optimizing the hyperparameters of DNNs. These methods need sufficient\nevaluation data to approximate and minimize the validation error function of\nhyperparameters. However, the expensive evaluation cost of DNNs leads to very\nfew evaluation data within a limited time, which greatly reduces the efficiency\nof Bayesian optimization. Besides, the previous researches focus on using the\ncomplete evaluation data to conduct Bayesian optimization, and ignore the\nintermediate evaluation data generated by early stopping methods. To alleviate\nthe insufficient evaluation data problem, we propose a fast hyperparameter\noptimization method, HOIST, that utilizes both the complete and intermediate\nevaluation data to accelerate the hyperparameter optimization of DNNs.\nSpecifically, we train multiple basic surrogates to gather information from the\nmixed evaluation data, and then combine all basic surrogates using weighted\nbagging to provide an accurate ensemble surrogate. Our empirical studies show\nthat HOIST outperforms the state-of-the-art approaches on a wide range of DNNs,\nincluding feed forward neural networks, convolutional neural networks,\nrecurrent neural networks, and variational autoencoder.",
          "link": "http://arxiv.org/abs/1811.02319",
          "publishedOn": "2021-07-22T02:03:12.271Z",
          "wordCount": 675,
          "title": "Fast Hyperparameter Optimization of Deep Neural Networks via Ensembling Multiple Surrogates. (arXiv:1811.02319v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pilat_M/0/1/0/all/0/1\">Martin Pil&#xe1;t</a>",
          "description": "The problem of coordinating the charging of electric vehicles gains more\nimportance as the number of such vehicles grows. In this paper, we develop a\nmethod for the training of controllers for the coordination of EV charging. In\ncontrast to most existing works on this topic, we require the controllers to\npreserve the privacy of the users, therefore we do not allow any communication\nfrom the controller to any third party.\n\nIn order to train the controllers, we use the idea of imitation learning --\nwe first find an optimum solution for a relaxed version of the problem using\nquadratic optimization and then train the controllers to imitate this solution.\nWe also investigate the effects of regularization of the optimum solution on\nthe performance of the controllers. The method is evaluated on realistic data\nand shows improved performance and training speed compared to similar\ncontrollers trained using evolutionary algorithms.",
          "link": "http://arxiv.org/abs/2107.10111",
          "publishedOn": "2021-07-22T02:03:12.263Z",
          "wordCount": 582,
          "title": "Training Electric Vehicle Charging Controllers with Imitation Learning. (arXiv:2107.10111v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Archiki Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehan_M/0/1/0/all/0/1\">Mohammad Ali Rehan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_S/0/1/0/all/0/1\">Shreya Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>",
          "description": "While recent benchmarks have spurred a lot of new work on improving the\ngeneralization of pretrained multilingual language models on multilingual\ntasks, techniques to improve code-switched natural language understanding tasks\nhave been far less explored. In this work, we propose the use of bilingual\nintermediate pretraining as a reliable technique to derive large and consistent\nperformance gains on three different NLP tasks using code-switched text. We\nachieve substantial absolute improvements of 7.87%, 20.15%, and 10.99%, on the\nmean accuracies and F1 scores over previous state-of-the-art systems for\nHindi-English Natural Language Inference (NLI), Question Answering (QA) tasks,\nand Spanish-English Sentiment Analysis (SA) respectively. We show consistent\nperformance gains on four different code-switched language-pairs\n(Hindi-English, Spanish-English, Tamil-English and Malayalam-English) for SA.\nWe also present a code-switched masked language modelling (MLM) pretraining\ntechnique that consistently benefits SA compared to standard MLM pretraining\nusing real code-switched text.",
          "link": "http://arxiv.org/abs/2107.09931",
          "publishedOn": "2021-07-22T02:03:12.243Z",
          "wordCount": 583,
          "title": "The Effectiveness of Intermediate-Task Training for Code-Switched Natural Language Understanding. (arXiv:2107.09931v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dockes_J/0/1/0/all/0/1\">J&#xe9;ro&#xf4;me Dock&#xe8;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varoquaux_G/0/1/0/all/0/1\">Ga&#xeb;l Varoquaux</a> (PARIETAL), <a href=\"http://arxiv.org/find/cs/1/au:+Poline_J/0/1/0/all/0/1\">Jean-Baptiste Poline</a>",
          "description": "Machine learning brings the hope of finding new biomarkers extracted from\ncohorts with rich biomedical measurements. A good biomarker is one that gives\nreliable detection of the corresponding condition. However, biomarkers are\noften extracted from a cohort that differs from the target population. Such a\nmismatch, known as a dataset shift, can undermine the application of the\nbiomarker to new individuals. Dataset shifts are frequent in biomedical\nresearch, e.g. because of recruitment biases. When a dataset shift occurs,\nstandard machine-learning techniques do not suffice to extract and validate\nbiomarkers. This article provides an overview of when and how dataset shifts\nbreaks machine-learning extracted biomarkers, as well as detection and\ncorrection strategies.",
          "link": "http://arxiv.org/abs/2107.09947",
          "publishedOn": "2021-07-22T02:03:12.235Z",
          "wordCount": 559,
          "title": "Preventing dataset shift from breaking machine-learning biomarkers. (arXiv:2107.09947v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bustos_A/0/1/0/all/0/1\">Aurelia Bustos</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Mas_Serrano_P/0/1/0/all/0/1\">Patricio Mas_Serrano</a> (2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Boquera_M/0/1/0/all/0/1\">Mari L. Boquera</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Salinas_J/0/1/0/all/0/1\">Jose M. Salinas</a> (4) ((1) MedBravo, (2) Hospital General Universitario de Alicante Spain -HGUA, (3) Institute for Health and Biomedical Research of Alicante -ISABIAL, (4) Department of Health Informatics, Hospital Universitario San Juan de Alicante Spain)",
          "description": "Introduction: Real-world data generated from clinical practice can be used to\nanalyze the real-world evidence (RWE) of COVID-19 pharmacotherapy and validate\nthe results of randomized clinical trials (RCTs). Machine learning (ML) methods\nare being used in RWE and are promising tools for precision-medicine. In this\nstudy, ML methods are applied to study the efficacy of therapies on COVID-19\nhospital admissions in the Valencian Region in Spain. Methods: 5244 and 1312\nCOVID-19 hospital admissions - dated between January 2020 and January 2021 from\n10 health departments, were used respectively for training and validation of\nseparate treatment-effect models (TE-ML) for remdesivir, corticosteroids,\ntocilizumab, lopinavir-ritonavir, azithromycin and\nchloroquine/hydroxychloroquine. 2390 admissions from 2 additional health\ndepartments were reserved as an independent test to analyze retrospectively the\nsurvival benefits of therapies in the population selected by the TE-ML models\nusing cox-proportional hazard models. TE-ML models were adjusted using\ntreatment propensity scores to control for pre-treatment confounding variables\nassociated to outcome and further evaluated for futility. ML architecture was\nbased on boosted decision-trees. Results: In the populations identified by the\nTE-ML models, only Remdesivir and Tocilizumab were significantly associated\nwith an increase in survival time, with hazard ratios of 0.41 (P = 0.04) and\n0.21 (P = 0.001), respectively. No survival benefits from chloroquine\nderivatives, lopinavir-ritonavir and azithromycin were demonstrated. Tools to\nexplain the predictions of TE-ML models are explored at patient-level as\npotential tools for personalized decision making and precision medicine.\nConclusion: ML methods are suitable tools toward RWE analysis of COVID-19\npharmacotherapies. Results obtained reproduce published results on RWE and\nvalidate the results from RCTs.",
          "link": "http://arxiv.org/abs/2107.10239",
          "publishedOn": "2021-07-22T02:03:12.228Z",
          "wordCount": 788,
          "title": "Machine Learning for Real-World Evidence Analysis of COVID-19 Pharmacotherapy. (arXiv:2107.10239v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tavakoli_M/0/1/0/all/0/1\">Mohammadamin Tavakoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadowsk_P/0/1/0/all/0/1\">Peter Sadowsk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldi_P/0/1/0/all/0/1\">Pierre Baldi</a>",
          "description": "In a physical neural system, backpropagation is faced with a number of\nobstacles including: the need for labeled data, the violation of the locality\nlearning principle, the need for symmetric connections, and the lack of\nmodularity. Tourbillon is a new architecture that addresses all these\nlimitations. At its core, it consists of a stack of circular autoencoders\nfollowed by an output layer. The circular autoencoders are trained in\nself-supervised mode by recirculation algorithms and the top layer in\nsupervised mode by stochastic gradient descent, with the option of propagating\nerror information through the entire stack using non-symmetric connections.\nWhile the Tourbillon architecture is meant primarily to address physical\nconstraints, and not to improve current engineering applications of deep\nlearning, we demonstrate its viability on standard benchmark datasets including\nMNIST, Fashion MNIST, and CIFAR10. We show that Tourbillon can achieve\ncomparable performance to models trained with backpropagation and outperform\nmodels that are trained with other physically plausible algorithms, such as\nfeedback alignment.",
          "link": "http://arxiv.org/abs/2107.06424",
          "publishedOn": "2021-07-22T02:03:12.221Z",
          "wordCount": 605,
          "title": "Tourbillon: a Physically Plausible Neural Architecture. (arXiv:2107.06424v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10098",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lachapelle_S/0/1/0/all/0/1\">S&#xe9;bastien Lachapelle</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lopez_P/0/1/0/all/0/1\">Pau Rodr&#xed;guez L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Priol_R/0/1/0/all/0/1\">R&#xe9;mi Le Priol</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lacoste_A/0/1/0/all/0/1\">Alexandre Lacoste</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lacoste_Julien_S/0/1/0/all/0/1\">Simon Lacoste-Julien</a>",
          "description": "It can be argued that finding an interpretable low-dimensional representation\nof a potentially high-dimensional phenomenon is central to the scientific\nenterprise. Independent component analysis (ICA) refers to an ensemble of\nmethods which formalize this goal and provide estimation procedure for\npractical application. This work proposes mechanism sparsity regularization as\na new principle to achieve nonlinear ICA when latent factors depend sparsely on\nobserved auxiliary variables and/or past latent factors. We show that the\nlatent variables can be recovered up to a permutation if one regularizes the\nlatent mechanisms to be sparse and if some graphical criterion is satisfied by\nthe data generating process. As a special case, our framework shows how one can\nleverage unknown-target interventions on the latent factors to disentangle\nthem, thus drawing further connections between ICA and causality. We validate\nour theoretical results with toy experiments.",
          "link": "http://arxiv.org/abs/2107.10098",
          "publishedOn": "2021-07-22T02:03:12.213Z",
          "wordCount": 614,
          "title": "Discovering Latent Causal Variables via Mechanism Sparsity: A New Principle for Nonlinear ICA. (arXiv:2107.10098v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaganathan_S/0/1/0/all/0/1\">Srikrishna Jaganathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borsdorf_A/0/1/0/all/0/1\">Anja Borsdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_K/0/1/0/all/0/1\">Karthik Shetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>",
          "description": "Deep Learning-based 2D/3D registration methods are highly robust but often\nlack the necessary registration accuracy for clinical application. A refinement\nstep using the classical optimization-based 2D/3D registration method applied\nin combination with Deep Learning-based techniques can provide the required\naccuracy. However, it also increases the runtime. In this work, we propose a\nnovel Deep Learning driven 2D/3D registration framework that can be used\nend-to-end for iterative registration tasks without relying on any further\nrefinement step. We accomplish this by learning the update step of the 2D/3D\nregistration framework using Point-to-Plane Correspondences. The update step is\nlearned using iterative residual refinement-based optical flow estimation, in\ncombination with the Point-to-Plane correspondence solver embedded as a known\noperator. Our proposed method achieves an average runtime of around 8s, a mean\nre-projection distance error of 0.60 $\\pm$ 0.40 mm with a success ratio of 97\npercent and a capture range of 60 mm. The combination of high registration\naccuracy, high robustness, and fast runtime makes our solution ideal for\nclinical applications.",
          "link": "http://arxiv.org/abs/2107.10004",
          "publishedOn": "2021-07-22T02:03:12.195Z",
          "wordCount": 616,
          "title": "Deep Iterative 2D/3D Registration. (arXiv:2107.10004v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Awasthi_P/0/1/0/all/0/1\">Pranjal Awasthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_A/0/1/0/all/0/1\">Alex Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijayaraghavan_A/0/1/0/all/0/1\">Aravindan Vijayaraghavan</a>",
          "description": "We present polynomial time and sample efficient algorithms for learning an\nunknown depth-2 feedforward neural network with general ReLU activations, under\nmild non-degeneracy assumptions. In particular, we consider learning an unknown\nnetwork of the form $f(x) = {a}^{\\mathsf{T}}\\sigma({W}^\\mathsf{T}x+b)$, where\n$x$ is drawn from the Gaussian distribution, and $\\sigma(t) := \\max(t,0)$ is\nthe ReLU activation. Prior works for learning networks with ReLU activations\nassume that the bias $b$ is zero. In order to deal with the presence of the\nbias terms, our proposed algorithm consists of robustly decomposing multiple\nhigher order tensors arising from the Hermite expansion of the function $f(x)$.\nUsing these ideas we also establish identifiability of the network parameters\nunder minimal assumptions.",
          "link": "http://arxiv.org/abs/2107.10209",
          "publishedOn": "2021-07-22T02:03:12.177Z",
          "wordCount": 564,
          "title": "Efficient Algorithms for Learning Depth-2 Neural Networks with General ReLU Activations. (arXiv:2107.10209v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kallis_R/0/1/0/all/0/1\">Rafael Kallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorbo_A/0/1/0/all/0/1\">Andrea Di Sorbo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canfora_G/0/1/0/all/0/1\">Gerardo Canfora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panichella_S/0/1/0/all/0/1\">Sebastiano Panichella</a>",
          "description": "Software maintenance and evolution involves critical activities for the\nsuccess of software projects. To support such activities and keep code\nup-to-date and error-free, software communities make use of issue trackers,\ni.e., tools for signaling, handling, and addressing the issues occurring in\nsoftware systems. However, in popular projects, tens or hundreds of issue\nreports are daily submitted. In this context, identifying the type of each\nsubmitted report (e.g., bug report, feature request, etc.) would facilitate the\nmanagement and the prioritization of the issues to address. To support issue\nhandling activities, in this paper, we propose Ticket Tagger, a GitHub app\nanalyzing the issue title and description through machine learning techniques\nto automatically recognize the types of reports submitted on GitHub and assign\nlabels to each issue accordingly. We empirically evaluated the tool's\nprediction performance on about 30,000 GitHub issues. Our results show that the\nTicket Tagger can identify the correct labels to assign to GitHub issues with\nreasonably high effectiveness. Considering these results and the fact that the\ntool is designed to be easily integrated in the GitHub issue management\nprocess, Ticket Tagger consists in a useful solution for developers.",
          "link": "http://arxiv.org/abs/2107.09936",
          "publishedOn": "2021-07-22T02:03:12.170Z",
          "wordCount": 618,
          "title": "Predicting Issue Types on GitHub. (arXiv:2107.09936v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chvalovsky_K/0/1/0/all/0/1\">Karel Chvalovsk&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakub%5Cr%7Bu%7Dv_J/0/1/0/all/0/1\">Jan Jakub&#x16f;v</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsak_M/0/1/0/all/0/1\">Miroslav Ol&#x161;&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1\">Josef Urban</a>",
          "description": "Saturation-style automated theorem provers (ATPs) based on the given clause\nprocedure are today the strongest general reasoners for classical first-order\nlogic. The clause selection heuristics in such systems are, however, often\nevaluating clauses in isolation, ignoring other clauses. This has changed\nrecently by equipping the E/ENIGMA system with a graph neural network (GNN)\nthat chooses the next given clause based on its evaluation in the context of\npreviously selected clauses. In this work, we describe several algorithms and\nexperiments with ENIGMA, advancing the idea of contextual evaluation based on\nlearning important components of the graph of clauses.",
          "link": "http://arxiv.org/abs/2107.10034",
          "publishedOn": "2021-07-22T02:03:12.132Z",
          "wordCount": 544,
          "title": "Learning Theorem Proving Components. (arXiv:2107.10034v1 [cs.LO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Venkataraman_S/0/1/0/all/0/1\">Shobha Venkataraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amos_B/0/1/0/all/0/1\">Brandon Amos</a>",
          "description": "Fixed-point iterations are at the heart of numerical computing and are often\na computational bottleneck in real-time applications, which typically instead\nneed a fast solution of moderate accuracy. Classical acceleration methods for\nfixed-point problems focus on designing algorithms with theoretical guarantees\nthat apply to any fixed-point problem. We present neural fixed-point\nacceleration, a framework to automatically learn to accelerate convex\nfixed-point problems that are drawn from a distribution, using ideas from\nmeta-learning and classical acceleration algorithms. We apply our framework to\nSCS, the state-of-the-art solver for convex cone programming, and design models\nand loss functions to overcome the challenges of learning over unrolled\noptimization and acceleration instabilities. Our work brings neural\nacceleration into any optimization problem expressible with CVXPY. The source\ncode behind this paper is available at\nhttps://github.com/facebookresearch/neural-scs",
          "link": "http://arxiv.org/abs/2107.10254",
          "publishedOn": "2021-07-22T02:03:12.124Z",
          "wordCount": 565,
          "title": "Neural Fixed-Point Acceleration for Convex Optimization. (arXiv:2107.10254v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10154",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Krajnak_V/0/1/0/all/0/1\">Vladim&#xed;r Kraj&#x148;&#xe1;k</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Naik_S/0/1/0/all/0/1\">Shibabrat Naik</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wiggins_S/0/1/0/all/0/1\">Stephen Wiggins</a>",
          "description": "In this paper we use support vector machines (SVM) to develop a machine\nlearning framework to discover the phase space structure that can distinguish\nbetween distinct reaction pathways. The machine learning model is trained using\ndata from trajectories of Hamilton's equations but lends itself for use in\nmolecular dynamics simulation. The framework is specifically designed to\nrequire minimal a priori knowledge of the dynamics in a system. We benchmark\nour approach with a model Hamiltonian for the reaction of an ion and a molecule\ndue to Chesnavich consisting of two parts: a rigid, symmetric top representing\nthe $\\text{CH}_3^{+}$ ion, and a mobile $\\text{H}$ atom. We begin with\ntrajectories and use support vector machines to determine the boundaries\nbetween initial conditions corresponding to different classes of trajectories.\nWe then show that these boundaries between different classes of trajectories\napproximate invariant phase space structures of the same type observed in\nearlier analyses of Chesnavich's model. Our approach is designed with\nextensions to higher-dimensional applications in mind. SVM is known to work\nwell even with small amounts of data, therefore our approach is computationally\nbetter suited than existing methods for high-dimensional systems and systems\nwhere integrating trajectories is expensive.",
          "link": "http://arxiv.org/abs/2107.10154",
          "publishedOn": "2021-07-22T02:03:12.093Z",
          "wordCount": 631,
          "title": "Predicting trajectory behaviour via machine-learned invariant manifolds. (arXiv:2107.10154v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09700",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hong_S/0/1/0/all/0/1\">Sungmin Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marinescu_R/0/1/0/all/0/1\">Razvan Marinescu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bonkhoff_A/0/1/0/all/0/1\">Anna K. Bonkhoff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bretzner_M/0/1/0/all/0/1\">Martin Bretzner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rost_N/0/1/0/all/0/1\">Natalia S. Rost</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1\">Polina Golland</a>",
          "description": "Image synthesis via Generative Adversarial Networks (GANs) of\nthree-dimensional (3D) medical images has great potential that can be extended\nto many medical applications, such as, image enhancement and disease\nprogression modeling. However, current GAN technologies for 3D medical image\nsynthesis need to be significantly improved to be readily adapted to real-world\nmedical problems. In this paper, we extend the state-of-the-art StyleGAN2\nmodel, which natively works with two-dimensional images, to enable 3D image\nsynthesis. In addition to the image synthesis, we investigate the\ncontrollability and interpretability of the 3D-StyleGAN via style vectors\ninherited form the original StyleGAN2 that are highly suitable for medical\napplications: (i) the latent space projection and reconstruction of unseen real\nimages, and (ii) style mixing. We demonstrate the 3D-StyleGAN's performance and\nfeasibility with ~12,000 three-dimensional full brain MR T1 images, although it\ncan be applied to any 3D volumetric images. Furthermore, we explore different\nconfigurations of hyperparameters to investigate potential improvement of the\nimage synthesis with larger networks. The codes and pre-trained networks are\navailable online: https://github.com/sh4174/3DStyleGAN.",
          "link": "http://arxiv.org/abs/2107.09700",
          "publishedOn": "2021-07-22T02:03:12.057Z",
          "wordCount": null,
          "title": "3D-StyleGAN: A Style-Based Generative Adversarial Network for Generative Modeling of Three-Dimensional Medical Images. (arXiv:2107.09700v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Saikat Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samavatian_M/0/1/0/all/0/1\">Mohammad Hossein Samavatian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barber_K/0/1/0/all/0/1\">Kristin Barber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_R/0/1/0/all/0/1\">Radu Teodorescu</a>",
          "description": "Deep neural network (DNN) classifiers are powerful tools that drive a broad\nspectrum of important applications, from image recognition to autonomous\nvehicles. Unfortunately, DNNs are known to be vulnerable to adversarial attacks\nthat affect virtually all state-of-the-art models. These attacks make small\nimperceptible modifications to inputs that are sufficient to induce the DNNs to\nproduce the wrong classification.\n\nIn this paper we propose a novel, lightweight adversarial correction and/or\ndetection mechanism for image classifiers that relies on undervolting (running\na chip at a voltage that is slightly below its safe margin). We propose using\ncontrolled undervolting of the chip running the inference process in order to\nintroduce a limited number of compute errors. We show that these errors disrupt\nthe adversarial input in a way that can be used either to correct the\nclassification or detect the input as adversarial. We evaluate the proposed\nsolution in an FPGA design and through software simulation. We evaluate 10\nattacks on two popular DNNs and show an average detection rate of 80% to 95%.",
          "link": "http://arxiv.org/abs/2107.09804",
          "publishedOn": "2021-07-22T02:03:11.978Z",
          "wordCount": null,
          "title": "Using Undervolting as an On-Device Defense Against Adversarial Machine Learning Attacks. (arXiv:2107.09804v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2002.01335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Slowik_A/0/1/0/all/0/1\">Agnieszka S&#x142;owik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1\">William L. Hamilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamnik_M/0/1/0/all/0/1\">Mateja Jamnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holden_S/0/1/0/all/0/1\">Sean B. Holden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>",
          "description": "In order to communicate, humans flatten a complex representation of ideas and\ntheir attributes into a single word or a sentence. We investigate the impact of\nrepresentation learning in artificial agents by developing graph referential\ngames. We empirically show that agents parametrized by graph neural networks\ndevelop a more compositional language compared to bag-of-words and sequence\nmodels, which allows them to systematically generalize to new combinations of\nfamiliar features.",
          "link": "http://arxiv.org/abs/2002.01335",
          "publishedOn": "2021-07-22T02:03:11.963Z",
          "wordCount": null,
          "title": "Structural Inductive Biases in Emergent Communication. (arXiv:2002.01335v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.11867",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fanglan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1\">Taoran Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Kaiqun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1\">Charu Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chang-Tien Lu</a>",
          "description": "Deep learning's success has been widely recognized in a variety of machine\nlearning tasks, including image classification, audio recognition, and natural\nlanguage processing. As an extension of deep learning beyond these domains,\ngraph neural networks (GNNs) are designed to handle the non-Euclidean\ngraph-structure which is intractable to previous deep learning techniques.\nExisting GNNs are presented using various techniques, making direct comparison\nand cross-reference more complex. Although existing studies categorize GNNs\ninto spatial-based and spectral-based techniques, there hasn't been a thorough\nexamination of their relationship. To close this gap, this study presents a\nsingle framework that systematically incorporates most GNNs. We organize\nexisting GNNs into spatial and spectral domains, as well as expose the\nconnections within each domain. A review of spectral graph theory and\napproximation theory builds a strong relationship across the spatial and\nspectral domains in further investigation.",
          "link": "http://arxiv.org/abs/2002.11867",
          "publishedOn": "2021-07-22T02:03:11.958Z",
          "wordCount": null,
          "title": "Bridging the Gap between Spatial and Spectral Domains: A Survey on Graph Neural Networks. (arXiv:2002.11867v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Michael Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaushik_S/0/1/0/all/0/1\">Sidhant Kaushik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>",
          "description": "Many transfer problems require re-using previously optimal decisions for\nsolving new tasks, which suggests the need for learning algorithms that can\nmodify the mechanisms for choosing certain actions independently of those for\nchoosing others. However, there is currently no formalism nor theory for how to\nachieve this kind of modular credit assignment. To answer this question, we\ndefine modular credit assignment as a constraint on minimizing the algorithmic\nmutual information among feedback signals for different decisions. We introduce\nwhat we call the modularity criterion for testing whether a learning algorithm\nsatisfies this constraint by performing causal analysis on the algorithm\nitself. We generalize the recently proposed societal decision-making framework\nas a more granular formalism than the Markov decision process to prove that for\ndecision sequences that do not contain cycles, certain single-step temporal\ndifference action-value methods meet this criterion while all policy-gradient\nmethods do not. Empirical evidence suggests that such action-value methods are\nmore sample efficient than policy-gradient methods on transfer problems that\nrequire only sparse changes to a sequence of previously optimal decisions.",
          "link": "http://arxiv.org/abs/2106.14993",
          "publishedOn": "2021-07-22T02:03:11.878Z",
          "wordCount": 692,
          "title": "Modularity in Reinforcement Learning via Algorithmic Independence in Credit Assignment. (arXiv:2106.14993v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_E/0/1/0/all/0/1\">Enpeng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hentenryck_P/0/1/0/all/0/1\">Pascal Van Hentenryck</a>",
          "description": "Large-scale ride-sharing systems combine real-time dispatching and routing\noptimization over a rolling time horizon with a model predictive control (MPC)\ncomponent that relocates idle vehicles to anticipate the demand. The MPC\noptimization operates over a longer time horizon to compensate for the inherent\nmyopic nature of the real-time dispatching. These longer time horizons are\nbeneficial for the quality of relocation decisions but increase computational\ncomplexity. Consequently, the ride-sharing operators are often forced to use a\nrelatively short time horizon. To address this computational challenge, this\npaper proposes a hybrid approach that combines machine learning and\noptimization. The machine-learning component learns the optimal solution to the\nMPC on the aggregated level to overcome the sparsity and high-dimensionality of\nthe solution. The optimization component transforms the machine-learning\nprediction back to the original granularity through a tractable transportation\nmodel. As a consequence, the original NP-hard MPC problem is reduced to a\npolynomial time prediction and optimization, which allows the ride-sharing\noperators to consider a longer time horizon. Experimental results show that the\nhybrid approach achieves significantly better service quality than the MPC\noptimization in terms of average rider waiting time, due to its ability to\nmodel a longer horizon.",
          "link": "http://arxiv.org/abs/2105.13461",
          "publishedOn": "2021-07-22T02:03:11.869Z",
          "wordCount": 666,
          "title": "Learning Model-Based Vehicle-Relocation Decisions for Real-Time Ride-Sharing: Hybridizing Learning and Optimization. (arXiv:2105.13461v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antonello_R/0/1/0/all/0/1\">Richard Antonello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turek_J/0/1/0/all/0/1\">Javier Turek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1\">Vy Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1\">Alexander Huth</a>",
          "description": "How related are the representations learned by neural language models,\ntranslation models, and language tagging tasks? We answer this question by\nadapting an encoder-decoder transfer learning method from computer vision to\ninvestigate the structure among 100 different feature spaces extracted from\nhidden representations of various networks trained on language tasks. This\nmethod reveals a low-dimensional structure where language models and\ntranslation models smoothly interpolate between word embeddings, syntactic and\nsemantic tasks, and future word embeddings. We call this low-dimensional\nstructure a language representation embedding because it encodes the\nrelationships between representations needed to process language for a variety\nof NLP tasks. We find that this representation embedding can predict how well\neach individual feature space maps to human brain responses to natural language\nstimuli recorded using fMRI. Additionally, we find that the principal dimension\nof this structure can be used to create a metric which highlights the brain's\nnatural language processing hierarchy. This suggests that the embedding\ncaptures some part of the brain's natural language representation structure.",
          "link": "http://arxiv.org/abs/2106.05426",
          "publishedOn": "2021-07-22T02:03:11.861Z",
          "wordCount": 653,
          "title": "Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses. (arXiv:2106.05426v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09729",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shaham_U/0/1/0/all/0/1\">Uri Shaham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>",
          "description": "We combine beam search with the probabilistic pruning technique of nucleus\nsampling to create two deterministic nucleus search algorithms for natural\nlanguage generation. The first algorithm, p-exact search, locally prunes the\nnext-token distribution and performs an exact search over the remaining space.\nThe second algorithm, dynamic beam search, shrinks and expands the beam size\naccording to the entropy of the candidate's probability distribution. Despite\nthe probabilistic intuition behind nucleus search, experiments on machine\ntranslation and summarization benchmarks show that both algorithms reach the\nsame performance levels as standard beam search.",
          "link": "http://arxiv.org/abs/2107.09729",
          "publishedOn": "2021-07-22T02:03:11.854Z",
          "wordCount": 534,
          "title": "What Do You Get When You Cross Beam Search with Nucleus Sampling?. (arXiv:2107.09729v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09853",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Furui_A/0/1/0/all/0/1\">Akira Furui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Igaue_T/0/1/0/all/0/1\">Takuya Igaue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsuji_T/0/1/0/all/0/1\">Toshio Tsuji</a>",
          "description": "Electromyogram (EMG) has been utilized to interface signals for prosthetic\nhands and information devices owing to its ability to reflect human motion\nintentions. Although various EMG classification methods have been introduced\ninto EMG-based control systems, they do not fully consider the stochastic\ncharacteristics of EMG signals. This paper proposes an EMG pattern\nclassification method incorporating a scale mixture-based generative model. A\nscale mixture model is a stochastic EMG model in which the EMG variance is\nconsidered as a random variable, enabling the representation of uncertainty in\nthe variance. This model is extended in this study and utilized for EMG pattern\nclassification. The proposed method is trained by variational Bayesian\nlearning, thereby allowing the automatic determination of the model complexity.\nFurthermore, to optimize the hyperparameters of the proposed method with a\npartial discriminative approach, a mutual information-based determination\nmethod is introduced. Simulation and EMG analysis experiments demonstrated the\nrelationship between the hyperparameters and classification accuracy of the\nproposed method as well as the validity of the proposed method. The comparison\nusing public EMG datasets revealed that the proposed method outperformed the\nvarious conventional classifiers. These results indicated the validity of the\nproposed method and its applicability to EMG-based control systems. In EMG\npattern recognition, a classifier based on a generative model that reflects the\nstochastic characteristics of EMG signals can outperform the conventional\ngeneral-purpose classifier.",
          "link": "http://arxiv.org/abs/2107.09853",
          "publishedOn": "2021-07-22T02:03:11.824Z",
          "wordCount": 686,
          "title": "EMG Pattern Recognition via Bayesian Inference with Scale Mixture-Based Stochastic Generative Models. (arXiv:2107.09853v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2104.09036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanqiao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>",
          "description": "Multimedia content is of predominance in the modern Web era. Investigating\nhow users interact with multimodal items is a continuing concern within the\nrapid development of recommender systems. The majority of previous work focuses\non modeling user-item interactions with multimodal features included as side\ninformation. However, this scheme is not well-designed for multimedia\nrecommendation. Specifically, only collaborative item-item relationships are\nimplicitly modeled through high-order item-user-item relations. Considering\nthat items are associated with rich contents in multiple modalities, we argue\nthat the latent semantic item-item structures underlying these multimodal\ncontents could be beneficial for learning better item representations and\nfurther boosting recommendation. To this end, we propose a LATent sTructure\nmining method for multImodal reCommEndation, which we term LATTICE for brevity.\nTo be specific, in the proposed LATTICE model, we devise a novel modality-aware\nstructure learning layer, which learns item-item structures for each modality\nand aggregates multiple modalities to obtain latent item graphs. Based on the\nlearned latent graphs, we perform graph convolutions to explicitly inject\nhigh-order item affinities into item representations. These enriched item\nrepresentations can then be plugged into existing collaborative filtering\nmethods to make more accurate recommendations. Extensive experiments on three\nreal-world datasets demonstrate the superiority of our method over\nstate-of-the-art multimedia recommendation methods and validate the efficacy of\nmining latent item-item relationships from multimodal features.",
          "link": "http://arxiv.org/abs/2104.09036",
          "publishedOn": "2021-07-22T02:03:11.794Z",
          "wordCount": 695,
          "title": "Mining Latent Structures for Multimedia Recommendation. (arXiv:2104.09036v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09814",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Kontolati_K/0/1/0/all/0/1\">Katiana Kontolati</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Loukrezis_D/0/1/0/all/0/1\">Dimitrios Loukrezis</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Santos_K/0/1/0/all/0/1\">Ketson R. M. dos Santos</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Giovanis_D/0/1/0/all/0/1\">Dimitrios G. Giovanis</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Shields_M/0/1/0/all/0/1\">Michael D. Shields</a>",
          "description": "In this work we introduce a manifold learning-based method for uncertainty\nquantification (UQ) in systems describing complex spatiotemporal processes. Our\nfirst objective is to identify the embedding of a set of high-dimensional data\nrepresenting quantities of interest of the computational or analytical model.\nFor this purpose, we employ Grassmannian diffusion maps, a two-step nonlinear\ndimension reduction technique which allows us to reduce the dimensionality of\nthe data and identify meaningful geometric descriptions in a parsimonious and\ninexpensive manner. Polynomial chaos expansion is then used to construct a\nmapping between the stochastic input parameters and the diffusion coordinates\nof the reduced space. An adaptive clustering technique is proposed to identify\nan optimal number of clusters of points in the latent space. The similarity of\npoints allows us to construct a number of geometric harmonic emulators which\nare finally utilized as a set of inexpensive pre-trained models to perform an\ninverse map of realizations of latent features to the ambient space and thus\nperform accurate out-of-sample predictions. Thus, the proposed method acts as\nan encoder-decoder system which is able to automatically handle very\nhigh-dimensional data while simultaneously operating successfully in the\nsmall-data regime. The method is demonstrated on two benchmark problems and on\na system of advection-diffusion-reaction equations which model a first-order\nchemical reaction between two species. In all test cases, the proposed method\nis able to achieve highly accurate approximations which ultimately lead to the\nsignificant acceleration of UQ tasks.",
          "link": "http://arxiv.org/abs/2107.09814",
          "publishedOn": "2021-07-22T02:03:11.771Z",
          "wordCount": 699,
          "title": "Manifold learning-based polynomial chaos expansions for high-dimensional surrogate models. (arXiv:2107.09814v1 [physics.data-an])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1\">Steve Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Prateek Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krichene_W/0/1/0/all/0/1\">Walid Krichene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rendle_S/0/1/0/all/0/1\">Steffen Rendle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakurta_A/0/1/0/all/0/1\">Abhradeep Thakurta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>",
          "description": "We study the problem of differentially private (DP) matrix completion under\nuser-level privacy. We design a joint differentially private variant of the\npopular Alternating-Least-Squares (ALS) method that achieves: i) (nearly)\noptimal sample complexity for matrix completion (in terms of number of items,\nusers), and ii) the best known privacy/utility trade-off both theoretically, as\nwell as on benchmark data sets. In particular, we provide the first global\nconvergence analysis of ALS with noise introduced to ensure DP, and show that,\nin comparison to the best known alternative (the Private Frank-Wolfe algorithm\nby Jain et al. (2018)), our error bounds scale significantly better with\nrespect to the number of items and users, which is critical in practical\nproblems. Extensive validation on standard benchmarks demonstrate that the\nalgorithm, in combination with carefully designed sampling procedures, is\nsignificantly more accurate than existing techniques, thus promising to be the\nfirst practical DP embedding model.",
          "link": "http://arxiv.org/abs/2107.09802",
          "publishedOn": "2021-07-22T02:03:11.763Z",
          "wordCount": 601,
          "title": "Private Alternating Least Squares: Practical Private Matrix Completion with Tighter Rates. (arXiv:2107.09802v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsukada_T/0/1/0/all/0/1\">Takeshi Tsukada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unno_H/0/1/0/all/0/1\">Hiroshi Unno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekiyama_T/0/1/0/all/0/1\">Taro Sekiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suenaga_K/0/1/0/all/0/1\">Kohei Suenaga</a>",
          "description": "Loop-invariant synthesis is the basis of every program verification\nprocedure. Due to its undecidability in general, a tool for invariant synthesis\nnecessarily uses heuristics. Despite the common belief that the design of\nheuristics is vital for the effective performance of a verifier, little work\nhas been performed toward obtaining the optimal heuristics for each\ninvariant-synthesis tool. Instead, developers have hand-tuned the heuristics of\ntools. This study demonstrates that we can effectively and automatically learn\na good heuristic via reinforcement learning for an invariant synthesizer PCSat.\nOur experiment shows that PCSat combined with the heuristic learned by\nreinforcement learning outperforms the state-of-the-art solvers for this task.\nTo the best of our knowledge, this is the first work that investigates learning\nthe heuristics of an invariant synthesis tool.",
          "link": "http://arxiv.org/abs/2107.09766",
          "publishedOn": "2021-07-22T02:03:11.756Z",
          "wordCount": 560,
          "title": "Enhancing Loop-Invariant Synthesis via Reinforcement Learning. (arXiv:2107.09766v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/1808.01345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cazzaniga_P/0/1/0/all/0/1\">Paolo Cazzaniga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nobile_M/0/1/0/all/0/1\">Marco S. Nobile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramazzotti_D/0/1/0/all/0/1\">Daniele Ramazzotti</a>",
          "description": "Bayesian Networks have been widely used in the last decades in many fields,\nto describe statistical dependencies among random variables. In general,\nlearning the structure of such models is a problem with considerable\ntheoretical interest that poses many challenges. On the one hand, it is a\nwell-known NP-complete problem, practically hardened by the huge search space\nof possible solutions. On the other hand, the phenomenon of I-equivalence,\ni.e., different graphical structures underpinning the same set of statistical\ndependencies, may lead to multimodal fitness landscapes further hindering\nmaximum likelihood approaches to solve the task. In particular, we exploit the\nNSGA-II multi-objective optimization procedure in order to explicitly account\nfor both the likelihood of a solution and the number of selected arcs, by\nsetting these as the two objective functions of the method. The aim of this\nwork is to investigate the behavior of NSGA-II and analyse the quality of its\nsolutions. We thus thoroughly examined the optimization results obtained on a\nwide set of simulated data, by considering both the goodness of the inferred\nsolutions in terms of the objective functions values achieved, and by comparing\nthe retrieved structures with the ground truth, i.e., the networks used to\ngenerate the target data. Our results show that NSGA-II can converge to\nsolutions characterized by better likelihood and less arcs than classic\napproaches, although paradoxically characterized in many cases by a lower\nsimilarity with the target network.",
          "link": "http://arxiv.org/abs/1808.01345",
          "publishedOn": "2021-07-22T02:03:11.747Z",
          "wordCount": 701,
          "title": "Investigating the performance of multi-objective optimization when learning Bayesian Networks. (arXiv:1808.01345v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_C/0/1/0/all/0/1\">Catarina Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torres_J/0/1/0/all/0/1\">Jo&#xe3;o Torres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1\">Maria In&#xea;s Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aparicio_D/0/1/0/all/0/1\">David Apar&#xed;cio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ascensao_J/0/1/0/all/0/1\">Jo&#xe3;o Tiago Ascens&#xe3;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bizarro_P/0/1/0/all/0/1\">Pedro Bizarro</a>",
          "description": "Money laundering is a global phenomenon with wide-reaching social and\neconomic consequences. Cryptocurrencies are particularly susceptible due to the\nlack of control by authorities and their anonymity. Thus, it is important to\ndevelop new techniques to detect and prevent illicit cryptocurrency\ntransactions. In our work, we propose new features based on the structure of\nthe graph and past labels to boost the performance of machine learning methods\nto detect money laundering. Our method, GuiltyWalker, performs random walks on\nthe bitcoin transaction graph and computes features based on the distance to\nillicit transactions. We combine these new features with features proposed by\nWeber et al. and observe an improvement of about 5pp regarding illicit\nclassification. Namely, we observe that our proposed features are particularly\nhelpful during a black market shutdown, where the algorithm by Weber et al. was\nlow performing.",
          "link": "http://arxiv.org/abs/2102.05373",
          "publishedOn": "2021-07-22T02:03:11.738Z",
          "wordCount": 613,
          "title": "GuiltyWalker: Distance to illicit nodes in the Bitcoin network. (arXiv:2102.05373v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiankai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuanshun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Weihao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Junyuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chong Wang</a>",
          "description": "Recently researchers have studied input leakage problems in Federated\nLearning (FL) where a malicious party can reconstruct sensitive training inputs\nprovided by users from shared gradient. It raises concerns about FL since input\nleakage contradicts the privacy-preserving intention of using FL. Despite a\nrelatively rich literature on attacks and defenses of input reconstruction in\nHorizontal FL, input leakage and protection in vertical FL starts to draw\nresearcher's attention recently. In this paper, we study how to defend against\ninput leakage attacks in Vertical FL. We design an adversarial training-based\nframework that contains three modules: adversarial reconstruction, noise\nregularization, and distance correlation minimization. Those modules can not\nonly be employed individually but also applied together since they are\nindependent to each other. Through extensive experiments on a large-scale\nindustrial online advertising dataset, we show our framework is effective in\nprotecting input privacy while retaining the model utility.",
          "link": "http://arxiv.org/abs/2107.09898",
          "publishedOn": "2021-07-22T02:03:11.719Z",
          "wordCount": 603,
          "title": "Defending against Reconstruction Attack in Vertical Federated Learning. (arXiv:2107.09898v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05855",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chourasia_R/0/1/0/all/0/1\">Rishav Chourasia</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ye_J/0/1/0/all/0/1\">Jiayuan Ye</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shokri_R/0/1/0/all/0/1\">Reza Shokri</a>",
          "description": "What is the information leakage of an iterative learning algorithm about its\ntraining data, when the internal state of the algorithm is \\emph{not}\nobservable? How much is the contribution of each specific training epoch to the\nfinal leakage? We study this problem for noisy gradient descent algorithms, and\nmodel the \\emph{dynamics} of R\\'enyi differential privacy loss throughout the\ntraining process. Our analysis traces a provably tight bound on the R\\'enyi\ndivergence between the pair of probability distributions over parameters of\nmodels with neighboring datasets. We prove that the privacy loss converges\nexponentially fast, for smooth and strongly convex loss functions, which is a\nsignificant improvement over composition theorems. For Lipschitz, smooth, and\nstrongly convex loss functions, we prove optimal utility for differential\nprivacy algorithms with a small gradient complexity.",
          "link": "http://arxiv.org/abs/2102.05855",
          "publishedOn": "2021-07-22T02:03:11.710Z",
          "wordCount": 597,
          "title": "Differential Privacy Dynamics of Langevin Diffusion and Noisy Gradient Descent. (arXiv:2102.05855v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Ximei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jinghan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>",
          "description": "Deep learning has made revolutionary advances to diverse applications in the\npresence of large-scale labeled datasets. However, it is prohibitively\ntime-costly and labor-expensive to collect sufficient labeled data in most\nrealistic scenarios. To mitigate the requirement for labeled data,\nsemi-supervised learning (SSL) focuses on simultaneously exploring both labeled\nand unlabeled data, while transfer learning (TL) popularizes a favorable\npractice of fine-tuning a pre-trained model to the target data. A dilemma is\nthus encountered: Without a decent pre-trained model to provide an implicit\nregularization, SSL through self-training from scratch will be easily misled by\ninaccurate pseudo-labels, especially in large-sized label space; Without\nexploring the intrinsic structure of unlabeled data, TL through fine-tuning\nfrom limited labeled data is at risk of under-transfer caused by model shift.\nTo escape from this dilemma, we present Self-Tuning to enable data-efficient\ndeep learning by unifying the exploration of labeled and unlabeled data and the\ntransfer of a pre-trained model, as well as a Pseudo Group Contrast (PGC)\nmechanism to mitigate the reliance on pseudo-labels and boost the tolerance to\nfalse labels. Self-Tuning outperforms its SSL and TL counterparts on five tasks\nby sharp margins, e.g. it doubles the accuracy of fine-tuning on Cars with 15%\nlabels.",
          "link": "http://arxiv.org/abs/2102.12903",
          "publishedOn": "2021-07-22T02:03:11.692Z",
          "wordCount": 661,
          "title": "Self-Tuning for Data-Efficient Deep Learning. (arXiv:2102.12903v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.01508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goffrier_G/0/1/0/all/0/1\">Graham W. Van Goffrier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mostajeran_C/0/1/0/all/0/1\">Cyrus Mostajeran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sepulchre_R/0/1/0/all/0/1\">Rodolphe Sepulchre</a>",
          "description": "Covariance data as represented by symmetric positive definite (SPD) matrices\nare ubiquitous throughout technical study as efficient descriptors of\ninterdependent systems. Euclidean analysis of SPD matrices, while\ncomputationally fast, can lead to skewed and even unphysical interpretations of\ndata. Riemannian methods preserve the geometric structure of SPD data at the\ncost of expensive eigenvalue computations. In this paper, we propose a\ngeometric method for unsupervised clustering of SPD data based on the Thompson\nmetric. This technique relies upon a novel \"inductive midrange\" centroid\ncomputation for SPD data, whose properties are examined and numerically\nconfirmed. We demonstrate the incorporation of the Thompson metric and\ninductive midrange into X-means and K-means++ clustering algorithms.",
          "link": "http://arxiv.org/abs/2006.01508",
          "publishedOn": "2021-07-22T02:03:11.685Z",
          "wordCount": 582,
          "title": "Inductive Geometric Matrix Midranges. (arXiv:2006.01508v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09989",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1\">Guangyuan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lv_J/0/1/0/all/0/1\">Jun Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tong_X/0/1/0/all/0/1\">Xiangrong Tong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chengyan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>",
          "description": "Magnetic resonance imaging (MRI) is an important medical imaging modality,\nbut its acquisition speed is quite slow due to the physiological limitations.\nRecently, super-resolution methods have shown excellent performance in\naccelerating MRI. In some circumstances, it is difficult to obtain\nhigh-resolution images even with prolonged scan time. Therefore, we proposed a\nnovel super-resolution method that uses a generative adversarial network (GAN)\nwith cyclic loss and attention mechanism to generate high-resolution MR images\nfrom low-resolution MR images by a factor of 2. We implemented our model on\npelvic images from healthy subjects as training and validation data, while\nthose data from patients were used for testing. The MR dataset was obtained\nusing different imaging sequences, including T2, T2W SPAIR, and mDIXON-W. Four\nmethods, i.e., BICUBIC, SRCNN, SRGAN, and EDSR were used for comparison.\nStructural similarity, peak signal to noise ratio, root mean square error, and\nvariance inflation factor were used as calculation indicators to evaluate the\nperformances of the proposed method. Various experimental results showed that\nour method can better restore the details of the high-resolution MR image as\ncompared to the other methods. In addition, the reconstructed high-resolution\nMR image can provide better lesion textures in the tumor patients, which is\npromising to be used in clinical diagnosis.",
          "link": "http://arxiv.org/abs/2107.09989",
          "publishedOn": "2021-07-22T02:03:11.657Z",
          "wordCount": 677,
          "title": "High-Resolution Pelvic MRI Reconstruction Using a Generative Adversarial Network with Attention and Cyclic Loss. (arXiv:2107.09989v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sancarlos_A/0/1/0/all/0/1\">Abel Sancarlos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cameron_M/0/1/0/all/0/1\">Morgan Cameron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peuvedic_J/0/1/0/all/0/1\">Jean-Marc Le Peuvedic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groulier_J/0/1/0/all/0/1\">Juliette Groulier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duval_J/0/1/0/all/0/1\">Jean-Louis Duval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cueto_E/0/1/0/all/0/1\">Elias Cueto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinesta_F/0/1/0/all/0/1\">Francisco Chinesta</a>",
          "description": "The concept of Hybrid Twin (HT) has recently received a growing interest\nthanks to the availability of powerful machine learning techniques. This twin\nconcept combines physics-based models within a model-order reduction\nframework-to obtain real-time feedback rates-and data science. Thus, the main\nidea of the HT is to develop on-the-fly data-driven models to correct possible\ndeviations between measurements and physics-based model predictions. This paper\nis focused on the computation of stable, fast and accurate corrections in the\nHybrid Twin framework. Furthermore, regarding the delicate and important\nproblem of stability, a new approach is proposed, introducing several\nsub-variants and guaranteeing a low computational cost as well as the\nachievement of a stable time-integration.",
          "link": "http://arxiv.org/abs/2106.03464",
          "publishedOn": "2021-07-22T02:03:11.649Z",
          "wordCount": 569,
          "title": "Learning stable reduced-order models for hybrid twins. (arXiv:2106.03464v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14496",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Janner_M/0/1/0/all/0/1\">Michael Janner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "We introduce the $\\gamma$-model, a predictive model of environment dynamics\nwith an infinite probabilistic horizon. Replacing standard single-step models\nwith $\\gamma$-models leads to generalizations of the procedures central to\nmodel-based control, including the model rollout and model-based value\nestimation. The $\\gamma$-model, trained with a generative reinterpretation of\ntemporal difference learning, is a natural continuous analogue of the successor\nrepresentation and a hybrid between model-free and model-based mechanisms. Like\na value function, it contains information about the long-term future; like a\nstandard predictive model, it is independent of task reward. We instantiate the\n$\\gamma$-model as both a generative adversarial network and normalizing flow,\ndiscuss how its training reflects an inescapable tradeoff between training-time\nand testing-time compounding errors, and empirically investigate its utility\nfor prediction and control.",
          "link": "http://arxiv.org/abs/2010.14496",
          "publishedOn": "2021-07-22T02:03:11.642Z",
          "wordCount": 594,
          "title": "$\\gamma$-Models: Generative Temporal Difference Learning for Infinite-Horizon Prediction. (arXiv:2010.14496v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nordmark_N/0/1/0/all/0/1\">Nils Nordmark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayenew_M/0/1/0/all/0/1\">Mola Ayenew</a>",
          "description": "The parsing of windows in building facades is a long-desired but challenging\ntask in computer vision. It is crucial to urban analysis, semantic\nreconstruction, lifecycle analysis, digital twins, and scene parsing amongst\nother building-related tasks that require high-quality semantic data. This\narticle investigates the usage of the mask R-CNN framework to be used for\nwindow detection of facade imagery input. We utilize transfer learning to train\nour proposed method on COCO weights with our own collected dataset of street\nview images of facades to produce instance segmentations of our new window\nclass. Experimental results show that our suggested approach with a relatively\nsmall dataset trains the network only with transfer learning and augmentation\nachieves results on par with prior state-of-the-art window detection\napproaches, even without post-optimization techniques.",
          "link": "http://arxiv.org/abs/2107.10006",
          "publishedOn": "2021-07-22T02:03:11.629Z",
          "wordCount": 577,
          "title": "Window Detection In Facade Imagery: A Deep Learning Approach Using Mask R-CNN. (arXiv:2107.10006v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.00130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abadal_S/0/1/0/all/0/1\">Sergi Abadal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Akshay Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guirado_R/0/1/0/all/0/1\">Robert Guirado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Alonso_J/0/1/0/all/0/1\">Jorge L&#xf3;pez-Alonso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alarcon_E/0/1/0/all/0/1\">Eduard Alarc&#xf3;n</a>",
          "description": "Graph Neural Networks (GNNs) have exploded onto the machine learning scene in\nrecent years owing to their capability to model and learn from graph-structured\ndata. Such an ability has strong implications in a wide variety of fields whose\ndata is inherently relational, for which conventional neural networks do not\nperform well. Indeed, as recent reviews can attest, research in the area of\nGNNs has grown rapidly and has lead to the development of a variety of GNN\nalgorithm variants as well as to the exploration of groundbreaking applications\nin chemistry, neurology, electronics, or communication networks, among others.\nAt the current stage of research, however, the efficient processing of GNNs is\nstill an open challenge for several reasons. Besides of their novelty, GNNs are\nhard to compute due to their dependence on the input graph, their combination\nof dense and very sparse operations, or the need to scale to huge graphs in\nsome applications. In this context, this paper aims to make two main\ncontributions. On the one hand, a review of the field of GNNs is presented from\nthe perspective of computing. This includes a brief tutorial on the GNN\nfundamentals, an overview of the evolution of the field in the last decade, and\na summary of operations carried out in the multiple phases of different GNN\nalgorithm variants. On the other hand, an in-depth analysis of current software\nand hardware acceleration schemes is provided, from which a hardware-software,\ngraph-aware, and communication-centric vision for GNN accelerators is\ndistilled.",
          "link": "http://arxiv.org/abs/2010.00130",
          "publishedOn": "2021-07-22T02:03:11.611Z",
          "wordCount": 739,
          "title": "Computing Graph Neural Networks: A Survey from Algorithms to Accelerators. (arXiv:2010.00130v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Janner_M/0/1/0/all/0/1\">Michael Janner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Reinforcement learning (RL) is typically concerned with estimating\nsingle-step policies or single-step models, leveraging the Markov property to\nfactorize the problem in time. However, we can also view RL as a sequence\nmodeling problem, with the goal being to predict a sequence of actions that\nleads to a sequence of high rewards. Viewed in this way, it is tempting to\nconsider whether powerful, high-capacity sequence prediction models that work\nwell in other domains, such as natural-language processing, can also provide\nsimple and effective solutions to the RL problem. To this end, we explore how\nRL can be reframed as \"one big sequence modeling\" problem, using\nstate-of-the-art Transformer architectures to model distributions over\nsequences of states, actions, and rewards. Addressing RL as a sequence modeling\nproblem significantly simplifies a range of design decisions: we no longer\nrequire separate behavior policy constraints, as is common in prior work on\noffline model-free RL, and we no longer require ensembles or other epistemic\nuncertainty estimators, as is common in prior work on model-based RL. All of\nthese roles are filled by the same Transformer sequence model. In our\nexperiments, we demonstrate the flexibility of this approach across\nlong-horizon dynamics prediction, imitation learning, goal-conditioned RL, and\noffline RL.",
          "link": "http://arxiv.org/abs/2106.02039",
          "publishedOn": "2021-07-22T02:03:11.604Z",
          "wordCount": 663,
          "title": "Reinforcement Learning as One Big Sequence Modeling Problem. (arXiv:2106.02039v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_W/0/1/0/all/0/1\">Wenbo Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaibo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingzhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1\">Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</a>",
          "description": "Sliced Stein discrepancy (SSD) and its kernelized variants have demonstrated\npromising successes in goodness-of-fit tests and model learning in high\ndimensions. Despite their theoretical elegance, their empirical performance\ndepends crucially on the search of optimal slicing directions to discriminate\nbetween two distributions. Unfortunately, previous gradient-based optimisation\napproaches for this task return sub-optimal results: they are computationally\nexpensive, sensitive to initialization, and they lack theoretical guarantees\nfor convergence. We address these issues in two steps. First, we provide\ntheoretical results stating that the requirement of using optimal slicing\ndirections in the kernelized version of SSD can be relaxed, validating the\nresulting discrepancy with finite random slicing directions. Second, given that\ngood slicing directions are crucial for practical performance, we propose a\nfast algorithm for finding such slicing directions based on ideas of active\nsub-space construction and spectral decomposition. Experiments on\ngoodness-of-fit tests and model learning show that our approach achieves both\nimproved performance and faster convergence. Especially, we demonstrate a\n14-80x speed-up in goodness-of-fit tests when comparing with gradient-based\nalternatives.",
          "link": "http://arxiv.org/abs/2102.03159",
          "publishedOn": "2021-07-22T02:03:11.596Z",
          "wordCount": 655,
          "title": "Active Slices for Sliced Stein Discrepancy. (arXiv:2102.03159v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.06043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_E/0/1/0/all/0/1\">Eric Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafailov_R/0/1/0/all/0/1\">Rafael Rafailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xue Bin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>",
          "description": "This paper introduces the offline meta-reinforcement learning (offline\nmeta-RL) problem setting and proposes an algorithm that performs well in this\nsetting. Offline meta-RL is analogous to the widely successful supervised\nlearning strategy of pre-training a model on a large batch of fixed,\npre-collected data (possibly from various tasks) and fine-tuning the model to a\nnew task with relatively little data. That is, in offline meta-RL, we\nmeta-train on fixed, pre-collected data from several tasks in order to adapt to\na new task with a very small amount (less than 5 trajectories) of data from the\nnew task. By nature of being offline, algorithms for offline meta-RL can\nutilize the largest possible pool of training data available and eliminate\npotentially unsafe or costly data collection during meta-training. This setting\ninherits the challenges of offline RL, but it differs significantly because\noffline RL does not generally consider a) transfer to new tasks or b) limited\ndata from the test task, both of which we face in offline meta-RL. Targeting\nthe offline meta-RL setting, we propose Meta-Actor Critic with Advantage\nWeighting (MACAW), an optimization-based meta-learning algorithm that uses\nsimple, supervised regression objectives for both the inner and outer loop of\nmeta-training. On offline variants of common meta-RL benchmarks, we empirically\nfind that this approach enables fully offline meta-reinforcement learning and\nachieves notable gains over prior methods.",
          "link": "http://arxiv.org/abs/2008.06043",
          "publishedOn": "2021-07-22T02:03:11.586Z",
          "wordCount": 713,
          "title": "Offline Meta-Reinforcement Learning with Advantage Weighting. (arXiv:2008.06043v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03733",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Lingwei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhebang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>",
          "description": "Model-free deep reinforcement learning has achieved great success in many\ndomains, such as video games, recommendation systems and robotic control tasks.\nIn continuous control tasks, widely used policies with Gaussian distributions\nresults in ineffective exploration of environments and limited performance of\nalgorithms in many cases. In this paper, we propose a density-free off-policy\nalgorithm, Generative Actor-Critic(GAC), using the push-forward model to\nincrease the expressiveness of policies, which also includes an entropy-like\ntechnique, MMD-entropy regularizer, to balance the exploration and\nexploitation. Additionnally, we devise an adaptive mechanism to automatically\nscale this regularizer, which further improves the stability and robustness of\nGAC. The experiment results show that push-forward policies possess desirable\nfeatures, such as multi-modality, which can improve the efficiency of\nexploration and asymptotic performance of algorithms obviously.",
          "link": "http://arxiv.org/abs/2105.03733",
          "publishedOn": "2021-07-22T02:03:11.579Z",
          "wordCount": 588,
          "title": "Generative Actor-Critic: An Off-policy Algorithm Using the Push-forward Model. (arXiv:2105.03733v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kongtao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franko_K/0/1/0/all/0/1\">Ken Franko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_R/0/1/0/all/0/1\">Ruoxin Sang</a>",
          "description": "The deployment of convolutional neural networks is often hindered by high\ncomputational and storage requirements. Structured model pruning is a promising\napproach to alleviate these requirements. Using the VGG-16 model as an example,\nwe measure the accuracy-efficiency trade-off for various structured model\npruning methods and datasets (CIFAR-10 and ImageNet) on Tensor Processing Units\n(TPUs). To measure the actual performance of models, we develop a structured\nmodel pruning library for TensorFlow2 to modify models in place (instead of\nadding mask layers). We show that structured model pruning can significantly\nimprove model memory usage and speed on TPUs without losing accuracy,\nespecially for small datasets (e.g., CIFAR-10).",
          "link": "http://arxiv.org/abs/2107.04191",
          "publishedOn": "2021-07-22T02:03:11.558Z",
          "wordCount": 570,
          "title": "Structured Model Pruning of Convolutional Networks on Tensor Processing Units. (arXiv:2107.04191v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suri_A/0/1/0/all/0/1\">Anshuman Suri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_D/0/1/0/all/0/1\">David Evans</a>",
          "description": "Property inference attacks reveal statistical properties about a training set\nbut are difficult to distinguish from the primary purposes of statistical\nmachine learning, which is to produce models that capture statistical\nproperties about a distribution. Motivated by Yeom et al.'s membership\ninference framework, we propose a formal and generic definition of property\ninference attacks. The proposed notion describes attacks that can distinguish\nbetween possible training distributions, extending beyond previous property\ninference attacks that infer the ratio of a particular type of data in the\ntraining data set. In this paper, we show how our definition captures previous\nproperty inference attacks as well as a new attack that reveals the average\ndegree of nodes of a training graph and report on experiments giving insight\ninto the potential risks of property inference attacks.",
          "link": "http://arxiv.org/abs/2106.03699",
          "publishedOn": "2021-07-22T02:03:11.549Z",
          "wordCount": 596,
          "title": "Formalizing Distribution Inference Risks. (arXiv:2106.03699v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.02356",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Chen_S/0/1/0/all/0/1\">Shixiang Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Deng_Z/0/1/0/all/0/1\">Zengde Deng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ma_S/0/1/0/all/0/1\">Shiqian Ma</a>, <a href=\"http://arxiv.org/find/math/1/au:+So_A/0/1/0/all/0/1\">Anthony Man-Cho So</a>",
          "description": "We consider the problem of maximizing the $\\ell_1$ norm of a linear map over\nthe sphere, which arises in various machine learning applications such as\northogonal dictionary learning (ODL) and robust subspace recovery (RSR). The\nproblem is numerically challenging due to its nonsmooth objective and nonconvex\nconstraint, and its algorithmic aspects have not been well explored. In this\npaper, we show how the manifold structure of the sphere can be exploited to\ndesign fast algorithms for tackling this problem. Specifically, our\ncontribution is threefold. First, we present a manifold proximal point\nalgorithm (ManPPA) for the problem and show that it converges at a sublinear\nrate. Furthermore, we show that ManPPA can achieve a quadratic convergence rate\nwhen applied to the ODL and RSR problems. Second, we propose a stochastic\nvariant of ManPPA called StManPPA, which is well suited for large-scale\ncomputation, and establish its sublinear convergence rate. Both ManPPA and\nStManPPA have provably faster convergence rates than existing subgradient-type\nmethods. Third, using ManPPA as a building block, we propose a new approach to\nsolving a matrix analog of the problem, in which the sphere is replaced by the\nStiefel manifold. The results from our extensive numerical experiments on the\nODL and RSR problems demonstrate the efficiency and efficacy of our proposed\nmethods.",
          "link": "http://arxiv.org/abs/2005.02356",
          "publishedOn": "2021-07-22T02:03:11.528Z",
          "wordCount": 701,
          "title": "Manifold Proximal Point Algorithms for Dual Principal Component Pursuit and Orthogonal Dictionary Learning. (arXiv:2005.02356v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanxiong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>",
          "description": "Recommender systems are gaining increasing and critical impacts on human and\nsociety since a growing number of users use them for information seeking and\ndecision making. Therefore, it is crucial to address the potential unfairness\nproblems in recommendations. Just like users have personalized preferences on\nitems, users' demands for fairness are also personalized in many scenarios.\nTherefore, it is important to provide personalized fair recommendations for\nusers to satisfy their personalized fairness demands. Besides, previous works\non fair recommendation mainly focus on association-based fairness. However, it\nis important to advance from associative fairness notions to causal fairness\nnotions for assessing fairness more properly in recommender systems. Based on\nthe above considerations, this paper focuses on achieving personalized\ncounterfactual fairness for users in recommender systems. To this end, we\nintroduce a framework for achieving counterfactually fair recommendations\nthrough adversary learning by generating feature-independent user embeddings\nfor recommendation. The framework allows recommender systems to achieve\npersonalized fairness for users while also covering non-personalized\nsituations. Experiments on two real-world datasets with shallow and deep\nrecommendation algorithms show that our method can generate fairer\nrecommendations for users with a desirable recommendation performance.",
          "link": "http://arxiv.org/abs/2105.09829",
          "publishedOn": "2021-07-22T02:03:11.520Z",
          "wordCount": 659,
          "title": "Personalized Counterfactual Fairness in Recommendation. (arXiv:2105.09829v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schreurs_J/0/1/0/all/0/1\">Joachim Schreurs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meulemeester_H/0/1/0/all/0/1\">Hannes De Meulemeester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fanuel_M/0/1/0/all/0/1\">Micha&#xeb;l Fanuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moor_B/0/1/0/all/0/1\">Bart De Moor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suykens_J/0/1/0/all/0/1\">Johan A.K. Suykens</a>",
          "description": "Commonly, machine learning models minimize an empirical expectation. As a\nresult, the trained models typically perform well for the majority of the data\nbut the performance may deteriorate in less dense regions of the dataset. This\nissue also arises in generative modeling. A generative model may overlook\nunderrepresented modes that are less frequent in the empirical data\ndistribution. This problem is known as complete mode coverage. We propose a\nsampling procedure based on ridge leverage scores which significantly improves\nmode coverage when compared to standard methods and can easily be combined with\nany GAN. Ridge leverage scores are computed by using an explicit feature map,\nassociated with the next-to-last layer of a GAN discriminator or of a\npre-trained network, or by using an implicit feature map corresponding to a\nGaussian kernel. Multiple evaluations against recent approaches of complete\nmode coverage show a clear improvement when using the proposed sampling\nstrategy.",
          "link": "http://arxiv.org/abs/2104.02373",
          "publishedOn": "2021-07-22T02:03:11.513Z",
          "wordCount": 637,
          "title": "Leverage Score Sampling for Complete Mode Coverage in Generative Adversarial Networks. (arXiv:2104.02373v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neu_G/0/1/0/all/0/1\">Gergely Neu</a>",
          "description": "We study the generalization properties of the popular stochastic optimization\nmethod known as stochastic gradient descent (SGD) for optimizing general\nnon-convex loss functions. Our main contribution is providing upper bounds on\nthe generalization error that depend on local statistics of the stochastic\ngradients evaluated along the path of iterates calculated by SGD. The key\nfactors our bounds depend on are the variance of the gradients (with respect to\nthe data distribution) and the local smoothness of the objective function along\nthe SGD path, and the sensitivity of the loss function to perturbations to the\nfinal output. Our key technical tool is combining the information-theoretic\ngeneralization bounds previously used for analyzing randomized variants of SGD\nwith a perturbation analysis of the iterates.",
          "link": "http://arxiv.org/abs/2102.00931",
          "publishedOn": "2021-07-22T02:03:11.505Z",
          "wordCount": 580,
          "title": "Information-Theoretic Generalization Bounds for Stochastic Gradient Descent. (arXiv:2102.00931v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.08616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alex Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_X/0/1/0/all/0/1\">Xiaohan Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsuei_S/0/1/0/all/0/1\">Stephanie Tsuei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We describe a method to infer dense depth from camera motion and sparse depth\nas estimated using a visual-inertial odometry system. Unlike other scenarios\nusing point clouds from lidar or structured light sensors, we have few hundreds\nto few thousand points, insufficient to inform the topology of the scene. Our\nmethod first constructs a piecewise planar scaffolding of the scene, and then\nuses it to infer dense depth using the image along with the sparse points. We\nuse a predictive cross-modal criterion, akin to `self-supervision,' measuring\nphotometric consistency across time, forward-backward pose consistency, and\ngeometric compatibility with the sparse point cloud. We also launch the first\nvisual-inertial + depth dataset, which we hope will foster additional\nexploration into combining the complementary strengths of visual and inertial\nsensors. To compare our method to prior work, we adopt the unsupervised KITTI\ndepth completion benchmark, and show state-of-the-art performance on it. Code\navailable at:\nhttps://github.com/alexklwong/unsupervised-depth-completion-visual-inertial-odometry.",
          "link": "http://arxiv.org/abs/1905.08616",
          "publishedOn": "2021-07-22T02:03:11.485Z",
          "wordCount": 646,
          "title": "Unsupervised Depth Completion from Visual Inertial Odometry. (arXiv:1905.08616v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1\">Anirudh Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Didolkar_A/0/1/0/all/0/1\">Aniket Didolkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_N/0/1/0/all/0/1\">Nan Rosemary Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1\">Charles Blundell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaudoin_P/0/1/0/all/0/1\">Philippe Beaudoin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1\">Nicolas Heess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1\">Michael Mozer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>",
          "description": "Visual environments are structured, consisting of distinct objects or\nentities. These entities have properties -- both visible and latent -- that\ndetermine the manner in which they interact with one another. To partition\nimages into entities, deep-learning researchers have proposed structural\ninductive biases such as slot-based architectures. To model interactions among\nentities, equivariant graph neural nets (GNNs) are used, but these are not\nparticularly well suited to the task for two reasons. First, GNNs do not\npredispose interactions to be sparse, as relationships among independent\nentities are likely to be. Second, GNNs do not factorize knowledge about\ninteractions in an entity-conditional manner. As an alternative, we take\ninspiration from cognitive science and resurrect a classic approach, production\nsystems, which consist of a set of rule templates that are applied by binding\nplaceholder variables in the rules to specific entities. Rules are scored on\ntheir match to entities, and the best fitting rules are applied to update\nentity properties. In a series of experiments, we demonstrate that this\narchitecture achieves a flexible, dynamic flow of control and serves to\nfactorize entity-specific and rule-based information. This disentangling of\nknowledge achieves robust future-state prediction in rich visual environments,\noutperforming state-of-the-art methods using GNNs, and allows for the\nextrapolation from simple (few object) environments to more complex\nenvironments.",
          "link": "http://arxiv.org/abs/2103.01937",
          "publishedOn": "2021-07-22T02:03:11.474Z",
          "wordCount": 683,
          "title": "Neural Production Systems. (arXiv:2103.01937v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banburski_A/0/1/0/all/0/1\">Andrzej Banburski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torre_F/0/1/0/all/0/1\">Fernanda De La Torre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pant_N/0/1/0/all/0/1\">Nishka Pant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shastri_I/0/1/0/all/0/1\">Ishana Shastri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poggio_T/0/1/0/all/0/1\">Tomaso Poggio</a>",
          "description": "Recent theoretical results show that gradient descent on deep neural networks\nunder exponential loss functions locally maximizes classification margin, which\nis equivalent to minimizing the norm of the weight matrices under margin\nconstraints. This property of the solution however does not fully characterize\nthe generalization performance. We motivate theoretically and show empirically\nthat the area under the curve of the margin distribution on the training set is\nin fact a good measure of generalization. We then show that, after data\nseparation is achieved, it is possible to dynamically reduce the training set\nby more than 99% without significant loss of performance. Interestingly, the\nresulting subset of \"high capacity\" features is not consistent across different\ntraining runs, which is consistent with the theoretical claim that all training\npoints should converge to the same asymptotic margin under SGD and in the\npresence of both batch normalization and weight decay.",
          "link": "http://arxiv.org/abs/2107.10199",
          "publishedOn": "2021-07-22T02:03:11.455Z",
          "wordCount": 601,
          "title": "Distribution of Classification Margins: Are All Data Equal?. (arXiv:2107.10199v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.12741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renshen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1\">Yasuhisa Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popat_A/0/1/0/all/0/1\">Ashok C. Popat</a>",
          "description": "Paragraphs are an important class of document entities. We propose a new\napproach for paragraph identification by spatial graph convolutional neural\nnetworks (GCN) applied on OCR text boxes. Two steps, namely line splitting and\nline clustering, are performed to extract paragraphs from the lines in OCR\nresults. Each step uses a beta-skeleton graph constructed from bounding boxes,\nwhere the graph edges provide efficient support for graph convolution\noperations. With only pure layout input features, the GCN model size is 3~4\norders of magnitude smaller compared to R-CNN based models, while achieving\ncomparable or better accuracies on PubLayNet and other datasets. Furthermore,\nthe GCN models show good generalization from synthetic training data to\nreal-world images, and good adaptivity for variable document styles.",
          "link": "http://arxiv.org/abs/2101.12741",
          "publishedOn": "2021-07-22T02:03:11.448Z",
          "wordCount": 603,
          "title": "Post-OCR Paragraph Recognition by Graph Convolutional Networks. (arXiv:2101.12741v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07487",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Si_S/0/1/0/all/0/1\">Shijing Si</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Oates_C/0/1/0/all/0/1\">Chris. J. Oates</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Duncan_A/0/1/0/all/0/1\">Andrew B. Duncan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Briol_F/0/1/0/all/0/1\">Fran&#xe7;ois-Xavier Briol</a>",
          "description": "Control variates are a well-established tool to reduce the variance of Monte\nCarlo estimators. However, for large-scale problems including high-dimensional\nand large-sample settings, their advantages can be outweighed by a substantial\ncomputational cost. This paper considers control variates based on Stein\noperators, presenting a framework that encompasses and generalizes existing\napproaches that use polynomials, kernels and neural networks. A learning\nstrategy based on minimising a variational objective through stochastic\noptimization is proposed, leading to scalable and effective control variates.\nNovel theoretical results are presented to provide insight into the variance\nreduction that can be achieved, and an empirical assessment, including\napplications to Bayesian inference, is provided in support.",
          "link": "http://arxiv.org/abs/2006.07487",
          "publishedOn": "2021-07-22T02:03:11.427Z",
          "wordCount": 574,
          "title": "Scalable Control Variates for Monte Carlo Methods via Stochastic Optimization. (arXiv:2006.07487v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10110",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Cheng_S/0/1/0/all/0/1\">Shuyu Cheng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wu_G/0/1/0/all/0/1\">Guoqiang Wu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "Zeroth-order (ZO) optimization is widely used to handle challenging tasks,\nsuch as query-based black-box adversarial attacks and reinforcement learning.\nVarious attempts have been made to integrate prior information into the\ngradient estimation procedure based on finite differences, with promising\nempirical results. However, their convergence properties are not well\nunderstood. This paper makes an attempt to fill this gap by analyzing the\nconvergence of prior-guided ZO algorithms under a greedy descent framework with\nvarious gradient estimators. We provide a convergence guarantee for the\nprior-guided random gradient-free (PRGF) algorithms. Moreover, to further\naccelerate over greedy descent methods, we present a new accelerated random\nsearch (ARS) algorithm that incorporates prior information, together with a\nconvergence analysis. Finally, our theoretical results are confirmed by\nexperiments on several numerical benchmarks as well as adversarial attacks.",
          "link": "http://arxiv.org/abs/2107.10110",
          "publishedOn": "2021-07-22T02:03:11.420Z",
          "wordCount": 574,
          "title": "On the Convergence of Prior-Guided Zeroth-Order Optimization Algorithms. (arXiv:2107.10110v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2101.07948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziheng Wang</a>",
          "description": "The last few years have seen gigantic leaps in algorithms and systems to\nsupport efficient deep learning inference. Pruning and quantization algorithms\ncan now consistently compress neural networks by an order of magnitude. For a\ncompressed neural network, a multitude of inference frameworks have been\ndesigned to maximize the performance of the target hardware. While we find\nmature support for quantized neural networks in production frameworks such as\nOpenVINO and MNN, support for pruned sparse neural networks is still lacking.\nTo tackle this challenge, we present SparseDNN, a sparse deep learning\ninference engine targeting CPUs. We present both kernel-level optimizations\nwith a sparse code generator to accelerate sparse operators and novel\nnetwork-level optimizations catering to sparse networks. We show that our\nsparse code generator can achieve significant speedups over state-of-the-art\nsparse and dense libraries. On end-to-end benchmarks such as Huggingface\npruneBERT, SparseDNN achieves up to 5x throughput improvement over dense\ninference with state-of-the-art OpenVINO. Open source library at:\nhttps://github.com/marsupialtail/sparsednn.",
          "link": "http://arxiv.org/abs/2101.07948",
          "publishedOn": "2021-07-22T02:03:11.399Z",
          "wordCount": 628,
          "title": "SparseDNN: Fast Sparse Deep Learning Inference on CPUs. (arXiv:2101.07948v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12063",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_P/0/1/0/all/0/1\">Perttu H&#xe4;m&#xe4;l&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trapp_M/0/1/0/all/0/1\">Martin Trapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saloheimo_T/0/1/0/all/0/1\">Tuure Saloheimo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1\">Arno Solin</a>",
          "description": "We propose Deep Residual Mixture Models (DRMMs), a novel deep generative\nmodel architecture. Compared to other deep models, DRMMs allow more flexible\nconditional sampling: The model can be trained once with all variables, and\nthen used for sampling with arbitrary combinations of conditioning variables,\nGaussian priors, and (in)equality constraints. This provides new opportunities\nfor interactive and exploratory machine learning, where one should minimize the\nuser waiting for retraining a model. We demonstrate DRMMs in constrained\nmulti-limb inverse kinematics and controllable generation of animations.",
          "link": "http://arxiv.org/abs/2006.12063",
          "publishedOn": "2021-07-22T02:03:11.380Z",
          "wordCount": 557,
          "title": "Deep Residual Mixture Models. (arXiv:2006.12063v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_A/0/1/0/all/0/1\">Akum S. Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1\">Loveleen C. Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mastorides_S/0/1/0/all/0/1\">Stephen M. Mastorides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foulis_P/0/1/0/all/0/1\">Philip R. Foulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeLand_L/0/1/0/all/0/1\">Lauren A. DeLand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seifert_R/0/1/0/all/0/1\">Robert P. Seifert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borkowski_A/0/1/0/all/0/1\">Andrew Borkowski</a>",
          "description": "Flow cytometry is a technique that measures multiple fluorescence and light\nscatter-associated parameters from individual cells as they flow a single file\nthrough an excitation light source. These cells are labeled with antibodies to\ndetect various antigens and the fluorescence signals reflect antigen\nexpression. Interpretation of the multiparameter flow cytometry data is\nlaborious, time-consuming, and expensive. It involves manual interpretation of\ncell distribution and pattern recognition on two-dimensional plots by highly\ntrained medical technologists and pathologists. Using various machine learning\nalgorithms, we attempted to develop an automated analysis for clinical flow\ncytometry cases that would automatically classify normal and chronic\nlymphocytic leukemia cases. We achieved the best success with the Gradient\nBoosting. The XGBoost classifier achieved a specificity of 1.00 and a\nsensitivity of 0.67, a negative predictive value of 0.75, a positive predictive\nvalue of 1.00, and an overall accuracy of 0.83 in prospectively classifying\ncases with malignancies.",
          "link": "http://arxiv.org/abs/2107.09728",
          "publishedOn": "2021-07-22T02:03:11.310Z",
          "wordCount": 606,
          "title": "Machine Learning Approaches to Automated Flow Cytometry Diagnosis of Chronic Lymphocytic Leukemia. (arXiv:2107.09728v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bitencourt_H/0/1/0/all/0/1\">Hugo Vinicius Bitencourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guimaraes_F/0/1/0/all/0/1\">Frederico Gadelha Guimar&#xe3;es</a>",
          "description": "In Internet of things (IoT), data is continuously recorded from different\ndata sources and devices can suffer faults in their embedded electronics, thus\nleading to a high-dimensional data sets and concept drift events. Therefore,\nmethods that are capable of high-dimensional non-stationary time series are of\ngreat value in IoT applications. Fuzzy Time Series (FTS) models stand out as\ndata-driven non-parametric models of easy implementation and high accuracy.\nUnfortunately, FTS encounters difficulties when dealing with data sets of many\nvariables and scenarios with concept drift. We present a new approach to handle\nhigh-dimensional non-stationary time series, by projecting the original\nhigh-dimensional data into a low dimensional embedding space and using FTS\napproach. Combining these techniques enables a better representation of the\ncomplex content of non-stationary multivariate time series and accurate\nforecasts. Our model is able to explain 98% of the variance and reach 11.52% of\nRMSE, 2.68% of MAE and 2.91% of MAPE.",
          "link": "http://arxiv.org/abs/2107.09785",
          "publishedOn": "2021-07-22T02:03:11.237Z",
          "wordCount": 628,
          "title": "High-dimensional Multivariate Time Series Forecasting in IoT Applications using Embedding Non-stationary Fuzzy Time Series. (arXiv:2107.09785v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shin_E/0/1/0/all/0/1\">Eura Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klasnja_P/0/1/0/all/0/1\">Pedja Klasnja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_S/0/1/0/all/0/1\">Susan Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1\">Finale Doshi-Velez</a>",
          "description": "Motivated by the need for efficient and personalized learning in mobile\nhealth, we investigate the problem of online kernel selection for Gaussian\nProcess regression in the multi-task setting. We propose a novel generative\nprocess on the kernel composition for this purpose. Our method demonstrates\nthat trajectories of kernel evolutions can be transferred between users to\nimprove learning and that the kernels themselves are meaningful for an mHealth\nprediction goal.",
          "link": "http://arxiv.org/abs/2107.09949",
          "publishedOn": "2021-07-22T02:03:11.218Z",
          "wordCount": 508,
          "title": "Online structural kernel selection for mobile health. (arXiv:2107.09949v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09817",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mei_X/0/1/0/all/0/1\">Xinhao Mei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xubo Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Q/0/1/0/all/0/1\">Qiushi Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plumbley_M/0/1/0/all/0/1\">Mark D. Plumbley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wenwu Wang</a>",
          "description": "Audio captioning aims to automatically generate a natural language\ndescription of an audio clip. Most captioning models follow an encoder-decoder\narchitecture, where the decoder predicts words based on the audio features\nextracted by the encoder. Convolutional neural networks (CNNs) and recurrent\nneural networks (RNNs) are often used as the audio encoder. However, CNNs can\nbe limited in modelling temporal relationships among the time frames in an\naudio signal, while RNNs can be limited in modelling the long-range\ndependencies among the time frames. In this paper, we propose an Audio\nCaptioning Transformer (ACT), which is a full Transformer network based on an\nencoder-decoder architecture and is totally convolution-free. The proposed\nmethod has a better ability to model the global information within an audio\nsignal as well as capture temporal relationships between audio events. We\nevaluate our model on AudioCaps, which is the largest audio captioning dataset\npublicly available. Our model shows competitive performance compared to other\nstate-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2107.09817",
          "publishedOn": "2021-07-22T02:03:11.212Z",
          "wordCount": 600,
          "title": "Audio Captioning Transformer. (arXiv:2107.09817v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dagan_Y/0/1/0/all/0/1\">Yuval Dagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daskalakis_C/0/1/0/all/0/1\">Constantinos Daskalakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dikkala_N/0/1/0/all/0/1\">Nishanth Dikkala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1\">Surbhi Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandiros_A/0/1/0/all/0/1\">Anthimos Vardis Kandiros</a>",
          "description": "We consider a general statistical estimation problem wherein binary labels\nacross different observations are not independent conditioned on their feature\nvectors, but dependent, capturing settings where e.g. these observations are\ncollected on a spatial domain, a temporal domain, or a social network, which\ninduce dependencies. We model these dependencies in the language of Markov\nRandom Fields and, importantly, allow these dependencies to be substantial, i.e\ndo not assume that the Markov Random Field capturing these dependencies is in\nhigh temperature. As our main contribution we provide algorithms and\nstatistically efficient estimation rates for this model, giving several\ninstantiations of our bounds in logistic regression, sparse logistic\nregression, and neural network settings with dependent data. Our estimation\nguarantees follow from novel results for estimating the parameters (i.e.\nexternal fields and interaction strengths) of Ising models from a {\\em single}\nsample. {We evaluate our estimation approach on real networked data, showing\nthat it outperforms standard regression approaches that ignore dependencies,\nacross three text classification datasets: Cora, Citeseer and Pubmed.}",
          "link": "http://arxiv.org/abs/2107.09773",
          "publishedOn": "2021-07-22T02:03:11.191Z",
          "wordCount": 609,
          "title": "Statistical Estimation from Dependent Data. (arXiv:2107.09773v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.04419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1\">Vikas Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_M/0/1/0/all/0/1\">Minh-Thang Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hieu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>",
          "description": "Despite recent success, most contrastive self-supervised learning methods are\ndomain-specific, relying heavily on data augmentation techniques that require\nknowledge about a particular domain, such as image cropping and rotation. To\novercome such limitation, we propose a novel domain-agnostic approach to\ncontrastive learning, named DACL, that is applicable to domains where\ninvariances, and thus, data augmentation techniques, are not readily available.\nKey to our approach is the use of Mixup noise to create similar and dissimilar\nexamples by mixing data samples differently either at the input or hidden-state\nlevels. To demonstrate the effectiveness of DACL, we conduct experiments across\nvarious domains such as tabular data, images, and graphs. Our results show that\nDACL not only outperforms other domain-agnostic noising methods, such as\nGaussian-noise, but also combines well with domain-specific methods, such as\nSimCLR, to improve self-supervised visual representation learning. Finally, we\ntheoretically analyze our method and show advantages over the Gaussian-noise\nbased contrastive learning approach.",
          "link": "http://arxiv.org/abs/2011.04419",
          "publishedOn": "2021-07-21T02:01:37.638Z",
          "wordCount": 624,
          "title": "Towards Domain-Agnostic Contrastive Learning. (arXiv:2011.04419v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09519",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gaetan_F/0/1/0/all/0/1\">Frusque Gaetan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gabriel_M/0/1/0/all/0/1\">Michau Gabriel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Olga_F/0/1/0/all/0/1\">Fink Olga</a>",
          "description": "Acoustic monitoring for machine fault detection is a recent and expanding\nresearch path that has already provided promising results for industries.\nHowever, it is impossible to collect enough data to learn all types of faults\nfrom a machine. Thus, new algorithms, trained using data from healthy\nconditions only, were developed to perform unsupervised anomaly detection. A\nkey issue in the development of these algorithms is the noise in the signals,\nas it impacts the anomaly detection performance. In this work, we propose a\npowerful data-driven and quasi non-parametric denoising strategy for spectral\ndata based on a tensor decomposition: the Non-negative Canonical Polyadic (CP)\ndecomposition. This method is particularly adapted for machine emitting\nstationary sound. We demonstrate in a case study, the Malfunctioning Industrial\nMachine Investigation and Inspection (MIMII) baseline, how the use of our\ndenoising strategy leads to a sensible improvement of the unsupervised anomaly\ndetection. Such approaches are capable to make sound-based monitoring of\nindustrial processes more reliable.",
          "link": "http://arxiv.org/abs/2107.09519",
          "publishedOn": "2021-07-21T02:01:37.625Z",
          "wordCount": 635,
          "title": "Canonical Polyadic Decomposition and Deep Learning for Machine Fault Detection. (arXiv:2107.09519v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1\">Christoph Reich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prangemeier_T/0/1/0/all/0/1\">Tim Prangemeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildner_C/0/1/0/all/0/1\">Christian Wildner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1\">Heinz Koeppl</a>",
          "description": "Time-lapse fluorescent microscopy (TLFM) combined with predictive\nmathematical modelling is a powerful tool to study the inherently dynamic\nprocesses of life on the single-cell level. Such experiments are costly,\ncomplex and labour intensive. A complimentary approach and a step towards in\nsilico experimentation, is to synthesise the imagery itself. Here, we propose\nMulti-StyleGAN as a descriptive approach to simulate time-lapse fluorescence\nmicroscopy imagery of living cells, based on a past experiment. This novel\ngenerative adversarial network synthesises a multi-domain sequence of\nconsecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple live\nyeast cells in microstructured environments and train on a dataset recorded in\nour laboratory. The simulation captures underlying biophysical factors and time\ndependencies, such as cell morphology, growth, physical interactions, as well\nas the intensity of a fluorescent reporter protein. An immediate application is\nto generate additional training and validation data for feature extraction\nalgorithms or to aid and expedite development of advanced experimental\ntechniques such as online monitoring or control of cells.\n\nCode and dataset is available at\nhttps://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.",
          "link": "http://arxiv.org/abs/2106.08285",
          "publishedOn": "2021-07-21T02:01:37.619Z",
          "wordCount": 674,
          "title": "Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell Microscopy. (arXiv:2106.08285v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.05041",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tran_G/0/1/0/all/0/1\">Gia-Lac Tran</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Milios_D/0/1/0/all/0/1\">Dimitrios Milios</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Michiardi_P/0/1/0/all/0/1\">Pietro Michiardi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Filippone_M/0/1/0/all/0/1\">Maurizio Filippone</a>",
          "description": "Approximations to Gaussian processes based on inducing variables, combined\nwith variational inference techniques, enable state-of-the-art sparse\napproaches to infer GPs at scale through mini batch-based learning. In this\nwork, we address one limitation of sparse GPs, which is due to the challenge in\ndealing with a large number of inducing variables without imposing a special\nstructure on the inducing inputs. In particular, we introduce a novel\nhierarchical prior, which imposes sparsity on the set of inducing variables. We\ntreat our model variationally, and we experimentally show considerable\ncomputational gains compared to standard sparse GPs when sparsity on the\ninducing variables is realized considering the nearest inducing inputs of a\nrandom mini-batch of the data. We perform an extensive experimental validation\nthat demonstrates the effectiveness of our approach compared to the\nstate-of-the-art. Our approach enables the possibility to use sparse GPs using\na large number of inducing points without incurring a prohibitive computational\ncost.",
          "link": "http://arxiv.org/abs/2011.05041",
          "publishedOn": "2021-07-21T02:01:37.612Z",
          "wordCount": 615,
          "title": "Sparse within Sparse Gaussian Processes using Neighbor Information. (arXiv:2011.05041v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Sumon Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_H/0/1/0/all/0/1\">Hridesh Rajan</a>",
          "description": "In recent years, many incidents have been reported where machine learning\nmodels exhibited discrimination among people based on race, sex, age, etc.\nResearch has been conducted to measure and mitigate unfairness in machine\nlearning models. For a machine learning task, it is a common practice to build\na pipeline that includes an ordered set of data preprocessing stages followed\nby a classifier. However, most of the research on fairness has considered a\nsingle classifier based prediction task. What are the fairness impacts of the\npreprocessing stages in machine learning pipeline? Furthermore, studies showed\nthat often the root cause of unfairness is ingrained in the data itself, rather\nthan the model. But no research has been conducted to measure the unfairness\ncaused by a specific transformation made in the data preprocessing stage. In\nthis paper, we introduced the causal method of fairness to reason about the\nfairness impact of data preprocessing stages in ML pipeline. We leveraged\nexisting metrics to define the fairness measures of the stages. Then we\nconducted a detailed fairness evaluation of the preprocessing stages in 37\npipelines collected from three different sources. Our results show that certain\ndata transformers are causing the model to exhibit unfairness. We identified a\nnumber of fairness patterns in several categories of data transformers.\nFinally, we showed how the local fairness of a preprocessing stage composes in\nthe global fairness of the pipeline. We used the fairness composition to choose\nappropriate downstream transformer that mitigates unfairness in the machine\nlearning pipeline.",
          "link": "http://arxiv.org/abs/2106.06054",
          "publishedOn": "2021-07-21T02:01:37.283Z",
          "wordCount": 773,
          "title": "Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline. (arXiv:2106.06054v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yue Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qinghua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengqi Zhang</a>",
          "description": "The heterogeneity across devices usually hinders the optimization convergence\nand generalization performance of federated learning (FL) when the aggregation\nof devices' knowledge occurs in the gradient space. For example, devices may\ndiffer in terms of data distribution, network latency, input/output space,\nand/or model architecture, which can easily lead to the misalignment of their\nlocal gradients. To improve the tolerance to heterogeneity, we propose a novel\nfederated prototype learning (FedProto) framework in which the devices and\nserver communicate the class prototypes instead of the gradients. FedProto\naggregates the local prototypes collected from different devices, and then\nsends the global prototypes back to all devices to regularize the training of\nlocal models. The training on each device aims to minimize the classification\nerror on the local data while keeping the resulting local prototypes\nsufficiently close to the corresponding global ones. Through experiments, we\npropose a benchmark setting tailored for heterogeneous FL, with FedProto\noutperforming several recent FL approaches on multiple datasets.",
          "link": "http://arxiv.org/abs/2105.00243",
          "publishedOn": "2021-07-21T02:01:37.276Z",
          "wordCount": 629,
          "title": "FedProto: Federated Prototype Learning over Heterogeneous Devices. (arXiv:2105.00243v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11289",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hawks_B/0/1/0/all/0/1\">Benjamin Hawks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duarte_J/0/1/0/all/0/1\">Javier Duarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_N/0/1/0/all/0/1\">Nicholas J. Fraser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappalardo_A/0/1/0/all/0/1\">Alessandro Pappalardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1\">Nhan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umuroglu_Y/0/1/0/all/0/1\">Yaman Umuroglu</a>",
          "description": "Efficient machine learning implementations optimized for inference in\nhardware have wide-ranging benefits, depending on the application, from lower\ninference latency to higher data throughput and reduced energy consumption. Two\npopular techniques for reducing computation in neural networks are pruning,\nremoving insignificant synapses, and quantization, reducing the precision of\nthe calculations. In this work, we explore the interplay between pruning and\nquantization during the training of neural networks for ultra low latency\napplications targeting high energy physics use cases. Techniques developed for\nthis study have potential applications across many other domains. We study\nvarious configurations of pruning during quantization-aware training, which we\nterm quantization-aware pruning, and the effect of techniques like\nregularization, batch normalization, and different pruning schemes on\nperformance, computational complexity, and information content metrics. We find\nthat quantization-aware pruning yields more computationally efficient models\nthan either pruning or quantization alone for our task. Further,\nquantization-aware pruning typically performs similar to or better in terms of\ncomputational efficiency compared to other neural architecture search\ntechniques like Bayesian optimization. Surprisingly, while networks with\ndifferent training configurations can have similar performance for the\nbenchmark application, the information content in the network can vary\nsignificantly, affecting its generalizability.",
          "link": "http://arxiv.org/abs/2102.11289",
          "publishedOn": "2021-07-21T02:01:37.257Z",
          "wordCount": 709,
          "title": "Ps and Qs: Quantization-aware pruning for efficient low latency neural network inference. (arXiv:2102.11289v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yarats_D/0/1/0/all/0/1\">Denis Yarats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1\">Rob Fergus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1\">Alessandro Lazaric</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_L/0/1/0/all/0/1\">Lerrel Pinto</a>",
          "description": "Learning effective representations in image-based environments is crucial for\nsample efficient Reinforcement Learning (RL). Unfortunately, in RL,\nrepresentation learning is confounded with the exploratory experience of the\nagent -- learning a useful representation requires diverse data, while\neffective exploration is only possible with coherent representations.\nFurthermore, we would like to learn representations that not only generalize\nacross tasks but also accelerate downstream exploration for efficient\ntask-specific training. To address these challenges we propose Proto-RL, a\nself-supervised framework that ties representation learning with exploration\nthrough prototypical representations. These prototypes simultaneously serve as\na summarization of the exploratory experience of an agent as well as a basis\nfor representing observations. We pre-train these task-agnostic representations\nand prototypes on environments without downstream task information. This\nenables state-of-the-art downstream policy learning on a set of difficult\ncontinuous control tasks.",
          "link": "http://arxiv.org/abs/2102.11271",
          "publishedOn": "2021-07-21T02:01:37.250Z",
          "wordCount": 596,
          "title": "Reinforcement Learning with Prototypical Representations. (arXiv:2102.11271v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.04125",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chepurko_N/0/1/0/all/0/1\">Nadiia Chepurko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarkson_K/0/1/0/all/0/1\">Kenneth L. Clarkson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horesh_L/0/1/0/all/0/1\">Lior Horesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1\">David P. Woodruff</a>",
          "description": "We create classical (non-quantum) dynamic data structures supporting queries\nfor recommender systems and least-squares regression that are comparable to\ntheir quantum analogues. De-quantizing such algorithms has received a flurry of\nattention in recent years; we obtain sharper bounds for these problems. More\nsignificantly, we achieve these improvements by arguing that the previous\nquantum-inspired algorithms for these problems are doing leverage or\nridge-leverage score sampling in disguise; these are powerful and standard\ntechniques in randomized numerical linear algebra. With this recognition, we\nare able to employ the large body of work in numerical linear algebra to obtain\nalgorithms for these problems that are simpler or faster (or both) than\nexisting approaches.",
          "link": "http://arxiv.org/abs/2011.04125",
          "publishedOn": "2021-07-21T02:01:37.244Z",
          "wordCount": 639,
          "title": "Quantum-Inspired Algorithms from Randomized Numerical Linear Algebra. (arXiv:2011.04125v5 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caballero_M/0/1/0/all/0/1\">Michael Caballero</a>",
          "description": "One major sub-domain in the subject of polling public opinion with social\nmedia data is electoral prediction. Electoral prediction utilizing social media\ndata potentially would significantly affect campaign strategies, complementing\ntraditional polling methods and providing cheaper polling in real-time. First,\nthis paper explores past successful methods from research for analysis and\nprediction of the 2020 US Presidential Election using Twitter data. Then, this\nresearch proposes a new method for electoral prediction which combines\nsentiment, from NLP on the text of tweets, and structural data with aggregate\npolling, a time series analysis, and a special focus on Twitter users critical\nto the election. Though this method performed worse than its baseline of\npolling predictions, it is inconclusive whether this is an accurate method for\npredicting elections due to scarcity of data. More research and more data are\nneeded to accurately measure this method's overall effectiveness.",
          "link": "http://arxiv.org/abs/2107.09640",
          "publishedOn": "2021-07-21T02:01:37.237Z",
          "wordCount": 592,
          "title": "Predicting the 2020 US Presidential Election with Twitter. (arXiv:2107.09640v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weize Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hexu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Hyperbolic neural networks have shown great potential for modeling complex\ndata. However, existing hyperbolic networks are not completely hyperbolic, as\nthey encode features in a hyperbolic space yet formalize most of their\noperations in the tangent space (a Euclidean subspace) at the origin of the\nhyperbolic space. This hybrid method greatly limits the modeling ability of\nnetworks. In this paper, we propose a fully hyperbolic framework to build\nhyperbolic networks based on the Lorentz model by adapting the Lorentz\ntransformations (including boost and rotation) to formalize essential\noperations of neural networks. Moreover, we also prove that linear\ntransformation in tangent spaces used by existing hyperbolic networks is a\nrelaxation of the Lorentz rotation and does not include the boost, implicitly\nlimiting the capabilities of existing hyperbolic networks. The experimental\nresults on four NLP tasks show that our method has better performance for\nbuilding both shallow and deep networks. Our code will be released to\nfacilitate follow-up research.",
          "link": "http://arxiv.org/abs/2105.14686",
          "publishedOn": "2021-07-21T02:01:37.231Z",
          "wordCount": 624,
          "title": "Fully Hyperbolic Neural Networks. (arXiv:2105.14686v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_J/0/1/0/all/0/1\">John Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1\">Kshitiz Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Hongyuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousefpour_A/0/1/0/all/0/1\">Ashkan Yousefpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1\">Michael Rabbat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malek_M/0/1/0/all/0/1\">Mani Malek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huba_D/0/1/0/all/0/1\">Dzmitry Huba</a>",
          "description": "Federated Learning (FL) trains a shared model across distributed devices\nwhile keeping the training data on the devices. Most FL schemes are\nsynchronous: they perform a synchronized aggregation of model updates from\nindividual devices. Synchronous training can be slow because of late-arriving\ndevices (stragglers). On the other hand, completely asynchronous training makes\nFL less private because of incompatibility with secure aggregation. In this\nwork, we propose a model aggregation scheme, FedBuff, that combines the best\nproperties of synchronous and asynchronous FL. Similar to synchronous FL,\nFedBuff is compatible with secure aggregation. Similar to asynchronous FL,\nFedBuff is robust to stragglers. In FedBuff, clients trains asynchronously and\nsend updates to the server. The server aggregates client updates in a private\nbuffer until updates have been received, at which point a server model update\nis immediately performed. We provide theoretical convergence guarantees for\nFedBuff in a non-convex setting. Empirically, FedBuff converges up to 3.8x\nfaster than previous proposals for synchronous FL (e.g., FedAvgM), and up to\n2.5x faster than previous proposals for asynchronous FL (e.g., FedAsync). We\nshow that FedBuff is robust to different staleness distributions and is more\nscalable than synchronous FL techniques.",
          "link": "http://arxiv.org/abs/2106.06639",
          "publishedOn": "2021-07-21T02:01:37.215Z",
          "wordCount": 653,
          "title": "Federated Learning with Buffered Asynchronous Aggregation. (arXiv:2106.06639v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1\">Stephen Roller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1\">Sainbayar Sukhbaatar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>",
          "description": "We investigate the training of sparse layers that use different parameters\nfor different inputs based on hashing in large Transformer models.\nSpecifically, we modify the feedforward layer to hash to different sets of\nweights depending on the current token, over all tokens in the sequence. We\nshow that this procedure either outperforms or is competitive with\nlearning-to-route mixture-of-expert methods such as Switch Transformers and\nBASE Layers, while requiring no routing parameters or extra terms in the\nobjective function such as a load balancing loss, and no sophisticated\nassignment algorithm. We study the performance of different hashing techniques,\nhash sizes and input features, and show that balanced and random hashes focused\non the most local features work best, compared to either learning clusters or\nusing longer-range context. We show our approach works well both on large\nlanguage modeling and dialogue tasks, and on downstream fine-tuning tasks.",
          "link": "http://arxiv.org/abs/2106.04426",
          "publishedOn": "2021-07-21T02:01:37.209Z",
          "wordCount": 614,
          "title": "Hash Layers For Large Sparse Models. (arXiv:2106.04426v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarafanov_M/0/1/0/all/0/1\">Mikhail Sarafanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikitin_N/0/1/0/all/0/1\">Nikolay O. Nikitin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyuzhnaya_A/0/1/0/all/0/1\">Anna V. Kalyuzhnaya</a>",
          "description": "In the paper, we propose an adaptive data-driven model-based approach for\nfilling the gaps in time series. The approach is based on the automated\nevolutionary identification of the optimal structure for a composite\ndata-driven model. It allows adapting the model for the effective gap-filling\nin a specific dataset without the involvement of the data scientist. As a case\nstudy, both synthetic and real datasets from different fields (environmental,\neconomic, etc) are used. The experiments confirm that the proposed approach\nallows achieving the higher quality of the gap restoration and improve the\neffectiveness of forecasting models.",
          "link": "http://arxiv.org/abs/2103.01124",
          "publishedOn": "2021-07-21T02:01:37.203Z",
          "wordCount": 584,
          "title": "Automated data-driven approach for gap filling in the time series using evolutionary learning. (arXiv:2103.01124v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reyes_J/0/1/0/all/0/1\">Jonatan Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorio_L/0/1/0/all/0/1\">Lisa Di Jorio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Low_Kam_C/0/1/0/all/0/1\">Cecile Low-Kam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersten_Oertel_M/0/1/0/all/0/1\">Marta Kersten-Oertel</a>",
          "description": "Federated Learning using the Federated Averaging algorithm has shown great\nadvantages for large-scale applications that rely on collaborative learning,\nespecially when the training data is either unbalanced or inaccessible due to\nprivacy constraints. We hypothesize that Federated Averaging underestimates the\nfull extent of heterogeneity of data when the aggregation is performed. We\npropose Precision-weighted Federated Learning a novel algorithm that takes into\naccount the variance of the stochastic gradients when computing the weighted\naverage of the parameters of models trained in a Federated Learning setting.\nWith Precision-weighted Federated Learning, we provide an alternate averaging\nscheme that leverages the heterogeneity of the data when it has a large\ndiversity of features in its composition. Our method was evaluated using\nstandard image classification datasets with two different data partitioning\nstrategies (IID/non-IID) to measure the performance and speed of our method in\nresource-constrained environments, such as mobile and IoT devices. We obtained\na good balance between computational efficiency and convergence rates with\nPrecision-weighted Federated Learning. Our performance evaluations show 9%\nbetter predictions with MNIST, 18% with Fashion-MNIST, and 5% with CIFAR-10 in\nthe non-IID setting. Further reliability evaluations ratify the stability in\nour method by reaching a 99% reliability index with IID partitions and 96% with\nnon-IID partitions. In addition, we obtained a 20x speedup on Fashion-MNIST\nwith only 10 clients and up to 37x with 100 clients participating in the\naggregation concurrently per communication round. The results indicate that\nPrecision-weighted Federated Learning is an effective and faster alternative\napproach for aggregating private data, especially in domains where data is\nhighly heterogeneous.",
          "link": "http://arxiv.org/abs/2107.09627",
          "publishedOn": "2021-07-21T02:01:37.196Z",
          "wordCount": 688,
          "title": "Precision-Weighted Federated Learning. (arXiv:2107.09627v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.00304",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Unke_O/0/1/0/all/0/1\">Oliver T. Unke</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chmiela_S/0/1/0/all/0/1\">Stefan Chmiela</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gastegger_M/0/1/0/all/0/1\">Michael Gastegger</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schutt_K/0/1/0/all/0/1\">Kristof T. Sch&#xfc;tt</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sauceda_H/0/1/0/all/0/1\">Huziel E. Sauceda</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Muller_K/0/1/0/all/0/1\">Klaus-Robert M&#xfc;ller</a>",
          "description": "Machine-learned force fields (ML-FFs) combine the accuracy of ab initio\nmethods with the efficiency of conventional force fields. However, current\nML-FFs typically ignore electronic degrees of freedom, such as the total charge\nor spin state, and assume chemical locality, which is problematic when\nmolecules have inconsistent electronic states, or when nonlocal effects play a\nsignificant role. This work introduces SpookyNet, a deep neural network for\nconstructing ML-FFs with explicit treatment of electronic degrees of freedom\nand quantum nonlocality. Chemically meaningful inductive biases and analytical\ncorrections built into the network architecture allow it to properly model\nphysical limits. SpookyNet improves upon the current state-of-the-art (or\nachieves similar performance) on popular quantum chemistry data sets. Notably,\nit is able to generalize across chemical and conformational space and can\nleverage the learned chemical insights, e.g. by predicting unknown spin states,\nthus helping to close a further important remaining gap for today's machine\nlearning models in quantum chemistry.",
          "link": "http://arxiv.org/abs/2105.00304",
          "publishedOn": "2021-07-21T02:01:37.189Z",
          "wordCount": 621,
          "title": "SpookyNet: Learning Force Fields with Electronic Degrees of Freedom and Nonlocal Effects. (arXiv:2105.00304v2 [physics.chem-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liangxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guo-Jun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Federated learning (FL) allows multiple clients to collaboratively learn a\nglobally shared model through cycles of model aggregation and local model\ntraining, without the need to share data. Most existing FL methods train local\nmodels separately on different clients, and then simply average their\nparameters to obtain a centralized model on the server side. However, these\napproaches generally suffer from large aggregation errors and severe local\nforgetting, which are particularly bad in heterogeneous data settings. To\ntackle these issues, in this paper, we propose a novel FL framework that uses\nonline Laplace approximation to approximate posteriors on both the client and\nserver side. On the server side, a multivariate Gaussian product mechanism is\nemployed to construct and maximize a global posterior, largely reducing the\naggregation errors induced by large discrepancies between local models. On the\nclient side, a prior loss that uses the global posterior probabilistic\nparameters delivered from the server is designed to guide the local training.\nBinding such learning constraints from other clients enables our method to\nmitigate local forgetting. Finally, we achieve state-of-the-art results on\nseveral benchmarks, clearly demonstrating the advantages of the proposed\nmethod.",
          "link": "http://arxiv.org/abs/2102.01936",
          "publishedOn": "2021-07-21T02:01:37.170Z",
          "wordCount": 660,
          "title": "A Bayesian Federated Learning Framework with Online Laplace Approximation. (arXiv:2102.01936v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Manish Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riek_L/0/1/0/all/0/1\">Laurel D. Riek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1\">Kamalika Chaudhuri</a>",
          "description": "In many real-world applications, multiple agents seek to learn how to perform\nhighly related yet slightly different tasks in an online bandit learning\nprotocol. We formulate this problem as the $\\epsilon$-multi-player multi-armed\nbandit problem, in which a set of players concurrently interact with a set of\narms, and for each arm, the reward distributions for all players are similar\nbut not necessarily identical. We develop an upper confidence bound-based\nalgorithm, RobustAgg$(\\epsilon)$, that adaptively aggregates rewards collected\nby different players. In the setting where an upper bound on the pairwise\nsimilarities of reward distributions between players is known, we achieve\ninstance-dependent regret guarantees that depend on the amenability of\ninformation sharing across players. We complement these upper bounds with\nnearly matching lower bounds. In the setting where pairwise similarities are\nunknown, we provide a lower bound, as well as an algorithm that trades off\nminimax regret guarantees for adaptivity to unknown similarity structure.",
          "link": "http://arxiv.org/abs/2010.15390",
          "publishedOn": "2021-07-21T02:01:37.164Z",
          "wordCount": 634,
          "title": "Multitask Bandit Learning Through Heterogeneous Feedback Aggregation. (arXiv:2010.15390v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sreenivasaiah_D/0/1/0/all/0/1\">Deepthi Sreenivasaiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otterbach_J/0/1/0/all/0/1\">Johannes Otterbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wollmann_T/0/1/0/all/0/1\">Thomas Wollmann</a>",
          "description": "Image segmentation is a common and challenging task in autonomous driving.\nAvailability of sufficient pixel-level annotations for the training data is a\nhurdle. Active learning helps learning from small amounts of data by suggesting\nthe most promising samples for labeling. In this work, we propose a new\npool-based method for active learning, which proposes promising patches\nextracted from full image, in each acquisition step. The problem is framed in\nan exploration-exploitation framework by combining an embedding based on\nUniform Manifold Approximation to model representativeness with entropy as\nuncertainty measure to model informativeness. We applied our proposed method to\nthe autonomous driving datasets CamVid and Cityscapes and performed a\nquantitative comparison with state-of-the-art baselines. We find that our\nactive learning method achieves better performance compared to previous\nmethods.",
          "link": "http://arxiv.org/abs/2106.11858",
          "publishedOn": "2021-07-21T02:01:37.157Z",
          "wordCount": 584,
          "title": "MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.09176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimizu_S/0/1/0/all/0/1\">Shohei Shimizu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Ruichu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1\">Feng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_M/0/1/0/all/0/1\">Michio Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1\">Zhifeng Hao</a>",
          "description": "Discovering causal structures among latent factors from observed data is a\nparticularly challenging problem. Despite some efforts for this problem,\nexisting methods focus on the single-domain data only. In this paper, we\npropose Multi-Domain Linear Non-Gaussian Acyclic Models for Latent Factors\n(MD-LiNA), where the causal structure among latent factors of interest is\nshared for all domains, and we provide its identification results. The model\nenriches the causal representation for multi-domain data. We propose an\nintegrated two-phase algorithm to estimate the model. In particular, we first\nlocate the latent factors and estimate the factor loading matrix. Then to\nuncover the causal structure among shared latent factors of interest, we derive\na score function based on the characterization of independence relations\nbetween external influences and the dependence relations between multi-domain\nlatent factors and latent factors of interest. We show that the proposed method\nprovides locally consistent estimators. Experimental results on both synthetic\nand real-world data demonstrate the efficacy and robustness of our approach.",
          "link": "http://arxiv.org/abs/2009.09176",
          "publishedOn": "2021-07-21T02:01:37.150Z",
          "wordCount": 635,
          "title": "Causal Discovery with Multi-Domain LiNGAM for Latent Factors. (arXiv:2009.09176v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01188",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Arefeen_Y/0/1/0/all/0/1\">Yamin Arefeen</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Beker_O/0/1/0/all/0/1\">Onur Beker</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Cho_J/0/1/0/all/0/1\">Jaejin Cho</a> (3), <a href=\"http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1\">Heng Yu</a> (4), <a href=\"http://arxiv.org/find/eess/1/au:+Adalsteinsson_E/0/1/0/all/0/1\">Elfar Adalsteinsson</a> (1 and 5 and 6), <a href=\"http://arxiv.org/find/eess/1/au:+Bilgic_B/0/1/0/all/0/1\">Berkin Bilgic</a> (3 and 5 and 7) ((1) Massachusetts Institute of Technology, (2) &#xc9;cole Polytechnique F&#xe9;d&#xe9;rale de Lausanne, (3) Athinoula A. Martinos Center for Biomedical Imaging (4) Tsinghua University, (5) Harvard-MIT Health Sciences and Technology, (6) Institute for Medical Engineering and Science, (7) Harvard Medical School)",
          "description": "Purpose: To develop a scan-specific model that estimates and corrects k-space\nerrors made when reconstructing accelerated Magnetic Resonance Imaging (MRI)\ndata.\n\nMethods: Scan-Specific Artifact Reduction in k-space (SPARK) trains a\nconvolutional-neural-network to estimate and correct k-space errors made by an\ninput reconstruction technique by back-propagating from the mean-squared-error\nloss between an auto-calibration signal (ACS) and the input technique's\nreconstructed ACS. First, SPARK is applied to GRAPPA and demonstrates improved\nrobustness over other scan-specific models, such as RAKI and residual-RAKI.\nSubsequent experiments demonstrate that SPARK synergizes with residual-RAKI to\nimprove reconstruction performance. SPARK also improves reconstruction quality\nwhen applied to advanced acquisition and reconstruction techniques like 2D\nvirtual coil (VC-) GRAPPA, 2D LORAKS, 3D GRAPPA without an integrated ACS\nregion, and 2D/3D wave-encoded images.\n\nResults: SPARK yields 1.5x - 2x RMSE reduction when applied to GRAPPA and\nimproves robustness to ACS size for various acceleration rates in comparison to\nother scan-specific techniques. When applied to advanced reconstruction\ntechniques such as residual-RAKI, 2D VC-GRAPPA and LORAKS, SPARK achieves up to\n20% RMSE improvement. SPARK with 3D GRAPPA also improves performance by ~2x and\nperceived image quality without a fully sampled ACS region. Finally, SPARK\nsynergizes with non-cartesian 2D and 3D wave-encoding imaging by reducing RMSE\nbetween 20-25% and providing qualitative improvements.\n\nConclusion: SPARK synergizes with physics-based acquisition and\nreconstruction techniques to improve accelerated MRI by training scan-specific\nmodels to estimate and correct reconstruction errors in k-space.",
          "link": "http://arxiv.org/abs/2104.01188",
          "publishedOn": "2021-07-21T02:01:37.134Z",
          "wordCount": 766,
          "title": "Scan Specific Artifact Reduction in K-space (SPARK) Neural Networks Synergize with Physics-based Reconstruction to Accelerate MRI. (arXiv:2104.01188v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04656",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gupta_C/0/1/0/all/0/1\">Chirag Gupta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1\">Aaditya K. Ramdas</a>",
          "description": "We prove calibration guarantees for the popular histogram binning (also\ncalled uniform-mass binning) method of Zadrozny and Elkan [2001]. Histogram\nbinning has displayed strong practical performance, but theoretical guarantees\nhave only been shown for sample split versions that avoid 'double dipping' the\ndata. We demonstrate that the statistical cost of sample splitting is\npractically significant on a credit default dataset. We then prove calibration\nguarantees for the original method that double dips the data, using a certain\nMarkov property of order statistics. Based on our results, we make practical\nrecommendations for choosing the number of bins in histogram binning. In our\nillustrative simulations, we propose a new tool for assessing calibration --\nvalidity plots -- which provide more information than an ECE estimate. Code for\nthis work will be made publicly available at\nhttps://github.com/aigen/df-posthoc-calibration.",
          "link": "http://arxiv.org/abs/2105.04656",
          "publishedOn": "2021-07-21T02:01:37.126Z",
          "wordCount": 594,
          "title": "Distribution-free calibration guarantees for histogram binning without sample splitting. (arXiv:2105.04656v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1\">Spencer Frei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "Although the optimization objectives for learning neural networks are highly\nnon-convex, gradient-based methods have been wildly successful at learning\nneural networks in practice. This juxtaposition has led to a number of recent\nstudies on provable guarantees for neural networks trained by gradient descent.\nUnfortunately, the techniques in these works are often highly specific to the\nproblem studied in each setting, relying on different assumptions on the\ndistribution, optimization parameters, and network architectures, making it\ndifficult to generalize across different settings. In this work, we propose a\nunified non-convex optimization framework for the analysis of neural network\ntraining. We introduce the notions of proxy convexity and proxy\nPolyak-Lojasiewicz (PL) inequalities, which are satisfied if the original\nobjective function induces a proxy objective function that is implicitly\nminimized when using gradient methods. We show that stochastic gradient descent\n(SGD) on objectives satisfying proxy convexity or the proxy PL inequality leads\nto efficient guarantees for proxy objective functions. We further show that\nmany existing guarantees for neural networks trained by gradient descent can be\nunified through proxy convexity and proxy PL inequalities.",
          "link": "http://arxiv.org/abs/2106.13792",
          "publishedOn": "2021-07-21T02:01:37.120Z",
          "wordCount": 659,
          "title": "Proxy Convexity: A Unified Framework for the Analysis of Neural Networks Trained by Gradient Descent. (arXiv:2106.13792v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tempelmeier_N/0/1/0/all/0/1\">Nicolas Tempelmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feuerhake_U/0/1/0/all/0/1\">Udo Feuerhake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wage_O/0/1/0/all/0/1\">Oskar Wage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demidova_E/0/1/0/all/0/1\">Elena Demidova</a>",
          "description": "The discovery of spatio-temporal dependencies within urban road networks that\ncause Recurrent Congestion (RC) patterns is crucial for numerous real-world\napplications, including urban planning and scheduling of public transportation\nservices. While most existing studies investigate temporal patterns of RC\nphenomena, the influence of the road network topology on RC is often\noverlooked. This article proposes the ST-Discovery algorithm, a novel\nunsupervised spatio-temporal data mining algorithm that facilitates the\neffective data-driven discovery of RC dependencies induced by the road network\ntopology using real-world traffic data. We factor out regularly reoccurring\ntraffic phenomena, such as rush hours, mainly induced by the daytime, by\nmodelling and systematically exploiting temporal traffic load outliers. We\npresent an algorithm that first constructs connected subgraphs of the road\nnetwork based on the traffic speed outliers. Second, the algorithm identifies\npairs of subgraphs that indicate spatio-temporal correlations in their traffic\nload behaviour to identify topological dependencies within the road network.\nFinally, we rank the identified subgraph pairs based on the dependency score\ndetermined by our algorithm. Our experimental results demonstrate that\nST-Discovery can effectively reveal topological dependencies in urban road\nnetworks.",
          "link": "http://arxiv.org/abs/2107.09554",
          "publishedOn": "2021-07-21T02:01:37.112Z",
          "wordCount": 630,
          "title": "Mining Topological Dependencies of Recurrent Congestion in Road Networks. (arXiv:2107.09554v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04754",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Aizenbud_Y/0/1/0/all/0/1\">Yariv Aizenbud</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sober_B/0/1/0/all/0/1\">Barak Sober</a>",
          "description": "A common observation in data-driven applications is that high dimensional\ndata has a low intrinsic dimension, at least locally. In this work, we consider\nthe problem of estimating a $d$ dimensional sub-manifold of $\\mathbb{R}^D$ from\na finite set of noisy samples. Assuming that the data was sampled uniformly\nfrom a tubular neighborhood of $\\mathcal{M}\\in \\mathcal{C}^k$, a compact\nmanifold without boundary, we present an algorithm that takes a point $r$ from\nthe tubular neighborhood and outputs $\\hat p_n\\in \\mathbb{R}^D$, and\n$\\widehat{T_{\\hat p_n}\\mathcal{M}}$ an element in the Grassmanian $Gr(d, D)$.\nWe prove that as the number of samples $n\\to\\infty$ the point $\\hat p_n$\nconverges to $p\\in \\mathcal{M}$ and $\\widehat{T_{\\hat p_n}\\mathcal{M}}$\nconverges to $T_p\\mathcal{M}$ (the tangent space at that point) with high\nprobability. Furthermore, we show that the estimation yields asymptotic rates\nof convergence of $n^{-\\frac{k}{2k + d}}$ for the point estimation and\n$n^{-\\frac{k-1}{2k + d}}$ for the estimation of the tangent space. These rates\nare known to be optimal for the case of function estimation.",
          "link": "http://arxiv.org/abs/2105.04754",
          "publishedOn": "2021-07-21T02:01:37.106Z",
          "wordCount": 610,
          "title": "Non-Parametric Estimation of Manifolds from Noisy Data. (arXiv:2105.04754v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09602",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bharati_S/0/1/0/all/0/1\">Subrato Bharati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Podder_P/0/1/0/all/0/1\">Prajoy Podder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mondal_M/0/1/0/all/0/1\">M. Rubaiyat Hossain Mondal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasath_V/0/1/0/all/0/1\">V.B. Surya Prasath</a>",
          "description": "The outbreak of novel coronavirus disease (COVID- 19) has claimed millions of\nlives and has affected all aspects of human life. This paper focuses on the\napplication of deep learning (DL) models to medical imaging and drug discovery\nfor managing COVID-19 disease. In this article, we detail various medical\nimaging-based studies such as X-rays and computed tomography (CT) images along\nwith DL methods for classifying COVID-19 affected versus pneumonia. The\napplications of DL techniques to medical images are further described in terms\nof image localization, segmentation, registration, and classification leading\nto COVID-19 detection. The reviews of recent papers indicate that the highest\nclassification accuracy of 99.80% is obtained when InstaCovNet-19 DL method is\napplied to an X-ray dataset of 361 COVID-19 patients, 362 pneumonia patients\nand 365 normal people. Furthermore, it can be seen that the best classification\naccuracy of 99.054% can be achieved when EDL_COVID DL method is applied to a CT\nimage dataset of 7500 samples where COVID-19 patients, lung tumor patients and\nnormal people are equal in number. Moreover, we illustrate the potential DL\ntechniques in drug or vaccine discovery in combating the coronavirus. Finally,\nwe address a number of problems, concerns and future research directions\nrelevant to DL applications for COVID-19.",
          "link": "http://arxiv.org/abs/2107.09602",
          "publishedOn": "2021-07-21T02:01:37.098Z",
          "wordCount": 729,
          "title": "Medical Imaging with Deep Learning for COVID- 19 Diagnosis: A Comprehensive Review. (arXiv:2107.09602v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00222",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Unlu_A/0/1/0/all/0/1\">Ali Unlu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1\">Laurence Aitchison</a>",
          "description": "We develop variational Laplace for Bayesian neural networks (BNNs) which\nexploits a local approximation of the curvature of the likelihood to estimate\nthe ELBO without the need for stochastic sampling of the neural-network\nweights. The Variational Laplace objective is simple to evaluate, as it is (in\nessence) the log-likelihood, plus weight-decay, plus a squared-gradient\nregularizer. Variational Laplace gave better test performance and expected\ncalibration errors than maximum a-posteriori inference and standard\nsampling-based variational inference, despite using the same variational\napproximate posterior. Finally, we emphasise care needed in benchmarking\nstandard VI as there is a risk of stopping before the variance parameters have\nconverged. We show that early-stopping can be avoided by increasing the\nlearning rate for the variance parameters.",
          "link": "http://arxiv.org/abs/2103.00222",
          "publishedOn": "2021-07-21T02:01:37.091Z",
          "wordCount": 583,
          "title": "Variational Laplace for Bayesian neural networks. (arXiv:2103.00222v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_U/0/1/0/all/0/1\">Uiwon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heeseung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1\">Dahuin Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Hyemi Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyungyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "Generative adversarial networks (GANs) with clustered latent spaces can\nperform conditional generation in a completely unsupervised manner. However,\nthe salient attributes of unlabeled data in the real-world are mostly\nimbalanced. Existing unsupervised conditional GANs cannot properly cluster the\nattributes in their latent spaces because they assume uniform distributions of\nthe attributes. To address this problem, we theoretically derive Stein latent\noptimization that provides reparameterizable gradient estimations of the latent\ndistribution parameters assuming a Gaussian mixture prior in a continuous\nlatent space. Structurally, we introduce an encoder network and a novel\ncontrastive loss to help generated data from a single mixture component to\nrepresent a single attribute. We confirm that the proposed method, named Stein\nLatent Optimization for GANs (SLOGAN), successfully learns the balanced or\nimbalanced attributes and performs unsupervised tasks such as unsupervised\nconditional generation, unconditional generation, and cluster assignment even\nin the absence of information of the attributes (e.g. the imbalance ratio).\nMoreover, we demonstrate that the attributes to be learned can be manipulated\nusing a small amount of probe data.",
          "link": "http://arxiv.org/abs/2106.05319",
          "publishedOn": "2021-07-21T02:01:37.085Z",
          "wordCount": 634,
          "title": "Stein Latent Optimization for GANs. (arXiv:2106.05319v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milbich_T/0/1/0/all/0/1\">Timo Milbich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1\">Karsten Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Samarth Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1\">Marzyeh Ghassemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>",
          "description": "Deep Metric Learning (DML) aims to find representations suitable for\nzero-shot transfer to a priori unknown test distributions. However, common\nevaluation protocols only test a single, fixed data split in which train and\ntest classes are assigned randomly. More realistic evaluations should consider\na broad spectrum of distribution shifts with potentially varying degree and\ndifficulty. In this work, we systematically construct train-test splits of\nincreasing difficulty and present the ooDML benchmark to characterize\ngeneralization under out-of-distribution shifts in DML. ooDML is designed to\nprobe the generalization performance on much more challenging, diverse\ntrain-to-test distribution shifts. Based on our new benchmark, we conduct a\nthorough empirical analysis of state-of-the-art DML methods. We find that while\ngeneralization tends to consistently degrade with difficulty, some methods are\nbetter at retaining performance as the distribution shift increases. Finally,\nwe propose few-shot DML as an efficient way to consistently improve\ngeneralization in response to unknown test shifts presented in ooDML. Code\navailable here:\nhttps://github.com/Confusezius/Characterizing_Generalization_in_DeepMetricLearning.",
          "link": "http://arxiv.org/abs/2107.09562",
          "publishedOn": "2021-07-21T02:01:37.052Z",
          "wordCount": 604,
          "title": "Characterizing Generalization under Out-Of-Distribution Shifts in Deep Metric Learning. (arXiv:2107.09562v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eiras_F/0/1/0/all/0/1\">Francisco Eiras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">M. Pawan Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1\">Puneet K. Dokania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1\">Adel Bibi</a>",
          "description": "Randomized smoothing has recently emerged as an effective tool that enables\ncertification of deep neural network classifiers at scale. All prior art on\nrandomized smoothing has focused on isotropic $\\ell_p$ certification, which has\nthe advantage of yielding certificates that can be easily compared among\nisotropic methods via $\\ell_p$-norm radius. However, isotropic certification\nlimits the region that can be certified around an input to worst-case\nadversaries, i.e., it cannot reason about other \"close\", potentially large,\nconstant prediction safe regions. To alleviate this issue, (i) we theoretically\nextend the isotropic randomized smoothing $\\ell_1$ and $\\ell_2$ certificates to\ntheir generalized anisotropic counterparts following a simplified analysis.\nMoreover, (ii) we propose evaluation metrics allowing for the comparison of\ngeneral certificates - a certificate is superior to another if it certifies a\nsuperset region - with the quantification of each certificate through the\nvolume of the certified region. We introduce ANCER, a practical framework for\nobtaining anisotropic certificates for a given test set sample via volume\nmaximization. Our empirical results demonstrate that ANCER achieves\nstate-of-the-art $\\ell_1$ and $\\ell_2$ certified accuracy on both CIFAR-10 and\nImageNet at multiple radii, while certifying substantially larger regions in\nterms of volume, thus highlighting the benefits of moving away from isotropic\nanalysis. Code used in our experiments is available in\nhttps://github.com/MotasemAlfarra/ANCER.",
          "link": "http://arxiv.org/abs/2107.04570",
          "publishedOn": "2021-07-21T02:01:37.040Z",
          "wordCount": 687,
          "title": "ANCER: Anisotropic Certification via Sample-wise Volume Maximization. (arXiv:2107.04570v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bernau_D/0/1/0/all/0/1\">Daniel Bernau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eibl_G/0/1/0/all/0/1\">G&#xfc;nther Eibl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grassal_P/0/1/0/all/0/1\">Philip W. Grassal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_H/0/1/0/all/0/1\">Hannah Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerschbaum_F/0/1/0/all/0/1\">Florian Kerschbaum</a>",
          "description": "Differential privacy allows bounding the influence that training data records\nhave on a machine learning model. To use differential privacy in machine\nlearning, data scientists must choose privacy parameters $(\\epsilon,\\delta)$.\nChoosing meaningful privacy parameters is key, since models trained with weak\nprivacy parameters might result in excessive privacy leakage, while strong\nprivacy parameters might overly degrade model utility. However, privacy\nparameter values are difficult to choose for two main reasons. First, the\ntheoretical upper bound on privacy loss $(\\epsilon,\\delta)$ might be loose,\ndepending on the chosen sensitivity and data distribution of practical\ndatasets. Second, legal requirements and societal norms for anonymization often\nrefer to individual identifiability, to which $(\\epsilon,\\delta)$ are only\nindirectly related.\n\nWe transform $(\\epsilon,\\delta)$ to a bound on the Bayesian posterior belief\nof the adversary assumed by differential privacy concerning the presence of any\nrecord in the training dataset. The bound holds for multidimensional queries\nunder composition, and we show that it can be tight in practice. Furthermore,\nwe derive an identifiability bound, which relates the adversary assumed in\ndifferential privacy to previous work on membership inference adversaries. We\nformulate an implementation of this differential privacy adversary that allows\ndata scientists to audit model training and compute empirical identifiability\nscores and empirical $(\\epsilon,\\delta)$.",
          "link": "http://arxiv.org/abs/2103.02913",
          "publishedOn": "2021-07-21T02:01:37.032Z",
          "wordCount": 690,
          "title": "Quantifying identifiability to choose and audit $\\epsilon$ in differentially private deep learning. (arXiv:2103.02913v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11203",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Tianle Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruiqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason D. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1\">Qi Lei</a>",
          "description": "One of the central problems in machine learning is domain adaptation. Unlike\npast theoretical work, we consider a new model for subpopulation shift in the\ninput or representation space. In this work, we propose a provably effective\nframework for domain adaptation based on label propagation. In our analysis, we\nuse a simple but realistic expansion assumption, proposed in\n\\citet{wei2021theoretical}. Using a teacher classifier trained on the source\ndomain, our algorithm not only propagates to the target domain but also\nimproves upon the teacher. By leveraging existing generalization bounds, we\nalso obtain end-to-end finite-sample guarantees on the entire algorithm. In\naddition, we extend our theoretical framework to a more general setting of\nsource-to-target transfer based on a third unlabeled dataset, which can be\neasily applied in various learning scenarios. Inspired by our theory, we adapt\nconsistency-based semi-supervised learning methods to domain adaptation\nsettings and gain significant improvements.",
          "link": "http://arxiv.org/abs/2102.11203",
          "publishedOn": "2021-07-21T02:01:37.021Z",
          "wordCount": 628,
          "title": "A Theory of Label Propagation for Subpopulation Shift. (arXiv:2102.11203v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Devos_A/0/1/0/all/0/1\">Arnout Devos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandi_Y/0/1/0/all/0/1\">Yatin Dandi</a>",
          "description": "In this paper, we propose a learning algorithm that enables a model to\nquickly exploit commonalities among related tasks from an unseen task\ndistribution, before quickly adapting to specific tasks from that same\ndistribution. We investigate how learning with different task distributions can\nfirst improve adaptability by meta-finetuning on related tasks before improving\ngoal task generalization with finetuning. Synthetic regression experiments\nvalidate the intuition that learning to meta-learn improves adaptability and\nconsecutively generalization. Experiments on more complex image classification,\ncontinual regression, and reinforcement learning tasks demonstrate that\nlearning to meta-learn generally improves task-specific adaptation. The\nmethodology, setup, and hypotheses in this proposal were positively evaluated\nby peer review before conclusive experiments were carried out.",
          "link": "http://arxiv.org/abs/2012.02684",
          "publishedOn": "2021-07-21T02:01:37.004Z",
          "wordCount": 577,
          "title": "Model-Agnostic Learning to Meta-Learn. (arXiv:2012.02684v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10586",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaidi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldhahn_R/0/1/0/all/0/1\">Ryan Goldhahn</a>",
          "description": "To tackle the susceptibility of deep neural networks to examples, the\nadversarial training has been proposed which provides a notion of robust\nthrough an inner maximization problem presenting the first-order embedded\nwithin the outer minimization of the training loss. To generalize the\nadversarial robustness over different perturbation types, the adversarial\ntraining method has been augmented with the improved inner maximization\npresenting a union of multiple perturbations e.g., various $\\ell_p$\nnorm-bounded perturbations.",
          "link": "http://arxiv.org/abs/2104.10586",
          "publishedOn": "2021-07-21T02:01:36.979Z",
          "wordCount": 591,
          "title": "Mixture of Robust Experts (MoRE):A Robust Denoising Method towards multiple perturbations. (arXiv:2104.10586v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.13677",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Wenqi Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ke_Z/0/1/0/all/0/1\">Ziwen Ke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_Z/0/1/0/all/0/1\">Zhuo-Xu Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1\">Jing Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhilang Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1\">Sen Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ying_L/0/1/0/all/0/1\">Leslie Ying</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanjie Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1\">Dong Liang</a>",
          "description": "In dynamic magnetic resonance (MR) imaging, low-rank plus sparse (L+S)\ndecomposition, or robust principal component analysis (PCA), has achieved\nstunning performance. However, the selection of the parameters of L+S is\nempirical, and the acceleration rate is limited, which are common failings of\niterative compressed sensing MR imaging (CS-MRI) reconstruction methods. Many\ndeep learning approaches have been proposed to address these issues, but few of\nthem use a low-rank prior. In this paper, a model-based low-rank plus sparse\nnetwork, dubbed L+S-Net, is proposed for dynamic MR reconstruction. In\nparticular, we use an alternating linearized minimization method to solve the\noptimization problem with low-rank and sparse regularization. Learned soft\nsingular value thresholding is introduced to ensure the clear separation of the\nL component and S component. Then, the iterative steps are unrolled into a\nnetwork in which the regularization parameters are learnable. We prove that the\nproposed L+S-Net achieves global convergence under two standard assumptions.\nExperiments on retrospective and prospective cardiac cine datasets show that\nthe proposed model outperforms state-of-the-art CS and existing deep learning\nmethods and has great potential for extremely high acceleration factors (up to\n24x).",
          "link": "http://arxiv.org/abs/2010.13677",
          "publishedOn": "2021-07-21T02:01:36.971Z",
          "wordCount": 677,
          "title": "Deep Low-rank plus Sparse Network for Dynamic MR Imaging. (arXiv:2010.13677v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05108",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1\">Jessica Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1\">Ognjen Arandjelovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1\">David J Harrison</a>",
          "description": "Understanding the predictions made by Artificial Intelligence (AI) systems is\nbecoming more and more important as deep learning models are used for\nincreasingly complex and high-stakes tasks. Saliency mapping - an easily\ninterpretable visual attribution method - is one important tool for this, but\nexisting formulations are limited by either computational cost or architectural\nconstraints. We therefore propose Hierarchical Perturbation, a very fast and\ncompletely model-agnostic method for explaining model predictions with robust\nsaliency maps. Using standard benchmarks and datasets, we show that our\nsaliency maps are of competitive or superior quality to those generated by\nexisting model-agnostic methods - and are over 20X faster to compute.",
          "link": "http://arxiv.org/abs/2103.05108",
          "publishedOn": "2021-07-21T02:01:36.929Z",
          "wordCount": 587,
          "title": "Believe The HiPe: Hierarchical Perturbation for Fast, Robust and Model-Agnostic Explanations. (arXiv:2103.05108v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yarats_D/0/1/0/all/0/1\">Denis Yarats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1\">Rob Fergus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1\">Alessandro Lazaric</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_L/0/1/0/all/0/1\">Lerrel Pinto</a>",
          "description": "We present DrQ-v2, a model-free reinforcement learning (RL) algorithm for\nvisual continuous control. DrQ-v2 builds on DrQ, an off-policy actor-critic\napproach that uses data augmentation to learn directly from pixels. We\nintroduce several improvements that yield state-of-the-art results on the\nDeepMind Control Suite. Notably, DrQ-v2 is able to solve complex humanoid\nlocomotion tasks directly from pixel observations, previously unattained by\nmodel-free RL. DrQ-v2 is conceptually simple, easy to implement, and provides\nsignificantly better computational footprint compared to prior work, with the\nmajority of tasks taking just 8 hours to train on a single GPU. Finally, we\npublicly release DrQ-v2's implementation to provide RL practitioners with a\nstrong and computationally efficient baseline.",
          "link": "http://arxiv.org/abs/2107.09645",
          "publishedOn": "2021-07-21T02:01:36.923Z",
          "wordCount": 545,
          "title": "Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning. (arXiv:2107.09645v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2103.10790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Katona_A/0/1/0/all/0/1\">Adam Katona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franks_D/0/1/0/all/0/1\">Daniel W. Franks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1\">James Alfred Walker</a>",
          "description": "One of the most important lessons from the success of deep learning is that\nlearned representations tend to perform much better at any task compared to\nrepresentations we design by hand. Yet evolution of evolvability algorithms,\nwhich aim to automatically learn good genetic representations, have received\nrelatively little attention, perhaps because of the large amount of\ncomputational power they require. The recent method Evolvability ES allows\ndirect selection for evolvability with little computation. However, it can only\nbe used to solve problems where evolvability and task performance are aligned.\nWe propose Quality Evolvability ES, a method that simultaneously optimizes for\ntask performance and evolvability and without this restriction. Our proposed\napproach Quality Evolvability has similar motivation to Quality Diversity\nalgorithms, but with some important differences. While Quality Diversity aims\nto find an archive of diverse and well-performing, but potentially genetically\ndistant individuals, Quality Evolvability aims to find a single individual with\na diverse and well-performing distribution of offspring. By doing so Quality\nEvolvability is forced to discover more evolvable representations. We\ndemonstrate on robotic locomotion control tasks that Quality Evolvability ES,\nsimilarly to Quality Diversity methods, can learn faster than objective-based\nmethods and can handle deceptive problems.",
          "link": "http://arxiv.org/abs/2103.10790",
          "publishedOn": "2021-07-21T02:01:36.890Z",
          "wordCount": 687,
          "title": "Quality Evolvability ES: Evolving Individuals With a Distribution of Well Performing and Diverse Offspring. (arXiv:2103.10790v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fisch_A/0/1/0/all/0/1\">Adam Fisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1\">Tommi Jaakkola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>",
          "description": "We develop a novel approach to conformal prediction when the target task has\nlimited data available for training. Conformal prediction identifies a small\nset of promising output candidates in place of a single prediction, with\nguarantees that the set contains the correct answer with high probability. When\ntraining data is limited, however, the predicted set can easily become unusably\nlarge. In this work, we obtain substantially tighter prediction sets while\nmaintaining desirable marginal guarantees by casting conformal prediction as a\nmeta-learning paradigm over exchangeable collections of auxiliary tasks. Our\nconformalization algorithm is simple, fast, and agnostic to the choice of\nunderlying model, learning algorithm, or dataset. We demonstrate the\neffectiveness of this approach across a number of few-shot classification and\nregression tasks in natural language processing, computer vision, and\ncomputational chemistry for drug discovery.",
          "link": "http://arxiv.org/abs/2102.08898",
          "publishedOn": "2021-07-21T02:01:36.863Z",
          "wordCount": 603,
          "title": "Few-shot Conformal Prediction with Auxiliary Tasks. (arXiv:2102.08898v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12922",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Turian_J/0/1/0/all/0/1\">Joseph Turian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shier_J/0/1/0/all/0/1\">Jordie Shier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzanetakis_G/0/1/0/all/0/1\">George Tzanetakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McNally_K/0/1/0/all/0/1\">Kirk McNally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_M/0/1/0/all/0/1\">Max Henry</a>",
          "description": "We release synth1B1, a multi-modal audio corpus consisting of 1 billion\n4-second synthesized sounds, paired with the synthesis parameters used to\ngenerate them. The dataset is 100x larger than any audio dataset in the\nliterature. We also introduce torchsynth, an open source modular synthesizer\nthat generates the synth1B1 samples on-the-fly at 16200x faster than real-time\n(714MHz) on a single GPU. Finally, we release two new audio datasets: FM synth\ntimbre and subtractive synth pitch. Using these datasets, we demonstrate new\nrank-based evaluation criteria for existing audio representations. Finally, we\npropose a novel approach to synthesizer hyperparameter optimization.",
          "link": "http://arxiv.org/abs/2104.12922",
          "publishedOn": "2021-07-21T02:01:36.832Z",
          "wordCount": 571,
          "title": "One Billion Audio Sounds from GPU-enabled Modular Synthesis. (arXiv:2104.12922v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.09067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Kensen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bieber_D/0/1/0/all/0/1\">David Bieber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1\">Charles Sutton</a>",
          "description": "Sampling is a fundamental technique, and sampling without replacement is\noften desirable when duplicate samples are not beneficial. Within machine\nlearning, sampling is useful for generating diverse outputs from a trained\nmodel. We present an elegant procedure for sampling without replacement from a\nbroad class of randomized programs, including generative neural models that\nconstruct outputs sequentially. Our procedure is efficient even for\nexponentially-large output spaces. Unlike prior work, our approach is\nincremental, i.e., samples can be drawn one at a time, allowing for increased\nflexibility. We also present a new estimator for computing expectations from\nsamples drawn without replacement. We show that incremental sampling without\nreplacement is applicable to many domains, e.g., program synthesis and\ncombinatorial optimization.",
          "link": "http://arxiv.org/abs/2002.09067",
          "publishedOn": "2021-07-21T02:01:36.825Z",
          "wordCount": 582,
          "title": "Incremental Sampling Without Replacement for Sequence Models. (arXiv:2002.09067v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09597",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hayakawa_S/0/1/0/all/0/1\">Satoshi Hayakawa</a>, <a href=\"http://arxiv.org/find/math/1/au:+Oberhauser_H/0/1/0/all/0/1\">Harald Oberhauser</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lyons_T/0/1/0/all/0/1\">Terry Lyons</a>",
          "description": "We study kernel quadrature rules with positive weights for probability\nmeasures on general domains. Our theoretical analysis combines the spectral\nproperties of the kernel with random sampling of points. This results in\neffective algorithms to construct kernel quadrature rules with positive weights\nand small worst-case error. Besides additional robustness, our numerical\nexperiments indicate that this can achieve fast convergence rates that compete\nwith the optimal bounds in well-known examples.",
          "link": "http://arxiv.org/abs/2107.09597",
          "publishedOn": "2021-07-21T02:01:36.719Z",
          "wordCount": 506,
          "title": "Positively Weighted Kernel Quadrature via Subsampling. (arXiv:2107.09597v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Shuting Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangxiang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Feng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Changzhi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangrong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shaoliang Peng</a>",
          "description": "The Corona Virus Disease 2019 (COVID-19) belongs to human coronaviruses\n(HCoVs), which spreads rapidly around the world. Compared with new drug\ndevelopment, drug repurposing may be the best shortcut for treating COVID-19.\nTherefore, we constructed a comprehensive heterogeneous network based on the\nHCoVs-related target proteins and use the previously proposed deepDTnet, to\ndiscover potential drug candidates for COVID-19. We obtain high performance in\npredicting the possible drugs effective for COVID-19 related proteins. In\nsummary, this work utilizes a powerful heterogeneous network-based deep\nlearning method, which may be beneficial to quickly identify candidate\nrepurposable drugs toward future clinical trials for COVID-19. The code and\ndata are available at https://github.com/stjin-XMU/HnDR-COVID.",
          "link": "http://arxiv.org/abs/2107.09217",
          "publishedOn": "2021-07-21T02:01:36.701Z",
          "wordCount": 601,
          "title": "Heterogeneous network-based drug repurposing for COVID-19. (arXiv:2107.09217v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.08032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wanguang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quanying Liu</a>",
          "description": "Linear discriminant analysis (LDA) is a widely used algorithm in machine\nlearning to extract a low-dimensional representation of high-dimensional data,\nit features to find the orthogonal discriminant projection subspace by using\nthe Fisher discriminant criterion. However, the traditional Euclidean-based\nmethods for solving LDA are easily convergent to spurious local minima and\nhardly obtain an optimal solution. To address such a problem, in this paper, we\npropose a novel algorithm namely Riemannian-based discriminant analysis (RDA)\nfor subspace learning. In order to obtain an explicit solution, we transform\nthe traditional Euclidean-based methods to the Riemannian manifold space and\nuse the trust-region method to learn the discriminant projection subspace. We\ncompare the proposed algorithm to existing variants of LDA, as well as the\nunsupervised tensor decomposition methods on image classification tasks. The\nnumerical results suggest that RDA achieves state-of-the-art performance in\nclassification accuracy.",
          "link": "http://arxiv.org/abs/2101.08032",
          "publishedOn": "2021-07-21T02:01:36.638Z",
          "wordCount": 618,
          "title": "Riemannian Manifold Optimization for Discriminant Subspace Learning. (arXiv:2101.08032v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1\">Shubhanshu Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fields_G/0/1/0/all/0/1\">Greg Fields</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1\">Mohammad Ghavamzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1\">Tara Javidi</a>",
          "description": "Machine learning models trained on uncurated datasets can often end up\nadversely affecting inputs belonging to underrepresented groups. To address\nthis issue, we consider the problem of adaptively constructing training sets\nwhich allow us to learn classifiers that are fair in a minimax sense. We first\npropose an adaptive sampling algorithm based on the principle of optimism, and\nderive theoretical bounds on its performance. We also propose heuristic\nextensions of this algorithm suitable for application to large scale, practical\nproblems. Next, by deriving algorithm independent lower-bounds for a specific\nclass of problems, we show that the performance achieved by our adaptive scheme\ncannot be improved in general. We then validate the benefits of adaptively\nconstructing training sets via experiments on synthetic tasks with logistic\nregression classifiers, as well as on several real-world tasks using\nconvolutional neural networks (CNNs).",
          "link": "http://arxiv.org/abs/2103.00755",
          "publishedOn": "2021-07-21T02:01:36.628Z",
          "wordCount": 596,
          "title": "Adaptive Sampling for Minimax Fair Classification. (arXiv:2103.00755v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09546",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petersen_E/0/1/0/all/0/1\">Eike Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potdevin_Y/0/1/0/all/0/1\">Yannik Potdevin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_E/0/1/0/all/0/1\">Esfandiar Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zidowitz_S/0/1/0/all/0/1\">Stephan Zidowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breyer_S/0/1/0/all/0/1\">Sabrina Breyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowotka_D/0/1/0/all/0/1\">Dirk Nowotka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henn_S/0/1/0/all/0/1\">Sandra Henn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechmann_L/0/1/0/all/0/1\">Ludwig Pechmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leucker_M/0/1/0/all/0/1\">Martin Leucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostalski_P/0/1/0/all/0/1\">Philipp Rostalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzog_C/0/1/0/all/0/1\">Christian Herzog</a>",
          "description": "Machine learning is expected to fuel significant improvements in medical\ncare. To ensure that fundamental principles such as beneficence, respect for\nhuman autonomy, prevention of harm, justice, privacy, and transparency are\nrespected, medical machine learning applications must be developed responsibly.\nIn this paper, we survey the technical challenges involved in creating medical\nmachine learning systems responsibly and in conformity with existing\nregulations, as well as possible solutions to address these challenges. We\nbegin by providing a brief overview of existing regulations affecting medical\nmachine learning, showing that properties such as safety, robustness,\nreliability, privacy, security, transparency, explainability, and\nnondiscrimination are all demanded already by existing law and regulations -\nalbeit, in many cases, to an uncertain degree. Next, we discuss the underlying\ntechnical challenges, possible ways for addressing them, and their respective\nmerits and drawbacks. We notice that distribution shift, spurious correlations,\nmodel underspecification, and data scarcity represent severe challenges in the\nmedical context (and others) that are very difficult to solve with classical\nblack-box deep neural networks. Important measures that may help to address\nthese challenges include the use of large and representative datasets and\nfederated learning as a means to that end, the careful exploitation of domain\nknowledge wherever feasible, the use of inherently transparent models,\ncomprehensive model testing and verification, as well as stakeholder inclusion.",
          "link": "http://arxiv.org/abs/2107.09546",
          "publishedOn": "2021-07-21T02:01:36.619Z",
          "wordCount": 693,
          "title": "Responsible and Regulatory Conform Machine Learning for Medicine: A Survey of Technical Challenges and Solutions. (arXiv:2107.09546v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.13972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_R/0/1/0/all/0/1\">Rory Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1\">Eibe Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holmes_G/0/1/0/all/0/1\">Geoffrey Holmes</a>",
          "description": "SHAP (SHapley Additive exPlanation) values provide a game theoretic\ninterpretation of the predictions of machine learning models based on Shapley\nvalues. While exact calculation of SHAP values is computationally intractable\nin general, a recursive polynomial-time algorithm called TreeShap is available\nfor decision tree models. However, despite its polynomial time complexity,\nTreeShap can become a significant bottleneck in practical machine learning\npipelines when applied to large decision tree ensembles. We present\nGPUTreeShap, a modified TreeShap algorithm suitable for massively parallel\ncomputation on graphics processing units. Our approach first preprocesses each\ndecision tree to isolate variable sized sub-problems from the original\nrecursive algorithm, then solves a bin packing problem, and finally maps\nsub-problems to single-instruction, multiple-thread (SIMT) tasks for parallel\nexecution with specialised hardware instructions. With a single NVIDIA Tesla\nV100-32 GPU, we achieve speedups of up to 19x for SHAP values, and speedups of\nup to 340x for SHAP interaction values, over a state-of-the-art multi-core CPU\nimplementation executed on two 20-core Xeon E5-2698 v4 2.2 GHz CPUs. We also\nexperiment with multi-GPU computing using eight V100 GPUs, demonstrating\nthroughput of 1.2M rows per second -- equivalent CPU-based performance is\nestimated to require 6850 CPU cores.",
          "link": "http://arxiv.org/abs/2010.13972",
          "publishedOn": "2021-07-21T02:01:36.593Z",
          "wordCount": 663,
          "title": "GPUTreeShap: Massively Parallel Exact Calculation of SHAP Scores for Tree Ensembles. (arXiv:2010.13972v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.05675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinhai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lin Chen</a>",
          "description": "Few-shot learning aims at rapidly adapting to novel categories with only a\nhandful of samples at test time, which has been predominantly tackled with the\nidea of meta-learning. However, meta-learning approaches essentially learn\nacross a variety of few-shot tasks and thus still require large-scale training\ndata with fine-grained supervision to derive a generalized model, thereby\ninvolving prohibitive annotation cost. In this paper, we advance the few-shot\nclassification paradigm towards a more challenging scenario, i.e.,\ncross-granularity few-shot classification, where the model observes only coarse\nlabels during training while is expected to perform fine-grained classification\nduring testing. This task largely relieves the annotation cost since\nfine-grained labeling usually requires strong domain-specific expertise. To\nbridge the cross-granularity gap, we approximate the fine-grained data\ndistribution by greedy clustering of each coarse-class into pseudo-fine-classes\naccording to the similarity of image embeddings. We then propose a\nmeta-embedder that jointly optimizes the visual- and semantic-discrimination,\nin both instance-wise and coarse class-wise, to obtain a good feature space for\nthis coarse-to-fine pseudo-labeling process. Extensive experiments and ablation\nstudies are conducted to demonstrate the effectiveness and robustness of our\napproach on three representative datasets.",
          "link": "http://arxiv.org/abs/2007.05675",
          "publishedOn": "2021-07-21T02:01:36.578Z",
          "wordCount": 674,
          "title": "Towards Cross-Granularity Few-Shot Learning: Coarse-to-Fine Pseudo-Labeling with Visual-Semantic Meta-Embedding. (arXiv:2007.05675v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05206",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Ke Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_D/0/1/0/all/0/1\">Dongxuan He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1\">Hancun Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaocheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1\">Sheng Chen</a>",
          "description": "Huge overhead of beam training imposes a significant challenge in\nmillimeter-wave (mmWave) wireless communications. To address this issue, in\nthis paper, we propose a wide beam based training approach to calibrate the\nnarrow beam direction according to the channel power leakage. To handle the\ncomplex nonlinear properties of the channel power leakage, deep learning is\nutilized to predict the optimal narrow beam directly. Specifically, three deep\nlearning assisted calibrated beam training schemes are proposed. The first\nscheme adopts convolution neural network to implement the prediction based on\nthe instantaneous received signals of wide beam training. We also perform the\nadditional narrow beam training based on the predicted probabilities for\nfurther beam direction calibrations. However, the first scheme only depends on\none wide beam training, which lacks the robustness to noise. To tackle this\nproblem, the second scheme adopts long-short term memory (LSTM) network for\ntracking the movement of users and calibrating the beam direction according to\nthe received signals of prior beam training, in order to enhance the robustness\nto noise. To further reduce the overhead of wide beam training, our third\nscheme, an adaptive beam training strategy, selects partial wide beams to be\ntrained based on the prior received signals. Two criteria, namely, optimal\nneighboring criterion and maximum probability criterion, are designed for the\nselection. Furthermore, to handle mobile scenarios, auxiliary LSTM is\nintroduced to calibrate the directions of the selected wide beams more\nprecisely. Simulation results demonstrate that our proposed schemes achieve\nsignificantly higher beamforming gain with smaller beam training overhead\ncompared with the conventional and existing deep-learning based counterparts.",
          "link": "http://arxiv.org/abs/2101.05206",
          "publishedOn": "2021-07-21T02:01:36.571Z",
          "wordCount": 740,
          "title": "Deep Learning Assisted Calibrated Beam Training for Millimeter-Wave Communication Systems. (arXiv:2101.05206v3 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Langevin_A/0/1/0/all/0/1\">Antoine Langevin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carbonneau_M/0/1/0/all/0/1\">Marc-Andr&#xe9; Carbonneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheriet_M/0/1/0/all/0/1\">Mohamed Cheriet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gagnon_G/0/1/0/all/0/1\">Ghyslain Gagnon</a>",
          "description": "Non-intrusive load monitoring (NILM) is a technique that uses a single sensor\nto measure the total power consumption of a building. Using an energy\ndisaggregation method, the consumption of individual appliances can be\nestimated from the aggregate measurement. Recent disaggregation algorithms have\nsignificantly improved the performance of NILM systems. However, the\ngeneralization capability of these methods to different houses as well as the\ndisaggregation of multi-state appliances are still major challenges. In this\npaper we address these issues and propose an energy disaggregation approach\nbased on the variational autoencoders framework. The probabilistic encoder\nmakes this approach an efficient model for encoding information relevant to the\nreconstruction of the target appliance consumption. In particular, the proposed\nmodel accurately generates more complex load profiles, thus improving the power\nsignal reconstruction of multi-state appliances. Moreover, its regularized\nlatent space improves the generalization capabilities of the model across\ndifferent houses. The proposed model is compared to state-of-the-art NILM\napproaches on the UK-DALE and REFIT datasets, and yields competitive results.\nThe mean absolute error reduces by 18% on average across all appliances\ncompared to the state-of-the-art. The F1-Score increases by more than 11%,\nshowing improvements for the detection of the target appliance in the aggregate\nmeasurement.",
          "link": "http://arxiv.org/abs/2103.12177",
          "publishedOn": "2021-07-21T02:01:36.564Z",
          "wordCount": 669,
          "title": "Energy Disaggregation using Variational Autoencoders. (arXiv:2103.12177v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08001",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hatua_A/0/1/0/all/0/1\">Amartya Hatua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Arjun Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_R/0/1/0/all/0/1\">Rakesh M. Verma</a>",
          "description": "This article describes research on claim verification carried out using a\nmultiple GAN-based model. The proposed model consists of three pairs of\ngenerators and discriminators. The generator and discriminator pairs are\nresponsible for generating synthetic data for supported and refuted claims and\nclaim labels. A theoretical discussion about the proposed model is provided to\nvalidate the equilibrium state of the model. The proposed model is applied to\nthe FEVER dataset, and a pre-trained language model is used for the input text\ndata. The synthetically generated data helps to gain information which helps\nthe model to perform better than state of the art models and other standard\nclassifiers.",
          "link": "http://arxiv.org/abs/2103.08001",
          "publishedOn": "2021-07-21T02:01:36.542Z",
          "wordCount": 598,
          "title": "Claim Verification using a Multi-GAN based Model. (arXiv:2103.08001v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Franzmeyer_T/0/1/0/all/0/1\">Tim Franzmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1\">Jo&#xe3;o F. Henriques</a>",
          "description": "Can artificial agents learn to assist others in achieving their goals without\nknowing what those goals are? Generic reinforcement learning agents could be\ntrained to behave altruistically towards others by rewarding them for\naltruistic behaviour, i.e., rewarding them for benefiting other agents in a\ngiven situation. Such an approach assumes that other agents' goals are known so\nthat the altruistic agent can cooperate in achieving those goals. However,\nexplicit knowledge of other agents' goals is often difficult to acquire. Even\nassuming such knowledge to be given, training of altruistic agents would\nrequire manually-tuned external rewards for each new environment. Thus, it is\nbeneficial to develop agents that do not depend on external supervision and can\nlearn altruistic behaviour in a task-agnostic manner. Assuming that other\nagents rationally pursue their goals, we hypothesize that giving them more\nchoices will allow them to pursue those goals better. Some concrete examples\ninclude opening a door for others or safeguarding them to pursue their\nobjectives without interference. We formalize this concept and propose an\naltruistic agent that learns to increase the choices another agent has by\nmaximizing the number of states that the other agent can reach in its future.\nWe evaluate our approach on three different multi-agent environments where\nanother agent's success depends on the altruistic agent's behaviour. Finally,\nwe show that our unsupervised agents can perform comparably to agents\nexplicitly trained to work cooperatively. In some cases, our agents can even\noutperform the supervised ones.",
          "link": "http://arxiv.org/abs/2107.09598",
          "publishedOn": "2021-07-21T02:01:36.535Z",
          "wordCount": 681,
          "title": "Learning Altruistic Behaviours in Reinforcement Learning without External Rewards. (arXiv:2107.09598v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brandfonbrener_D/0/1/0/all/0/1\">David Brandfonbrener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitney_W/0/1/0/all/0/1\">William F. Whitney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1\">Rajesh Ranganath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>",
          "description": "Most prior approaches to offline reinforcement learning (RL) have taken an\niterative actor-critic approach involving off-policy evaluation. In this paper\nwe show that simply doing one step of constrained/regularized policy\nimprovement using an on-policy Q estimate of the behavior policy performs\nsurprisingly well. This one-step algorithm beats the previously reported\nresults of iterative algorithms on a large portion of the D4RL benchmark. The\nsimple one-step baseline achieves this strong performance without many of the\ntricks used by previously proposed iterative algorithms and is more robust to\nhyperparameters. We argue that the relatively poor performance of iterative\napproaches is a result of the high variance inherent in doing off-policy\nevaluation and magnified by the repeated optimization of policies against those\nhigh-variance estimates. In addition, we hypothesize that the strong\nperformance of the one-step algorithm is due to a combination of favorable\nstructure in the environment and behavior policy.",
          "link": "http://arxiv.org/abs/2106.08909",
          "publishedOn": "2021-07-21T02:01:36.527Z",
          "wordCount": 607,
          "title": "Offline RL Without Off-Policy Evaluation. (arXiv:2106.08909v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09543",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1\">Richard Osuala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1\">Kaisar Kushibar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garrucho_L/0/1/0/all/0/1\">Lidia Garrucho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Linardos_A/0/1/0/all/0/1\">Akis Linardos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szafranowska_Z/0/1/0/all/0/1\">Zuzanna Szafranowska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1\">Stefan Klein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1\">Oliver Diaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>",
          "description": "Despite technological and medical advances, the detection, interpretation,\nand treatment of cancer based on imaging data continue to pose significant\nchallenges. These include high inter-observer variability, difficulty of\nsmall-sized lesion detection, nodule interpretation and malignancy\ndetermination, inter- and intra-tumour heterogeneity, class imbalance,\nsegmentation inaccuracies, and treatment effect uncertainty. The recent\nadvancements in Generative Adversarial Networks (GANs) in computer vision as\nwell as in medical imaging may provide a basis for enhanced capabilities in\ncancer detection and analysis. In this review, we assess the potential of GANs\nto address a number of key challenges of cancer imaging, including data\nscarcity and imbalance, domain and dataset shifts, data access and privacy,\ndata annotation and quantification, as well as cancer detection, tumour\nprofiling and treatment planning. We provide a critical appraisal of the\nexisting literature of GANs applied to cancer imagery, together with\nsuggestions on future research directions to address these challenges. We\nanalyse and discuss 163 papers that apply adversarial training techniques in\nthe context of cancer imaging and elaborate their methodologies, advantages and\nlimitations. With this work, we strive to bridge the gap between the needs of\nthe clinical cancer imaging community and the current and prospective research\non GANs in the artificial intelligence community.",
          "link": "http://arxiv.org/abs/2107.09543",
          "publishedOn": "2021-07-21T02:01:36.520Z",
          "wordCount": 691,
          "title": "A Review of Generative Adversarial Networks in Cancer Imaging: New Applications, New Solutions. (arXiv:2107.09543v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mayer_J/0/1/0/all/0/1\">Jana Mayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westermann_J/0/1/0/all/0/1\">Johannes Westermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muriedas_J/0/1/0/all/0/1\">Juan Pedro Guti&#xe9;rrez H. Muriedas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mettin_U/0/1/0/all/0/1\">Uwe Mettin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampe_A/0/1/0/all/0/1\">Alexander Lampe</a>",
          "description": "In recent years, reinforcement learning (RL) has gained increasing attention\nin control engineering. Especially, policy gradient methods are widely used. In\nthis work, we improve the tracking performance of proximal policy optimization\n(PPO) for arbitrary reference signals by incorporating information about future\nreference values. Two variants of extending the argument of the actor and the\ncritic taking future reference values into account are presented. In the first\nvariant, global future reference values are added to the argument. For the\nsecond variant, a novel kind of residual space with future reference values\napplicable to model-free reinforcement learning is introduced. Our approach is\nevaluated against a PI controller on a simple drive train model. We expect our\nmethod to generalize to arbitrary references better than previous approaches,\npointing towards the applicability of RL to control real systems.",
          "link": "http://arxiv.org/abs/2107.09647",
          "publishedOn": "2021-07-21T02:01:36.513Z",
          "wordCount": 584,
          "title": "Proximal Policy Optimization for Tracking Control Exploiting Future Reference Information. (arXiv:2107.09647v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1\">Song Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>",
          "description": "Modern machine learning models with high accuracy are often miscalibrated --\nthe predicted top probability does not reflect the actual accuracy, and tends\nto be over-confident. It is commonly believed that such over-confidence is\nmainly due to over-parametrization, in particular when the model is large\nenough to memorize the training data and maximize the confidence.\n\nIn this paper, we show theoretically that over-parametrization is not the\nonly reason for over-confidence. We prove that logistic regression is\ninherently over-confident, in the realizable, under-parametrized setting where\nthe data is generated from the logistic model, and the sample size is much\nlarger than the number of parameters. Further, this over-confidence happens for\ngeneral well-specified binary classification problems as long as the activation\nis symmetric and concave on the positive part. Perhaps surprisingly, we also\nshow that over-confidence is not always the case -- there exists another\nactivation function (and a suitable loss function) under which the learned\nclassifier is under-confident at some probability values. Overall, our theory\nprovides a precise characterization of calibration in realizable binary\nclassification, which we verify on simulations and real data experiments.",
          "link": "http://arxiv.org/abs/2102.07856",
          "publishedOn": "2021-07-21T02:01:36.497Z",
          "wordCount": 668,
          "title": "Don't Just Blame Over-parametrization for Over-confidence: Theoretical Analysis of Calibration in Binary Classification. (arXiv:2102.07856v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13416",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Burton_C/0/1/0/all/0/1\">Charles Burton</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Stubbs_S/0/1/0/all/0/1\">Spencer Stubbs</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Onyisi_P/0/1/0/all/0/1\">Peter Onyisi</a>",
          "description": "Mixture Density Networks (MDNs) can be used to generate probability density\nfunctions of model parameters $\\boldsymbol{\\theta}$ given a set of observables\n$\\mathbf{x}$. In some applications, training data are available only for\ndiscrete values of a continuous parameter $\\boldsymbol{\\theta}$. In such\nsituations a number of performance-limiting issues arise which can result in\nbiased estimates. We demonstrate the usage of MDNs for parameter estimation,\ndiscuss the origins of the biases, and propose a corrective method for each\nissue.",
          "link": "http://arxiv.org/abs/2103.13416",
          "publishedOn": "2021-07-21T02:01:36.490Z",
          "wordCount": 549,
          "title": "Mixture Density Network Estimation of Continuous Variable Maximum Likelihood Using Discrete Training Samples. (arXiv:2103.13416v2 [physics.data-an] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Ye Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_V/0/1/0/all/0/1\">Vincent Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Songfu Cai</a>",
          "description": "Sparse coding is a class of unsupervised methods for learning a sparse\nrepresentation of the input data in the form of a linear combination of a\ndictionary and a sparse code. This learning framework has led to\nstate-of-the-art results in various image and video processing tasks. However,\nclassical methods learn the dictionary and the sparse code based on alternating\noptimizations, usually without theoretical guarantees for either optimality or\nconvergence due to non-convexity of the problem. Recent works on sparse coding\nwith a complete dictionary provide strong theoretical guarantees thanks to the\ndevelopment of the non-convex optimization. However, initial non-convex\napproaches learn the dictionary in the sparse coding problem sequentially in an\natom-by-atom manner, which leads to a long execution time. More recent works\nseek to directly learn the entire dictionary at once, which substantially\nreduces the execution time. However, the associated recovery performance is\ndegraded with a finite number of data samples. In this paper, we propose an\nefficient sparse coding scheme with a two-stage optimization. The proposed\nscheme leverages the global and local Riemannian geometry of the two-stage\noptimization problem and facilitates fast implementation for superb dictionary\nrecovery performance by a finite number of samples without atom-by-atom\ncalculation. We further prove that, with high probability, the proposed scheme\ncan exactly recover any atom in the target dictionary with a finite number of\nsamples if it is adopted to recover one atom of the dictionary. An application\non wireless sensor data compression is also proposed. Experiments on both\nsynthetic and real-world data verify the efficiency and effectiveness of the\nproposed scheme.",
          "link": "http://arxiv.org/abs/2104.10314",
          "publishedOn": "2021-07-21T02:01:36.482Z",
          "wordCount": 753,
          "title": "Efficient Sparse Coding using Hierarchical Riemannian Pursuit. (arXiv:2104.10314v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02081",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Vargas_F/0/1/0/all/0/1\">Francisco Vargas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thodoroff_P/0/1/0/all/0/1\">Pierre Thodoroff</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lawrence_N/0/1/0/all/0/1\">Neil D. Lawrence</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lamacraft_A/0/1/0/all/0/1\">Austen Lamacraft</a>",
          "description": "The Schr\\\"odinger bridge problem (SBP) finds the most likely stochastic\nevolution between two probability distributions given a prior stochastic\nevolution. As well as applications in the natural sciences, problems of this\nkind have important applications in machine learning such as dataset alignment\nand hypothesis testing. Whilst the theory behind this problem is relatively\nmature, scalable numerical recipes to estimate the Schr\\\"odinger bridge remain\nan active area of research. We prove an equivalence between the SBP and maximum\nlikelihood estimation enabling direct application of successful machine\nlearning techniques. We propose a numerical procedure to estimate SBPs using\nGaussian process and demonstrate the practical usage of our approach in\nnumerical simulations and experiments.",
          "link": "http://arxiv.org/abs/2106.02081",
          "publishedOn": "2021-07-21T02:01:36.474Z",
          "wordCount": 576,
          "title": "Solving Schr\\\"odinger Bridges via Maximum Likelihood. (arXiv:2106.02081v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meihong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiaohui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tony Xiao Han</a>",
          "description": "Integrated sensing and communication (ISAC) is a promising technology to\nimprove the band-utilization efficiency via spectrum sharing or hardware\nsharing between radar and communication systems. Since a common radio resource\nbudget is shared by both functionalities, there exists a tradeoff between the\nsensing and communication performance. However, this tradeoff curve is\ncurrently unknown in ISAC systems with human motion recognition tasks based on\ndeep learning. To fill this gap, this paper formulates and solves a\nmulti-objective optimization problem which simultaneously maximizes the\nrecognition accuracy and the communication data rate. The key ingredient of\nthis new formulation is a nonlinear recognition accuracy model with respect to\nthe wireless resources, where the model is derived from power function\nregression of the system performance of the deep spectrogram network. To avoid\ncost-expensive data collection procedures, a primitive-based autoregressive\nhybrid (PBAH) channel model is developed, which facilitates efficient training\nand testing dataset generation for human motion recognition in a virtual\nenvironment. Extensive results demonstrate that the proposed wireless\nrecognition accuracy and PBAH channel models match the actual experimental data\nvery well. Moreover, it is found that the accuracy-rate region consists of a\ncommunication saturation zone, a sensing saturation zone, and a\ncommunication-sensing adversarial zone, of which the third zone achieves the\ndesirable balanced performance for ISAC systems.",
          "link": "http://arxiv.org/abs/2107.09621",
          "publishedOn": "2021-07-21T02:01:36.468Z",
          "wordCount": 682,
          "title": "Rethinking the Tradeoff in Integrated Sensing and Communication: Recognition Accuracy versus Communication Rate. (arXiv:2107.09621v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pong_V/0/1/0/all/0/1\">Vitchyr H. Pong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1\">Ashvin Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1\">Laura Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Catherine Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Meta-reinforcement learning (RL) can meta-train policies that adapt to new\ntasks with orders of magnitude less data than standard RL, but meta-training\nitself is costly and time-consuming. If we can meta-train on offline data, then\nwe can reuse the same static dataset, labeled once with rewards for different\ntasks, to meta-train policies that adapt to a variety of new tasks at meta-test\ntime. Although this capability would make meta-RL a practical tool for\nreal-world use, offline meta-RL presents additional challenges beyond online\nmeta-RL or standard offline RL settings. Meta-RL learns an exploration strategy\nthat collects data for adapting, and also meta-trains a policy that quickly\nadapts to data from a new task. Since this policy was meta-trained on a fixed,\noffline dataset, it might behave unpredictably when adapting to data collected\nby the learned exploration strategy, which differs systematically from the\noffline data and thus induces distributional shift. We do not want to remove\nthis distributional shift by simply adopting a conservative exploration\nstrategy, because learning an exploration strategy enables an agent to collect\nbetter data for faster adaptation. Instead, we propose a hybrid offline meta-RL\nalgorithm, which uses offline data with rewards to meta-train an adaptive\npolicy, and then collects additional unsupervised online data, without any\nreward labels to bridge this distribution shift. By not requiring reward labels\nfor online collection, this data can be much cheaper to collect. We compare our\nmethod to prior work on offline meta-RL on simulated robot locomotion and\nmanipulation tasks and find that using additional unsupervised online data\ncollection leads to a dramatic improvement in the adaptive capabilities of the\nmeta-trained policies, matching the performance of fully online meta-RL on a\nrange of challenging domains that require generalization to new tasks.",
          "link": "http://arxiv.org/abs/2107.03974",
          "publishedOn": "2021-07-21T02:01:36.449Z",
          "wordCount": 747,
          "title": "Offline Meta-Reinforcement Learning with Online Self-Supervision. (arXiv:2107.03974v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.03814",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Birrell_J/0/1/0/all/0/1\">Jeremiah Birrell</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dupuis_P/0/1/0/all/0/1\">Paul Dupuis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Katsoulakis_M/0/1/0/all/0/1\">Markos A. Katsoulakis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rey_Bellet_L/0/1/0/all/0/1\">Luc Rey-Bellet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>",
          "description": "We derive a new variational formula for the R\\'enyi family of divergences,\n$R_\\alpha(Q\\|P)$, between probability measures $Q$ and $P$. Our result\ngeneralizes the classical Donsker-Varadhan variational formula for the\nKullback-Leibler divergence. We further show that this R\\'enyi variational\nformula holds over a range of function spaces; this leads to a formula for the\noptimizer under very weak assumptions and is also key in our development of a\nconsistency theory for R\\'enyi divergence estimators. By applying this theory\nto neural-network estimators, we show that if a neural network family satisfies\none of several strengthened versions of the universal approximation property\nthen the corresponding R\\'enyi divergence estimator is consistent. In contrast\nto density-estimator based methods, our estimators involve only expectations\nunder $Q$ and $P$ and hence are more effective in high dimensional systems. We\nillustrate this via several numerical examples of neural network estimation in\nsystems of up to 5000 dimensions.",
          "link": "http://arxiv.org/abs/2007.03814",
          "publishedOn": "2021-07-21T02:01:36.442Z",
          "wordCount": 643,
          "title": "Variational Representations and Neural Network Estimation of R\\'enyi Divergences. (arXiv:2007.03814v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Erez_L/0/1/0/all/0/1\">Liad Erez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1\">Tomer Koren</a>",
          "description": "We study the online learning with feedback graphs framework introduced by\nMannor and Shamir (2011), in which the feedback received by the online learner\nis specified by a graph $G$ over the available actions. We develop an algorithm\nthat simultaneously achieves regret bounds of the form:\n$\\smash{\\mathcal{O}(\\sqrt{\\theta(G) T})}$ with adversarial losses;\n$\\mathcal{O}(\\theta(G)\\operatorname{polylog}{T})$ with stochastic losses; and\n$\\mathcal{O}(\\theta(G)\\operatorname{polylog}{T} + \\smash{\\sqrt{\\theta(G) C})}$\nwith stochastic losses subject to $C$ adversarial corruptions. Here,\n$\\theta(G)$ is the clique covering number of the graph $G$. Our algorithm is an\ninstantiation of Follow-the-Regularized-Leader with a novel regularization that\ncan be seen as a product of a Tsallis entropy component (inspired by Zimmert\nand Seldin (2019)) and a Shannon entropy component (analyzed in the corrupted\nstochastic case by Amir et al. (2020)), thus subtly interpolating between the\ntwo forms of entropies. One of our key technical contributions is in\nestablishing the convexity of this regularizer and controlling its inverse\nHessian, despite its complex product structure.",
          "link": "http://arxiv.org/abs/2107.09572",
          "publishedOn": "2021-07-21T02:01:36.436Z",
          "wordCount": 583,
          "title": "Best-of-All-Worlds Bounds for Online Learning with Feedback Graphs. (arXiv:2107.09572v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haotian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qianxiao Li</a>",
          "description": "We study the approximation properties of convolutional architectures applied\nto time series modelling, which can be formulated mathematically as a\nfunctional approximation problem. In the recurrent setting, recent results\nreveal an intricate connection between approximation efficiency and memory\nstructures in the data generation process. In this paper, we derive parallel\nresults for convolutional architectures, with WaveNet being a prime example.\nOur results reveal that in this new setting, approximation efficiency is not\nonly characterised by memory, but also additional fine structures in the target\nrelationship. This leads to a novel definition of spectrum-based regularity\nthat measures the complexity of temporal relationships under the convolutional\napproximation scheme. These analyses provide a foundation to understand the\ndifferences between architectural choices for time series modelling and can\ngive theoretically grounded guidance for practical applications.",
          "link": "http://arxiv.org/abs/2107.09355",
          "publishedOn": "2021-07-21T02:01:36.429Z",
          "wordCount": 578,
          "title": "Approximation Theory of Convolutional Architectures for Time Series Modelling. (arXiv:2107.09355v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1\">Steve Hanneke</a>",
          "description": "This open problem asks whether there exists an online learning algorithm for\nbinary classification that guarantees, for all target concepts, to make a\nsublinear number of mistakes, under only the assumption that the (possibly\nrandom) sequence of points X allows that such a learning algorithm can exist\nfor that sequence. As a secondary problem, it also asks whether a specific\nconcise condition completely determines whether a given (possibly random)\nsequence of points X admits the existence of online learning algorithms\nguaranteeing a sublinear number of mistakes for all target concepts.",
          "link": "http://arxiv.org/abs/2107.09542",
          "publishedOn": "2021-07-21T02:01:36.422Z",
          "wordCount": 547,
          "title": "Open Problem: Is There an Online Learning Algorithm That Learns Whenever Online Learning Is Possible?. (arXiv:2107.09542v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1\">Bharadwaj Manda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhayarkar_S/0/1/0/all/0/1\">Shubham Dhayarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viekash_V/0/1/0/all/0/1\">V.K. Viekash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1\">Ramanathan Muthuganapathy</a>",
          "description": "Ongoing advancements in the fields of 3D modelling and digital archiving have\nled to an outburst in the amount of data stored digitally. Consequently,\nseveral retrieval systems have been developed depending on the type of data\nstored in these databases. However, unlike text data or images, performing a\nsearch for 3D models is non-trivial. Among 3D models, retrieving 3D\nEngineering/CAD models or mechanical components is even more challenging due to\nthe presence of holes, volumetric features, presence of sharp edges etc., which\nmake CAD a domain unto itself. The research work presented in this paper aims\nat developing a dataset suitable for building a retrieval system for 3D CAD\nmodels based on deep learning. 3D CAD models from the available CAD databases\nare collected, and a dataset of computer-generated sketch data, termed\n'CADSketchNet', has been prepared. Additionally, hand-drawn sketches of the\ncomponents are also added to CADSketchNet. Using the sketch images from this\ndataset, the paper also aims at evaluating the performance of various retrieval\nsystem or a search engine for 3D CAD models that accepts a sketch image as the\ninput query. Many experimental models are constructed and tested on\nCADSketchNet. These experiments, along with the model architecture, choice of\nsimilarity metrics are reported along with the search results.",
          "link": "http://arxiv.org/abs/2107.06212",
          "publishedOn": "2021-07-21T02:01:36.416Z",
          "wordCount": 713,
          "title": "'CADSketchNet' -- An Annotated Sketch dataset for 3D CAD Model Retrieval with Deep Neural Networks. (arXiv:2107.06212v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gauthier_S/0/1/0/all/0/1\">Shanel Gauthier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Therien_B/0/1/0/all/0/1\">Benjamin Th&#xe9;rien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alsene_Racicot_L/0/1/0/all/0/1\">Laurent Als&#xe8;ne-Racicot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1\">Irina Rish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1\">Eugene Belilovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickenberg_M/0/1/0/all/0/1\">Michael Eickenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1\">Guy Wolf</a>",
          "description": "The wavelet scattering transform creates geometric invariants and deformation\nstability from an initial structured signal. In multiple signal domains it has\nbeen shown to yield more discriminative representations compared to other\nnon-learned representations, and to outperform learned representations in\ncertain tasks, particularly on limited labeled data and highly structured\nsignals. The wavelet filters used in the scattering transform are typically\nselected to create a tight frame via a parameterized mother wavelet. Focusing\non Morlet wavelets, we propose to instead adapt the scales, orientations, and\nslants of the filters to produce problem-specific parametrizations of the\nscattering transform. We show that our learned versions of the scattering\ntransform yield significant performance gains over the standard scattering\ntransform in the small sample classification settings, and our empirical\nresults suggest that tight frames may not always be necessary for scattering\ntransforms to extract effective representations.",
          "link": "http://arxiv.org/abs/2107.09539",
          "publishedOn": "2021-07-21T02:01:36.391Z",
          "wordCount": 583,
          "title": "Parametric Scattering Networks. (arXiv:2107.09539v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tung T. Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1\">Hien Quoc Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marzetta_T/0/1/0/all/0/1\">Thomas L. Marzetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthaiou_M/0/1/0/all/0/1\">Michail Matthaiou</a>",
          "description": "Federated learning (FL) has been considered as a promising learning framework\nfor future machine learning systems due to its privacy preservation and\ncommunication efficiency. In beyond-5G/6G systems, it is likely to have\nmultiple FL groups with different learning purposes. This scenario leads to a\nquestion: How does a wireless network support multiple FL groups? As an answer,\nwe first propose to use a cell-free massive multiple-input multiple-output\n(MIMO) network to guarantee the stable operation of multiple FL processes by\nletting the iterations of these FL processes be executed together within a\nlarge-scale coherence time. We then develop a novel scheme that asynchronously\nexecutes the iterations of FL processes under multicasting downlink and\nconventional uplink transmission protocols. Finally, we propose a\nsimple/low-complexity resource allocation algorithm which optimally chooses the\npower and computation resources to minimize the execution time of each\niteration of each FL process.",
          "link": "http://arxiv.org/abs/2107.09577",
          "publishedOn": "2021-07-21T02:01:36.385Z",
          "wordCount": 611,
          "title": "How Does Cell-Free Massive MIMO Support Multiple Federated Learning Groups?. (arXiv:2107.09577v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuefeng Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiabao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a>",
          "description": "Deep neural networks (DNNs) are under threat from adversarial example\nattacks. The adversary can easily change the outputs of DNNs by adding small\nwell-designed perturbations to inputs. Adversarial example detection is a\nfundamental work for robust DNNs-based service. Adversarial examples show the\ndifference between humans and DNNs in image recognition. From a human-centric\nperspective, image features could be divided into dominant features that are\ncomprehensible to humans, and recessive features that are incomprehensible to\nhumans, yet are exploited by DNNs. In this paper, we reveal that imperceptible\nadversarial examples are the product of recessive features misleading neural\nnetworks, and an adversarial attack is essentially a kind of method to enrich\nthese recessive features in the image. The imperceptibility of the adversarial\nexamples indicates that the perturbations enrich recessive features, yet hardly\naffect dominant features. Therefore, adversarial examples are sensitive to\nfiltering off recessive features, while benign examples are immune to such\noperation. Inspired by this idea, we propose a label-only adversarial detection\napproach that is referred to as feature-filter. Feature-filter utilizes\ndiscrete cosine transform to approximately separate recessive features from\ndominant features, and gets a mutant image that is filtered off recessive\nfeatures. By only comparing DNN's prediction labels on the input and its\nmutant, feature-filter can real-time detect imperceptible adversarial examples\nat high accuracy and few false positives.",
          "link": "http://arxiv.org/abs/2107.09502",
          "publishedOn": "2021-07-21T02:01:36.368Z",
          "wordCount": 653,
          "title": "Feature-Filter: Detecting Adversarial Examples through Filtering off Recessive Features. (arXiv:2107.09502v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Emma Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1\">Andy Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_R/0/1/0/all/0/1\">Rayan Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1\">Jin Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>",
          "description": "A major obstacle to the integration of deep learning models for chest x-ray\ninterpretation into clinical settings is the lack of understanding of their\nfailure modes. In this work, we first investigate whether there are patient\nsubgroups that chest x-ray models are likely to misclassify. We find that\npatient age and the radiographic finding of lung lesion, pneumothorax or\nsupport devices are statistically relevant features for predicting\nmisclassification for some chest x-ray models. Second, we develop\nmisclassification predictors on chest x-ray models using their outputs and\nclinical features. We find that our best performing misclassification\nidentifier achieves an AUROC close to 0.9 for most diseases. Third, employing\nour misclassification identifiers, we develop a corrective algorithm to\nselectively flip model predictions that have high likelihood of\nmisclassification at inference time. We observe F1 improvement on the\nprediction of Consolidation (0.008 [95% CI 0.005, 0.010]) and Edema (0.003,\n[95% CI 0.001, 0.006]). By carrying out our investigation on ten distinct and\nhigh-performing chest x-ray models, we are able to derive insights across model\narchitectures and offer a generalizable framework applicable to other medical\nimaging tasks.",
          "link": "http://arxiv.org/abs/2103.09957",
          "publishedOn": "2021-07-21T02:01:36.362Z",
          "wordCount": 697,
          "title": "CheXbreak: Misclassification Identification for Deep Learning Models Interpreting Chest X-rays. (arXiv:2103.09957v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09362",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ito_H/0/1/0/all/0/1\">Hiroki Ito</a>, <a href=\"http://arxiv.org/find/eess/1/au:+AprilPyone_M/0/1/0/all/0/1\">MaungMaung AprilPyone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>",
          "description": "Since production-level trained deep neural networks (DNNs) are of a great\nbusiness value, protecting such DNN models against copyright infringement and\nunauthorized access is in a rising demand. However, conventional model\nprotection methods focused only the image classification task, and these\nprotection methods were never applied to semantic segmentation although it has\nan increasing number of applications. In this paper, we propose to protect\nsemantic segmentation models from unauthorized access by utilizing block-wise\ntransformation with a secret key for the first time. Protected models are\ntrained by using transformed images. Experiment results show that the proposed\nprotection method allows rightful users with the correct key to access the\nmodel to full capacity and deteriorate the performance for unauthorized users.\nHowever, protected models slightly drop the segmentation performance compared\nto non-protected models.",
          "link": "http://arxiv.org/abs/2107.09362",
          "publishedOn": "2021-07-21T02:01:36.354Z",
          "wordCount": 606,
          "title": "Protecting Semantic Segmentation Models by Using Block-wise Image Encryption with Secret Key from Unauthorized Access. (arXiv:2107.09362v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yasaei_R/0/1/0/all/0/1\">Rozhin Yasaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shih-Yuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naeini_E/0/1/0/all/0/1\">Emad Kasaeyan Naeini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1\">Mohammad Abdullah Al Faruque</a>",
          "description": "Aggressive time-to-market constraints and enormous hardware design and\nfabrication costs have pushed the semiconductor industry toward hardware\nIntellectual Properties (IP) core design. However, the globalization of the\nintegrated circuits (IC) supply chain exposes IP providers to theft and illegal\nredistribution of IPs. Watermarking and fingerprinting are proposed to detect\nIP piracy. Nevertheless, they come with additional hardware overhead and cannot\nguarantee IP security as advanced attacks are reported to remove the watermark,\nforge, or bypass it. In this work, we propose a novel methodology, GNN4IP, to\nassess similarities between circuits and detect IP piracy. We model the\nhardware design as a graph and construct a graph neural network model to learn\nits behavior using the comprehensive dataset of register transfer level codes\nand gate-level netlists that we have gathered. GNN4IP detects IP piracy with\n96% accuracy in our dataset and recognizes the original IP in its obfuscated\nversion with 100% accuracy.",
          "link": "http://arxiv.org/abs/2107.09130",
          "publishedOn": "2021-07-21T02:01:36.345Z",
          "wordCount": 594,
          "title": "GNN4IP: Graph Neural Network for Hardware Intellectual Property Piracy Detection. (arXiv:2107.09130v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09510",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1\">Han Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vaessen_T/0/1/0/all/0/1\">Thomas Vaessen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Myin_Germeys_I/0/1/0/all/0/1\">Inez Myin-Germeys</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sano_A/0/1/0/all/0/1\">Akane Sano</a>",
          "description": "Multimodal wearable physiological data in daily life settings have been used\nto estimate self-reported stress labels.However, missing data modalities in\ndata collection make it challenging to leverage all the collected samples.\nBesides, heterogeneous sensor data and labels among individuals add challenges\nin building robust stress detection models. In this paper, we proposed a\nmodality fusion network (MFN) to train models and infer self-reported binary\nstress labels under both complete and incomplete modality condition. In\naddition, we applied a personalized attention (PA) strategy to leverage\npersonalized representation along with the generalized one-size-fits-all model.\nWe evaluated our methods on a multimodal wearable sensor dataset (N=41)\nincluding galvanic skin response (GSR) and electrocardiogram (ECG). Compared to\nthe baseline method using the samples with complete modalities, the performance\nof the MFN improved by 1.6\\% in f1-scores. On the other hand, the proposed PA\nstrategy showed a 2.3\\% higher stress detection f1-score and approximately up\nto 70\\% reduction in personalized model parameter size (9.1 MB) compared to the\nprevious state-of-the-art transfer learning strategy (29.3 MB).",
          "link": "http://arxiv.org/abs/2107.09510",
          "publishedOn": "2021-07-21T02:01:36.339Z",
          "wordCount": 618,
          "title": "Modality Fusion Network and Personalized Attention in Momentary Stress Detection in the Wild. (arXiv:2107.09510v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tornede_A/0/1/0/all/0/1\">Alexander Tornede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehring_L/0/1/0/all/0/1\">Lukas Gehring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tornede_T/0/1/0/all/0/1\">Tanja Tornede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wever_M/0/1/0/all/0/1\">Marcel Wever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>",
          "description": "The problem of selecting an algorithm that appears most suitable for a\nspecific instance of an algorithmic problem class, such as the Boolean\nsatisfiability problem, is called instance-specific algorithm selection. Over\nthe past decade, the problem has received considerable attention, resulting in\na number of different methods for algorithm selection. Although most of these\nmethods are based on machine learning, surprisingly little work has been done\non meta learning, that is, on taking advantage of the complementarity of\nexisting algorithm selection methods in order to combine them into a single\nsuperior algorithm selector. In this paper, we introduce the problem of meta\nalgorithm selection, which essentially asks for the best way to combine a given\nset of algorithm selectors. We present a general methodological framework for\nmeta algorithm selection as well as several concrete learning methods as\ninstantiations of this framework, essentially combining ideas of meta learning\nand ensemble learning. In an extensive experimental evaluation, we demonstrate\nthat ensembles of algorithm selectors can significantly outperform single\nalgorithm selectors and have the potential to form the new state of the art in\nalgorithm selection.",
          "link": "http://arxiv.org/abs/2107.09414",
          "publishedOn": "2021-07-21T02:01:36.271Z",
          "wordCount": 624,
          "title": "Algorithm Selection on a Meta Level. (arXiv:2107.09414v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siqi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weilong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xianliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suo_H/0/1/0/all/0/1\">Hongbin Suo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jinwei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhijie Yan</a>",
          "description": "In this paper we describe a speaker diarization system that enables\nlocalization and identification of all speakers present in a conversation or\nmeeting. We propose a novel systematic approach to tackle several long-standing\nchallenges in speaker diarization tasks: (1) to segment and separate\noverlapping speech from two speakers; (2) to estimate the number of speakers\nwhen participants may enter or leave the conversation at any time; (3) to\nprovide accurate speaker identification on short text-independent utterances;\n(4) to track down speakers movement during the conversation; (5) to detect\nspeaker change incidence real-time. First, a differential directional\nmicrophone array-based approach is exploited to capture the target speakers'\nvoice in far-field adverse environment. Second, an online speaker-location\njoint clustering approach is proposed to keep track of speaker location. Third,\nan instant speaker number detector is developed to trigger the mechanism that\nseparates overlapped speech. The results suggest that our system effectively\nincorporates spatial information and achieves significant gains.",
          "link": "http://arxiv.org/abs/2107.09321",
          "publishedOn": "2021-07-21T02:01:36.264Z",
          "wordCount": 619,
          "title": "A Real-time Speaker Diarization System Based on Spatial Spectrum. (arXiv:2107.09321v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09099",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiushi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">H Lilian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xubo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>",
          "description": "Punctuation is critical in understanding natural language text. Currently,\nmost automatic speech recognition (ASR) systems do not generate punctuation,\nwhich affects the performance of downstream tasks, such as intent detection and\nslot filling. This gives rise to the need for punctuation restoration. Recent\nwork in punctuation restoration heavily utilizes pre-trained language models\nwithout considering data imbalance when predicting punctuation classes. In this\nwork, we address this problem by proposing a token-level supervised contrastive\nlearning method that aims at maximizing the distance of representation of\ndifferent punctuation marks in the embedding space. The result shows that\ntraining with token-level supervised contrastive learning obtains up to 3.2%\nabsolute F1 improvement on the test set.",
          "link": "http://arxiv.org/abs/2107.09099",
          "publishedOn": "2021-07-21T02:01:36.257Z",
          "wordCount": 554,
          "title": "Token-Level Supervised Contrastive Learning for Punctuation Restoration. (arXiv:2107.09099v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09145",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ha_W/0/1/0/all/0/1\">Wooseok Ha</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Singh_C/0/1/0/all/0/1\">Chandan Singh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lanusse_F/0/1/0/all/0/1\">Francois Lanusse</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Song_E/0/1/0/all/0/1\">Eli Song</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dang_S/0/1/0/all/0/1\">Song Dang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+He_K/0/1/0/all/0/1\">Kangmin He</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Upadhyayula_S/0/1/0/all/0/1\">Srigokul Upadhyayula</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1\">Bin Yu</a>",
          "description": "Recent deep-learning models have achieved impressive prediction performance,\nbut often sacrifice interpretability and computational efficiency.\nInterpretability is crucial in many disciplines, such as science and medicine,\nwhere models must be carefully vetted or where interpretation is the goal\nitself. Moreover, interpretable models are concise and often yield\ncomputational efficiency. Here, we propose adaptive wavelet distillation (AWD),\na method which aims to distill information from a trained neural network into a\nwavelet transform. Specifically, AWD penalizes feature attributions of a neural\nnetwork in the wavelet domain to learn an effective multi-resolution wavelet\ntransform. The resulting model is highly predictive, concise, computationally\nefficient, and has properties (such as a multi-scale structure) which make it\neasy to interpret. In close collaboration with domain experts, we showcase how\nAWD addresses challenges in two real-world settings: cosmological parameter\ninference and molecular-partner prediction. In both cases, AWD yields a\nscientifically interpretable and concise model which gives predictive\nperformance better than state-of-the-art neural networks. Moreover, AWD\nidentifies predictive features that are scientifically meaningful in the\ncontext of respective domains. All code and models are released in a\nfull-fledged package available on Github\n(https://github.com/Yu-Group/adaptive-wavelets).",
          "link": "http://arxiv.org/abs/2107.09145",
          "publishedOn": "2021-07-21T02:01:36.204Z",
          "wordCount": 630,
          "title": "Adaptive wavelet distillation from neural networks through interpretations. (arXiv:2107.09145v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09095",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pikoulis_E/0/1/0/all/0/1\">Erion-Vasilis Pikoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavrokefalidis_C/0/1/0/all/0/1\">Christos Mavrokefalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalos_A/0/1/0/all/0/1\">Aris S. Lalos</a>",
          "description": "Deep learning and especially the use of Deep Neural Networks (DNNs) provides\nimpressive results in various regression and classification tasks. However, to\nachieve these results, there is a high demand for computing and storing\nresources. This becomes problematic when, for instance, real-time, mobile\napplications are considered, in which the involved (embedded) devices have\nlimited resources. A common way of addressing this problem is to transform the\noriginal large pre-trained networks into new smaller models, by utilizing Model\nCompression and Acceleration (MCA) techniques. Within the MCA framework, we\npropose a clustering-based approach that is able to increase the number of\nemployed centroids/representatives, while at the same time, have an\nacceleration gain compared to conventional, $k$-means based approaches. This is\nachieved by imposing a special structure to the employed representatives, which\nis enabled by the particularities of the problem at hand. Moreover, the\ntheoretical acceleration gains are presented and the key system\nhyper-parameters that affect that gain, are identified. Extensive evaluation\nstudies carried out using various state-of-the-art DNN models trained in image\nclassification, validate the superiority of the proposed method as compared for\nits use in MCA tasks.",
          "link": "http://arxiv.org/abs/2107.09095",
          "publishedOn": "2021-07-21T02:01:36.197Z",
          "wordCount": 623,
          "title": "A New Clustering-Based Technique for the Acceleration of Deep Convolutional Networks. (arXiv:2107.09095v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09545",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ayoub_J/0/1/0/all/0/1\">Jackie Ayoub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Na Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">X. Jessie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feng Zhou</a>",
          "description": "It is extremely important to ensure a safe takeover transition in\nconditionally automated driving. One of the critical factors that quantifies\nthe safe takeover transition is takeover time. Previous studies identified the\neffects of many factors on takeover time, such as takeover lead time,\nnon-driving tasks, modalities of the takeover requests (TORs), and scenario\nurgency. However, there is a lack of research to predict takeover time by\nconsidering these factors all at the same time. Toward this end, we used\neXtreme Gradient Boosting (XGBoost) to predict the takeover time using a\ndataset from a meta-analysis study [1]. In addition, we used SHAP (SHapley\nAdditive exPlanation) to analyze and explain the effects of the predictors on\ntakeover time. We identified seven most critical predictors that resulted in\nthe best prediction performance. Their main effects and interaction effects on\ntakeover time were examined. The results showed that the proposed approach\nprovided both good performance and explainability. Our findings have\nimplications on the design of in-vehicle monitoring and alert systems to\nfacilitate the interaction between the drivers and the automated vehicle.",
          "link": "http://arxiv.org/abs/2107.09545",
          "publishedOn": "2021-07-21T02:01:36.176Z",
          "wordCount": 615,
          "title": "Predicting Driver Takeover Time in Conditionally Automated Driving. (arXiv:2107.09545v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wilmet_V/0/1/0/all/0/1\">Vincent Wilmet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1\">Sauraj Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redl_T/0/1/0/all/0/1\">Tabea Redl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandaker_H/0/1/0/all/0/1\">H&#xe5;kon Sandaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenning Li</a>",
          "description": "Anomaly detection in images plays a significant role for many applications\nacross all industries, such as disease diagnosis in healthcare or quality\nassurance in manufacturing. Manual inspection of images, when extended over a\nmonotonously repetitive period of time is very time consuming and can lead to\nanomalies being overlooked.Artificial neural networks have proven themselves\nvery successful on simple, repetitive tasks, in some cases even outperforming\nhumans. Therefore, in this paper we investigate different methods of deep\nlearning, including supervised and unsupervised learning, for anomaly detection\napplied to a quality assurance use case. We utilize the MVTec anomaly dataset\nand develop three different models, a CNN for supervised anomaly detection,\nKD-CAE for autoencoder anomaly detection, NI-CAE for noise induced anomaly\ndetection and a DCGAN for generating reconstructed images. By experiments, we\nfound that KD-CAE performs better on the anomaly datasets compared to CNN and\nNI-CAE, with NI-CAE performing the best on the Transistor dataset. We also\nimplemented a DCGAN for the creation of new training data but due to\ncomputational limitation and lack of extrapolating the mechanics of AnoGAN, we\nrestricted ourselves just to the generation of GAN based images. We conclude\nthat unsupervised methods are more powerful for anomaly detection in images,\nespecially in a setting where only a small amount of anomalous data is\navailable, or the data is unlabeled.",
          "link": "http://arxiv.org/abs/2107.09204",
          "publishedOn": "2021-07-21T02:01:36.145Z",
          "wordCount": 686,
          "title": "A Comparison of Supervised and Unsupervised Deep Learning Methods for Anomaly Detection in Images. (arXiv:2107.09204v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09507",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cui_J/0/1/0/all/0/1\">Jian Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yisi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lan_Z/0/1/0/all/0/1\">Zirui Lan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sourina_O/0/1/0/all/0/1\">Olga Sourina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muller_Wittig_W/0/1/0/all/0/1\">Wolfgang M&#xfc;ller-Wittig</a>",
          "description": "In the context of electroencephalogram (EEG)-based driver drowsiness\nrecognition, it is still a challenging task to design a calibration-free\nsystem, since there exists a significant variability of EEG signals among\ndifferent subjects and recording sessions. As deep learning has received much\nresearch attention in recent years, many efforts have been made to use deep\nlearning methods for EEG signal recognition. However, existing works mostly\ntreat deep learning models as blackbox classifiers, while what have been\nlearned by the models and to which extent they are affected by the noise from\nEEG data are still underexplored. In this paper, we develop a novel\nconvolutional neural network that can explain its decision by highlighting the\nlocal areas of the input sample that contain important information for the\nclassification. The network has a compact structure for ease of interpretation\nand takes advantage of separable convolutions to process the EEG signals in a\nspatial-temporal sequence. Results show that the model achieves an average\naccuracy of 78.35% on 11 subjects for leave-one-out cross-subject drowsiness\nrecognition, which is higher than the conventional baseline methods of\n53.4%-72.68% and state-of-art deep learning methods of 63.90%-65.61%.\nVisualization results show that the model has learned to recognize biologically\nexplainable features from EEG signals, e.g., Alpha spindles, as strong\nindicators of drowsiness across different subjects. In addition, we also\nexplore reasons behind some wrongly classified samples and how the model is\naffected by artifacts and noise in the data. Our work illustrates a promising\ndirection on using interpretable deep learning models to discover meaning\npatterns related to different mental states from complex EEG signals.",
          "link": "http://arxiv.org/abs/2107.09507",
          "publishedOn": "2021-07-21T02:01:36.039Z",
          "wordCount": 713,
          "title": "EEG-based Cross-Subject Driver Drowsiness Recognition with Interpretable CNN. (arXiv:2107.09507v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09518",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying-Jun Angela Zhang</a>",
          "description": "Federated learning (FL) has recently emerged as a promising technology to\nenable artificial intelligence (AI) at the network edge, where distributed\nmobile devices collaboratively train a shared AI model under the coordination\nof an edge server. To significantly improve the communication efficiency of FL,\nover-the-air computation allows a large number of mobile devices to\nconcurrently upload their local models by exploiting the superposition property\nof wireless multi-access channels. Due to wireless channel fading, the model\naggregation error at the edge server is dominated by the weakest channel among\nall devices, causing severe straggler issues. In this paper, we propose a\nrelay-assisted cooperative FL scheme to effectively address the straggler\nissue. In particular, we deploy multiple half-duplex relays to cooperatively\nassist the devices in uploading the local model updates to the edge server. The\nnature of the over-the-air computation poses system objectives and constraints\nthat are distinct from those in traditional relay communication systems.\nMoreover, the strong coupling between the design variables renders the\noptimization of such a system challenging. To tackle the issue, we propose an\nalternating-optimization-based algorithm to optimize the transceiver and relay\noperation with low complexity. Then, we analyze the model aggregation error in\na single-relay case and show that our relay-assisted scheme achieves a smaller\nerror than the one without relays provided that the relay transmit power and\nthe relay channel gains are sufficiently large. The analysis provides critical\ninsights on relay deployment in the implementation of cooperative FL. Extensive\nnumerical results show that our design achieves faster convergence compared\nwith state-of-the-art schemes.",
          "link": "http://arxiv.org/abs/2107.09518",
          "publishedOn": "2021-07-21T02:01:36.023Z",
          "wordCount": 721,
          "title": "Relay-Assisted Cooperative Federated Learning. (arXiv:2107.09518v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09509",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nath_R/0/1/0/all/0/1\">Rajdeep Kumar Nath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thapliyal_H/0/1/0/all/0/1\">Himanshu Thapliyal</a>",
          "description": "The advent of IoT has enabled the design of connected and integrated smart\nhealth monitoring systems. These smart health monitoring systems could be\nrealized in a smart home context to render long-term care to the elderly\npopulation. In this paper, we present the design of a wearable health\nmonitoring system suitable for older adults in a smart home context. The\nproposed system offers solutions to monitor the stress, blood pressure, and\nlocation of an individual within a smart home environment. The stress detection\nmodel proposed in this work uses Electrodermal Activity (EDA),\nPhotoplethysmogram (PPG), and Skin Temperature (ST) sensors embedded in a smart\nwristband for detecting physiological stress. The stress detection model is\ntrained and tested using stress labels obtained from salivary cortisol which is\na clinically established biomarker for physiological stress. A voice-based\nprototype is also implemented and the feasibility of the proposed system for\nintegration in a smart home environment is analyzed by simulating a data\nacquisition and streaming scenario. We have also proposed a blood pressure\nestimation model using PPG signal and advanced regression techniques for\nintegration with the stress detection model in the wearable health monitoring\nsystem. Finally, the design of a voice-assisted indoor location system is\nproposed for integration with the proposed system within a smart home\nenvironment. The proposed wearable health monitoring system is an important\ndirection to realize a smart home environment with extensive diagnostic\ncapabilities so that such a system could be useful for rendering long-term and\npersonalized care to the aging population in the comfort of their home.",
          "link": "http://arxiv.org/abs/2107.09509",
          "publishedOn": "2021-07-21T02:01:36.014Z",
          "wordCount": 719,
          "title": "Wearable Health Monitoring System for Older Adults in a Smart Home Environment. (arXiv:2107.09509v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1\">James A. Michaelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bardolph_M/0/1/0/all/0/1\">Megan D. Bardolph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coulson_S/0/1/0/all/0/1\">Seana Coulson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin K. Bergen</a>",
          "description": "Despite being designed for performance rather than cognitive plausibility,\ntransformer language models have been found to be better at predicting metrics\nused to assess human language comprehension than language models with other\narchitectures, such as recurrent neural networks. Based on how well they\npredict the N400, a neural signal associated with processing difficulty, we\npropose and provide evidence for one possible explanation - their predictions\nare affected by the preceding context in a way analogous to the effect of\nsemantic facilitation in humans.",
          "link": "http://arxiv.org/abs/2107.09648",
          "publishedOn": "2021-07-21T02:01:35.987Z",
          "wordCount": 554,
          "title": "Different kinds of cognitive plausibility: why are transformers better than RNNs at predicting N400 amplitude?. (arXiv:2107.09648v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merchant_A/0/1/0/all/0/1\">Amil Merchant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metz_L/0/1/0/all/0/1\">Luke Metz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoenholz_S/0/1/0/all/0/1\">Sam Schoenholz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cubuk_E/0/1/0/all/0/1\">Ekin Dogus Cubuk</a>",
          "description": "Optimization of non-convex loss surfaces containing many local minima remains\na critical problem in a variety of domains, including operations research,\ninformatics, and material design. Yet, current techniques either require\nextremely high iteration counts or a large number of random restarts for good\nperformance. In this work, we propose adapting recent developments in\nmeta-learning to these many-minima problems by learning the optimization\nalgorithm for various loss landscapes. We focus on problems from atomic\nstructural optimization--finding low energy configurations of many-atom\nsystems--including widely studied models such as bimetallic clusters and\ndisordered silicon. We find that our optimizer learns a 'hopping' behavior\nwhich enables efficient exploration and improves the rate of low energy minima\ndiscovery. Finally, our learned optimizers show promising generalization with\nefficiency gains on never before seen tasks (e.g. new elements or\ncompositions). Code will be made available shortly.",
          "link": "http://arxiv.org/abs/2107.09661",
          "publishedOn": "2021-07-21T02:01:35.981Z",
          "wordCount": 574,
          "title": "Learn2Hop: Learned Optimization on Rough Landscapes. (arXiv:2107.09661v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Mingkai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>",
          "description": "Self-supervised Learning (SSL) including the mainstream contrastive learning\nhas achieved great success in learning visual representations without data\nannotations. However, most of methods mainly focus on the instance level\ninformation (\\ie, the different augmented images of the same instance should\nhave the same feature or cluster into the same class), but there is a lack of\nattention on the relationships between different instances. In this paper, we\nintroduced a novel SSL paradigm, which we term as relational self-supervised\nlearning (ReSSL) framework that learns representations by modeling the\nrelationship between different instances. Specifically, our proposed method\nemploys sharpened distribution of pairwise similarities among different\ninstances as \\textit{relation} metric, which is thus utilized to match the\nfeature embeddings of different augmentations. Moreover, to boost the\nperformance, we argue that weak augmentations matter to represent a more\nreliable relation, and leverage momentum strategy for practical efficiency.\nExperimental results show that our proposed ReSSL significantly outperforms the\nprevious state-of-the-art algorithms in terms of both performance and training\nefficiency. Code is available at \\url{https://github.com/KyleZheng1997/ReSSL}.",
          "link": "http://arxiv.org/abs/2107.09282",
          "publishedOn": "2021-07-21T02:01:35.973Z",
          "wordCount": 615,
          "title": "ReSSL: Relational Self-Supervised Learning with Weak Augmentation. (arXiv:2107.09282v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Ling Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Choy Heng Lai</a>",
          "description": "The success of deep neural networks in real-world problems has prompted many\nattempts to explain their training dynamics and generalization performance, but\nmore guiding principles for the training of neural networks are still needed.\nMotivated by the edge of chaos principle behind the optimal performance of\nneural networks, we study the role of various hyperparameters in modern neural\nnetwork training algorithms in terms of the order-chaos phase diagram. In\nparticular, we study a fully analytical feedforward neural network trained on\nthe widely adopted Fashion-MNIST dataset, and study the dynamics associated\nwith the hyperparameters in back-propagation during the training process. We\nfind that for the basic algorithm of stochastic gradient descent with momentum,\nin the range around the commonly used hyperparameter values, clear scaling\nrelations are present with respect to the training time during the ordered\nphase in the phase diagram, and the model's optimal generalization power at the\nedge of chaos is similar across different training parameter combinations. In\nthe chaotic phase, the same scaling no longer exists. The scaling allows us to\nchoose the training parameters to achieve faster training without sacrificing\nperformance. In addition, we find that the commonly used model regularization\nmethod - weight decay - effectively pushes the model towards the ordered phase\nto achieve better performance. Leveraging on this fact and the scaling\nrelations in the other hyperparameters, we derived a principled guideline for\nhyperparameter determination, such that the model can achieve optimal\nperformance by saturating it at the edge of chaos. Demonstrated on this simple\nneural network model and training algorithm, our work improves the\nunderstanding of neural network training dynamics, and can potentially be\nextended to guiding principles of more complex model architectures and\nalgorithms.",
          "link": "http://arxiv.org/abs/2107.09437",
          "publishedOn": "2021-07-21T02:01:35.966Z",
          "wordCount": 740,
          "title": "Edge of chaos as a guiding principle for modern neural network training. (arXiv:2107.09437v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi-Fu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jaesik Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungjin Ahn</a>",
          "description": "Transformers have been successful for many natural language processing tasks.\nHowever, applying transformers to the video domain for tasks such as long-term\nvideo generation and scene understanding has remained elusive due to the high\ncomputational complexity and the lack of natural tokenization. In this paper,\nwe propose the Object-Centric Video Transformer (OCVT) which utilizes an\nobject-centric approach for decomposing scenes into tokens suitable for use in\na generative video transformer. By factoring the video into objects, our fully\nunsupervised model is able to learn complex spatio-temporal dynamics of\nmultiple interacting objects in a scene and generate future frames of the\nvideo. Our model is also significantly more memory-efficient than pixel-based\nmodels and thus able to train on videos of length up to 70 frames with a single\n48GB GPU. We compare our model with previous RNN-based approaches as well as\nother possible video transformer baselines. We demonstrate OCVT performs well\nwhen compared to baselines in generating future frames. OCVT also develops\nuseful representations for video reasoning, achieving start-of-the-art\nperformance on the CATER task.",
          "link": "http://arxiv.org/abs/2107.09240",
          "publishedOn": "2021-07-21T02:01:35.947Z",
          "wordCount": 618,
          "title": "Generative Video Transformer: Can Objects be the Words?. (arXiv:2107.09240v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09224",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiuyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1\">Ian Osband</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1\">Benjamin Van Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zheng Wen</a>",
          "description": "A fundamental challenge for any intelligent system is prediction: given some\ninputs $X_1,..,X_\\tau$ can you predict outcomes $Y_1,.., Y_\\tau$. The KL\ndivergence $\\mathbf{d}_{\\mathrm{KL}}$ provides a natural measure of prediction\nquality, but the majority of deep learning research looks only at the marginal\npredictions per input $X_t$. In this technical report we propose a scoring rule\n$\\mathbf{d}_{\\mathrm{KL}}^\\tau$, parameterized by $\\tau \\in \\mathcal{N}$ that\nevaluates the joint predictions at $\\tau$ inputs simultaneously. We show that\nthe commonly-used $\\tau=1$ can be insufficient to drive good decisions in many\nsettings of interest. We also show that, as $\\tau$ grows, performing well\naccording to $\\mathbf{d}_{\\mathrm{KL}}^\\tau$ recovers universal guarantees for\nany possible decision. Finally, we provide problem-dependent guidance on the\nscale of $\\tau$ for which our score provides sufficient guarantees for good\nperformance.",
          "link": "http://arxiv.org/abs/2107.09224",
          "publishedOn": "2021-07-21T02:01:35.941Z",
          "wordCount": 566,
          "title": "Evaluating Probabilistic Inference in Deep Learning: Beyond Marginal Predictions. (arXiv:2107.09224v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09402",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Safiuddin_M/0/1/0/all/0/1\">Mohammad Safiuddin</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Reddy_C/0/1/0/all/0/1\">CH Likith Reddy</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Vasantada_G/0/1/0/all/0/1\">Ganesh Vasantada</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Harsha_C/0/1/0/all/0/1\">CHJNS Harsha</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Gangolu_S/0/1/0/all/0/1\">Srinu Gangolu</a>",
          "description": "The microstructure of material strongly influences its mechanical properties\nand the microstructure itself is influenced by the processing conditions. Thus,\nestablishing a Process-Structure-Property relationship is a crucial task in\nmaterial design and is of interest in many engineering applications. We develop\na GAN (Generative Adversarial Network) to synthesize microstructures based on\ngiven processing conditions. This approach is devoid of feature engineering,\nneeds little domain awareness, and can be applied to a wide variety of material\nsystems. Results show that our GAN model can produce high-fidelity multi-phase\nmicrostructures which have a good correlation with the given processing\nconditions.",
          "link": "http://arxiv.org/abs/2107.09402",
          "publishedOn": "2021-07-21T02:01:35.934Z",
          "wordCount": 537,
          "title": "Establishing process-structure linkages using Generative Adversarial Networks. (arXiv:2107.09402v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09301",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mourdoukoutas_N/0/1/0/all/0/1\">Nikolaos Mourdoukoutas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Federici_M/0/1/0/all/0/1\">Marco Federici</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pantalos_G/0/1/0/all/0/1\">Georges Pantalos</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wilk_M/0/1/0/all/0/1\">Mark van der Wilk</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>",
          "description": "We propose a novel Bayesian neural network architecture that can learn\ninvariances from data alone by inferring a posterior distribution over\ndifferent weight-sharing schemes. We show that our model outperforms other\nnon-invariant architectures, when trained on datasets that contain specific\ninvariances. The same holds true when no data augmentation is performed.",
          "link": "http://arxiv.org/abs/2107.09301",
          "publishedOn": "2021-07-21T02:01:35.929Z",
          "wordCount": 501,
          "title": "A Bayesian Approach to Invariant Deep Neural Networks. (arXiv:2107.09301v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09392",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hu_C/0/1/0/all/0/1\">Cheng-Hung Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1\">Yu-Huai Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>",
          "description": "Neural evaluation metrics derived for numerous speech generation tasks have\nrecently attracted great attention. In this paper, we propose SVSNet, the first\nend-to-end neural network model to assess the speaker voice similarity between\nnatural speech and synthesized speech. Unlike most neural evaluation metrics\nthat use hand-crafted features, SVSNet directly takes the raw waveform as input\nto more completely utilize speech information for prediction. SVSNet consists\nof encoder, co-attention, distance calculation, and prediction modules and is\ntrained in an end-to-end manner. The experimental results on the Voice\nConversion Challenge 2018 and 2020 (VCC2018 and VCC2020) datasets show that\nSVSNet notably outperforms well-known baseline systems in the assessment of\nspeaker similarity at the utterance and system levels.",
          "link": "http://arxiv.org/abs/2107.09392",
          "publishedOn": "2021-07-21T02:01:35.922Z",
          "wordCount": 589,
          "title": "SVSNet: An End-to-end Speaker Voice Similarity Assessment Model. (arXiv:2107.09392v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhize Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1\">Peter Richt&#xe1;rik</a>",
          "description": "Due to the high communication cost in distributed and federated learning,\nmethods relying on compressed communication are becoming increasingly popular.\nBesides, the best theoretically and practically performing gradient-type\nmethods invariably rely on some form of acceleration/momentum to reduce the\nnumber of communications (faster convergence), e.g., Nesterov's accelerated\ngradient descent (Nesterov, 2004) and Adam (Kingma and Ba, 2014). In order to\ncombine the benefits of communication compression and convergence acceleration,\nwe propose a \\emph{compressed and accelerated} gradient method for distributed\noptimization, which we call CANITA. Our CANITA achieves the \\emph{first\naccelerated rate}\n$O\\bigg(\\sqrt{\\Big(1+\\sqrt{\\frac{\\omega^3}{n}}\\Big)\\frac{L}{\\epsilon}} +\n\\omega\\big(\\frac{1}{\\epsilon}\\big)^{\\frac{1}{3}}\\bigg)$, which improves upon\nthe state-of-the-art non-accelerated rate\n$O\\left((1+\\frac{\\omega}{n})\\frac{L}{\\epsilon} +\n\\frac{\\omega^2+n}{\\omega+n}\\frac{1}{\\epsilon}\\right)$ of DIANA (Khaled et al.,\n2020b) for distributed general convex problems, where $\\epsilon$ is the target\nerror, $L$ is the smooth parameter of the objective, $n$ is the number of\nmachines/devices, and $\\omega$ is the compression parameter (larger $\\omega$\nmeans more compression can be applied, and no compression implies $\\omega=0$).\nOur results show that as long as the number of devices $n$ is large (often true\nin distributed/federated learning), or the compression $\\omega$ is not very\nhigh, CANITA achieves the faster convergence rate\n$O\\Big(\\sqrt{\\frac{L}{\\epsilon}}\\Big)$, i.e., the number of communication\nrounds is $O\\Big(\\sqrt{\\frac{L}{\\epsilon}}\\Big)$ (vs.\n$O\\big(\\frac{L}{\\epsilon}\\big)$ achieved by previous works). As a result,\nCANITA enjoys the advantages of both compression (compressed communication in\neach round) and acceleration (much fewer communication rounds).",
          "link": "http://arxiv.org/abs/2107.09461",
          "publishedOn": "2021-07-21T02:01:35.903Z",
          "wordCount": 676,
          "title": "CANITA: Faster Rates for Distributed Convex Optimization with Communication Compression. (arXiv:2107.09461v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Riera_M/0/1/0/all/0/1\">Marc Riera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnau_J/0/1/0/all/0/1\">Jose-Maria Arnau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_A/0/1/0/all/0/1\">Antonio Gonzalez</a>",
          "description": "Deep Neural Networks (DNNs) have achieved tremendous success for cognitive\napplications. The core operation in a DNN is the dot product between quantized\ninputs and weights. Prior works exploit the weight/input repetition that arises\ndue to quantization to avoid redundant computations in Convolutional Neural\nNetworks (CNNs). However, in this paper we show that their effectiveness is\nseverely limited when applied to Fully-Connected (FC) layers, which are\ncommonly used in state-of-the-art DNNs, as it is the case of modern Recurrent\nNeural Networks (RNNs) and Transformer models.\n\nTo improve energy-efficiency of FC computation we present CREW, a hardware\naccelerator that implements Computation Reuse and an Efficient Weight Storage\nmechanism to exploit the large number of repeated weights in FC layers. CREW\nfirst performs the multiplications of the unique weights by their respective\ninputs and stores the results in an on-chip buffer. The storage requirements\nare modest due to the small number of unique weights and the relatively small\nsize of the input compared to convolutional layers. Next, CREW computes each\noutput by fetching and adding its required products. To this end, each weight\nis replaced offline by an index in the buffer of unique products. Indices are\ntypically smaller than the quantized weights, since the number of unique\nweights for each input tends to be much lower than the range of quantized\nweights, which reduces storage and memory bandwidth requirements.\n\nOverall, CREW greatly reduces the number of multiplications and provides\nsignificant savings in model memory footprint and memory bandwidth usage. We\nevaluate CREW on a diverse set of modern DNNs. On average, CREW provides 2.61x\nspeedup and 2.42x energy savings over a TPU-like accelerator. Compared to UCNN,\na state-of-art computation reuse technique, CREW achieves 2.10x speedup and\n2.08x energy savings on average.",
          "link": "http://arxiv.org/abs/2107.09408",
          "publishedOn": "2021-07-21T02:01:35.897Z",
          "wordCount": 730,
          "title": "CREW: Computation Reuse and Efficient Weight Storage for Hardware-accelerated MLPs and RNNs. (arXiv:2107.09408v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09384",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ostwald_D/0/1/0/all/0/1\">Dirk Ostwald</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Usee_F/0/1/0/all/0/1\">Franziska Us&#xe9;e</a>",
          "description": "Backpropagation (BP) is a core component of the contemporary deep learning\nincarnation of neural networks. Briefly, BP is an algorithm that exploits the\ncomputational architecture of neural networks to efficiently evaluate the\ngradient of a cost function during neural network parameter optimization. The\nvalidity of BP rests on the application of a multivariate chain rule to the\ncomputational architecture of neural networks and their associated objective\nfunctions. Introductions to deep learning theory commonly present the\ncomputational architecture of neural networks in matrix form, but eschew a\nparallel formulation and justification of BP in the framework of matrix\ndifferential calculus. This entails several drawbacks for the theory and\ndidactics of deep learning. In this work, we overcome these limitations by\nproviding a full induction proof of the BP algorithm in matrix notation.\nSpecifically, we situate the BP algorithm in the framework of matrix\ndifferential calculus, encompass affine-linear potential functions, prove the\nvalidity of the BP algorithm in inductive form, and exemplify the\nimplementation of the matrix form BP algorithm in computer code.",
          "link": "http://arxiv.org/abs/2107.09384",
          "publishedOn": "2021-07-21T02:01:35.890Z",
          "wordCount": 616,
          "title": "An induction proof of the backpropagation algorithm in matrix notation. (arXiv:2107.09384v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Delong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaomin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zewen Li</a>",
          "description": "Computational intelligence-based ocean characteristics forecasting\napplications, such as Significant Wave Height (SWH) prediction, are crucial for\navoiding social and economic loss in coastal cities. Compared to the\ntraditional empirical-based or numerical-based forecasting models, \"soft\ncomputing\" approaches, including machine learning and deep learning models,\nhave shown numerous success in recent years. In this paper, we focus on\nenabling the deep learning model to learn both short-term and long-term\nspatial-temporal dependencies for SWH prediction. A Wavelet Graph Neural\nNetwork (WGNN) approach is proposed to integrate the advantages of wavelet\ntransform and graph neural network. Several parallel graph neural networks are\nseparately trained on wavelet decomposed data, and the reconstruction of each\nmodel's prediction forms the final SWH prediction. Experimental results show\nthat the proposed WGNN approach outperforms other models, including the\nnumerical models, the machine learning models, and several deep learning\nmodels.",
          "link": "http://arxiv.org/abs/2107.09483",
          "publishedOn": "2021-07-21T02:01:35.883Z",
          "wordCount": 596,
          "title": "Significant Wave Height Prediction based on Wavelet Graph Neural Network. (arXiv:2107.09483v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tetali_H/0/1/0/all/0/1\">Harsha Vardhan Tetali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harley_J/0/1/0/all/0/1\">Joel B. Harley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haeffele_B/0/1/0/all/0/1\">Benjamin D. Haeffele</a>",
          "description": "With the recent success of representation learning methods, which includes\ndeep learning as a special case, there has been considerable interest in\ndeveloping representation learning techniques that can incorporate known\nphysical constraints into the learned representation. As one example, in many\napplications that involve a signal propagating through physical media (e.g.,\noptics, acoustics, fluid dynamics, etc), it is known that the dynamics of the\nsignal must satisfy constraints imposed by the wave equation. Here we propose a\nmatrix factorization technique that decomposes such signals into a sum of\ncomponents, where each component is regularized to ensure that it satisfies\nwave equation constraints. Although our proposed formulation is non-convex, we\nprove that our model can be efficiently solved to global optimality in\npolynomial time. We demonstrate the benefits of our work by applications in\nstructural health monitoring, where prior work has attempted to solve this\nproblem using sparse dictionary learning approaches that do not come with any\ntheoretical guarantees regarding convergence to global optimality and employ\nheuristics to capture desired physical constraints.",
          "link": "http://arxiv.org/abs/2107.09144",
          "publishedOn": "2021-07-21T02:01:35.877Z",
          "wordCount": 599,
          "title": "Wave-Informed Matrix Factorization withGlobal Optimality Guarantees. (arXiv:2107.09144v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Wenxian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuxuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Deep neural networks often have a huge number of parameters, which posts\nchallenges in deployment in application scenarios with limited memory and\ncomputation capacity. Knowledge distillation is one approach to derive compact\nmodels from bigger ones. However, it has been observed that a converged heavy\nteacher model is strongly constrained for learning a compact student network\nand could make the optimization subject to poor local optima. In this paper, we\npropose ProKT, a new model-agnostic method by projecting the supervision\nsignals of a teacher model into the student's parameter space. Such projection\nis implemented by decomposing the training objective into local intermediate\ntargets with an approximate mirror descent technique. The proposed method could\nbe less sensitive with the quirks during optimization which could result in a\nbetter local optimum. Experiments on both image and text datasets show that our\nproposed ProKT consistently achieves superior performance compared to other\nexisting knowledge distillation methods.",
          "link": "http://arxiv.org/abs/2107.09305",
          "publishedOn": "2021-07-21T02:01:35.858Z",
          "wordCount": 597,
          "title": "Follow Your Path: a Progressive Method for Knowledge Distillation. (arXiv:2107.09305v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09200",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Zlokapa_A/0/1/0/all/0/1\">Alexander Zlokapa</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Neven_H/0/1/0/all/0/1\">Hartmut Neven</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lloyd_S/0/1/0/all/0/1\">Seth Lloyd</a>",
          "description": "Given the success of deep learning in classical machine learning, quantum\nalgorithms for traditional neural network architectures may provide one of the\nmost promising settings for quantum machine learning. Considering a\nfully-connected feedforward neural network, we show that conditions amenable to\nclassical trainability via gradient descent coincide with those necessary for\nefficiently solving quantum linear systems. We propose a quantum algorithm to\napproximately train a wide and deep neural network up to $O(1/n)$ error for a\ntraining set of size $n$ by performing sparse matrix inversion in $O(\\log n)$\ntime. To achieve an end-to-end exponential speedup over gradient descent, the\ndata distribution must permit efficient state preparation and readout. We\nnumerically demonstrate that the MNIST image dataset satisfies such conditions;\nmoreover, the quantum algorithm matches the accuracy of the fully-connected\nnetwork. Beyond the proven architecture, we provide empirical evidence for\n$O(\\log n)$ training of a convolutional neural network with pooling.",
          "link": "http://arxiv.org/abs/2107.09200",
          "publishedOn": "2021-07-21T02:01:35.851Z",
          "wordCount": 602,
          "title": "A quantum algorithm for training wide and deep classical neural networks. (arXiv:2107.09200v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09194",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Stephenson_W/0/1/0/all/0/1\">William T. Stephenson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Frangella_Z/0/1/0/all/0/1\">Zachary Frangella</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Udell_M/0/1/0/all/0/1\">Madeleine Udell</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Broderick_T/0/1/0/all/0/1\">Tamara Broderick</a>",
          "description": "Models like LASSO and ridge regression are extensively used in practice due\nto their interpretability, ease of use, and strong theoretical guarantees.\nCross-validation (CV) is widely used for hyperparameter tuning in these models,\nbut do practical optimization methods minimize the true out-of-sample loss? A\nrecent line of research promises to show that the optimum of the CV loss\nmatches the optimum of the out-of-sample loss (possibly after simple\ncorrections). It remains to show how tractable it is to minimize the CV loss.\nIn the present paper, we show that, in the case of ridge regression, the CV\nloss may fail to be quasiconvex and thus may have multiple local optima. We can\nguarantee that the CV loss is quasiconvex in at least one case: when the\nspectrum of the covariate matrix is nearly flat and the noise in the observed\nresponses is not too high. More generally, we show that quasiconvexity status\nis independent of many properties of the observed data (response norm,\ncovariate-matrix right singular vectors and singular-value scaling) and has a\ncomplex dependence on the few that remain. We empirically confirm our theory\nusing simulated experiments.",
          "link": "http://arxiv.org/abs/2107.09194",
          "publishedOn": "2021-07-21T02:01:35.844Z",
          "wordCount": 635,
          "title": "Can we globally optimize cross-validation loss? Quasiconvexity in ridge regression. (arXiv:2107.09194v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siblini_W/0/1/0/all/0/1\">Wissam Siblini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coter_G/0/1/0/all/0/1\">Guillaume Coter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabry_R/0/1/0/all/0/1\">R&#xe9;my Fabry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Guelton_L/0/1/0/all/0/1\">Liyun He-Guelton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oble_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Obl&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lebichot_B/0/1/0/all/0/1\">Bertrand Lebichot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgne_Y/0/1/0/all/0/1\">Yann-A&#xeb;l Le Borgne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bontempi_G/0/1/0/all/0/1\">Gianluca Bontempi</a>",
          "description": "The dark face of digital commerce generalization is the increase of fraud\nattempts. To prevent any type of attacks, state of the art fraud detection\nsystems are now embedding Machine Learning (ML) modules. The conception of such\nmodules is only communicated at the level of research and papers mostly focus\non results for isolated benchmark datasets and metrics. But research is only a\npart of the journey, preceded by the right formulation of the business problem\nand collection of data, and followed by a practical integration. In this paper,\nwe give a wider vision of the process, on a case study of transfer learning for\nfraud detection, from business to research, and back to business.",
          "link": "http://arxiv.org/abs/2107.09323",
          "publishedOn": "2021-07-21T02:01:35.837Z",
          "wordCount": 568,
          "title": "Transfer Learning for Credit Card Fraud Detection: A Journey from Research to Production. (arXiv:2107.09323v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09088",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Strupl_M/0/1/0/all/0/1\">Miroslav &#x160;trupl</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Faccio_F/0/1/0/all/0/1\">Francesco Faccio</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ashley_D/0/1/0/all/0/1\">Dylan R. Ashley</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Srivastava_R/0/1/0/all/0/1\">Rupesh Kumar Srivastava</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schmidhuber_J/0/1/0/all/0/1\">J&#xfc;rgen Schmidhuber</a>",
          "description": "Reward-Weighted Regression (RWR) belongs to a family of widely known\niterative Reinforcement Learning algorithms based on the\nExpectation-Maximization framework. In this family, learning at each iteration\nconsists of sampling a batch of trajectories using the current policy and\nfitting a new policy to maximize a return-weighted log-likelihood of actions.\nAlthough RWR is known to yield monotonic improvement of the policy under\ncertain circumstances, whether and under which conditions RWR converges to the\noptimal policy have remained open questions. In this paper, we provide for the\nfirst time a proof that RWR converges to a global optimum when no function\napproximation is used.",
          "link": "http://arxiv.org/abs/2107.09088",
          "publishedOn": "2021-07-21T02:01:35.829Z",
          "wordCount": 575,
          "title": "Reward-Weighted Regression Converges to a Global Optimum. (arXiv:2107.09088v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Souza_M/0/1/0/all/0/1\">Mila Soares de Oliveira de Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moura_P/0/1/0/all/0/1\">Pedro Nuno de Souza Moura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briot_J/0/1/0/all/0/1\">Jean-Pierre Briot</a>",
          "description": "This paper presents a comparative analysis on two artificial neural networks\n(with different architectures) for the task of tempo estimation. For this\npurpose, it also proposes the modeling, training and evaluation of a B-RNN\n(Bidirectional Recurrent Neural Network) model capable of estimating tempo in\nbpm (beats per minutes) of musical pieces, without using external auxiliary\nmodules. An extensive database (12,550 pieces in total) was curated to conduct\na quantitative and qualitative analysis over the experiment. Percussion-only\ntracks were also included in the dataset. The performance of the B-RNN is\ncompared to that of state-of-the-art models. For further comparison, a\nstate-of-the-art CNN was also retrained with the same datasets used for the\nB-RNN training. Evaluation results for each model and datasets are presented\nand discussed, as well as observations and ideas for future research. Tempo\nestimation was more accurate for the percussion only dataset, suggesting that\nthe estimation can be more accurate for percussion-only tracks, although\nfurther experiments (with more of such datasets) should be made to gather\nstronger evidence.",
          "link": "http://arxiv.org/abs/2107.09208",
          "publishedOn": "2021-07-21T02:01:35.810Z",
          "wordCount": 621,
          "title": "Music Tempo Estimation via Neural Networks -- A Comparative Analysis. (arXiv:2107.09208v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1\">Arya Mazumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Soumyabrata Pal</a>",
          "description": "One-bit compressed sensing (1bCS) is an extreme-quantized signal acquisition\nmethod that has been widely studied in the past decade. In 1bCS, linear samples\nof a high dimensional signal are quantized to only one bit per sample (sign of\nthe measurement). Assuming the original signal vector to be sparse, existing\nresults either aim to find the support of the vector, or approximate the signal\nwithin an $\\epsilon$-ball. The focus of this paper is support recovery, which\noften also computationally facilitates approximate signal recovery. A universal\nmeasurement matrix for 1bCS refers to one set of measurements that work for all\nsparse signals. With universality, it is known that $\\tilde{\\Theta}(k^2)$ 1bCS\nmeasurements are necessary and sufficient for support recovery (where $k$\ndenotes the sparsity). In this work, we show that it is possible to universally\nrecover the support with a small number of false positives with\n$\\tilde{O}(k^{3/2})$ measurements. If the dynamic range of the signal vector is\nknown, then with a different technique, this result can be improved to only\n$\\tilde{O}(k)$ measurements. Further results on support recovery are also\nprovided.",
          "link": "http://arxiv.org/abs/2107.09091",
          "publishedOn": "2021-07-21T02:01:35.804Z",
          "wordCount": 620,
          "title": "Support Recovery in Universal One-bit Compressed Sensing. (arXiv:2107.09091v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eloff_K/0/1/0/all/0/1\">Kevin Eloff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelbrecht_H/0/1/0/all/0/1\">Herman Engelbrecht</a>",
          "description": "Communication between agents in collaborative multi-agent settings is in\ngeneral implicit or a direct data stream. This paper considers text-based\nnatural language as a novel form of communication between multiple agents\ntrained with reinforcement learning. This could be considered first steps\ntoward a truly autonomous communication without the need to define a limited\nset of instructions, and natural collaboration between humans and robots.\nInspired by the game of Blind Leads, we propose an environment where one agent\nuses natural language instructions to guide another through a maze. We test the\nability of reinforcement learning agents to effectively communicate through\ndiscrete word-level symbols and show that the agents are able to sufficiently\ncommunicate through natural language with a limited vocabulary. Although the\ncommunication is not always perfect English, the agents are still able to\nnavigate the maze. We achieve a BLEU score of 0.85, which is an improvement of\n0.61 over randomly generated sequences while maintaining a 100% maze completion\nrate. This is a 3.5 times the performance of the random baseline using our\nreference set.",
          "link": "http://arxiv.org/abs/2107.09356",
          "publishedOn": "2021-07-21T02:01:35.797Z",
          "wordCount": 665,
          "title": "Toward Collaborative Reinforcement Learning Agents that Communicate Through Text-Based Natural Language. (arXiv:2107.09356v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Spantidi_O/0/1/0/all/0/1\">Ourania Spantidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zervakis_G/0/1/0/all/0/1\">Georgios Zervakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anagnostopoulos_I/0/1/0/all/0/1\">Iraklis Anagnostopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amrouch_H/0/1/0/all/0/1\">Hussam Amrouch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henkel_J/0/1/0/all/0/1\">J&#xf6;rg Henkel</a>",
          "description": "Recent Deep Neural Networks (DNNs) managed to deliver superhuman accuracy\nlevels on many AI tasks. Several applications rely more and more on DNNs to\ndeliver sophisticated services and DNN accelerators are becoming integral\ncomponents of modern systems-on-chips. DNNs perform millions of arithmetic\noperations per inference and DNN accelerators integrate thousands of\nmultiply-accumulate units leading to increased energy requirements. Approximate\ncomputing principles are employed to significantly lower the energy consumption\nof DNN accelerators at the cost of some accuracy loss. Nevertheless, recent\nresearch demonstrated that complex DNNs are increasingly sensitive to\napproximation. Hence, the obtained energy savings are often limited when\ntargeting tight accuracy constraints. In this work, we present a dynamically\nconfigurable approximate multiplier that supports three operation modes, i.e.,\nexact, positive error, and negative error. In addition, we propose a\nfilter-oriented approximation method to map the weights to the appropriate\nmodes of the approximate multiplier. Our mapping algorithm balances the\npositive with the negative errors due to the approximate multiplications,\naiming at maximizing the energy reduction while minimizing the overall\nconvolution error. We evaluate our approach on multiple DNNs and datasets\nagainst state-of-the-art approaches, where our method achieves 18.33% energy\ngains on average across 7 NNs on 4 different datasets for a maximum accuracy\ndrop of only 1%.",
          "link": "http://arxiv.org/abs/2107.09366",
          "publishedOn": "2021-07-21T02:01:35.789Z",
          "wordCount": 656,
          "title": "Positive/Negative Approximate Multipliers for DNN Accelerators. (arXiv:2107.09366v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Sanchita Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prevost_J/0/1/0/all/0/1\">John J. Prevost</a>",
          "description": "Deep learning based visual to sound generation systems essentially need to be\ndeveloped particularly considering the synchronicity aspects of visual and\naudio features with time. In this research we introduce a novel task of guiding\na class conditioned generative adversarial network with the temporal visual\ninformation of a video input for visual to sound generation task adapting the\nsynchronicity traits between audio-visual modalities. Our proposed FoleyGAN\nmodel is capable of conditioning action sequences of visual events leading\ntowards generating visually aligned realistic sound tracks. We expand our\npreviously proposed Automatic Foley dataset to train with FoleyGAN and evaluate\nour synthesized sound through human survey that shows noteworthy (on average\n81\\%) audio-visual synchronicity performance. Our approach also outperforms in\nstatistical experiments compared with other baseline models and audio-visual\ndatasets.",
          "link": "http://arxiv.org/abs/2107.09262",
          "publishedOn": "2021-07-21T02:01:35.782Z",
          "wordCount": 610,
          "title": "FoleyGAN: Visually Guided Generative Adversarial Network-Based Synchronous Sound Generation in Silent Videos. (arXiv:2107.09262v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Addanki_R/0/1/0/all/0/1\">Ravichandra Addanki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1\">Peter W. Battaglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budden_D/0/1/0/all/0/1\">David Budden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deac_A/0/1/0/all/0/1\">Andreea Deac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godwin_J/0/1/0/all/0/1\">Jonathan Godwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keck_T/0/1/0/all/0/1\">Thomas Keck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wai Lok Sibon Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Gonzalez_A/0/1/0/all/0/1\">Alvaro Sanchez-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stott_J/0/1/0/all/0/1\">Jacklynn Stott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakoor_S/0/1/0/all/0/1\">Shantanu Thakoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1\">Petar Veli&#x10d;kovi&#x107;</a>",
          "description": "Effectively and efficiently deploying graph neural networks (GNNs) at scale\nremains one of the most challenging aspects of graph representation learning.\nMany powerful solutions have only ever been validated on comparatively small\ndatasets, often with counter-intuitive outcomes -- a barrier which has been\nbroken by the Open Graph Benchmark Large-Scale Challenge (OGB-LSC). We entered\nthe OGB-LSC with two large-scale GNNs: a deep transductive node classifier\npowered by bootstrapping, and a very deep (up to 50-layer) inductive graph\nregressor regularised by denoising objectives. Our models achieved an\naward-level (top-3) performance on both the MAG240M and PCQM4M benchmarks. In\ndoing so, we demonstrate evidence of scalable self-supervised graph\nrepresentation learning, and utility of very deep GNNs -- both very important\nopen issues. Our code is publicly available at:\nhttps://github.com/deepmind/deepmind-research/tree/master/ogb_lsc.",
          "link": "http://arxiv.org/abs/2107.09422",
          "publishedOn": "2021-07-21T02:01:35.764Z",
          "wordCount": 610,
          "title": "Large-scale graph representation learning with very deep GNNs and self-supervision. (arXiv:2107.09422v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petersen_B/0/1/0/all/0/1\">Brenden K. Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santiago_C/0/1/0/all/0/1\">Claudio P. Santiago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larma_M/0/1/0/all/0/1\">Mikel Landajuela Larma</a>",
          "description": "Many AutoML problems involve optimizing discrete objects under a black-box\nreward. Neural-guided search provides a flexible means of searching these\ncombinatorial spaces using an autoregressive recurrent neural network. A major\nbenefit of this approach is that builds up objects sequentially--this provides\nan opportunity to incorporate domain knowledge into the search by directly\nmodifying the logits emitted during sampling. In this work, we formalize a\nframework for incorporating such in situ priors and constraints into\nneural-guided search, and provide sufficient conditions for enforcing\nconstraints. We integrate several priors and constraints from existing works\ninto this framework, propose several new ones, and demonstrate their efficacy\nin informing the task of symbolic regression.",
          "link": "http://arxiv.org/abs/2107.09182",
          "publishedOn": "2021-07-21T02:01:35.757Z",
          "wordCount": 537,
          "title": "Incorporating domain knowledge into neural-guided search. (arXiv:2107.09182v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09428",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1\">Tianzi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fujita_Y/0/1/0/all/0/1\">Yuya Fujita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>",
          "description": "Non-autoregressive (NAR) modeling has gained more and more attention in\nspeech processing. With recent state-of-the-art attention-based automatic\nspeech recognition (ASR) structure, NAR can realize promising real-time factor\n(RTF) improvement with only small degradation of accuracy compared to the\nautoregressive (AR) models. However, the recognition inference needs to wait\nfor the completion of a full speech utterance, which limits their applications\non low latency scenarios. To address this issue, we propose a novel end-to-end\nstreaming NAR speech recognition system by combining blockwise-attention and\nconnectionist temporal classification with mask-predict (Mask-CTC) NAR. During\ninference, the input audio is separated into small blocks and then processed in\na blockwise streaming way. To address the insertion and deletion error at the\nedge of the output of each block, we apply an overlapping decoding strategy\nwith a dynamic mapping trick that can produce more coherent sentences.\nExperimental results show that the proposed method improves online ASR\nrecognition in low latency conditions compared to vanilla Mask-CTC. Moreover,\nit can achieve a much faster inference speed compared to the AR attention-based\nmodels. All of our codes will be publicly available at\nhttps://github.com/espnet/espnet.",
          "link": "http://arxiv.org/abs/2107.09428",
          "publishedOn": "2021-07-21T02:01:35.750Z",
          "wordCount": 638,
          "title": "Streaming End-to-End ASR based on Blockwise Non-Autoregressive Models. (arXiv:2107.09428v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1\">Daniel Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">Daniel S. Brown</a>",
          "description": "We study how an offline dataset of prior (possibly random) experience can be\nused to address two challenges that autonomous systems face when they endeavor\nto learn from, adapt to, and collaborate with humans : (1) identifying the\nhuman's intent and (2) safely optimizing the autonomous system's behavior to\nachieve this inferred intent. First, we use the offline dataset to efficiently\ninfer the human's reward function via pool-based active preference learning.\nSecond, given this learned reward function, we perform offline reinforcement\nlearning to optimize a policy based on the inferred human intent. Crucially,\nour proposed approach does not require actual physical rollouts or an accurate\nsimulator for either the reward learning or policy optimization steps, enabling\nboth safe and efficient apprenticeship learning. We identify and evaluate our\napproach on a subset of existing offline RL benchmarks that are well suited for\noffline reward learning and also evaluate extensions of these benchmarks which\nallow more open-ended behaviors. Our experiments show that offline\npreference-based reward learning followed by offline reinforcement learning\nenables efficient and high-performing policies, while only requiring small\nnumbers of preference queries. Videos available at\nhttps://sites.google.com/view/offline-prefs.",
          "link": "http://arxiv.org/abs/2107.09251",
          "publishedOn": "2021-07-21T02:01:35.743Z",
          "wordCount": 619,
          "title": "OPAL: Offline Preference-Based Apprenticeship Learning. (arXiv:2107.09251v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Odema_M/0/1/0/all/0/1\">Mohanad Odema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_N/0/1/0/all/0/1\">Nafiul Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demirel_B/0/1/0/all/0/1\">Berken Utku Demirel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1\">Mohammad Abdullah Al Faruque</a>",
          "description": "Edge-Cloud hierarchical systems employing intelligence through Deep Neural\nNetworks (DNNs) endure the dilemma of workload distribution within them.\nPrevious solutions proposed to distribute workloads at runtime according to the\nstate of the surroundings, like the wireless conditions. However, such\nconditions are usually overlooked at design time. This paper addresses this\nissue for DNN architectural design by presenting a novel methodology, LENS,\nwhich administers multi-objective Neural Architecture Search (NAS) for\ntwo-tiered systems, where the performance objectives are refashioned to\nconsider the wireless communication parameters. From our experimental search\nspace, we demonstrate that LENS improves upon the traditional solution's Pareto\nset by 76.47% and 75% with respect to the energy and latency metrics,\nrespectively.",
          "link": "http://arxiv.org/abs/2107.09309",
          "publishedOn": "2021-07-21T02:01:35.736Z",
          "wordCount": 579,
          "title": "LENS: Layer Distribution Enabled Neural Architecture Search in Edge-Cloud Hierarchies. (arXiv:2107.09309v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stock_P/0/1/0/all/0/1\">Pierre Stock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gribonval_R/0/1/0/all/0/1\">R&#xe9;mi Gribonval</a>",
          "description": "Neural networks with the Rectified Linear Unit (ReLU) nonlinearity are\ndescribed by a vector of parameters $\\theta$, and realized as a piecewise\nlinear continuous function $R_{\\theta}: x \\in \\mathbb R^{d} \\mapsto\nR_{\\theta}(x) \\in \\mathbb R^{k}$. Natural scalings and permutations operations\non the parameters $\\theta$ leave the realization unchanged, leading to\nequivalence classes of parameters that yield the same realization. These\nconsiderations in turn lead to the notion of identifiability -- the ability to\nrecover (the equivalence class of) $\\theta$ from the sole knowledge of its\nrealization $R_{\\theta}$. The overall objective of this paper is to introduce\nan embedding for ReLU neural networks of any depth, $\\Phi(\\theta)$, that is\ninvariant to scalings and that provides a locally linear parameterization of\nthe realization of the network. Leveraging these two key properties, we derive\nsome conditions under which a deep ReLU network is indeed locally identifiable\nfrom the knowledge of the realization on a finite set of samples $x_{i} \\in\n\\mathbb R^{d}$. We study the shallow case in more depth, establishing necessary\nand sufficient conditions for the network to be identifiable from a bounded\nsubset $\\mathcal X \\subseteq \\mathbb R^{d}$.",
          "link": "http://arxiv.org/abs/2107.09370",
          "publishedOn": "2021-07-21T02:01:35.729Z",
          "wordCount": 618,
          "title": "An Embedding of ReLU Networks and an Analysis of their Identifiability. (arXiv:2107.09370v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Whitehead_S/0/1/0/all/0/1\">Spencer Whitehead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>",
          "description": "Generalization to out-of-distribution data has been a problem for Visual\nQuestion Answering (VQA) models. To measure generalization to novel questions,\nwe propose to separate them into \"skills\" and \"concepts\". \"Skills\" are visual\ntasks, such as counting or attribute recognition, and are applied to \"concepts\"\nmentioned in the question, such as objects and people. VQA methods should be\nable to compose skills and concepts in novel ways, regardless of whether the\nspecific composition has been seen in training, yet we demonstrate that\nexisting models have much to improve upon towards handling new compositions. We\npresent a novel method for learning to compose skills and concepts that\nseparates these two factors implicitly within a model by learning grounded\nconcept representations and disentangling the encoding of skills from that of\nconcepts. We enforce these properties with a novel contrastive learning\nprocedure that does not rely on external annotations and can be learned from\nunlabeled image-question pairs. Experiments demonstrate the effectiveness of\nour approach for improving compositional and grounding performance.",
          "link": "http://arxiv.org/abs/2107.09106",
          "publishedOn": "2021-07-21T02:01:35.711Z",
          "wordCount": 624,
          "title": "Separating Skills and Concepts for Novel Visual Question Answering. (arXiv:2107.09106v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09234",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1\">Angie Boggust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1\">Benjamin Hoover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satyanarayan_A/0/1/0/all/0/1\">Arvind Satyanarayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1\">Hendrik Strobelt</a>",
          "description": "Saliency methods -- techniques to identify the importance of input features\non a model's output -- are a common first step in understanding neural network\nbehavior. However, interpreting saliency requires tedious manual inspection to\nidentify and aggregate patterns in model behavior, resulting in ad hoc or\ncherry-picked analysis. To address these concerns, we present Shared Interest:\na set of metrics for comparing saliency with human annotated ground truths. By\nproviding quantitative descriptors, Shared Interest allows ranking, sorting,\nand aggregation of inputs thereby facilitating large-scale systematic analysis\nof model behavior. We use Shared Interest to identify eight recurring patterns\nin model behavior including focusing on a sufficient subset of ground truth\nfeatures or being distracted by contextual features. Working with\nrepresentative real-world users, we show how Shared Interest can be used to\nrapidly develop or lose trust in a model's reliability, uncover issues that are\nmissed in manual analyses, and enable interactive probing of model behavior.",
          "link": "http://arxiv.org/abs/2107.09234",
          "publishedOn": "2021-07-21T02:01:35.704Z",
          "wordCount": 605,
          "title": "Shared Interest: Large-Scale Visual Analysis of Model Behavior by Measuring Human-AI Alignment. (arXiv:2107.09234v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1\">Gabriel Kronberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kommenda_M/0/1/0/all/0/1\">Michael Kommenda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Promberger_A/0/1/0/all/0/1\">Andreas Promberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickel_F/0/1/0/all/0/1\">Falk Nickel</a>",
          "description": "Friction systems are mechanical systems wherein friction is used for force\ntransmission (e.g. mechanical braking systems or automatic gearboxes). For\nfinding optimal and safe design parameters, engineers have to predict friction\nsystem performance. This is especially difficult in real-world applications,\nbecause it is affected by many parameters. We have used symbolic regression and\ngenetic programming for finding accurate and trustworthy prediction models for\nthis task. However, it is not straight-forward how nominal variables can be\nincluded. In particular, a one-hot-encoding is unsatisfactory because genetic\nprogramming tends to remove such indicator variables. We have therefore used\nso-called factor variables for representing nominal variables in symbolic\nregression models. Our results show that GP is able to produce symbolic\nregression models for predicting friction performance with predictive accuracy\nthat is comparable to artificial neural networks. The symbolic regression\nmodels with factor variables are less complex than models using a one-hot\nencoding.",
          "link": "http://arxiv.org/abs/2107.09484",
          "publishedOn": "2021-07-21T02:01:35.697Z",
          "wordCount": 628,
          "title": "Predicting Friction System Performance with Symbolic Regression and Genetic Programming with Factor Variables. (arXiv:2107.09484v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_J/0/1/0/all/0/1\">Jo&#xe3;o Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tateo_D/0/1/0/all/0/1\">Davide Tateo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muratore_F/0/1/0/all/0/1\">Fabio Muratore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1\">Jan Peters</a>",
          "description": "Reinforcement learning methods for robotics are increasingly successful due\nto the constant development of better policy gradient techniques. A precise\n(low variance) and accurate (low bias) gradient estimator is crucial to face\nincreasingly complex tasks. Traditional policy gradient algorithms use the\nlikelihood-ratio trick, which is known to produce unbiased but high variance\nestimates. More modern approaches exploit the reparametrization trick, which\ngives lower variance gradient estimates but requires differentiable value\nfunction approximators. In this work, we study a different type of stochastic\ngradient estimator: the Measure-Valued Derivative. This estimator is unbiased,\nhas low variance, and can be used with differentiable and non-differentiable\nfunction approximators. We empirically evaluate this estimator in the\nactor-critic policy gradient setting and show that it can reach comparable\nperformance with methods based on the likelihood-ratio or reparametrization\ntricks, both in low and high-dimensional action spaces.",
          "link": "http://arxiv.org/abs/2107.09359",
          "publishedOn": "2021-07-21T02:01:35.690Z",
          "wordCount": 575,
          "title": "An Empirical Analysis of Measure-Valued Derivatives for Policy Gradients. (arXiv:2107.09359v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gulshad_S/0/1/0/all/0/1\">Sadaf Gulshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sosnovik_I/0/1/0/all/0/1\">Ivan Sosnovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1\">Arnold Smeulders</a>",
          "description": "We focus on building robustness in the convolutions of neural visual\nclassifiers, especially against natural perturbations like elastic\ndeformations, occlusions and Gaussian noise. Existing CNNs show outstanding\nperformance on clean images, but fail to tackle naturally occurring\nperturbations. In this paper, we start from elastic perturbations, which\napproximate (local) view-point changes of the object. We present\nelastically-augmented convolutions (EAConv) by parameterizing filters as a\ncombination of fixed elastically-perturbed bases functions and trainable\nweights for the purpose of integrating unseen viewpoints in the CNN. We show on\nCIFAR-10 and STL-10 datasets that the general robustness of our method on\nunseen occlusion and Gaussian perturbations improves, while even improving the\nperformance on clean images slightly without performing any data augmentation.",
          "link": "http://arxiv.org/abs/2107.09391",
          "publishedOn": "2021-07-21T02:01:35.682Z",
          "wordCount": 556,
          "title": "Built-in Elastic Transformations for Improved Robustness. (arXiv:2107.09391v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09286",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingzhong Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lirong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>",
          "description": "Recent studies show that advanced priors play a major role in deep generative\nmodels. Exemplar VAE, as a variant of VAE with an exemplar-based prior, has\nachieved impressive results. However, due to the nature of model design, an\nexemplar-based model usually requires vast amounts of data to participate in\ntraining, which leads to huge computational complexity. To address this issue,\nwe propose Bayesian Pseudocoresets Exemplar VAE (ByPE-VAE), a new variant of\nVAE with a prior based on Bayesian pseudocoreset. The proposed prior is\nconditioned on a small-scale pseudocoreset rather than the whole dataset for\nreducing the computational cost and avoiding overfitting. Simultaneously, we\nobtain the optimal pseudocoreset via a stochastic optimization algorithm during\nVAE training aiming to minimize the Kullback-Leibler divergence between the\nprior based on the pseudocoreset and that based on the whole dataset.\nExperimental results show that ByPE-VAE can achieve competitive improvements\nover the state-of-the-art VAEs in the tasks of density estimation,\nrepresentation learning, and generative data augmentation. Particularly, on a\nbasic VAE architecture, ByPE-VAE is up to 3 times faster than Exemplar VAE\nwhile almost holding the performance. Code is available at our supplementary\nmaterials.",
          "link": "http://arxiv.org/abs/2107.09286",
          "publishedOn": "2021-07-21T02:01:35.665Z",
          "wordCount": 612,
          "title": "ByPE-VAE: Bayesian Pseudocoresets Exemplar VAE. (arXiv:2107.09286v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amato_D/0/1/0/all/0/1\">Domenico Amato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giancarlo_R/0/1/0/all/0/1\">Raffaele Giancarlo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosco_G/0/1/0/all/0/1\">Giosu&#xe8; Lo Bosco</a>",
          "description": "Sorted Table Search Procedures are the quintessential query-answering tool,\nstill very useful, e.g, Search Engines (Google Chrome). Speeding them up, in\nsmall additional space with respect to the table being searched into, is still\na quite significant achievement. Static Learned Indexes have been very\nsuccessful in achieving such a speed-up, but leave open a major question: To\nwhat extent one can enjoy the speed-up of Learned Indexes while using constant\nor nearly constant additional space. By generalizing the experimental\nmethodology of a recent benchmarking study on Learned Indexes, we shed light on\nthis question, by considering two scenarios. The first, quite elementary, i.e.,\ntextbook code, and the second using advanced Learned Indexing algorithms and\nthe supporting sophisticated software platforms. Although in both cases one\nwould expect a positive answer, its achievement is not as simple as it seems.\nIndeed, our extensive set of experiments reveal a complex relationship between\nquery time and model space. The findings regarding this relationship and the\ncorresponding quantitative estimates, across memory levels, can be of interest\nto algorithm designers and of use to practitioners as well. As an essential\npart of our research, we introduce two new models that are of interest in their\nown right. The first is a constant space model that can be seen as a\ngeneralization of $k$-ary search, while the second is a synoptic {\\bf RMI}, in\nwhich we can control model space usage.",
          "link": "http://arxiv.org/abs/2107.09480",
          "publishedOn": "2021-07-21T02:01:35.658Z",
          "wordCount": 696,
          "title": "Learned Sorted Table Search and Static Indexes in Small Space: Methodological and Practical Insights via an Experimental Study. (arXiv:2107.09480v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09203",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gama_F/0/1/0/all/0/1\">Fernando Gama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1\">Alejandro Ribeiro</a>",
          "description": "Graph neural networks (GNNs) are naturally distributed architectures for\nlearning representations from network data. This renders them suitable\ncandidates for decentralized tasks. In these scenarios, the underlying graph\noften changes with time due to link failures or topology variations, creating a\nmismatch between the graphs on which GNNs were trained and the ones on which\nthey are tested. Online learning can be leveraged to retrain GNNs at testing\ntime to overcome this issue. However, most online algorithms are centralized\nand usually offer guarantees only on convex problems, which GNNs rarely lead\nto. This paper develops the Wide and Deep GNN (WD-GNN), a novel architecture\nthat can be updated with distributed online learning mechanisms. The WD-GNN\nconsists of two components: the wide part is a linear graph filter and the deep\npart is a nonlinear GNN. At training time, the joint wide and deep architecture\nlearns nonlinear representations from data. At testing time, the wide, linear\npart is retrained, while the deep, nonlinear one remains fixed. This often\nleads to a convex formulation. We further propose a distributed online learning\nalgorithm that can be implemented in a decentralized setting. We also show the\nstability of the WD-GNN to changes of the underlying graph and analyze the\nconvergence of the proposed online learning procedure. Experiments on movie\nrecommendation, source localization and robot swarm control corroborate\ntheoretical findings and show the potential of the WD-GNN for distributed\nonline learning.",
          "link": "http://arxiv.org/abs/2107.09203",
          "publishedOn": "2021-07-21T02:01:35.651Z",
          "wordCount": 681,
          "title": "Wide and Deep Graph Neural Network with Distributed Online Learning. (arXiv:2107.09203v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uy_W/0/1/0/all/0/1\">Wayne Isaac Tan Uy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuepeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuxiao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peherstorfer_B/0/1/0/all/0/1\">Benjamin Peherstorfer</a>",
          "description": "Noise poses a challenge for learning dynamical-system models because already\nsmall variations can distort the dynamics described by trajectory data. This\nwork builds on operator inference from scientific machine learning to infer\nlow-dimensional models from high-dimensional state trajectories polluted with\nnoise. The presented analysis shows that, under certain conditions, the\ninferred operators are unbiased estimators of the well-studied projection-based\nreduced operators from traditional model reduction. Furthermore, the connection\nbetween operator inference and projection-based model reduction enables\nbounding the mean-squared errors of predictions made with the learned models\nwith respect to traditional reduced models. The analysis also motivates an\nactive operator inference approach that judiciously samples high-dimensional\ntrajectories with the aim of achieving a low mean-squared error by reducing the\neffect of noise. Numerical experiments with high-dimensional linear and\nnonlinear state dynamics demonstrate that predictions obtained with active\noperator inference have orders of magnitude lower mean-squared errors than\noperator inference with traditional, equidistantly sampled trajectory data.",
          "link": "http://arxiv.org/abs/2107.09256",
          "publishedOn": "2021-07-21T02:01:35.603Z",
          "wordCount": 601,
          "title": "Active operator inference for learning low-dimensional dynamical-system models from noisy data. (arXiv:2107.09256v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kunin_D/0/1/0/all/0/1\">Daniel Kunin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagastuy_Brena_J/0/1/0/all/0/1\">Javier Sagastuy-Brena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillespie_L/0/1/0/all/0/1\">Lauren Gillespie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Margalit_E/0/1/0/all/0/1\">Eshed Margalit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1\">Hidenori Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Surya Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamins_D/0/1/0/all/0/1\">Daniel L. K. Yamins</a>",
          "description": "In this work we explore the limiting dynamics of deep neural networks trained\nwith stochastic gradient descent (SGD). We find empirically that long after\nperformance has converged, networks continue to move through parameter space by\na process of anomalous diffusion in which distance travelled grows as a power\nlaw in the number of gradient updates with a nontrivial exponent. We reveal an\nintricate interaction between the hyperparameters of optimization, the\nstructure in the gradient noise, and the Hessian matrix at the end of training\nthat explains this anomalous diffusion. To build this understanding, we first\nderive a continuous-time model for SGD with finite learning rates and batch\nsizes as an underdamped Langevin equation. We study this equation in the\nsetting of linear regression, where we can derive exact, analytic expressions\nfor the phase space dynamics of the parameters and their instantaneous\nvelocities from initialization to stationarity. Using the Fokker-Planck\nequation, we show that the key ingredient driving these dynamics is not the\noriginal training loss, but rather the combination of a modified loss, which\nimplicitly regularizes the velocity, and probability currents, which cause\noscillations in phase space. We identify qualitative and quantitative\npredictions of this theory in the dynamics of a ResNet-18 model trained on\nImageNet. Through the lens of statistical physics, we uncover a mechanistic\norigin for the anomalous limiting dynamics of deep neural networks trained with\nSGD.",
          "link": "http://arxiv.org/abs/2107.09133",
          "publishedOn": "2021-07-21T02:01:35.596Z",
          "wordCount": 700,
          "title": "Rethinking the limiting dynamics of SGD: modified loss, phase space oscillations, and anomalous diffusion. (arXiv:2107.09133v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Utimula_K/0/1/0/all/0/1\">Keishu Utimula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayaschi_K/0/1/0/all/0/1\">Ken-taro Hayaschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakano_K/0/1/0/all/0/1\">Kousuke Nakano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hongo_K/0/1/0/all/0/1\">Kenta Hongo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maezono_R/0/1/0/all/0/1\">Ryo Maezono</a>",
          "description": "When agents are swarmed to carry out a mission, there is often a sudden\nfailure of some of the agents observed from the command base. It is generally\ndifficult to distinguish whether the failure is caused by actuators\n(hypothesis, $h_a$) or sensors (hypothesis, $h_s$) solely by the communication\nbetween the command base and the concerning agent. By making a collision to the\nagent by another, we would be able to distinguish which hypothesis is likely:\nFor $h_a$, we expect to detect corresponding displacements while for $h_a$ we\ndo not. Such swarm strategies to grasp the situation are preferably to be\ngenerated autonomously by artificial intelligence (AI). Preferable actions\n($e.g.$, the collision) for the distinction would be those maximizing the\ndifference between the expected behaviors for each hypothesis, as a value\nfunction. Such actions exist, however, only very sparsely in the whole\npossibilities, for which the conventional search based on gradient methods does\nnot make sense. Instead, we have successfully applied the reinforcement\nlearning technique, achieving the maximization of such a sparse value function.\nThe machine learning actually concluded autonomously the colliding action to\ndistinguish the hypothesises. Getting recognized an agent with actuator error\nby the action, the agents behave as if other ones want to assist the\nmalfunctioning one to achieve a given mission.",
          "link": "http://arxiv.org/abs/2107.09232",
          "publishedOn": "2021-07-21T02:01:35.577Z",
          "wordCount": 674,
          "title": "Reinforcement learning autonomously identifying the source of errors for agents in a group mission. (arXiv:2107.09232v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varga_B/0/1/0/all/0/1\">Bal&#xe1;zs Varga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulcsar_B/0/1/0/all/0/1\">Bal&#xe1;zs Kulcs&#xe1;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chehreghani_M/0/1/0/all/0/1\">Morteza Haghir Chehreghani</a>",
          "description": "This paper presents a constrained policy gradient algorithm. We introduce\nconstraints for safe learning with the following steps. First, learning is\nslowed down (lazy learning) so that the episodic policy change can be computed\nwith the help of the policy gradient theorem and the neural tangent kernel.\nThen, this enables us the evaluation of the policy at arbitrary states too. In\nthe same spirit, learning can be guided, ensuring safety via augmenting episode\nbatches with states where the desired action probabilities are prescribed.\nFinally, exogenous discounted sum of future rewards (returns) can be computed\nat these specific state-action pairs such that the policy network satisfies\nconstraints. Computing the returns is based on solving a system of linear\nequations (equality constraints) or a constrained quadratic program (inequality\nconstraints). Simulation results suggest that adding constraints (external\ninformation) to the learning can improve learning in terms of speed and safety\nreasonably if constraints are appropriately selected. The efficiency of the\nconstrained learning was demonstrated with a shallow and wide ReLU network in\nthe Cartpole and Lunar Lander OpenAI gym environments. The main novelty of the\npaper is giving a practical use of the neural tangent kernel in reinforcement\nlearning.",
          "link": "http://arxiv.org/abs/2107.09139",
          "publishedOn": "2021-07-21T02:01:35.570Z",
          "wordCount": 641,
          "title": "Constrained Policy Gradient Method for Safe and Fast Reinforcement Learning: a Neural Tangent Kernel Based Approach. (arXiv:2107.09139v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hawthorne_C/0/1/0/all/0/1\">Curtis Hawthorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_I/0/1/0/all/0/1\">Ian Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swavely_R/0/1/0/all/0/1\">Rigel Swavely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manilow_E/0/1/0/all/0/1\">Ethan Manilow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engel_J/0/1/0/all/0/1\">Jesse Engel</a>",
          "description": "Automatic Music Transcription has seen significant progress in recent years\nby training custom deep neural networks on large datasets. However, these\nmodels have required extensive domain-specific design of network architectures,\ninput/output representations, and complex decoding schemes. In this work, we\nshow that equivalent performance can be achieved using a generic\nencoder-decoder Transformer with standard decoding methods. We demonstrate that\nthe model can learn to translate spectrogram inputs directly to MIDI-like\noutput events for several transcription tasks. This sequence-to-sequence\napproach simplifies transcription by jointly modeling audio features and\nlanguage-like output dependencies, thus removing the need for task-specific\narchitectures. These results point toward possibilities for creating new Music\nInformation Retrieval models by focusing on dataset creation and labeling\nrather than custom model design.",
          "link": "http://arxiv.org/abs/2107.09142",
          "publishedOn": "2021-07-21T02:01:35.563Z",
          "wordCount": 555,
          "title": "Sequence-to-Sequence Piano Transcription with Transformers. (arXiv:2107.09142v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09207",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hou_T/0/1/0/all/0/1\">Thomas Y. Hou</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_Z/0/1/0/all/0/1\">Zhenzhen Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziyun Zhang</a>",
          "description": "We show that the Riemannian gradient descent algorithm on the low-rank matrix\nmanifold almost surely escapes some spurious critical points on the boundary of\nthe manifold. Given that the low-rank matrix manifold is an incomplete set,\nthis result is the first to overcome this difficulty and partially justify the\nglobal use of the Riemannian gradient descent on the manifold. The spurious\ncritical points are some rank-deficient matrices that capture only part of the\nSVD components of the ground truth. They exhibit very singular behavior and\nevade the classical analysis of strict saddle points. We show that using the\ndynamical low-rank approximation and a rescaled gradient flow, some of the\nspurious critical points can be converted to classical strict saddle points,\nwhich leads to the desired result. Numerical experiments are provided to\nsupport our theoretical findings.",
          "link": "http://arxiv.org/abs/2107.09207",
          "publishedOn": "2021-07-21T02:01:35.534Z",
          "wordCount": 586,
          "title": "Asymptotic Escape of Spurious Critical Points on the Low-rank Matrix Manifold. (arXiv:2107.09207v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingzhong Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>",
          "description": "Stein variational gradient descent (SVGD) and its variants have shown\npromising successes in approximate inference for complex distributions.\nHowever, their empirical performance depends crucially on the choice of optimal\nkernel. Unfortunately, RBF kernel with median heuristics is a common choice in\nprevious approaches which has been proved sub-optimal. Inspired by the paradigm\nof multiple kernel learning, our solution to this issue is using a combination\nof multiple kernels to approximate the optimal kernel instead of a single one\nwhich may limit the performance and flexibility. To do so, we extend Kernelized\nStein Discrepancy (KSD) to its multiple kernel view called Multiple Kernelized\nStein Discrepancy (MKSD). Further, we leverage MKSD to construct a general\nalgorithm based on SVGD, which be called Multiple Kernel SVGD (MK-SVGD).\nBesides, we automatically assign a weight to each kernel without any other\nparameters. The proposed method not only gets rid of optimal kernel dependence\nbut also maintains computational effectiveness. Experiments on various tasks\nand models show the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2107.09338",
          "publishedOn": "2021-07-21T02:01:35.518Z",
          "wordCount": 596,
          "title": "Kernel Selection for Stein Variational Gradient Descent. (arXiv:2107.09338v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Severo_D/0/1/0/all/0/1\">Daniel Severo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Townsend_J/0/1/0/all/0/1\">James Townsend</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khisti_A/0/1/0/all/0/1\">Ashish Khisti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makhzani_A/0/1/0/all/0/1\">Alireza Makhzani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullrich_K/0/1/0/all/0/1\">Karen Ullrich</a>",
          "description": "Current methods that optimally compress multisets are not suitable for\nhigh-dimensional symbols, as their compute time scales linearly with alphabet\nsize. Compressing a multiset as an ordered sequence with off-the-shelf codecs\nis computationally more efficient, but has a sub-optimal compression rate, as\nbits are wasted encoding the order between symbols. We present a method that\ncan recover those bits, assuming symbols are i.i.d., at the cost of an\nadditional $\\mathcal{O}(|\\mathcal{M}|\\log M)$ in average time complexity, where\n$|\\mathcal{M}|$ and $M$ are the total and unique number of symbols in the\nmultiset. Our method is compatible with any prefix-free code. Experiments show\nthat, when paired with efficient coders, our method can efficiently compress\nhigh-dimensional sources such as multisets of images and collections of JSON\nfiles.",
          "link": "http://arxiv.org/abs/2107.09202",
          "publishedOn": "2021-07-21T02:01:35.511Z",
          "wordCount": 560,
          "title": "Compressing Multisets with Large Alphabets. (arXiv:2107.09202v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09060",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kustner_T/0/1/0/all/0/1\">Thomas K&#xfc;stner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_J/0/1/0/all/0/1\">Jiazhen Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_H/0/1/0/all/0/1\">Haikun Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cruz_G/0/1/0/all/0/1\">Gastao Cruz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gilliam_C/0/1/0/all/0/1\">Christopher Gilliam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blu_T/0/1/0/all/0/1\">Thierry Blu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_B/0/1/0/all/0/1\">Bin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gatidis_S/0/1/0/all/0/1\">Sergios Gatidis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Botnar_R/0/1/0/all/0/1\">Ren&#xe9; Botnar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prieto_C/0/1/0/all/0/1\">Claudia Prieto</a>",
          "description": "Physiological motion, such as cardiac and respiratory motion, during Magnetic\nResonance (MR) image acquisition can cause image artifacts. Motion correction\ntechniques have been proposed to compensate for these types of motion during\nthoracic scans, relying on accurate motion estimation from undersampled\nmotion-resolved reconstruction. A particular interest and challenge lie in the\nderivation of reliable non-rigid motion fields from the undersampled\nmotion-resolved data. Motion estimation is usually formulated in image space\nvia diffusion, parametric-spline, or optical flow methods. However, image-based\nregistration can be impaired by remaining aliasing artifacts due to the\nundersampled motion-resolved reconstruction. In this work, we describe a\nformalism to perform non-rigid registration directly in the sampled Fourier\nspace, i.e. k-space. We propose a deep-learning based approach to perform fast\nand accurate non-rigid registration from the undersampled k-space data. The\nbasic working principle originates from the Local All-Pass (LAP) technique, a\nrecently introduced optical flow-based registration. The proposed LAPNet is\ncompared against traditional and deep learning image-based registrations and\ntested on fully-sampled and highly-accelerated (with two undersampling\nstrategies) 3D respiratory motion-resolved MR images in a cohort of 40 patients\nwith suspected liver or lung metastases and 25 healthy subjects. The proposed\nLAPNet provided consistent and superior performance to image-based approaches\nthroughout different sampling trajectories and acceleration factors.",
          "link": "http://arxiv.org/abs/2107.09060",
          "publishedOn": "2021-07-21T02:01:35.437Z",
          "wordCount": 677,
          "title": "LAPNet: Non-rigid Registration derived in k-space for Magnetic Resonance Imaging. (arXiv:2107.09060v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nousias_S/0/1/0/all/0/1\">Stavros Nousias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pikoulis_E/0/1/0/all/0/1\">Erion-Vasilis Pikoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavrokefalidis_C/0/1/0/all/0/1\">Christos Mavrokefalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalos_A/0/1/0/all/0/1\">Aris S. Lalos</a>",
          "description": "Automotive Cyber-Physical Systems (ACPS) have attracted a significant amount\nof interest in the past few decades, while one of the most critical operations\nin these systems is the perception of the environment. Deep learning and,\nespecially, the use of Deep Neural Networks (DNNs) provides impressive results\nin analyzing and understanding complex and dynamic scenes from visual data. The\nprediction horizons for those perception systems are very short and inference\nmust often be performed in real time, stressing the need of transforming the\noriginal large pre-trained networks into new smaller models, by utilizing Model\nCompression and Acceleration (MCA) techniques. Our goal in this work is to\ninvestigate best practices for appropriately applying novel weight sharing\ntechniques, optimizing the available variables and the training procedures\ntowards the significant acceleration of widely adopted DNNs. Extensive\nevaluation studies carried out using various state-of-the-art DNN models in\nobject detection and tracking experiments, provide details about the type of\nerrors that manifest after the application of weight sharing techniques,\nresulting in significant acceleration gains with negligible accuracy losses.",
          "link": "http://arxiv.org/abs/2107.09101",
          "publishedOn": "2021-07-21T02:01:35.427Z",
          "wordCount": 620,
          "title": "Accelerating deep neural networks for efficient scene understanding in automotive cyber-physical systems. (arXiv:2107.09101v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vicente_J/0/1/0/all/0/1\">Juan Pablo de Vicente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1\">Alvaro Soto</a>",
          "description": "Current datasets to train social behaviors are usually borrowed from\nsurveillance applications that capture visual data from a bird's-eye\nperspective. This leaves aside precious relationships and visual cues that\ncould be captured through a first-person view of a scene. In this work, we\npropose a strategy to exploit the power of current game engines, such as Unity,\nto transform pre-existing bird's-eye view datasets into a first-person view, in\nparticular, a depth view. Using this strategy, we are able to generate large\nvolumes of synthetic data that can be used to pre-train a social navigation\nmodel. To test our ideas, we present DeepSocNav, a deep learning based model\nthat takes advantage of the proposed approach to generate synthetic data.\nFurthermore, DeepSocNav includes a self-supervised strategy that is included as\nan auxiliary task. This consists of predicting the next depth frame that the\nagent will face. Our experiments show the benefits of the proposed model that\nis able to outperform relevant baselines in terms of social navigation scores.",
          "link": "http://arxiv.org/abs/2107.09170",
          "publishedOn": "2021-07-21T02:01:35.400Z",
          "wordCount": 624,
          "title": "DeepSocNav: Social Navigation by Imitating Human Behaviors. (arXiv:2107.09170v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09078",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Cai_H/0/1/0/all/0/1\">Haoyuan Cai</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ye_Q/0/1/0/all/0/1\">Qi Ye</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Deng_D/0/1/0/all/0/1\">Dong-Ling Deng</a>",
          "description": "Quantum computers hold unprecedented potentials for machine learning\napplications. Here, we prove that physical quantum circuits are PAC (probably\napproximately correct) learnable on a quantum computer via empirical risk\nminimization: to learn a quantum circuit with at most $n^c$ gates and each gate\nacting on a constant number of qubits, the sample complexity is bounded by\n$\\tilde{O}(n^{c+1})$. In particular, we explicitly construct a family of\nvariational quantum circuits with $O(n^{c+1})$ elementary gates arranged in a\nfixed pattern, which can represent all physical quantum circuits consisting of\nat most $n^c$ elementary gates. Our results provide a valuable guide for\nquantum machine learning in both theory and experiment.",
          "link": "http://arxiv.org/abs/2107.09078",
          "publishedOn": "2021-07-21T02:01:35.387Z",
          "wordCount": 544,
          "title": "Sample Complexity of Learning Quantum Circuits. (arXiv:2107.09078v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Abhinav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriharsha_R/0/1/0/all/0/1\">Ram Sriharsha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1\">Sichen Zhong</a>",
          "description": "Decomposing a complex time series into trend, seasonality, and remainder\ncomponents is an important primitive that facilitates time series anomaly\ndetection, change point detection and forecasting. Although numerous batch\nalgorithms are known for time series decomposition, none operate well in an\nonline scalable setting where high throughput and real-time response are\nparamount. In this paper, we propose OnlineSTL, a novel online algorithm for\ntime series decomposition which solves the scalability problem and is deployed\nfor real-time metrics monitoring on high resolution, high ingest rate data.\nExperiments on different synthetic and real world time series datasets\ndemonstrate that OnlineSTL achieves orders of magnitude speedups while\nmaintaining quality of decomposition.",
          "link": "http://arxiv.org/abs/2107.09110",
          "publishedOn": "2021-07-21T02:01:35.365Z",
          "wordCount": 550,
          "title": "OnlineSTL: Scaling Time Series Decomposition by 100x. (arXiv:2107.09110v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Larma_M/0/1/0/all/0/1\">Mikel Landajuela Larma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_B/0/1/0/all/0/1\">Brenden K. Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soo K. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santiago_C/0/1/0/all/0/1\">Claudio P. Santiago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glatt_R/0/1/0/all/0/1\">Ruben Glatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mundhenk_T/0/1/0/all/0/1\">T. Nathan Mundhenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pettit_J/0/1/0/all/0/1\">Jacob F. Pettit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faissol_D/0/1/0/all/0/1\">Daniel M. Faissol</a>",
          "description": "Many machine learning strategies designed to automate mathematical tasks\nleverage neural networks to search large combinatorial spaces of mathematical\nsymbols. In contrast to traditional evolutionary approaches, using a neural\nnetwork at the core of the search allows learning higher-level symbolic\npatterns, providing an informed direction to guide the search. When no labeled\ndata is available, such networks can still be trained using reinforcement\nlearning. However, we demonstrate that this approach can suffer from an early\ncommitment phenomenon and from initialization bias, both of which limit\nexploration. We present two exploration methods to tackle these issues,\nbuilding upon ideas of entropy regularization and distribution initialization.\nWe show that these techniques can improve the performance, increase sample\nefficiency, and lower the complexity of solutions for the task of symbolic\nregression.",
          "link": "http://arxiv.org/abs/2107.09158",
          "publishedOn": "2021-07-21T02:01:35.334Z",
          "wordCount": 610,
          "title": "Improving exploration in policy gradient search: Application to symbolic optimization. (arXiv:2107.09158v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09051",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Cao_L/0/1/0/all/0/1\">Longbing Cao</a>",
          "description": "AI in finance broadly refers to the applications of AI techniques in\nfinancial businesses. This area has been lasting for decades with both classic\nand modern AI techniques applied to increasingly broader areas of finance,\neconomy and society. In contrast to either discussing the problems, aspects and\nopportunities of finance that have benefited from specific AI techniques and in\nparticular some new-generation AI and data science (AIDS) areas or reviewing\nthe progress of applying specific techniques to resolving certain financial\nproblems, this review offers a comprehensive and dense roadmap of the\noverwhelming challenges, techniques and opportunities of AI research in finance\nover the past decades. The landscapes and challenges of financial businesses\nand data are firstly outlined, followed by a comprehensive categorization and a\ndense overview of the decades of AI research in finance. We then structure and\nillustrate the data-driven analytics and learning of financial businesses and\ndata. The comparison, criticism and discussion of classic vs. modern AI\ntechniques for finance are followed. Lastly, open issues and opportunities\naddress future AI-empowered finance and finance-motivated AI research.",
          "link": "http://arxiv.org/abs/2107.09051",
          "publishedOn": "2021-07-21T02:01:35.316Z",
          "wordCount": 627,
          "title": "AI in Finance: Challenges, Techniques and Opportunities. (arXiv:2107.09051v1 [q-fin.CP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_T/0/1/0/all/0/1\">Tanmay Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avaneesh/0/1/0/all/0/1\">Avaneesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_R/0/1/0/all/0/1\">Rohit Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shorey_R/0/1/0/all/0/1\">Rajeev Shorey</a>",
          "description": "With the increasing reliance of users on smart devices, bringing essential\ncomputation at the edge has become a crucial requirement for any type of\nbusiness. Many such computations utilize Convolution Neural Networks (CNNs) to\nperform AI tasks, having high resource and computation requirements, that are\ninfeasible for edge devices. Splitting the CNN architecture to perform part of\nthe computation on edge and remaining on the cloud is an area of research that\nhas seen increasing interest in the field. In this paper, we assert that\nrunning CNNs between an edge device and the cloud is synonymous to solving a\nresource-constrained optimization problem that minimizes the latency and\nmaximizes resource utilization at the edge. We formulate a multi-objective\noptimization problem and propose the LMOS algorithm to achieve a Pareto\nefficient solution. Experiments done on real-world edge devices show that, LMOS\nensures feasible execution of different CNN models at the edge and also\nimproves upon existing state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2107.09123",
          "publishedOn": "2021-07-21T02:01:35.299Z",
          "wordCount": 598,
          "title": "Latency-Memory Optimized Splitting of Convolution Neural Networks for Resource Constrained Edge Devices. (arXiv:2107.09123v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09118",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Khaledyan_D/0/1/0/all/0/1\">Donya Khaledyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tajally_A/0/1/0/all/0/1\">AmirReza Tajally</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarkhosh_R/0/1/0/all/0/1\">Reza Sarkhosh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shamsi_A/0/1/0/all/0/1\">Afshar Shamsi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asgharnezhad_H/0/1/0/all/0/1\">Hamzeh Asgharnezhad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>",
          "description": "Deep learning (DL) models have received particular attention in medical\nimaging due to their promising pattern recognition capabilities. However, Deep\nNeural Networks (DNNs) require a huge amount of data, and because of the lack\nof sufficient data in this field, transfer learning can be a great solution.\nDNNs used for disease diagnosis meticulously concentrate on improving the\naccuracy of predictions without providing a figure about their confidence of\npredictions. Knowing how much a DNN model is confident in a computer-aided\ndiagnosis model is necessary for gaining clinicians' confidence and trust in\nDL-based solutions. To address this issue, this work presents three different\nmethods for quantifying uncertainties for skin cancer detection from images. It\nalso comprehensively evaluates and compares performance of these DNNs using\nnovel uncertainty-related metrics. The obtained results reveal that the\npredictive uncertainty estimation methods are capable of flagging risky and\nerroneous predictions with a high uncertainty estimate. We also demonstrate\nthat ensemble approaches are more reliable in capturing uncertainties through\ninference.",
          "link": "http://arxiv.org/abs/2107.09118",
          "publishedOn": "2021-07-21T02:01:35.272Z",
          "wordCount": 625,
          "title": "Confidence Aware Neural Networks for Skin Cancer Detection. (arXiv:2107.09118v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09070",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+List_F/0/1/0/all/0/1\">Florian List</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Rodd_N/0/1/0/all/0/1\">Nicholas L. Rodd</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Lewis_G/0/1/0/all/0/1\">Geraint F. Lewis</a>",
          "description": "The two leading hypotheses for the Galactic Center Excess (GCE) in the\n$\\textit{Fermi}$ data are an unresolved population of faint millisecond pulsars\n(MSPs) and dark-matter (DM) annihilation. The dichotomy between these\nexplanations is typically reflected by modeling them as two separate emission\ncomponents. However, point-sources (PSs) such as MSPs become statistically\ndegenerate with smooth Poisson emission in the ultra-faint limit (formally\nwhere each source is expected to contribute much less than one photon on\naverage), leading to an ambiguity that can render questions such as whether the\nemission is PS-like or Poissonian in nature ill-defined. We present a\nconceptually new approach that describes the PS and Poisson emission in a\nunified manner and only afterwards derives constraints on the Poissonian\ncomponent from the so obtained results. For the implementation of this\napproach, we leverage deep learning techniques, centered around a neural\nnetwork-based method for histogram regression that expresses uncertainties in\nterms of quantiles. We demonstrate that our method is robust against a number\nof systematics that have plagued previous approaches, in particular DM / PS\nmisattribution. In the $\\textit{Fermi}$ data, we find a faint GCE described by\na median source-count distribution (SCD) peaked at a flux of $\\sim4 \\times\n10^{-11} \\ \\text{counts} \\ \\text{cm}^{-2} \\ \\text{s}^{-1}$ (corresponding to\n$\\sim3 - 4$ expected counts per PS), which would require $N \\sim\n\\mathcal{O}(10^4)$ sources to explain the entire excess (median value $N =\n\\text{29,300}$ across the sky). Although faint, this SCD allows us to derive\nthe constraint $\\eta_P \\leq 66\\%$ for the Poissonian fraction of the GCE flux\n$\\eta_P$ at 95% confidence, suggesting that a substantial amount of the GCE\nflux is due to PSs.",
          "link": "http://arxiv.org/abs/2107.09070",
          "publishedOn": "2021-07-21T02:01:35.252Z",
          "wordCount": 759,
          "title": "Dim but not entirely dark: Extracting the Galactic Center Excess' source-count distribution with neural nets. (arXiv:2107.09070v1 [astro-ph.HE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09082",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Veiga_M/0/1/0/all/0/1\">Maria Han Veiga</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Meng_X/0/1/0/all/0/1\">Xi Meng</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Gnedin_O/0/1/0/all/0/1\">Oleg Y. Gnedin</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Gnedin_N/0/1/0/all/0/1\">Nickolay Y. Gnedin</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Huan_X/0/1/0/all/0/1\">Xun Huan</a>",
          "description": "We describe a novel end-to-end approach using Machine Learning to reconstruct\nthe power spectrum of cosmological density perturbations at high redshift from\nobserved quasar spectra. State-of-the-art cosmological simulations of structure\nformation are used to generate a large synthetic dataset of line-of-sight\nabsorption spectra paired with 1-dimensional fluid quantities along the same\nline-of-sight, such as the total density of matter and the density of neutral\natomic hydrogen. With this dataset, we build a series of data-driven models to\npredict the power spectrum of total matter density. We are able to produce\nmodels which yield reconstruction to accuracy of about 1% for wavelengths $k\n\\leq 2 h Mpc^{-1}$, while the error increases at larger $k$. We show the size\nof data sample required to reach a particular error rate, giving a sense of how\nmuch data is necessary to reach a desired accuracy. This work provides a\nfoundation for developing methods to analyse very large upcoming datasets with\nthe next-generation observational facilities.",
          "link": "http://arxiv.org/abs/2107.09082",
          "publishedOn": "2021-07-21T02:01:35.213Z",
          "wordCount": 625,
          "title": "Reconstruction of the Density Power Spectrum from Quasar Spectra using Machine Learning. (arXiv:2107.09082v1 [astro-ph.CO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09086",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Auddy_S/0/1/0/all/0/1\">Sayantan Auddy</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Dey_R/0/1/0/all/0/1\">Ramit Dey</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Lin_M/0/1/0/all/0/1\">Min-Kai Lin</a> (ASIAA, NCTS Physics Division), <a href=\"http://arxiv.org/find/astro-ph/1/au:+Hall_C/0/1/0/all/0/1\">Cassandra Hall</a>",
          "description": "The observed sub-structures, like annular gaps, in dust emissions from\nprotoplanetary disk, are often interpreted as signatures of embedded planets.\nFitting a model of planetary gaps to these observed features using customized\nsimulations or empirical relations can reveal the characteristics of the hidden\nplanets. However, customized fitting is often impractical owing to the\nincreasing sample size and the complexity of disk-planet interaction. In this\npaper we introduce the architecture of DPNNet-2.0, second in the series after\nDPNNet \\citep{aud20}, designed using a Convolutional Neural Network ( CNN, here\nspecifically ResNet50) for predicting exoplanet masses directly from simulated\nimages of protoplanetary disks hosting a single planet. DPNNet-2.0 additionally\nconsists of a multi-input framework that uses both a CNN and multi-layer\nperceptron (a class of artificial neural network) for processing image and disk\nparameters simultaneously. This enables DPNNet-2.0 to be trained using images\ndirectly, with the added option of considering disk parameters (disk\nviscosities, disk temperatures, disk surface density profiles, dust abundances,\nand particle Stokes numbers) generated from disk-planet hydrodynamic\nsimulations as inputs. This work provides the required framework and is the\nfirst step towards the use of computer vision (implementing CNN) to directly\nextract mass of an exoplanet from planetary gaps observed in dust-surface\ndensity maps by telescopes such as the Atacama Large (sub-)Millimeter Array.",
          "link": "http://arxiv.org/abs/2107.09086",
          "publishedOn": "2021-07-21T02:01:35.180Z",
          "wordCount": 681,
          "title": "DPNNet-2.0 Part I: Finding hidden planets from simulated images of protoplanetary disk gaps. (arXiv:2107.09086v1 [astro-ph.EP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09055",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Sonkiya_P/0/1/0/all/0/1\">Priyank Sonkiya</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Bajpai_V/0/1/0/all/0/1\">Vikas Bajpai</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Bansal_A/0/1/0/all/0/1\">Anukriti Bansal</a>",
          "description": "The stock market has been a popular topic of interest in the recent past. The\ngrowth in the inflation rate has compelled people to invest in the stock and\ncommodity markets and other areas rather than saving. Further, the ability of\nDeep Learning models to make predictions on the time series data has been\nproven time and again. Technical analysis on the stock market with the help of\ntechnical indicators has been the most common practice among traders and\ninvestors. One more aspect is the sentiment analysis - the emotion of the\ninvestors that shows the willingness to invest. A variety of techniques have\nbeen used by people around the globe involving basic Machine Learning and\nNeural Networks. Ranging from the basic linear regression to the advanced\nneural networks people have experimented with all possible techniques to\npredict the stock market. It's evident from recent events how news and\nheadlines affect the stock markets and cryptocurrencies. This paper proposes an\nensemble of state-of-the-art methods for predicting stock prices. Firstly\nsentiment analysis of the news and the headlines for the company Apple Inc,\nlisted on the NASDAQ is performed using a version of BERT, which is a\npre-trained transformer model by Google for Natural Language Processing (NLP).\nAfterward, a Generative Adversarial Network (GAN) predicts the stock price for\nApple Inc using the technical indicators, stock indexes of various countries,\nsome commodities, and historical prices along with the sentiment scores.\nComparison is done with baseline models like - Long Short Term Memory (LSTM),\nGated Recurrent Units (GRU), vanilla GAN, and Auto-Regressive Integrated Moving\nAverage (ARIMA) model.",
          "link": "http://arxiv.org/abs/2107.09055",
          "publishedOn": "2021-07-21T02:01:35.125Z",
          "wordCount": 699,
          "title": "Stock price prediction using BERT and GAN. (arXiv:2107.09055v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00088",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Hooten_S/0/1/0/all/0/1\">Sean Hooten</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Beausoleil_R/0/1/0/all/0/1\">Raymond G. Beausoleil</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Vaerenbergh_T/0/1/0/all/0/1\">Thomas Van Vaerenbergh</a>",
          "description": "We present a proof-of-concept technique for the inverse design of\nelectromagnetic devices motivated by the policy gradient method in\nreinforcement learning, named PHORCED (PHotonic Optimization using REINFORCE\nCriteria for Enhanced Design). This technique uses a probabilistic generative\nneural network interfaced with an electromagnetic solver to assist in the\ndesign of photonic devices, such as grating couplers. We show that PHORCED\nobtains better performing grating coupler designs than local gradient-based\ninverse design via the adjoint method, while potentially providing faster\nconvergence over competing state-of-the-art generative methods. Furthermore, we\nimplement transfer learning with PHORCED, demonstrating that a neural network\ntrained to optimize 8$^\\circ$ grating couplers can then be re-trained on\ngrating couplers with alternate scattering angles while requiring >$10\\times$\nfewer simulations than control cases.",
          "link": "http://arxiv.org/abs/2107.00088",
          "publishedOn": "2021-07-20T02:04:49.165Z",
          "wordCount": 591,
          "title": "Inverse Design of Grating Couplers Using the Policy Gradient Method from Reinforcement Learning. (arXiv:2107.00088v2 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03920",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dalmasso_N/0/1/0/all/0/1\">Niccol&#xf2; Dalmasso</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhao_D/0/1/0/all/0/1\">David Zhao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Izbicki_R/0/1/0/all/0/1\">Rafael Izbicki</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1\">Ann B. Lee</a>",
          "description": "Many areas of science make extensive use of computer simulators that\nimplicitly encode likelihood functions of complex systems. Classical\nstatistical methods are poorly suited for these so-called likelihood-free\ninference (LFI) settings, outside the asymptotic and low-dimensional regimes.\nAlthough new machine learning methods, such as normalizing flows, have\nrevolutionized the sample efficiency and capacity of LFI methods, it remains an\nopen question whether they produce reliable measures of uncertainty. This paper\npresents a statistical framework for LFI that unifies classical statistics with\nmodern machine learning to: (1) efficiently construct frequentist confidence\nsets and hypothesis tests with finite-sample guarantees of nominal coverage\n(type I error control) and power; (2) provide practical diagnostics for\nassessing empirical coverage over the entire parameter space. We refer to our\nframework as likelihood-free frequentist inference (LF2I). Any method that\nestimates a test statistic, like the likelihood ratio, can be plugged into our\nframework to create valid confidence sets and compute diagnostics, without\ncostly Monte Carlo samples at fixed parameter settings. In this work, we\nspecifically study the power of two test statistics (ACORE and BFF), which,\nrespectively, maximize versus integrate an odds function over the parameter\nspace. Our study offers multifaceted perspectives on the challenges in LF2I.",
          "link": "http://arxiv.org/abs/2107.03920",
          "publishedOn": "2021-07-20T02:04:49.148Z",
          "wordCount": 674,
          "title": "Likelihood-Free Frequentist Inference: Bridging Classical Statistics and Machine Learning in Simulation and Uncertainty Quantification. (arXiv:2107.03920v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wagener_N/0/1/0/all/0/1\">Nolan Wagener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boots_B/0/1/0/all/0/1\">Byron Boots</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Ching-An Cheng</a>",
          "description": "Many sequential decision problems involve finding a policy that maximizes\ntotal reward while obeying safety constraints. Although much recent research\nhas focused on the development of safe reinforcement learning (RL) algorithms\nthat produce a safe policy after training, ensuring safety during training as\nwell remains an open problem. A fundamental challenge is performing exploration\nwhile still satisfying constraints in an unknown Markov decision process (MDP).\nIn this work, we address this problem for the chance-constrained setting. We\npropose a new algorithm, SAILR, that uses an intervention mechanism based on\nadvantage functions to keep the agent safe throughout training and optimizes\nthe agent's policy using off-the-shelf RL algorithms designed for unconstrained\nMDPs. Our method comes with strong guarantees on safety during both training\nand deployment (i.e., after training and without the intervention mechanism)\nand policy performance compared to the optimal safety-constrained policy. In\nour experiments, we show that SAILR violates constraints far less during\ntraining than standard safe RL and constrained MDP approaches and converges to\na well-performing policy that can be deployed safely without intervention. Our\ncode is available at https://github.com/nolanwagener/safe_rl.",
          "link": "http://arxiv.org/abs/2106.09110",
          "publishedOn": "2021-07-20T02:04:49.130Z",
          "wordCount": 653,
          "title": "Safe Reinforcement Learning Using Advantage-Based Intervention. (arXiv:2106.09110v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.01089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_C/0/1/0/all/0/1\">Chirag Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_S/0/1/0/all/0/1\">Sivaraman Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramdas_A/0/1/0/all/0/1\">Aaditya Ramdas</a>",
          "description": "We derive bounds on the path length $\\zeta$ of gradient descent (GD) and\ngradient flow (GF) curves for various classes of smooth convex and nonconvex\nfunctions. Among other results, we prove that: (a) if the iterates are linearly\nconvergent with factor $(1-c)$, then $\\zeta$ is at most $\\mathcal{O}(1/c)$; (b)\nunder the Polyak-Kurdyka-Lojasiewicz (PKL) condition, $\\zeta$ is at most\n$\\mathcal{O}(\\sqrt{\\kappa})$, where $\\kappa$ is the condition number, and at\nleast $\\widetilde\\Omega(\\sqrt{d} \\wedge \\kappa^{1/4})$; (c) for quadratics,\n$\\zeta$ is $\\Theta(\\min\\{\\sqrt{d},\\sqrt{\\log \\kappa}\\})$ and in some cases can\nbe independent of $\\kappa$; (d) assuming just convexity, $\\zeta$ can be at most\n$2^{4d\\log d}$; (e) for separable quasiconvex functions, $\\zeta$ is\n${\\Theta}(\\sqrt{d})$. Thus, we advance current understanding of the properties\nof GD and GF curves beyond rates of convergence. We expect our techniques to\nfacilitate future studies for other algorithms.",
          "link": "http://arxiv.org/abs/1908.01089",
          "publishedOn": "2021-07-20T02:04:49.113Z",
          "wordCount": 637,
          "title": "Path Length Bounds for Gradient Descent and Flow. (arXiv:1908.01089v4 [cs.LG] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04086",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_M/0/1/0/all/0/1\">Mohit Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1\">Lingyang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zi Yu Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lanjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_P/0/1/0/all/0/1\">Peter Cho-Ho Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>",
          "description": "Massive deployment of Graph Neural Networks (GNNs) in high-stake applications\ngenerates a strong demand for explanations that are robust to noise and align\nwell with human intuition. Most existing methods generate explanations by\nidentifying a subgraph of an input graph that has a strong correlation with the\nprediction. These explanations are not robust to noise because independently\noptimizing the correlation for a single input can easily overfit noise.\nMoreover, they do not align well with human intuition because removing an\nidentified subgraph from an input graph does not necessarily change the\nprediction result. In this paper, we propose a novel method to generate robust\ncounterfactual explanations on GNNs by explicitly modelling the common decision\nlogic of GNNs on similar input graphs. Our explanations are naturally robust to\nnoise because they are produced from the common decision boundaries of a GNN\nthat govern the predictions of many similar input graphs. The explanations also\nalign well with human intuition because removing the set of edges identified by\nan explanation from the input graph changes the prediction significantly.\nExhaustive experiments on many public datasets demonstrate the superior\nperformance of our method.",
          "link": "http://arxiv.org/abs/2107.04086",
          "publishedOn": "2021-07-20T02:04:49.068Z",
          "wordCount": 645,
          "title": "Robust Counterfactual Explanations on Graph Neural Networks. (arXiv:2107.04086v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04631",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_F/0/1/0/all/0/1\">Fangcao Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cervone_G/0/1/0/all/0/1\">Guido Cervone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salvador_M/0/1/0/all/0/1\">Mark Salvador</a>",
          "description": "Atmospheric correction is a fundamental task in remote sensing because\nobservations are taken either of the atmosphere or looking through the\natmosphere. Atmospheric correction errors can significantly alter the spectral\nsignature of the observations, and lead to invalid classifications or target\ndetection. This is even more crucial when working with hyperspectral data,\nwhere a precise measurement of spectral properties is required.\nState-of-the-art physics-based atmospheric correction approaches require\nextensive prior knowledge about sensor characteristics, collection geometry,\nand environmental characteristics of the scene being collected. These\napproaches are computationally expensive, prone to inaccuracy due to lack of\nsufficient environmental and collection information, and often impossible for\nreal-time applications. In this paper, a geometry-dependent hybrid neural\nnetwork is proposed for automatic atmospheric correction using multi-scan\nhyperspectral data collected from different geometries. The proposed network\ncan characterize the atmosphere without any additional meteorological data. A\ngrid-search method is also proposed to solve the temperature emissivity\nseparation problem. Results show that the proposed network has the capacity to\naccurately characterize the atmosphere and estimate target emissivity spectra\nwith a Mean Absolute Error (MAE) under 0.02 for 29 different materials. This\nsolution can lead to accurate atmospheric correction to improve target\ndetection for real time applications.",
          "link": "http://arxiv.org/abs/2107.04631",
          "publishedOn": "2021-07-20T02:04:49.049Z",
          "wordCount": 670,
          "title": "Ill-posed Surface Emissivity Retrieval from Multi-Geometry Hyperspectral Images using a Hybrid Deep Neural Network. (arXiv:2107.04631v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05318",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Skandarani_Y/0/1/0/all/0/1\">Youssef Skandarani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jodoin_P/0/1/0/all/0/1\">Pierre-Marc Jodoin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lalande_A/0/1/0/all/0/1\">Alain Lalande</a>",
          "description": "Generative Adversarial Networks (GANs) have become increasingly powerful,\ngenerating mind-blowing photorealistic images that mimic the content of\ndatasets they were trained to replicate. One recurrent theme in medical imaging\nis whether GANs can also be effective at generating workable medical data as\nthey are for generating realistic RGB images. In this paper, we perform a\nmulti-GAN and multi-application study to gauge the benefits of GANs in medical\nimaging. We tested various GAN architectures from basic DCGAN to more\nsophisticated style-based GANs on three medical imaging modalities and organs\nnamely : cardiac cine-MRI, liver CT and RGB retina images. GANs were trained on\nwell-known and widely utilized datasets from which their FID score were\ncomputed to measure the visual acuity of their generated images. We further\ntested their usefulness by measuring the segmentation accuracy of a U-Net\ntrained on these generated images.\n\nResults reveal that GANs are far from being equal as some are ill-suited for\nmedical imaging applications while others are much better off. The\ntop-performing GANs are capable of generating realistic-looking medical images\nby FID standards that can fool trained experts in a visual Turing test and\ncomply to some metrics. However, segmentation results suggests that no GAN is\ncapable of reproducing the full richness of a medical datasets.",
          "link": "http://arxiv.org/abs/2105.05318",
          "publishedOn": "2021-07-20T02:04:49.029Z",
          "wordCount": 674,
          "title": "GANs for Medical Image Synthesis: An Empirical Study. (arXiv:2105.05318v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yifan Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tianjun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1\">David Wipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We propose a hierarchical graph neural network (GNN) model that learns how to\ncluster a set of images into an unknown number of identities using a training\nset of images annotated with labels belonging to a disjoint set of identities.\nOur hierarchical GNN uses a novel approach to merge connected components\npredicted at each level of the hierarchy to form a new graph at the next level.\nUnlike fully unsupervised hierarchical clustering, the choice of grouping and\ncomplexity criteria stems naturally from supervision in the training set. The\nresulting method, Hi-LANDER, achieves an average of 54% improvement in F-score\nand 8% increase in Normalized Mutual Information (NMI) relative to current\nGNN-based clustering algorithms. Additionally, state-of-the-art GNN-based\nmethods rely on separate models to predict linkage probabilities and node\ndensities as intermediate steps of the clustering process. In contrast, our\nunified framework achieves a seven-fold decrease in computational cost. We\nrelease our training and inference code at\nhttps://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander.",
          "link": "http://arxiv.org/abs/2107.01319",
          "publishedOn": "2021-07-20T02:04:49.013Z",
          "wordCount": 624,
          "title": "Learning Hierarchical Graph Neural Networks for Image Clustering. (arXiv:2107.01319v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.10564",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gupta_C/0/1/0/all/0/1\">Chirag Gupta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Podkopaev_A/0/1/0/all/0/1\">Aleksandr Podkopaev</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1\">Aaditya Ramdas</a>",
          "description": "We study three notions of uncertainty quantification -- calibration,\nconfidence intervals and prediction sets -- for binary classification in the\ndistribution-free setting, that is without making any distributional\nassumptions on the data. With a focus towards calibration, we establish a\n'tripod' of theorems that connect these three notions for score-based\nclassifiers. A direct implication is that distribution-free calibration is only\npossible, even asymptotically, using a scoring function whose level sets\npartition the feature space into at most countably many sets. Parametric\ncalibration schemes such as variants of Platt scaling do not satisfy this\nrequirement, while nonparametric schemes based on binning do. To close the\nloop, we derive distribution-free confidence intervals for binned probabilities\nfor both fixed-width and uniform-mass binning. As a consequence of our 'tripod'\ntheorems, these confidence intervals for binned probabilities lead to\ndistribution-free calibration. We also derive extensions to settings with\nstreaming data and covariate shift.",
          "link": "http://arxiv.org/abs/2006.10564",
          "publishedOn": "2021-07-20T02:04:48.995Z",
          "wordCount": 637,
          "title": "Distribution-free binary classification: prediction sets, confidence intervals and calibration. (arXiv:2006.10564v3 [stat.ML] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_E/0/1/0/all/0/1\">Erin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anirudh Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1\">Meher Anand Kasam</a>",
          "description": "Data imbalance is a ubiquitous problem in machine learning. In large scale\ncollected and annotated datasets, data imbalance is either mitigated manually\nby undersampling frequent classes and oversampling rare classes, or planned for\nwith imputation and augmentation techniques. In both cases balancing data\nrequires labels. In other words, only annotated data can be balanced.\nCollecting fully annotated datasets is challenging, especially for large scale\nsatellite systems such as the unlabeled NASA's 35 PB Earth Imagery dataset.\nAlthough the NASA Earth Imagery dataset is unlabeled, there are implicit\nproperties of the data source that we can rely on to hypothesize about its\nimbalance, such as distribution of land and water in the case of the Earth's\nimagery. We present a new iterative method to balance unlabeled data. Our\nmethod utilizes image embeddings as a proxy for image labels that can be used\nto balance data, and ultimately when trained increases overall accuracy.",
          "link": "http://arxiv.org/abs/2107.03227",
          "publishedOn": "2021-07-20T02:04:48.978Z",
          "wordCount": 618,
          "title": "Scalable Data Balancing for Unlabeled Satellite Imagery. (arXiv:2107.03227v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burkhalter_L/0/1/0/all/0/1\">Lukas Burkhalter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lycklama_H/0/1/0/all/0/1\">Hidde Lycklama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viand_A/0/1/0/all/0/1\">Alexander Viand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuchler_N/0/1/0/all/0/1\">Nicolas K&#xfc;chler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hithnawi_A/0/1/0/all/0/1\">Anwar Hithnawi</a>",
          "description": "Federated Learning is an emerging decentralized machine learning paradigm\nthat allows a large number of clients to train a joint model without the need\nto share their private data. Participants instead only share ephemeral updates\nnecessary to train the model. To ensure the confidentiality of the client\nupdates, Federated Learning systems employ secure aggregation; clients encrypt\ntheir gradient updates, and only the aggregated model is revealed to the\nserver. Achieving this level of data protection, however, presents new\nchallenges to the robustness of Federated Learning, i.e., the ability to\ntolerate failures and attacks. Unfortunately, in this setting, a malicious\nclient can now easily exert influence on the model behavior without being\ndetected. As Federated Learning is being deployed in practice in a range of\nsensitive applications, its robustness is growing in importance. In this paper,\nwe take a step towards understanding and improving the robustness of secure\nFederated Learning. We start this paper with a systematic study that evaluates\nand analyzes existing attack vectors and discusses potential defenses and\nassesses their effectiveness. We then present RoFL, a secure Federated Learning\nsystem that improves robustness against malicious clients through input checks\non the encrypted model updates. RoFL extends Federated Learning's secure\naggregation protocol to allow expressing a variety of properties and\nconstraints on model updates using zero-knowledge proofs. To enable RoFL to\nscale to typical Federated Learning settings, we introduce several ML and\ncryptographic optimizations specific to Federated Learning. We implement and\nevaluate a prototype of RoFL and show that realistic ML models can be trained\nin a reasonable time while improving robustness.",
          "link": "http://arxiv.org/abs/2107.03311",
          "publishedOn": "2021-07-20T02:04:48.935Z",
          "wordCount": 731,
          "title": "RoFL: Attestable Robustness for Secure Federated Learning. (arXiv:2107.03311v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12627",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Huang_H/0/1/0/all/0/1\">Hsin-Yuan Huang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kueng_R/0/1/0/all/0/1\">Richard Kueng</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Torlai_G/0/1/0/all/0/1\">Giacomo Torlai</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Albert_V/0/1/0/all/0/1\">Victor V. Albert</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Preskill_J/0/1/0/all/0/1\">John Preskill</a>",
          "description": "Classical machine learning (ML) provides a potentially powerful approach to\nsolving challenging quantum many-body problems in physics and chemistry.\nHowever, the advantages of ML over more traditional methods have not been\nfirmly established. In this work, we prove that classical ML algorithms can\nefficiently predict ground state properties of gapped Hamiltonians in finite\nspatial dimensions, after learning from data obtained by measuring other\nHamiltonians in the same quantum phase of matter. In contrast, under widely\naccepted complexity theory assumptions, classical algorithms that do not learn\nfrom data cannot achieve the same guarantee. We also prove that classical ML\nalgorithms can efficiently classify a wide range of quantum phases of matter.\nOur arguments are based on the concept of a classical shadow, a succinct\nclassical description of a many-body quantum state that can be constructed in\nfeasible quantum experiments and be used to predict many properties of the\nstate. Extensive numerical experiments corroborate our theoretical results in a\nvariety of scenarios, including Rydberg atom systems, 2D random Heisenberg\nmodels, symmetry-protected topological phases, and topologically ordered\nphases.",
          "link": "http://arxiv.org/abs/2106.12627",
          "publishedOn": "2021-07-20T02:04:48.915Z",
          "wordCount": 642,
          "title": "Provably efficient machine learning for quantum many-body problems. (arXiv:2106.12627v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poskitt_C/0/1/0/all/0/1\">Christopher M. Poskitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>",
          "description": "Cyber-physical systems (CPSs) are widespread in critical domains, and\nsignificant damage can be caused if an attacker is able to modify the code of\ntheir programmable logic controllers (PLCs). Unfortunately, traditional\ntechniques for attesting code integrity (i.e. verifying that it has not been\nmodified) rely on firmware access or roots-of-trust, neither of which\nproprietary or legacy PLCs are likely to provide. In this paper, we propose a\npractical code integrity checking solution based on privacy-preserving black\nbox models that instead attest the input/output behaviour of PLC programs.\nUsing faithful offline copies of the PLC programs, we identify their most\nimportant inputs through an information flow analysis, execute them on multiple\ncombinations to collect data, then train neural networks able to predict PLC\noutputs (i.e. actuator commands) from their inputs. By exploiting the black box\nnature of the model, our solution maintains the privacy of the original PLC\ncode and does not assume that attackers are unaware of its presence. The trust\ninstead comes from the fact that it is extremely hard to attack the PLC code\nand neural networks at the same time and with consistent outcomes. We evaluated\nour approach on a modern six-stage water treatment plant testbed, finding that\nit could predict actuator states from PLC inputs with near-100% accuracy, and\nthus could detect all 120 effective code mutations that we subjected the PLCs\nto. Finally, we found that it is not practically possible to simultaneously\nmodify the PLC code and apply discreet adversarial noise to our attesters in a\nway that leads to consistent (mis-)predictions.",
          "link": "http://arxiv.org/abs/2106.07851",
          "publishedOn": "2021-07-20T02:04:48.896Z",
          "wordCount": 750,
          "title": "Code Integrity Attestation for PLCs using Black Box Neural Network Predictions. (arXiv:2106.07851v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_S/0/1/0/all/0/1\">Sara Hajj Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassar_M/0/1/0/all/0/1\">Mohamed Nassar</a>",
          "description": "Deep learning is a type of machine learning that adapts a deep hierarchy of\nconcepts. Deep learning classifiers link the most basic version of concepts at\nthe input layer to the most abstract version of concepts at the output layer,\nalso known as a class or label. However, once trained over a finite set of\nclasses, some deep learning models do not have the power to say that a given\ninput does not belong to any of the classes and simply cannot be linked.\nCorrectly invalidating the prediction of unrelated classes is a challenging\nproblem that has been tackled in many ways in the literature. Novelty detection\ngives deep learning the ability to output \"do not know\" for novel/unseen\nclasses. Still, no attention has been given to the security aspects of novelty\ndetection. In this paper, we consider the case study of abstraction-based\nnovelty detection and show that it is not robust against adversarial samples.\nMoreover, we show the feasibility of crafting adversarial samples that fool the\ndeep learning classifier and bypass the novelty detection monitoring at the\nsame time. In other words, these monitoring boxes are hackable. We demonstrate\nthat novelty detection itself ends up as an attack surface.",
          "link": "http://arxiv.org/abs/2107.04764",
          "publishedOn": "2021-07-20T02:04:48.877Z",
          "wordCount": 661,
          "title": "Hack The Box: Fooling Deep Learning Abstraction-Based Monitors. (arXiv:2107.04764v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1\">Gail Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yahav_E/0/1/0/all/0/1\">Eran Yahav</a>",
          "description": "What is the computational model behind a Transformer? Where recurrent neural\nnetworks have direct parallels in finite state machines, allowing clear\ndiscussion and thought around architecture variants or trained models,\nTransformers have no such familiar parallel. In this paper we aim to change\nthat, proposing a computational model for the transformer-encoder in the form\nof a programming language. We map the basic components of a transformer-encoder\n-- attention and feed-forward computation -- into simple primitives, around\nwhich we form a programming language: the Restricted Access Sequence Processing\nLanguage (RASP). We show how RASP can be used to program solutions to tasks\nthat could conceivably be learned by a Transformer, and how a Transformer can\nbe trained to mimic a RASP solution. In particular, we provide RASP programs\nfor histograms, sorting, and Dyck-languages. We further use our model to relate\ntheir difficulty in terms of the number of required layers and attention heads:\nanalyzing a RASP program implies a maximum number of heads and layers necessary\nto encode a task in a transformer. Finally, we see how insights gained from our\nabstraction might be used to explain phenomena seen in recent works.",
          "link": "http://arxiv.org/abs/2106.06981",
          "publishedOn": "2021-07-20T02:04:48.833Z",
          "wordCount": 651,
          "title": "Thinking Like Transformers. (arXiv:2106.06981v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1\">Dongsheng An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_N/0/1/0/all/0/1\">Na Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xianfeng Gu</a>",
          "description": "Optimal transport (OT) plays an essential role in various areas like machine\nlearning and deep learning. However, computing discrete optimal transport plan\nfor large scale problems with adequate accuracy and efficiency is still highly\nchallenging. Recently, methods based on the Sinkhorn algorithm add an entropy\nregularizer to the prime problem and get a trade off between efficiency and\naccuracy. In this paper, we propose a novel algorithm to further improve the\nefficiency and accuracy based on Nesterov's smoothing technique. Basically, the\nnon-smooth c-transform of the Kantorovich potential is approximated by the\nsmooth Log-Sum-Exp function, which finally smooths the original non-smooth\nKantorovich dual functional (energy). The smooth Kantorovich functional can be\noptimized by the fast proximal gradient algorithm (FISTA) efficiently.\nTheoretically, the computational complexity of the proposed method is given by\n$O(n^{\\frac{5}{2}} \\sqrt{\\log n} /\\epsilon)$, which is lower than that of the\nSinkhorn algorithm. Empirically, compared with the Sinkhorn algorithm, our\nexperimental results demonstrate that the proposed method achieves faster\nconvergence and better accuracy with the same parameter.",
          "link": "http://arxiv.org/abs/2104.05802",
          "publishedOn": "2021-07-20T02:04:48.814Z",
          "wordCount": 634,
          "title": "Efficient Optimal Transport Algorithm by Accelerated Gradient descent. (arXiv:2104.05802v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1\">Prashant Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_M/0/1/0/all/0/1\">Mrigank Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varambally_S/0/1/0/all/0/1\">Sumanth Varambally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1\">Prathosh AP</a>",
          "description": "Generalization of machine learning models trained on a set of source domains\non unseen target domains with different statistics, is a challenging problem.\nWhile many approaches have been proposed to solve this problem, they only\nutilize source data during training but do not take advantage of the fact that\na single target example is available at the time of inference. Motivated by\nthis, we propose a method that effectively uses the target sample during\ninference beyond mere classification. Our method has three components - (i) A\nlabel-preserving feature or metric transformation on source data such that the\nsource samples are clustered in accordance with their class irrespective of\ntheir domain (ii) A generative model trained on the these features (iii) A\nlabel-preserving projection of the target point on the source-feature manifold\nduring inference via solving an optimization problem on the input space of the\ngenerative model using the learned metric. Finally, the projected target is\nused in the classifier. Since the projected target feature comes from the\nsource manifold and has the same label as the real target by design, the\nclassifier is expected to perform better on it than the true target. We\ndemonstrate that our method outperforms the state-of-the-art Domain\nGeneralization methods on multiple datasets and tasks.",
          "link": "http://arxiv.org/abs/2103.01134",
          "publishedOn": "2021-07-20T02:04:48.795Z",
          "wordCount": 678,
          "title": "Domain Generalization via Inference-time Label-Preserving Target Projections. (arXiv:2103.01134v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "Current out-of-distribution detection approaches usually present special\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\nproduce side effects (classification accuracy drop and slow/inefficient\ninferences). Recently, entropic out-of-distribution detection has been proposed\nas a seamless approach (i.e., a solution that avoids all the previously\nmentioned drawbacks). The entropic out-of-distribution detection solution\ncomprises the IsoMax loss for training and the entropic score for\nout-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in\nreplacement because swapping the SoftMax loss with the IsoMax loss requires no\nchanges in the model's architecture or training procedures/hyperparameters. In\nthis paper, we propose to perform what we call an isometrization of the\ndistances used in the IsoMax loss. Additionally, we propose to replace the\nentropic score with the minimum distance score. Our experiments showed that\nthese simple modifications increase out-of-distribution detection performance\nwhile keeping the solution seamless. Code available at\n$\\href{https://github.com/dlmacedo/entropic-out-of-distribution-detection}{\\text{entropic\nout-of-distribution detection}}$.",
          "link": "http://arxiv.org/abs/2105.14399",
          "publishedOn": "2021-07-20T02:04:48.643Z",
          "wordCount": 638,
          "title": "Improving Entropic Out-of-Distribution Detection using Isometric Distances and the Minimum Distance Score. (arXiv:2105.14399v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tian Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lixin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhanxing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "This is the Proceedings of ICML 2021 Workshop on Theoretic Foundation,\nCriticism, and Application Trend of Explainable AI. Deep neural networks (DNNs)\nhave undoubtedly brought great success to a wide range of applications in\ncomputer vision, computational linguistics, and AI. However, foundational\nprinciples underlying the DNNs' success and their resilience to adversarial\nattacks are still largely missing. Interpreting and theorizing the internal\nmechanisms of DNNs becomes a compelling yet controversial topic. This workshop\npays a special interest in theoretic foundations, limitations, and new\napplication trends in the scope of XAI. These issues reflect new bottlenecks in\nthe future development of XAI.",
          "link": "http://arxiv.org/abs/2107.08821",
          "publishedOn": "2021-07-20T02:04:48.623Z",
          "wordCount": 567,
          "title": "Proceedings of ICML 2021 Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI. (arXiv:2107.08821v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Zhenhou Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoyang Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chendong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>",
          "description": "Text to speech (TTS) is a crucial task for user interaction, but TTS model\ntraining relies on a sizable set of high-quality original datasets. Due to\nprivacy and security issues, the original datasets are usually unavailable\ndirectly. Recently, federated learning proposes a popular distributed machine\nlearning paradigm with an enhanced privacy protection mechanism. It offers a\npractical and secure framework for data owners to collaborate with others, thus\nobtaining a better global model trained on the larger dataset. However, due to\nthe high complexity of transformer models, the convergence process becomes slow\nand unstable in the federated learning setting. Besides, the transformer model\ntrained in federated learning is costly communication and limited computational\nspeed on clients, impeding its popularity. To deal with these challenges, we\npropose the federated dynamic transformer. On the one hand, the performance is\ngreatly improved comparing with the federated transformer, approaching\ncentralize-trained Transformer-TTS when increasing clients number. On the other\nhand, it achieves faster and more stable convergence in the training phase and\nsignificantly reduces communication time. Experiments on the LJSpeech dataset\nalso strongly prove our method's advantage.",
          "link": "http://arxiv.org/abs/2107.08795",
          "publishedOn": "2021-07-20T02:04:48.580Z",
          "wordCount": 624,
          "title": "Federated Learning with Dynamic Transformer for Text to Speech. (arXiv:2107.08795v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiuqi/0/1/0/all/0/1\">Jiuqi</a> (Elise) <a href=\"http://arxiv.org/find/cs/1/au:+Zhang/0/1/0/all/0/1\">Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulet_B/0/1/0/all/0/1\">Benoit Boulet</a>",
          "description": "With the rapid increase in the integration of renewable energy generation and\nthe wide adoption of various electric appliances, power grids are now faced\nwith more and more challenges. One prominent challenge is to implement\nefficient anomaly detection for different types of anomalous behaviors within\npower grids. These anomalous behaviors might be induced by unusual consumption\npatterns of the users, faulty grid infrastructures, outages, external\ncyberattacks, or energy fraud. Identifying such anomalies is of critical\nimportance for the reliable and efficient operation of modern power grids.\nVarious methods have been proposed for anomaly detection on power grid\ntime-series data. This paper presents a short survey of the recent advances in\nanomaly detection for power grid time-series data. Specifically, we first\noutline current research challenges in the power grid anomaly detection domain\nand further review the major anomaly detection approaches. Finally, we conclude\nthe survey by identifying the potential directions for future research.",
          "link": "http://arxiv.org/abs/2107.08835",
          "publishedOn": "2021-07-20T02:04:48.542Z",
          "wordCount": 602,
          "title": "Time Series Anomaly Detection for Smart Grids: A Survey. (arXiv:2107.08835v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wentao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiawei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bolin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wentao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bin Cui</a>",
          "description": "End-to-end AutoML has attracted intensive interests from both academia and\nindustry, which automatically searches for ML pipelines in a space induced by\nfeature engineering, algorithm/model selection, and hyper-parameter tuning.\nExisting AutoML systems, however, suffer from scalability issues when applying\nto application domains with large, high-dimensional search spaces. We present\nVolcanoML, a scalable and extensible framework that facilitates systematic\nexploration of large AutoML search spaces. VolcanoML introduces and implements\nbasic building blocks that decompose a large search space into smaller ones,\nand allows users to utilize these building blocks to compose an execution plan\nfor the AutoML problem at hand. VolcanoML further supports a Volcano-style\nexecution model - akin to the one supported by modern database systems - to\nexecute the plan constructed. Our evaluation demonstrates that, not only does\nVolcanoML raise the level of expressiveness for search space decomposition in\nAutoML, it also leads to actual findings of decomposition strategies that are\nsignificantly more efficient than the ones employed by state-of-the-art AutoML\nsystems such as auto-sklearn.",
          "link": "http://arxiv.org/abs/2107.08861",
          "publishedOn": "2021-07-20T02:04:48.525Z",
          "wordCount": 616,
          "title": "VolcanoML: Speeding up End-to-End AutoML via Scalable Search Space Decomposition. (arXiv:2107.08861v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingqi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pun_M/0/1/0/all/0/1\">Mon-on Pun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haojun Li</a>",
          "description": "Maintaining long-term exploration ability remains one of the challenges of\ndeep reinforcement learning (DRL). In practice, the reward shaping-based\napproaches are leveraged to provide intrinsic rewards for the agent to\nincentivize motivation. However, most existing IRS modules rely on attendant\nmodels or additional memory to record and analyze learning procedures, which\nleads to high computational complexity and low robustness. Moreover, they\noveremphasize the influence of a single state on exploration, which cannot\nevaluate the exploration performance from a global perspective. To tackle the\nproblem, state entropy-based methods are proposed to encourage the agent to\nvisit the state space more equitably. However, the estimation error and sample\ncomplexity are prohibitive when handling environments with high-dimensional\nobservation. In this paper, we introduce a novel metric entitled Jain's\nfairness index (JFI) to replace the entropy regularizer, which requires no\nadditional models or memory. In particular, JFI overcomes the vanishing\nintrinsic rewards problem and can be generalized into arbitrary tasks.\nFurthermore, we use a variational auto-encoder (VAE) model to capture the\nlife-long novelty of states. Finally, the global JFI score and local state\nnovelty are combined to form a multimodal intrinsic reward, controlling the\nexploration extent more precisely. Finally, extensive simulation results\ndemonstrate that our multimodal reward shaping (MMRS) method can achieve higher\nperformance in contrast to other benchmark schemes.",
          "link": "http://arxiv.org/abs/2107.08888",
          "publishedOn": "2021-07-20T02:04:48.508Z",
          "wordCount": 658,
          "title": "Multimodal Reward Shaping for Efficient Exploration in Reinforcement Learning. (arXiv:2107.08888v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koutini_K/0/1/0/all/0/1\">Khaled Koutini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eghbal_zadeh_H/0/1/0/all/0/1\">Hamid Eghbal-zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henkel_F/0/1/0/all/0/1\">Florian Henkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_J/0/1/0/all/0/1\">Jan Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1\">Gerhard Widmer</a>",
          "description": "Convolutional Neural Networks (CNNs) have been dominating classification\ntasks in various domains, such as machine vision, machine listening, and\nnatural language processing. In machine listening, while generally exhibiting\nvery good generalization capabilities, CNNs are sensitive to the specific audio\nrecording device used, which has been recognized as a substantial problem in\nthe acoustic scene classification (DCASE) community. In this study, we\ninvestigate the relationship between over-parameterization of acoustic scene\nclassification models, and their resulting generalization abilities.\nSpecifically, we test scaling CNNs in width and depth, under different\nconditions. Our results indicate that increasing width improves generalization\nto unseen devices, even without an increase in the number of parameters.",
          "link": "http://arxiv.org/abs/2107.08933",
          "publishedOn": "2021-07-20T02:04:48.490Z",
          "wordCount": 562,
          "title": "Over-Parameterization and Generalization in Audio Classification. (arXiv:2107.08933v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1\">Ian Osband</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zheng Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asghari_M/0/1/0/all/0/1\">Mohammad Asghari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahimi_M/0/1/0/all/0/1\">Morteza Ibrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1\">Benjamin Van Roy</a>",
          "description": "We introduce the \\textit{epistemic neural network} (ENN) as an interface for\nuncertainty modeling in deep learning. All existing approaches to uncertainty\nmodeling can be expressed as ENNs, and any ENN can be identified with a\nBayesian neural network. However, this new perspective provides several\npromising directions for future research. Where prior work has developed\nprobabilistic inference tools for neural networks; we ask instead, `which\nneural networks are suitable as tools for probabilistic inference?'. We propose\na clear and simple metric for progress in ENNs: the KL-divergence with respect\nto a target distribution. We develop a computational testbed based on inference\nin a neural network Gaussian process and release our code as a benchmark at\n\\url{https://github.com/deepmind/enn}. We evaluate several canonical approaches\nto uncertainty modeling in deep learning, and find they vary greatly in their\nperformance. We provide insight to the sensitivity of these results and show\nthat our metric is highly correlated with performance in sequential decision\nproblems. Finally, we provide indications that new ENN architectures can\nimprove performance in both the statistical quality and computational cost.",
          "link": "http://arxiv.org/abs/2107.08924",
          "publishedOn": "2021-07-20T02:04:48.445Z",
          "wordCount": 612,
          "title": "Epistemic Neural Networks. (arXiv:2107.08924v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ntakouris_T/0/1/0/all/0/1\">Theodoros Ntakouris</a>",
          "description": "In this document, a neural network is employed in order to estimate the\nsolution of the initial value problem in the context of non linear\ntrajectories. Such trajectories can be subject to gravity, thrust, drag,\ncentrifugal force, temperature, ambient air density and pressure. First, we\ngenerate a grid of trajectory points given a specified uniform density as a\ndesign parameter and then we investigate the performance of a neural network in\na compression and inverse problem task: the network is trained to predict the\ninitial conditions of the dynamics model we used in the simulation, given a\ntarget point in space. We investigate this as a regression task, with error\npropagation in consideration. For target points, up to a radius of 2\nkilometers, the model is able to accurately predict the initial conditions of\nthe trajectories, with sub-meter deviation. This simulation-based training\nprocess and novel real-world evaluation method is capable of computing\ntrajectories of arbitrary dimensions.",
          "link": "http://arxiv.org/abs/2107.08849",
          "publishedOn": "2021-07-20T02:04:48.411Z",
          "wordCount": 604,
          "title": "Exploring the efficacy of neural networks for trajectory compression and the inverse problem. (arXiv:2107.08849v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_B/0/1/0/all/0/1\">Bhumika Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1\">Anuj Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anjum/0/1/0/all/0/1\">Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katarya_R/0/1/0/all/0/1\">Rahul Katarya</a>",
          "description": "Cyberbullying is of extreme prevalence today. Online-hate comments, toxicity,\ncyberbullying amongst children and other vulnerable groups are only growing\nover online classes, and increased access to social platforms, especially post\nCOVID-19. It is paramount to detect and ensure minors' safety across social\nplatforms so that any violence or hate-crime is automatically detected and\nstrict action is taken against it. In our work, we explore binary\nclassification by using a combination of datasets from various social media\nplatforms that cover a wide range of cyberbullying such as sexism, racism,\nabusive, and hate-speech. We experiment through multiple models such as\nBi-LSTM, GloVe, state-of-the-art models like BERT, and apply a unique\npreprocessing technique by introducing a slang-abusive corpus, achieving a\nhigher precision in comparison to models without slang preprocessing.",
          "link": "http://arxiv.org/abs/2107.08902",
          "publishedOn": "2021-07-20T02:04:48.393Z",
          "wordCount": 626,
          "title": "Analysing Cyberbullying using Natural Language Processing by Understanding Jargon in Social Media. (arXiv:2107.08902v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_K/0/1/0/all/0/1\">Ke Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chunhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhijia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_T/0/1/0/all/0/1\">Tierui Gong</a>",
          "description": "Federated learning is a widely used distributed deep learning framework that\nprotects the privacy of each client by exchanging model parameters rather than\nraw data. However, federated learning suffers from high communication costs, as\na considerable number of model parameters need to be transmitted many times\nduring the training process, making the approach inefficient, especially when\nthe communication network bandwidth is limited. This article proposes RingFed,\na novel framework to reduce communication overhead during the training process\nof federated learning. Rather than transmitting parameters between the center\nserver and each client, as in original federated learning, in the proposed\nRingFed, the updated parameters are transmitted between each client in turn,\nand only the final result is transmitted to the central server, thereby\nreducing the communication overhead substantially. After several local updates,\nclients first send their parameters to another proximal client, not to the\ncenter server directly, to preaggregate. Experiments on two different public\ndatasets show that RingFed has fast convergence, high model accuracy, and low\ncommunication cost.",
          "link": "http://arxiv.org/abs/2107.08873",
          "publishedOn": "2021-07-20T02:04:48.329Z",
          "wordCount": 609,
          "title": "RingFed: Reducing Communication Costs in Federated Learning on Non-IID Data. (arXiv:2107.08873v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1\">Adish Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafferty_A/0/1/0/all/0/1\">Anna N. Rafferty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radanovic_G/0/1/0/all/0/1\">Goran Radanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_N/0/1/0/all/0/1\">Neil T. Heffernan</a>",
          "description": "This survey article has grown out of the RL4ED workshop organized by the\nauthors at the Educational Data Mining (EDM) 2021 conference. We organized this\nworkshop as part of a community-building effort to bring together researchers\nand practitioners interested in the broad areas of reinforcement learning (RL)\nand education (ED). This article aims to provide an overview of the workshop\nactivities and summarize the main research directions in the area of RL for ED.",
          "link": "http://arxiv.org/abs/2107.08828",
          "publishedOn": "2021-07-20T02:04:48.308Z",
          "wordCount": 509,
          "title": "Reinforcement Learning for Education: Opportunities and Challenges. (arXiv:2107.08828v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08787",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1\">Yichen Lu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fridlyand_J/0/1/0/all/0/1\">Jane Fridlyand</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tang_T/0/1/0/all/0/1\">Tiffany Tang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Qi_T/0/1/0/all/0/1\">Ting Qi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Simon_N/0/1/0/all/0/1\">Noah Simon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Leng_N/0/1/0/all/0/1\">Ning Leng</a>",
          "description": "Finding translational biomarkers stands center stage of the future of\npersonalized medicine in healthcare. We observed notable challenges in\nidentifying robust biomarkers as some with great performance in one scenario\noften fail to perform well in new trials (e.g. different population,\nindications). With rapid development in the clinical trial world (e.g. assay,\ndisease definition), new trials very likely differ from legacy ones in many\nperspectives and in development of biomarkers this heterogeneity should be\nconsidered. In response, we recommend considering building in the heterogeneity\nwhen evaluating biomarkers. In this paper, we present one evaluation strategy\nby using leave-one-study-out (LOSO) in place of conventional cross-validation\n(cv) methods to account for the potential heterogeneity across trials used for\nbuilding and testing the biomarkers. To demonstrate the performance of K-fold\nvs LOSO cv in estimating the effect size of biomarkers, we leveraged data from\nclinical trials and simulation studies. In our assessment, LOSO cv provided a\nmore objective estimate of the future performance. This conclusion remained\ntrue across different evaluation metrics and different statistical methods.",
          "link": "http://arxiv.org/abs/2107.08787",
          "publishedOn": "2021-07-20T02:04:48.291Z",
          "wordCount": 633,
          "title": "The Future will be Different than Today: Model Evaluation Considerations when Developing Translational Clinical Biomarker. (arXiv:2107.08787v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2008.01683",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Azzimonti_L/0/1/0/all/0/1\">Laura Azzimonti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Corani_G/0/1/0/all/0/1\">Giorgio Corani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scutari_M/0/1/0/all/0/1\">Marco Scutari</a>",
          "description": "Score functions for learning the structure of Bayesian networks in the\nliterature assume that data are a homogeneous set of observations; whereas it\nis often the case that they comprise different related, but not homogeneous,\ndata sets collected in different ways. In this paper we propose a new Bayesian\nDirichlet score, which we call Bayesian Hierarchical Dirichlet (BHD). The\nproposed score is based on a hierarchical model that pools information across\ndata sets to learn a single encompassing network structure, while taking into\naccount the differences in their probabilistic structures. We derive a\nclosed-form expression for BHD using a variational approximation of the\nmarginal likelihood, we study the associated computational cost and we evaluate\nits performance using simulated data. We find that, when data comprise multiple\nrelated data sets, BHD outperforms the Bayesian Dirichlet equivalent uniform\n(BDeu) score in terms of reconstruction accuracy as measured by the Structural\nHamming distance, and that it is as accurate as BDeu when data are homogeneous.\nThis improvement is particularly clear when either the number of variables in\nthe network or the number of observations is large. Moreover, the estimated\nnetworks are sparser and therefore more interpretable than those obtained with\nBDeu thanks to a lower number of false positive arcs.",
          "link": "http://arxiv.org/abs/2008.01683",
          "publishedOn": "2021-07-20T02:04:47.806Z",
          "wordCount": 682,
          "title": "A Bayesian Hierarchical Score for Structure Learning from Related Data Sets. (arXiv:2008.01683v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takeuchi_K/0/1/0/all/0/1\">Koh Takeuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishida_R/0/1/0/all/0/1\">Ryo Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1\">Hisashi Kashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onishi_M/0/1/0/all/0/1\">Masaki Onishi</a>",
          "description": "Crowd movement guidance has been a fascinating problem in various fields,\nsuch as easing traffic congestion in unusual events and evacuating people from\nan emergency-affected area. To grab the reins of crowds, there has been\nconsiderable demand for a decision support system that can answer a typical\nquestion: ``what will be the outcomes of each of the possible options in the\ncurrent situation. In this paper, we consider the problem of estimating the\neffects of crowd movement guidance from past data. To cope with limited amount\nof available data biased by past decision-makers, we leverage two recent\ntechniques in deep representation learning for spatial data analysis and causal\ninference. We use a spatial convolutional operator to extract effective spatial\nfeatures of crowds from a small amount of data and use balanced representation\nlearning based on the integral probability metrics to mitigate the selection\nbias and missing counterfactual outcomes. To evaluate the performance on\nestimating the treatment effects of possible guidance, we use a multi-agent\nsimulator to generate realistic data on evacuation scenarios in a crowded\ntheater, since there are no available datasets recording outcomes of all\npossible crowd movement guidance. The results of three experiments demonstrate\nthat our proposed method reduces the estimation error by at most 56% from\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2102.03980",
          "publishedOn": "2021-07-20T02:04:47.788Z",
          "wordCount": 685,
          "title": "Grab the Reins of Crowds: Estimating the Effects of Crowd Movement Guidance Using Causal Inference. (arXiv:2102.03980v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malach_E/0/1/0/all/0/1\">Eran Malach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yehudai_G/0/1/0/all/0/1\">Gilad Yehudai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalev_Shwartz_S/0/1/0/all/0/1\">Shai Shalev-Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1\">Ohad Shamir</a>",
          "description": "Several recent works have shown separation results between deep neural\nnetworks, and hypothesis classes with inferior approximation capacity such as\nshallow networks or kernel classes. On the other hand, the fact that deep\nnetworks can efficiently express a target function does not mean that this\ntarget function can be learned efficiently by deep neural networks. In this\nwork we study the intricate connection between learnability and approximation\ncapacity. We show that learnability with deep networks of a target function\ndepends on the ability of simpler classes to approximate the target.\nSpecifically, we show that a necessary condition for a function to be learnable\nby gradient descent on deep neural networks is to be able to approximate the\nfunction, at least in a weak sense, with shallow neural networks. We also show\nthat a class of functions can be learned by an efficient statistical query\nalgorithm if and only if it can be approximated in a weak sense by some kernel\nclass. We give several examples of functions which demonstrate depth\nseparation, and conclude that they cannot be efficiently learned, even by a\nhypothesis class that can efficiently approximate them.",
          "link": "http://arxiv.org/abs/2102.00434",
          "publishedOn": "2021-07-20T02:04:47.732Z",
          "wordCount": 672,
          "title": "The Connection Between Approximation, Depth Separation and Learnability in Neural Networks. (arXiv:2102.00434v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16336",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Goren_E/0/1/0/all/0/1\">Emily M. Goren</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1\">Ranjan Maitra</a>",
          "description": "Partially recorded data are frequently encountered in many applications and\nusually clustered by first removing incomplete cases or features with missing\nvalues, or by imputing missing values, followed by application of a clustering\nalgorithm to the resulting altered dataset. Here, we develop clustering\nmethodology through a model-based approach using the marginal density for the\nobserved values, assuming a finite mixture model of multivariate $t$\ndistributions. We compare our approximate algorithm to the corresponding full\nexpectation-maximization (EM) approach that considers the missing values in the\nincomplete data set and makes a missing at random (MAR) assumption, as well as\ncase deletion and imputation methods. Since only the observed values are\nutilized, our approach is computationally more efficient than imputation or\nfull EM. Simulation studies demonstrate that our approach has favorable\nrecovery of the true cluster partition compared to case deletion and imputation\nunder various missingness mechanisms, and is at least competitive with the full\nEM approach, even when MAR assumptions are violated. Our methodology is\ndemonstrated on a problem of clustering gamma-ray bursts and is implemented at\nhttps://github.com/emilygoren/MixtClust.",
          "link": "http://arxiv.org/abs/2103.16336",
          "publishedOn": "2021-07-20T02:04:47.479Z",
          "wordCount": null,
          "title": "Model-based clustering of partial records. (arXiv:2103.16336v3 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04046",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ahfock_D/0/1/0/all/0/1\">Daniel Ahfock</a>, <a href=\"http://arxiv.org/find/stat/1/au:+McLachlan_G/0/1/0/all/0/1\">Geoffrey J. McLachlan</a>",
          "description": "There has been increasing attention to semi-supervised learning (SSL)\napproaches in machine learning to forming a classifier in situations where the\ntraining data for a classifier consists of a limited number of classified\nobservations but a much larger number of unclassified observations. This is\nbecause the procurement of classified data can be quite costly due to high\nacquisition costs and subsequent financial, time, and ethical issues that can\narise in attempts to provide the true class labels for the unclassified data\nthat have been acquired. We provide here a review of statistical SSL approaches\nto this problem, focussing on the recent result that a classifier formed from a\npartially classified sample can actually have smaller expected error rate than\nthat if the sample were completely classified.",
          "link": "http://arxiv.org/abs/2104.04046",
          "publishedOn": "2021-07-20T02:04:47.476Z",
          "wordCount": null,
          "title": "Semi-Supervised Learning of Classifiers from a Statistical Perspective: A Brief Review. (arXiv:2104.04046v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haibin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>",
          "description": "Automatic speaker verification (ASV) is a well developed technology for\nbiometric identification, and has been ubiquitous implemented in\nsecurity-critic applications, such as banking and access control. However,\nprevious works have shown that ASV is under the radar of adversarial attacks,\nwhich are very similar to their original counterparts from human's perception,\nyet will manipulate the ASV render wrong prediction. Due to the very late\nemergence of adversarial attacks for ASV, effective countermeasures against\nthem are limited. Given that the security of ASV is of high priority, in this\nwork, we propose the idea of \"voting for the right answer\" to prevent risky\ndecisions of ASV in blind spot areas, by employing random sampling and voting.\nExperimental results show that our proposed method improves the robustness\nagainst both the limited-knowledge attackers by pulling the adversarial samples\nout of the blind spots, and the perfect-knowledge attackers by introducing\nrandomness and increasing the attackers' budgets.",
          "link": "http://arxiv.org/abs/2106.07868",
          "publishedOn": "2021-07-20T02:04:47.463Z",
          "wordCount": null,
          "title": "Voting for the right answer: Adversarial defense for speaker verification. (arXiv:2106.07868v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1\">Wei Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_G/0/1/0/all/0/1\">Guang Tan</a>",
          "description": "We consider graph representation learning in a self-supervised manner. Graph\nneural networks (GNNs) use neighborhood aggregation as a core component that\nresults in feature smoothing among nodes in proximity. While successful in\nvarious prediction tasks, such a paradigm falls short of capturing nodes'\nsimilarities over a long distance, which proves to be important for\nhigh-quality learning. To tackle this problem, we strengthen the graph with two\nadditional graph views, in which nodes are directly linked to those with the\nmost similar features or local structures. Not restricted by connectivity in\nthe original graph, the generated views allow the model to enhance its\nexpressive power with new and complementary perspectives from which to look at\nthe relationship between nodes. Following a contrastive learning approach, we\npropose a method that aims to maximize the agreement between representations\nacross generated views and the original graph. We also propose a channel-level\ncontrast approach that greatly reduces computation cost, compared to the\ncommonly used node level contrast, which requires computation cost quadratic in\nthe number of nodes. Extensive experiments on seven assortative graphs and four\ndisassortative graphs demonstrate the effectiveness of our approach.",
          "link": "http://arxiv.org/abs/2106.03723",
          "publishedOn": "2021-07-20T02:04:47.461Z",
          "wordCount": null,
          "title": "Self-Supervised Graph Learning with Proximity-based Views and Channel Contrast. (arXiv:2106.03723v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moreira_T/0/1/0/all/0/1\">T&#xfa;lio Marcondes Moreira</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Faria_J/0/1/0/all/0/1\">Jackson Geraldo de Faria Jr</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Melo_P/0/1/0/all/0/1\">Pedro O.S. Vaz de Melo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chaimowicz_L/0/1/0/all/0/1\">Luiz Chaimowicz</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Medeiros_Ribeiro_G/0/1/0/all/0/1\">Gilberto Medeiros-Ribeiro</a> (1) ((1) Universidade Federal de Minas Gerais, Belo Horizonte, Brazil)",
          "description": "Tidal range structures have been considered for large scale electricity\ngeneration for their potential ability to produce reasonable predictable energy\nwithout the emission of greenhouse gases. Once the main forcing components for\ndriving the tides have deterministic dynamics, the available energy in a given\ntidal power plant has been estimated, through analytical and numerical\noptimisation routines, as a mostly predictable event. This constraint imposes\nstate-of-art flexible operation methods to rely on tidal predictions\n(concurrent with measured data and up to a multiple of half-tidal cycles into\nthe future) to infer best operational strategies for tidal lagoons, with the\nadditional cost of requiring to run optimisation routines for every new tide.\nIn this paper, we propose a novel optimised operation of tidal lagoons with\nproximal policy optimisation through Unity ML-Agents. We compare this technique\nwith 6 different operation optimisation approaches (baselines) devised from the\nliterature, utilising the Swansea Bay Tidal Lagoon as a case study. We show\nthat our approach is successful in maximising energy generation through an\noptimised operational policy of turbines and sluices, yielding competitive\nresults with state-of-the-art methods of optimisation, regardless of test data\nused, requiring training once and performing real-time flexible control with\nmeasured ocean data only.",
          "link": "http://arxiv.org/abs/2106.10360",
          "publishedOn": "2021-07-20T02:04:47.421Z",
          "wordCount": null,
          "title": "Prediction-Free, Real-Time Flexible Control of Tidal Lagoons through Proximal Policy Optimisation: A Case Study for the Swansea Lagoon. (arXiv:2106.10360v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hafez_Kolahi_H/0/1/0/all/0/1\">Hassan Hafez-Kolahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniri_B/0/1/0/all/0/1\">Behrad Moniri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1\">Shohreh Kasaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baghshah_M/0/1/0/all/0/1\">Mahdieh Soleymani Baghshah</a>",
          "description": "In parametric Bayesian learning, a prior is assumed on the parameter $W$\nwhich determines the distribution of samples. In this setting, Minimum Excess\nRisk (MER) is defined as the difference between the minimum expected loss\nachievable when learning from data and the minimum expected loss that could be\nachieved if $W$ was observed. In this paper, we build upon and extend the\nrecent results of (Xu & Raginsky, 2020) to analyze the MER in Bayesian learning\nand derive information-theoretic bounds on it. We formulate the problem as a\n(constrained) rate-distortion optimization and show how the solution can be\nbounded above and below by two other rate-distortion functions that are easier\nto study. The lower bound represents the minimum possible excess risk\nachievable by any process using $R$ bits of information from the parameter $W$.\nFor the upper bound, the optimization is further constrained to use $R$ bits\nfrom the training set, a setting which relates MER to information-theoretic\nbounds on the generalization gap in frequentist learning. We derive\ninformation-theoretic bounds on the difference between these upper and lower\nbounds and show that they can provide order-wise tight rates for MER under\ncertain conditions. This analysis gives more insight into the\ninformation-theoretic nature of Bayesian learning as well as providing novel\nbounds.",
          "link": "http://arxiv.org/abs/2105.04180",
          "publishedOn": "2021-07-20T02:04:47.413Z",
          "wordCount": null,
          "title": "Rate-Distortion Analysis of Minimum Excess Risk in Bayesian Learning. (arXiv:2105.04180v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05556",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Ozcelik_R/0/1/0/all/0/1\">R&#x131;za &#xd6;z&#xe7;elik</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bag_A/0/1/0/all/0/1\">Alperen Ba&#x11f;</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Atil_B/0/1/0/all/0/1\">Berk At&#x131;l</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ozgur_A/0/1/0/all/0/1\">Arzucan &#xd6;zg&#xfc;r</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ozkirimli_E/0/1/0/all/0/1\">Elif &#xd6;zk&#x131;r&#x131;ml&#x131;</a>",
          "description": "Motivation: Computational models that accurately identify high-affinity\nprotein-compound pairs can accelerate drug discovery pipelines. These models\naim to learn binding mechanics through drug-target interaction datasets and use\nthe learned knowledge for predicting the affinity of an input protein-compound\npair. However, the datasets they rely on bear misleading patterns that bias\nmodels towards memorizing dataset-specific biomolecule properties, instead of\nlearning binding mechanics. This results in models that struggle while\npredicting drug-target affinities (DTA), especially between de novo\nbiomolecules. Here we present DebiasedDTA, the first DTA model debiasing\napproach that avoids dataset biases in order to boost affinity prediction for\nnovel biomolecules. DebiasedDTA uses ensemble learning and sample weight\nadaptation for bias identification and avoidance and is applicable to almost\nall existing DTA prediction models. Results: The results show that DebiasedDTA\ncan boost models while predicting the interactions between novel biomolecules.\nKnown biomolecules also benefit from the performance improvement, especially\nwhen the test biomolecules are dissimilar to the training set. The experiments\nalso show that DebiasedDTA can augment DTA prediction models of different input\nand model structures and is able to avoid biases of different sources.\nAvailability and Implementation: The source code, the models, and the datasets\nare freely available for download at\nhttps://github.com/boun-tabi/debiaseddta-reproduce, implementation in Python3,\nand supported for Linux, MacOS and MS Windows. Contact:\narzucan.ozgur@boun.edu.tr, elif.ozkirimli@roche.com",
          "link": "http://arxiv.org/abs/2107.05556",
          "publishedOn": "2021-07-20T02:04:47.411Z",
          "wordCount": null,
          "title": "DebiasedDTA: Model Debiasing to Boost Drug-Target Affinity Prediction. (arXiv:2107.05556v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiaqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1\">Rex Ying</a>",
          "description": "Structural features are important features in graph datasets. However,\nalthough there are some correlation analysis of features based on covariance,\nthere is no relevant research on exploring structural feature correlation on\ngraphs with graph neural network based models. In this paper, we introduce\ngraph feature to feature (Fea2Fea) prediction pipelines in a low dimensional\nspace to explore some preliminary results on structural feature correlation,\nwhich is based on graph neural network. The results show that there exists high\ncorrelation between some of the structural features. A non-redundant feature\ncombination with initial node features, which is filtered by graph neural\nnetwork has improved its classification accuracy in some graph datasets. We\ncompare the difference between concatenation methods on connecting embeddings\nbetween features and show that the simplest is the best. We generalize on the\nsynthetic geometric graphs and certify the results on prediction difficulty\nbetween two structural features.",
          "link": "http://arxiv.org/abs/2106.13061",
          "publishedOn": "2021-07-20T02:04:47.406Z",
          "wordCount": null,
          "title": "Fea2Fea: Exploring Structural Feature Correlations via Graph Neural Networks. (arXiv:2106.13061v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06419",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jinke Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lv_P/0/1/0/all/0/1\">Peiqing Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haiying Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1\">Changfa Shi</a>",
          "description": "Background and objective: In this paper, a modified U-Net based framework is\npresented, which leverages techniques from Squeeze-and-Excitation (SE) block,\nAtrous Spatial Pyramid Pooling (ASPP) and residual learning for accurate and\nrobust liver CT segmentation, and the effectiveness of the proposed method was\ntested on two public datasets LiTS17 and SLiver07.\n\nMethods: A new network architecture called SAR-U-Net was designed. Firstly,\nthe SE block is introduced to adaptively extract image features after each\nconvolution in the U-Net encoder, while suppressing irrelevant regions, and\nhighlighting features of specific segmentation task; Secondly, ASPP was\nemployed to replace the transition layer and the output layer, and acquire\nmulti-scale image information via different receptive fields. Thirdly, to\nalleviate the degradation problem, the traditional convolution block was\nreplaced with the residual block and thus prompt the network to gain accuracy\nfrom considerably increased depth.\n\nResults: In the LiTS17 experiment, the mean values of Dice, VOE, RVD, ASD and\nMSD were 95.71, 9.52, -0.84, 1.54 and 29.14, respectively. Compared with other\nclosely related 2D-based models, the proposed method achieved the highest\naccuracy. In the experiment of the SLiver07, the mean values of Dice, VOE, RVD,\nASD and MSD were 97.31, 5.37, -1.08, 1.85 and 27.45, respectively. Compared\nwith other closely related models, the proposed method achieved the highest\nsegmentation accuracy except for the RVD.\n\nConclusion: The proposed model enables a great improvement on the accuracy\ncompared to 2D-based models, and its robustness in circumvent challenging\nproblems, such as small liver regions, discontinuous liver regions, and fuzzy\nliver boundaries, is also well demonstrated and validated.",
          "link": "http://arxiv.org/abs/2103.06419",
          "publishedOn": "2021-07-20T02:04:47.391Z",
          "wordCount": null,
          "title": "SAR-U-Net: squeeze-and-excitation block and atrous spatial pyramid pooling based residual U-Net for automatic liver segmentation in Computed Tomography. (arXiv:2103.06419v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09022",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chung_H/0/1/0/all/0/1\">Hyungjin Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huh_J/0/1/0/all/0/1\">Jaeyoung Huh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_G/0/1/0/all/0/1\">Geon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_Y/0/1/0/all/0/1\">Yong Keun Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>",
          "description": "Optical diffraction tomography (ODT) produces three dimensional distribution\nof refractive index (RI) by measuring scattering fields at various angles.\nAlthough the distribution of RI index is highly informative, due to the missing\ncone problem stemming from the limited-angle acquisition of holograms,\nreconstructions have very poor resolution along axial direction compared to the\nhorizontal imaging plane. To solve this issue, here we present a novel\nunsupervised deep learning framework, which learns the probability distribution\nof missing projection views through optimal transport driven cycleGAN.\nExperimental results show that missing cone artifact in ODT can be\nsignificantly resolved by the proposed method.",
          "link": "http://arxiv.org/abs/2103.09022",
          "publishedOn": "2021-07-20T02:04:47.260Z",
          "wordCount": null,
          "title": "Missing Cone Artifacts Removal in ODT using Unsupervised Deep Learning in Projection Domain. (arXiv:2103.09022v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiqi Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Q/0/1/0/all/0/1\">Qi Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>",
          "description": "In deep learning with differential privacy (DP), the neural network achieves\nthe privacy usually at the cost of slower convergence (and thus lower\nperformance) than its non-private counterpart. This work gives the first\nconvergence analysis of the DP deep learning, through the lens of training\ndynamics and the neural tangent kernel (NTK). Our convergence theory\nsuccessfully characterizes the effects of two key components in the DP\ntraining: the per-sample clipping (flat or layerwise) and the noise addition.\nOur analysis not only initiates a general principled framework to understand\nthe DP deep learning with any network architecture and loss function, but also\nmotivates a new clipping method -- the global clipping, that significantly\nimproves the convergence while preserving the same privacy guarantee as the\nexisting local clipping.\n\nIn terms of theoretical results, we establish the precise connection between\nthe per-sample clipping and NTK matrix. We show that in the gradient flow,\ni.e., with infinitesimal learning rate, the noise level of DP optimizers does\nnot affect the convergence. We prove that DP gradient descent (GD) with global\nclipping guarantees the monotone convergence to zero loss, which can be\nviolated by the existing DP-GD with local clipping. Notably, our analysis\nframework easily extends to other optimizers, e.g., DP-Adam. Empirically\nspeaking, DP optimizers equipped with global clipping perform strongly on a\nwide range of classification and regression tasks. In particular, our global\nclipping is surprisingly effective at learning calibrated classifiers, in\ncontrast to the existing DP classifiers which are oftentimes over-confident and\nunreliable. Implementation-wise, the new clipping can be realized by adding one\nline of code into the Opacus library.",
          "link": "http://arxiv.org/abs/2106.07830",
          "publishedOn": "2021-07-20T02:04:47.255Z",
          "wordCount": null,
          "title": "On the Convergence of Deep Learning with Differential Privacy. (arXiv:2106.07830v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaolin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zejin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongji Wang</a>",
          "description": "The increasing concerns about data privacy and security drive an emerging\nfield of studying privacy-preserving machine learning from isolated data\nsources, i.e., federated learning. A class of federated learning, vertical\nfederated learning, where different parties hold different features for common\nusers, has a great potential of driving a more variety of business cooperation\namong enterprises in many fields. In machine learning, decision tree ensembles\nsuch as gradient boosting decision tree (GBDT) and random forest are widely\napplied powerful models with high interpretability and modeling efficiency.\nHowever, the interpretability is compromised in state-of-the-art vertical\nfederated learning frameworks such as SecureBoost with anonymous features to\navoid possible data breaches. To address this issue in the inference process,\nin this paper, we propose Fed-EINI to protect data privacy and allow the\ndisclosure of feature meaning by concealing decision paths with a\ncommunication-efficient secure computation method for inference outputs. The\nadvantages of Fed-EINI will be demonstrated through both theoretical analysis\nand extensive numerical results.",
          "link": "http://arxiv.org/abs/2105.09540",
          "publishedOn": "2021-07-20T02:04:47.253Z",
          "wordCount": null,
          "title": "Fed-EINI: An Efficient and Interpretable Inference Framework for Decision Tree Ensembles in Federated Learning. (arXiv:2105.09540v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11793",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Wanjun Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1\">Xiang Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1\">Minghua Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Low_S/0/1/0/all/0/1\">Steven H. Low</a>",
          "description": "AC optimal power flow (AC-OPF) problems need to be solved more frequently in\nthe future to maintain stable and economic power system operation. To tackle\nthis challenge, a deep neural network-based voltage-constrained approach\n(DeepOPF-V) is proposed to solve AC-OPF problems with high computational\nefficiency. Its unique design predicts voltages of all buses and then uses them\nto reconstruct the remaining variables without solving non-linear AC power flow\nequations. A fast post-processing process is developed to enforce the box\nconstraints. The effectiveness of DeepOPF-V is validated by simulations on IEEE\n118/300-bus systems and a 2000-bus test system. Compared with existing studies,\nDeepOPF-V achieves decent computation speedup up to four orders of magnitude\nand comparable performance in optimality gap and preserving the feasibility of\nthe solution.",
          "link": "http://arxiv.org/abs/2103.11793",
          "publishedOn": "2021-07-20T02:04:47.241Z",
          "wordCount": null,
          "title": "DeepOPF-V: Solving AC-OPF Problems Efficiently. (arXiv:2103.11793v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1\">Meng Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zuo-Jun Shen</a>",
          "description": "In this work, we propose a deep reinforcement learning (DRL) model for\nfinding a feasible solution for (mixed) integer programming (MIP) problems.\nFinding a feasible solution for MIP problems is critical because many\nsuccessful heuristics rely on a known initial feasible solution. However, it is\nin general NP-hard. Inspired by the feasibility pump (FP), a well-known\nheuristic for searching feasible MIP solutions, we develop a smart feasibility\npump (SFP) method using DRL. In addition to multi-layer perception (MLP), we\npropose a novel convolution neural network (CNN) structure for the policy\nnetwork to capture the hidden information of the constraint matrix of the MIP\nproblem. Numerical experiments on various problem instances show that SFP\nsignificantly outperforms the classic FP in terms of the number of steps\nrequired to reach the first feasible solution. Moreover, the CNN structure\nworks without the projection of the current solution as the input, which saves\nthe computational effort at each step of the FP algorithms to find projections.\nThis highlights the representational power of the CNN structure.",
          "link": "http://arxiv.org/abs/2102.09663",
          "publishedOn": "2021-07-20T02:04:47.225Z",
          "wordCount": null,
          "title": "Smart Feasibility Pump: Reinforcement Learning for (Mixed) Integer Programming. (arXiv:2102.09663v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1\">Aaron M. Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>",
          "description": "We present a novel sensor-based learning navigation algorithm to compute a\ncollision-free trajectory for a robot in dense and dynamic environments with\nmoving obstacles or targets. Our approach uses deep reinforcement\nlearning-based expert policy that is trained using a sim2real paradigm. In\norder to increase the reliability and handle the failure cases of the expert\npolicy, we combine with a policy extraction technique to transform the\nresulting policy into a decision tree format. The resulting decision tree has\nproperties which we use to analyze and modify the policy and improve\nperformance on navigation metrics including smoothness, frequency of\noscillation, frequency of immobilization, and obstruction of target. We are\nable to modify the policy to address these imperfections without retraining,\ncombining the learning power of deep learning with the control of\ndomain-specific algorithms. We highlight the benefits of our algorithm in\nsimulated environments and navigating a Clearpath Jackal robot among moving\npedestrians. (Videos at this url:\nhttps://gamma.umd.edu/researchdirections/xrl/navviper)",
          "link": "http://arxiv.org/abs/2104.10818",
          "publishedOn": "2021-07-20T02:04:47.223Z",
          "wordCount": null,
          "title": "XAI-N: Sensor-based Robot Navigation using Expert Policies and Decision Trees. (arXiv:2104.10818v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xueying Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>",
          "description": "Automated surgical gesture recognition is of great importance in\nrobot-assisted minimally invasive surgery. However, existing methods assume\nthat training and testing data are from the same domain, which suffers from\nsevere performance degradation when a domain gap exists, such as the simulator\nand real robot. In this paper, we propose a novel unsupervised domain\nadaptation framework which can simultaneously transfer multi-modality\nknowledge, i.e., both kinematic and visual data, from simulator to real robot.\nIt remedies the domain gap with enhanced transferable features by using\ntemporal cues in videos, and inherent correlations in multi-modal towards\nrecognizing gesture. Specifically, we first propose an MDO-K to align\nkinematics, which exploits temporal continuity to transfer motion directions\nwith smaller gap rather than position values, relieving the adaptation burden.\nMoreover, we propose a KV-Relation-ATT to transfer the co-occurrence signals of\nkinematics and vision. Such features attended by correlation similarity are\nmore informative for enhancing domain-invariance of the model. Two feature\nalignment strategies benefit the model mutually during the end-to-end learning\nprocess. We extensively evaluate our method for gesture recognition using DESK\ndataset with peg transfer procedure. Results show that our approach recovers\nthe performance with great improvement gains, up to 12.91% in ACC and 20.16% in\nF1score without using any annotations in real robot.",
          "link": "http://arxiv.org/abs/2103.04075",
          "publishedOn": "2021-07-20T02:04:47.219Z",
          "wordCount": null,
          "title": "Domain Adaptive Robotic Gesture Recognition with Unsupervised Kinematic-Visual Data Alignment. (arXiv:2103.04075v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Michelle M. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1\">Marinka Zitnik</a>",
          "description": "Spatial context is central to understanding health and disease. Yet reference\nprotein interaction networks lack such contextualization, thereby limiting the\nstudy of where protein interactions likely occur in the human body and how they\nmay be altered in disease. Contextualized protein interactions could better\ncharacterize genes with disease-specific interactions and elucidate diseases'\nmanifestation in specific cell types. Here, we introduce AWARE, a graph neural\nmessage passing approach to inject cellular and tissue context into protein\nembeddings. AWARE optimizes for a multi-scale embedding space, whose structure\nreflects network topology at a single-cell resolution. We construct a\nmulti-scale network of the Human Cell Atlas and apply AWARE to learn protein,\ncell type, and tissue embeddings that uphold cell type and tissue hierarchies.\nWe demonstrate AWARE's utility on the novel task of predicting whether a\nprotein is altered in disease and where that association most likely manifests\nin the human body. To this end, AWARE outperforms generic embeddings without\ncontextual information by at least 12.5%, showing AWARE's potential to reveal\ncontext-dependent roles of proteins in disease.",
          "link": "http://arxiv.org/abs/2106.02246",
          "publishedOn": "2021-07-20T02:04:47.216Z",
          "wordCount": null,
          "title": "Deep Contextual Learners for Protein Networks. (arXiv:2106.02246v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvirul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1\">Muhammad Imran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1\">Ferda Ofli</a>",
          "description": "Images shared on social media help crisis managers gain situational awareness\nand assess incurred damages, among other response tasks. As the volume and\nvelocity of such content are typically high, real-time image classification has\nbecome an urgent need for a faster disaster response. Recent advances in\ncomputer vision and deep neural networks have enabled the development of models\nfor real-time image classification for a number of tasks, including detecting\ncrisis incidents, filtering irrelevant images, classifying images into specific\nhumanitarian categories, and assessing the severity of the damage. To develop\nrobust real-time models, it is necessary to understand the capability of the\npublicly available pre-trained models for these tasks, which remains to be\nunder-explored in the crisis informatics literature. In this study, we address\nsuch limitations by investigating ten different network architectures for four\ndifferent tasks using the largest publicly available datasets for these tasks.\nWe also explore various data augmentation strategies, semi-supervised\ntechniques, and a multitask learning setup. In our extensive experiments, we\nachieve promising results.",
          "link": "http://arxiv.org/abs/2104.04184",
          "publishedOn": "2021-07-20T02:04:47.190Z",
          "wordCount": null,
          "title": "Robust Training of Social Media Image Classification Models for Rapid Disaster Response. (arXiv:2104.04184v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zuowei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shijun Zhang</a>",
          "description": "This paper concentrates on the approximation power of deep feed-forward\nneural networks in terms of width and depth. It is proved by construction that\nReLU networks with width $\\mathcal{O}\\big(\\max\\{d\\lfloor N^{1/d}\\rfloor,\\,\nN+2\\}\\big)$ and depth $\\mathcal{O}(L)$ can approximate a H\\\"older continuous\nfunction on $[0,1]^d$ with an approximation rate\n$\\mathcal{O}\\big(\\lambda\\sqrt{d} (N^2L^2\\ln N)^{-\\alpha/d}\\big)$, where\n$\\alpha\\in (0,1]$ and $\\lambda>0$ are H\\\"older order and constant,\nrespectively. Such a rate is optimal up to a constant in terms of width and\ndepth separately, while existing results are only nearly optimal without the\nlogarithmic factor in the approximation rate. More generally, for an arbitrary\ncontinuous function $f$ on $[0,1]^d$, the approximation rate becomes\n$\\mathcal{O}\\big(\\,\\sqrt{d}\\,\\omega_f\\big( (N^2L^2\\ln N)^{-1/d}\\big)\\,\\big)$,\nwhere $\\omega_f(\\cdot)$ is the modulus of continuity. We also extend our\nanalysis to any continuous function $f$ on a bounded set. Particularly, if ReLU\nnetworks with depth $31$ and width $\\mathcal{O}(N)$ are used to approximate\none-dimensional Lipschitz continuous functions on $[0,1]$ with a Lipschitz\nconstant $\\lambda>0$, the approximation rate in terms of the total number of\nparameters, $W=\\mathcal{O}(N^2)$, becomes $\\mathcal{O}(\\tfrac{\\lambda}{W\\ln\nW})$, which has not been discovered in the literature for fixed-depth ReLU\nnetworks.",
          "link": "http://arxiv.org/abs/2103.00502",
          "publishedOn": "2021-07-20T02:04:47.189Z",
          "wordCount": null,
          "title": "Optimal Approximation Rate of ReLU Networks in terms of Width and Depth. (arXiv:2103.00502v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12534",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Kim_J/0/1/0/all/0/1\">Joonho Kim</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Oz_Y/0/1/0/all/0/1\">Yaron Oz</a>",
          "description": "We consider information spreading measures in randomly initialized\nvariational quantum circuits and introduce entanglement diagnostics for\nefficient variational quantum/classical computations. We establish a robust\nconnection between entanglement measures and optimization accuracy by solving\ntwo eigensolver problems for Ising Hamiltonians with nearest-neighbor and\nlong-range spin interactions. As the circuit depth affects the average\nentanglement of random circuit states, the entanglement diagnostics can\nidentify a high-performing depth range for optimization tasks encoded in local\nHamiltonians. We argue, based on an eigensolver problem for the\nSachdev-Ye-Kitaev model, that entanglement alone is insufficient as a\ndiagnostic to the approximation of volume-law entangled target states and that\na large number of circuit parameters is needed for such an optimization task.",
          "link": "http://arxiv.org/abs/2102.12534",
          "publishedOn": "2021-07-20T02:04:47.181Z",
          "wordCount": null,
          "title": "Entanglement Diagnostics for Efficient Quantum Computation. (arXiv:2102.12534v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>",
          "description": "Generalization to out-of-distribution (OOD) data is a capability natural to\nhumans yet challenging for machines to reproduce. This is because most learning\nalgorithms strongly rely on the i.i.d.~assumption on source/target data, which\nis often violated in practice due to domain shift. Domain generalization (DG)\naims to achieve OOD generalization by using only source data for model\nlearning. Since first introduced in 2011, research in DG has made great\nprogresses. In particular, intensive research in this topic has led to a broad\nspectrum of methodologies, e.g., those based on domain alignment,\nmeta-learning, data augmentation, or ensemble learning, just to name a few; and\nhas covered various vision applications such as object recognition,\nsegmentation, action recognition, and person re-identification. In this paper,\nfor the first time a comprehensive literature review is provided to summarize\nthe developments in DG for computer vision over the past decade. Specifically,\nwe first cover the background by formally defining DG and relating it to other\nresearch fields like domain adaptation and transfer learning. Second, we\nconduct a thorough review into existing methods and present a categorization\nbased on their methodologies and motivations. Finally, we conclude this survey\nwith insights and discussions on future research directions.",
          "link": "http://arxiv.org/abs/2103.02503",
          "publishedOn": "2021-07-20T02:04:47.181Z",
          "wordCount": null,
          "title": "Domain Generalization in Vision: A Survey. (arXiv:2103.02503v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.05461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mello_R/0/1/0/all/0/1\">Rodrigo Fernandes de Mello</a>",
          "description": "The Statistical Learning Theory (SLT) provides the foundation to ensure that\na supervised algorithm generalizes the mapping $f: \\mathcal{X} \\to \\mathcal{Y}$\ngiven $f$ is selected from its search space bias $\\mathcal{F}$. SLT depends on\nthe Shattering coefficient function $\\mathcal{N}(\\mathcal{F},n)$ to upper bound\nthe empirical risk minimization principle, from which one can estimate the\nnecessary training sample size to ensure the probabilistic learning convergence\nand, most importantly, the characterization of the capacity of $\\mathcal{F}$,\nincluding its underfitting and overfitting abilities while addressing specific\ntarget problems. However, the analytical solution of the Shattering coefficient\nis still an open problem since the first studies by Vapnik and Chervonenkis in\n$1962$, which we address on specific datasets, in this paper, by employing\nequivalence relations from Topology, data separability results by Har-Peled and\nJones, and combinatorics. Our approach computes the Shattering coefficient for\nboth binary and multi-class datasets, leading to the following additional\ncontributions: (i) the estimation of the required number of hyperplanes in the\nworst and best-case classification scenarios and the respective $\\Omega$ and\n$O$ complexities; (ii) the estimation of the training sample sizes required to\nensure supervised learning; and (iii) the comparison of dataset embeddings,\nonce they (re)organize samples into some new space configuration. All results\nintroduced and discussed along this paper are supported by the R package\nshattering (https://cran.r-project.org/web/packages/shattering).",
          "link": "http://arxiv.org/abs/1911.05461",
          "publishedOn": "2021-07-20T02:04:47.162Z",
          "wordCount": null,
          "title": "On the Complexity of Labeled Datasets. (arXiv:1911.05461v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Paniagua_V/0/1/0/all/0/1\">V&#xed;ctor Su&#xe1;rez-Paniagua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteley_W/0/1/0/all/0/1\">William Whiteley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghan Wu</a>",
          "description": "Diagnostic or procedural coding of clinical notes aims to derive a coded\nsummary of disease-related information about patients. Such coding is usually\ndone manually in hospitals but could potentially be automated to improve the\nefficiency and accuracy of medical coding. Recent studies on deep learning for\nautomated medical coding achieved promising performances. However, the\nexplainability of these models is usually poor, preventing them to be used\nconfidently in supporting clinical practice. Another limitation is that these\nmodels mostly assume independence among labels, ignoring the complex\ncorrelation among medical codes which can potentially be exploited to improve\nthe performance. We propose a Hierarchical Label-wise Attention Network (HLAN),\nwhich aimed to interpret the model by quantifying importance (as attention\nweights) of words and sentences related to each of the labels. Secondly, we\npropose to enhance the major deep learning models with a label embedding (LE)\ninitialisation approach, which learns a dense, continuous vector representation\nand then injects the representation into the final layers and the label-wise\nattention layers in the models. We evaluated the methods using three settings\non the MIMIC-III discharge summaries: full codes, top-50 codes, and the UK NHS\nCOVID-19 shielding codes. Experiments were conducted to compare HLAN and LE\ninitialisation to the state-of-the-art neural network based methods. HLAN\nachieved the best Micro-level AUC and $F_1$ on the top-50 code prediction and\ncomparable results on the NHS COVID-19 shielding code prediction to other\nmodels. By highlighting the most salient words and sentences for each label,\nHLAN showed more meaningful and comprehensive model interpretation compared to\nits downgraded baselines and the CNN-based models. LE initialisation\nconsistently boosted most deep learning models for automated medical coding.",
          "link": "http://arxiv.org/abs/2010.15728",
          "publishedOn": "2021-07-20T02:04:47.162Z",
          "wordCount": null,
          "title": "Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation. (arXiv:2010.15728v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.07545",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wadia_N/0/1/0/all/0/1\">Neha S. Wadia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duckworth_D/0/1/0/all/0/1\">Daniel Duckworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoenholz_S/0/1/0/all/0/1\">Samuel S. Schoenholz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_E/0/1/0/all/0/1\">Ethan Dyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1\">Jascha Sohl-Dickstein</a>",
          "description": "Machine learning is predicated on the concept of generalization: a model\nachieving low error on a sufficiently large training set should also perform\nwell on novel samples from the same distribution. We show that both data\nwhitening and second order optimization can harm or entirely prevent\ngeneralization. In general, model training harnesses information contained in\nthe sample-sample second moment matrix of a dataset. For a general class of\nmodels, namely models with a fully connected first layer, we prove that the\ninformation contained in this matrix is the only information which can be used\nto generalize. Models trained using whitened data, or with certain second order\noptimization schemes, have less access to this information, resulting in\nreduced or nonexistent generalization ability. We experimentally verify these\npredictions for several architectures, and further demonstrate that\ngeneralization continues to be harmed even when theoretical requirements are\nrelaxed. However, we also show experimentally that regularized second order\noptimization can provide a practical tradeoff, where training is accelerated\nbut less information is lost, and generalization can in some circumstances even\nimprove.",
          "link": "http://arxiv.org/abs/2008.07545",
          "publishedOn": "2021-07-20T02:04:47.161Z",
          "wordCount": null,
          "title": "Whitening and second order optimization both make information in the dataset unusable during training, and can reduce or prevent generalization. (arXiv:2008.07545v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colas_C/0/1/0/all/0/1\">C&#xe9;dric Colas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karch_T/0/1/0/all/0/1\">Tristan Karch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigaud_O/0/1/0/all/0/1\">Olivier Sigaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>",
          "description": "Building autonomous machines that can explore open-ended environments,\ndiscover possible interactions and autonomously build repertoires of skills is\na general objective of artificial intelligence. Developmental approaches argue\nthat this can only be achieved by autonomous and intrinsically motivated\nlearning agents that can generate, select and learn to solve their own\nproblems. In recent years, we have seen a convergence of developmental\napproaches, and developmental robotics in particular, with deep reinforcement\nlearning (RL) methods, forming the new domain of developmental machine\nlearning. Within this new domain, we review here a set of methods where deep RL\nalgorithms are trained to tackle the developmental robotics problem of the\nautonomous acquisition of open-ended repertoires of skills. Intrinsically\nmotivated goal-conditioned RL algorithms train agents to learn to represent,\ngenerate and pursue their own goals. The self-generation of goals requires the\nlearning of compact goal encodings as well as their associated goal-achievement\nfunctions, which results in new challenges compared to traditional RL\nalgorithms designed to tackle pre-defined sets of goals using external reward\nsignals. This paper proposes a typology of these methods at the intersection of\ndeep RL and developmental approaches, surveys recent approaches and discusses\nfuture avenues.",
          "link": "http://arxiv.org/abs/2012.09830",
          "publishedOn": "2021-07-20T02:04:47.161Z",
          "wordCount": null,
          "title": "Intrinsically Motivated Goal-Conditioned Reinforcement Learning: a Short Survey. (arXiv:2012.09830v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rojas_J/0/1/0/all/0/1\">Junior Rojas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifakis_E/0/1/0/all/0/1\">Eftychios Sifakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavan_L/0/1/0/all/0/1\">Ladislav Kavan</a>",
          "description": "We present a differentiable soft-body physics simulator that can be composed\nwith neural networks as a differentiable layer. In contrast to other\ndifferentiable physics approaches that use explicit forward models to define\nstate transitions, we focus on implicit state transitions defined via function\nminimization. Implicit state transitions appear in implicit numerical\nintegration methods, which offer the benefits of large time steps and excellent\nnumerical stability, but require a special treatment to achieve\ndifferentiability due to the absence of an explicit differentiable forward\npass. In contrast to other implicit differentiation approaches that require\nexplicit formulas for the force function and the force Jacobian matrix, we\npresent an energy-based approach that allows us to compute these derivatives\nautomatically and in a matrix-free fashion via reverse-mode automatic\ndifferentiation. This allows for more flexibility and productivity when\ndefining physical models and is particularly important in the context of neural\nnetwork training, which often relies on reverse-mode automatic differentiation\n(backpropagation). We demonstrate the effectiveness of our differentiable\nsimulator in policy optimization for locomotion tasks and show that it achieves\nbetter sample efficiency than model-free reinforcement learning.",
          "link": "http://arxiv.org/abs/2102.05791",
          "publishedOn": "2021-07-20T02:04:47.160Z",
          "wordCount": null,
          "title": "Differentiable Implicit Soft-Body Physics. (arXiv:2102.05791v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08925",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Martin_C/0/1/0/all/0/1\">Christoph Martin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sharafi_N/0/1/0/all/0/1\">Nahal Sharafi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hallerberg_S/0/1/0/all/0/1\">Sarah Hallerberg</a>",
          "description": "Covariant Lyapunov vectors (CLVs) characterize the directions along which\nperturbations in dynamical systems grow. They have also been studied as\npotential predictors of critical transitions and extreme events. For many\napplications, it is, however, necessary to estimate the vectors from data since\nmodel equations are unknown for many interesting phenomena. We propose a novel\nmethod for estimating CLVs based on data records without knowing the underlying\nequations of the system which is suitable also for high-dimensional data and\ncomputationally inexpensive. We demonstrate that this purely data-driven\napproach can accurately estimate CLVs from data records generated by chaotic\ndynamical systems of dimension 128 and multiple lower-dimensional systems and\nthus provides the foundation for numerous future applications in data-analysis\nand data-based predictions.",
          "link": "http://arxiv.org/abs/2107.08925",
          "publishedOn": "2021-07-20T02:04:47.152Z",
          "wordCount": null,
          "title": "Estimating covariant Lyapunov vectors from data. (arXiv:2107.08925v1 [physics.data-an])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1\">Kyeongbo Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyunghun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Woo-Jin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Suk-Ju Kang</a>",
          "description": "Conditional generative adversarial networks (cGANs) have demonstrated\nremarkable success due to their class-wise controllability and superior quality\nfor complex generation tasks. Typical cGANs solve the joint distribution\nmatching problem by decomposing two easier sub-problems: marginal matching and\nconditional matching. From our toy experiments, we found that it is the best to\napply only conditional matching to certain samples due to the content-aware\noptimization of the discriminator. This paper proposes a simple (a few lines of\ncode) but effective training methodology, selective focusing learning, which\nenforces the discriminator and generator to learn easy samples of each class\nrapidly while maintaining diversity. Our key idea is to selectively apply\nconditional and joint matching for the data in each mini-batch. We conducted\nexperiments on recent cGAN variants in ImageNet (64x64 and 128x128), CIFAR-10,\nand CIFAR-100 datasets, and improved the performance significantly (up to\n35.18% in terms of FID) without sacrificing diversity.",
          "link": "http://arxiv.org/abs/2107.08792",
          "publishedOn": "2021-07-20T02:04:47.039Z",
          "wordCount": null,
          "title": "Selective Focusing Learning in Conditional GANs. (arXiv:2107.08792v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miura_T/0/1/0/all/0/1\">Takayuki Miura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_S/0/1/0/all/0/1\">Satoshi Hasegawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shibahara_T/0/1/0/all/0/1\">Toshiki Shibahara</a>",
          "description": "The advance of explainable artificial intelligence, which provides reasons\nfor its predictions, is expected to accelerate the use of deep neural networks\nin the real world like Machine Learning as a Service (MLaaS) that returns\npredictions on queried data with the trained model. Deep neural networks\ndeployed in MLaaS face the threat of model extraction attacks. A model\nextraction attack is an attack to violate intellectual property and privacy in\nwhich an adversary steals trained models in a cloud using only their\npredictions. In particular, a data-free model extraction attack has been\nproposed recently and is more critical. In this attack, an adversary uses a\ngenerative model instead of preparing input data. The feasibility of this\nattack, however, needs to be studied since it requires more queries than that\nwith surrogate datasets. In this paper, we propose MEGEX, a data-free model\nextraction attack against a gradient-based explainable AI. In this method, an\nadversary uses the explanations to train the generative model and reduces the\nnumber of queries to steal the model. Our experiments show that our proposed\nmethod reconstructs high-accuracy models -- 0.97$\\times$ and 0.98$\\times$ the\nvictim model accuracy on SVHN and CIFAR-10 datasets given 2M and 20M queries,\nrespectively. This implies that there is a trade-off between the\ninterpretability of models and the difficulty of stealing them.",
          "link": "http://arxiv.org/abs/2107.08909",
          "publishedOn": "2021-07-20T02:04:47.019Z",
          "wordCount": null,
          "title": "MEGEX: Data-Free Model Extraction Attack against Gradient-Based Explainable AI. (arXiv:2107.08909v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1\">Petar Veli&#x10d;kovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosnjak_M/0/1/0/all/0/1\">Matko Bo&#x161;njak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kipf_T/0/1/0/all/0/1\">Thomas Kipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerchner_A/0/1/0/all/0/1\">Alexander Lerchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadsell_R/0/1/0/all/0/1\">Raia Hadsell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1\">Charles Blundell</a>",
          "description": "Neural networks leverage robust internal representations in order to\ngeneralise. Learning them is difficult, and often requires a large training set\nthat covers the data distribution densely. We study a common setting where our\ntask is not purely opaque. Indeed, very often we may have access to information\nabout the underlying system (e.g. that observations must obey certain laws of\nphysics) that any \"tabula rasa\" neural network would need to re-learn from\nscratch, penalising data efficiency. We incorporate this information into a\npre-trained reasoning module, and investigate its role in shaping the\ndiscovered representations in diverse self-supervised learning settings from\npixels. Our approach paves the way for a new class of data-efficient\nrepresentation learning.",
          "link": "http://arxiv.org/abs/2107.08881",
          "publishedOn": "2021-07-20T02:04:47.018Z",
          "wordCount": null,
          "title": "Reasoning-Modulated Representations. (arXiv:2107.08881v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1\">Jakub M. Tomczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eiben_A/0/1/0/all/0/1\">Agoston E. Eiben</a>",
          "description": "When controllers (brains) and morphologies (bodies) of robots simultaneously\nevolve, this can lead to a problem, namely the brain & body mismatch problem.\nIn this research, we propose a solution of lifetime learning. We set up a\nsystem where modular robots can create offspring that inherit the bodies of\nparents by recombination and mutation. With regards to the brains of the\noffspring, we use two methods to create them. The first one entails solely\nevolution which means the brain of a robot child is inherited from its parents.\nThe second approach is evolution plus learning which means the brain of a child\nis inherited as well, but additionally is developed by a learning algorithm -\nRevDEknn. We compare these two methods by running experiments in a simulator\ncalled Revolve and use efficiency, efficacy, and the morphology intelligence of\nthe robots for the comparison. The experiments show that the evolution plus\nlearning method does not only lead to a higher fitness level, but also to more\nmorphologically evolving robots. This constitutes a quantitative demonstration\nthat changes in the brain can induce changes in the body, leading to the\nconcept of morphological intelligence, which is quantified by the learning\ndelta, meaning the ability of a morphology to facilitate learning.",
          "link": "http://arxiv.org/abs/2107.08249",
          "publishedOn": "2021-07-20T02:04:46.996Z",
          "wordCount": null,
          "title": "The Effects of Learning in Morphologically Evolving Robot Systems. (arXiv:2107.08249v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guoliang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Ting Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jin Song Dong</a>",
          "description": "Although deep learning has demonstrated astonishing performance in many\napplications, there are still concerns on their dependability. One desirable\nproperty of deep learning applications with societal impact is fairness (i.e.,\nnon-discrimination). Unfortunately, discrimination might be intrinsically\nembedded into the models due to discrimination in the training data. As a\ncountermeasure, fairness testing systemically identifies discriminative\nsamples, which can be used to retrain the model and improve its fairness.\nExisting fairness testing approaches however have two major limitations. First,\nthey only work well on traditional machine learning models and have poor\nperformance (e.g., effectiveness and efficiency) on deep learning models.\nSecond, they only work on simple tabular data and are not applicable for\ndomains such as text. In this work, we bridge the gap by proposing a scalable\nand effective approach for systematically searching for discriminative samples\nwhile extending fairness testing to address a challenging domain, i.e., text\nclassification. Compared with state-of-the-art methods, our approach only\nemploys lightweight procedures like gradient computation and clustering, which\nmakes it significantly more scalable. Experimental results show that on\naverage, our approach explores the search space more effectively (9.62 and 2.38\ntimes more than the state-of-art methods respectively on tabular and text\ndatasets) and generates much more individual discriminatory instances (24.95\nand 2.68 times) within reasonable time. The retrained models reduce\ndiscrimination by 57.2% and 60.2% respectively on average.",
          "link": "http://arxiv.org/abs/2107.08176",
          "publishedOn": "2021-07-20T02:04:46.989Z",
          "wordCount": null,
          "title": "Automatic Fairness Testing of Neural Classifiers through Adversarial Sampling. (arXiv:2107.08176v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schott_L/0/1/0/all/0/1\">Lukas Schott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1\">Frederik Tr&#xe4;uble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1\">Peter Gehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1\">Chris Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>",
          "description": "An important component for generalization in machine learning is to uncover\nunderlying latent factors of variation as well as the mechanism through which\neach factor acts in the world. In this paper, we test whether 17 unsupervised,\nweakly supervised, and fully supervised representation learning approaches\ncorrectly infer the generative factors of variation in simple datasets\n(dSprites, Shapes3D, MPI3D). In contrast to prior robustness work that\nintroduces novel factors of variation during test time, such as blur or other\n(un)structured noise, we here recompose, interpolate, or extrapolate only\nexisting factors of variation from the training data set (e.g., small and\nmedium-sized objects during training and large objects during testing). Models\nthat learn the correct mechanism should be able to generalize to this\nbenchmark. In total, we train and test 2000+ models and observe that all of\nthem struggle to learn the underlying mechanism regardless of supervision\nsignal and architectural bias. Moreover, the generalization capabilities of all\ntested models drop significantly as we move from artificial datasets towards\nmore realistic real-world datasets. Despite their inability to identify the\ncorrect mechanism, the models are quite modular as their ability to infer other\nin-distribution factors remains fairly stable, providing only a single factor\nis out-of-distribution. These results point to an important yet understudied\nproblem of learning mechanistic models of observations that can facilitate\ngeneralization.",
          "link": "http://arxiv.org/abs/2107.08221",
          "publishedOn": "2021-07-20T02:04:46.986Z",
          "wordCount": null,
          "title": "Visual Representation Learning Does Not Generalize Strongly Within the Same Domain. (arXiv:2107.08221v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08225",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Muehlebach_M/0/1/0/all/0/1\">Michael Muehlebach</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "We introduce a class of first-order methods for smooth constrained\noptimization that are based on an analogy to non-smooth dynamical systems. Two\ndistinctive features of our approach are that (i) projections or optimizations\nover the entire feasible set are avoided, in stark contrast to projected\ngradient methods or the Frank-Wolfe method, and (ii) iterates are allowed to\nbecome infeasible, which differs from active set or feasible direction methods,\nwhere the descent motion stops as soon as a new constraint is encountered. The\nresulting algorithmic procedure is simple to implement even when constraints\nare nonlinear, and is suitable for large-scale constrained optimization\nproblems in which the feasible set fails to have a simple structure. The key\nunderlying idea is that constraints are expressed in terms of velocities\ninstead of positions, which has the algorithmic consequence that optimizations\nover feasible sets at each iteration are replaced with optimizations over\nlocal, sparse convex approximations. The result is a simplified suite of\nalgorithms and an expanded range of possible applications in machine learning.",
          "link": "http://arxiv.org/abs/2107.08225",
          "publishedOn": "2021-07-20T02:04:46.985Z",
          "wordCount": null,
          "title": "On Constraints in First-Order Optimization: A View from Non-Smooth Dynamical Systems. (arXiv:2107.08225v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Geun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Carole-Jean Wu</a>",
          "description": "Federated learning enables a cluster of decentralized mobile devices at the\nedge to collaboratively train a shared machine learning model, while keeping\nall the raw training samples on device. This decentralized training approach is\ndemonstrated as a practical solution to mitigate the risk of privacy leakage.\nHowever, enabling efficient FL deployment at the edge is challenging because of\nnon-IID training data distribution, wide system heterogeneity and\nstochastic-varying runtime effects in the field. This paper jointly optimizes\ntime-to-convergence and energy efficiency of state-of-the-art FL use cases by\ntaking into account the stochastic nature of edge execution. We propose AutoFL\nby tailor-designing a reinforcement learning algorithm that learns and\ndetermines which K participant devices and per-device execution targets for\neach FL model aggregation round in the presence of stochastic runtime variance,\nsystem and data heterogeneity. By considering the unique characteristics of FL\nedge deployment judiciously, AutoFL achieves 3.6 times faster model convergence\ntime and 4.7 and 5.2 times higher energy efficiency for local clients and\nglobally over the cluster of K participants, respectively.",
          "link": "http://arxiv.org/abs/2107.08147",
          "publishedOn": "2021-07-20T02:04:46.984Z",
          "wordCount": null,
          "title": "AutoFL: Enabling Heterogeneity-Aware Energy Efficient Federated Learning. (arXiv:2107.08147v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Avati_A/0/1/0/all/0/1\">Anand Avati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seneviratne_M/0/1/0/all/0/1\">Martin Seneviratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_E/0/1/0/all/0/1\">Emily Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>",
          "description": "Machine learning has recently demonstrated impressive progress in predictive\naccuracy across a wide array of tasks. Most ML approaches focus on\ngeneralization performance on unseen data that are similar to the training data\n(In-Distribution, or IND). However, real world applications and deployments of\nML rarely enjoy the comfort of encountering examples that are always IND. In\nsuch situations, most ML models commonly display erratic behavior on\nOut-of-Distribution (OOD) examples, such as assigning high confidence to wrong\npredictions, or vice-versa. Implications of such unusual model behavior are\nfurther exacerbated in the healthcare setting, where patient health can\npotentially be put at risk. It is crucial to study the behavior and robustness\nproperties of models under distributional shift, understand common failure\nmodes, and take mitigation steps before the model is deployed. Having a\nbenchmark that shines light upon these aspects of a model is a first and\nnecessary step in addressing the issue. Recent work and interest in increasing\nmodel robustness in OOD settings have focused more on image modality, while the\nElectronic Health Record (EHR) modality is still largely under-explored. We aim\nto bridge this gap by releasing BEDS-Bench, a benchmark for quantifying the\nbehavior of ML models over EHR data under OOD settings. We use two open access,\nde-identified EHR datasets to construct several OOD data settings to run tests\non, and measure relevant metrics that characterize crucial aspects of a model's\nOOD behavior. We evaluate several learning algorithms under BEDS-Bench and find\nthat all of them show poor generalization performance under distributional\nshift in general. Our results highlight the need and the potential to improve\nrobustness of EHR models under distributional shift, and BEDS-Bench provides\none way to measure progress towards that goal.",
          "link": "http://arxiv.org/abs/2107.08189",
          "publishedOn": "2021-07-20T02:04:46.984Z",
          "wordCount": null,
          "title": "BEDS-Bench: Behavior of EHR-models under Distributional Shift--A Benchmark. (arXiv:2107.08189v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korb_K/0/1/0/all/0/1\">Kevin B Korb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allison_L/0/1/0/all/0/1\">Lloyd Allison</a>",
          "description": "Causal discovery automates the learning of causal Bayesian networks from data\nand has been of active interest from their beginning. With the sourcing of\nlarge data sets off the internet, interest in scaling up to very large data\nsets has grown. One approach to this is to parallelize search using Markov\nBlanket (MB) discovery as a first step, followed by a process of combining MBs\nin a global causal model. We develop and explore three new methods of MB\ndiscovery using Minimum Message Length (MML) and compare them empirically to\nthe best existing methods, whether developed specifically as MB discovery or as\nfeature selection. Our best MML method is consistently competitive and has some\nadvantageous features.",
          "link": "http://arxiv.org/abs/2107.08140",
          "publishedOn": "2021-07-20T02:04:46.983Z",
          "wordCount": null,
          "title": "Markov Blanket Discovery using Minimum Message Length. (arXiv:2107.08140v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petrenko_A/0/1/0/all/0/1\">Aleksei Petrenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijmans_E/0/1/0/all/0/1\">Erik Wijmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shacklett_B/0/1/0/all/0/1\">Brennan Shacklett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>",
          "description": "We present Megaverse, a new 3D simulation platform for reinforcement learning\nand embodied AI research. The efficient design of our engine enables\nphysics-based simulation with high-dimensional egocentric observations at more\nthan 1,000,000 actions per second on a single 8-GPU node. Megaverse is up to\n70x faster than DeepMind Lab in fully-shaded 3D scenes with interactive\nobjects. We achieve this high simulation performance by leveraging batched\nsimulation, thereby taking full advantage of the massive parallelism of modern\nGPUs. We use Megaverse to build a new benchmark that consists of several\nsingle-agent and multi-agent tasks covering a variety of cognitive challenges.\nWe evaluate model-free RL on this benchmark to provide baselines and facilitate\nfuture research. The source code is available at https://www.megaverse.info",
          "link": "http://arxiv.org/abs/2107.08170",
          "publishedOn": "2021-07-20T02:04:46.983Z",
          "wordCount": null,
          "title": "Megaverse: Simulating Embodied Agents at One Million Experiences per Second. (arXiv:2107.08170v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Molino_P/0/1/0/all/0/1\">Piero Molino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>",
          "description": "In the last years machine learning (ML) has moved from a academic endeavor to\na pervasive technology adopted in almost every aspect of computing. ML-powered\nproducts are now embedded in our digital lives: from recommendations of what to\nwatch, to divining our search intent, to powering virtual assistants in\nconsumer and enterprise settings. Recent successes in applying ML in natural\nsciences revealed that ML can be used to tackle some of the hardest real-world\nproblems humanity faces today. For these reasons ML has become central in the\nstrategy of tech companies and has gathered even more attention from academia\nthan ever before. Despite these successes, what we have witnessed so far is\njust the beginning. Right now the people training and using ML models are\nexpert developers working within large organizations, but we believe the next\nwave of ML systems will allow a larger amount of people, potentially without\ncoding skills, to perform the same tasks. These new ML systems will not require\nusers to fully understand all the details of how models are trained and\nutilized for obtaining predictions. Declarative interfaces are well suited for\nthis goal, by hiding complexity and favouring separation of interests, and can\nlead to increased productivity. We worked on such abstract interfaces by\ndeveloping two declarative ML systems, Overton and Ludwig, that require users\nto declare only their data schema (names and types of inputs) and tasks rather\nthen writing low level ML code. In this article we will describe how ML systems\nare currently structured, highlight important factors for their success and\nadoption, what are the issues current ML systems are facing and how the systems\nwe developed addressed them. Finally we will talk about learnings from the\ndevelopment of ML systems throughout the years and how we believe the next\ngeneration of ML systems will look like.",
          "link": "http://arxiv.org/abs/2107.08148",
          "publishedOn": "2021-07-20T02:04:46.981Z",
          "wordCount": null,
          "title": "Declarative Machine Learning Systems. (arXiv:2107.08148v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Girgis_A/0/1/0/all/0/1\">Antonious M. Girgis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Data_D/0/1/0/all/0/1\">Deepesh Data</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diggavi_S/0/1/0/all/0/1\">Suhas Diggavi</a>",
          "description": "We study privacy in a distributed learning framework, where clients\ncollaboratively build a learning model iteratively through interactions with a\nserver from whom we need privacy. Motivated by stochastic optimization and the\nfederated learning (FL) paradigm, we focus on the case where a small fraction\nof data samples are randomly sub-sampled in each round to participate in the\nlearning process, which also enables privacy amplification. To obtain even\nstronger local privacy guarantees, we study this in the shuffle privacy model,\nwhere each client randomizes its response using a local differentially private\n(LDP) mechanism and the server only receives a random permutation (shuffle) of\nthe clients' responses without their association to each client. The principal\nresult of this paper is a privacy-optimization performance trade-off for\ndiscrete randomization mechanisms in this sub-sampled shuffle privacy model.\nThis is enabled through a new theoretical technique to analyze the Renyi\nDifferential Privacy (RDP) of the sub-sampled shuffle model. We numerically\ndemonstrate that, for important regimes, with composition our bound yields\nsignificant improvement in privacy guarantee over the state-of-the-art\napproximate Differential Privacy (DP) guarantee (with strong composition) for\nsub-sampled shuffled models. We also demonstrate numerically significant\nimprovement in privacy-learning performance operating point using real data\nsets.",
          "link": "http://arxiv.org/abs/2107.08763",
          "publishedOn": "2021-07-20T02:04:46.980Z",
          "wordCount": null,
          "title": "Renyi Differential Privacy of the Subsampled Shuffle Model in Distributed Learning. (arXiv:2107.08763v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zecevic_M/0/1/0/all/0/1\">Matej Ze&#x10d;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhami_D/0/1/0/all/0/1\">Devendra Singh Dhami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_A/0/1/0/all/0/1\">Athresh Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_S/0/1/0/all/0/1\">Sriraam Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>",
          "description": "While probabilistic models are an important tool for studying causality,\ndoing so suffers from the intractability of inference. As a step towards\ntractable causal models, we consider the problem of learning interventional\ndistributions using sum-product networks (SPNs) that are over-parameterized by\ngate functions, e.g., neural networks. Providing an arbitrarily intervened\ncausal graph as input, effectively subsuming Pearl's do-operator, the gate\nfunction predicts the parameters of the SPN. The resulting interventional SPNs\nare motivated and illustrated by a structural causal model themed around\npersonal health. Our empirical evaluation on three benchmark data sets as well\nas a synthetic health data set clearly demonstrates that interventional SPNs\nindeed are both expressive in modelling and flexible in adapting to the\ninterventions.",
          "link": "http://arxiv.org/abs/2102.10440",
          "publishedOn": "2021-07-20T02:04:46.980Z",
          "wordCount": null,
          "title": "Interventional Sum-Product Networks: Causal Inference with Tractable Probabilistic Models. (arXiv:2102.10440v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_I/0/1/0/all/0/1\">Iker Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skalski_P/0/1/0/all/0/1\">Piotr Skalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barns_Graham_A/0/1/0/all/0/1\">Alec Barns-Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Jason Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_D/0/1/0/all/0/1\">David Sutton</a>",
          "description": "Enabling interpretations of model uncertainties is of key importance in\nBayesian machine learning applications. Often, this requires to meaningfully\nattribute predictive uncertainties to source features in an image, text or\ncategorical array. However, popular attribution methods are particularly\ndesigned for classification and regression scores. In order to explain\nuncertainties, state of the art alternatives commonly procure counterfactual\nfeature vectors, and proceed by making direct comparisons. In this paper, we\nleverage path integrals to attribute uncertainties in Bayesian differentiable\nmodels. We present a novel algorithm that relies on in-distribution curves\nconnecting a feature vector to some counterfactual counterpart, and we retain\ndesirable properties of interpretability methods. We validate our approach on\nbenchmark image data sets with varying resolution, and show that it\nsignificantly simplifies interpretability over the existing alternatives.",
          "link": "http://arxiv.org/abs/2107.08756",
          "publishedOn": "2021-07-20T02:04:46.979Z",
          "wordCount": null,
          "title": "Path Integrals for the Attribution of Model Uncertainties. (arXiv:2107.08756v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.05601",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tsai_K/0/1/0/all/0/1\">Katherine Tsai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kolar_M/0/1/0/all/0/1\">Mladen Kolar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Koyejo_O/0/1/0/all/0/1\">Oluwasanmi Koyejo</a>",
          "description": "We propose a flexible yet interpretable model for high-dimensional data with\ntime-varying second order statistics, motivated and applied to functional\nneuroimaging data. Motivated by the neuroscience literature, we factorize the\ncovariances into sparse spatial and smooth temporal components. While this\nfactorization results in both parsimony and domain interpretability, the\nresulting estimation problem is nonconvex. To this end, we design a two-stage\noptimization scheme with a carefully tailored spectral initialization, combined\nwith iteratively refined alternating projected gradient descent. We prove a\nlinear convergence rate up to a nontrivial statistical error for the proposed\ndescent scheme and establish sample complexity guarantees for the estimator. We\nfurther quantify the statistical error for the multivariate Gaussian case.\nEmpirical results using simulated and real brain imaging data illustrate that\nour approach outperforms existing baselines.",
          "link": "http://arxiv.org/abs/2011.05601",
          "publishedOn": "2021-07-20T02:04:46.979Z",
          "wordCount": null,
          "title": "A Nonconvex Framework for Structured Dynamic Covariance Recovery. (arXiv:2011.05601v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhizhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Kun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenfeng Zhu</a>",
          "description": "Treatment effect estimation, which refers to the estimation of causal effects\nand aims to measure the strength of the causal relationship, is of great\nimportance in many fields but is a challenging problem in practice. As present,\ndata-driven causal effect estimation faces two main challenges, i.e., selection\nbias and the missing of counterfactual. To address these two issues, most of\nthe existing approaches tend to reduce the selection bias by learning a\nbalanced representation, and then to estimate the counterfactual through the\nrepresentation. However, they heavily rely on the finely hand-crafted metric\nfunctions when learning balanced representations, which generally doesn't work\nwell for the situations where the original distribution is complicated. In this\npaper, we propose a CETransformer model for casual effect estimation via\ntransformer based representation learning. To learn the representation of\ncovariates(features) robustly, a self-supervised transformer is proposed, by\nwhich the correlation between covariates can be well exploited through\nself-attention mechanism. In addition, an adversarial network is adopted to\nbalance the distribution of the treated and control groups in the\nrepresentation space. Experimental results on three real-world datasets\ndemonstrate the advantages of the proposed CETransformer, compared with the\nstate-of-the-art treatment effect estimation methods.",
          "link": "http://arxiv.org/abs/2107.08714",
          "publishedOn": "2021-07-20T02:04:46.978Z",
          "wordCount": null,
          "title": "CETransformer: Casual Effect Estimation via Transformer Based Representation Learning. (arXiv:2107.08714v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08765",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xueting Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhenhuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1\">Bang An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jing Bai</a>",
          "description": "Graph neural networks (GNNs) is widely used to learn a powerful\nrepresentation of graph-structured data. Recent work demonstrates that\ntransferring knowledge from self-supervised tasks to downstream tasks could\nfurther improve graph representation. However, there is an inherent gap between\nself-supervised tasks and downstream tasks in terms of optimization objective\nand training data. Conventional pre-training methods may be not effective\nenough on knowledge transfer since they do not make any adaptation for\ndownstream tasks. To solve such problems, we propose a new transfer learning\nparadigm on GNNs which could effectively leverage self-supervised tasks as\nauxiliary tasks to help the target task. Our methods would adaptively select\nand combine different auxiliary tasks with the target task in the fine-tuning\nstage. We design an adaptive auxiliary loss weighting model to learn the\nweights of auxiliary tasks by quantifying the consistency between auxiliary\ntasks and the target task. In addition, we learn the weighting model through\nmeta-learning. Our methods can be applied to various transfer learning\napproaches, it performs well not only in multi-task learning but also in\npre-training and fine-tuning. Comprehensive experiments on multiple downstream\ntasks demonstrate that the proposed methods can effectively combine auxiliary\ntasks with the target task and significantly improve the performance compared\nto state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.08765",
          "publishedOn": "2021-07-20T02:04:46.977Z",
          "wordCount": null,
          "title": "Adaptive Transfer Learning on Graph Neural Networks. (arXiv:2107.08765v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08721",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Chen_Q/0/1/0/all/0/1\">Qinkai Chen</a>",
          "description": "News events can greatly influence equity markets. In this paper, we are\ninterested in predicting the short-term movement of stock prices after\nfinancial news events using only the headlines of the news. To achieve this\ngoal, we introduce a new text mining method called Fine-Tuned\nContextualized-Embedding Recurrent Neural Network (FT-CE-RNN). Compared with\nprevious approaches which use static vector representations of the news (static\nembedding), our model uses contextualized vector representations of the\nheadlines (contextualized embeddings) generated from Bidirectional Encoder\nRepresentations from Transformers (BERT). Our model obtains the\nstate-of-the-art result on this stock movement prediction task. It shows\nsignificant improvement compared with other baseline models, in both accuracy\nand trading simulations. Through various trading simulations based on millions\nof headlines from Bloomberg News, we demonstrate the ability of this model in\nreal scenarios.",
          "link": "http://arxiv.org/abs/2107.08721",
          "publishedOn": "2021-07-20T02:04:46.976Z",
          "wordCount": null,
          "title": "Stock Movement Prediction with Financial News using Contextualized Embedding from BERT. (arXiv:2107.08721v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuangjia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Ying Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jiahua Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuedong Yang</a>",
          "description": "Constructing appropriate representations of molecules lies at the core of\nnumerous tasks such as material science, chemistry and drug designs. Recent\nresearches abstract molecules as attributed graphs and employ graph neural\nnetworks (GNN) for molecular representation learning, which have made\nremarkable achievements in molecular graph modeling. Albeit powerful, current\nmodels either are based on local aggregation operations and thus miss\nhigher-order graph properties or focus on only node information without fully\nusing the edge information. For this sake, we propose a Communicative Message\nPassing Transformer (CoMPT) neural network to improve the molecular graph\nrepresentation by reinforcing message interactions between nodes and edges\nbased on the Transformer architecture. Unlike the previous transformer-style\nGNNs that treat molecules as fully connected graphs, we introduce a message\ndiffusion mechanism to leverage the graph connectivity inductive bias and\nreduce the message enrichment explosion. Extensive experiments demonstrated\nthat the proposed model obtained superior performances (around 4$\\%$ on\naverage) against state-of-the-art baselines on seven chemical property datasets\n(graph-level tasks) and two chemical shift datasets (node-level tasks). Further\nvisualization studies also indicated a better representation capacity achieved\nby our model.",
          "link": "http://arxiv.org/abs/2107.08773",
          "publishedOn": "2021-07-20T02:04:46.975Z",
          "wordCount": null,
          "title": "Learning Attributed Graph Representations with Communicative Message Passing Transformer. (arXiv:2107.08773v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shehabi_S/0/1/0/all/0/1\">Shadi Al Shehabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baba_A/0/1/0/all/0/1\">Abdullatif Baba</a>",
          "description": "Association rules are useful to discover relationships, which are mostly\nhidden, between the different items in large datasets. Symbolic models are the\nprincipal tools to extract association rules. This basic technique is\ntime-consuming, and it generates a big number of associated rules. To overcome\nthis drawback, we suggest a new method, called MARC, to extract the more\nimportant association rules of two important levels: Type I, and Type II. This\napproach relies on a multi-topographic unsupervised neural network model as\nwell as clustering quality measures that evaluate the success of a given\nnumerical classification model to behave as a natural symbolic model.",
          "link": "http://arxiv.org/abs/2107.08814",
          "publishedOn": "2021-07-20T02:04:46.975Z",
          "wordCount": null,
          "title": "MARC: Mining Association Rules from datasets by using Clustering models. (arXiv:2107.08814v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jo_H/0/1/0/all/0/1\">Ha Young Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Recently, semiconductors' demand has exploded in virtual reality,\nsmartphones, wearable devices, the internet of things, robotics, and\nautomobiles. Semiconductor manufacturers want to make semiconductors with high\nyields. To do this, manufacturers conduct many quality assurance activities.\nWafer map pattern classification is a typical way of quality assurance. The\ndefect pattern on the wafer map can tell us which process has a problem. Most\nof the existing wafer map classification methods are based on supervised\nmethods. The supervised methods tend to have high performance, but they require\nextensive labor and expert knowledge to produce labeled datasets with a\nbalanced distribution in mind. In the semiconductor manufacturing process, it\nis challenging to get defect data with balanced distribution. In this paper, we\npropose a one-class classification method using an Adversarial Autoencoder\n(AAE) with Deep Support Vector Data Description (DSVDD) prior, which generates\nrandom vectors within the hypersphere of DSVDD. We use the WM-811k dataset,\nwhich consists of a real-world wafer map. We compare the F1 score performance\nof our model with DSVDD and AAE.",
          "link": "http://arxiv.org/abs/2107.08823",
          "publishedOn": "2021-07-20T02:04:46.974Z",
          "wordCount": null,
          "title": "One-Class Classification for Wafer Map using Adversarial Autoencoder with DSVDD Prior. (arXiv:2107.08823v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rafailov_R/0/1/0/all/0/1\">Rafael Rafailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajeswaran_A/0/1/0/all/0/1\">Aravind Rajeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>",
          "description": "Reward function specification, which requires considerable human effort and\niteration, remains a major impediment for learning behaviors through deep\nreinforcement learning. In contrast, providing visual demonstrations of desired\nbehaviors often presents an easier and more natural way to teach agents. We\nconsider a setting where an agent is provided a fixed dataset of visual\ndemonstrations illustrating how to perform a task, and must learn to solve the\ntask using the provided demonstrations and unsupervised environment\ninteractions. This setting presents a number of challenges including\nrepresentation learning for visual observations, sample complexity due to high\ndimensional spaces, and learning instability due to the lack of a fixed reward\nor learning signal. Towards addressing these challenges, we develop a\nvariational model-based adversarial imitation learning (V-MAIL) algorithm. The\nmodel-based approach provides a strong signal for representation learning,\nenables sample efficiency, and improves the stability of adversarial training\nby enabling on-policy learning. Through experiments involving several\nvision-based locomotion and manipulation tasks, we find that V-MAIL learns\nsuccessful visuomotor policies in a sample-efficient manner, has better\nstability compared to prior work, and also achieves higher asymptotic\nperformance. We further find that by transferring the learned models, V-MAIL\ncan learn new tasks from visual demonstrations without any additional\nenvironment interactions. All results including videos can be found online at\n\\url{https://sites.google.com/view/variational-mail}.",
          "link": "http://arxiv.org/abs/2107.08829",
          "publishedOn": "2021-07-20T02:04:46.973Z",
          "wordCount": null,
          "title": "Visual Adversarial Imitation Learning using Variational Models. (arXiv:2107.08829v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaolong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanegas_F/0/1/0/all/0/1\">Fernando Vanegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1\">Felipe Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanderson_C/0/1/0/all/0/1\">Conrad Sanderson</a>",
          "description": "The use of multi-rotor Unmanned Aerial Vehicles (UAVs) for search and rescue\nas well as remote sensing is rapidly increasing. Multi-rotor UAVs, however,\nhave limited endurance. The range of UAV applications can be widened if teams\nof multiple UAVs are used. We propose a framework for a team of UAVs to\ncooperatively explore and find a target in complex GPS-denied environments with\nobstacles. The team of UAVs autonomously navigates, explores, detects, and\nfinds the target in a cluttered environment with a known map. Examples of such\nenvironments include indoor scenarios, urban or natural canyons, caves, and\ntunnels, where the GPS signal is limited or blocked. The framework is based on\na probabilistic decentralised Partially Observable Markov Decision Process\nwhich accounts for the uncertainties in sensing and the environment. The team\ncan cooperate efficiently, with each UAV sharing only limited processed\nobservations and their locations during the mission. The system is simulated\nusing the Robotic Operating System and Gazebo. Performance of the system with\nan increasing number of UAVs in several indoor scenarios with obstacles is\ntested. Results indicate that the proposed multi-UAV system has improvements in\nterms of time-cost, the proportion of search area surveyed, as well as\nsuccessful rates for search and rescue missions.",
          "link": "http://arxiv.org/abs/2107.08834",
          "publishedOn": "2021-07-20T02:04:46.973Z",
          "wordCount": null,
          "title": "A Multi-UAV System for Exploration and Target Finding in Cluttered and GPS-Denied Environments. (arXiv:2107.08834v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Itoh_T/0/1/0/all/0/1\">Takeshi D. Itoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubo_T/0/1/0/all/0/1\">Takatomi Kubo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikeda_K/0/1/0/all/0/1\">Kazushi Ikeda</a>",
          "description": "Graph neural networks (GNNs) have been widely used to learn vector\nrepresentation of graph-structured data and achieved better task performance\nthan conventional methods. The foundation of GNNs is the message passing\nprocedure, which propagates the information in a node to its neighbors. Since\nthis procedure proceeds one step per layer, the range of the information\npropagation among nodes is small in the lower layers, and it expands toward the\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\nstructural information in a graph. On the other hand, it is known that deep GNN\nmodels suffer from performance degradation because they lose nodes' local\ninformation, which would be essential for good model performance, through many\nmessage passing steps. In this study, we propose a multi-level attention\npooling (MLAP) for graph-level classification tasks, which can adapt to both\nlocal and global structural information in a graph. It has an attention pooling\nlayer for each message passing step and computes the final graph representation\nby unifying the layer-wise graph representations. The MLAP architecture allows\nmodels to utilize the structural information of graphs with multiple levels of\nlocalities because it preserves layer-wise information before losing them due\nto oversmoothing. Results of our experiments show that the MLAP architecture\nimproves deeper models' performance in graph classification tasks compared to\nthe baseline architectures. In addition, analyses on the layer-wise graph\nrepresentations suggest that aggregating information from multiple levels of\nlocalities indeed has the potential to improve the discriminability of learned\ngraph representations.",
          "link": "http://arxiv.org/abs/2103.01488",
          "publishedOn": "2021-07-20T02:04:46.972Z",
          "wordCount": null,
          "title": "Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities. (arXiv:2103.01488v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xiaoyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shuai L&#xfc;</a>",
          "description": "Generative Adversarial Networks (GAN) is an adversarial model, and it has\nbeen demonstrated to be effective for various generative tasks. However, GAN\nand its variants also suffer from many training problems, such as mode collapse\nand gradient vanish. In this paper, we firstly propose a general crossover\noperator, which can be widely applied to GANs using evolutionary strategies.\nThen we design an evolutionary GAN framework C-GAN based on it. And we combine\nthe crossover operator with evolutionary generative adversarial networks (EGAN)\nto implement the evolutionary generative adversarial networks with crossover\n(CE-GAN). Under the premise that a variety of loss functions are used as\nmutation operators to generate mutation individuals, we evaluate the generated\nsamples and allow the mutation individuals to learn experiences from the output\nin a knowledge distillation manner, imitating the best output outcome,\nresulting in better offspring. Then, we greedily selected the best offspring as\nparents for subsequent training using discriminator as evaluator. Experiments\non real datasets demonstrate the effectiveness of CE-GAN and show that our\nmethod is competitive in terms of generated images quality and time efficiency.",
          "link": "http://arxiv.org/abs/2101.11186",
          "publishedOn": "2021-07-20T02:04:46.971Z",
          "wordCount": null,
          "title": "Evolutionary Generative Adversarial Networks with Crossover Based Knowledge Distillation. (arXiv:2101.11186v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1\">Aske Plaat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosters_W/0/1/0/all/0/1\">Walter Kosters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preuss_M/0/1/0/all/0/1\">Mike Preuss</a>",
          "description": "Deep reinforcement learning has shown remarkable success in the past few\nyears. Highly complex sequential decision making problems from game playing and\nrobotics have been solved with deep model-free methods. Unfortunately, the\nsample complexity of model-free methods is often high. To reduce the number of\nenvironment samples, model-based reinforcement learning creates an explicit\nmodel of the environment dynamics. Achieving high model accuracy is a challenge\nin high-dimensional problems. In recent years, a diverse landscape of\nmodel-based methods has been introduced to improve model accuracy, using\nmethods such as uncertainty modeling, model-predictive control, latent models,\nand end-to-end learning and planning. Some of these methods succeed in\nachieving high accuracy at low sample complexity, most do so either in a\nrobotics or in a games context. In this paper, we survey these methods; we\nexplain in detail how they work and what their strengths and weaknesses are. We\nconclude with a research agenda for future work to make the methods more robust\nand more widely applicable to other applications.",
          "link": "http://arxiv.org/abs/2107.08241",
          "publishedOn": "2021-07-20T02:04:46.880Z",
          "wordCount": null,
          "title": "High-Accuracy Model-Based Reinforcement Learning, a Survey. (arXiv:2107.08241v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08265",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jain_A/0/1/0/all/0/1\">Ayush Jain</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Srijith_P/0/1/0/all/0/1\">P. K. Srijith</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Emtiyaz Khan</a> (2) ((1) Department of Computer Science and Engineering, Indian Institute of Technology Hyderabad, India, (2) RIKEN Center for AI Project, Tokyo, Japan)",
          "description": "Deep Gaussian Processes (DGPs) are multi-layer, flexible extensions of\nGaussian processes but their training remains challenging. Sparse\napproximations simplify the training but often require optimization over a\nlarge number of inducing inputs and their locations across layers. In this\npaper, we simplify the training by setting the locations to a fixed subset of\ndata and sampling the inducing inputs from a variational distribution. This\nreduces the trainable parameters and computation cost without significant\nperformance degradations, as demonstrated by our empirical results on\nregression problems. Our modifications simplify and stabilize DGP training\nwhile making it amenable to sampling schemes for setting the inducing inputs.",
          "link": "http://arxiv.org/abs/2107.08265",
          "publishedOn": "2021-07-20T02:04:46.879Z",
          "wordCount": null,
          "title": "Subset-of-Data Variational Inference for Deep Gaussian-Processes Regression. (arXiv:2107.08265v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2008.12623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milli_S/0/1/0/all/0/1\">Smitha Milli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belli_L/0/1/0/all/0/1\">Luca Belli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1\">Moritz Hardt</a>",
          "description": "Most recommendation engines today are based on predicting user engagement,\ne.g. predicting whether a user will click on an item or not. However, there is\npotentially a large gap between engagement signals and a desired notion of\n\"value\" that is worth optimizing for. We use the framework of measurement\ntheory to (a) confront the designer with a normative question about what the\ndesigner values, (b) provide a general latent variable model approach that can\nbe used to operationalize the target construct and directly optimize for it,\nand (c) guide the designer in evaluating and revising their operationalization.\nWe implement our approach on the Twitter platform on millions of users. In line\nwith established approaches to assessing the validity of measurements, we\nperform a qualitative evaluation of how well our model captures a desired\nnotion of \"value\".",
          "link": "http://arxiv.org/abs/2008.12623",
          "publishedOn": "2021-07-20T02:04:46.878Z",
          "wordCount": null,
          "title": "From Optimizing Engagement to Measuring Value. (arXiv:2008.12623v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parry_H/0/1/0/all/0/1\">Hishan Parry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xun_L/0/1/0/all/0/1\">Lei Xun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_A/0/1/0/all/0/1\">Amin Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jia Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrett_G/0/1/0/all/0/1\">Geoff V. Merrett</a>",
          "description": "The Transformer architecture is widely used for machine translation tasks.\nHowever, its resource-intensive nature makes it challenging to implement on\nconstrained embedded devices, particularly where available hardware resources\ncan vary at run-time. We propose a dynamic machine translation model that\nscales the Transformer architecture based on the available resources at any\nparticular time. The proposed approach, 'Dynamic-HAT', uses a HAT\nSuperTransformer as the backbone to search for SubTransformers with different\naccuracy-latency trade-offs at design time. The optimal SubTransformers are\nsampled from the SuperTransformer at run-time, depending on latency\nconstraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses\ninherited SubTransformers sampled directly from the SuperTransformer with a\nswitching time of <1s. Using inherited SubTransformers results in a BLEU score\nloss of <1.5% because the SubTransformer configuration is not retrained from\nscratch after sampling. However, to recover this loss in performance, the\ndimensions of the design space can be reduced to tailor it to a family of\ntarget hardware. The new reduced design space results in a BLEU score increase\nof approximately 1% for sub-optimal models from the original design space, with\na wide range for performance scaling between 0.356s - 1.526s for the GPU and\n2.9s - 7.31s for the CPU.",
          "link": "http://arxiv.org/abs/2107.08199",
          "publishedOn": "2021-07-20T02:04:46.868Z",
          "wordCount": null,
          "title": "Dynamic Transformer for Efficient Machine Translation on Embedded Devices. (arXiv:2107.08199v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.10483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Linjun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zheyan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wei Cui</a>",
          "description": "Nowadays fairness issues have raised great concerns in decision-making\nsystems. Various fairness notions have been proposed to measure the degree to\nwhich an algorithm is unfair. In practice, there frequently exist a certain set\nof variables we term as fair variables, which are pre-decision covariates such\nas users' choices. The effects of fair variables are irrelevant in assessing\nthe fairness of the decision support algorithm. We thus define conditional\nfairness as a more sound fairness metric by conditioning on the fairness\nvariables. Given different prior knowledge of fair variables, we demonstrate\nthat traditional fairness notations, such as demographic parity and equalized\nodds, are special cases of our conditional fairness notations. Moreover, we\npropose a Derivable Conditional Fairness Regularizer (DCFR), which can be\nintegrated into any decision-making model, to track the trade-off between\nprecision and fairness of algorithmic decision making. Specifically, an\nadversarial representation based conditional independence loss is proposed in\nour DCFR to measure the degree of unfairness. With extensive experiments on\nthree real-world datasets, we demonstrate the advantages of our conditional\nfairness notation and DCFR.",
          "link": "http://arxiv.org/abs/2006.10483",
          "publishedOn": "2021-07-20T02:04:46.867Z",
          "wordCount": null,
          "title": "Algorithmic Decision Making with Conditional Fairness. (arXiv:2006.10483v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1705.07164",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guo_X/0/1/0/all/0/1\">Xin Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hong_J/0/1/0/all/0/1\">Johnny Hong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lin_T/0/1/0/all/0/1\">Tianyi Lin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>",
          "description": "Wasserstein Generative Adversarial Networks (WGANs) provide a versatile class\nof models, which have attracted great attention in various applications.\nHowever, this framework has two main drawbacks: (i) Wasserstein-1 (or\nEarth-Mover) distance is restrictive such that WGANs cannot always fit data\ngeometry well; (ii) It is difficult to achieve fast training of WGANs. In this\npaper, we propose a new class of \\textit{Relaxed Wasserstein} (RW) distances by\ngeneralizing Wasserstein-1 distance with Bregman cost functions. We show that\nRW distances achieve nice statistical properties while not sacrificing the\ncomputational tractability. Combined with the GANs framework, we develop\nRelaxed WGANs (RWGANs) which are not only statistically flexible but can be\napproximated efficiently using heuristic approaches. Experiments on real images\ndemonstrate that the RWGAN with Kullback-Leibler (KL) cost function outperforms\nother competing approaches, e.g., WGANs, even with gradient penalty.",
          "link": "http://arxiv.org/abs/1705.07164",
          "publishedOn": "2021-07-20T02:04:46.866Z",
          "wordCount": null,
          "title": "Relaxed Wasserstein with Applications to GANs. (arXiv:1705.07164v8 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sayak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>",
          "description": "Floods wreak havoc throughout the world, causing billions of dollars in\ndamages, and uprooting communities, ecosystems and economies. Accurate and\nrobust flood detection including delineating open water flood areas and\nidentifying flood levels can aid in disaster response and mitigation. However,\nestimating flood levels remotely is of essence as physical access to flooded\nareas is limited and the ability to deploy instruments in potential flood zones\ncan be dangerous. Aligning flood extent mapping with local topography can\nprovide a plan-of-action that the disaster response team can consider. Thus,\nremote flood level estimation via satellites like Sentinel-1 can prove to be\nremedial. The Emerging Techniques in Computational Intelligence (ETCI)\ncompetition on Flood Detection tasked participants with predicting flooded\npixels after training with synthetic aperture radar (SAR) images in a\nsupervised setting. We use a cyclical approach involving two stages (1)\ntraining an ensemble model of multiple UNet architectures with available high\nand low confidence labeled data and, (2) generating pseudo labels or low\nconfidence labels on the unlabeled test dataset, and then, combining the\ngenerated labels with the previously available high confidence labeled dataset.\nThis assimilated dataset is used for the next round of training ensemble\nmodels. This cyclical process is repeated until the performance improvement\nplateaus. Additionally, we post process our results with Conditional Random\nFields. Our approach sets a high score on the public leaderboard for the ETCI\ncompetition with 0.7654 IoU. Our method, which we release with all the code\nincluding trained models, can also be used as an open science benchmark for the\nSentinel-1 released dataset on GitHub.",
          "link": "http://arxiv.org/abs/2107.08369",
          "publishedOn": "2021-07-20T02:04:46.865Z",
          "wordCount": null,
          "title": "Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1910.09739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Chuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meng Chang Chen</a>",
          "description": "This work investigates the framework and performance issues of the composite\nneural network, which is composed of a collection of pre-trained and\nnon-instantiated neural network models connected as a rooted directed acyclic\ngraph for solving complicated applications. A pre-trained neural network model\nis generally well trained, targeted to approximate a specific function. Despite\na general belief that a composite neural network may perform better than a\nsingle component, the overall performance characteristics are not clear. In\nthis work, we construct the framework of a composite network, and prove that a\ncomposite neural network performs better than any of its pre-trained components\nwith a high probability bound. In addition, if an extra pre-trained component\nis added to a composite network, with high probability, the overall performance\nwill not be degraded. In the study, we explore a complicated application --\nPM2.5 prediction -- to illustrate the correctness of the proposed composite\nnetwork theory. In the empirical evaluations of PM2.5 prediction, the\nconstructed composite neural network models support the proposed theory and\nperform better than other machine learning models, demonstrate the advantages\nof the proposed framework.",
          "link": "http://arxiv.org/abs/1910.09739",
          "publishedOn": "2021-07-20T02:04:46.865Z",
          "wordCount": null,
          "title": "Composite Neural Network: Theory and Application to PM2.5 Prediction. (arXiv:1910.09739v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dandekar_A/0/1/0/all/0/1\">Abhishek Dandekar</a>",
          "description": "Machine learning (ML) techniques are being increasingly used in mobile\nnetworks for network planning, operation, management, optimisation and much\nmore. These techniques are realised using a set of logical nodes known as ML\npipeline. A single network operator might have thousands of such ML pipelines\ndistributed across its network. These pipelines need to be managed and\norchestrated across network domains. Thus it is essential to have autonomic\nmulti-domain orchestration of ML pipelines in mobile networks. International\nTelecommunications Union (ITU) has provided an architectural framework for\nmanagement and orchestration of ML pipelines in future networks. We extend this\nframework to enable autonomic orchestration of ML pipelines across multiple\nnetwork domains. We present our system architecture and describe its\napplication using a smart factory use case. Our work allows autonomic\norchestration of multi-domain ML pipelines in a standardised, technology\nagnostic, privacy preserving fashion.",
          "link": "http://arxiv.org/abs/2107.08194",
          "publishedOn": "2021-07-20T02:04:46.864Z",
          "wordCount": null,
          "title": "Towards autonomic orchestration of machine learning pipelines in future networks. (arXiv:2107.08194v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2002.06470",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ashukha_A/0/1/0/all/0/1\">Arsenii Ashukha</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lyzhov_A/0/1/0/all/0/1\">Alexander Lyzhov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Molchanov_D/0/1/0/all/0/1\">Dmitry Molchanov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vetrov_D/0/1/0/all/0/1\">Dmitry Vetrov</a>",
          "description": "Uncertainty estimation and ensembling methods go hand-in-hand. Uncertainty\nestimation is one of the main benchmarks for assessment of ensembling\nperformance. At the same time, deep learning ensembles have provided\nstate-of-the-art results in uncertainty estimation. In this work, we focus on\nin-domain uncertainty for image classification. We explore the standards for\nits quantification and point out pitfalls of existing metrics. Avoiding these\npitfalls, we perform a broad study of different ensembling techniques. To\nprovide more insight in this study, we introduce the deep ensemble equivalent\nscore (DEE) and show that many sophisticated ensembling techniques are\nequivalent to an ensemble of only few independently trained networks in terms\nof test performance.",
          "link": "http://arxiv.org/abs/2002.06470",
          "publishedOn": "2021-07-20T02:04:46.864Z",
          "wordCount": null,
          "title": "Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning. (arXiv:2002.06470v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.08170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingfeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chongjie Zhang</a>",
          "description": "Meta reinforcement learning (meta-RL) extracts knowledge from previous tasks\nand achieves fast adaptation to new tasks. Despite recent progress, efficient\nexploration in meta-RL remains a key challenge in sparse-reward tasks, as it\nrequires quickly finding informative task-relevant experiences in both\nmeta-training and adaptation. To address this challenge, we explicitly model an\nexploration policy learning problem for meta-RL, which is separated from\nexploitation policy learning, and introduce a novel empowerment-driven\nexploration objective, which aims to maximize information gain for task\nidentification. We derive a corresponding intrinsic reward and develop a new\noff-policy meta-RL framework, which efficiently learns separate context-aware\nexploration and exploitation policies by sharing the knowledge of task\ninference. Experimental evaluation shows that our meta-RL method significantly\noutperforms state-of-the-art baselines on various sparse-reward MuJoCo\nlocomotion tasks and more complex sparse-reward Meta-World tasks.",
          "link": "http://arxiv.org/abs/2006.08170",
          "publishedOn": "2021-07-20T02:04:46.863Z",
          "wordCount": null,
          "title": "MetaCURE: Meta Reinforcement Learning with Empowerment-Driven Exploration. (arXiv:2006.08170v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>",
          "description": "Previous studies have shown that initializing neural machine translation\n(NMT) models with the pre-trained language models (LM) can speed up the model\ntraining and boost the model performance. In this work, we identify a critical\nside-effect of pre-training for NMT, which is due to the discrepancy between\nthe training objectives of LM-based pre-training and NMT. Since the LM\nobjective learns to reconstruct a few source tokens and copy most of them, the\npre-training initialization would affect the copying behaviors of NMT models.\nWe provide a quantitative analysis of copying behaviors by introducing a metric\ncalled copying ratio, which empirically shows that pre-training based NMT\nmodels have a larger copying ratio than the standard one. In response to this\nproblem, we propose a simple and effective method named copying penalty to\ncontrol the copying behaviors in decoding. Extensive experiments on both\nin-domain and out-of-domain benchmarks show that the copying penalty method\nconsistently improves translation performance by controlling copying behaviors\nfor pre-training based NMT models. Source code is freely available at\nhttps://github.com/SunbowLiu/CopyingPenalty.",
          "link": "http://arxiv.org/abs/2107.08212",
          "publishedOn": "2021-07-20T02:04:46.862Z",
          "wordCount": null,
          "title": "On the Copying Behaviors of Pre-Training for Neural Machine Translation. (arXiv:2107.08212v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.11860",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Garg_A/0/1/0/all/0/1\">Aksh Garg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salehi_S/0/1/0/all/0/1\">Sana Salehi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rocca_M/0/1/0/all/0/1\">Marianna La Rocca</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garner_R/0/1/0/all/0/1\">Rachael Garner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duncan_D/0/1/0/all/0/1\">Dominique Duncan</a>",
          "description": "With COVID-19 cases rising rapidly, deep learning has emerged as a promising\ndiagnosis technique. However, identifying the most accurate models to\ncharacterize COVID-19 patients is challenging because comparing results\nobtained with different types of data and acquisition processes is non-trivial.\nIn this paper we designed, evaluated, and compared the performance of 20\nconvolutional neutral networks in classifying patients as COVID-19 positive,\nhealthy, or suffering from other pulmonary lung infections based on Chest CT\nscans, serving as the first to consider the EfficientNet family for COVID-19\ndiagnosis and employ intermediate activation maps for visualizing model\nperformance. All models are trained and evaluated in Python using 4173 Chest CT\nimages from the dataset entitled \"A COVID multiclass dataset of CT scans,\" with\n2168, 758, and 1247 images of patients that are COVID-19 positive, healthy, or\nsuffering from other pulmonary infections, respectively. EfficientNet-B5 was\nidentified as the best model with an F1 score of 0.9769+/-0.0046, accuracy of\n0.9759+/-0.0048, sensitivity of 0.9788+/-0.0055, specificity of\n0.9730+/-0.0057, and precision of 0.9751 +/- 0.0051. On an alternate 2-class\ndataset, EfficientNetB5 obtained an accuracy of 0.9845+/-0.0109, F1 score of\n0.9599+/-0.0251, sensitivity of 0.9682+/-0.0099, specificity of\n0.9883+/-0.0150, and precision of 0.9526 +/- 0.0523. Intermediate activation\nmaps and Gradient-weighted Class Activation Mappings offered\nhuman-interpretable evidence of the model's perception of ground-class\nopacities and consolidations, hinting towards a promising use-case of\nartificial intelligence-assisted radiology tools. With a prediction speed of\nunder 0.1 seconds on GPUs and 0.5 seconds on CPUs, our proposed model offers a\nrapid, scalable, and accurate diagnostic for COVID-19.",
          "link": "http://arxiv.org/abs/2012.11860",
          "publishedOn": "2021-07-20T02:04:46.862Z",
          "wordCount": null,
          "title": "Efficient and Visualizable Convolutional Neural Networks for COVID-19 Classification Using Chest CT. (arXiv:2012.11860v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08135",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yamane_I/0/1/0/all/0/1\">Ikko Yamane</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Honda_J/0/1/0/all/0/1\">Junya Honda</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yger_F/0/1/0/all/0/1\">Florian Yger</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "Ordinary supervised learning is useful when we have paired training data of\ninput $X$ and output $Y$. However, such paired data can be difficult to collect\nin practice. In this paper, we consider the task of predicting $Y$ from $X$\nwhen we have no paired data of them, but we have two separate, independent\ndatasets of $X$ and $Y$ each observed with some mediating variable $U$, that\nis, we have two datasets $S_X = \\{(X_i, U_i)\\}$ and $S_Y = \\{(U'_j, Y'_j)\\}$. A\nnaive approach is to predict $U$ from $X$ using $S_X$ and then $Y$ from $U$\nusing $S_Y$, but we show that this is not statistically consistent. Moreover,\npredicting $U$ can be more difficult than predicting $Y$ in practice, e.g.,\nwhen $U$ has higher dimensionality. To circumvent the difficulty, we propose a\nnew method that avoids predicting $U$ but directly learns $Y = f(X)$ by\ntraining $f(X)$ with $S_{X}$ to predict $h(U)$ which is trained with $S_{Y}$ to\napproximate $Y$. We prove statistical consistency and error bounds of our\nmethod and experimentally confirm its practical usefulness.",
          "link": "http://arxiv.org/abs/2107.08135",
          "publishedOn": "2021-07-20T02:04:46.861Z",
          "wordCount": null,
          "title": "Mediated Uncoupled Learning: Learning Functions without Direct Input-output Correspondences. (arXiv:2107.08135v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">JoonSung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">YeongHyeon Park</a>",
          "description": "Recently Autoencoder(AE) based models are widely used in the field of anomaly\ndetection. A model trained with normal data generates a larger restoration\nerror for abnormal data. Whether or not abnormal data is determined by\nobserving the restoration error. It takes a lot of cost and time to obtain\nabnormal data in the industrial field. Therefore the model trains only normal\ndata and detects abnormal data in the inference phase. However, the restoration\narea for the input data of AE is limited in the latent space. To solve this\nproblem, we propose Multiple-hypothesis Autoencoder(MH-AE) model composed of\nseveral decoders. MH-AE model increases the restoration area through contention\nbetween decoders. The proposed method shows that the anomaly detection\nperformance is improved compared to the traditional AE for various input\ndatasets.",
          "link": "http://arxiv.org/abs/2107.08790",
          "publishedOn": "2021-07-20T02:04:46.860Z",
          "wordCount": null,
          "title": "Anomaly Detection Based on Multiple-Hypothesis Autoencoder. (arXiv:2107.08790v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiahua Luo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Vong_C/0/1/0/all/0/1\">Chi-Man Vong</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jie Du</a> (2) ((1) Department of Computer and Information Science, University of Macau, Macao SAR, China, (2) School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen, China)",
          "description": "Sparse Bayesian Learning (SBL) constructs an extremely sparse probabilistic\nmodel with very competitive generalization. However, SBL needs to invert a big\ncovariance matrix with complexity O(M^3 ) (M: feature size) for updating the\nregularization priors, making it difficult for practical use. There are three\nissues in SBL: 1) Inverting the covariance matrix may obtain singular solutions\nin some cases, which hinders SBL from convergence; 2) Poor scalability to\nproblems with high dimensional feature space or large data size; 3) SBL easily\nsuffers from memory overflow for large-scale data. This paper addresses these\nissues with a newly proposed diagonal Quasi-Newton (DQN) method for SBL called\nDQN-SBL where the inversion of big covariance matrix is ignored so that the\ncomplexity and memory storage are reduced to O(M). The DQN-SBL is thoroughly\nevaluated on non-linear classifiers and linear feature selection using various\nbenchmark datasets of different sizes. Experimental results verify that DQN-SBL\nreceives competitive generalization with a very sparse model and scales well to\nlarge-scale problems.",
          "link": "http://arxiv.org/abs/2107.08195",
          "publishedOn": "2021-07-20T02:04:46.829Z",
          "wordCount": null,
          "title": "Sparse Bayesian Learning with Diagonal Quasi-Newton Method For Large Scale Classification. (arXiv:2107.08195v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.12804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1\">Martin Fajcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jon_J/0/1/0/all/0/1\">Josef Jon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>",
          "description": "This work demonstrates that, contrary to a common belief, using the objective\nwith independence assumption for modelling the span probability $P(a_s,a_e) =\nP(a_s)P(a_e)$ of span starting at position $a_s$ and ending at position $a_e$\nhas adverse effects. Therefore we propose multiple approaches to modelling\njoint probability $P(a_s,a_e)$ directly. Among those, we propose a compound\nobjective, composed from the joint probability while still keeping the\nobjective with independence assumption as an auxiliary objective. We find that\nthe compound objective is consistently superior or equal to other assumptions\nin exact match. Additionally, we identified common errors caused by the\nassumption of independence and manually checked the counterpart predictions,\ndemonstrating the impact of the compound objective on the real examples. Our\nfindings are supported via experiments with three extractive QA models (BIDAF,\nBERT, ALBERT) over six datasets and our code, individual results and manual\nanalysis are available online.",
          "link": "http://arxiv.org/abs/2008.12804",
          "publishedOn": "2021-07-20T02:04:46.806Z",
          "wordCount": null,
          "title": "Rethinking the Objectives of Extractive Question Answering. (arXiv:2008.12804v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chi-Heng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azabou_M/0/1/0/all/0/1\">Mehdi Azabou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_E/0/1/0/all/0/1\">Eva L. Dyer</a>",
          "description": "Optimal transport (OT) is a widely used technique for distribution alignment,\nwith applications throughout the machine learning, graphics, and vision\ncommunities. Without any additional structural assumptions on trans-port,\nhowever, OT can be fragile to outliers or noise, especially in high dimensions.\nHere, we introduce a new form of structured OT that simultaneously learns\nlow-dimensional structure in data while leveraging this structure to solve the\nalignment task. Compared with OT, the resulting transport plan has better\nstructural interpretability, highlighting the connections between individual\ndata points and local geometry, and is more robust to noise and sampling. We\napply the method to synthetic as well as real datasets, where we show that our\nmethod can facilitate alignment in noisy settings and can be used to both\ncorrect and interpret domain shift.",
          "link": "http://arxiv.org/abs/2012.11589",
          "publishedOn": "2021-07-20T02:04:46.806Z",
          "wordCount": null,
          "title": "Making transport more robust and interpretable by moving data through a small number of anchor points. (arXiv:2012.11589v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.13955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zafarani_F/0/1/0/all/0/1\">Farzad Zafarani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_C/0/1/0/all/0/1\">Chris Clifton</a>",
          "description": "With the increasing collection of users' data, protecting individual privacy\nhas gained more interest. Differential Privacy is a strong concept of\nprotecting individuals. Naive Bayes is one of the popular machine learning\nalgorithm, used as a baseline for many tasks. In this work, we have provided a\ndifferentially private Naive Bayes classifier that adds noise proportional to\nthe Smooth Sensitivity of its parameters. We have compared our result to\nVaidya, Shafiq, Basu, and Hong in which they have scaled the noise to the\nglobal sensitivity of the parameters. Our experiment results on the real-world\ndatasets show that the accuracy of our method has improved significantly while\nstill preserving $\\varepsilon$-differential privacy.",
          "link": "http://arxiv.org/abs/2003.13955",
          "publishedOn": "2021-07-20T02:04:46.805Z",
          "wordCount": null,
          "title": "Differentially Private Naive Bayes Classifier using Smooth Sensitivity. (arXiv:2003.13955v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen-Yu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chung-Wei Lee</a>",
          "description": "Policy optimization is a widely-used method in reinforcement learning. Due to\nits local-search nature, however, theoretical guarantees on global optimality\noften rely on extra assumptions on the Markov Decision Processes (MDPs) that\nbypass the challenge of global exploration. To eliminate the need of such\nassumptions, in this work, we develop a general solution that adds dilated\nbonuses to the policy update to facilitate global exploration. To showcase the\npower and generality of this technique, we apply it to several episodic MDP\nsettings with adversarial losses and bandit feedback, improving and\ngeneralizing the state-of-the-art. Specifically, in the tabular case, we obtain\n$\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret where $T$ is the number of episodes,\nimproving the $\\widetilde{\\mathcal{O}}({T}^{2/3})$ regret bound by Shani et al.\n(2020). When the number of states is infinite, under the assumption that the\nstate-action values are linear in some low-dimensional features, we obtain\n$\\widetilde{\\mathcal{O}}({T}^{2/3})$ regret with the help of a simulator,\nmatching the result of Neu and Olkhovskaya (2020) while importantly removing\nthe need of an exploratory policy that their algorithm requires. When a\nsimulator is unavailable, we further consider a linear MDP setting and obtain\n$\\widetilde{\\mathcal{O}}({T}^{14/15})$ regret, which is the first result for\nlinear MDPs with adversarial losses and bandit feedback.",
          "link": "http://arxiv.org/abs/2107.08346",
          "publishedOn": "2021-07-20T02:04:46.804Z",
          "wordCount": null,
          "title": "Policy Optimization in Adversarial MDPs: Improved Exploration via Dilated Bonuses. (arXiv:2107.08346v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meiyazhagan_J/0/1/0/all/0/1\">J.Meiyazhagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudharsan_S/0/1/0/all/0/1\">S. Sudharsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senthilvelan_M/0/1/0/all/0/1\">M. Senthilvelan</a>",
          "description": "We predict the emergence of extreme events in a parametrically driven\nnonlinear dynamical system using three Deep Learning models, namely Multi-Layer\nPerceptron, Convolutional Neural Network and Long Short-Term Memory. The Deep\nLearning models are trained using the training set and are allowed to predict\nthe test set data. After prediction, the time series of the actual and the\npredicted values are plotted one over the other in order to visualize the\nperformance of the models. Upon evaluating the Root Mean Square Error value\nbetween predicted and the actual values of all three models, we find that the\nLong Short-Term Memory model can serve as the best model to forecast the\nchaotic time series and to predict the emergence of extreme events for the\nconsidered system.",
          "link": "http://arxiv.org/abs/2107.08819",
          "publishedOn": "2021-07-20T02:04:46.804Z",
          "wordCount": null,
          "title": "Model-free prediction of emergence of extreme events in a parametrically driven nonlinear dynamical system by Deep Learning. (arXiv:2107.08819v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.09478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parunandi_K/0/1/0/all/0/1\">Karthikeya S. Parunandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Aayushman Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_R/0/1/0/all/0/1\">Raman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravorty_S/0/1/0/all/0/1\">Suman Chakravorty</a>",
          "description": "The problem of Reinforcement Learning (RL) in an unknown nonlinear dynamical\nsystem is equivalent to the search for an optimal feedback law utilizing the\nsimulations/ rollouts of the unknown dynamical system. Most RL techniques\nsearch over a complex global nonlinear feedback parametrization making them\nsuffer from high training times as well as variance. Instead, we advocate\nsearching over a local feedback representation consisting of an open-loop\nsequence, and an associated optimal linear feedback law completely determined\nby the open-loop. We show that this alternate approach results in highly\nefficient training, the answers obtained are repeatable and hence reliable, and\nthe resulting closed performance is superior to global state-of-the-art RL\ntechniques. Finally, if we replan, whenever required, which is feasible due to\nthe fast and reliable local solution, allows us to recover global optimality of\nthe resulting feedback law.",
          "link": "http://arxiv.org/abs/2002.09478",
          "publishedOn": "2021-07-20T02:04:46.803Z",
          "wordCount": null,
          "title": "On the Search for Feedback in Reinforcement Learning. (arXiv:2002.09478v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.02463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mounir_R/0/1/0/all/0/1\">Ramy Mounir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gula_R/0/1/0/all/0/1\">Roman Gula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theuerkauf_J/0/1/0/all/0/1\">J&#xf6;rn Theuerkauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Sudeep Sarkar</a>",
          "description": "Using offline training schemes, researchers have tackled the event\nsegmentation problem by providing full or weak-supervision through manually\nannotated labels or self-supervised epoch-based training. Most works consider\nvideos that are at most 10's of minutes long. We present a self-supervised\nperceptual prediction framework capable of temporal event segmentation by\nbuilding stable representations of objects over time and demonstrate it on long\nvideos, spanning several days. The approach is deceptively simple but quite\neffective. We rely on predictions of high-level features computed by a standard\ndeep learning backbone. For prediction, we use an LSTM, augmented with an\nattention mechanism, trained in a self-supervised manner using the prediction\nerror. The self-learned attention maps effectively localize and track the\nevent-related objects in each frame. The proposed approach does not require\nlabels. It requires only a single pass through the video, with no separate\ntraining set. Given the lack of datasets of very long videos, we demonstrate\nour method on video from 10 days (254 hours) of continuous wildlife monitoring\ndata that we had collected with required permissions. We find that the approach\nis robust to various environmental conditions such as day/night conditions,\nrain, sharp shadows, and windy conditions. For the task of temporally locating\nevents, we had an 80% recall rate at 20% false-positive rate for frame-level\nsegmentation. At the activity level, we had an 80% activity recall rate for one\nfalse activity detection every 50 minutes. We will make the dataset, which is\nthe first of its kind, and the code available to the research community.",
          "link": "http://arxiv.org/abs/2005.02463",
          "publishedOn": "2021-07-20T02:04:46.803Z",
          "wordCount": null,
          "title": "Spatio-Temporal Event Segmentation and Localization for Wildlife Extended Videos. (arXiv:2005.02463v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07458",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tianyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chenyou Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Nhat Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuturi_M/0/1/0/all/0/1\">Marco Cuturi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "Projection robust Wasserstein (PRW) distance, or Wasserstein projection\npursuit (WPP), is a robust variant of the Wasserstein distance. Recent work\nsuggests that this quantity is more robust than the standard Wasserstein\ndistance, in particular when comparing probability measures in high-dimensions.\nHowever, it is ruled out for practical application because the optimization\nmodel is essentially non-convex and non-smooth which makes the computation\nintractable. Our contribution in this paper is to revisit the original\nmotivation behind WPP/PRW, but take the hard route of showing that, despite its\nnon-convexity and lack of nonsmoothness, and even despite some hardness results\nproved by~\\citet{Niles-2019-Estimation} in a minimax sense, the original\nformulation for PRW/WPP \\textit{can} be efficiently computed in practice using\nRiemannian optimization, yielding in relevant cases better behavior than its\nconvex relaxation. More specifically, we provide three simple algorithms with\nsolid theoretical guarantee on their complexity bound (one in the appendix),\nand demonstrate their effectiveness and efficiency by conducing extensive\nexperiments on synthetic and real data. This paper provides a first step into a\ncomputational theory of the PRW distance and provides the links between optimal\ntransport and Riemannian optimization.",
          "link": "http://arxiv.org/abs/2006.07458",
          "publishedOn": "2021-07-20T02:04:46.802Z",
          "wordCount": null,
          "title": "Projection Robust Wasserstein Distance and Riemannian Optimization. (arXiv:2006.07458v8 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.00038",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Birmpa_P/0/1/0/all/0/1\">Panagiota Birmpa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Katsoulakis_M/0/1/0/all/0/1\">Markos A. Katsoulakis</a>",
          "description": "We present an information-based uncertainty quantification method for general\nMarkov Random Fields. Markov Random Fields (MRF) are structured, probabilistic\ngraphical models over undirected graphs, and provide a fundamental unifying\nmodeling tool for statistical mechanics, probabilistic machine learning, and\nartificial intelligence. Typically MRFs are complex and high-dimensional with\nnodes and edges (connections) built in a modular fashion from simpler,\nlow-dimensional probabilistic models and their local connections; in turn, this\nmodularity allows to incorporate available data to MRFs and efficiently\nsimulate them by leveraging their graph-theoretic structure. Learning graphical\nmodels from data and/or constructing them from physical modeling and\nconstraints necessarily involves uncertainties inherited from data, modeling\nchoices, or numerical approximations. These uncertainties in the MRF can be\nmanifested either in the graph structure or the probability distribution\nfunctions, and necessarily will propagate in predictions for quantities of\ninterest. Here we quantify such uncertainties using tight, information based\nbounds on the predictions of quantities of interest; these bounds take\nadvantage of the graphical structure of MRFs and are capable of handling the\ninherent high-dimensionality of such graphical models. We demonstrate our\nmethods in MRFs for medical diagnostics and statistical mechanics models. In\nthe latter, we develop uncertainty quantification bounds for finite size\neffects and phase diagrams, which constitute two of the typical predictions\ngoals of statistical mechanics modeling.",
          "link": "http://arxiv.org/abs/2009.00038",
          "publishedOn": "2021-07-20T02:04:46.801Z",
          "wordCount": null,
          "title": "Uncertainty quantification for Markov Random Fields. (arXiv:2009.00038v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_G/0/1/0/all/0/1\">Gautam Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carnahan_M/0/1/0/all/0/1\">Mason Carnahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamapant_S/0/1/0/all/0/1\">Shilpa Shamapant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surendranath_Y/0/1/0/all/0/1\">Yashitha Surendranath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saumya Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Arundhati Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1\">Co Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millan_J/0/1/0/all/0/1\">Jose del R Millan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewfik_A/0/1/0/all/0/1\">Ahmed H Tewfik</a>",
          "description": "In this paper, we propose a deep learning-based algorithm to improve the\nperformance of automatic speech recognition (ASR) systems for aphasia, apraxia,\nand dysarthria speech by utilizing electroencephalography (EEG) features\nrecorded synchronously with aphasia, apraxia, and dysarthria speech. We\ndemonstrate a significant decoding performance improvement by more than 50\\%\nduring test time for isolated speech recognition task and we also provide\npreliminary results indicating performance improvement for the more challenging\ncontinuous speech recognition task by utilizing EEG features. The results\npresented in this paper show the first step towards demonstrating the\npossibility of utilizing non-invasive neural signals to design a real-time\nrobust speech prosthetic for stroke survivors recovering from aphasia, apraxia,\nand dysarthria. Our aphasia, apraxia, and dysarthria speech-EEG data set will\nbe released to the public to help further advance this interesting and crucial\nresearch.",
          "link": "http://arxiv.org/abs/2103.00383",
          "publishedOn": "2021-07-20T02:04:46.801Z",
          "wordCount": null,
          "title": "Brain Signals to Rescue Aphasia, Apraxia and Dysarthria Speech Recognition. (arXiv:2103.00383v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jack Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1\">Raphael Lenain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1\">Udeepa Meepegama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1\">Emil Fristed</a>",
          "description": "We introduce ParaBLEU, a paraphrase representation learning model and\nevaluation metric for text generation. Unlike previous approaches, ParaBLEU\nlearns to understand paraphrasis using generative conditioning as a pretraining\nobjective. ParaBLEU correlates more strongly with human judgements than\nexisting metrics, obtaining new state-of-the-art results on the 2017 WMT\nMetrics Shared Task. We show that our model is robust to data scarcity,\nexceeding previous state-of-the-art performance using only $50\\%$ of the\navailable training data and surpassing BLEU, ROUGE and METEOR with only $40$\nlabelled examples. Finally, we demonstrate that ParaBLEU can be used to\nconditionally generate novel paraphrases from a single demonstration, which we\nuse to confirm our hypothesis that it learns abstract, generalized paraphrase\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.08251",
          "publishedOn": "2021-07-20T02:04:46.799Z",
          "wordCount": null,
          "title": "Generative Pretraining for Paraphrase Evaluation. (arXiv:2107.08251v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Ting Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lijun Zhang</a>",
          "description": "Fairness is crucial for neural networks which are used in applications with\nimportant societal implication. Recently, there have been multiple attempts on\nimproving fairness of neural networks, with a focus on fairness testing (e.g.,\ngenerating individual discriminatory instances) and fairness training (e.g.,\nenhancing fairness through augmented training). In this work, we propose an\napproach to formally verify neural networks against fairness, with a focus on\nindependence-based fairness such as group fairness. Our method is built upon an\napproach for learning Markov Chains from a user-provided neural network (i.e.,\na feed-forward neural network or a recurrent neural network) which is\nguaranteed to facilitate sound analysis. The learned Markov Chain not only\nallows us to verify (with Probably Approximate Correctness guarantee) whether\nthe neural network is fair or not, but also facilities sensitivity analysis\nwhich helps to understand why fairness is violated. We demonstrate that with\nour analysis results, the neural weights can be optimized to improve fairness.\nOur approach has been evaluated with multiple models trained on benchmark\ndatasets and the experiment results show that our approach is effective and\nefficient.",
          "link": "http://arxiv.org/abs/2107.08362",
          "publishedOn": "2021-07-20T02:04:46.799Z",
          "wordCount": null,
          "title": "Probabilistic Verification of Neural Networks Against Group Fairness. (arXiv:2107.08362v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.11068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhengping Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>",
          "description": "In deep model compression, the recent finding \"Lottery Ticket Hypothesis\"\n(LTH) (Frankle & Carbin, 2018) pointed out that there could exist a winning\nticket (i.e., a properly pruned sub-network together with original weight\ninitialization) that can achieve competitive performance than the original\ndense network. However, it is not easy to observe such winning property in many\nscenarios, where for example, a relatively large learning rate is used even if\nit benefits training the original dense model. In this work, we investigate the\nunderlying condition and rationale behind the winning property, and find that\nthe underlying reason is largely attributed to the correlation between\ninitialized weights and final-trained weights when the learning rate is not\nsufficiently large. Thus, the existence of winning property is correlated with\nan insufficient DNN pretraining, and is unlikely to occur for a well-trained\nDNN. To overcome this limitation, we propose the \"pruning & fine-tuning\" method\nthat consistently outperforms lottery ticket sparse training under the same\npruning algorithm and the same total training epochs. Extensive experiments\nover multiple deep models (VGG, ResNet, MobileNet-v2) on different datasets\nhave been conducted to justify our proposals.",
          "link": "http://arxiv.org/abs/2102.11068",
          "publishedOn": "2021-07-20T02:04:46.798Z",
          "wordCount": null,
          "title": "Lottery Ticket Preserves Weight Correlation: Is It Desirable or Not?. (arXiv:2102.11068v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08850",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ganz_J/0/1/0/all/0/1\">Jonathan Ganz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kirsch_T/0/1/0/all/0/1\">Tobias Kirsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_L/0/1/0/all/0/1\">Lucas Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bertram_C/0/1/0/all/0/1\">Christof A. Bertram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_C/0/1/0/all/0/1\">Christoph Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blumcke_I/0/1/0/all/0/1\">Ingmar Bl&#xfc;mcke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jabari_S/0/1/0/all/0/1\">Samir Jabari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aubreville_M/0/1/0/all/0/1\">Marc Aubreville</a>",
          "description": "Meningioma is one of the most prevalent brain tumors in adults. To determine\nits malignancy, it is graded by a pathologist into three grades according to\nWHO standards. This grade plays a decisive role in treatment, and yet may be\nsubject to inter-rater discordance. In this work, we present and compare three\napproaches towards fully automatic meningioma grading from histology whole\nslide images. All approaches are following a two-stage paradigm, where we first\nidentify a region of interest based on the detection of mitotic figures in the\nslide using a state-of-the-art object detection deep learning network. This\nregion of highest mitotic rate is considered characteristic for biological\ntumor behavior. In the second stage, we calculate a score corresponding to\ntumor malignancy based on information contained in this region using three\ndifferent settings. In a first approach, image patches are sampled from this\nregion and regression is based on morphological features encoded by a\nResNet-based network. We compare this to learning a logistic regression from\nthe determined mitotic count, an approach which is easily traceable and\nexplainable. Lastly, we combine both approaches in a single network. We trained\nthe pipeline on 951 slides from 341 patients and evaluated them on a separate\nset of 141 slides from 43 patients. All approaches yield a high correlation to\nthe WHO grade. The logistic regression and the combined approach had the best\nresults in our experiments, yielding correct predictions in 32 and 33 of all\ncases, respectively, with the image-based approach only predicting 25 cases\ncorrectly. Spearman's correlation was 0.716, 0.792 and 0.790 respectively. It\nmay seem counterintuitive at first that morphological features provided by\nimage patches do not improve model performance. Yet, this mirrors the criteria\nof the grading scheme, where mitotic count is the only unequivocal parameter.",
          "link": "http://arxiv.org/abs/2107.08850",
          "publishedOn": "2021-07-20T02:04:46.797Z",
          "wordCount": null,
          "title": "Automatic and explainable grading of meningiomas from histopathology images. (arXiv:2107.08850v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jack Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1\">Raphael Lenain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1\">Udeepa Meepegama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1\">Emil Fristed</a>",
          "description": "We propose a method for learning de-identified prosody representations from\nraw audio using a contrastive self-supervised signal. Whereas prior work has\nrelied on conditioning models on bottlenecks, we introduce a set of inductive\nbiases that exploit the natural structure of prosody to minimize timbral\ninformation and decouple prosody from speaker representations. Despite\naggressive downsampling of the input and having no access to linguistic\ninformation, our model performs comparably to state-of-the-art speech\nrepresentations on DAMMP, a new benchmark we introduce for spoken language\nunderstanding. We use minimum description length probing to show that our\nrepresentations have selectively learned the subcomponents of non-timbral\nprosody, and that the product quantizer naturally disentangles them without\nusing bottlenecks. We derive an information-theoretic definition of speech\nde-identifiability and use it to demonstrate that our prosody representations\nare less identifiable than other speech representations.",
          "link": "http://arxiv.org/abs/2107.08248",
          "publishedOn": "2021-07-20T02:04:46.796Z",
          "wordCount": null,
          "title": "Learning De-identified Representations of Prosody from Raw Audio. (arXiv:2107.08248v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Alan Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_H/0/1/0/all/0/1\">Hugo Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Sungsu Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozuno_T/0/1/0/all/0/1\">Tadashi Kozuno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1\">A. Rupam Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1\">Martha White</a>",
          "description": "Approximate Policy Iteration (API) algorithms alternate between (approximate)\npolicy evaluation and (approximate) greedification. Many different approaches\nhave been explored for approximate policy evaluation, but less is understood\nabout approximate greedification and what choices guarantee policy improvement.\nIn this work, we investigate approximate greedification when reducing the KL\ndivergence between the parameterized policy and the Boltzmann distribution over\naction values. In particular, we investigate the difference between the forward\nand reverse KL divergences, with varying degrees of entropy regularization. We\nshow that the reverse KL has stronger policy improvement guarantees, but that\nreducing the forward KL can result in a worse policy. We also demonstrate,\nhowever, that a large enough reduction of the forward KL can induce improvement\nunder additional assumptions. Empirically, we show on simple continuous-action\nenvironments that the forward KL can induce more exploration, but at the cost\nof a more suboptimal policy. No significant differences were observed in the\ndiscrete-action setting or on a suite of benchmark problems. Throughout, we\nhighlight that many policy gradient methods can be seen as an instance of API,\nwith either the forward or reverse KL for the policy update, and discuss next\nsteps for understanding and improving our policy optimization algorithms.",
          "link": "http://arxiv.org/abs/2107.08285",
          "publishedOn": "2021-07-20T02:04:46.795Z",
          "wordCount": null,
          "title": "Greedification Operators for Policy Optimization: Investigating Forward and Reverse KL Divergences. (arXiv:2107.08285v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1\">Jiandong Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengdi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feiwen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>",
          "description": "Recently, neural network compression schemes like channel pruning have been\nwidely used to reduce the model size and computational complexity of deep\nneural network (DNN) for applications in power-constrained scenarios such as\nembedded systems. Reinforcement learning (RL)-based auto-pruning has been\nfurther proposed to automate the DNN pruning process to avoid expensive\nhand-crafted work. However, the RL-based pruner involves a time-consuming\ntraining process and the high expense of each sample further exacerbates this\nproblem. These impediments have greatly restricted the real-world application\nof RL-based auto-pruning. Thus, in this paper, we propose an efficient\nauto-pruning framework which solves this problem by taking advantage of the\nhistorical data from the previous auto-pruning process. In our framework, we\nfirst boost the convergence of the RL-pruner by transfer learning. Then, an\naugmented transfer learning scheme is proposed to further speed up the training\nprocess by improving the transferability. Finally, an assistant learning\nprocess is proposed to improve the sample efficiency of the RL agent. The\nexperiments have shown that our framework can accelerate the auto-pruning\nprocess by 1.5-2.5 times for ResNet20, and 1.81-2.375 times for other neural\nnetworks like ResNet56, ResNet18, and MobileNet v1.",
          "link": "http://arxiv.org/abs/2107.08815",
          "publishedOn": "2021-07-20T02:04:46.794Z",
          "wordCount": null,
          "title": "Boosting the Convergence of Reinforcement Learning-based Auto-pruning Using Historical Data. (arXiv:2107.08815v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fotakis_D/0/1/0/all/0/1\">Dimitris Fotakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gergatsouli_E/0/1/0/all/0/1\">Evangelia Gergatsouli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouleakis_T/0/1/0/all/0/1\">Themis Gouleakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patris_N/0/1/0/all/0/1\">Nikolas Patris</a>",
          "description": "Following the research agenda initiated by Munoz & Vassilvitskii [1] and\nLykouris & Vassilvitskii [2] on learning-augmented online algorithms for\nclassical online optimization problems, in this work, we consider the Online\nFacility Location problem under this framework. In Online Facility Location\n(OFL), demands arrive one-by-one in a metric space and must be (irrevocably)\nassigned to an open facility upon arrival, without any knowledge about future\ndemands.\n\nWe present an online algorithm for OFL that exploits potentially imperfect\npredictions on the locations of the optimal facilities. We prove that the\ncompetitive ratio decreases smoothly from sublogarithmic in the number of\ndemands to constant, as the error, i.e., the total distance of the predicted\nlocations to the optimal facility locations, decreases towards zero. We\ncomplement our analysis with a matching lower bound establishing that the\ndependence of the algorithm's competitive ratio on the error is optimal, up to\nconstant factors. Finally, we evaluate our algorithm on real world data and\ncompare our learning augmented approach with the current best online algorithm\nfor the problem.",
          "link": "http://arxiv.org/abs/2107.08277",
          "publishedOn": "2021-07-20T02:04:46.793Z",
          "wordCount": null,
          "title": "Learning Augmented Online Facility Location. (arXiv:2107.08277v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Triet H. M. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huaming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babar_M/0/1/0/all/0/1\">M. Ali Babar</a>",
          "description": "Software Vulnerabilities (SVs) are increasing in complexity and scale, posing\ngreat security risks to many software systems. Given the limited resources in\npractice, SV assessment and prioritization help practitioners devise optimal SV\nmitigation plans based on various SV characteristics. The surge in SV data\nsources and data-driven techniques such as Machine Learning and Deep Learning\nhave taken SV assessment and prioritization to the next level. Our survey\nprovides a taxonomy of the past research efforts and highlights the best\npractices for data-driven SV assessment and prioritization. We also discuss the\ncurrent limitations and propose potential solutions to address such issues.",
          "link": "http://arxiv.org/abs/2107.08364",
          "publishedOn": "2021-07-20T02:04:46.792Z",
          "wordCount": null,
          "title": "A Survey on Data-driven Software Vulnerability Assessment and Prioritization. (arXiv:2107.08364v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sushant Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabbari_S/0/1/0/all/0/1\">Shahin Jabbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Sohini Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiwei Steven Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>",
          "description": "As machine learning black boxes are increasingly being deployed in critical\ndomains such as healthcare and criminal justice, there has been a growing\nemphasis on developing techniques for explaining these black boxes in a post\nhoc manner. In this work, we analyze two popular post hoc interpretation\ntechniques: SmoothGrad which is a gradient based method, and a variant of LIME\nwhich is a perturbation based method. More specifically, we derive explicit\nclosed form expressions for the explanations output by these two methods and\nshow that they both converge to the same explanation in expectation, i.e., when\nthe number of perturbed samples used by these methods is large. We then\nleverage this connection to establish other desirable properties, such as\nrobustness, for these techniques. We also derive finite sample complexity\nbounds for the number of perturbations required for these methods to converge\nto their expected explanation. Finally, we empirically validate our theory\nusing extensive experimentation on both synthetic and real world datasets.",
          "link": "http://arxiv.org/abs/2102.10618",
          "publishedOn": "2021-07-20T02:04:46.792Z",
          "wordCount": null,
          "title": "Towards the Unification and Robustness of Perturbation and Gradient Based Explanations. (arXiv:2102.10618v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feriani_A/0/1/0/all/0/1\">Amal Feriani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mezghani_A/0/1/0/all/0/1\">Amine Mezghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1\">Ekram Hossain</a>",
          "description": "We consider an Intelligent Reflecting Surface (IRS)-aided multiple-input\nsingle-output (MISO) system for downlink transmission. We compare the\nperformance of Deep Reinforcement Learning (DRL) and conventional optimization\nmethods in finding optimal phase shifts of the IRS elements to maximize the\nuser signal-to-noise (SNR) ratio. Furthermore, we evaluate the robustness of\nthese methods to channel impairments and changes in the system. We demonstrate\nnumerically that DRL solutions show more robustness to noisy channels and user\nmobility.",
          "link": "http://arxiv.org/abs/2107.08293",
          "publishedOn": "2021-07-20T02:04:46.791Z",
          "wordCount": null,
          "title": "On the Robustness of Deep Reinforcement Learning in IRS-Aided Wireless Communications Systems. (arXiv:2107.08293v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_K/0/1/0/all/0/1\">Karishma Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1\">Emilio Ferrara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>",
          "description": "Identifying and characterizing disinformation in political discourse on\nsocial media is critical to ensure the integrity of elections and democratic\nprocesses around the world. Persistent manipulation of social media has\nresulted in increased concerns regarding the 2020 U.S. Presidential Election,\ndue to its potential to influence individual opinions and social dynamics. In\nthis work, we focus on the identification of distorted facts, in the form of\nunreliable and conspiratorial narratives in election-related tweets, to\ncharacterize discourse manipulation prior to the election. We apply a detection\nmodel to separate factual from unreliable (or conspiratorial) claims analyzing\na dataset of 242 million election-related tweets. The identified claims are\nused to investigate targeted topics of disinformation, and conspiracy groups,\nmost notably the far-right QAnon conspiracy group. Further, we characterize\naccount engagements with unreliable and conspiracy tweets, and with the QAnon\nconspiracy group, by political leaning and tweet types. Finally, using a\nregression discontinuity design, we investigate whether Twitter's actions to\ncurb QAnon activity on the platform were effective, and how QAnon accounts\nadapt to Twitter's restrictions.",
          "link": "http://arxiv.org/abs/2107.08319",
          "publishedOn": "2021-07-20T02:04:46.791Z",
          "wordCount": null,
          "title": "Characterizing Online Engagement with Disinformation and Conspiracies in the 2020 U.S. Presidential Election. (arXiv:2107.08319v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2009.06342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paassen_B/0/1/0/all/0/1\">Benjamin Paa&#xdf;en</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_A/0/1/0/all/0/1\">Alexander Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stewart_T/0/1/0/all/0/1\">Terrence C. Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1\">Barbara Hammer</a>",
          "description": "Differentiable neural computers extend artificial neural networks with an\nexplicit memory without interference, thus enabling the model to perform\nclassic computation tasks such as graph traversal. However, such models are\ndifficult to train, requiring long training times and large datasets. In this\nwork, we achieve some of the computational capabilities of differentiable\nneural computers with a model that can be trained very efficiently, namely an\necho state network with an explicit memory without interference. This extension\nenables echo state networks to recognize all regular languages, including those\nthat contractive echo state networks provably can not recognize. Further, we\ndemonstrate experimentally that our model performs comparably to its\nfully-trained deep version on several typical benchmark tasks for\ndifferentiable neural computers.",
          "link": "http://arxiv.org/abs/2009.06342",
          "publishedOn": "2021-07-20T02:04:46.774Z",
          "wordCount": null,
          "title": "Reservoir Memory Machines as Neural Computers. (arXiv:2009.06342v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.06070",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laval_J/0/1/0/all/0/1\">Jorge Laval</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_A/0/1/0/all/0/1\">Anye Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Wenchao Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qing_Z/0/1/0/all/0/1\">Zhu Qing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peeta_S/0/1/0/all/0/1\">Srinivas Peeta</a>",
          "description": "Self-driving technology companies and the research community are accelerating\ntheir pace to use machine learning longitudinal motion planning (mMP) for\nautonomous vehicles (AVs). This paper reviews the current state of the art in\nmMP, with an exclusive focus on its impact on traffic congestion. We identify\nthe availability of congestion scenarios in current datasets, and summarize the\nrequired features for training mMP. For learning methods, we survey the major\nmethods in both imitation learning and non-imitation learning. We also\nhighlight the emerging technologies adopted by some leading AV companies, e.g.\nTesla, Waymo, and Comma.ai. We find that: i) the AV industry has been mostly\nfocusing on the long tail problem related to safety and overlooked the impact\non traffic congestion, ii) the current public self-driving datasets have not\nincluded enough congestion scenarios, and mostly lack the necessary input\nfeatures/output labels to train mMP, and iii) albeit reinforcement learning\n(RL) approach can integrate congestion mitigation into the learning goal, the\nmajor mMP method adopted by industry is still behavior cloning (BC), whose\ncapability to learn a congestion-mitigating mMP remains to be seen. Based on\nthe review, the study identifies the research gaps in current mMP development.\nSome suggestions towards congestion mitigation for future mMP studies are\nproposed: i) enrich data collection to facilitate the congestion learning, ii)\nincorporate non-imitation learning methods to combine traffic efficiency into a\nsafety-oriented technical route, and iii) integrate domain knowledge from the\ntraditional car following (CF) theory to improve the string stability of mMP.",
          "link": "http://arxiv.org/abs/1910.06070",
          "publishedOn": "2021-07-20T02:04:46.773Z",
          "wordCount": null,
          "title": "Review of Learning-based Longitudinal Motion Planning for Autonomous Vehicles: Research Gaps between Self-driving and Traffic Congestion. (arXiv:1910.06070v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shibata_K/0/1/0/all/0/1\">Katsunari Shibata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ejima_T/0/1/0/all/0/1\">Takuya Ejima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tokumaru_Y/0/1/0/all/0/1\">Yuki Tokumaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuki_T/0/1/0/all/0/1\">Toshitaka Matsuki</a>",
          "description": "Here, we introduce a fully local index named \"sensitivity\" for each neuron to\ncontrol chaoticity or gradient globally in a neural network (NN). We also\npropose a learning method to adjust it named \"sensitivity adjustment learning\n(SAL)\". The index is the gradient magnitude of its output with respect to its\ninputs. By adjusting its time average to 1.0 in each neuron, information\ntransmission in the neuron changes to be moderate without shrinking or\nexpanding for both forward and backward computations. That results in moderate\ninformation transmission through a layer of neurons when the weights and inputs\nare random. Therefore, SAL can control the chaoticity of the network dynamics\nin a recurrent NN (RNN). It can also solve the vanishing gradient problem in\nerror backpropagation (BP) learning in a deep feedforward NN or an RNN. We\ndemonstrate that when applying SAL to an RNN with small and random initial\nweights, log-sensitivity, which is the logarithm of RMS (root mean square)\nsensitivity over all the neurons, is equivalent to the maximum Lyapunov\nexponent until it reaches 0.0. We also show that SAL works with BP or BPTT (BP\nthrough time) to avoid the vanishing gradient problem in a 300-layer NN or an\nRNN that learns a problem with a lag of 300 steps between the first input and\nthe output. Compared with manually fine-tuning the spectral radius of the\nweight matrix before learning, SAL's continuous nonlinear learning nature\nprevents loss of sensitivities during learning, resulting in a significant\nimprovement in learning performance.",
          "link": "http://arxiv.org/abs/2012.13134",
          "publishedOn": "2021-07-20T02:04:46.773Z",
          "wordCount": null,
          "title": "Sensitivity -- Local Index to Control Chaoticity or Gradient Globally. (arXiv:2012.13134v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08865",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yunfan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1\">David Hsu</a>",
          "description": "This paper presents Particle-based Object Manipulation (Prompt), a new\napproach to robot manipulation of novel objects ab initio, without prior object\nmodels or pre-training on a large object data set. The key element of Prompt is\na particle-based object representation, in which each particle represents a\npoint in the object, the local geometric, physical, and other features of the\npoint, and also its relation with other particles. Like the model-based\nanalytic approaches to manipulation, the particle representation enables the\nrobot to reason about the object's geometry and dynamics in order to choose\nsuitable manipulation actions. Like the data-driven approaches, the particle\nrepresentation is learned online in real-time from visual sensor input,\nspecifically, multi-view RGB images. The particle representation thus connects\nvisual perception with robot control. Prompt combines the benefits of both\nmodel-based reasoning and data-driven learning. We show empirically that Prompt\nsuccessfully handles a variety of everyday objects, some of which are\ntransparent. It handles various manipulation tasks, including grasping,\npushing, etc,. Our experiments also show that Prompt outperforms a\nstate-of-the-art data-driven grasping method on the daily objects, even though\nit does not use any offline training data.",
          "link": "http://arxiv.org/abs/2107.08865",
          "publishedOn": "2021-07-20T02:04:46.772Z",
          "wordCount": null,
          "title": "Ab Initio Particle-based Object Manipulation. (arXiv:2107.08865v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_D/0/1/0/all/0/1\">Divya Shanmugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabanian_S/0/1/0/all/0/1\">Samira Shabanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finck_M/0/1/0/all/0/1\">Mich&#xe8;le Finck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biega_A/0/1/0/all/0/1\">Asia Biega</a>",
          "description": "Data minimization is a legal obligation defined in the European Union's\nGeneral Data Protection Regulation (GDPR) as the responsibility to process an\nadequate, relevant, and limited amount of personal data in relation to a\nprocessing purpose. However, unlike fairness or transparency, the principle has\nnot seen wide adoption for machine learning systems due to a lack of\ncomputational interpretation. In this paper, we build on literature in machine\nlearning and law to propose the first learning framework for limiting data\ncollection based on an interpretation that ties the data collection purpose to\nsystem performance. We formalize a data minimization criterion based on\nperformance curve derivatives and provide an effective and interpretable\npiecewise power law technique that models distinct stages of an algorithm's\nperformance throughout data collection. Results from our empirical\ninvestigation offer deeper insights into the relevant considerations when\ndesigning a data minimization framework, including the choice of feature\nacquisition algorithm, initialization conditions, as well as impacts on\nindividuals that hint at tensions between data minimization and fairness.",
          "link": "http://arxiv.org/abs/2107.08096",
          "publishedOn": "2021-07-20T02:04:46.699Z",
          "wordCount": null,
          "title": "Learning to Limit Data Collection via Scaling Laws: Data Minimization Compliance in Practice. (arXiv:2107.08096v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elflein_S/0/1/0/all/0/1\">Sven Elflein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charpentier_B/0/1/0/all/0/1\">Bertrand Charpentier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zugner_D/0/1/0/all/0/1\">Daniel Z&#xfc;gner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>",
          "description": "Several density estimation methods have shown to fail to detect\nout-of-distribution (OOD) samples by assigning higher likelihoods to anomalous\ndata. Energy-based models (EBMs) are flexible, unnormalized density models\nwhich seem to be able to improve upon this failure mode. In this work, we\nprovide an extensive study investigating OOD detection with EBMs trained with\ndifferent approaches on tabular and image data and find that EBMs do not\nprovide consistent advantages. We hypothesize that EBMs do not learn semantic\nfeatures despite their discriminative structure similar to Normalizing Flows.\nTo verify this hypotheses, we show that supervision and architectural\nrestrictions improve the OOD detection of EBMs independent of the training\napproach.",
          "link": "http://arxiv.org/abs/2107.08785",
          "publishedOn": "2021-07-20T02:04:46.698Z",
          "wordCount": null,
          "title": "On Out-of-distribution Detection with Energy-based Models. (arXiv:2107.08785v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chepurko_N/0/1/0/all/0/1\">Nadiia Chepurko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarkson_K/0/1/0/all/0/1\">Kenneth L. Clarkson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kacham_P/0/1/0/all/0/1\">Praneeth Kacham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1\">David P. Woodruff</a>",
          "description": "Currently, in the numerical linear algebra community, it is thought that to\nobtain nearly-optimal bounds for various problems such as rank computation,\nfinding a maximal linearly independent subset of columns, regression, low rank\napproximation, maximum matching on general graphs and linear matroid union, one\nwould need to resolve the main open question of Nelson and Nguyen (FOCS, 2013)\nregarding the logarithmic factors in the sketching dimension for existing\nconstant factor approximation oblivious subspace embeddings. We show how to\nbypass this question using a refined sketching technique, and obtain optimal or\nnearly optimal bounds for these problems. A key technique we use is an explicit\nmapping of Indyk based on uncertainty principles and extractors, which after\nfirst applying known oblivious subspace embeddings, allows us to quickly spread\nout the mass of the vector so that sampling is now effective, and we avoid a\nlogarithmic factor that is standard in the sketching dimension resulting from\nmatrix Chernoff bounds. For the fundamental problems of rank computation and\nfinding a linearly independent subset of columns, our algorithms improve\nCheung, Kwok, and Lau (JACM, 2013) and are optimal to within a constant factor\nand a $\\log\\log(n)$-factor, respectively. Further, for constant factor\nregression and low rank approximation we give the first optimal algorithms, for\nthe current matrix multiplication exponent.",
          "link": "http://arxiv.org/abs/2107.08090",
          "publishedOn": "2021-07-20T02:04:46.697Z",
          "wordCount": null,
          "title": "Near-Optimal Algorithms for Linear Algebra in the Current Matrix Multiplication Time. (arXiv:2107.08090v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">JaeYoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_J/0/1/0/all/0/1\">Junyu Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Christy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_F/0/1/0/all/0/1\">Farookh Hussain</a>",
          "description": "The high-dimensional or sparse reward task of a reinforcement learning (RL)\nenvironment requires a superior potential controller such as hierarchical\nreinforcement learning (HRL) rather than an atomic RL because it absorbs the\ncomplexity of commands to achieve the purpose of the task in its hierarchical\nstructure. One of the HRL issues is how to train each level policy with the\noptimal data collection from its experience. That is to say, how to synchronize\nadjacent level policies optimally. Our research finds that a HRL model through\nthe off-policy correction technique of HRL, which trains a higher-level policy\nwith the goal of reflecting a lower-level policy which is newly trained using\nthe off-policy method, takes the critical role of synchronizing both level\npolicies at all times while they are being trained. We propose a novel HRL\nmodel supporting the optimal level synchronization using the off-policy\ncorrection technique with a deep generative model. This uses the advantage of\nthe inverse operation of a flow-based deep generative model (FDGM) to achieve\nthe goal corresponding to the current state of the lower-level policy. The\nproposed model also considers the freedom of the goal dimension between HRL\npolicies which makes it the generalized inverse model of the model-free RL in\nHRL with the optimal synchronization method. The comparative experiment results\nshow the performance of our proposed model.",
          "link": "http://arxiv.org/abs/2107.08183",
          "publishedOn": "2021-07-20T02:04:46.690Z",
          "wordCount": null,
          "title": "Hierarchical Reinforcement Learning with Optimal Level Synchronization based on a Deep Generative Model. (arXiv:2107.08183v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lui_K/0/1/0/all/0/1\">Kry Yik Chau Lui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Leal_P/0/1/0/all/0/1\">Pablo Hernandez-Leal</a>",
          "description": "Trading markets represent a real-world financial application to deploy\nreinforcement learning agents, however, they carry hard fundamental challenges\nsuch as high variance and costly exploration. Moreover, markets are inherently\na multiagent domain composed of many actors taking actions and changing the\nenvironment. To tackle these type of scenarios agents need to exhibit certain\ncharacteristics such as risk-awareness, robustness to perturbations and low\nlearning variance. We take those as building blocks and propose a family of\nfour algorithms. First, we contribute with two algorithms that use risk-averse\nobjective functions and variance reduction techniques. Then, we augment the\nframework to multi-agent learning and assume an adversary which can take over\nand perturb the learning process. Our third and fourth algorithms perform well\nunder this setting and balance theoretical guarantees with practical use.\nAdditionally, we consider the multi-agent nature of the environment and our\nwork is the first one extending empirical game theory analysis for multi-agent\nlearning by considering risk-sensitive payoffs.",
          "link": "http://arxiv.org/abs/2107.08083",
          "publishedOn": "2021-07-20T02:04:46.689Z",
          "wordCount": null,
          "title": "Robust Risk-Sensitive Reinforcement Learning Agents for Trading Markets. (arXiv:2107.08083v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1810.03024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheung_W/0/1/0/all/0/1\">Wang Chi Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simchi_Levi_D/0/1/0/all/0/1\">David Simchi-Levi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Ruihao Zhu</a>",
          "description": "We introduce algorithms that achieve state-of-the-art \\emph{dynamic regret}\nbounds for non-stationary linear stochastic bandit setting. It captures natural\napplications such as dynamic pricing and ads allocation in a changing\nenvironment. We show how the difficulty posed by the non-stationarity can be\novercome by a novel marriage between stochastic and adversarial bandits\nlearning algorithms. Defining $d,B_T,$ and $T$ as the problem dimension, the\n\\emph{variation budget}, and the total time horizon, respectively, our main\ncontributions are the tuned Sliding Window UCB (\\texttt{SW-UCB}) algorithm with\noptimal $\\widetilde{O}(d^{2/3}(B_T+1)^{1/3}T^{2/3})$ dynamic regret, and the\ntuning free bandit-over-bandit (\\texttt{BOB}) framework built on top of the\n\\texttt{SW-UCB} algorithm with best\n$\\widetilde{O}(d^{2/3}(B_T+1)^{1/4}T^{3/4})$ dynamic regret.",
          "link": "http://arxiv.org/abs/1810.03024",
          "publishedOn": "2021-07-20T02:04:46.689Z",
          "wordCount": null,
          "title": "Learning to Optimize under Non-Stationarity. (arXiv:1810.03024v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1\">Anderson da Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "This works proposes a methodology to searching for automatically Artificial\nNeural Networks (ANN) by using Cellular Genetic Algorithm (CGA). The goal of\nthis methodology is to find compact networks whit good performance for\nclassification problems. The main reason for developing this work is centered\nat the difficulties of configuring compact ANNs with good performance rating.\nThe use of CGAs aims at seeking the components of the RNA in the same way that\na common Genetic Algorithm (GA), but it has the differential of incorporating a\nCellular Automaton (CA) to give location for the GA individuals. The location\nimposed by the CA aims to control the spread of solutions in the populations to\nmaintain the genetic diversity for longer time. This genetic diversity is\nimportant for obtain good results with the GAs.",
          "link": "http://arxiv.org/abs/2107.08326",
          "publishedOn": "2021-07-20T02:04:46.687Z",
          "wordCount": null,
          "title": "Otimizacao de Redes Neurais atraves de Algoritmos Geneticos Celulares. (arXiv:2107.08326v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peiris_V/0/1/0/all/0/1\">Vinesha Peiris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhorukova_N/0/1/0/all/0/1\">Nadezda Sukhorukova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roshchina_V/0/1/0/all/0/1\">Vera Roshchina</a>",
          "description": "We explore the potential for using a nonsmooth loss function based on the\nmax-norm in the training of an artificial neural network. We hypothesise that\nthis may lead to superior classification results in some special cases where\nthe training data is either very small or unbalanced.\n\nOur numerical experiments performed on a simple artificial neural network\nwith no hidden layers (a setting immediately amenable to standard nonsmooth\noptimisation techniques) appear to confirm our hypothesis that uniform\napproximation based approaches may be more suitable for the datasets with\nreliable training data that either is limited size or biased in terms of\nrelative cluster sizes.",
          "link": "http://arxiv.org/abs/2107.08800",
          "publishedOn": "2021-07-20T02:04:46.686Z",
          "wordCount": null,
          "title": "Deep Learning with Nonsmooth Objectives. (arXiv:2107.08800v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1\">Rong Pan</a>",
          "description": "Recurrence data arise from multi-disciplinary domains spanning reliability,\ncyber security, healthcare, online retailing, etc. This paper investigates an\nadditive-tree-based approach, known as Boost-R (Boosting for Recurrence Data),\nfor recurrent event data with both static and dynamic features. Boost-R\nconstructs an ensemble of gradient boosted additive trees to estimate the\ncumulative intensity function of the recurrent event process, where a new tree\nis added to the ensemble by minimizing the regularized L2 distance between the\nobserved and predicted cumulative intensity. Unlike conventional regression\ntrees, a time-dependent function is constructed by Boost-R on each tree leaf.\nThe sum of these functions, from multiple trees, yields the ensemble estimator\nof the cumulative intensity. The divide-and-conquer nature of tree-based\nmethods is appealing when hidden sub-populations exist within a heterogeneous\npopulation. The non-parametric nature of regression trees helps to avoid\nparametric assumptions on the complex interactions between event processes and\nfeatures. Critical insights and advantages of Boost-R are investigated through\ncomprehensive numerical examples. Datasets and computer code of Boost-R are\nmade available on GitHub. To our best knowledge, Boost-R is the first gradient\nboosted additive-tree-based approach for modeling large-scale recurrent event\ndata with both static and dynamic feature information.",
          "link": "http://arxiv.org/abs/2107.08784",
          "publishedOn": "2021-07-20T02:04:46.685Z",
          "wordCount": null,
          "title": "Boost-R: Gradient Boosted Trees for Recurrence Data. (arXiv:2107.08784v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Navarro_C/0/1/0/all/0/1\">Carlos Mougan Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanellos_G/0/1/0/all/0/1\">Georgios Kanellos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottron_T/0/1/0/all/0/1\">Thomas Gottron</a>",
          "description": "Explainable AI constitutes a fundamental step towards establishing fairness\nand addressing bias in algorithmic decision-making. Despite the large body of\nwork on the topic, the benefit of solutions is mostly evaluated from a\nconceptual or theoretical point of view and the usefulness for real-world use\ncases remains uncertain. In this work, we aim to state clear user-centric\ndesiderata for explainable AI reflecting common explainability needs\nexperienced in statistical production systems of the European Central Bank. We\nlink the desiderata to archetypical user roles and give examples of techniques\nand methods which can be used to address the user's needs. To this end, we\nprovide two concrete use cases from the domain of statistical data production\nin central banks: the detection of outliers in the Centralised Securities\nDatabase and the data-driven identification of data quality checks for the\nSupervisory Banking data system.",
          "link": "http://arxiv.org/abs/2107.08045",
          "publishedOn": "2021-07-20T02:04:46.586Z",
          "wordCount": null,
          "title": "Desiderata for Explainable AI in statistical production systems of the European Central Bank. (arXiv:2107.08045v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanchao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feriani_A/0/1/0/all/0/1\">Amal Feriani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1\">Ekram Hossain</a>",
          "description": "Multi-Agent Reinforcement Learning (MARL) is a challenging subarea of\nReinforcement Learning due to the non-stationarity of the environments and the\nlarge dimensionality of the combined action space. Deep MARL algorithms have\nbeen applied to solve different task offloading problems. However, in\nreal-world applications, information required by the agents (i.e. rewards and\nstates) are subject to noise and alterations. The stability and the robustness\nof deep MARL to practical challenges is still an open research problem. In this\nwork, we apply state-of-the art MARL algorithms to solve task offloading with\nreward uncertainty. We show that perturbations in the reward signal can induce\ndecrease in the performance compared to learning with perfect rewards. We\nexpect this paper to stimulate more research in studying and addressing the\npractical challenges of deploying deep MARL solutions in wireless\ncommunications systems.",
          "link": "http://arxiv.org/abs/2107.08114",
          "publishedOn": "2021-07-20T02:04:46.584Z",
          "wordCount": null,
          "title": "Decentralized Multi-Agent Reinforcement Learning for Task Offloading Under Uncertainty. (arXiv:2107.08114v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guevara_J/0/1/0/all/0/1\">Jorge Guevara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borges_D/0/1/0/all/0/1\">Dario Borges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watson_C/0/1/0/all/0/1\">Campbell Watson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadrozny_B/0/1/0/all/0/1\">Bianca Zadrozny</a>",
          "description": "Future climate change scenarios are usually hypothesized using simulations\nfrom weather generators. However, there only a few works comparing and\nevaluating promising deep learning models for weather generation against\nclassical approaches. This study shows preliminary results making such\nevaluations for the multisite precipitation synthesis task. We compared two\nopen-source weather generators: IBMWeathergen (an extension of the Weathergen\nlibrary) and RGeneratePrec, and two deep generative models: GAN and VAE, on a\nvariety of metrics. Our preliminary results can serve as a guide for improving\nthe design of deep learning architectures and algorithms for the multisite\nprecipitation synthesis task.",
          "link": "http://arxiv.org/abs/2107.08074",
          "publishedOn": "2021-07-20T02:04:46.581Z",
          "wordCount": null,
          "title": "A comparative study of stochastic and deep generative models for multisite precipitation synthesis. (arXiv:2107.08074v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bartz_E/0/1/0/all/0/1\">Eva Bartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaefferer_M/0/1/0/all/0/1\">Martin Zaefferer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mersmann_O/0/1/0/all/0/1\">Olaf Mersmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartz_Beielstein_T/0/1/0/all/0/1\">Thomas Bartz-Beielstein</a>",
          "description": "Machine learning algorithms such as random forests or xgboost are gaining\nmore importance and are increasingly incorporated into production processes in\norder to enable comprehensive digitization and, if possible, automation of\nprocesses. Hyperparameters of these algorithms used have to be set\nappropriately, which can be referred to as hyperparameter tuning or\noptimization. Based on the concept of tunability, this article presents an\noverview of theoretical and practical results for popular machine learning\nalgorithms. This overview is accompanied by an experimental analysis of 30\nhyperparameters from six relevant machine learning algorithms. In particular,\nit provides (i) a survey of important hyperparameters, (ii) two parameter\ntuning studies, and (iii) one extensive global parameter tuning study, as well\nas (iv) a new way, based on consensus ranking, to analyze results from multiple\nalgorithms. The R package mlr is used as a uniform interface to the machine\nlearning models. The R package SPOT is used to perform the actual tuning\n(optimization). All additional code is provided together with this paper.",
          "link": "http://arxiv.org/abs/2107.08761",
          "publishedOn": "2021-07-20T02:04:46.575Z",
          "wordCount": null,
          "title": "Experimental Investigation and Evaluation of Model-based Hyperparameter Optimization. (arXiv:2107.08761v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08751",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Memmel_M/0/1/0/all/0/1\">Marius Memmel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1\">Camila Gonzalez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Anirban Mukhopadhyay</a>",
          "description": "Deep learning for medical imaging suffers from temporal and privacy-related\nrestrictions on data availability. To still obtain viable models, continual\nlearning aims to train in sequential order, as and when data is available. The\nmain challenge that continual learning methods face is to prevent catastrophic\nforgetting, i.e., a decrease in performance on the data encountered earlier.\nThis issue makes continuous training of segmentation models for medical\napplications extremely difficult. Yet, often, data from at least two different\ndomains is available which we can exploit to train the model in a way that it\ndisregards domain-specific information. We propose an architecture that\nleverages the simultaneous availability of two or more datasets to learn a\ndisentanglement between the content and domain in an adversarial fashion. The\ndomain-invariant content representation then lays the base for continual\nsemantic segmentation. Our approach takes inspiration from domain adaptation\nand combines it with continual learning for hippocampal segmentation in brain\nMRI. We showcase that our method reduces catastrophic forgetting and\noutperforms state-of-the-art continual learning methods.",
          "link": "http://arxiv.org/abs/2107.08751",
          "publishedOn": "2021-07-20T02:04:46.574Z",
          "wordCount": null,
          "title": "Adversarial Continual Learning for Multi-Domain Hippocampal Segmentation. (arXiv:2107.08751v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Soumyadeep Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjay Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_J/0/1/0/all/0/1\">Janu Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Awanish Kumar</a>",
          "description": "In order to train robust deep learning models, large amounts of labelled data\nis required. However, in the absence of such large repositories of labelled\ndata, unlabeled data can be exploited for the same. Semi-Supervised learning\naims to utilize such unlabeled data for training classification models. Recent\nprogress of self-training based approaches have shown promise in this area,\nwhich leads to this study where we utilize an ensemble approach for the same. A\nby-product of any semi-supervised approach may be loss of calibration of the\ntrained model especially in scenarios where unlabeled data may contain\nout-of-distribution samples, which leads to this investigation on how to adapt\nto such effects. Our proposed algorithm carefully avoids common pitfalls in\nutilizing unlabeled data and leads to a more accurate and calibrated supervised\nmodel compared to vanilla self-training based student-teacher algorithms. We\nperform several experiments on the popular STL-10 database followed by an\nextensive analysis of our approach and study its effects on model accuracy and\ncalibration.",
          "link": "http://arxiv.org/abs/2107.08211",
          "publishedOn": "2021-07-20T02:04:46.557Z",
          "wordCount": null,
          "title": "Self Training with Ensemble of Teacher Models. (arXiv:2107.08211v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yao Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Haipeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Despite being a critical communication skill, grasping humor is challenging\n-- a successful use of humor requires a mixture of both engaging content\nbuild-up and an appropriate vocal delivery (e.g., pause). Prior studies on\ncomputational humor emphasize the textual and audio features immediately next\nto the punchline, yet overlooking longer-term context setup. Moreover, the\ntheories are usually too abstract for understanding each concrete humor\nsnippet. To fill in the gap, we develop DeHumor, a visual analytical system for\nanalyzing humorous behaviors in public speaking. To intuitively reveal the\nbuilding blocks of each concrete example, DeHumor decomposes each humorous\nvideo into multimodal features and provides inline annotations of them on the\nvideo script. In particular, to better capture the build-ups, we introduce\ncontent repetition as a complement to features introduced in theories of\ncomputational humor and visualize them in a context linking graph. To help\nusers locate the punchlines that have the desired features to learn, we\nsummarize the content (with keywords) and humor feature statistics on an\naugmented time matrix. With case studies on stand-up comedy shows and TED\ntalks, we show that DeHumor is able to highlight various building blocks of\nhumor examples. In addition, expert interviews with communication coaches and\nhumor researchers demonstrate the effectiveness of DeHumor for multimodal humor\nanalysis of speech content and vocal delivery.",
          "link": "http://arxiv.org/abs/2107.08356",
          "publishedOn": "2021-07-20T02:04:46.339Z",
          "wordCount": null,
          "title": "DeHumor: Visual Analytics for Decomposing Humor. (arXiv:2107.08356v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.05094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murray_M/0/1/0/all/0/1\">Michael Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanner_J/0/1/0/all/0/1\">Jared Tanner</a>",
          "description": "In its most elementary form, compressed sensing studies the design of\ndecoding algorithms to recover a sufficiently sparse vector or code from a\nlower dimensional linear measurement vector. Typically it is assumed that the\ndecoder has access to the encoder matrix, which in the combinatorial case is\nsparse and binary. In this paper we consider the problem of designing a decoder\nto recover a set of sparse codes from their linear measurements alone, that is\nwithout access to encoder matrix. To this end we study the matrix factorisation\ntask of recovering both the encoder and sparse coding matrices from the\nassociated linear measurement matrix. The contribution of this paper is a\ncomputationally efficient decoding algorithm, Decoder-Expander Based\nFactorisation, with strong performance guarantees. In particular, under mild\nassumptions on the sparse coding matrix and by deploying a novel random encoder\nmatrix, we prove that Decoder-Expander Based Factorisation recovers both the\nencoder and sparse coding matrix at the optimal measurement rate with high\nprobability and from a near optimal number of measurement vectors. In addition,\nour experiments demonstrate the efficacy and computational efficiency of our\nalgorithm in practice. Beyond compressed sensing our results may be of interest\nfor researchers working in areas such as linear sketching, coding theory and\nmatrix compression.",
          "link": "http://arxiv.org/abs/2004.05094",
          "publishedOn": "2021-07-20T02:04:46.339Z",
          "wordCount": null,
          "title": "Encoder blind combinatorial compressed sensing. (arXiv:2004.05094v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_C/0/1/0/all/0/1\">Chirag Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramdas_A/0/1/0/all/0/1\">Aaditya K. Ramdas</a>",
          "description": "We study the problem of post-hoc calibration for multiclass classification,\nwith an emphasis on histogram binning. Multiple works have focused on\ncalibration with respect to the confidence of just the predicted class (or\n'top-label'). We find that the popular notion of confidence calibration [Guo et\nal., 2017] is not sufficiently strong -- there exist predictors that are not\ncalibrated in any meaningful way but are perfectly confidence calibrated. We\npropose a closely related (but subtly different) notion, top-label calibration,\nthat accurately captures the intuition and simplicity of confidence\ncalibration, but addresses its drawbacks. We formalize a histogram binning (HB)\nalgorithm that reduces top-label multiclass calibration to the binary case,\nprove that it has clean theoretical guarantees without distributional\nassumptions, and perform a methodical study of its practical performance. Some\nprediction tasks require stricter notions of multiclass calibration such as\nclass-wise or canonical calibration. We formalize appropriate HB algorithms\ncorresponding to each of these goals. In experiments with deep neural nets, we\nfind that our principled versions of HB are often better than temperature\nscaling, for both top-label and class-wise calibration. Code for this work will\nbe made publicly available at https://github.com/aigen/df-posthoc-calibration.",
          "link": "http://arxiv.org/abs/2107.08353",
          "publishedOn": "2021-07-20T02:04:46.338Z",
          "wordCount": null,
          "title": "Top-label calibration. (arXiv:2107.08353v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.11830",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Richter_L/0/1/0/all/0/1\">Lorenz Richter</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sallandt_L/0/1/0/all/0/1\">Leon Sallandt</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nusken_N/0/1/0/all/0/1\">Nikolas N&#xfc;sken</a>",
          "description": "High-dimensional partial differential equations (PDEs) are ubiquitous in\neconomics, science and engineering. However, their numerical treatment poses\nformidable challenges since traditional grid-based methods tend to be\nfrustrated by the curse of dimensionality. In this paper, we argue that tensor\ntrains provide an appealing approximation framework for parabolic PDEs: the\ncombination of reformulations in terms of backward stochastic differential\nequations and regression-type methods in the tensor format holds the promise of\nleveraging latent low-rank structures enabling both compression and efficient\ncomputation. Following this paradigm, we develop novel iterative schemes,\ninvolving either explicit and fast or implicit and accurate updates. We\ndemonstrate in a number of examples that our methods achieve a favorable\ntrade-off between accuracy and computational efficiency in comparison with\nstate-of-the-art neural network based approaches.",
          "link": "http://arxiv.org/abs/2102.11830",
          "publishedOn": "2021-07-20T02:04:46.338Z",
          "wordCount": null,
          "title": "Solving high-dimensional parabolic PDEs using the tensor train format. (arXiv:2102.11830v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12056",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1\">Changfa Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1\">Min Xian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xiancheng Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haotian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_H/0/1/0/all/0/1\">Heng-Da Cheng</a>",
          "description": "Liver segmentation from abdominal CT images is an essential step for liver\ncancer computer-aided diagnosis and surgical planning. However, both the\naccuracy and robustness of existing liver segmentation methods cannot meet the\nrequirements of clinical applications. In particular, for the common clinical\ncases where the liver tissue contains major pathology, current segmentation\nmethods show poor performance. In this paper, we propose a novel low-rank\ntensor decomposition (LRTD) based multi-atlas segmentation (MAS) framework that\nachieves accurate and robust pathological liver segmentation of CT images.\nFirstly, we propose a multi-slice LRTD scheme to recover the underlying\nlow-rank structure embedded in 3D medical images. It performs the LRTD on small\nimage segments consisting of multiple consecutive image slices. Then, we\npresent an LRTD-based atlas construction method to generate tumor-free liver\natlases that mitigates the performance degradation of liver segmentation due to\nthe presence of tumors. Finally, we introduce an LRTD-based MAS algorithm to\nderive patient-specific liver atlases for each test image, and to achieve\naccurate pairwise image registration and label propagation. Extensive\nexperiments on three public databases of pathological liver cases validate the\neffectiveness of the proposed method. Both qualitative and quantitative results\ndemonstrate that, in the presence of major pathology, the proposed method is\nmore accurate and robust than state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2102.12056",
          "publishedOn": "2021-07-20T02:04:46.337Z",
          "wordCount": null,
          "title": "Multi-Slice Low-Rank Tensor Decomposition Based Multi-Atlas Segmentation: Application to Automatic Pathological Liver CT Segmentation. (arXiv:2102.12056v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blanzeisky_W/0/1/0/all/0/1\">William Blanzeisky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cunningham_P/0/1/0/all/0/1\">P&#xe1;draig Cunningham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_K/0/1/0/all/0/1\">Kenneth Kennedy</a>",
          "description": "A significant impediment to progress in research on bias in machine learning\n(ML) is the availability of relevant datasets. This situation is unlikely to\nchange much given the sensitivity of such data. For this reason, there is a\nrole for synthetic data in this research. In this short paper, we present one\nsuch family of synthetic data sets. We provide an overview of the data,\ndescribe how the level of bias can be varied, and present a simple example of\nan experiment on the data.",
          "link": "http://arxiv.org/abs/2107.08928",
          "publishedOn": "2021-07-20T02:04:46.336Z",
          "wordCount": null,
          "title": "Introducing a Family of Synthetic Datasets for Research on Bias in Machine Learning. (arXiv:2107.08928v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08273",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hengguan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ye Wang</a>",
          "description": "Perception of time from sequentially acquired sensory inputs is rooted in\neveryday behaviors of individual organisms. Yet, most algorithms for\ntime-series modeling fail to learn dynamics of random event timings directly\nfrom visual or audio inputs, requiring timing annotations during training that\nare usually unavailable for real-world applications. For instance, neuroscience\nperspectives on postdiction imply that there exist variable temporal ranges\nwithin which the incoming sensory inputs can affect the earlier perception, but\nsuch temporal ranges are mostly unannotated for real applications such as\nautomatic speech recognition (ASR). In this paper, we present a probabilistic\nordinary differential equation (ODE), called STochastic boundaRy ODE (STRODE),\nthat learns both the timings and the dynamics of time series data without\nrequiring any timing annotations during training. STRODE allows the usage of\ndifferential equations to sample from the posterior point processes,\nefficiently and analytically. We further provide theoretical guarantees on the\nlearning of STRODE. Our empirical results show that our approach successfully\ninfers event timings of time series data. Our method achieves competitive or\nsuperior performances compared to existing state-of-the-art methods for both\nsynthetic and real-world datasets.",
          "link": "http://arxiv.org/abs/2107.08273",
          "publishedOn": "2021-07-20T02:04:46.247Z",
          "wordCount": null,
          "title": "STRODE: Stochastic Boundary Ordinary Differential Equation. (arXiv:2107.08273v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.",
          "link": "http://arxiv.org/abs/2107.08264",
          "publishedOn": "2021-07-20T02:04:46.210Z",
          "wordCount": null,
          "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhe Yu</a>",
          "description": "This paper aims to improve machine learning fairness on multiple protected\nat-tributes. Machine learning fairness has attracted increasing attention since\nmachine learning models are increasingly used for high-stakes and high-risk\ndecisions. Most existing solutions for machine learning fairness only target\none protected attribute(e.g. sex) at a time. These solutions cannot generate a\nmachine learning model which is fair against every protected attribute (e.g.\nboth sex and race) at the same time. To solve this problem, we propose\nFairBalance in this paper to balance the distribution of training data across\nevery protected attribute before training the machine learning models. Our\nresults show that, under the assumption of unbiased ground truth labels,\nFairBalance can significantly reduce bias metrics (AOD, EOD, and SPD) on every\nknown protected attribute without much, if not any damage to the prediction\nperformance.",
          "link": "http://arxiv.org/abs/2107.08310",
          "publishedOn": "2021-07-20T02:04:46.210Z",
          "wordCount": null,
          "title": "Fair Balance: Mitigating Machine Learning Bias Against Multiple Protected Attributes With Data Balancing. (arXiv:2107.08310v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huaimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haibo Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy M. Hospedales</a>",
          "description": "Federated learning (FL) enables distributed participants to collectively\nlearn a strong global model without sacrificing their individual data privacy.\nMainstream FL approaches require each participant to share a common network\narchitecture and further assume that data are are sampled IID across\nparticipants. However, in real-world deployments participants may require\nheterogeneous network architectures; and the data distribution is almost\ncertainly non-uniform across participants. To address these issues we introduce\nFedH2L, which is agnostic to both the model architecture and robust to\ndifferent data distributions across participants. In contrast to approaches\nsharing parameters or gradients, FedH2L relies on mutual distillation,\nexchanging only posteriors on a shared seed set between participants in a\ndecentralized manner. This makes it extremely bandwidth efficient, model\nagnostic, and crucially produces models capable of performing well on the whole\ndata distribution when learning from heterogeneous silos.",
          "link": "http://arxiv.org/abs/2101.11296",
          "publishedOn": "2021-07-20T02:04:46.209Z",
          "wordCount": null,
          "title": "FedH2L: Federated Learning with Model and Statistical Heterogeneity. (arXiv:2101.11296v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1\">Masahiro Kato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teshima_T/0/1/0/all/0/1\">Takeshi Teshima</a>",
          "description": "Density ratio estimation (DRE) is at the core of various machine learning\ntasks such as anomaly detection and domain adaptation. In existing studies on\nDRE, methods based on Bregman divergence (BD) minimization have been\nextensively studied. However, BD minimization when applied with highly flexible\nmodels, such as deep neural networks, tends to suffer from what we call\ntrain-loss hacking, which is a source of overfitting caused by a typical\ncharacteristic of empirical BD estimators. In this paper, to mitigate\ntrain-loss hacking, we propose a non-negative correction for empirical BD\nestimators. Theoretically, we confirm the soundness of the proposed method\nthrough a generalization error bound. Through our experiments, the proposed\nmethods show a favorable performance in inlier-based outlier detection.",
          "link": "http://arxiv.org/abs/2006.06979",
          "publishedOn": "2021-07-20T02:04:46.206Z",
          "wordCount": null,
          "title": "Non-Negative Bregman Divergence Minimization for Deep Direct Density Ratio Estimation. (arXiv:2006.06979v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.02443",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pomponi_J/0/1/0/all/0/1\">Jary Pomponi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scardapane_S/0/1/0/all/0/1\">Simone Scardapane</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Uncini_A/0/1/0/all/0/1\">Aurelio Uncini</a>",
          "description": "Catastrophic forgetting (CF) happens whenever a neural network overwrites\npast knowledge while being trained on new tasks. Common techniques to handle CF\ninclude regularization of the weights (using, e.g., their importance on past\ntasks), and rehearsal strategies, where the network is constantly re-trained on\npast data. Generative models have also been applied for the latter, in order to\nhave endless sources of data. In this paper, we propose a novel method that\ncombines the strengths of regularization and generative-based rehearsal\napproaches. Our generative model consists of a normalizing flow (NF), a\nprobabilistic and invertible neural network, trained on the internal embeddings\nof the network. By keeping a single NF conditioned on the task, we show that\nour memory overhead remains constant. In addition, exploiting the invertibility\nof the NF, we propose a simple approach to regularize the network's embeddings\nwith respect to past tasks. We show that our method performs favorably with\nrespect to state-of-the-art approaches in the literature, with bounded\ncomputational power and memory overheads.",
          "link": "http://arxiv.org/abs/2007.02443",
          "publishedOn": "2021-07-20T02:04:46.205Z",
          "wordCount": null,
          "title": "Pseudo-Rehearsal for Continual Learning with Normalizing Flows. (arXiv:2007.02443v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08209",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tasche_D/0/1/0/all/0/1\">Dirk Tasche</a>",
          "description": "For the binary prevalence quantification problem under prior probability\nshift, we determine the asymptotic variance of the maximum likelihood\nestimator. We find that it is a function of the Brier score for the regression\nof the class label against the features under the test data set distribution.\nThis observation suggests that optimising the accuracy of a base classifier on\nthe training data set helps to reduce the variance of the related quantifier on\nthe test data set. Therefore, we also point out training criteria for the base\nclassifier that imply optimisation of both of the Brier scores on the training\nand the test data sets.",
          "link": "http://arxiv.org/abs/2107.08209",
          "publishedOn": "2021-07-20T02:04:46.127Z",
          "wordCount": null,
          "title": "Minimising quantifier variance under prior probability shift. (arXiv:2107.08209v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03088",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xianghao Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rumeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1\">Xiaoqing Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1\">Mert Pilanci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">You Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhiyuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guang Li</a>",
          "description": "Electronic nose has been proven to be effective in alternative herbal\nmedicine classification, but due to the nature of supervised learning, previous\nresearch heavily relies on the labelled training data, which are time-costly\nand labor-intensive to collect. To alleviate the critical dependency on the\ntraining data in real-world applications, this study aims to improve\nclassification accuracy via data augmentation strategies. The effectiveness of\nfive data augmentation strategies under different training data inadequacy are\ninvestigated in two scenarios: the noise-free scenario where different\navailabilities of unlabelled data were considered, and the noisy scenario where\ndifferent levels of Gaussian noises and translational shifts were added to\nrepresent sensor drifts. The five augmentation strategies, namely noise-adding\ndata augmentation, semi-supervised learning, classifier-based online learning,\nInductive Conformal Prediction (ICP) online learning and our novel ensemble ICP\nonline learning proposed in this study, are experimented and compared against\nsupervised learning baseline, with Linear Discriminant Analysis (LDA) and\nSupport Vector Machine (SVM) as the classifiers. Our novel strategy, ensemble\nICP online learning, outperforms the others by showing non-decreasing\nclassification accuracy on all tasks and a significant improvement on most\nsimulated tasks (25out of 36 tasks,p<=0.05). Furthermore, this study provides a\nsystematic analysis of different augmentation strategies. It shows at least one\nstrategy significantly improved the classification accuracy with LDA (p<=0.05)\nand non-decreasing classification accuracy with SVM in each task. In\nparticular, our proposed strategy demonstrated both effectiveness and\nrobustness in boosting the classification model generalizability, which can be\nemployed in other machine learning applications.",
          "link": "http://arxiv.org/abs/2102.03088",
          "publishedOn": "2021-07-20T02:04:46.115Z",
          "wordCount": null,
          "title": "Boost AI Power: Data Augmentation Strategies with unlabelled Data and Conformal Prediction, a Case in Alternative Herbal Medicine Discrimination with Electronic Nose. (arXiv:2102.03088v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12301",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lin_T/0/1/0/all/0/1\">Tianyi Lin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zheng_Z/0/1/0/all/0/1\">Zeyu Zheng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_E/0/1/0/all/0/1\">Elynn Y. Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cuturi_M/0/1/0/all/0/1\">Marco Cuturi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "Optimal transport (OT) distances are increasingly used as loss functions for\nstatistical inference, notably in the learning of generative models or\nsupervised learning. Yet, the behavior of minimum Wasserstein estimators is\npoorly understood, notably in high-dimensional regimes or under model\nmisspecification. In this work we adopt the viewpoint of projection robust (PR)\nOT, which seeks to maximize the OT cost between two measures by choosing a\n$k$-dimensional subspace onto which they can be projected. Our first\ncontribution is to establish several fundamental statistical properties of PR\nWasserstein distances, complementing and improving previous literature that has\nbeen restricted to one-dimensional and well-specified cases. Next, we propose\nthe integral PR Wasserstein (IPRW) distance as an alternative to the PRW\ndistance, by averaging rather than optimizing on subspaces. Our complexity\nbounds can help explain why both PRW and IPRW distances outperform Wasserstein\ndistances empirically in high-dimensional inference tasks. Finally, we consider\nparametric inference using the PRW distance. We provide an asymptotic guarantee\nof two types of minimum PRW estimators and formulate a central limit theorem\nfor max-sliced Wasserstein estimator under model misspecification. To enable\nour analysis on PRW with projection dimension larger than one, we devise a\nnovel combination of variational analysis and statistical theory.",
          "link": "http://arxiv.org/abs/2006.12301",
          "publishedOn": "2021-07-20T02:04:46.114Z",
          "wordCount": null,
          "title": "On Projection Robust Optimal Transport: Sample Complexity and Model Misspecification. (arXiv:2006.12301v5 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ashesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1\">Luca Del Pero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1\">Hugo Grimmett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1\">Peter Ondruska</a>",
          "description": "Despite the numerous successes of machine learning over the past decade\n(image recognition, decision-making, NLP, image synthesis), self-driving\ntechnology has not yet followed the same trend. In this paper, we study the\nhistory, composition, and development bottlenecks of the modern self-driving\nstack. We argue that the slow progress is caused by approaches that require too\nmuch hand-engineering, an over-reliance on road testing, and high fleet\ndeployment costs. We observe that the classical stack has several bottlenecks\nthat preclude the necessary scale needed to capture the long tail of rare\nevents. To resolve these problems, we outline the principles of Autonomy 2.0,\nan ML-first approach to self-driving, as a viable alternative to the currently\nadopted state-of-the-art. This approach is based on (i) a fully differentiable\nAV stack trainable from human demonstrations, (ii) closed-loop data-driven\nreactive simulation, and (iii) large-scale, low-cost data collections as\ncritical solutions towards scalability issues. We outline the general\narchitecture, survey promising works in this direction and propose key\nchallenges to be addressed by the community in the future.",
          "link": "http://arxiv.org/abs/2107.08142",
          "publishedOn": "2021-07-20T02:04:46.113Z",
          "wordCount": null,
          "title": "Autonomy 2.0: Why is self-driving always 5 years away?. (arXiv:2107.08142v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08179",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Birmpa_P/0/1/0/all/0/1\">Panagiota Birmpa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Feng_J/0/1/0/all/0/1\">Jinchao Feng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Katsoulakis_M/0/1/0/all/0/1\">Markos A. Katsoulakis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rey_Bellet_L/0/1/0/all/0/1\">Luc Rey-Bellet</a>",
          "description": "Probabilistic graphical models are a fundamental tool in probabilistic\nmodeling, machine learning and artificial intelligence. They allow us to\nintegrate in a natural way expert knowledge, physical modeling, heterogeneous\nand correlated data and quantities of interest. For exactly this reason,\nmultiple sources of model uncertainty are inherent within the modular structure\nof the graphical model. In this paper we develop information-theoretic, robust\nuncertainty quantification methods and non-parametric stress tests for directed\ngraphical models to assess the effect and the propagation through the graph of\nmulti-sourced model uncertainties to quantities of interest. These methods\nallow us to rank the different sources of uncertainty and correct the graphical\nmodel by targeting its most impactful components with respect to the quantities\nof interest. Thus, from a machine learning perspective, we provide a\nmathematically rigorous approach to correctability that guarantees a systematic\nselection for improvement of components of a graphical model while controlling\npotential new errors created in the process in other parts of the model. We\ndemonstrate our methods in two physico-chemical examples, namely quantum\nscale-informed chemical kinetics and materials screening to improve the\nefficiency of fuel cells.",
          "link": "http://arxiv.org/abs/2107.08179",
          "publishedOn": "2021-07-20T02:04:46.106Z",
          "wordCount": null,
          "title": "Model Uncertainty and Correctability for Directed Graphical Models. (arXiv:2107.08179v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samo_Y/0/1/0/all/0/1\">Yves-Laurent Kom Samo</a>",
          "description": "We introduce the first application of the lean methodology to machine\nlearning projects. Similar to lean startups and lean manufacturing, we argue\nthat lean machine learning (LeanML) can drastically slash avoidable wastes in\ncommercial machine learning projects, reduce the business risk in investing in\nmachine learning capabilities and, in so doing, further democratize access to\nmachine learning. The lean design pattern we propose in this paper is based on\ntwo realizations. First, it is possible to estimate the best performance one\nmay achieve when predicting an outcome $y \\in \\mathcal{Y}$ using a given set of\nexplanatory variables $x \\in \\mathcal{X}$, for a wide range of performance\nmetrics, and without training any predictive model. Second, doing so is\nconsiderably easier, faster, and cheaper than learning the best predictive\nmodel. We derive formulae expressing the best $R^2$, MSE, classification\naccuracy, and log-likelihood per observation achievable when using $x$ to\npredict $y$ as a function of the mutual information $I\\left(y; x\\right)$, and\npossibly a measure of the variability of $y$ (e.g. its Shannon entropy in the\ncase of classification accuracy, and its variance in the case regression MSE).\nWe illustrate the efficacy of the LeanML design pattern on a wide range of\nregression and classification problems, synthetic and real-life.",
          "link": "http://arxiv.org/abs/2107.08066",
          "publishedOn": "2021-07-20T02:04:46.095Z",
          "wordCount": null,
          "title": "LeanML: A Design Pattern To Slash Avoidable Wastes in Machine Learning Projects. (arXiv:2107.08066v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">J. G. Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gluzman_M/0/1/0/all/0/1\">Mark Gluzman</a>",
          "description": "The policy improvement bound on the difference of the discounted returns\nplays a crucial role in the theoretical justification of the trust-region\npolicy optimization (TRPO) algorithm. The existing bound leads to a degenerate\nbound when the discount factor approaches one, making the applicability of TRPO\nand related algorithms questionable when the discount factor is close to one.\nWe refine the results in \\cite{Schulman2015, Achiam2017} and propose a novel\nbound that is \"continuous\" in the discount factor. In particular, our bound is\napplicable for MDPs with the long-run average rewards as well.",
          "link": "http://arxiv.org/abs/2107.08068",
          "publishedOn": "2021-07-20T02:04:46.094Z",
          "wordCount": null,
          "title": "Refined Policy Improvement Bounds for MDPs. (arXiv:2107.08068v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Holderrieth_P/0/1/0/all/0/1\">Peter Holderrieth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutchinson_M/0/1/0/all/0/1\">Michael Hutchinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>",
          "description": "Motivated by objects such as electric fields or fluid streams, we study the\nproblem of learning stochastic fields, i.e. stochastic processes whose samples\nare fields like those occurring in physics and engineering. Considering general\ntransformations such as rotations and reflections, we show that spatial\ninvariance of stochastic fields requires an inference model to be equivariant.\nLeveraging recent advances from the equivariance literature, we study\nequivariance in two classes of models. Firstly, we fully characterise\nequivariant Gaussian processes. Secondly, we introduce Steerable Conditional\nNeural Processes (SteerCNPs), a new, fully equivariant member of the Neural\nProcess family. In experiments with Gaussian process vector fields, images, and\nreal-world weather data, we observe that SteerCNPs significantly improve the\nperformance of previous models and equivariance leads to improvements in\ntransfer learning tasks.",
          "link": "http://arxiv.org/abs/2011.12916",
          "publishedOn": "2021-07-20T02:04:45.372Z",
          "wordCount": null,
          "title": "Equivariant Learning of Stochastic Fields: Gaussian Processes and Steerable Conditional Neural Processes. (arXiv:2011.12916v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tengyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yingbin Liang</a>",
          "description": "Designing off-policy reinforcement learning algorithms is typically a very\nchallenging task, because a desirable iteration update often involves an\nexpectation over an on-policy distribution. Prior off-policy actor-critic (AC)\nalgorithms have introduced a new critic that uses the density ratio for\nadjusting the distribution mismatch in order to stabilize the convergence, but\nat the cost of potentially introducing high biases due to the estimation errors\nof both the density ratio and value function. In this paper, we develop a\ndoubly robust off-policy AC (DR-Off-PAC) for discounted MDP, which can take\nadvantage of learned nuisance functions to reduce estimation errors. Moreover,\nDR-Off-PAC adopts a single timescale structure, in which both actor and critics\nare updated simultaneously with constant stepsize, and is thus more sample\nefficient than prior algorithms that adopt either two timescale or nested-loop\nstructure. We study the finite-time convergence rate and characterize the\nsample complexity for DR-Off-PAC to attain an $\\epsilon$-accurate optimal\npolicy. We also show that the overall convergence of DR-Off-PAC is doubly\nrobust to the approximation errors that depend only on the expressive power of\napproximation functions. To the best of our knowledge, our study establishes\nthe first overall sample complexity analysis for a single time-scale off-policy\nAC algorithm.",
          "link": "http://arxiv.org/abs/2102.11866",
          "publishedOn": "2021-07-20T02:04:45.244Z",
          "wordCount": null,
          "title": "Doubly Robust Off-Policy Actor-Critic: Convergence and Optimality. (arXiv:2102.11866v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eren_M/0/1/0/all/0/1\">Maksim E. Eren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solovyev_N/0/1/0/all/0/1\">Nick Solovyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamer_C/0/1/0/all/0/1\">Chris Hamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1\">Renee McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexandrov_B/0/1/0/all/0/1\">Boian S. Alexandrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicholas_C/0/1/0/all/0/1\">Charles Nicholas</a>",
          "description": "The unprecedented outbreak of Severe Acute Respiratory Syndrome Coronavirus-2\n(SARS-CoV-2), or COVID-19, continues to be a significant worldwide problem. As\na result, a surge of new COVID-19 related research has followed suit. The\ngrowing number of publications requires document organization methods to\nidentify relevant information. In this paper, we expand upon our previous work\nwith clustering the CORD-19 dataset by applying multi-dimensional analysis\nmethods. Tensor factorization is a powerful unsupervised learning method\ncapable of discovering hidden patterns in a document corpus. We show that a\nhigher-order representation of the corpus allows for the simultaneous grouping\nof similar articles, relevant journals, authors with similar research\ninterests, and topic keywords. These groupings are identified within and among\nthe latent components extracted via tensor decomposition. We further\ndemonstrate the application of this method with a publicly available\ninteractive visualization of the dataset.",
          "link": "http://arxiv.org/abs/2107.08190",
          "publishedOn": "2021-07-20T02:04:44.973Z",
          "wordCount": null,
          "title": "COVID-19 Multidimensional Kaggle Literature Organization. (arXiv:2107.08190v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chrostoforidis_A/0/1/0/all/0/1\">Aristeidis Chrostoforidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyriakides_G/0/1/0/all/0/1\">George Kyriakides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Margaritis_K/0/1/0/all/0/1\">Konstantinos Margaritis</a>",
          "description": "In this work, we propose a novel evolutionary algorithm for neural\narchitecture search, applicable to global search spaces. The algorithm's\narchitectural representation organizes the topology in multiple hierarchical\nmodules, while the design process exploits this representation, in order to\nexplore the search space. We also employ a curation system, which promotes the\nutilization of well performing sub-structures to subsequent generations. We\napply our method to Fashion-MNIST and NAS-Bench101, achieving accuracies of\n$93.2\\%$ and $94.8\\%$ respectively in a relatively small number of generations.",
          "link": "http://arxiv.org/abs/2107.08484",
          "publishedOn": "2021-07-20T02:04:44.663Z",
          "wordCount": 519,
          "title": "A Novel Evolutionary Algorithm for Hierarchical Neural Architecture Search. (arXiv:2107.08484v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08649",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lim_D/0/1/0/all/0/1\">Dong-Young Lim</a>, <a href=\"http://arxiv.org/find/math/1/au:+Neufeld_A/0/1/0/all/0/1\">Ariel Neufeld</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sabanis_S/0/1/0/all/0/1\">Sotirios Sabanis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>",
          "description": "We consider non-convex stochastic optimization problems where the objective\nfunctions have super-linearly growing and discontinuous stochastic gradients.\nIn such a setting, we provide a non-asymptotic analysis for the tamed\nunadjusted stochastic Langevin algorithm (TUSLA) introduced in Lovas et al.\n(2021). In particular, we establish non-asymptotic error bounds for the TUSLA\nalgorithm in Wasserstein-1 and Wasserstein-2 distances. The latter result\nenables us to further derive non-asymptotic estimates for the expected excess\nrisk. To illustrate the applicability of the main results, we consider an\nexample from transfer learning with ReLU neural networks, which represents a\nkey paradigm in machine learning. Numerical experiments are presented for the\naforementioned example which supports our theoretical findings. Hence, in this\nsetting, we demonstrate both theoretically and numerically that the TUSLA\nalgorithm can solve the optimization problem involving neural networks with\nReLU activation function. Besides, we provide simulation results for synthetic\nexamples where popular algorithms, e.g. ADAM, AMSGrad, RMSProp, and (vanilla)\nSGD, may fail to find the minimizer of the objective functions due to the\nsuper-linear growth and the discontinuity of the corresponding stochastic\ngradient, while the TUSLA algorithm converges rapidly to the optimal solution.",
          "link": "http://arxiv.org/abs/2107.08649",
          "publishedOn": "2021-07-20T02:04:44.607Z",
          "wordCount": 655,
          "title": "Non-asymptotic estimates for TUSLA algorithm for non-convex learning with applications to neural networks with ReLU activation function. (arXiv:2107.08649v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huafeng Wang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chonggang Lu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhimin Hu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaodong Yuan</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingshu Zhang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanquan Liu</a> (3) ((1) School of Information, North China University of Technology,(2) Department of Neurology, Kailuan General Hospital, Tangshan,(3) School of Intelligent Systems Engineering, Sun Yat-sen University)",
          "description": "Sleep staging assumes an important role in the diagnosis of sleep disorders.\nIn general, experts classify sleep stages manually based on polysomnography\n(PSG), which is quite time-consuming. Meanwhile, the acquisition of multiple\nsignals is complex, which can affect the subject's sleep. Therefore, the use of\nsingle-channel electroencephalogram (EEG) for automatic sleep staging has\nbecome mainstream. In the literature, a large number of sleep staging methods\nbased on single-channel EEG have been proposed with good results and realize\nthe preliminary automation of sleep staging. However, the performance for most\nof these methods in the N1 stage is generally not high. In this paper, we\npropose a deep learning model SDAN based on raw EEG. The method utilises a\none-dimensional convolutional neural network (CNN) to automatically extract\nfeatures from raw EEG. It serially combines the channel attention and spatial\nattention mechanisms to filter and highlight key information and then uses soft\nthreshold to eliminate redundant information. Additionally, we introduce a\nresidual network to avoid degradation problems caused by network deepening.\nExperiments were conducted using two datasets with 5-fold cross-validation and\nhold-out validation method. The final average accuracy, overall accuracy, macro\nF1 score and Cohen's Kappa coefficient of the model reach 96.74%, 91.86%,\n82.64% and 0.8742 on the Sleep-EDF dataset, and 95.98%, 89.96%, 79.08% and\n0.8216 on the Sleep-EDFx dataset. Significantly, our model performed superiorly\nin the N1 stage, with F1 scores of 54.08% and 52.49% on the two datasets\nrespectively. The results show the superiority of our network over the best\nexisting methods, reaching a new state-of-the-art. In particular, the present\nmethod achieves excellent results in the N1 sleep stage compared to other\nmethods.",
          "link": "http://arxiv.org/abs/2107.08442",
          "publishedOn": "2021-07-20T02:04:44.589Z",
          "wordCount": 744,
          "title": "Sleep Staging Based on Serialized Dual Attention Network. (arXiv:2107.08442v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mrabah_N/0/1/0/all/0/1\">Nairouz Mrabah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouguessa_M/0/1/0/all/0/1\">Mohamed Bouguessa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Touati_M/0/1/0/all/0/1\">Mohamed Fawzi Touati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ksantini_R/0/1/0/all/0/1\">Riadh Ksantini</a>",
          "description": "Most recent graph clustering methods have resorted to Graph Auto-Encoders\n(GAEs) to perform joint clustering and embedding learning. However, two\ncritical issues have been overlooked. First, the accumulative error, inflicted\nby learning with noisy clustering assignments, degrades the effectiveness and\nrobustness of the clustering model. This problem is called Feature Randomness.\nSecond, reconstructing the adjacency matrix sets the model to learn irrelevant\nsimilarities for the clustering task. This problem is called Feature Drift.\nInterestingly, the theoretical relation between the aforementioned problems has\nnot yet been investigated. We study these issues from two aspects: (1) the\nexistence of a trade-off between Feature Randomness and Feature Drift when\nclustering and reconstruction are performed at the same level, and (2) the\nproblem of Feature Drift is more pronounced for GAE models, compared with\nvanilla auto-encoder models, due to the graph convolutional operation and the\ngraph decoding design. Motivated by these findings, we reformulate the\nGAE-based clustering methodology. Our solution is two-fold. First, we propose a\nsampling operator $\\Xi$ that triggers a protection mechanism against the noisy\nclustering assignments. Second, we propose an operator $\\Upsilon$ that triggers\na correction mechanism against Feature Drift by gradually transforming the\nreconstructed graph into a clustering-oriented one. As principal advantages,\nour solution grants a considerable improvement in clustering effectiveness and\nrobustness and can be easily tailored to existing GAE models.",
          "link": "http://arxiv.org/abs/2107.08562",
          "publishedOn": "2021-07-20T02:04:44.571Z",
          "wordCount": 653,
          "title": "Rethinking Graph Autoencoder Models for Attributed Graph Clustering. (arXiv:2107.08562v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08593",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Wang_Y/0/1/0/all/0/1\">Yiran Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>",
          "description": "In this work, we use an explainable convolutional neural network (NLS-Net) to\nsolve an inverse problem of the nonlinear Schr\\\"odinger equation, which is\nwidely used in fiber-optic communications. The landscape and minimizers of the\nnon-convex loss function of the learning problem are studied empirically. It\nprovides a guidance for choosing hyper-parameters of the method. The estimation\nerror of the optimal solution is discussed in terms of expressive power of the\nNLS-Net and data. Besides, we compare the performance of several training\nalgorithms that are popular in deep learning. It is shown that one can obtain a\nrelatively accurate estimate of the considered parameters using the proposed\nmethod. The study provides a natural framework of solving inverse problems of\nnonlinear partial differential equations with deep learning.",
          "link": "http://arxiv.org/abs/2107.08593",
          "publishedOn": "2021-07-20T02:04:44.554Z",
          "wordCount": 566,
          "title": "Inverse Problem of Nonlinear Schr\\\"odinger Equation as Learning of Convolutional Neural Network. (arXiv:2107.08593v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Peng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiabao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xuemin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Libin Zheng</a>",
          "description": "With the rapid development of smart mobile devices, the car-hailing platforms\n(e.g., Uber or Lyft) have attracted much attention from both the academia and\nthe industry. In this paper, we consider an important dynamic car-hailing\nproblem, namely \\textit{maximum revenue vehicle dispatching} (MRVD), in which\nrider requests dynamically arrive and drivers need to serve as many riders as\npossible such that the entire revenue of the platform is maximized. We prove\nthat the MRVD problem is NP-hard and intractable. In addition, the dynamic\ncar-hailing platforms have no information of the future riders, which makes the\nproblem even harder. To handle the MRVD problem, we propose a queueing-based\nvehicle dispatching framework, which first uses existing machine learning\nalgorithms to predict the future vehicle demand of each region, then estimates\nthe idle time periods of drivers through a queueing model for each region. With\nthe information of the predicted vehicle demands and estimated idle time\nperiods of drivers, we propose two batch-based vehicle dispatching algorithms\nto efficiently assign suitable drivers to riders such that the expected overall\nrevenue of the platform is maximized during each batch processing. Through\nextensive experiments, we demonstrate the efficiency and effectiveness of our\nproposed approaches over both real and synthetic datasets.",
          "link": "http://arxiv.org/abs/2107.08662",
          "publishedOn": "2021-07-20T02:04:44.536Z",
          "wordCount": 649,
          "title": "A Queueing-Theoretic Framework for Vehicle Dispatching in Dynamic Car-Hailing [technical report]. (arXiv:2107.08662v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Momeni_A/0/1/0/all/0/1\">Ali Momeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleury_R/0/1/0/all/0/1\">Romain Fleury</a>",
          "description": "Wave-based analog signal processing holds the promise of extremely fast,\non-the-fly, power-efficient data processing, occurring as a wave propagates\nthrough an artificially engineered medium. Yet, due to the fundamentally weak\nnon-linearities of traditional wave materials, such analog processors have been\nso far largely confined to simple linear projections such as image edge\ndetection or matrix multiplications. Complex neuromorphic computing tasks,\nwhich inherently require strong non-linearities, have so far remained\nout-of-reach of wave-based solutions, with a few attempts that implemented\nnon-linearities on the digital front, or used weak and inflexible non-linear\nsensors, restraining the learning performance. Here, we tackle this issue by\ndemonstrating the relevance of Time-Floquet physics to induce a strong\nnon-linear entanglement between signal inputs at different frequencies,\nenabling a power-efficient and versatile wave platform for analog extreme deep\nlearning involving a single, uniformly modulated dielectric layer and a\nscattering medium. We prove the efficiency of the method for extreme learning\nmachines and reservoir computing to solve a range of challenging learning\ntasks, from forecasting chaotic time series to the simultaneous classification\nof distinct datasets. Our results open the way for wave-based machine learning\nwith high energy efficiency, speed, and scalability.",
          "link": "http://arxiv.org/abs/2107.08564",
          "publishedOn": "2021-07-20T02:04:44.473Z",
          "wordCount": 643,
          "title": "Wave-based extreme deep learning based on non-linear time-Floquet entanglement. (arXiv:2107.08564v1 [cs.ET])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Podda_M/0/1/0/all/0/1\">Marco Podda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>",
          "description": "The problem of labeled graph generation is gaining attention in the Deep\nLearning community. The task is challenging due to the sparse and discrete\nnature of graph spaces. Several approaches have been proposed in the\nliterature, most of which require to transform the graphs into sequences that\nencode their structure and labels and to learn the distribution of such\nsequences through an auto-regressive generative model. Among this family of\napproaches, we focus on the GraphGen model. The preprocessing phase of GraphGen\ntransforms graphs into unique edge sequences called Depth-First Search (DFS)\ncodes, such that two isomorphic graphs are assigned the same DFS code. Each\nelement of a DFS code is associated with a graph edge: specifically, it is a\nquintuple comprising one node identifier for each of the two endpoints, their\nnode labels, and the edge label. GraphGen learns to generate such sequences\nauto-regressively and models the probability of each component of the quintuple\nindependently. While effective, the independence assumption made by the model\nis too loose to capture the complex label dependencies of real-world graphs\nprecisely. By introducing a novel graph preprocessing approach, we are able to\nprocess the labeling information of both nodes and edges jointly. The\ncorresponding model, which we term GraphGen-Redux, improves upon the generative\nperformances of GraphGen in a wide range of datasets of chemical and social\ngraphs. In addition, it uses approximately 78% fewer parameters than the\nvanilla variant and requires 50% fewer epochs of training on average.",
          "link": "http://arxiv.org/abs/2107.08396",
          "publishedOn": "2021-07-20T02:04:44.455Z",
          "wordCount": 677,
          "title": "GraphGen-Redux: a Fast and Lightweight Recurrent Model for labeled Graph Generation. (arXiv:2107.08396v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alon_N/0/1/0/all/0/1\">Noga Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1\">Steve Hanneke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holzman_R/0/1/0/all/0/1\">Ron Holzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1\">Shay Moran</a>",
          "description": "We extend the theory of PAC learning in a way which allows to model a rich\nvariety of learning tasks where the data satisfy special properties that ease\nthe learning process. For example, tasks where the distance of the data from\nthe decision boundary is bounded away from zero. The basic and simple idea is\nto consider partial concepts: these are functions that can be undefined on\ncertain parts of the space. When learning a partial concept, we assume that the\nsource distribution is supported only on points where the partial concept is\ndefined.\n\nThis way, one can naturally express assumptions on the data such as lying on\na lower dimensional surface or margin conditions. In contrast, it is not at all\nclear that such assumptions can be expressed by the traditional PAC theory. In\nfact we exhibit easy-to-learn partial concept classes which provably cannot be\ncaptured by the traditional PAC theory. This also resolves a question posed by\nAttias, Kontorovich, and Mansour 2019.\n\nWe characterize PAC learnability of partial concept classes and reveal an\nalgorithmic landscape which is fundamentally different than the classical one.\nFor example, in the classical PAC model, learning boils down to Empirical Risk\nMinimization (ERM). In stark contrast, we show that the ERM principle fails in\nexplaining learnability of partial concept classes. In fact, we demonstrate\nclasses that are incredibly easy to learn, but such that any algorithm that\nlearns them must use an hypothesis space with unbounded VC dimension. We also\nfind that the sample compression conjecture fails in this setting.\n\nThus, this theory features problems that cannot be represented nor solved in\nthe traditional way. We view this as evidence that it might provide insights on\nthe nature of learnability in realistic scenarios which the classical theory\nfails to explain.",
          "link": "http://arxiv.org/abs/2107.08444",
          "publishedOn": "2021-07-20T02:04:44.435Z",
          "wordCount": 746,
          "title": "A Theory of PAC Learnability of Partial Concept Classes. (arXiv:2107.08444v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klimek_J/0/1/0/all/0/1\">Jacek Klimek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klimek_J/0/1/0/all/0/1\">Jakub Klimek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraskiewicz_W/0/1/0/all/0/1\">Witold Kraskiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topolewski_M/0/1/0/all/0/1\">Mateusz Topolewski</a>",
          "description": "Various modifications of TRANSFORMER were recently used to solve time-series\nforecasting problem. We propose Query Selector - an efficient, deterministic\nalgorithm for sparse attention matrix. Experiments show it achieves\nstate-of-the art results on ETT data set.",
          "link": "http://arxiv.org/abs/2107.08687",
          "publishedOn": "2021-07-20T02:04:44.416Z",
          "wordCount": 478,
          "title": "Long-term series forecasting with Query Selector -- efficient model of sparse attention. (arXiv:2107.08687v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08595",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tuo_R/0/1/0/all/0/1\">Rui Tuo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaowei Zhang</a>",
          "description": "High-dimensional simulation optimization is notoriously challenging. We\npropose a new sampling algorithm that converges to a global optimal solution\nand suffers minimally from the curse of dimensionality. The algorithm consists\nof two stages. First, we take samples following a sparse grid experimental\ndesign and approximate the response surface via kernel ridge regression with a\nBrownian field kernel. Second, we follow the expected improvement strategy --\nwith critical modifications that boost the algorithm's sample efficiency -- to\niteratively sample from the next level of the sparse grid. Under mild\nconditions on the smoothness of the response surface and the simulation noise,\nwe establish upper bounds on the convergence rate for both noise-free and noisy\nsimulation samples. These upper rates deteriorate only slightly in the\ndimension of the feasible set, and they can be improved if the objective\nfunction is known be of a higher-order smoothness. Extensive numerical\nexperiments demonstrate that the proposed algorithm dramatically outperforms\ntypical alternatives in practice.",
          "link": "http://arxiv.org/abs/2107.08595",
          "publishedOn": "2021-07-20T02:04:44.400Z",
          "wordCount": 615,
          "title": "High-Dimensional Simulation Optimization via Brownian Fields and Sparse Grids. (arXiv:2107.08595v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gautam Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peri_S/0/1/0/all/0/1\">Skand Peri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunseok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungjin Ahn</a>",
          "description": "Object-centric world models provide structured representation of the scene\nand can be an important backbone in reinforcement learning and planning.\nHowever, existing approaches suffer in partially-observable environments due to\nthe lack of belief states. In this paper, we propose Structured World Belief, a\nmodel for learning and inference of object-centric belief states. Inferred by\nSequential Monte Carlo (SMC), our belief states provide multiple object-centric\nscene hypotheses. To synergize the benefits of SMC particles with object\nrepresentations, we also propose a new object-centric dynamics model that\nconsiders the inductive bias of object permanence. This enables tracking of\nobject states even when they are invisible for a long time. To further\nfacilitate object tracking in this regime, we allow our model to attend\nflexibly to any spatial location in the image which was restricted in previous\nmodels. In experiments, we show that object-centric belief provides a more\naccurate and robust performance for filtering and generation. Furthermore, we\nshow the efficacy of structured world belief in improving the performance of\nreinforcement learning, planning and supervised reasoning.",
          "link": "http://arxiv.org/abs/2107.08577",
          "publishedOn": "2021-07-20T02:04:44.344Z",
          "wordCount": 614,
          "title": "Structured World Belief for Reinforcement Learning in POMDP. (arXiv:2107.08577v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lonkar_S/0/1/0/all/0/1\">Subodh Lonkar</a>",
          "description": "Over the centuries, humans have developed and acquired a number of ways to\ncommunicate. But hardly any of them can be as natural and instinctive as facial\nexpressions. On the other hand, neural networks have taken the world by storm.\nAnd no surprises, that the area of Computer Vision and the problem of facial\nexpressions recognitions hasn't remained untouched. Although a wide range of\ntechniques have been applied, achieving extremely high accuracies and preparing\nhighly robust FER systems still remains a challenge due to heterogeneous\ndetails in human faces. In this paper, we will be deep diving into implementing\na system for recognition of facial expressions (FER) by leveraging neural\nnetworks, and more specifically, Convolutional Neural Networks (CNNs). We adopt\nthe fundamental concepts of deep learning and computer vision with various\narchitectures, fine-tune it's hyperparameters and experiment with various\noptimization methods and demonstrate a state-of-the-art single-network-accuracy\nof 70.10% on the FER2013 dataset without using any additional training data.",
          "link": "http://arxiv.org/abs/2107.08640",
          "publishedOn": "2021-07-20T02:04:44.326Z",
          "wordCount": 592,
          "title": "Facial Expressions Recognition with Convolutional Neural Networks. (arXiv:2107.08640v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>",
          "description": "Generalization performance of stochastic optimization stands a central place\nin machine learning. In this paper, we investigate the excess risk performance\nand towards improved learning rates for two popular approaches of stochastic\noptimization: empirical risk minimization (ERM) and stochastic gradient descent\n(SGD). Although there exists plentiful generalization analysis of ERM and SGD\nfor supervised learning, current theoretical understandings of ERM and SGD are\neither have stronger assumptions in convex learning, e.g., strong convexity\ncondition, or show slow rates and less studied in nonconvex learning. Motivated\nby these problems, we aim to provide improved rates under milder assumptions in\nconvex learning and derive faster rates in nonconvex learning. It is notable\nthat our analysis span two popular theoretical viewpoints: stability and\nuniform convergence. To be specific, in stability regime, we present high\nprobability rates of order $\\mathcal{O} (1/n)$ w.r.t. the sample size $n$ for\nERM and SGD with milder assumptions in convex learning and similar high\nprobability rates of order $\\mathcal{O} (1/n)$ in nonconvex learning, rather\nthan in expectation. Furthermore, this type of learning rate is improved to\nfaster order $\\mathcal{O} (1/n^2)$ in uniform convergence regime. To the best\nof our knowledge, for ERM and SGD, the learning rates presented in this paper\nare all state-of-the-art.",
          "link": "http://arxiv.org/abs/2107.08686",
          "publishedOn": "2021-07-20T02:04:44.309Z",
          "wordCount": 645,
          "title": "Improved Learning Rates for Stochastic Optimization: Two Theoretical Viewpoints. (arXiv:2107.08686v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomerantz_R/0/1/0/all/0/1\">Roi Pomerantz</a>",
          "description": "We present Translatotron 2, a neural direct speech-to-speech translation\nmodel that can be trained end-to-end. Translatotron 2 consists of a speech\nencoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention\nmodule that connects all the previous three components. Experimental results\nsuggest that Translatotron 2 outperforms the original Translatotron by a large\nmargin in terms of translation quality and predicted speech naturalness, and\ndrastically improves the robustness of the predicted speech by mitigating\nover-generation, such as babbling or long pause. We also propose a new method\nfor retaining the source speaker's voice in the translated speech. The trained\nmodel is restricted to retain the source speaker's voice, and unlike the\noriginal Translatotron, it is not able to generate speech in a different\nspeaker's voice, making the model more robust for production deployment, by\nmitigating potential misuse for creating spoofing audio artifacts. When the new\nmethod is used together with a simple concatenation-based data augmentation,\nthe trained Translatotron 2 model is able to retain each speaker's voice for\ninput with speaker turns.",
          "link": "http://arxiv.org/abs/2107.08661",
          "publishedOn": "2021-07-20T02:04:44.288Z",
          "wordCount": 614,
          "title": "Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maran_D/0/1/0/all/0/1\">D. Maran</a>",
          "description": "We improve a theoretical result of the article \"On Exploiting Spectral\nProperties for Solving MDP with Large State Space\" showing that their\nalgorithm, which was proved to converge under some unrealistic assumptions, is\nactually guaranteed to converge always.",
          "link": "http://arxiv.org/abs/2107.08488",
          "publishedOn": "2021-07-20T02:04:44.270Z",
          "wordCount": 484,
          "title": "A note on the article \"On Exploiting Spectral Properties for Solving MDP with Large State Space\". (arXiv:2107.08488v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ibeling_D/0/1/0/all/0/1\">Duligur Ibeling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Icard_T/0/1/0/all/0/1\">Thomas Icard</a>",
          "description": "This paper presents a topological learning-theoretic perspective on causal\ninference by introducing a series of topologies defined on general spaces of\nstructural causal models (SCMs). As an illustration of the framework we prove a\ntopological causal hierarchy theorem, showing that substantive assumption-free\ncausal inference is possible only in a meager set of SCMs. Thanks to a known\ncorrespondence between open sets in the weak topology and statistically\nverifiable hypotheses, our results show that inductive assumptions sufficient\nto license valid causal inferences are statistically unverifiable in principle.\nSimilar to no-free-lunch theorems for statistical inference, the present\nresults clarify the inevitability of substantial assumptions for causal\ninference. An additional benefit of our topological approach is that it easily\naccommodates SCMs with infinitely many variables. We finally suggest that the\nframework may be helpful for the positive project of exploring and assessing\nalternative causal-inductive assumptions.",
          "link": "http://arxiv.org/abs/2107.08558",
          "publishedOn": "2021-07-20T02:04:44.206Z",
          "wordCount": 578,
          "title": "A Topological Perspective on Causal Inference. (arXiv:2107.08558v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>",
          "description": "We study multi-task reinforcement learning (RL) in tabular episodic Markov\ndecision processes (MDPs). We formulate a heterogeneous multi-player RL\nproblem, in which a group of players concurrently face similar but not\nnecessarily identical MDPs, with a goal of improving their collective\nperformance through inter-player information sharing. We design and analyze an\nalgorithm based on the idea of model transfer, and provide gap-dependent and\ngap-independent upper and lower bounds that characterize the intrinsic\ncomplexity of the problem.",
          "link": "http://arxiv.org/abs/2107.08622",
          "publishedOn": "2021-07-20T02:04:44.188Z",
          "wordCount": 502,
          "title": "Provably Efficient Multi-Task Reinforcement Learning with Model Transfer. (arXiv:2107.08622v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdelhack_M/0/1/0/all/0/1\">Mohamed Abdelhack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Sandhya Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_B/0/1/0/all/0/1\">Bradley Fritz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avidan_M/0/1/0/all/0/1\">Michael Avidan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_C/0/1/0/all/0/1\">Christopher King</a>",
          "description": "Data quality is a common problem in machine learning, especially in\nhigh-stakes settings such as healthcare. Missing data affects accuracy,\ncalibration, and feature attribution in complex patterns. Developers often\ntrain models on carefully curated datasets to minimize missing data bias;\nhowever, this reduces the usability of such models in production environments,\nsuch as real-time healthcare records. Making machine learning models robust to\nmissing data is therefore crucial for practical application. While some\nclassifiers naturally handle missing data, others, such as deep neural\nnetworks, are not designed for unknown values. We propose a novel neural\nnetwork modification to mitigate the impacts of missing data. The approach is\ninspired by neuromodulation that is performed by biological neural networks.\nOur proposal replaces the fixed weights of a fully-connected layer with a\nfunction of an additional input (reliability score) at each input, mimicking\nthe ability of cortex to up- and down-weight inputs based on the presence of\nother data. The modulation function is jointly learned with the main task using\na multi-layer perceptron. We tested our modulating fully connected layer on\nmultiple classification, regression, and imputation problems, and it either\nimproved performance or generated comparable performance to conventional neural\nnetwork architectures concatenating reliability to the inputs. Models with\nmodulating layers were more robust against degradation of data quality by\nintroducing additional missingness at evaluation time. These results suggest\nthat explicitly accounting for reduced information quality with a modulating\nfully connected layer can enable the deployment of artificial intelligence\nsystems in real-time settings.",
          "link": "http://arxiv.org/abs/2107.08574",
          "publishedOn": "2021-07-20T02:04:44.170Z",
          "wordCount": 695,
          "title": "A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues. (arXiv:2107.08574v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sisejkovic_D/0/1/0/all/0/1\">Dominik Sisejkovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merchant_F/0/1/0/all/0/1\">Farhad Merchant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimann_L/0/1/0/all/0/1\">Lennart M. Reimann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leupers_R/0/1/0/all/0/1\">Rainer Leupers</a>",
          "description": "Logic locking has emerged as a prominent key-driven technique to protect the\nintegrity of integrated circuits. However, novel machine-learning-based attacks\nhave recently been introduced to challenge the security foundations of locking\nschemes. These attacks are able to recover a significant percentage of the key\nwithout having access to an activated circuit. This paper address this issue\nthrough two focal points. First, we present a theoretical model to test locking\nschemes for key-related structural leakage that can be exploited by machine\nlearning. Second, based on the theoretical model, we introduce D-MUX: a\ndeceptive multiplexer-based logic-locking scheme that is resilient against\nstructure-exploiting machine learning attacks. Through the design of D-MUX, we\nuncover a major fallacy in existing multiplexer-based locking schemes in the\nform of a structural-analysis attack. Finally, an extensive cost evaluation of\nD-MUX is presented. To the best of our knowledge, D-MUX is the first\nmachine-learning-resilient locking scheme capable of protecting against all\nknown learning-based attacks. Hereby, the presented work offers a starting\npoint for the design and evaluation of future-generation logic locking in the\nera of machine learning.",
          "link": "http://arxiv.org/abs/2107.08695",
          "publishedOn": "2021-07-20T02:04:44.143Z",
          "wordCount": 626,
          "title": "Deceptive Logic Locking for Hardware Integrity Protection against Machine Learning Attacks. (arXiv:2107.08695v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jinke Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chonghe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Guanding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Dongning Guo</a>",
          "description": "Generative adversarial networks (GANs) are emerging machine learning models\nfor generating synthesized data similar to real data by jointly training a\ngenerator and a discriminator. In many applications, data and computational\nresources are distributed over many devices, so centralized computation with\nall data in one location is infeasible due to privacy and/or communication\nconstraints. This paper proposes a new framework for training GANs in a\ndistributed fashion: Each device computes a local discriminator using local\ndata; a single server aggregates their results and computes a global GAN.\nSpecifically, in each iteration, the server sends the global GAN to the\ndevices, which then update their local discriminators; the devices send their\nresults to the server, which then computes their average as the global\ndiscriminator and updates the global generator accordingly. Two different\nupdate schedules are designed with different levels of parallelism between the\ndevices and the server. Numerical results obtained using three popular datasets\ndemonstrate that the proposed framework can outperform a state-of-the-art\nframework in terms of convergence speed.",
          "link": "http://arxiv.org/abs/2107.08681",
          "publishedOn": "2021-07-20T02:04:44.124Z",
          "wordCount": 627,
          "title": "A New Distributed Method for Training Generative Adversarial Networks. (arXiv:2107.08681v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pimpley_A/0/1/0/all/0/1\">Anish Pimpley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Anubha Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohra_V/0/1/0/all/0/1\">Vishal Rohra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Soundararajan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jindal_A/0/1/0/all/0/1\">Alekh Jindal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1\">Hiren Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1\">Shi Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_R/0/1/0/all/0/1\">Rathijit Sen</a>",
          "description": "Optimizing resource allocation for analytical workloads is vital for reducing\ncosts of cloud-data services. At the same time, it is incredibly hard for users\nto allocate resources per query in serverless processing systems, and they\nfrequently misallocate by orders of magnitude. Unfortunately, prior work\nfocused on predicting peak allocation while ignoring aggressive trade-offs\nbetween resource allocation and run-time. Additionally, these methods fail to\npredict allocation for queries that have not been observed in the past. In this\npaper, we tackle both these problems. We introduce a system for optimal\nresource allocation that can predict performance with aggressive trade-offs,\nfor both new and past observed queries. We introduce the notion of a\nperformance characteristic curve (PCC) as a parameterized representation that\ncan compactly capture the relationship between resources and performance. To\ntackle training data sparsity, we introduce a novel data augmentation technique\nto efficiently synthesize the entire PCC using a single run of the query.\nLastly, we demonstrate the advantages of a constrained loss function coupled\nwith GNNs, over traditional ML methods, for capturing the domain specific\nbehavior through an extensive experimental evaluation over SCOPE big data\nworkloads at Microsoft.",
          "link": "http://arxiv.org/abs/2107.08594",
          "publishedOn": "2021-07-20T02:04:44.093Z",
          "wordCount": 628,
          "title": "Optimal Resource Allocation for Serverless Queries. (arXiv:2107.08594v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yinjun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weimer_J/0/1/0/all/0/1\">James Weimer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_S/0/1/0/all/0/1\">Susan B. Davidson</a>",
          "description": "High-quality labels are expensive to obtain for many machine learning tasks,\nsuch as medical image classification tasks. Therefore, probabilistic (weak)\nlabels produced by weak supervision tools are used to seed a process in which\ninfluential samples with weak labels are identified and cleaned by several\nhuman annotators to improve the model performance. To lower the overall cost\nand computational overhead of this process, we propose a solution called\nChef(CHEap and Fast label cleaning), which consists of the following three\ncomponents. First, to reduce the cost of human annotators, we use Infl, which\nprioritizes the most influential training samples for cleaning and provides\ncleaned labels to save the cost of one human annotator. Second, to accelerate\nthe sample selector phase and the model constructor phase, we use Increm-Infl\nto incrementally produce influential samples, and DeltaGrad-L to incrementally\nupdate the model. Third, we redesign the typical label cleaning pipeline so\nthat human annotators iteratively clean smaller batch of samples rather than\none big batch of samples. This yields better over all model performance and\nenables possible early termination when the expected model performance has been\nachieved. Extensive experiments show that our approach gives good model\nprediction performance while achieving significant speed-ups.",
          "link": "http://arxiv.org/abs/2107.08588",
          "publishedOn": "2021-07-20T02:04:44.023Z",
          "wordCount": 636,
          "title": "Chef: a cheap and fast pipeline for iteratively cleaning label uncertainties. (arXiv:2107.08588v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08673",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Massalimova_A/0/1/0/all/0/1\">Aidana Massalimova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varol_H/0/1/0/all/0/1\">Huseyin Atakan Varol</a>",
          "description": "Alzheimer's disease (AD) is a progressive brain disorder that causes memory\nand functional impairments. The advances in machine learning and publicly\navailable medical datasets initiated multiple studies in AD diagnosis. In this\nwork, we utilize a multi-modal deep learning approach in classifying normal\ncognition, mild cognitive impairment and AD classes on the basis of structural\nMRI and diffusion tensor imaging (DTI) scans from the OASIS-3 dataset. In\naddition to a conventional multi-modal network, we also present an input\nagnostic architecture that allows diagnosis with either sMRI or DTI scan, which\ndistinguishes our method from previous multi-modal machine learning-based\nmethods. The results show that the input agnostic model achieves 0.96 accuracy\nwhen both structural MRI and DTI scans are provided as inputs.",
          "link": "http://arxiv.org/abs/2107.08673",
          "publishedOn": "2021-07-20T02:04:44.004Z",
          "wordCount": 581,
          "title": "Input Agnostic Deep Learning for Alzheimer's Disease Classification Using Multimodal MRI Images. (arXiv:2107.08673v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Velichko_A/0/1/0/all/0/1\">Andrei Velichko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidari_H/0/1/0/all/0/1\">Hanif Heidari</a>",
          "description": "Measuring the predictability and complexity of time series is an essential\ntool in designing and controlling the nonlinear system. There exist different\nentropy measures in the literature to analyze the predictability and complexity\nof time series. However, these measures have some drawbacks especially in short\ntime series. To overcome the difficulties, this paper proposes a new method for\nestimating the entropy of a time series using the LogNNet 784:25:10 neural\nnetwork model. The LogNNet reservoir matrix consists of 19625 elements which is\nfilled with the time series elements. After that, the network is trained on\nMNIST-10 dataset and the classification accuracy is calculated. The accuracy is\nconsidered as the entropy measure and denoted by NNetEn. A more complex\ntransformation of the input information by the time series in the reservoir\nleads to higher NNetEn values. Many practical time series data have less than\n19625 elements. Some duplicating or stretching methods are investigated to\novercome this difficulty and the most successful method is identified for\npractical applications. The epochs number in the training process of LogNNet is\nconsidered as the input parameter. A new time series characteristic called time\nseries learning inertia is introduced to investigate the effect of epochs\nnumber in the efficiency of neural network. To show the robustness and\nefficiency of the proposed method, it is applied on some chaotic, periodic,\nrandom, binary and constant time series. The NNetEn is compared with some\nexisting entropy measures. The results show that the proposed method is more\nrobust and accurate than existing methods.",
          "link": "http://arxiv.org/abs/2107.08399",
          "publishedOn": "2021-07-20T02:04:43.986Z",
          "wordCount": 712,
          "title": "A method for estimating the entropy of time series using artificial neural network. (arXiv:2107.08399v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhou Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tong Lin</a>",
          "description": "Adaptive gradient methods, especially Adam-type methods (such as Adam,\nAMSGrad, and AdaBound), have been proposed to speed up the training process\nwith an element-wise scaling term on learning rates. However, they often\ngeneralize poorly compared with stochastic gradient descent (SGD) and its\naccelerated schemes such as SGD with momentum (SGDM). In this paper, we propose\na new adaptive method called DecGD, which simultaneously achieves good\ngeneralization like SGDM and obtain rapid convergence like Adam-type methods.\nIn particular, DecGD decomposes the current gradient into the product of two\nterms including a surrogate gradient and a loss based vector. Our method\nadjusts the learning rates adaptively according to the current loss based\nvector instead of the squared gradients used in Adam-type methods. The\nintuition for adaptive learning rates of DecGD is that a good optimizer, in\ngeneral cases, needs to decrease the learning rates as the loss decreases,\nwhich is similar to the learning rates decay scheduling technique. Therefore,\nDecGD gets a rapid convergence in the early phases of training and controls the\neffective learning rates according to the loss based vectors which help lead to\na better generalization. Convergence analysis is discussed in both convex and\nnon-convex situations. Finally, empirical results on widely-used tasks and\nmodels demonstrate that DecGD shows better generalization performance than SGDM\nand rapid convergence like Adam-type methods.",
          "link": "http://arxiv.org/abs/2107.08377",
          "publishedOn": "2021-07-20T02:04:43.967Z",
          "wordCount": 652,
          "title": "A New Adaptive Gradient Method with Gradient Decomposition. (arXiv:2107.08377v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08710",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Higham_C/0/1/0/all/0/1\">Catherine F. Higham</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Bedford_A/0/1/0/all/0/1\">Adrian Bedford</a>",
          "description": "We demonstrate the feasibility of framing a classically learned deep neural\nnetwork as an energy based model that can be processed on a one-step quantum\nannealer in order to exploit fast sampling times. We propose approaches to\novercome two hurdles for high resolution image classification on a quantum\nprocessing unit (QPU): the required number and binary nature of the model\nstates. With this novel method we successfully transfer a convolutional neural\nnetwork to the QPU and show the potential for classification speedup of at\nleast one order of magnitude.",
          "link": "http://arxiv.org/abs/2107.08710",
          "publishedOn": "2021-07-20T02:04:43.913Z",
          "wordCount": 522,
          "title": "Quantum Deep Learning: Sampling Neural Nets with a Quantum Annealer. (arXiv:2107.08710v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08596",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Katsman_I/0/1/0/all/0/1\">Isay Katsman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lou_A/0/1/0/all/0/1\">Aaron Lou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lim_D/0/1/0/all/0/1\">Derek Lim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jiang_Q/0/1/0/all/0/1\">Qingxuan Jiang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sa_C/0/1/0/all/0/1\">Christopher De Sa</a>",
          "description": "Tractably modelling distributions over manifolds has long been an important\ngoal in the natural sciences. Recent work has focused on developing general\nmachine learning models to learn such distributions. However, for many\napplications these distributions must respect manifold symmetries -- a trait\nwhich most previous models disregard. In this paper, we lay the theoretical\nfoundations for learning symmetry-invariant distributions on arbitrary\nmanifolds via equivariant manifold flows. We demonstrate the utility of our\napproach by using it to learn gauge invariant densities over $SU(n)$ in the\ncontext of quantum field theory.",
          "link": "http://arxiv.org/abs/2107.08596",
          "publishedOn": "2021-07-20T02:04:43.826Z",
          "wordCount": 526,
          "title": "Equivariant Manifold Flows. (arXiv:2107.08596v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nieto_J/0/1/0/all/0/1\">Juan Jos&#xe9; Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creus_R/0/1/0/all/0/1\">Roger Creus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1\">Xavier Giro-i-Nieto</a>",
          "description": "Pre-training Reinforcement Learning agents in a task-agnostic manner has\nshown promising results. However, previous works still struggle in learning and\ndiscovering meaningful skills in high-dimensional state-spaces, such as\npixel-spaces. We approach the problem by leveraging unsupervised skill\ndiscovery and self-supervised learning of state representations. In our work,\nwe learn a compact latent representation by making use of variational and\ncontrastive techniques. We demonstrate that both enable RL agents to learn a\nset of basic navigation skills by maximizing an information theoretic\nobjective. We assess our method in Minecraft 3D pixel maps with different\ncomplexities. Our results show that representations and conditioned policies\nlearned from pixels are enough for toy examples, but do not scale to realistic\nand complex maps. To overcome these limitations, we explore alternative input\nobservations such as the relative position of the agent along with the raw\npixels.",
          "link": "http://arxiv.org/abs/2107.08398",
          "publishedOn": "2021-07-20T02:04:43.808Z",
          "wordCount": 579,
          "title": "Unsupervised Skill-Discovery and Skill-Learning in Minecraft. (arXiv:2107.08398v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08429",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Naik_S/0/1/0/all/0/1\">Shibabrat Naik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Krajnak_V/0/1/0/all/0/1\">Vladim&#xed;r Kraj&#x148;&#xe1;k</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wiggins_S/0/1/0/all/0/1\">Stephen Wiggins</a>",
          "description": "We develop a machine learning framework that can be applied to data sets\nderived from the trajectories of Hamilton's equations. The goal is to learn the\nphase space structures that play the governing role for phase space transport\nrelevant to particular applications. Our focus is on learning reactive islands\nin two degrees-of-freedom Hamiltonian systems. Reactive islands are constructed\nfrom the stable and unstable manifolds of unstable periodic orbits and play the\nrole of quantifying transition dynamics. We show that support vector machines\n(SVM) is an appropriate machine learning framework for this purpose as it\nprovides an approach for finding the boundaries between qualitatively distinct\ndynamical behaviors, which is in the spirit of the phase space transport\nframework. We show how our method allows us to find reactive islands directly\nin the sense that we do not have to first compute unstable periodic orbits and\ntheir stable and unstable manifolds. We apply our approach to the\nH\\'enon-Heiles Hamiltonian system, which is a benchmark system in the dynamical\nsystems community. We discuss different sampling and learning approaches and\ntheir advantages and disadvantages.",
          "link": "http://arxiv.org/abs/2107.08429",
          "publishedOn": "2021-07-20T02:04:43.788Z",
          "wordCount": 625,
          "title": "Support vector machines for learning reactive islands. (arXiv:2107.08429v1 [math.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ampanavos_S/0/1/0/all/0/1\">Spyridon Ampanavos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkawi_A/0/1/0/all/0/1\">Ali Malkawi</a>",
          "description": "Current performance-driven building design methods are not widely adopted\noutside the research field for several reasons that make them difficult to\nintegrate into a typical design process. In the early design phase, in\nparticular, the time-intensity and the cognitive load associated with\noptimization and form parametrization are incompatible with design exploration,\nwhich requires quick iteration. This research introduces a novel method for\nperformance-driven geometry generation that can afford interaction directly in\nthe 3d modeling environment, eliminating the need for explicit parametrization,\nand is multiple orders faster than the equivalent form optimization. The method\nuses Machine Learning techniques to train a generative model offline. The\ngenerative model learns a distribution of optimal performing geometries and\ntheir simulation contexts based on a dataset that addresses the performance(s)\nof interest. By navigating the generative model's latent space, geometries with\nthe desired characteristics can be quickly generated. A case study is\npresented, demonstrating the generation of a synthetic dataset and the use of a\nVariational Autoencoder (VAE) as a generative model for geometries with optimal\nsolar gain. The results show that the VAE-generated geometries perform on\naverage at least as well as the optimized ones, suggesting that the introduced\nmethod shows a feasible path towards more intuitive and interactive early-phase\nperformance-driven design assistance.",
          "link": "http://arxiv.org/abs/2107.08572",
          "publishedOn": "2021-07-20T02:04:43.769Z",
          "wordCount": 637,
          "title": "Early-Phase Performance-Driven Design using Generative Models. (arXiv:2107.08572v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08470",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ho_Y/0/1/0/all/0/1\">Yung-Han Ho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_C/0/1/0/all/0/1\">Chih-Chun Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_W/0/1/0/all/0/1\">Wen-Hsiao Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hang_H/0/1/0/all/0/1\">Hsueh-Ming Hang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Domanski_M/0/1/0/all/0/1\">Marek Domanski</a>",
          "description": "This paper introduces an end-to-end learned image compression system, termed\nANFIC, based on Augmented Normalizing Flows (ANF). ANF is a new type of flow\nmodel, which stacks multiple variational autoencoders (VAE) for greater model\nexpressiveness. The VAE-based image compression has gone mainstream, showing\npromising compression performance. Our work presents the first attempt to\nleverage VAE-based compression in a flow-based framework. ANFIC advances\nfurther compression efficiency by stacking and extending hierarchically\nmultiple VAE's. The invertibility of ANF, together with our training\nstrategies, enables ANFIC to support a wide range of quality levels without\nchanging the encoding and decoding networks. Extensive experimental results\nshow that in terms of PSNR-RGB, ANFIC performs comparably to or better than the\nstate-of-the-art learned image compression. Moreover, it performs close to VVC\nintra coding, from low-rate compression up to nearly-lossless compression. In\nparticular, ANFIC achieves the state-of-the-art performance, when extended with\nconditional convolution for variable rate compression with a single model.",
          "link": "http://arxiv.org/abs/2107.08470",
          "publishedOn": "2021-07-20T02:04:43.752Z",
          "wordCount": 602,
          "title": "ANFIC: Image Compression Using Augmented Normalizing Flows. (arXiv:2107.08470v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chin_H/0/1/0/all/0/1\">Hsu-Hsun Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsay_R/0/1/0/all/0/1\">Ren-Song Tsay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hsin-I Wu</a>",
          "description": "Recent convolutional neural network (CNN) development continues to advance\nthe state-of-the-art model accuracy for various applications. However, the\nenhanced accuracy comes at the cost of substantial memory bandwidth and storage\nrequirements and demanding computational resources. Although in the past the\nquantization methods have effectively reduced the deployment cost for edge\ndevices, it suffers from significant information loss when processing the\nbiased activations of contemporary CNNs. In this paper, we hence introduce an\nadaptive high-performance quantization method to resolve the issue of biased\nactivation by dynamically adjusting the scaling and shifting factors based on\nthe task loss. Our proposed method has been extensively evaluated on image\nclassification models (ResNet-18/34/50, MobileNet-V2, EfficientNet-B0) with\nImageNet dataset, object detection model (YOLO-V4) with COCO dataset, and\nlanguage models with PTB dataset. The results show that our 4-bit integer\n(INT4) quantization models achieve better accuracy than the state-of-the-art\n4-bit models, and in some cases, even surpass the golden full-precision models.\nThe final designs have been successfully deployed onto extremely\nresource-constrained edge devices for many practical applications.",
          "link": "http://arxiv.org/abs/2107.08382",
          "publishedOn": "2021-07-20T02:04:43.694Z",
          "wordCount": 609,
          "title": "A High-Performance Adaptive Quantization Approach for Edge CNN Applications. (arXiv:2107.08382v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Chihcheng Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_C/0/1/0/all/0/1\">Catarina Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1\">Chun Ouyang</a>",
          "description": "Predictive process analytics often apply machine learning to predict the\nfuture states of a running business process. However, the internal mechanisms\nof many existing predictive algorithms are opaque and a human decision-maker is\nunable to understand \\emph{why} a certain activity was predicted. Recently,\ncounterfactuals have been proposed in the literature to derive\nhuman-understandable explanations from predictive models. Current\ncounterfactual approaches consist of finding the minimum feature change that\ncan make a certain prediction flip its outcome. Although many algorithms have\nbeen proposed, their application to the sequence and multi-dimensional data\nlike event logs has not been explored in the literature.\n\nIn this paper, we explore the use of a recent, popular model-agnostic\ncounterfactual algorithm, DiCE, in the context of predictive process analytics.\nThe analysis reveals that the algorithm is limited when being applied to derive\nexplanations of process predictions, due to (1) process domain knowledge not\nbeing taken into account, (2) long traces that often tend to be less\nunderstandable, and (3) difficulties in optimising the counterfactual search\nwith categorical variables. We design an extension of DiCE that can generate\ncounterfactuals for process predictions, and propose an approach that supports\nderiving milestone-aware counterfactuals at different stages of a trace to\npromote interpretability. We apply our approach to BPIC2012 event log and the\nanalysis results demonstrate the effectiveness of the proposed approach.",
          "link": "http://arxiv.org/abs/2107.08697",
          "publishedOn": "2021-07-20T02:04:43.676Z",
          "wordCount": 652,
          "title": "Interpreting Process Predictions using a Milestone-Aware Counterfactual Approach. (arXiv:2107.08697v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Assayag_S/0/1/0/all/0/1\">Shai Ben-Assayag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Yaniv_R/0/1/0/all/0/1\">Ran El-Yaniv</a>",
          "description": "Playing board games is considered a major challenge for both humans and AI\nresearchers. Because some complicated board games are quite hard to learn,\nhumans usually begin with playing on smaller boards and incrementally advance\nto master larger board strategies. Most neural network frameworks that are\ncurrently tasked with playing board games neither perform such incremental\nlearning nor possess capabilities to automatically scale up. In this work, we\nlook at the board as a graph and combine a graph neural network architecture\ninside the AlphaZero framework, along with some other innovative improvements.\nOur ScalableAlphaZero is capable of learning to play incrementally on small\nboards, and advancing to play on large ones. Our model can be trained quickly\nto play different challenging board games on multiple board sizes, without\nusing any domain knowledge. We demonstrate the effectiveness of\nScalableAlphaZero and show, for example, that by training it for only three\ndays on small Othello boards, it can defeat the AlphaZero model on a large\nboard, which was trained to play the large board for $30$ days.",
          "link": "http://arxiv.org/abs/2107.08387",
          "publishedOn": "2021-07-20T02:04:43.657Z",
          "wordCount": 616,
          "title": "Train on Small, Play the Large: Scaling Up Board Games with AlphaZero and GNN. (arXiv:2107.08387v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuesi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huzhang_G/0/1/0/all/0/1\">Guangda Huzhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qianying Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Da_Q/0/1/0/all/0/1\">Qing Da</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dan Shen</a>",
          "description": "Ensemble models in E-commerce combine predictions from multiple sub-models\nfor ranking and revenue improvement. Industrial ensemble models are typically\ndeep neural networks, following the supervised learning paradigm to infer\nconversion rate given inputs from sub-models. However, this process has the\nfollowing two problems. Firstly, the point-wise scoring approach disregards the\nrelationships between items and leads to homogeneous displayed results, while\ndiversified display benefits user experience and revenue. Secondly, the\nlearning paradigm focuses on the ranking metrics and does not directly optimize\nthe revenue. In our work, we propose a new Learning-To-Ensemble (LTE) framework\nRAEGO, which replaces the ensemble model with a contextual Rank Aggregator (RA)\nand explores the best weights of sub-models by the Evaluator-Generator\nOptimization (EGO). To achieve the best online performance, we propose a new\nrank aggregation algorithm TournamentGreedy as a refinement of classic rank\naggregators, which also produces the best average weighted Kendall Tau Distance\n(KTD) amongst all the considered algorithms with quadratic time complexity.\nUnder the assumption that the best output list should be Pareto Optimal on the\nKTD metric for sub-models, we show that our RA algorithm has higher efficiency\nand coverage in exploring the optimal weights. Combined with the idea of\nBayesian Optimization and gradient descent, we solve the online contextual\nBlack-Box Optimization task that finds the optimal weights for sub-models given\na chosen RA model. RA-EGO has been deployed in our online system and has\nimproved the revenue significantly.",
          "link": "http://arxiv.org/abs/2107.08598",
          "publishedOn": "2021-07-20T02:04:43.636Z",
          "wordCount": 670,
          "title": "Learning-To-Ensemble by Contextual Rank Aggregation in E-Commerce. (arXiv:2107.08598v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Onoszko_N/0/1/0/all/0/1\">Noa Onoszko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_G/0/1/0/all/0/1\">Gustav Karlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mogren_O/0/1/0/all/0/1\">Olof Mogren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zec_E/0/1/0/all/0/1\">Edvin Listo Zec</a>",
          "description": "We tackle the non-convex problem of learning a personalized deep learning\nmodel in a decentralized setting. More specifically, we study decentralized\nfederated learning, a peer-to-peer setting where data is distributed among many\nclients and where there is no central server to orchestrate the training. In\nreal world scenarios, the data distributions are often heterogeneous between\nclients. Therefore, in this work we study the problem of how to efficiently\nlearn a model in a peer-to-peer system with non-iid client data. We propose a\nmethod named Performance-Based Neighbor Selection (PENS) where clients with\nsimilar data distributions detect each other and cooperate by evaluating their\ntraining losses on each other's data to learn a model suitable for the local\ndata distribution. Our experiments on benchmark datasets show that our proposed\nmethod is able to achieve higher accuracies as compared to strong baselines.",
          "link": "http://arxiv.org/abs/2107.08517",
          "publishedOn": "2021-07-20T02:04:43.618Z",
          "wordCount": 579,
          "title": "Decentralized federated learning of deep neural networks on non-iid data. (arXiv:2107.08517v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08471",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dengshan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chengjun Xie</a>",
          "description": "Artificial neural networks that simulate human achieves great successes. From\nthe perspective of simulating human memory method, we propose a stepped sampler\nbased on the \"repeated input\". We repeatedly inputted data to the LSTM model\nstepwise in a batch. The stepped sampler is used to strengthen the ability of\nfusing the temporal information in LSTM. We tested the stepped sampler on the\nLSTM built-in in PyTorch. Compared with the traditional sampler of PyTorch,\nsuch as sequential sampler, batch sampler, the training loss of the proposed\nstepped sampler converges faster in the training of the model, and the training\nloss after convergence is more stable. Meanwhile, it can maintain a higher test\naccuracy. We quantified the algorithm of the stepped sampler.",
          "link": "http://arxiv.org/abs/2107.08471",
          "publishedOn": "2021-07-20T02:04:43.555Z",
          "wordCount": 556,
          "title": "A stepped sampling method for video detection using LSTM. (arXiv:2107.08471v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdolshah_M/0/1/0/all/0/1\">Majid Abdolshah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_T/0/1/0/all/0/1\">Thommen Karimpanal George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_S/0/1/0/all/0/1\">Santu Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>",
          "description": "Transfer in reinforcement learning is usually achieved through generalisation\nacross tasks. Whilst many studies have investigated transferring knowledge when\nthe reward function changes, they have assumed that the dynamics of the\nenvironments remain consistent. Many real-world RL problems require transfer\namong environments with different dynamics. To address this problem, we propose\nan approach based on successor features in which we model successor feature\nfunctions with Gaussian Processes permitting the source successor features to\nbe treated as noisy measurements of the target successor feature function. Our\ntheoretical analysis proves the convergence of this approach as well as the\nbounded error on modelling successor feature functions with Gaussian Processes\nin environments with both different dynamics and rewards. We demonstrate our\nmethod on benchmark datasets and show that it outperforms current baselines.",
          "link": "http://arxiv.org/abs/2107.08426",
          "publishedOn": "2021-07-20T02:04:43.537Z",
          "wordCount": 569,
          "title": "A New Representation of Successor Features for Transfer across Dissimilar Environments. (arXiv:2107.08426v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Feiyang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1\">Xiang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yanrong Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_A/0/1/0/all/0/1\">Ao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "The exploration/exploitation (E&E) dilemma lies at the core of interactive\nsystems such as online advertising, for which contextual bandit algorithms have\nbeen proposed. Bayesian approaches provide guided exploration with principled\nuncertainty estimation, but the applicability is often limited due to\nover-simplified assumptions. Non-Bayesian bootstrap methods, on the other hand,\ncan apply to complex problems by using deep reward models, but lacks clear\nguidance to the exploration behavior. It still remains largely unsolved to\ndevelop a practical method for complex deep contextual bandits.\n\nIn this paper, we introduce Guided Bootstrap (GuideBoot for short), combining\nthe best of both worlds. GuideBoot provides explicit guidance to the\nexploration behavior by training multiple models over both real samples and\nnoisy samples with fake labels, where the noise is added according to the\npredictive uncertainty. The proposed method is efficient as it can make\ndecisions on-the-fly by utilizing only one randomly chosen model, but is also\neffective as we show that it can be viewed as a non-Bayesian approximation of\nThompson sampling. Moreover, we extend it to an online version that can learn\nsolely from streaming data, which is favored in real applications. Extensive\nexperiments on both synthetic task and large-scale advertising environments\nshow that GuideBoot achieves significant improvements against previous\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.08383",
          "publishedOn": "2021-07-20T02:04:43.515Z",
          "wordCount": 649,
          "title": "GuideBoot: Guided Bootstrap for Deep Contextual Bandits. (arXiv:2107.08383v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Liangqiong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandar_N/0/1/0/all/0/1\">Niranjan Balachandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel L Rubin</a>",
          "description": "Federated learning enables multiple institutions to collaboratively train\nmachine learning models on their local data in a privacy-preserving way.\nHowever, its distributed nature often leads to significant heterogeneity in\ndata distributions across institutions. In this paper, we investigate the\ndeleterious impact of a taxonomy of data heterogeneity regimes on federated\nlearning methods, including quantity skew, label distribution skew, and imaging\nacquisition skew. We show that the performance degrades with the increasing\ndegrees of data heterogeneity. We present several mitigation strategies to\novercome performance drops from data heterogeneity, including weighted average\nfor data quantity skew, weighted loss and batch normalization averaging for\nlabel distribution skew. The proposed optimizations to federated learning\nmethods improve their capability of handling heterogeneity across institutions,\nwhich provides valuable guidance for the deployment of federated learning in\nreal clinical applications.",
          "link": "http://arxiv.org/abs/2107.08371",
          "publishedOn": "2021-07-20T02:04:43.497Z",
          "wordCount": 583,
          "title": "An Experimental Study of Data Heterogeneity in Federated Learning Methods for Medical Imaging. (arXiv:2107.08371v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gruenbacher_S/0/1/0/all/0/1\">Sophie Gruenbacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1\">Mathias Lechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1\">Ramin Hasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henzinger_T/0/1/0/all/0/1\">Thomas A. Henzinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smolka_S/0/1/0/all/0/1\">Scott Smolka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosu_R/0/1/0/all/0/1\">Radu Grosu</a>",
          "description": "We introduce a new stochastic verification algorithm that formally quantifies\nthe behavioral robustness of any time-continuous process formulated as a\ncontinuous-depth model. The algorithm solves a set of global optimization (Go)\nproblems over a given time horizon to construct a tight enclosure (Tube) of the\nset of all process executions starting from a ball of initial states. We call\nour algorithm GoTube. Through its construction, GoTube ensures that the\nbounding tube is conservative up to a desired probability. GoTube is\nimplemented in JAX and optimized to scale to complex continuous-depth models.\nCompared to advanced reachability analysis tools for time-continuous neural\nnetworks, GoTube provably does not accumulate over-approximation errors between\ntime steps and avoids the infamous wrapping effect inherent in symbolic\ntechniques. We show that GoTube substantially outperforms state-of-the-art\nverification tools in terms of the size of the initial ball, speed,\ntime-horizon, task completion, and scalability, on a large set of experiments.\nGoTube is stable and sets the state-of-the-art for its ability to scale up to\ntime horizons well beyond what has been possible before.",
          "link": "http://arxiv.org/abs/2107.08467",
          "publishedOn": "2021-07-20T02:04:43.457Z",
          "wordCount": 631,
          "title": "GoTube: Scalable Stochastic Verification of Continuous-Depth Models. (arXiv:2107.08467v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ampanavos_S/0/1/0/all/0/1\">Spyridon Ampanavos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nourbakhsh_M/0/1/0/all/0/1\">Mehdi Nourbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Chin-Yi Cheng</a>",
          "description": "Structural engineering knowledge can be of significant importance to the\narchitectural design team during the early design phase. However, architects\nand engineers do not typically work together during the conceptual phase; in\nfact, structural engineers are often called late into the process. As a result,\nupdates in the design are more difficult and time-consuming to complete. At the\nsame time, there is a lost opportunity for better design exploration guided by\nstructural feedback. In general, the earlier in the design process the\niteration happens, the greater the benefits in cost efficiency and informed\nde-sign exploration, which can lead to higher-quality creative results. In\norder to facilitate an informed exploration in the early design stage, we\nsuggest the automation of fundamental structural engineering tasks and\nintroduce ApproxiFramer, a Machine Learning-based system for the automatic\ngeneration of structural layouts from building plan sketches in real-time. The\nsystem aims to assist architects by presenting them with feasible structural\nsolutions during the conceptual phase so that they proceed with their design\nwith adequate knowledge of its structural implications. In this paper, we\ndescribe the system and evaluate the performance of a proof-of-concept\nimplementation in the domain of orthogonal, metal, rigid structures. We trained\na Convolutional Neural Net to iteratively generate structural design solutions\nfor sketch-level building plans using a synthetic dataset and achieved an\naverage error of 2.2% in the predicted positions of the columns.",
          "link": "http://arxiv.org/abs/2107.08567",
          "publishedOn": "2021-07-20T02:04:43.390Z",
          "wordCount": 671,
          "title": "Structural Design Recommendations in the Early Design Phase using Machine Learning. (arXiv:2107.08567v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiyiwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiqi Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Q/0/1/0/all/0/1\">Qi Long</a>",
          "description": "Bayesian neural network (BNN) allows for uncertainty quantification in\nprediction, offering an advantage over regular neural networks that has not\nbeen explored in the differential privacy (DP) framework. We fill this\nimportant gap by leveraging recent development in Bayesian deep learning and\nprivacy accounting to offer a more precise analysis of the trade-off between\nprivacy and accuracy in BNN. We propose three DP-BNNs that characterize the\nweight uncertainty for the same network architecture in distinct ways, namely\nDP-SGLD (via the noisy gradient method), DP-BBP (via changing the parameters of\ninterest) and DP-MC Dropout (via the model architecture). Interestingly, we\nshow a new equivalence between DP-SGD and DP-SGLD, implying that some\nnon-Bayesian DP training naturally allows for uncertainty quantification.\nHowever, the hyperparameters such as learning rate and batch size, can have\ndifferent or even opposite effects in DP-SGD and DP-SGLD.\n\nExtensive experiments are conducted to compare DP-BNNs, in terms of privacy\nguarantee, prediction accuracy, uncertainty quantification, calibration,\ncomputation speed, and generalizability to network architecture. As a result,\nwe observe a new tradeoff between the privacy and the reliability. When\ncompared to non-DP and non-Bayesian approaches, DP-SGLD is remarkably accurate\nunder strong privacy guarantee, demonstrating the great potential of DP-BNN in\nreal-world tasks.",
          "link": "http://arxiv.org/abs/2107.08461",
          "publishedOn": "2021-07-20T02:04:43.317Z",
          "wordCount": 645,
          "title": "Differentially Private Bayesian Neural Networks on Accuracy, Privacy and Reliability. (arXiv:2107.08461v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08514",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kokate_P/0/1/0/all/0/1\">Pranali Kokate</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pancholi_S/0/1/0/all/0/1\">Sidharth Pancholi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joshi_A/0/1/0/all/0/1\">Amit M. Joshi</a>",
          "description": "The Brain-Computer Interface system is a profoundly developing area of\nexperimentation for Motor activities which plays vital role in decoding\ncognitive activities. Classification of Cognitive-Motor Imagery activities from\nEEG signals is a critical task. Hence proposed a unique algorithm for\nclassifying left/right-hand movements by utilizing Multi-layer Perceptron\nNeural Network. Handcrafted statistical Time domain and Power spectral density\nfrequency domain features were extracted and obtained a combined accuracy of\n96.02%. Results were compared with the deep learning framework. In addition to\naccuracy, Precision, F1-Score, and recall was considered as the performance\nmetrics. The intervention of unwanted signals contaminates the EEG signals\nwhich influence the performance of the algorithm. Therefore, a novel approach\nwas approached to remove the artifacts using Independent Components Analysis\nwhich boosted the performance. Following the selection of appropriate feature\nvectors that provided acceptable accuracy. The same method was used on all nine\nsubjects. As a result, intra-subject accuracy was obtained for 9 subjects\n94.72%. The results show that the proposed approach would be useful to classify\nthe upper limb movements accurately.",
          "link": "http://arxiv.org/abs/2107.08514",
          "publishedOn": "2021-07-20T02:04:43.244Z",
          "wordCount": 639,
          "title": "Classification of Upper Arm Movements from EEG signals using Machine Learning with ICA Analysis. (arXiv:2107.08514v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tahmasebian_F/0/1/0/all/0/1\">Farnaz Tahmasebian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Li Xiong</a>",
          "description": "Federated learning is a prominent framework that enables clients (e.g.,\nmobile devices or organizations) to train a collaboratively global model under\na central server's orchestration while keeping local training datasets'\nprivacy. However, the aggregation step in federated learning is vulnerable to\nadversarial attacks as the central server cannot manage clients' behavior.\nTherefore, the global model's performance and convergence of the training\nprocess will be affected under such attacks.To mitigate this vulnerability\nissue, we propose a novel robust aggregation algorithm inspired by the truth\ninference methods in crowdsourcing via incorporating the worker's reliability\ninto aggregation. We evaluate our solution on three real-world datasets with a\nvariety of machine learning models. Experimental results show that our solution\nensures robust federated learning and is resilient to various types of attacks,\nincluding noisy data attacks, Byzantine attacks, and label flipping attacks.",
          "link": "http://arxiv.org/abs/2107.08402",
          "publishedOn": "2021-07-20T02:04:43.220Z",
          "wordCount": 582,
          "title": "RobustFed: A Truth Inference Approach for Robust Federated Learning. (arXiv:2107.08402v1 [cs.LG])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}