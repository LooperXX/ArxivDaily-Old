{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2005.14408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muralidharan_D/0/1/0/all/0/1\">Deepak Muralidharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1\">Joel Ruben Antony Moniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Sida Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_J/0/1/0/all/0/1\">Justine Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulman_S/0/1/0/all/0/1\">Stephen Pulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1\">Atish Kothari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1\">Ray Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yinying Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaul_V/0/1/0/all/0/1\">Vivek Kaul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1\">Mubarak Seyed Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_G/0/1/0/all/0/1\">Gang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dun_N/0/1/0/all/0/1\">Nan Dun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yidan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+O_A/0/1/0/all/0/1\">Andy O</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitkara_P/0/1/0/all/0/1\">Pooja Chitkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Alkesh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tayal_K/0/1/0/all/0/1\">Kushal Tayal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Roger Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grasch_P/0/1/0/all/0/1\">Peter Grasch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1\">Jason D. Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>",
          "description": "Named Entity Recognition (NER) and Entity Linking (EL) play an essential role\nin voice assistant interaction, but are challenging due to the special\ndifficulties associated with spoken user queries. In this paper, we propose a\nnovel architecture that jointly solves the NER and EL tasks by combining them\nin a joint reranking module. We show that our proposed framework improves NER\naccuracy by up to 3.13% and EL accuracy by up to 3.6% in F1 score. The features\nused also lead to better accuracies in other natural language understanding\ntasks, such as domain classification and semantic parsing.",
          "link": "http://arxiv.org/abs/2005.14408",
          "publishedOn": "2021-08-11T01:55:21.360Z",
          "wordCount": 638,
          "title": "Noise Robust Named Entity Understanding for Voice Assistants. (arXiv:2005.14408v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karami_A/0/1/0/all/0/1\">Amir Karami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Michael Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldschmidt_B/0/1/0/all/0/1\">Bailey Goldschmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyajieff_H/0/1/0/all/0/1\">Hannah R. Boyajieff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najafabadi_M/0/1/0/all/0/1\">Mahdi M. Najafabadi</a>",
          "description": "Public response to COVID-19 vaccines is the key success factor to control the\nCOVID-19 pandemic. To understand the public response, there is a need to\nexplore public opinion. Traditional surveys are expensive and time-consuming,\naddress limited health topics, and obtain small-scale data. Twitter can provide\na great opportunity to understand public opinion regarding COVID-19 vaccines.\nThe current study proposes an approach using computational and human coding\nmethods to collect and analyze a large number of tweets to provide a wider\nperspective on the COVID-19 vaccine. This study identifies the sentiment of\ntweets and their temporal trend, discovers major topics, compares topics of\nnegative and non-negative tweets, and discloses top topics of negative and\nnon-negative tweets. Our findings show that the negative sentiment regarding\nthe COVID-19 vaccine had a decreasing trend between November 2020 and February\n2021. We found Twitter users have discussed a wide range of topics from\nvaccination sites to the 2020 U.S. election between November 2020 and February\n2021. The findings show that there was a significant difference between\nnegative and non-negative tweets regarding the weight of most topics. Our\nresults also indicate that the negative and non-negative tweets had different\ntopic priorities and focuses.",
          "link": "http://arxiv.org/abs/2108.04816",
          "publishedOn": "2021-08-11T01:55:21.091Z",
          "wordCount": 689,
          "title": "COVID-19 Vaccine and Social Media: Exploring Emotions and Discussions on Twitter. (arXiv:2108.04816v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dunbar_E/0/1/0/all/0/1\">Ewan Dunbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_M/0/1/0/all/0/1\">Mathieu Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilakis_N/0/1/0/all/0/1\">Nicolas Hamilakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seyssel_M/0/1/0/all/0/1\">Maureen de Seyssel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roze_P/0/1/0/all/0/1\">Patricia Roz&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riviere_M/0/1/0/all/0/1\">Morgane Rivi&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>",
          "description": "We present the Zero Resource Speech Challenge 2021, which asks participants\nto learn a language model directly from audio, without any text or labels. The\nchallenge is based on the Libri-light dataset, which provides up to 60k hours\nof audio from English audio books without any associated text. We provide a\npipeline baseline system consisting on an encoder based on contrastive\npredictive coding (CPC), a quantizer ($k$-means) and a standard language model\n(BERT or LSTM). The metrics evaluate the learned representations at the\nacoustic (ABX discrimination), lexical (spot-the-word), syntactic\n(acceptability judgment) and semantic levels (similarity judgment). We present\nan overview of the eight submitted systems from four groups and discuss the\nmain results.",
          "link": "http://arxiv.org/abs/2104.14700",
          "publishedOn": "2021-08-11T01:55:21.085Z",
          "wordCount": 607,
          "title": "The Zero Resource Speech Challenge 2021: Spoken language modelling. (arXiv:2104.14700v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Songlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>",
          "description": "We propose a headed span-based method for projective dependency parsing. In a\nprojective tree, the subtree rooted at each word occurs in a contiguous\nsequence (i.e., span) in the surface order, we call the span-headword pair\n\\textit{headed span}. In this view, a projective tree can be regarded as a\ncollection of headed spans. It is similar to the case in constituency parsing\nsince a constituency tree can be regarded as a collection of constituent spans.\nSpan-based methods decompose the score of a constituency tree sorely into the\nscore of constituent spans and use the CYK algorithm for global training and\nexact inference, obtaining state-of-the-art results in constituency parsing.\nInspired by them, we decompose the score of a dependency tree into the score of\nheaded spans. We use neural networks to score headed spans and design a novel\n$O(n^3)$ dynamic programming algorithm to enable global training and exact\ninference. We evaluate our method on PTB, CTB, and UD, achieving\nstate-of-the-art or comparable results.",
          "link": "http://arxiv.org/abs/2108.04750",
          "publishedOn": "2021-08-11T01:55:21.072Z",
          "wordCount": 583,
          "title": "Headed Span-Based Projective Dependency Parsing. (arXiv:2108.04750v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.12007",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yubo Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_P/0/1/0/all/0/1\">Pearl Pu</a>",
          "description": "Humor recognition has been widely studied as a text classification problem\nusing data-driven approaches. However, most existing work does not examine the\nactual joke mechanism to understand humor. We break down any joke into two\ndistinct components: the set-up and the punchline, and further explore the\nspecial relationship between them. Inspired by the incongruity theory of humor,\nwe model the set-up as the part developing semantic uncertainty, and the\npunchline disrupting audience expectations. With increasingly powerful language\nmodels, we were able to feed the set-up along with the punchline into the GPT-2\nlanguage model, and calculate the uncertainty and surprisal values of the\njokes. By conducting experiments on the SemEval 2021 Task 7 dataset, we found\nthat these two features have better capabilities of telling jokes from\nnon-jokes, compared with existing baselines.",
          "link": "http://arxiv.org/abs/2012.12007",
          "publishedOn": "2021-08-11T01:55:21.049Z",
          "wordCount": 607,
          "title": "Uncertainty and Surprisal Jointly Deliver the Punchline: Exploiting Incongruity-Based Features for Humor Recognition. (arXiv:2012.12007v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kojima_N/0/1/0/all/0/1\">Noriyuki Kojima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1\">Alane Suhr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>",
          "description": "We study continual learning for natural language instruction generation, by\nobserving human users' instruction execution. We focus on a collaborative\nscenario, where the system both acts and delegates tasks to human users using\nnatural language. We compare user execution of generated instructions to the\noriginal system intent as an indication to the system's success communicating\nits intent. We show how to use this signal to improve the system's ability to\ngenerate instructions via contextual bandit learning. In interaction with real\nusers, our system demonstrates dramatic improvements in its ability to generate\nlanguage over time.",
          "link": "http://arxiv.org/abs/2108.04812",
          "publishedOn": "2021-08-11T01:55:21.042Z",
          "wordCount": 553,
          "title": "Continual Learning for Grounded Instruction Generation by Observing Human Following Behavior. (arXiv:2108.04812v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Onta%7Bn%7D%7Bo%7Dn_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cvicek_V/0/1/0/all/0/1\">Vaclav Cvicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_Z/0/1/0/all/0/1\">Zachary Fisher</a>",
          "description": "Several studies have reported the inability of Transformer models to\ngeneralize compositionally, a key type of generalization in many NLP tasks such\nas semantic parsing. In this paper we explore the design space of Transformer\nmodels showing that the inductive biases given to the model by several design\ndecisions significantly impact compositional generalization. Through this\nexploration, we identified Transformer configurations that generalize\ncompositionally significantly better than previously reported in the literature\nin a diverse set of compositional tasks, and that achieve state-of-the-art\nresults in a semantic parsing compositional generalization benchmark (COGS),\nand a string edit operation composition benchmark (PCFG).",
          "link": "http://arxiv.org/abs/2108.04378",
          "publishedOn": "2021-08-11T01:55:21.037Z",
          "wordCount": 528,
          "title": "Making Transformers Solve Compositional Tasks. (arXiv:2108.04378v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hande_A/0/1/0/all/0/1\">Adeep Hande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1\">Ruba Priyadharshini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampath_A/0/1/0/all/0/1\">Anbukkarasi Sampath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thamburaj_K/0/1/0/all/0/1\">Kingston Pal Thamburaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandran_P/0/1/0/all/0/1\">Prabakaran Chandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>",
          "description": "Numerous methods have been developed to monitor the spread of negativity in\nmodern years by eliminating vulgar, offensive, and fierce comments from social\nmedia platforms. However, there are relatively lesser amounts of study that\nconverges on embracing positivity, reinforcing supportive and reassuring\ncontent in online forums. Consequently, we propose creating an English-Kannada\nHope speech dataset, KanHope and comparing several experiments to benchmark the\ndataset. The dataset consists of 6,176 user-generated comments in code mixed\nKannada scraped from YouTube and manually annotated as bearing hope speech or\nNot-hope speech. In addition, we introduce DC-BERT4HOPE, a dual-channel model\nthat uses the English translation of KanHope for additional training to promote\nhope speech detection. The approach achieves a weighted F1-score of 0.756,\nbettering other models. Henceforth, KanHope aims to instigate research in\nKannada while broadly promoting researchers to take a pragmatic approach\ntowards online content that encourages, positive, and supportive.",
          "link": "http://arxiv.org/abs/2108.04616",
          "publishedOn": "2021-08-11T01:55:21.030Z",
          "wordCount": 581,
          "title": "Hope Speech detection in under-resourced Kannada language. (arXiv:2108.04616v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_T/0/1/0/all/0/1\">Teakgyu Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1\">Mingi Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonseok Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1\">Daehyun Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungrae Park</a>",
          "description": "Understanding documents from their visual snapshots is an emerging problem\nthat requires both advanced computer vision and NLP methods. The recent advance\nin OCR enables the accurate recognition of text blocks, yet it is still\nchallenging to extract key information from documents due to the diversity of\ntheir layouts. Although recent studies on pre-trained language models show the\nimportance of incorporating layout information on this task, the conjugation of\ntexts and their layouts still follows the style of BERT optimized for\nunderstanding the 1D text. This implies there is room for further improvement\nconsidering the 2D nature of text layouts. This paper introduces a pre-trained\nlanguage model, BERT Relying On Spatiality (BROS), which effectively utilizes\nthe information included in individual text blocks and their layouts.\nSpecifically, BROS encodes spatial information by utilizing relative positions\nand learns spatial dependencies between OCR blocks with a novel area-masking\nstrategy. These two novel approaches lead to an efficient encoding of spatial\nlayout information highlighted by the robust performance of BROS under\nlow-resource environments. We also introduce a general-purpose parser that can\nbe combined with BROS to extract key information even when there is no order\ninformation between text blocks. BROS shows its superiority on four public\nbenchmarks---FUNSD, SROIE*, CORD, and SciTSR---and its robustness in practical\ncases where order information of text blocks is not available. Further\nexperiments with a varying number of training examples demonstrate the high\ntraining efficiency of our approach. Our code will be open to the public.",
          "link": "http://arxiv.org/abs/2108.04539",
          "publishedOn": "2021-08-11T01:55:21.011Z",
          "wordCount": 689,
          "title": "BROS: A Layout-Aware Pre-trained Language Model for Understanding Documents. (arXiv:2108.04539v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.00260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wallbridge_S/0/1/0/all/0/1\">Sarenne Wallbridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_P/0/1/0/all/0/1\">Peter Bell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Catherine Lai</a>",
          "description": "People convey information extremely effectively through spoken interaction\nusing multiple channels of information transmission: the lexical channel of\nwhat is said, and the non-lexical channel of how it is said. We propose\nstudying human perception of spoken communication as a means to better\nunderstand how information is encoded across these channels, focusing on the\nquestion 'What characteristics of communicative context affect listener's\nexpectations of speech?'. To investigate this, we present a novel behavioural\ntask testing whether listeners can discriminate between the true utterance in a\ndialogue and utterances sampled from other contexts with the same lexical\ncontent. We characterize how perception - and subsequent discriminative\ncapability - is affected by different degrees of additional contextual\ninformation across both the lexical and non-lexical channel of speech. Results\ndemonstrate that people can effectively discriminate between different prosodic\nrealisations, that non-lexical context is informative, and that this channel\nprovides more salient information than the lexical channel, highlighting the\nimportance of the non-lexical channel in spoken interaction.",
          "link": "http://arxiv.org/abs/2105.00260",
          "publishedOn": "2021-08-11T01:55:20.998Z",
          "wordCount": 648,
          "title": "It's not what you said, it's how you said it: discriminative perception of speech as a multichannel communication system. (arXiv:2105.00260v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yubo Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_P/0/1/0/all/0/1\">Pearl Pu</a>",
          "description": "In this paper, we give an overview of commonsense reasoning in natural\nlanguage processing, which requires a deeper understanding of the contexts and\nusually involves inference over implicit external knowledge. We first review\nsome popular commonsense knowledge bases and commonsense reasoning benchmarks,\nbut give more emphasis on the methodologies, including recent approaches that\naim at solving some general natural language problems that take advantage of\nexternal knowledge bases. Finally, we discuss some future directions in pushing\nthe boundary of commonsense reasoning in natural language processing.",
          "link": "http://arxiv.org/abs/2108.04674",
          "publishedOn": "2021-08-11T01:55:20.988Z",
          "wordCount": 527,
          "title": "How Commonsense Knowledge Helps with Natural Language Tasks: A Survey of Recent Resources and Methodologies. (arXiv:2108.04674v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1\">Andrei-Marius Avram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1\">Vasile Pais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1\">Dan Tufis</a>",
          "description": "EuroVoc is a multilingual thesaurus that was built for organizing the\nlegislative documentary of the European Union institutions. It contains\nthousands of categories at different levels of specificity and its descriptors\nare targeted by legal texts in almost thirty languages. In this work we propose\na unified framework for EuroVoc classification on 22 languages by fine-tuning\nmodern Transformer-based pretrained language models. We study extensively the\nperformance of our trained models and show that they significantly improve the\nresults obtained by a similar tool - JEX - on the same dataset. The code and\nthe fine-tuned models were open sourced, together with a programmatic interface\nthat eases the process of loading the weights of a trained model and of\nclassifying a new document.",
          "link": "http://arxiv.org/abs/2108.01139",
          "publishedOn": "2021-08-11T01:55:20.980Z",
          "wordCount": 594,
          "title": "PyEuroVoc: A Tool for Multilingual Legal Document Classification with EuroVoc Descriptors. (arXiv:2108.01139v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Shruti Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>",
          "description": "Comparing research papers is a conventional method to demonstrate progress in\nexperimental research. We present COMPARE, a taxonomy and a dataset of\ncomparison discussions in peer reviews of research papers in the domain of\nexperimental deep learning. From a thorough observation of a large set of\nreview sentences, we build a taxonomy of categories in comparison discussions\nand present a detailed annotation scheme to analyze this. Overall, we annotate\n117 reviews covering 1,800 sentences. We experiment with various methods to\nidentify comparison sentences in peer reviews and report a maximum F1 Score of\n0.49. We also pretrain two language models specifically on ML, NLP, and CV\npaper abstracts and reviews to learn informative representations of peer\nreviews. The annotated dataset and the pretrained models are available at\nhttps://github.com/shruti-singh/COMPARE .",
          "link": "http://arxiv.org/abs/2108.04366",
          "publishedOn": "2021-08-11T01:55:20.962Z",
          "wordCount": 571,
          "title": "COMPARE: A Taxonomy and Dataset of Comparison Discussions in Peer Reviews. (arXiv:2108.04366v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04324",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bensaid_E/0/1/0/all/0/1\">Eden Bensaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_M/0/1/0/all/0/1\">Mauro Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1\">Benjamin Hoover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1\">Hendrik Strobelt</a>",
          "description": "Storytelling is an open-ended task that entails creative thinking and\nrequires a constant flow of ideas. Natural language generation (NLG) for\nstorytelling is especially challenging because it requires the generated text\nto follow an overall theme while remaining creative and diverse to engage the\nreader. In this work, we introduce a system and a web-based demo, FairyTailor,\nfor human-in-the-loop visual story co-creation. Users can create a cohesive\nchildren's fairytale by weaving generated texts and retrieved images with their\ninput. FairyTailor adds another modality and modifies the text generation\nprocess to produce a coherent and creative sequence of text and images. To our\nknowledge, this is the first dynamic tool for multimodal story generation that\nallows interactive co-formation of both texts and images. It allows users to\ngive feedback on co-created stories and share their results.",
          "link": "http://arxiv.org/abs/2108.04324",
          "publishedOn": "2021-08-11T01:55:20.955Z",
          "wordCount": 592,
          "title": "FairyTailor: A Multimodal Generative Framework for Storytelling. (arXiv:2108.04324v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaoyan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shizhu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>",
          "description": "Conventional Intent Detection (ID) models are usually trained offline, which\nrelies on a fixed dataset and a predefined set of intent classes. However, in\nreal-world applications, online systems usually involve continually emerging\nnew user intents, which pose a great challenge to the offline training\nparadigm. Recently, lifelong learning has received increasing attention and is\nconsidered to be the most promising solution to this challenge. In this paper,\nwe propose Lifelong Intent Detection (LID), which continually trains an ID\nmodel on new data to learn newly emerging intents while avoiding\ncatastrophically forgetting old data. Nevertheless, we find that existing\nlifelong learning methods usually suffer from a serious imbalance between old\nand new data in the LID task. Therefore, we propose a novel lifelong learning\nmethod, Multi-Strategy Rebalancing (MSR), which consists of cosine\nnormalization, hierarchical knowledge distillation, and inter-class margin loss\nto alleviate the multiple negative effects of the imbalance problem.\nExperimental results demonstrate the effectiveness of our method, which\nsignificantly outperforms previous state-of-the-art lifelong learning methods\non the ATIS, SNIPS, HWU64, and CLINC150 benchmarks.",
          "link": "http://arxiv.org/abs/2108.04445",
          "publishedOn": "2021-08-11T01:55:20.941Z",
          "wordCount": 603,
          "title": "Lifelong Intent Detection via Multi-Strategy Rebalancing. (arXiv:2108.04445v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koh_A/0/1/0/all/0/1\">Andrew Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chng_E/0/1/0/all/0/1\">Eng Siong Chng</a>",
          "description": "In this paper, we examine the use of Transfer Learning using Pretrained Audio\nNeural Networks (PANNs), and propose an architecture that is able to better\nleverage the acoustic features provided by PANNs for the Automated Audio\nCaptioning Task. We also introduce a novel self-supervised objective,\nReconstruction Latent Space Similarity Regularization (RLSSR). The RLSSR module\nsupplements the training of the model by minimizing the similarity between the\nencoder and decoder embedding. The combination of both methods allows us to\nsurpass state of the art results by a significant margin on the Clotho dataset\nacross several metrics and benchmarks.",
          "link": "http://arxiv.org/abs/2108.04692",
          "publishedOn": "2021-08-11T01:55:20.930Z",
          "wordCount": 551,
          "title": "Automated Audio Captioning using Transfer Learning and Reconstruction Latent Space Similarity Regularization. (arXiv:2108.04692v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.00502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1\">Gautam Kishore Shahi</a>",
          "description": "In this paper, we present a semi-automated framework called AMUSED for\ngathering multi-modal annotated data from the multiple social media platforms.\nThe framework is designed to mitigate the issues of collecting and annotating\nsocial media data by cohesively combining machine and human in the data\ncollection process. From a given list of the articles from professional news\nmedia or blog, AMUSED detects links to the social media posts from news\narticles and then downloads contents of the same post from the respective\nsocial media platform to gather details about that specific post. The framework\nis capable of fetching the annotated data from multiple platforms like Twitter,\nYouTube, Reddit. The framework aims to reduce the workload and problems behind\nthe data annotation from the social media platforms. AMUSED can be applied in\nmultiple application domains, as a use case, we have implemented the framework\nfor collecting COVID-19 misinformation data from different social media\nplatforms.",
          "link": "http://arxiv.org/abs/2010.00502",
          "publishedOn": "2021-08-11T01:55:20.915Z",
          "wordCount": 670,
          "title": "AMUSED: An Annotation Framework of Multi-modal Social Media Data. (arXiv:2010.00502v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pingyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Meng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yadao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>",
          "description": "Pre-trained models for programming languages have proven their significant\nvalues in various code-related tasks, such as code search, code clone\ndetection, and code translation. Currently, most pre-trained models treat a\ncode snippet as a sequence of tokens or only focus on the data flow between\ncode identifiers. However, rich code syntax and hierarchy are ignored which can\nprovide important structure information and semantic rules of codes to help\nenhance code representations. In addition, although the BERT-based code\npre-trained models achieve high performance on many downstream tasks, the\nnative derived sequence representations of BERT are proven to be of\nlow-quality, it performs poorly on code matching and similarity tasks. To\naddress these problems, we propose CLSEBERT, a Constrastive Learning Framework\nfor Syntax Enhanced Code Pre-Trained Model, to deal with various code\nintelligence tasks. In the pre-training stage, we consider the code syntax and\nhierarchy contained in the Abstract Syntax Tree (AST) and leverage the\nconstrastive learning to learn noise-invariant code representations. Besides\nthe masked language modeling (MLM), we also introduce two novel pre-training\nobjectives. One is to predict the edges between nodes in the abstract syntax\ntree, and the other is to predict the types of code tokens. Through extensive\nexperiments on four code intelligence tasks, we successfully show the\neffectiveness of our proposed model.",
          "link": "http://arxiv.org/abs/2108.04556",
          "publishedOn": "2021-08-11T01:55:20.907Z",
          "wordCount": 673,
          "title": "CLSEBERT: Contrastive Learning for Syntax Enhanced Code Pre-Trained Model. (arXiv:2108.04556v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04718",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eikema_B/0/1/0/all/0/1\">Bryan Eikema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1\">Wilker Aziz</a>",
          "description": "In neural machine translation (NMT), we search for the mode of the model\ndistribution to form predictions. The mode as well as other high probability\ntranslations found by beam search have been shown to often be inadequate in a\nnumber of ways. This prevents practitioners from improving translation quality\nthrough better search, as these idiosyncratic translations end up being\nselected by the decoding algorithm, a problem known as the beam search curse.\nRecently, a sampling-based approximation to minimum Bayes risk (MBR) decoding\nhas been proposed as an alternative decision rule for NMT that would likely not\nsuffer from the same problems. We analyse this approximation and establish that\nit has no equivalent to the beam search curse, i.e. better search always leads\nto better translations. We also design different approximations aimed at\ndecoupling the cost of exploration from the cost of robust estimation of\nexpected utility. This allows for exploration of much larger hypothesis spaces,\nwhich we show to be beneficial. We also show that it can be beneficial to make\nuse of strategies like beam search and nucleus sampling to construct hypothesis\nspaces efficiently. We show on three language pairs (English into and from\nGerman, Romanian, and Nepali) that MBR can improve upon beam search with\nmoderate computation.",
          "link": "http://arxiv.org/abs/2108.04718",
          "publishedOn": "2021-08-11T01:55:20.898Z",
          "wordCount": 638,
          "title": "Sampling-Based Minimum Bayes Risk Decoding for Neural Machine Translation. (arXiv:2108.04718v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaoda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>",
          "description": "Multi-head attention, a collection of several attention mechanisms that\nindependently attend to different parts of the input, is the key ingredient in\nthe Transformer (Vaswaniet al., 2017). Recent work has shown, however, that a\nlarge proportion of the heads in a Transformer's multi-head attention mechanism\ncan be safely pruned away without significantly harming the performance of the\nmodel; such pruning leads to models that are noticeably smaller and faster in\npractice. Our work introduces a new head pruning technique that we term\ndifferentiable subset pruning. Intuitively, our method learns per-head\nimportance variables and then enforces a user-specified hard constraint on the\nnumber of unpruned heads. The importance variables are learned via stochastic\ngradient descent. We conduct experiments on natural language inference and\nmachine translation; we show that differentiable subset pruning performs\ncomparably or better than Voita et al. (2019) while offering the same exact\ncontrol over the number of heads as Michel et al. (2019).",
          "link": "http://arxiv.org/abs/2108.04657",
          "publishedOn": "2021-08-11T01:55:20.866Z",
          "wordCount": 581,
          "title": "Differentiable Subset Pruning of Transformer Heads. (arXiv:2108.04657v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>",
          "description": "Current open-domain question answering systems often follow a\nRetriever-Reader architecture, where the retriever first retrieves relevant\npassages and the reader then reads the retrieved passages to form an answer. In\nthis paper, we propose a simple and effective passage reranking method, named\nReader-guIDEd Reranker (RIDER), which does not involve training and reranks the\nretrieved passages solely based on the top predictions of the reader before\nreranking. We show that RIDER, despite its simplicity, achieves 10 to 20\nabsolute gains in top-1 retrieval accuracy and 1 to 4 Exact Match (EM) gains\nwithout refining the retriever or reader. In addition, RIDER, without any\ntraining, outperforms state-of-the-art transformer-based supervised rerankers.\nRemarkably, RIDER achieves 48.3 EM on the Natural Questions dataset and 66.4 EM\non the TriviaQA dataset when only 1,024 tokens (7.8 passages on average) are\nused as the reader input after passage reranking.",
          "link": "http://arxiv.org/abs/2101.00294",
          "publishedOn": "2021-08-10T02:00:07.583Z",
          "wordCount": 657,
          "title": "Rider: Reader-Guided Passage Reranking for Open-Domain Question Answering. (arXiv:2101.00294v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1\">Tianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>",
          "description": "Neural text generation models are typically trained by maximizing\nlog-likelihood with the sequence cross entropy loss, which encourages an exact\ntoken-by-token match between a target sequence with a generated sequence. Such\ntraining objective is sub-optimal when the target sequence not perfect, e.g.,\nwhen the target sequence is corrupted with noises, or when only weak sequence\nsupervision is available. To address this challenge, we propose a novel\nEdit-Invariant Sequence Loss (EISL), which computes the matching loss of a\ntarget n-gram with all n-grams in the generated sequence. EISL draws\ninspirations from convolutional networks (ConvNets) which are shift-invariant\nto images, hence is robust to the shift of n-grams to tolerate edits in the\ntarget sequences. Moreover, the computation of EISL is essentially a\nconvolution operation with target n-grams as kernels, which is easy to\nimplement with existing libraries. To demonstrate the effectiveness of EISL, we\nconduct experiments on three tasks: machine translation with noisy target\nsequences, unsupervised text style transfer, and non-autoregressive machine\ntranslation. Experimental results show our method significantly outperforms\ncross entropy loss on these three tasks.",
          "link": "http://arxiv.org/abs/2106.15078",
          "publishedOn": "2021-08-10T02:00:07.546Z",
          "wordCount": 671,
          "title": "Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation. (arXiv:2106.15078v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.15082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1\">Rui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Le Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xianyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiamang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Di Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>",
          "description": "Mixture-of-Experts (MoE) models can achieve promising results with outrageous\nlarge amount of parameters but constant computation cost, and thus it has\nbecome a trend in model scaling. Still it is a mystery how MoE layers bring\nquality gains by leveraging the parameters with sparse activation. In this\nwork, we investigate several key factors in sparse expert models. We observe\nthat load imbalance may not be a significant problem affecting model quality,\ncontrary to the perspectives of recent studies, while the number of sparsely\nactivated experts $k$ and expert capacity $C$ in top-$k$ routing can\nsignificantly make a difference in this context. Furthermore, we take a step\nforward to propose a simple method called expert prototyping that splits\nexperts into different prototypes and applies $k$ top-$1$ routing. This\nstrategy improves the model quality but maintains constant computational costs,\nand our further exploration on extremely large-scale models reflects that it is\nmore effective in training larger models. We push the model scale to over $1$\ntrillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in\ncomparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model\nachieves substantial speedup in convergence over the same-size baseline.",
          "link": "http://arxiv.org/abs/2105.15082",
          "publishedOn": "2021-08-10T02:00:07.537Z",
          "wordCount": 714,
          "title": "M6-T: Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08553",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>",
          "description": "We propose Generation-Augmented Retrieval (GAR) for answering open-domain\nquestions, which augments a query through text generation of heuristically\ndiscovered relevant contexts without external resources as supervision. We\ndemonstrate that the generated contexts substantially enrich the semantics of\nthe queries and GAR with sparse representations (BM25) achieves comparable or\nbetter performance than state-of-the-art dense retrieval methods such as DPR.\nWe show that generating diverse contexts for a query is beneficial as fusing\ntheir results consistently yields better retrieval accuracy. Moreover, as\nsparse and dense representations are often complementary, GAR can be easily\ncombined with DPR to achieve even better performance. GAR achieves\nstate-of-the-art performance on Natural Questions and TriviaQA datasets under\nthe extractive QA setup when equipped with an extractive reader, and\nconsistently outperforms other retrieval methods when the same generative\nreader is used.",
          "link": "http://arxiv.org/abs/2009.08553",
          "publishedOn": "2021-08-10T02:00:07.502Z",
          "wordCount": 624,
          "title": "Generation-Augmented Retrieval for Open-domain Question Answering. (arXiv:2009.08553v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kuo Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>",
          "description": "Recent work has shown success in incorporating pre-trained models like BERT\nto improve NLP systems. However, existing pre-trained models lack of causal\nknowledge which prevents today's NLP systems from thinking like humans. In this\npaper, we investigate the problem of injecting causal knowledge into\npre-trained models. There are two fundamental problems: 1) how to collect\nvarious granularities of causal pairs from unstructured texts; 2) how to\neffectively inject causal knowledge into pre-trained models. To address these\nissues, we extend the idea of CausalBERT from previous studies, and conduct\nexperiments on various datasets to evaluate its effectiveness. In addition, we\nadopt a regularization-based method to preserve the already learned knowledge\nwith an extra regularization term while injecting causal knowledge. Extensive\nexperiments on 7 datasets, including four causal pair classification tasks, two\ncausal QA tasks and a causal inference task, demonstrate that CausalBERT\ncaptures rich causal knowledge and outperforms all pre-trained models-based\nstate-of-the-art methods, achieving a new causal inference benchmark.",
          "link": "http://arxiv.org/abs/2107.09852",
          "publishedOn": "2021-08-10T02:00:07.477Z",
          "wordCount": 625,
          "title": "CausalBERT: Injecting Causal Knowledge Into Pre-trained Models with Minimal Supervision. (arXiv:2107.09852v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lal_Y/0/1/0/all/0/1\">Yash Kumar Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambers_N/0/1/0/all/0/1\">Nathanael Chambers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1\">Raymond Mooney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>",
          "description": "Answering questions about why characters perform certain actions is central\nto understanding and reasoning about narratives. Despite recent progress in QA,\nit is not clear if existing models have the ability to answer \"why\" questions\nthat may require commonsense knowledge external to the input narrative. In this\nwork, we introduce TellMeWhy, a new crowd-sourced dataset that consists of more\nthan 30k questions and free-form answers concerning why characters in short\nnarratives perform the actions described. For a third of this dataset, the\nanswers are not present within the narrative. Given the limitations of\nautomated evaluation for this task, we also present a systematized human\nevaluation interface for this dataset. Our evaluation of state-of-the-art\nmodels show that they are far below human performance on answering such\nquestions. They are especially worse on questions whose answers are external to\nthe narrative, thus providing a challenge for future QA and narrative\nunderstanding research.",
          "link": "http://arxiv.org/abs/2106.06132",
          "publishedOn": "2021-08-10T02:00:07.470Z",
          "wordCount": 630,
          "title": "TellMeWhy: A Dataset for Answering Why-Questions in Narratives. (arXiv:2106.06132v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01384",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leow_C/0/1/0/all/0/1\">Chee Siang Leow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kobayashi_A/0/1/0/all/0/1\">Akio Kobayashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Utsuro_T/0/1/0/all/0/1\">Takehito Utsuro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nishizaki_H/0/1/0/all/0/1\">Hiromitsu Nishizaki</a>",
          "description": "This paper describes the ExKaldi-RT online automatic speech recognition (ASR)\ntoolkit that is implemented based on the Kaldi ASR toolkit and Python language.\nExKaldi-RT provides tools for building online recognition pipelines. While\nsimilar tools are available built on Kaldi, a key feature of ExKaldi-RT that it\nworks on Python, which has an easy-to-use interface that allows online ASR\nsystem developers to develop original research, such as by applying neural\nnetwork-based signal processing and by decoding model trained with deep\nlearning frameworks. We performed benchmark experiments on the minimum\nLibriSpeech corpus, and it showed that ExKaldi-RT could achieve competitive ASR\nperformance in real-time recognition.",
          "link": "http://arxiv.org/abs/2104.01384",
          "publishedOn": "2021-08-10T02:00:07.428Z",
          "wordCount": 581,
          "title": "ExKaldi-RT: A Real-Time Automatic Speech Recognition Extension Toolkit of Kaldi. (arXiv:2104.01384v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1\">Mayank Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborti_T/0/1/0/all/0/1\">Tathagata Chakraborti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Quchen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gros_D/0/1/0/all/0/1\">David Gros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xi Victoria Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maene_J/0/1/0/all/0/1\">Jaron Maene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talamadupula_K/0/1/0/all/0/1\">Kartik Talamadupula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhongwei Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1\">Jules White</a>",
          "description": "The NLC2CMD Competition hosted at NeurIPS 2020 aimed to bring the power of\nnatural language processing to the command line. Participants were tasked with\nbuilding models that can transform descriptions of command line tasks in\nEnglish to their Bash syntax. This is a report on the competition with details\nof the task, metrics, data, attempted solutions, and lessons learned.",
          "link": "http://arxiv.org/abs/2103.02523",
          "publishedOn": "2021-08-10T02:00:07.421Z",
          "wordCount": 554,
          "title": "NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash Commands. (arXiv:2103.02523v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schlicht_I/0/1/0/all/0/1\">Ipek Baris Schlicht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sezerer_E/0/1/0/all/0/1\">Erhan Sezerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tekir_S/0/1/0/all/0/1\">Selma Tekir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_O/0/1/0/all/0/1\">Oul Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukhers_Z/0/1/0/all/0/1\">Zeyd Boukhers</a>",
          "description": "Widespread and rapid dissemination of false news has made fact-checking an\nindispensable requirement. Given its time-consuming and labor-intensive nature,\nthe task calls for an automated support to meet the demand. In this paper, we\npropose to leverage commonsense knowledge for the tasks of false news\nclassification and check-worthy claim detection. Arguing that commonsense\nknowledge is a factor in human believability, we fine-tune the BERT language\nmodel with a commonsense question answering task and the aforementioned tasks\nin a multi-task learning environment. For predicting fine-grained false news\ntypes, we compare the proposed fine-tuned model's performance with the false\nnews classification models on a public dataset as well as a newly collected\ndataset. We compare the model's performance with the single-task BERT model and\na state-of-the-art check-worthy claim detection tool to evaluate the\ncheck-worthy claim detection. Our experimental analysis demonstrates that\ncommonsense knowledge can improve performance in both tasks.",
          "link": "http://arxiv.org/abs/2108.03731",
          "publishedOn": "2021-08-10T02:00:07.402Z",
          "wordCount": 599,
          "title": "Leveraging Commonsense Knowledge on Classifying False News and Determining Checkworthiness of Claims. (arXiv:2108.03731v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ashwin Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Adriel Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Steven QH Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1\">Du Nguyen Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>",
          "description": "Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.",
          "link": "http://arxiv.org/abs/2106.14463",
          "publishedOn": "2021-08-10T02:00:07.395Z",
          "wordCount": 725,
          "title": "RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03509",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1\">Ruixiang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aralikatte_R/0/1/0/all/0/1\">Rahul Aralikatte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lent_H/0/1/0/all/0/1\">Heather Lent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>",
          "description": "Semantic parsing allows humans to leverage vast knowledge resources through\nnatural interaction. However, parsers are mostly designed for and evaluated on\nEnglish resources, such as CFQ (Keysers et al., 2020), the current standard\nbenchmark based on English data generated from grammar rules and oriented\ntowards Freebase, an outdated knowledge base. We propose a method for creating\na multilingual, parallel dataset of question-query pairs, grounded in Wikidata,\nand introduce such a dataset called Compositional Wikidata Questions (CWQ). We\nutilize this data to train and evaluate semantic parsers for Hebrew, Kannada,\nChinese and English, to better understand the current strengths and weaknesses\nof multilingual semantic parsing. Experiments on zero-shot cross-lingual\ntransfer demonstrate that models fail to generate valid queries even with\npretrained multilingual encoders. Our methodology, dataset and results will\nfacilitate future research on semantic parsing in more realistic and diverse\nsettings than has been possible with existing resources.",
          "link": "http://arxiv.org/abs/2108.03509",
          "publishedOn": "2021-08-10T02:00:07.374Z",
          "wordCount": 569,
          "title": "Multilingual Compositional Wikidata Questions. (arXiv:2108.03509v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.02733",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Doat_J/0/1/0/all/0/1\">Jo&#xeb;l A. Doat</a>",
          "description": "The syntactic behaviour of texts can highly vary depending on their contexts\n(e.g. author, genre, etc.). From the standpoint of stylometry, it can be\nhelpful to objectively measure this behaviour. In this paper, we discuss how\ncoalgebras are used to formalise the notion of behaviour by embedding syntactic\nfeatures of a given text into probabilistic transition systems. By introducing\nthe behavioural distance, we are then able to quantitatively measure\ndifferences between points in these systems and thus, comparing features of\ndifferent texts. Furthermore, the behavioural distance of points can be\napproximated by a polynomial-time algorithm.",
          "link": "http://arxiv.org/abs/2010.02733",
          "publishedOn": "2021-08-10T02:00:07.366Z",
          "wordCount": 548,
          "title": "Notes on Coalgebras in Stylometry. (arXiv:2010.02733v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_E/0/1/0/all/0/1\">Emily Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jieyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanseverino_M/0/1/0/all/0/1\">Mattie Sanseverino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>",
          "description": "Natural Language Processing (NLP) models propagate social biases about\nprotected attributes such as gender, race, and nationality. To create\ninterventions and mitigate these biases and associated harms, it is vital to be\nable to detect and measure such biases. While many existing works propose bias\nevaluation methodologies for different tasks, there remains a need to\ncohesively understand what biases and normative harms each of these measures\ncaptures and how different measures compare. To address this gap, this work\npresents a comprehensive survey of existing bias measures in NLP as a function\nof the associated NLP tasks, metrics, datasets, and social biases and\ncorresponding harms. This survey also organizes metrics into different\ncategories to present advantages and disadvantages. Finally, we propose a\ndocumentation standard for bias measures to aid their development,\ncategorization, and appropriate usage.",
          "link": "http://arxiv.org/abs/2108.03362",
          "publishedOn": "2021-08-10T02:00:07.359Z",
          "wordCount": 574,
          "title": "What do Bias Measures Measure?. (arXiv:2108.03362v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1\">Francielle Alves Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_I/0/1/0/all/0/1\">Isabelle Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goes_F/0/1/0/all/0/1\">Fabiana Rodrigues de G&#xf3;es</a>",
          "description": "Classic information extraction techniques consist in building questions and\nanswers about the facts. Indeed, it is still a challenge to subjective\ninformation extraction systems to identify opinions and feelings in context. In\nsentiment-based NLP tasks, there are few resources to information extraction,\nabove all offensive or hateful opinions in context. To fill this important gap,\nthis short paper provides a new cross-lingual and contextual offensive lexicon,\nwhich consists of explicit and implicit offensive and swearing expressions of\nopinion, which were annotated in two different classes: context dependent and\ncontext-independent offensive. In addition, we provide markers to identify hate\nspeech. Annotation approach was evaluated at the expression-level and achieves\nhigh human inter-annotator agreement. The provided offensive lexicon is\navailable in Portuguese and English languages.",
          "link": "http://arxiv.org/abs/2104.12227",
          "publishedOn": "2021-08-10T02:00:07.342Z",
          "wordCount": 617,
          "title": "Identifying Offensive Expressions of Opinion in Context. (arXiv:2104.12227v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kayo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1\">Danish Pruthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Aditi Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "Context-aware machine translation models are designed to leverage contextual\ninformation, but often fail to do so. As a result, they inaccurately\ndisambiguate pronouns and polysemous words that require context for resolution.\nIn this paper, we ask several questions: What contexts do human translators use\nto resolve ambiguous words? Are models paying large amounts of attention to the\nsame context? What if we explicitly train them to do so? To answer these\nquestions, we introduce SCAT (Supporting Context for Ambiguous Translations), a\nnew English-French dataset comprising supporting context words for 14K\ntranslations that professional translators found useful for pronoun\ndisambiguation. Using SCAT, we perform an in-depth analysis of the context used\nto disambiguate, examining positional and lexical characteristics of the\nsupporting words. Furthermore, we measure the degree of alignment between the\nmodel's attention scores and the supporting context from SCAT, and apply a\nguided attention strategy to encourage agreement between the two.",
          "link": "http://arxiv.org/abs/2105.06977",
          "publishedOn": "2021-08-10T02:00:07.331Z",
          "wordCount": 643,
          "title": "Do Context-Aware Translation Models Pay the Right Attention?. (arXiv:2105.06977v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taille_B/0/1/0/all/0/1\">Bruno Taill&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guigue_V/0/1/0/all/0/1\">Vincent Guigue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scoutheeten_G/0/1/0/all/0/1\">Geoffrey Scoutheeten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>",
          "description": "Despite efforts to distinguish three different evaluation setups (Bekoulis et\nal., 2018), numerous end-to-end Relation Extraction (RE) articles present\nunreliable performance comparison to previous work. In this paper, we first\nidentify several patterns of invalid comparisons in published papers and\ndescribe them to avoid their propagation. We then propose a small empirical\nstudy to quantify the impact of the most common mistake and evaluate it leads\nto overestimating the final RE performance by around 5% on ACE05. We also seize\nthis opportunity to study the unexplored ablations of two recent developments:\nthe use of language model pretraining (specifically BERT) and span-level NER.\nThis meta-analysis emphasizes the need for rigor in the report of both the\nevaluation setting and the datasets statistics and we call for unifying the\nevaluation setting in end-to-end RE.",
          "link": "http://arxiv.org/abs/2009.10684",
          "publishedOn": "2021-08-10T02:00:07.310Z",
          "wordCount": 613,
          "title": "Let's Stop Incorrect Comparisons in End-to-end Relation Extraction!. (arXiv:2009.10684v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Md. Rezaul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Sumon Kanti Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tanhim Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarker_S/0/1/0/all/0/1\">Sagor Sarker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_M/0/1/0/all/0/1\">Mehadi Hasan Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_K/0/1/0/all/0/1\">Kabir Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md. Azam Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decker_S/0/1/0/all/0/1\">Stefan Decker</a>",
          "description": "The exponential growths of social media and micro-blogging sites not only\nprovide platforms for empowering freedom of expressions and individual voices,\nbut also enables people to express anti-social behaviour like online\nharassment, cyberbullying, and hate speech. Numerous works have been proposed\nto utilize textual data for social and anti-social behaviour analysis, by\npredicting the contexts mostly for highly-resourced languages like English.\nHowever, some languages are under-resourced, e.g., South Asian languages like\nBengali, that lack computational resources for accurate natural language\nprocessing (NLP). In this paper, we propose an explainable approach for hate\nspeech detection from the under-resourced Bengali language, which we called\nDeepHateExplainer. Bengali texts are first comprehensively preprocessed, before\nclassifying them into political, personal, geopolitical, and religious hates\nusing a neural ensemble method of transformer-based neural architectures (i.e.,\nmonolingual Bangla BERT-base, multilingual BERT-cased/uncased, and\nXLM-RoBERTa). Important(most and least) terms are then identified using\nsensitivity analysis and layer-wise relevance propagation(LRP), before\nproviding human-interpretable explanations. Finally, we compute\ncomprehensiveness and sufficiency scores to measure the quality of explanations\nw.r.t faithfulness. Evaluations against machine learning~(linear and tree-based\nmodels) and neural networks (i.e., CNN, Bi-LSTM, and Conv-LSTM with word\nembeddings) baselines yield F1-scores of 78%, 91%, 89%, and 84%, for political,\npersonal, geopolitical, and religious hates, respectively, outperforming both\nML and DNN baselines.",
          "link": "http://arxiv.org/abs/2012.14353",
          "publishedOn": "2021-08-10T02:00:07.303Z",
          "wordCount": 726,
          "title": "DeepHateExplainer: Explainable Hate Speech Detection in Under-resourced Bengali Language. (arXiv:2012.14353v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kostic_B/0/1/0/all/0/1\">Bogdan Kosti&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risch_J/0/1/0/all/0/1\">Julian Risch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1\">Timo M&#xf6;ller</a>",
          "description": "Open-domain extractive question answering works well on textual data by first\nretrieving candidate texts and then extracting the answer from those\ncandidates. However, some questions cannot be answered by text alone but\nrequire information stored in tables. In this paper, we present an approach for\nretrieving both texts and tables relevant to a question by jointly encoding\ntexts, tables and questions into a single vector space. To this end, we create\na new multi-modal dataset based on text and table datasets from related work\nand compare the retrieval performance of different encoding schemata. We find\nthat dense vector embeddings of transformer models outperform sparse embeddings\non four out of six evaluation datasets. Comparing different dense embedding\nmodels, tri-encoders, with one encoder for each question, text and table,\nincrease retrieval performance compared to bi-encoders with one encoder for the\nquestion and one for both text and tables. We release the newly created\nmulti-modal dataset to the community so that it can be used for training and\nevaluation.",
          "link": "http://arxiv.org/abs/2108.04049",
          "publishedOn": "2021-08-10T02:00:07.295Z",
          "wordCount": 601,
          "title": "Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models. (arXiv:2108.04049v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Calderone_B/0/1/0/all/0/1\">Basilio Calderone</a> (CLLE), <a href=\"http://arxiv.org/find/cs/1/au:+Hathout_N/0/1/0/all/0/1\">Nabil Hathout</a> (CLLE), <a href=\"http://arxiv.org/find/cs/1/au:+Bonami_O/0/1/0/all/0/1\">Olivier Bonami</a> (LLF UMR7110)",
          "description": "The paper presents four models submitted to Part 2 of the SIGMORPHON 2021\nShared Task 0, which aims at replicating human judgements on the inflection of\nnonce lexemes. Our goal is to explore the usefulness of combining pre-compiled\nanalogical patterns with an encoder-decoder architecture. Two models are\ndesigned using such patterns either in the input or the output of the network.\nTwo extra models controlled for the role of raw similarity of nonce inflected\nforms to existing inflected forms in the same paradigm cell, and the role of\nthe type frequency of analogical patterns. Our strategy is entirely endogenous\nin the sense that the models appealing solely to the data provided by the\nSIGMORPHON organisers, without using external resources. Our model 2 ranks\nsecond among all submitted systems, suggesting that the inclusion of analogical\npatterns in the network architecture is useful in mimicking speakers'\npredictions.",
          "link": "http://arxiv.org/abs/2108.03968",
          "publishedOn": "2021-08-10T02:00:07.281Z",
          "wordCount": 613,
          "title": "Not quite there yet: Combining analogical patterns and encoder-decoder networks for cognitively plausible inflection. (arXiv:2108.03968v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shangbin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Minnan Luo</a>",
          "description": "Identifying political perspective in news media has become an important task\ndue to the rapid growth of political commentary and the increasingly polarized\nideologies. Previous approaches only focus on leveraging the semantic\ninformation and leaves out the rich social and political context that helps\nindividuals understand political stances. In this paper, we propose a\nperspective detection method that incorporates external knowledge of real-world\npolitics. Specifically, we construct a contemporary political knowledge graph\nwith 1,071 entities and 10,703 triples. We then build a heterogeneous\ninformation network for each news document that jointly models article\nsemantics and external knowledge in knowledge graphs. Finally, we apply gated\nrelational graph convolutional networks and conduct political perspective\ndetection as graph-level classification. Extensive experiments show that our\nmethod achieves the best performance and outperforms state-of-the-art methods\nby 5.49\\%. Numerous ablation studies further bear out the necessity of external\nknowledge and the effectiveness of our graph-based approach.",
          "link": "http://arxiv.org/abs/2108.03861",
          "publishedOn": "2021-08-10T02:00:07.264Z",
          "wordCount": 592,
          "title": "Knowledge Graph Augmented Political Perspective Detection in News Media. (arXiv:2108.03861v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Opazo_C/0/1/0/all/0/1\">Cristian Rodriguez-Opazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1\">Damien Teney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1\">Stephen Gould</a>",
          "description": "We extend the task of composed image retrieval, where an input query consists\nof an image and short textual description of how to modify the image. Existing\nmethods have only been applied to non-complex images within narrow domains,\nsuch as fashion products, thereby limiting the scope of study on in-depth\nvisual reasoning in rich image and language contexts. To address this issue, we\ncollect the Compose Image Retrieval on Real-life images (CIRR) dataset, which\nconsists of over 36,000 pairs of crowd-sourced, open-domain images with\nhuman-generated modifying text. To extend current methods to the open-domain,\nwe propose CIRPLANT, a transformer based model that leverages rich pre-trained\nvision-and-language (V&L) knowledge for modifying visual features conditioned\non natural language. Retrieval is then done by nearest neighbor lookup on the\nmodified features. We demonstrate that with a relatively simple architecture,\nCIRPLANT outperforms existing methods on open-domain images, while matching\nstate-of-the-art accuracy on the existing narrow datasets, such as fashion.\nTogether with the release of CIRR, we believe this work will inspire further\nresearch on composed image retrieval.",
          "link": "http://arxiv.org/abs/2108.04024",
          "publishedOn": "2021-08-10T02:00:07.249Z",
          "wordCount": 634,
          "title": "Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models. (arXiv:2108.04024v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alsaidi_S/0/1/0/all/0/1\">Safa Alsaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decker_A/0/1/0/all/0/1\">Amandine Decker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lay_P/0/1/0/all/0/1\">Puthineath Lay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marquer_E/0/1/0/all/0/1\">Esteban Marquer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murena_P/0/1/0/all/0/1\">Pierre-Alexandre Murena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couceiro_M/0/1/0/all/0/1\">Miguel Couceiro</a>",
          "description": "Analogical proportions are statements of the form \"A is to B as C is to D\"\nthat are used for several reasoning and classification tasks in artificial\nintelligence and natural language processing (NLP). For instance, there are\nanalogy based approaches to semantics as well as to morphology. In fact,\nsymbolic approaches were developed to solve or to detect analogies between\ncharacter strings, e.g., the axiomatic approach as well as that based on\nKolmogorov complexity. In this paper, we propose a deep learning approach to\ndetect morphological analogies, for instance, with reinflexion or conjugation.\nWe present empirical results that show that our framework is competitive with\nthe above-mentioned state of the art symbolic approaches. We also explore\nempirically its transferability capacity across languages, which highlights\ninteresting similarities between them.",
          "link": "http://arxiv.org/abs/2108.03945",
          "publishedOn": "2021-08-10T02:00:07.242Z",
          "wordCount": 586,
          "title": "A Neural Approach for Detecting Morphological Analogies. (arXiv:2108.03945v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shangbin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Peisheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Minnan Luo</a>",
          "description": "Political stance detection has become an important task due to the\nincreasingly polarized political ideologies. Most existing works focus on\nidentifying perspectives in news articles or social media posts, while social\nentities, such as individuals and organizations, produce these texts and\nactually take stances. In this paper, we propose the novel task of entity\nstance prediction, which aims to predict entities' stances given their social\nand political context. Specifically, we retrieve facts from Wikipedia about\nsocial entities regarding contemporary U.S. politics. We then annotate social\nentities' stances towards political ideologies with the help of domain experts.\nAfter defining the task of entity stance prediction, we propose a graph-based\nsolution, which constructs a heterogeneous information network from collected\nfacts and adopts gated relational graph convolutional networks for\nrepresentation learning. Our model is then trained with a combination of\nsupervised, self-supervised and unsupervised loss functions, which are\nmotivated by multiple social and political phenomenons. We conduct extensive\nexperiments to compare our method with existing text and graph analysis\nbaselines. Our model achieves highest stance detection accuracy and yields\ninspiring insights regarding social entity stances. We further conduct ablation\nstudy and parameter analysis to study the mechanism and effectiveness of our\nproposed approach.",
          "link": "http://arxiv.org/abs/2108.03881",
          "publishedOn": "2021-08-10T02:00:07.234Z",
          "wordCount": 644,
          "title": "Encoding Heterogeneous Social and Political Context for Entity Stance Prediction. (arXiv:2108.03881v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.06336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moodley_A/0/1/0/all/0/1\">Avashlin Moodley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saba_A/0/1/0/all/0/1\">Athandiwe Saba</a>",
          "description": "Social Media can be used to extract discussion topics during a disaster. With\nthe COVID-19 pandemic impact on South Africa, we need to understand how the law\nand regulation promulgated by the government in response to the pandemic\ncontrasts with discussion topics social media users have been engaging in. In\nthis work, we expand on traditional media analysis by using Social Media\ndiscussions driven by or directed to South African government officials. We\nfind topics that are similar as well as different in some cases. The findings\ncan inform further study into social media during disaster settings in South\nAfrica and beyond. This paper sets a framework for future analysis in\nunderstanding the opinions of the public during a pandemic and how these\nopinions can be distilled [in a semi-automated approach] to inform government\ncommunication in the future.",
          "link": "http://arxiv.org/abs/2006.06336",
          "publishedOn": "2021-08-10T02:00:07.197Z",
          "wordCount": 679,
          "title": "Extracting and categorising the reactions to COVID-19 by the South African public -- A social media study. (arXiv:2006.06336v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalpakchi_D/0/1/0/all/0/1\">Dmytro Kalpakchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boye_J/0/1/0/all/0/1\">Johan Boye</a>",
          "description": "An important part when constructing multiple-choice questions (MCQs) for\nreading comprehension assessment are the distractors, the incorrect but\npreferably plausible answer options. In this paper, we present a new BERT-based\nmethod for automatically generating distractors using only a small-scale\ndataset. We also release a new such dataset of Swedish MCQs (used for training\nthe model), and propose a methodology for assessing the generated distractors.\nEvaluation shows that from a student's perspective, our method generated one or\nmore plausible distractors for more than 50% of the MCQs in our test set. From\na teacher's perspective, about 50% of the generated distractors were deemed\nappropriate. We also do a thorough analysis of the results.",
          "link": "http://arxiv.org/abs/2108.03973",
          "publishedOn": "2021-08-10T02:00:07.190Z",
          "wordCount": 560,
          "title": "BERT-based distractor generation for Swedish reading comprehension questions using a small-scale dataset. (arXiv:2108.03973v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hegde_S/0/1/0/all/0/1\">Siddhanth U Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hande_A/0/1/0/all/0/1\">Adeep Hande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1\">Ruba Priyadharshini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thavareesan_S/0/1/0/all/0/1\">Sajeetha Thavareesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakuntharaj_R/0/1/0/all/0/1\">Ratnasingam Sakuntharaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thangasamy_S/0/1/0/all/0/1\">Sathiyaraj Thangasamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharathi_B/0/1/0/all/0/1\">B Bharathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>",
          "description": "A meme is an part of media created to share an opinion or emotion across the\ninternet. Due to its popularity, memes have become the new forms of\ncommunication on social media. However, due to its nature, they are being used\nin harmful ways such as trolling and cyberbullying progressively. Various data\nmodelling methods create different possibilities in feature extraction and\nturning them into beneficial information. The variety of modalities included in\ndata plays a significant part in predicting the results. We try to explore the\nsignificance of visual features of images in classifying memes. Memes are a\nblend of both image and text, where the text is embedded into the image. We try\nto incorporate the memes as troll and non-trolling memes based on the images\nand the text on them. However, the images are to be analysed and combined with\nthe text to increase performance. Our work illustrates different textual\nanalysis methods and contrasting multimodal methods ranging from simple merging\nto cross attention to utilising both worlds' - best visual and textual\nfeatures. The fine-tuned cross-lingual language model, XLM, performed the best\nin textual analysis, and the multimodal transformer performs the best in\nmultimodal analysis.",
          "link": "http://arxiv.org/abs/2108.03886",
          "publishedOn": "2021-08-10T02:00:06.988Z",
          "wordCount": 658,
          "title": "Do Images really do the Talking? Analysing the significance of Images in Tamil Troll meme classification. (arXiv:2108.03886v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1\">Chau Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1\">Shruti Bhosale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edunov_S/0/1/0/all/0/1\">Sergey Edunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>",
          "description": "We describe Facebook's multilingual model submission to the WMT2021 shared\ntask on news translation. We participate in 14 language directions: English to\nand from Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese. To\ndevelop systems covering all these directions, we focus on multilingual models.\nWe utilize data from all available sources --- WMT, large-scale data mining,\nand in-domain backtranslation --- to create high quality bilingual and\nmultilingual baselines. Subsequently, we investigate strategies for scaling\nmultilingual model size, such that one system has sufficient capacity for high\nquality representations of all eight languages. Our final submission is an\nensemble of dense and sparse Mixture-of-Expert multilingual translation models,\nfollowed by finetuning on in-domain news data and noisy channel reranking.\nCompared to previous year's winning submissions, our multilingual system\nimproved the translation quality on all language directions, with an average\nimprovement of 2.0 BLEU. In the WMT2021 task, our system ranks first in 10\ndirections based on automatic evaluation.",
          "link": "http://arxiv.org/abs/2108.03265",
          "publishedOn": "2021-08-10T02:00:06.959Z",
          "wordCount": 590,
          "title": "Facebook AI WMT21 News Translation Task Submission. (arXiv:2108.03265v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">An Nguyen</a>",
          "description": "Although current state-of-the-art language models have achieved impressive\nresults in numerous natural language processing tasks, still they could not\nsolve the problem of producing repetitive, dull and sometimes inconsistent text\nin open-ended text generation. Studies often attribute this problem to the\nmaximum likelihood training objective, and propose alternative approaches by\nusing stochastic decoding methods or altering the training objective. However,\nthere is still a lack of consistent evaluation metrics to directly compare the\nefficacy of these solutions. In this work, we study different evaluation\nmetrics that have been proposed to evaluate quality, diversity and consistency\nof machine-generated text. From there, we propose a practical pipeline to\nevaluate language models in open-ended generation task, and research on how to\nimprove the model's performance in all dimensions by leveraging different\nauxiliary training objectives.",
          "link": "http://arxiv.org/abs/2108.03578",
          "publishedOn": "2021-08-10T02:00:06.935Z",
          "wordCount": 558,
          "title": "Language Model Evaluation in Open-ended Text Generation. (arXiv:2108.03578v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1\">Sakib Shahriar</a>",
          "description": "\"Art is the lie that enables us to realize the truth.\" - Pablo Picasso. For\ncenturies, humans have dedicated themselves to producing arts to convey their\nimagination. The advancement in technology and deep learning in particular, has\ncaught the attention of many researchers trying to investigate whether art\ngeneration is possible by computers and algorithms. Using generative\nadversarial networks (GANs), applications such as synthesizing photorealistic\nhuman faces and creating captions automatically from images were realized. This\nsurvey takes a comprehensive look at the recent works using GANs for generating\nvisual arts, music, and literary text. A performance comparison and description\nof the various GAN architecture are also presented. Finally, some of the key\nchallenges in art generation using GANs are highlighted along with\nrecommendations for future work.",
          "link": "http://arxiv.org/abs/2108.03857",
          "publishedOn": "2021-08-10T02:00:06.928Z",
          "wordCount": 597,
          "title": "GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network. (arXiv:2108.03857v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jyun-yu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiusi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "COVID-19 has caused lasting damage to almost every domain in public health,\nsociety, and economy. To monitor the pandemic trend, existing studies rely on\nthe aggregation of traditional statistical models and epidemic spread theory.\nIn other words, historical statistics of COVID-19, as well as the population\nmobility data, become the essential knowledge for monitoring the pandemic\ntrend. However, these solutions can barely provide precise prediction and\nsatisfactory explanations on the long-term disease surveillance while the\nubiquitous social media resources can be the key enabler for solving this\nproblem. For example, serious discussions may occur on social media before and\nafter some breaking events take place. These events, such as marathon and\nparade, may impact the spread of the virus. To take advantage of the social\nmedia data, we propose a novel framework, Social Media enhAnced pandemic\nsuRveillance Technique (SMART), which is composed of two modules: (i)\ninformation extraction module to construct heterogeneous knowledge graphs based\non the extracted events and relationships among them; (ii) time series\nprediction module to provide both short-term and long-term forecasts of the\nconfirmed cases and fatality at the state-level in the United States and to\ndiscover risk factors for COVID-19 interventions. Extensive experiments show\nthat our method largely outperforms the state-of-the-art baselines by 7.3% and\n7.4% in confirmed case/fatality prediction, respectively.",
          "link": "http://arxiv.org/abs/2108.03670",
          "publishedOn": "2021-08-10T02:00:06.914Z",
          "wordCount": 712,
          "title": "#StayHome or #Marathon? Social Media Enhanced Pandemic Surveillance on Spatial-temporal Dynamic Graphs. (arXiv:2108.03670v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kandoor_A/0/1/0/all/0/1\">Arun Kandoor</a>",
          "description": "Semantic parsing models with applications in task oriented dialog systems\nrequire efficient sequence to sequence (seq2seq) architectures to be run\non-device. To this end, we propose a projection based encoder-decoder model\nreferred to as pQRNN-MAtt. Studies based on projection methods were restricted\nto encoder-only models, and we believe this is the first study extending it to\nseq2seq architectures. The resulting quantized models are less than 3.5MB in\nsize and are well suited for on-device latency critical applications. We show\nthat on MTOP, a challenging multilingual semantic parsing dataset, the average\nmodel performance surpasses LSTM based seq2seq model that uses pre-trained\nembeddings despite being 85x smaller. Furthermore, the model can be an\neffective student for distilling large pre-trained models such as T5/BERT.",
          "link": "http://arxiv.org/abs/2108.03340",
          "publishedOn": "2021-08-10T02:00:06.899Z",
          "wordCount": 541,
          "title": "Tiny Neural Models for Seq2Seq. (arXiv:2108.03340v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minghan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaxin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingtao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yujia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Shimin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Ying Qin</a>",
          "description": "This paper describes our work in participation of the IWSLT-2021 offline\nspeech translation task. Our system was built in a cascade form, including a\nspeaker diarization module, an Automatic Speech Recognition (ASR) module and a\nMachine Translation (MT) module. We directly use the LIUM SpkDiarization tool\nas the diarization module. The ASR module is trained with three ASR datasets\nfrom different sources, by multi-source training, using a modified Transformer\nencoder. The MT module is pretrained on the large-scale WMT news translation\ndataset and fine-tuned on the TED corpus. Our method achieves 24.6 BLEU score\non the 2021 test set.",
          "link": "http://arxiv.org/abs/2108.03845",
          "publishedOn": "2021-08-10T02:00:06.879Z",
          "wordCount": 550,
          "title": "The HW-TSC's Offline Speech Translation Systems for IWSLT 2021 Evaluation. (arXiv:2108.03845v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jing Yang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kong Aik Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Woon Seng Gan</a>",
          "description": "Conventional approaches to personalized dialogue generation typically require\na large corpus, as well as predefined persona information. However, in a\nreal-world setting, neither a large corpus of training data nor persona\ninformation are readily available. To address these practical limitations, we\npropose a novel multi-task meta-learning approach which involves training a\nmodel to adapt to new personas without relying on a large corpus, or on any\npredefined persona information. Instead, the model is tasked with generating\npersonalized responses based on only the dialogue context. Unlike prior work,\nour approach leverages on the provided persona information only during training\nvia the introduction of an auxiliary persona reconstruction task. In this\npaper, we introduce 2 frameworks that adopt the proposed multi-task\nmeta-learning approach: the Multi-Task Meta-Learning (MTML) framework, and the\nAlternating Multi-Task Meta-Learning (AMTML) framework. Experimental results\nshow that utilizing MTML and AMTML results in dialogue responses with greater\npersona consistency.",
          "link": "http://arxiv.org/abs/2108.03377",
          "publishedOn": "2021-08-10T02:00:06.872Z",
          "wordCount": 589,
          "title": "Generating Personalized Dialogue via Multi-Task Meta-Learning. (arXiv:2108.03377v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alsaidi_S/0/1/0/all/0/1\">Safa Alsaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decker_A/0/1/0/all/0/1\">Amandine Decker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lay_P/0/1/0/all/0/1\">Puthineath Lay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marquer_E/0/1/0/all/0/1\">Esteban Marquer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murena_P/0/1/0/all/0/1\">Pierre-Alexandre Murena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couceiro_M/0/1/0/all/0/1\">Miguel Couceiro</a>",
          "description": "Analogical proportions are statements expressed in the form \"A is to B as C\nis to D\" and are used for several reasoning and classification tasks in\nartificial intelligence and natural language processing (NLP). In this paper,\nwe focus on morphological tasks and we propose a deep learning approach to\ndetect morphological analogies. We present an empirical study to see how our\nframework transfers across languages, and that highlights interesting\nsimilarities and differences between these languages. In view of these results,\nwe also discuss the possibility of building a multilingual morphological model.",
          "link": "http://arxiv.org/abs/2108.03938",
          "publishedOn": "2021-08-10T02:00:06.865Z",
          "wordCount": 556,
          "title": "On the Transferability of Neural Models of Morphological Analogies. (arXiv:2108.03938v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>",
          "description": "We study controllable text summarization which allows users to gain control\non a particular attribute (e.g., length limit) of the generated summaries. In\nthis work, we propose a novel training framework based on Constrained Markov\nDecision Process (CMDP), which conveniently includes a reward function along\nwith a set of constraints, to facilitate better summarization control. The\nreward function encourages the generation to resemble the human-written\nreference, while the constraints are used to explicitly prevent the generated\nsummaries from violating user-imposed requirements. Our framework can be\napplied to control important attributes of summarization, including length,\ncovered entities, and abstractiveness, as we devise specific constraints for\neach of these aspects. Extensive experiments on popular benchmarks show that\nour CMDP framework helps generate informative summaries while complying with a\ngiven attribute's requirement.",
          "link": "http://arxiv.org/abs/2108.03405",
          "publishedOn": "2021-08-10T02:00:06.858Z",
          "wordCount": 563,
          "title": "Controllable Summarization with Constrained Markov Decision Process. (arXiv:2108.03405v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Matej%5Cr%7Bu%7D_L/0/1/0/all/0/1\">Luk&#xe1;&#x161; Mat&#x11b;j&#x16f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griol_D/0/1/0/all/0/1\">David Griol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callejas_Z/0/1/0/all/0/1\">Zoraida Callejas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molina_J/0/1/0/all/0/1\">Jos&#xe9; Manuel Molina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchis_A/0/1/0/all/0/1\">Araceli Sanchis</a>",
          "description": "Deep learning is providing very positive results in areas related to\nconversational interfaces, such as speech recognition, but its potential\nbenefit for dialog management has still not been fully studied. In this paper,\nwe perform an assessment of different configurations for deep-learned dialog\nmanagement with three dialog corpora from different application domains and\nvarying in size, dimensionality and possible system responses. Our results have\nallowed us to identify several aspects that can have an impact on accuracy,\nincluding the approaches used for feature extraction, input representation,\ncontext consideration and the hyper-parameters of the deep neural networks\nemployed.",
          "link": "http://arxiv.org/abs/2108.03478",
          "publishedOn": "2021-08-10T02:00:06.851Z",
          "wordCount": 574,
          "title": "An empirical assessment of deep learning approaches to task-oriented dialog management. (arXiv:2108.03478v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Rui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>",
          "description": "Transfer learning has been an important technique for low-resource neural\nmachine translation. In this work, we build two systems to study how\nrelatedness can benefit the translation performance. The primary system adopts\nmachine translation model pre-trained on related language pair and the\ncontrastive system adopts that pre-trained on unrelated language pair. We show\nthat relatedness is not required for transfer learning to work but does benefit\nthe performance.",
          "link": "http://arxiv.org/abs/2108.03739",
          "publishedOn": "2021-08-10T02:00:06.842Z",
          "wordCount": 491,
          "title": "Machine Translation of Low-Resource Indo-European Languages. (arXiv:2108.03739v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Adebara_I/0/1/0/all/0/1\">Ife Adebara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>",
          "description": "We investigate transfer learning based on pre-trained neural machine\ntranslation models to translate between (low-resource) similar languages. This\nwork is part of our contribution to the WMT 2021 Similar Languages Translation\nShared Task where we submitted models for different language pairs, including\nFrench-Bambara, Spanish-Catalan, and Spanish-Portuguese in both directions. Our\nmodels for Catalan-Spanish ($82.79$ BLEU) and Portuguese-Spanish ($87.11$ BLEU)\nrank top 1 in the official shared task evaluation, and we are the only team to\nsubmit models for the French-Bambara pairs.",
          "link": "http://arxiv.org/abs/2108.03533",
          "publishedOn": "2021-08-10T02:00:06.822Z",
          "wordCount": 519,
          "title": "Improving Similar Language Translation With Transfer Learning. (arXiv:2108.03533v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Maria Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>",
          "description": "Can we construct a neural model that is inductively biased towards learning\nhuman languages? Motivated by this question, we aim at constructing an\ninformative prior over neural weights, in order to adapt quickly to held-out\nlanguages in the task of character-level language modeling. We infer this\ndistribution from a sample of typologically diverse training languages via\nLaplace approximation. The use of such a prior outperforms baseline models with\nan uninformative prior (so-called \"fine-tuning\") in both zero-shot and few-shot\nsettings. This shows that the prior is imbued with universal phonological\nknowledge. Moreover, we harness additional language-specific side information\nas distant supervision for held-out languages. Specifically, we condition\nlanguage models on features from typological databases, by concatenating them\nto hidden states or generating weights with hyper-networks. These features\nappear beneficial in the few-shot setting, but not in the zero-shot setting.\nSince the paucity of digital texts affects the majority of the world's\nlanguages, we hope that these findings will help broaden the scope of\napplications for language technology.",
          "link": "http://arxiv.org/abs/2108.03334",
          "publishedOn": "2021-08-10T02:00:06.725Z",
          "wordCount": 594,
          "title": "Towards Zero-shot Language Modeling. (arXiv:2108.03334v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nikolich_A/0/1/0/all/0/1\">Alexandr Nikolich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puchkova_A/0/1/0/all/0/1\">Arina Puchkova</a>",
          "description": "Automatic summarization techniques aim to shorten and generalize information\ngiven in the text while preserving its core message and the most relevant\nideas. This task can be approached and treated with a variety of methods,\nhowever, not many attempts have been made to produce solutions specifically for\nthe Russian language despite existing localizations of the state-of-the-art\nmodels. In this paper, we aim to showcase ruGPT3 ability to summarize texts,\nfine-tuning it on the corpora of Russian news with their corresponding\nhuman-generated summaries. Additionally, we employ hyperparameter tuning so\nthat the model's output becomes less random and more tied to the original text.\nWe evaluate the resulting texts with a set of metrics, showing that our\nsolution can surpass the state-of-the-art model's performance without\nadditional changes in architecture or loss function. Despite being able to\nproduce sensible summaries, our model still suffers from a number of flaws,\nnamely, it is prone to altering Named Entities present in the original text\n(such as surnames, places, dates), deviating from facts stated in the given\ndocument, and repeating the information in the summary.",
          "link": "http://arxiv.org/abs/2108.03502",
          "publishedOn": "2021-08-10T02:00:06.705Z",
          "wordCount": 602,
          "title": "Fine-tuning GPT-3 for Russian Text Summarization. (arXiv:2108.03502v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_B/0/1/0/all/0/1\">Bencheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jason Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ajay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umair_H/0/1/0/all/0/1\">Hafiza Umair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vovor_A/0/1/0/all/0/1\">Atsu Vovor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durzynski_N/0/1/0/all/0/1\">Natalie Durzynski</a>",
          "description": "Toxic online speech has become a crucial problem nowadays due to an\nexponential increase in the use of internet by people from different cultures\nand educational backgrounds. Differentiating if a text message belongs to hate\nspeech and offensive language is a key challenge in automatic detection of\ntoxic text content. In this paper, we propose an approach to automatically\nclassify tweets into three classes: Hate, offensive and Neither. Using public\ntweet data set, we first perform experiments to build BI-LSTM models from empty\nembedding and then we also try the same neural network architecture with\npre-trained Glove embedding. Next, we introduce a transfer learning approach\nfor hate speech detection using an existing pre-trained language model BERT\n(Bidirectional Encoder Representations from Transformers), DistilBert\n(Distilled version of BERT) and GPT-2 (Generative Pre-Training). We perform\nhyper parameters tuning analysis of our best model (BI-LSTM) considering\ndifferent neural network architectures, learn-ratings and normalization methods\netc. After tuning the model and with the best combination of parameters, we\nachieve over 92 percent accuracy upon evaluating it on test data. We also\ncreate a class module which contains main functionality including text\nclassification, sentiment checking and text data augmentation. This model could\nserve as an intermediate module between user and Twitter.",
          "link": "http://arxiv.org/abs/2108.03305",
          "publishedOn": "2021-08-10T02:00:06.656Z",
          "wordCount": 651,
          "title": "Offensive Language and Hate Speech Detection with Deep Learning and Transfer Learning. (arXiv:2108.03305v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.04130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jun_Y/0/1/0/all/0/1\">Yennie Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_H/0/1/0/all/0/1\">Haider Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benussi_E/0/1/0/all/0/1\">Elias Benussi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volpin_F/0/1/0/all/0/1\">Filippo Volpin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dreyer_F/0/1/0/all/0/1\">Frederic A. Dreyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1\">Aleksandar Shtedritski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1\">Yuki M. Asano</a>",
          "description": "The capabilities of natural language models trained on large-scale data have\nincreased immensely over the past few years. Open source libraries such as\nHuggingFace have made these models easily available and accessible. While prior\nresearch has identified biases in large language models, this paper considers\nbiases contained in the most popular versions of these models when applied\n`out-of-the-box' for downstream tasks. We focus on generative language models\nas they are well-suited for extracting biases inherited from training data.\nSpecifically, we conduct an in-depth analysis of GPT-2, which is the most\ndownloaded text generation model on HuggingFace, with over half a million\ndownloads in the past month alone. We assess biases related to occupational\nassociations for different protected categories by intersecting gender with\nreligion, sexuality, ethnicity, political affiliation, and continental name\norigin. Using a template-based data collection pipeline, we collect 396K\nsentence completions made by GPT-2 and find: (i) The machine-predicted jobs are\nless diverse and more stereotypical for women than for men, especially for\nintersections; (ii) Intersectional interactions are highly relevant for\noccupational associations, which we quantify by fitting 262 logistic models;\n(iii) For most occupations, GPT-2 reflects the skewed gender and ethnicity\ndistribution found in US Labour Bureau data, and even pulls the\nsocietally-skewed distribution towards gender parity in cases where its\npredictions deviate from real labor market observations. This raises the\nnormative question of what language models _should_ learn - whether they should\nreflect or correct for existing inequalities.",
          "link": "http://arxiv.org/abs/2102.04130",
          "publishedOn": "2021-08-09T00:49:26.852Z",
          "wordCount": 733,
          "title": "Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models. (arXiv:2102.04130v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jie_C/0/1/0/all/0/1\">Cheng Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Da Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zigeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>",
          "description": "With the increasing scale of search engine marketing, designing an efficient\nbidding system is becoming paramount for the success of e-commerce companies.\nThe critical challenges faced by a modern industrial-level bidding system\ninclude: 1. the catalog is enormous, and the relevant bidding features are of\nhigh sparsity; 2. the large volume of bidding requests induces significant\ncomputation burden to both the offline and online serving. Leveraging\nextraneous user-item information proves essential to mitigate the sparsity\nissue, for which we exploit the natural language signals from the users' query\nand the contextual knowledge from the products. In particular, we extract the\nvector representations of ads via the Transformer model and leverage their\ngeometric relation to building collaborative bidding predictions via\nclustering. The two-step procedure also significantly reduces the computation\nstress of bid evaluation and optimization. In this paper, we introduce the\nend-to-end structure of the bidding system for search engine marketing for\nWalmart e-commerce, which successfully handles tens of millions of bids each\nday. We analyze the online and offline performances of our approach and discuss\nhow we find it as a production-efficient solution.",
          "link": "http://arxiv.org/abs/2106.12700",
          "publishedOn": "2021-08-09T00:49:26.734Z",
          "wordCount": 666,
          "title": "An Efficient Group-based Search Engine Marketing System for E-Commerce. (arXiv:2106.12700v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.07942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zanella_Beguelin_S/0/1/0/all/0/1\">Santiago Zanella-B&#xe9;guelin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1\">Lukas Wutschitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1\">Shruti Tople</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruhle_V/0/1/0/all/0/1\">Victor R&#xfc;hle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paverd_A/0/1/0/all/0/1\">Andrew Paverd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohrimenko_O/0/1/0/all/0/1\">Olga Ohrimenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopf_B/0/1/0/all/0/1\">Boris K&#xf6;pf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockschmidt_M/0/1/0/all/0/1\">Marc Brockschmidt</a>",
          "description": "To continuously improve quality and reflect changes in data, machine learning\napplications have to regularly retrain and update their core models. We show\nthat a differential analysis of language model snapshots before and after an\nupdate can reveal a surprising amount of detailed information about changes in\nthe training data. We propose two new metrics---\\emph{differential score} and\n\\emph{differential rank}---for analyzing the leakage due to updates of natural\nlanguage models. We perform leakage analysis using these metrics across models\ntrained on several different datasets using different methods and\nconfigurations. We discuss the privacy implications of our findings, propose\nmitigation strategies and evaluate their effect.",
          "link": "http://arxiv.org/abs/1912.07942",
          "publishedOn": "2021-08-09T00:49:26.673Z",
          "wordCount": 609,
          "title": "Analyzing Information Leakage of Updates to Natural Language Models. (arXiv:1912.07942v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1\">Aiqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaohan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>",
          "description": "Online sexism has become an increasing concern in social media platforms as\nit has affected the healthy development of the Internet and can have negative\neffects in society. While research in the sexism detection domain is growing,\nmost of this research focuses on English as the language and on Twitter as the\nplatform. Our objective here is to broaden the scope of this research by\nconsidering the Chinese language on Sina Weibo. We propose the first Chinese\nsexism dataset -- Sina Weibo Sexism Review (SWSR) dataset --, as well as a\nlarge Chinese lexicon SexHateLex made of abusive and gender-related terms. We\nintroduce our data collection and annotation process, and provide an\nexploratory analysis of the dataset characteristics to validate its quality and\nto show how sexism is manifested in Chinese. The SWSR dataset provides labels\nat different levels of granularity including (i) sexism or non-sexism, (ii)\nsexism category and (iii) target type, which can be exploited, among others,\nfor building computational methods to identify and investigate finer-grained\ngender-related abusive language. We conduct experiments for the three sexism\nclassification tasks making use of state-of-the-art machine learning models.\nOur results show competitive performance, providing a benchmark for sexism\ndetection in the Chinese language, as well as an error analysis highlighting\nopen challenges needing more research in Chinese NLP. The SWSR dataset and\nSexHateLex lexicon are publicly available.",
          "link": "http://arxiv.org/abs/2108.03070",
          "publishedOn": "2021-08-09T00:49:26.638Z",
          "wordCount": 668,
          "title": "SWSR: A Chinese Dataset and Lexicon for Online Sexism Detection. (arXiv:2108.03070v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1\">Aiqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>",
          "description": "Most hate speech detection research focuses on a single language, generally\nEnglish, which limits their generalisability to other languages. In this paper\nwe investigate the cross-lingual hate speech detection task, tackling the\nproblem by adapting the hate speech resources from one language to another. We\npropose a cross-lingual capsule network learning model coupled with extra\ndomain-specific lexical semantics for hate speech (CCNL-Ex). Our model achieves\nstate-of-the-art performance on benchmark datasets from AMI@Evalita2018 and\nAMI@Ibereval2018 involving three languages: English, Spanish and Italian,\noutperforming state-of-the-art baselines on all six language pairs.",
          "link": "http://arxiv.org/abs/2108.03089",
          "publishedOn": "2021-08-09T00:49:26.629Z",
          "wordCount": 529,
          "title": "Cross-lingual Capsule Network for Hate Speech Detection in Social Media. (arXiv:2108.03089v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02915",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1\">Guangyi Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Le Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>",
          "description": "Sentence semantic matching requires an agent to determine the semantic\nrelation between two sentences, which is widely used in various natural\nlanguage tasks, such as Natural Language Inference (NLI), Paraphrase\nIdentification (PI), and so on. Much recent progress has been made in this\narea, especially attention-based methods and pre-trained language model based\nmethods. However, most of these methods focus on all the important parts in\nsentences in a static way and only emphasize how important the words are to the\nquery, inhibiting the ability of attention mechanism. In order to overcome this\nproblem and boost the performance of attention mechanism, we propose a novel\ndynamic re-read attention, which can pay close attention to one small region of\nsentences at each step and re-read the important parts for better sentence\nrepresentations. Based on this attention variation, we develop a novel Dynamic\nRe-read Network (DRr-Net) for sentence semantic matching. Moreover, selecting\none small region in dynamic re-read attention seems insufficient for sentence\nsemantics, and employing pre-trained language models as input encoders will\nintroduce incomplete and fragile representation problems. To this end, we\nextend DRrNet to Locally-Aware Dynamic Re-read Attention Net (LadRa-Net), in\nwhich local structure of sentences is employed to alleviate the shortcoming of\nByte-Pair Encoding (BPE) in pre-trained language models and boost the\nperformance of dynamic reread attention. Extensive experiments on two popular\nsentence semantic matching tasks demonstrate that DRr-Net can significantly\nimprove the performance of sentence semantic matching. Meanwhile, LadRa-Net is\nable to achieve better performance by considering the local structures of\nsentences. In addition, it is exceedingly interesting that some discoveries in\nour experiments are consistent with some findings of psychological research.",
          "link": "http://arxiv.org/abs/2108.02915",
          "publishedOn": "2021-08-09T00:49:26.493Z",
          "wordCount": 723,
          "title": "LadRa-Net: Locally-Aware Dynamic Re-read Attention Net for Sentence Semantic Matching. (arXiv:2108.02915v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1\">Aotao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stellar_J/0/1/0/all/0/1\">Jennifer E. Stellar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>",
          "description": "Humans possess the unique ability to communicate emotions through language.\nAlthough concepts like anger or awe are abstract, there is a shared consensus\nabout what these English emotion words mean. This consensus may give the\nimpression that their meaning is static, but we propose this is not the case.\nWe cannot travel back to earlier periods to study emotion concepts directly,\nbut we can examine text corpora, which have partially preserved the meaning of\nemotion words. Using natural language processing of historical text, we found\nevidence for semantic change in emotion words over the past century and that\nvarying rates of change were predicted in part by an emotion concept's\nprototypicality - how representative it is of the broader category of\n\"emotion\". Prototypicality negatively correlated with historical rates of\nemotion semantic change obtained from text-based word embeddings, beyond more\nestablished variables including usage frequency in English and a second\ncomparison language, French. This effect for prototypicality did not\nconsistently extend to the semantic category of birds, suggesting its relevance\nfor predicting semantic change may be category-dependent. Our results suggest\nemotion semantics are evolving over time, with prototypical emotion words\nremaining semantically stable, while other emotion words evolve more freely.",
          "link": "http://arxiv.org/abs/2108.02887",
          "publishedOn": "2021-08-09T00:49:26.483Z",
          "wordCount": 621,
          "title": "Evolution of emotion semantics. (arXiv:2108.02887v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1\">Markus Dreyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_F/0/1/0/all/0/1\">Feng Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atluri_S/0/1/0/all/0/1\">Sandeep Atluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Sujith Ravi</a>",
          "description": "We analyze the tradeoff between factuality and abstractiveness of summaries.\nWe introduce abstractiveness constraints to control the degree of\nabstractiveness at decoding time, and we apply this technique to characterize\nthe abstractiveness-factuality tradeoff across multiple widely-studied\ndatasets, using extensive human evaluations. We train a neural summarization\nmodel on each dataset and visualize the rates of change in factuality as we\ngradually increase abstractiveness using our abstractiveness constraints. We\nobserve that, while factuality generally drops with increased abstractiveness,\ndifferent datasets lead to different rates of factuality decay. We propose new\nmeasures to quantify the tradeoff between factuality and abstractiveness, incl.\nmuQAGS, which balances factuality with abstractiveness. We also quantify this\ntradeoff in previous works, aiming to establish baselines for the\nabstractiveness-factuality tradeoff that future publications can compare\nagainst.",
          "link": "http://arxiv.org/abs/2108.02859",
          "publishedOn": "2021-08-09T00:49:26.458Z",
          "wordCount": 561,
          "title": "Analyzing the Abstractiveness-Factuality Tradeoff With Nonlinear Abstractiveness Constraints. (arXiv:2108.02859v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Alexander Hanbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1\">Patrick Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiguo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>",
          "description": "The current state-of-the-art generative models for open-domain question\nanswering (ODQA) have focused on generating direct answers from unstructured\ntextual information. However, a large amount of world's knowledge is stored in\nstructured databases, and need to be accessed using query languages such as\nSQL. Furthermore, query languages can answer questions that require complex\nreasoning, as well as offering full explainability. In this paper, we propose a\nhybrid framework that takes both textual and tabular evidence as input and\ngenerates either direct answers or SQL queries depending on which form could\nbetter answer the question. The generated SQL queries can then be executed on\nthe associated databases to obtain the final answers. To the best of our\nknowledge, this is the first paper that applies Text2SQL to ODQA tasks.\nEmpirically, we demonstrate that on several ODQA datasets, the hybrid methods\nconsistently outperforms the baseline models that only take homogeneous input\nby a large margin. Specifically we achieve state-of-the-art performance on\nOpenSQuAD dataset using a T5-base model. In a detailed analysis, we demonstrate\nthat the being able to generate structural SQL queries can always bring gains,\nespecially for those questions that requires complex reasoning.",
          "link": "http://arxiv.org/abs/2108.02866",
          "publishedOn": "2021-08-09T00:49:26.438Z",
          "wordCount": 642,
          "title": "Dual Reader-Parser on Hybrid Textual and Tabular Evidence for Open Domain Question Answering. (arXiv:2108.02866v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tuxworth_D/0/1/0/all/0/1\">David Tuxworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antypas_D/0/1/0/all/0/1\">Dimosthenis Antypas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1\">Luis Espinosa-Anke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preece_A/0/1/0/all/0/1\">Alun Preece</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_D/0/1/0/all/0/1\">David Rogers</a>",
          "description": "This paper demonstrates a two-stage method for deriving insights from social\nmedia data relating to disinformation by applying a combination of geospatial\nclassification and embedding-based language modelling across multiple\nlanguages. In particular, the analysis in centered on Twitter and\ndisinformation for three European languages: English, French and Spanish.\nFirstly, Twitter data is classified into European and non-European sets using\nBERT. Secondly, Word2vec is applied to the classified texts resulting in\nEurocentric, non-Eurocentric and global representations of the data for the\nthree target languages. This comparative analysis demonstrates not only the\nefficacy of the classification method but also highlights geographic, temporal\nand linguistic differences in the disinformation-related media. Thus, the\ncontributions of the work are threefold: (i) a novel language-independent\ntransformer-based geolocation method; (ii) an analytical approach that exploits\nlexical specificity and word embeddings to interrogate user-generated content;\nand (iii) a dataset of 36 million disinformation related tweets in English,\nFrench and Spanish.",
          "link": "http://arxiv.org/abs/2108.03067",
          "publishedOn": "2021-08-09T00:49:26.427Z",
          "wordCount": 604,
          "title": "Deriving Disinformation Insights from Geolocalized Twitter Callouts. (arXiv:2108.03067v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03016",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petruzzellis_L/0/1/0/all/0/1\">L Petruzzellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1\">A Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visentin_M/0/1/0/all/0/1\">M Visentin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chebat_J/0/1/0/all/0/1\">J.-C. Chebat</a>",
          "description": "In this paper we investigate the verbal expression of shopping experience\nobtained by a sample of customers asked to freely verbalize how they felt when\nentering a store. Using novel tools of Text Mining and Social Network Analysis,\nwe analyzed the interviews to understand the connection between the emotions\naroused during the shopping experience, satisfaction and the way participants\nlink these concepts to self-satisfaction and self-identity. The results show a\nprominent role of emotions in the discourse about the shopping experience\nbefore purchasing and an inward-looking connection to the self. Our results\nalso suggest that modern retail environment should enhance the hedonic shopping\nexperience in terms of fun, fantasy, moods, and emotions.",
          "link": "http://arxiv.org/abs/2108.03016",
          "publishedOn": "2021-08-09T00:49:26.413Z",
          "wordCount": 585,
          "title": "Tell me a story about yourself: The words of shopping experience and self-satisfaction. (arXiv:2108.03016v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupte_A/0/1/0/all/0/1\">Amit Gupte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romanov_A/0/1/0/all/0/1\">Alexey Romanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mantravadi_S/0/1/0/all/0/1\">Sahitya Mantravadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banda_D/0/1/0/all/0/1\">Dalitso Banda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1\">Raza Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meenal_L/0/1/0/all/0/1\">Lakshmanan Ramu Meenal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Benjamin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Soundar Srinivasan</a>",
          "description": "Document digitization is essential for the digital transformation of our\nsocieties, yet a crucial step in the process, Optical Character Recognition\n(OCR), is still not perfect. Even commercial OCR systems can produce\nquestionable output depending on the fidelity of the scanned documents. In this\npaper, we demonstrate an effective framework for mitigating OCR errors for any\ndownstream NLP task, using Named Entity Recognition (NER) as an example. We\nfirst address the data scarcity problem for model training by constructing a\ndocument synthesis pipeline, generating realistic but degraded data with NER\nlabels. We measure the NER accuracy drop at various degradation levels and show\nthat a text restoration model, trained on the degraded data, significantly\ncloses the NER accuracy gaps caused by OCR errors, including on an\nout-of-domain dataset. For the benefit of the community, we have made the\ndocument synthesis pipeline available as an open-source project.",
          "link": "http://arxiv.org/abs/2108.02899",
          "publishedOn": "2021-08-09T00:49:26.398Z",
          "wordCount": 621,
          "title": "Lights, Camera, Action! A Framework to Improve NLP Accuracy over OCR documents. (arXiv:2108.02899v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02923",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yulin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuxi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yuchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiameng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Kun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>",
          "description": "Structured text understanding on Visually Rich Documents (VRDs) is a crucial\npart of Document Intelligence. Due to the complexity of content and layout in\nVRDs, structured text understanding has been a challenging task. Most existing\nstudies decoupled this problem into two sub-tasks: entity labeling and entity\nlinking, which require an entire understanding of the context of documents at\nboth token and segment levels. However, little work has been concerned with the\nsolutions that efficiently extract the structured data from different levels.\nThis paper proposes a unified framework named StrucTexT, which is flexible and\neffective for handling both sub-tasks. Specifically, based on the transformer,\nwe introduce a segment-token aligned encoder to deal with the entity labeling\nand entity linking tasks at different levels of granularity. Moreover, we\ndesign a novel pre-training strategy with three self-supervised tasks to learn\na richer representation. StrucTexT uses the existing Masked Visual Language\nModeling task and the new Sentence Length Prediction and Paired Boxes Direction\ntasks to incorporate the multi-modal information across text, image, and\nlayout. We evaluate our method for structured text understanding at\nsegment-level and token-level and show it outperforms the state-of-the-art\ncounterparts with significantly superior performance on the FUNSD, SROIE, and\nEPHOIE datasets.",
          "link": "http://arxiv.org/abs/2108.02923",
          "publishedOn": "2021-08-09T00:49:26.382Z",
          "wordCount": 654,
          "title": "StrucTexT: Structured Text Understanding with Multi-Modal Transformers. (arXiv:2108.02923v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wet_H/0/1/0/all/0/1\">Harm de Wet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>",
          "description": "Disinformation through fake news is an ongoing problem in our society and has\nbecome easily spread through social media. The most cost and time effective way\nto filter these large amounts of data is to use a combination of human and\ntechnical interventions to identify it. From a technical perspective, Natural\nLanguage Processing (NLP) is widely used in detecting fake news. Social media\ncompanies use NLP techniques to identify the fake news and warn their users,\nbut fake news may still slip through undetected. It is especially a problem in\nmore localised contexts (outside the United States of America). How do we\nadjust fake news detection systems to work better for local contexts such as in\nSouth Africa. In this work we investigate fake news detection on South African\nwebsites. We curate a dataset of South African fake news and then train\ndetection models. We contrast this with using widely available fake news\ndatasets (from mostly USA website). We also explore making the datasets more\ndiverse by combining them and observe the differences in behaviour in writing\nbetween nations' fake news using interpretable machine learning.",
          "link": "http://arxiv.org/abs/2108.02941",
          "publishedOn": "2021-08-09T00:49:26.371Z",
          "wordCount": 640,
          "title": "Is it Fake? News Disinformation Detection on South African News Websites. (arXiv:2108.02941v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>",
          "description": "Recall the classical text generation works, the generation framework can be\nbriefly divided into two phases: \\textbf{idea reasoning} and \\textbf{surface\nrealization}. The target of idea reasoning is to figure out the main idea which\nwill be presented in the following talking/writing periods. Surface realization\naims to arrange the most appropriate sentence to depict and convey the\ninformation distilled from the main idea. However, the current popular\ntoken-by-token text generation methods ignore this crucial process and suffer\nfrom many serious issues, such as idea/topic drift. To tackle the problems and\nrealize this two-phase paradigm, we propose a new framework named Sentence\nSemantic Regression (\\textbf{SSR}) based on sentence-level language modeling.\nFor idea reasoning, two architectures \\textbf{SSR-AR} and \\textbf{SSR-NonAR}\nare designed to conduct sentence semantic regression autoregressively (like\nGPT2/3) and bidirectionally (like BERT). In the phase of surface realization, a\nmixed-granularity sentence decoder is designed to generate text with better\nconsistency by jointly incorporating the predicted sentence-level main idea as\nwell as the preceding contextual token-level information. We conduct\nexperiments on four tasks of story ending prediction, story ending generation,\ndialogue generation, and sentence infilling. The results show that SSR can\nobtain better performance in terms of automatic metrics and human evaluation.",
          "link": "http://arxiv.org/abs/2108.02984",
          "publishedOn": "2021-08-09T00:49:26.323Z",
          "wordCount": 625,
          "title": "Sentence Semantic Regression for Text Generation. (arXiv:2108.02984v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vanmassenhove_E/0/1/0/all/0/1\">Eva Vanmassenhove</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monti_J/0/1/0/all/0/1\">Johanna Monti</a>",
          "description": "Languages differ in terms of the absence or presence of gender features, the\nnumber of gender classes and whether and where gender features are explicitly\nmarked. These cross-linguistic differences can lead to ambiguities that are\ndifficult to resolve, especially for sentence-level MT systems. The\nidentification of ambiguity and its subsequent resolution is a challenging task\nfor which currently there aren't any specific resources or challenge sets\navailable. In this paper, we introduce gENder-IT, an English--Italian challenge\nset focusing on the resolution of natural gender phenomena by providing\nword-level gender tags on the English source side and multiple gender\nalternative translations, where needed, on the Italian target side.",
          "link": "http://arxiv.org/abs/2108.02854",
          "publishedOn": "2021-08-09T00:49:26.127Z",
          "wordCount": 552,
          "title": "GENder-IT: An Annotated English-Italian Parallel Challenge Set for Cross-Linguistic Natural Gender Phenomena. (arXiv:2108.02854v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Moin Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahzad_K/0/1/0/all/0/1\">Khurram Shahzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1\">Kamran Malik</a>",
          "description": "Hate speech is a specific type of controversial content that is widely\nlegislated as a crime that must be identified and blocked. However, due to the\nsheer volume and velocity of the Twitter data stream, hate speech detection\ncannot be performed manually. To address this issue, several studies have been\nconducted for hate speech detection in European languages, whereas little\nattention has been paid to low-resource South Asian languages, making the\nsocial media vulnerable for millions of users. In particular, to the best of\nour knowledge, no study has been conducted for hate speech detection in Roman\nUrdu text, which is widely used in the sub-continent. In this study, we have\nscrapped more than 90,000 tweets and manually parsed them to identify 5,000\nRoman Urdu tweets. Subsequently, we have employed an iterative approach to\ndevelop guidelines and used them for generating the Hate Speech Roman Urdu 2020\ncorpus. The tweets in the this corpus are classified at three levels:\nNeutral-Hostile, Simple-Complex, and Offensive-Hate speech. As another\ncontribution, we have used five supervised learning techniques, including a\ndeep learning technique, to evaluate and compare their effectiveness for hate\nspeech detection. The results show that Logistic Regression outperformed all\nother techniques, including deep learning techniques for the two levels of\nclassification, by achieved an F1 score of 0.906 for distinguishing between\nNeutral-Hostile tweets, and 0.756 for distinguishing between Offensive-Hate\nspeech tweets.",
          "link": "http://arxiv.org/abs/2108.02830",
          "publishedOn": "2021-08-09T00:49:26.085Z",
          "wordCount": 719,
          "title": "Hate Speech Detection in Roman Urdu. (arXiv:2108.02830v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02765",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+ElFadeel_H/0/1/0/all/0/1\">Haytham ElFadeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peshterliev_S/0/1/0/all/0/1\">Stan Peshterliev</a>",
          "description": "Large transformer models, such as BERT, achieve state-of-the-art results in\nmachine reading comprehension (MRC) for open-domain question answering (QA).\nHowever, transformers have a high computational cost for inference which makes\nthem hard to apply to online QA systems for applications like voice assistants.\nTo reduce computational cost and latency, we propose decoupling the transformer\nMRC model into input-component and cross-component. The decoupling allows for\npart of the representation computation to be performed offline and cached for\nonline use. To retain the decoupled transformer accuracy, we devised a\nknowledge distillation objective from a standard transformer model. Moreover,\nwe introduce learned representation compression layers which help reduce by\nfour times the storage requirement for the cache. In experiments on the SQUAD\n2.0 dataset, a decoupled transformer reduces the computational cost and latency\nof open-domain MRC by 30-40% with only 1.2 points worse F1-score compared to a\nstandard transformer.",
          "link": "http://arxiv.org/abs/2108.02765",
          "publishedOn": "2021-08-06T00:51:45.090Z",
          "wordCount": 579,
          "title": "Decoupled Transformer for Scalable Inference in Open-domain Question Answering. (arXiv:2108.02765v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Popov_V/0/1/0/all/0/1\">Vadim Popov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vovk_I/0/1/0/all/0/1\">Ivan Vovk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gogoryan_V/0/1/0/all/0/1\">Vladimir Gogoryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadekova_T/0/1/0/all/0/1\">Tasnima Sadekova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kudinov_M/0/1/0/all/0/1\">Mikhail Kudinov</a>",
          "description": "Recently, denoising diffusion probabilistic models and generative score\nmatching have shown high potential in modelling complex data distributions\nwhile stochastic calculus has provided a unified point of view on these\ntechniques allowing for flexible inference schemes. In this paper we introduce\nGrad-TTS, a novel text-to-speech model with score-based decoder producing\nmel-spectrograms by gradually transforming noise predicted by encoder and\naligned with text input by means of Monotonic Alignment Search. The framework\nof stochastic differential equations helps us to generalize conventional\ndiffusion probabilistic models to the case of reconstructing data from noise\nwith different parameters and allows to make this reconstruction flexible by\nexplicitly controlling trade-off between sound quality and inference speed.\nSubjective human evaluation shows that Grad-TTS is competitive with\nstate-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We\nwill make the code publicly available shortly.",
          "link": "http://arxiv.org/abs/2105.06337",
          "publishedOn": "2021-08-06T00:51:45.076Z",
          "wordCount": 602,
          "title": "Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech. (arXiv:2105.06337v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seongmin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihwa Lee</a>",
          "description": "Text variational autoencoders (VAEs) are notorious for posterior collapse, a\nphenomenon where the model's decoder learns to ignore signals from the encoder.\nBecause posterior collapse is known to be exacerbated by expressive decoders,\nTransformers have seen limited adoption as components of text VAEs. Existing\nstudies that incorporate Transformers into text VAEs (Li et al., 2020; Fang et\nal., 2021) mitigate posterior collapse using massive pretraining, a technique\nunavailable to most of the research community without extensive computing\nresources. We present a simple two-phase training scheme to convert a\nsequence-to-sequence Transformer into a VAE with just finetuning. The resulting\nlanguage model is competitive with massively pretrained Transformer-based VAEs\nin some internal metrics while falling short on others. To facilitate training\nwe comprehensively explore the impact of common posterior collapse alleviation\ntechniques in the literature. We release our code for reproducability.",
          "link": "http://arxiv.org/abs/2108.02446",
          "publishedOn": "2021-08-06T00:51:45.066Z",
          "wordCount": 567,
          "title": "Finetuning Pretrained Transformers into Variational Autoencoders. (arXiv:2108.02446v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02725",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shaunak Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_M/0/1/0/all/0/1\">Mikhail Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_G/0/1/0/all/0/1\">Gaurav Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sviridenko_M/0/1/0/all/0/1\">Maxim Sviridenko</a>",
          "description": "Numerous online stock image libraries offer high quality yet copyright free\nimages for use in marketing campaigns. To assist advertisers in navigating such\nthird party libraries, we study the problem of automatically fetching relevant\nad images given the ad text (via a short textual query for images). Motivated\nby our observations in logged data on ad image search queries (given ad text),\nwe formulate a keyword extraction problem, where a keyword extracted from the\nad text (or its augmented version) serves as the ad image query. In this\ncontext, we propose VisualTextRank: an unsupervised method to (i) augment input\nad text using semantically similar ads, and (ii) extract the image query from\nthe augmented ad text. VisualTextRank builds on prior work on graph based\ncontext extraction (biased TextRank in particular) by leveraging both the text\nand image of similar ads for better keyword extraction, and using advertiser\ncategory specific biasing with sentence-BERT embeddings. Using data collected\nfrom the Verizon Media Native (Yahoo Gemini) ad platform's stock image search\nfeature for onboarding advertisers, we demonstrate the superiority of\nVisualTextRank compared to competitive keyword extraction baselines (including\nan $11\\%$ accuracy lift over biased TextRank). For the case when the stock\nimage library is restricted to English queries, we show the effectiveness of\nVisualTextRank on multilingual ads (translated to English) while leveraging\nsemantically similar English ads. Online tests with a simplified version of\nVisualTextRank led to a 28.7% increase in the usage of stock image search, and\na 41.6% increase in the advertiser onboarding rate in the Verizon Media Native\nad platform.",
          "link": "http://arxiv.org/abs/2108.02725",
          "publishedOn": "2021-08-06T00:51:45.047Z",
          "wordCount": 714,
          "title": "VisualTextRank: Unsupervised Graph-based Content Extraction for Automating Ad Text to Image Search. (arXiv:2108.02725v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkhasov_A/0/1/0/all/0/1\">Alex Malkhasov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manoshin_A/0/1/0/all/0/1\">Andrey Manoshin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dima_G/0/1/0/all/0/1\">George Dima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cserhati_R/0/1/0/all/0/1\">R&#xe9;ka Cserh&#xe1;ti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1\">Md.Sadek Hossain Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sardi_M/0/1/0/all/0/1\">Matt S&#xe1;rdi</a>",
          "description": "This report presents the results of the EENLP project, done as a part of EEML\n2021 summer school.\n\nIt presents a broad index of NLP resources for Eastern European languages,\nwhich, we hope, could be helpful for the NLP community; several new\nhand-crafted cross-lingual datasets focused on Eastern European languages, and\na sketch evaluation of cross-lingual transfer learning abilities of several\nmodern multilingual Transformer-based models.",
          "link": "http://arxiv.org/abs/2108.02605",
          "publishedOn": "2021-08-06T00:51:45.020Z",
          "wordCount": 520,
          "title": "EENLP: Cross-lingual Eastern European NLP Index. (arXiv:2108.02605v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuxin Ding</a>",
          "description": "Single online handwritten Chinese character recognition~(single OLHCCR) has\nachieved prominent performance. However, in real application scenarios, users\nalways write multiple Chinese characters to form one complete sentence and the\ncontextual information within these characters holds the significant potential\nto improve the accuracy, robustness and efficiency of sentence-level OLHCCR. In\nthis work, we first propose a simple and straightforward end-to-end network,\nnamely vanilla compositional network~(VCN) to tackle the sentence-level OLHCCR.\nIt couples convolutional neural network with sequence modeling architecture to\nexploit the handwritten character's previous contextual information. Although\nVCN performs much better than the state-of-the-art single OLHCCR model, it\nexposes high fragility when confronting with not well written characters such\nas sloppy writing, missing or broken strokes. To improve the robustness of\nsentence-level OLHCCR, we further propose a novel deep spatial-temporal fusion\nnetwork~(DSTFN). It utilizes a pre-trained autoregresssive framework as the\nbackbone component, which projects each Chinese character into word embeddings,\nand integrates the spatial glyph features of handwritten characters and their\ncontextual information multiple times at multi-layer fusion module. We also\nconstruct a large-scale sentence-level handwriting dataset, named as CSOHD to\nevaluate models. Extensive experiment results demonstrate that DSTFN achieves\nthe state-of-the-art performance, which presents strong robustness compared\nwith VCN and exiting single OLHCCR models. The in-depth empirical analysis and\ncase studies indicate that DSTFN can significantly improve the efficiency of\nhandwriting input, with the handwritten Chinese character with incomplete\nstrokes being recognized precisely.",
          "link": "http://arxiv.org/abs/2108.02561",
          "publishedOn": "2021-08-06T00:51:45.013Z",
          "wordCount": 682,
          "title": "Sentence-level Online Handwritten Chinese Character Recognition. (arXiv:2108.02561v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Larson_S/0/1/0/all/0/1\">Stefan Larson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Navtej Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_S/0/1/0/all/0/1\">Saarthak Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stewart_S/0/1/0/all/0/1\">Shanti Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_U/0/1/0/all/0/1\">Uma Krishnaswamy</a>",
          "description": "To be robust enough for widespread adoption, document analysis systems\ninvolving machine learning models must be able to respond correctly to inputs\nthat fall outside of the data distribution that was used to generate the data\non which the models were trained. This paper explores the ability of text\nclassifiers trained on standard document classification datasets to generalize\nto out-of-distribution documents at inference time. We take the Tobacco-3482\nand RVL-CDIP datasets as a starting point and generate new out-of-distribution\nevaluation datasets in order to analyze the generalization performance of\nmodels trained on these standard datasets. We find that models trained on the\nsmaller Tobacco-3482 dataset perform poorly on our new out-of-distribution\ndata, while text classification models trained on the larger RVL-CDIP exhibit\nsmaller performance drops.",
          "link": "http://arxiv.org/abs/2108.02684",
          "publishedOn": "2021-08-06T00:51:44.999Z",
          "wordCount": 570,
          "title": "Exploring Out-of-Distribution Generalization in Text Classifiers Trained on Tobacco-3482 and RVL-CDIP. (arXiv:2108.02684v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_B/0/1/0/all/0/1\">Bowen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor W. Tsang</a>",
          "description": "Aspect-level sentiment classification (ASC) aims to predict the fine-grained\nsentiment polarity towards a given aspect mentioned in a review. Despite recent\nadvances in ASC, enabling machines to preciously infer aspect sentiments is\nstill challenging. This paper tackles two challenges in ASC: (1) due to lack of\naspect knowledge, aspect representation derived in prior works is inadequate to\nrepresent aspect's exact meaning and property information; (2) prior works only\ncapture either local syntactic information or global relational information,\nthus missing either one of them leads to insufficient syntactic information. To\ntackle these challenges, we propose a novel ASC model which not only end-to-end\nembeds and leverages aspect knowledge but also marries the two kinds of\nsyntactic information and lets them compensate for each other. Our model\nincludes three key components: (1) a knowledge-aware gated recurrent memory\nnetwork recurrently integrates dynamically summarized aspect knowledge; (2) a\ndual syntax graph network combines both kinds of syntactic information to\ncomprehensively capture sufficient syntactic information; (3) a knowledge\nintegrating gate re-enhances the final representation with further needed\naspect knowledge; (4) an aspect-to-context attention mechanism aggregates the\naspect-related semantics from all hidden states into the final representation.\nExperimental results on several benchmark datasets demonstrate the\neffectiveness of our model, which overpass previous state-of-the-art models by\nlarge margins in terms of both Accuracy and Macro-F1.",
          "link": "http://arxiv.org/abs/2108.02352",
          "publishedOn": "2021-08-06T00:51:44.953Z",
          "wordCount": 673,
          "title": "Understand me, if you refer to Aspect Knowledge: Knowledge-aware Gated Recurrent Memory Network. (arXiv:2108.02352v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>",
          "description": "Probing classifiers have emerged as one of the prominent methodologies for\ninterpreting and analyzing deep neural network models of natural language\nprocessing. The basic idea is simple---a classifier is trained to predict some\nlinguistic property from a model's representations---and has been used to\nexamine a wide variety of models and properties. However, recent studies have\ndemonstrated various methodological weaknesses of this approach. This article\ncritically reviews the probing classifiers framework, highlighting their\npromises, shortcomings, and advances.",
          "link": "http://arxiv.org/abs/2102.12452",
          "publishedOn": "2021-08-06T00:51:44.821Z",
          "wordCount": 543,
          "title": "Probing Classifiers: Promises, Shortcomings, and Advances. (arXiv:2102.12452v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03095",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Having numerous potential applications and great impact, end-to-end speech\ntranslation (ST) has long been treated as an independent task, failing to fully\ndraw strength from the rapid advances of its sibling - text machine translation\n(MT). With text and audio inputs represented differently, the modality gap has\nrendered MT data and its end-to-end models incompatible with their ST\ncounterparts. In observation of this obstacle, we propose to bridge this\nrepresentation gap with Chimera. By projecting audio and text features to a\ncommon semantic representation, Chimera unifies MT and ST tasks and boosts the\nperformance on ST benchmarks, MuST-C and Augmented Librispeech, to a new\nstate-of-the-art. Specifically, Chimera obtains 27.1 BLEU on MuST-C EN-DE,\nimproving the SOTA by a +1.9 BLEU margin. Further experimental analyses\ndemonstrate that the shared semantic space indeed conveys common knowledge\nbetween these two tasks and thus paves a new way for augmenting training\nresources across modalities. Code, data, and resources are available at\nhttps://github.com/Glaciohound/Chimera-ST.",
          "link": "http://arxiv.org/abs/2105.03095",
          "publishedOn": "2021-08-06T00:51:44.807Z",
          "wordCount": 638,
          "title": "Learning Shared Semantic Space for Speech-to-Text Translation. (arXiv:2105.03095v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tjandra_A/0/1/0/all/0/1\">Andros Tjandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_D/0/1/0/all/0/1\">Diptanu Gon Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Frank Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kritika Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1\">Alexis Conneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sela_A/0/1/0/all/0/1\">Assaf Sela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saraf_Y/0/1/0/all/0/1\">Yatharth Saraf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>",
          "description": "Language identification greatly impacts the success of downstream tasks such\nas automatic speech recognition. Recently, self-supervised speech\nrepresentations learned by wav2vec 2.0 have been shown to be very effective for\na range of speech tasks. We extend previous self-supervised work on language\nidentification by experimenting with pre-trained models which were learned on\nreal-world unconstrained speech in multiple languages and not just on English.\nWe show that models pre-trained on many languages perform better and enable\nlanguage identification systems that require very little labeled data to\nperform well. Results on a 25 languages setup show that with only 10 minutes of\nlabeled data per language, a cross-lingually pre-trained model can achieve over\n93% accuracy.",
          "link": "http://arxiv.org/abs/2107.04082",
          "publishedOn": "2021-08-06T00:51:44.765Z",
          "wordCount": 606,
          "title": "Improved Language Identification Through Cross-Lingual Self-Supervised Learning. (arXiv:2107.04082v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02776",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hono_Y/0/1/0/all/0/1\">Yukiya Hono</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kei Hashimoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oura_K/0/1/0/all/0/1\">Keiichiro Oura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nankaku_Y/0/1/0/all/0/1\">Yoshihiko Nankaku</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tokuda_K/0/1/0/all/0/1\">Keiichi Tokuda</a>",
          "description": "This paper presents Sinsy, a deep neural network (DNN)-based singing voice\nsynthesis (SVS) system. In recent years, DNNs have been utilized in statistical\nparametric SVS systems, and DNN-based SVS systems have demonstrated better\nperformance than conventional hidden Markov model-based ones. SVS systems are\nrequired to synthesize a singing voice with pitch and timing that strictly\nfollow a given musical score. Additionally, singing expressions that are not\ndescribed on the musical score, such as vibrato and timing fluctuations, should\nbe reproduced. The proposed system is composed of four modules: a time-lag\nmodel, a duration model, an acoustic model, and a vocoder, and singing voices\ncan be synthesized taking these characteristics of singing voices into account.\nTo better model a singing voice, the proposed system incorporates improved\napproaches to modeling pitch and vibrato and better training criteria into the\nacoustic model. In addition, we incorporated PeriodNet, a non-autoregressive\nneural vocoder with robustness for the pitch, into our systems to generate a\nhigh-fidelity singing voice waveform. Moreover, we propose automatic pitch\ncorrection techniques for DNN-based SVS to synthesize singing voices with\ncorrect pitch even if the training data has out-of-tune phrases. Experimental\nresults show our system can synthesize a singing voice with better timing, more\nnatural vibrato, and correct pitch, and it can achieve better mean opinion\nscores in subjective evaluation tests.",
          "link": "http://arxiv.org/abs/2108.02776",
          "publishedOn": "2021-08-06T00:51:44.726Z",
          "wordCount": 690,
          "title": "Sinsy: A Deep Neural Network-Based Singing Voice Synthesis System. (arXiv:2108.02776v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2010.07079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_D/0/1/0/all/0/1\">Da Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Margaret Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1\">Y-Lan Boureau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinan_E/0/1/0/all/0/1\">Emily Dinan</a>",
          "description": "Models trained on large unlabeled corpora of human interactions will learn\npatterns and mimic behaviors therein, which include offensive or otherwise\ntoxic behavior and unwanted biases. We investigate a variety of methods to\nmitigate these issues in the context of open-domain generative dialogue models.\nWe introduce a new human-and-model-in-the-loop framework for both training\nsafer models and for evaluating them, as well as a novel method to distill\nsafety considerations inside generative models without the use of an external\nclassifier at deployment time. We conduct experiments comparing these methods\nand find our new techniques are (i) safer than existing models as measured by\nautomatic and human evaluations while (ii) maintaining usability metrics such\nas engagingness relative to the state of the art. We then discuss the\nlimitations of this work by analyzing failure cases of our models.",
          "link": "http://arxiv.org/abs/2010.07079",
          "publishedOn": "2021-08-06T00:51:44.713Z",
          "wordCount": 612,
          "title": "Recipes for Safety in Open-domain Chatbots. (arXiv:2010.07079v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longyin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guodong Zhou</a>",
          "description": "Various neural-based methods have been proposed so far for joint mention\ndetection and coreference resolution. However, existing works on coreference\nresolution are mainly dependent on filtered mention representation, while other\nspans are largely neglected. In this paper, we aim at increasing the\nutilization rate of data and investigating whether those eliminated spans are\ntotally useless, or to what extent they can improve the performance of\ncoreference resolution. To achieve this, we propose a mention representation\nrefining strategy where spans highly related to mentions are well leveraged\nusing a pointer network for representation enhancing. Notably, we utilize an\nadditional loss term in this work to encourage the diversity between entity\nclusters. Experimental results on the document-level CoNLL-2012 Shared Task\nEnglish dataset show that eliminated spans are indeed much effective and our\napproach can achieve competitive results when compared with previous\nstate-of-the-art in coreference resolution.",
          "link": "http://arxiv.org/abs/2101.00737",
          "publishedOn": "2021-08-06T00:51:44.672Z",
          "wordCount": 619,
          "title": "Coreference Resolution: Are the eliminated spans totally worthless?. (arXiv:2101.00737v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yidi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_B/0/1/0/all/0/1\">Bidisha Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhavi_M/0/1/0/all/0/1\">Maulik Madhavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>",
          "description": "End-to-end intent classification using speech has numerous advantages\ncompared to the conventional pipeline approach using automatic speech\nrecognition (ASR), followed by natural language processing modules. It attempts\nto predict intent from speech without using an intermediate ASR module.\nHowever, such end-to-end framework suffers from the unavailability of large\nspeech resources with higher acoustic variation in spoken language\nunderstanding. In this work, we exploit the scope of the transformer\ndistillation method that is specifically designed for knowledge distillation\nfrom a transformer based language model to a transformer based speech model. In\nthis regard, we leverage the reliable and widely used bidirectional encoder\nrepresentations from transformers (BERT) model as a language model and transfer\nthe knowledge to build an acoustic model for intent classification using the\nspeech. In particular, a multilevel transformer based teacher-student model is\ndesigned, and knowledge distillation is performed across attention and hidden\nsub-layers of different transformer layers of the student and teacher models.\nWe achieve an intent classification accuracy of 99.10% and 88.79% for Fluent\nspeech corpus and ATIS database, respectively. Further, the proposed method\ndemonstrates better performance and robustness in acoustically degraded\ncondition compared to the baseline method.",
          "link": "http://arxiv.org/abs/2108.02598",
          "publishedOn": "2021-08-06T00:51:44.641Z",
          "wordCount": 635,
          "title": "Knowledge Distillation from BERT Transformer to Speech Transformer for Intent Classification. (arXiv:2108.02598v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demirel_E/0/1/0/all/0/1\">Emir Demirel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahlback_S/0/1/0/all/0/1\">Sven Ahlb&#xe4;ck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixon_S/0/1/0/all/0/1\">Simon Dixon</a>",
          "description": "This paper makes several contributions to automatic lyrics transcription\n(ALT) research. Our main contribution is a novel variant of the Multistreaming\nTime-Delay Neural Network (MTDNN) architecture, called MSTRE-Net, which\nprocesses the temporal information using multiple streams in parallel with\nvarying resolutions keeping the network more compact, and thus with a faster\ninference and an improved recognition rate than having identical TDNN streams.\nIn addition, two novel preprocessing steps prior to training the acoustic model\nare proposed. First, we suggest using recordings from both monophonic and\npolyphonic domains during training the acoustic model. Second, we tag\nmonophonic and polyphonic recordings with distinct labels for discriminating\nnon-vocal silence and music instances during alignment. Moreover, we present a\nnew test set with a considerably larger size and a higher musical variability\ncompared to the existing datasets used in ALT literature, while maintaining the\ngender balance of the singers. Our best performing model sets the\nstate-of-the-art in lyrics transcription by a large margin. For\nreproducibility, we publicly share the identifiers to retrieve the data used in\nthis paper.",
          "link": "http://arxiv.org/abs/2108.02625",
          "publishedOn": "2021-08-06T00:51:44.625Z",
          "wordCount": 615,
          "title": "MSTRE-Net: Multistreaming Acoustic Modeling for Automatic Lyrics Transcription. (arXiv:2108.02625v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weinzierl_M/0/1/0/all/0/1\">Maxwell A. Weinzierl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harabagiu_S/0/1/0/all/0/1\">Sanda M. Harabagiu</a>",
          "description": "Enormous hope in the efficacy of vaccines became recently a successful\nreality in the fight against the COVID-19 pandemic. However, vaccine hesitancy,\nfueled by exposure to social media misinformation about COVID-19 vaccines\nbecame a major hurdle. Therefore, it is essential to automatically detect where\nmisinformation about COVID-19 vaccines on social media is spread and what kind\nof misinformation is discussed, such that inoculation interventions can be\ndelivered at the right time and in the right place, in addition to\ninterventions designed to address vaccine hesitancy. This paper is addressing\nthe first step in tackling hesitancy against COVID-19 vaccines, namely the\nautomatic detection of misinformation about the vaccines on Twitter, the social\nmedia platform that has the highest volume of conversations about COVID-19 and\nits vaccines. We present CoVaxLies, a new dataset of tweets judged relevant to\nseveral misinformation targets about COVID-19 vaccines on which a novel method\nof detecting misinformation was developed. Our method organizes CoVaxLies in a\nMisinformation Knowledge Graph as it casts misinformation detection as a graph\nlink prediction problem. The misinformation detection method detailed in this\npaper takes advantage of the link scoring functions provided by several\nknowledge embedding methods. The experimental results demonstrate the\nsuperiority of this method when compared with classification-based methods,\nwidely used currently.",
          "link": "http://arxiv.org/abs/2108.02314",
          "publishedOn": "2021-08-06T00:51:44.610Z",
          "wordCount": 688,
          "title": "Automatic Detection of COVID-19 Vaccine Misinformation with Graph Link Prediction. (arXiv:2108.02314v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.13838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhiying Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Raphael Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Ji Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>",
          "description": "Pretrained transformers achieve the state of the art across tasks in natural\nlanguage processing, motivating researchers to investigate their inner\nmechanisms. One common direction is to understand what features are important\nfor prediction. In this paper, we apply information bottlenecks to analyze the\nattribution of each feature for prediction on a black-box model. We use BERT as\nthe example and evaluate our approach both quantitatively and qualitatively. We\nshow the effectiveness of our method in terms of attribution and the ability to\nprovide insight into how information flows through layers. We demonstrate that\nour technique outperforms two competitive methods in degradation tests on four\ndatasets. Code is available at https://github.com/bazingagin/IBA.",
          "link": "http://arxiv.org/abs/2012.13838",
          "publishedOn": "2021-08-06T00:51:44.601Z",
          "wordCount": 594,
          "title": "Inserting Information Bottlenecks for Attribution in Transformers. (arXiv:2012.13838v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khorrami_K/0/1/0/all/0/1\">Khazar Khorrami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>",
          "description": "Systems that can find correspondences between multiple modalities, such as\nbetween speech and images, have great potential to solve different recognition\nand data analysis tasks in an unsupervised manner. This work studies multimodal\nlearning in the context of visually grounded speech (VGS) models, and focuses\non their recently demonstrated capability to extract spatiotemporal alignments\nbetween spoken words and the corresponding visual objects without ever been\nexplicitly trained for object localization or word recognition. As the main\ncontributions, we formalize the alignment problem in terms of an audiovisual\nalignment tensor that is based on earlier VGS work, introduce systematic\nmetrics for evaluating model performance in aligning visual objects and spoken\nwords, and propose a new VGS model variant for the alignment task utilizing\ncross-modal attention layer. We test our model and a previously proposed model\nin the alignment task using SPEECH-COCO captions coupled with MSCOCO images. We\ncompare the alignment performance using our proposed evaluation metrics to the\nsemantic retrieval task commonly used to evaluate VGS models. We show that\ncross-modal attention layer not only helps the model to achieve higher semantic\ncross-modal retrieval performance, but also leads to substantial improvements\nin the alignment performance between image object and spoken words.",
          "link": "http://arxiv.org/abs/2108.02562",
          "publishedOn": "2021-08-06T00:51:44.588Z",
          "wordCount": 652,
          "title": "Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models. (arXiv:2108.02562v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diallo_M/0/1/0/all/0/1\">Mountaga Diallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fourati_C/0/1/0/all/0/1\">Chayma Fourati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddad_H/0/1/0/all/0/1\">Hatem Haddad</a>",
          "description": "For easier communication, posting, or commenting on each others posts, people\nuse their dialects. In Africa, various languages and dialects exist. However,\nthey are still underrepresented and not fully exploited for analytical studies\nand research purposes. In order to perform approaches like Machine Learning and\nDeep Learning, datasets are required. One of the African languages is Bambara,\nused by citizens in different countries. However, no previous work on datasets\nfor this language was performed for Sentiment Analysis. In this paper, we\npresent the first common-crawl-based Bambara dialectal dataset dedicated for\nSentiment Analysis, available freely for Natural Language Processing research\npurposes.",
          "link": "http://arxiv.org/abs/2108.02524",
          "publishedOn": "2021-08-06T00:51:44.560Z",
          "wordCount": 550,
          "title": "Bambara Language Dataset for Sentiment Analysis. (arXiv:2108.02524v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xianfeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Ernan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_Q/0/1/0/all/0/1\">Qiu Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "This paper introduces WeChat AI's participation in WMT 2021 shared news\ntranslation task on English->Chinese, English->Japanese, Japanese->English and\nEnglish->German. Our systems are based on the Transformer (Vaswani et al.,\n2017) with several novel and effective variants. In our experiments, we employ\ndata filtering, large-scale synthetic data generation (i.e., back-translation,\nknowledge distillation, forward-translation, iterative in-domain knowledge\ntransfer), advanced finetuning approaches, and boosted Self-BLEU based model\nensemble. Our constrained systems achieve 36.9, 46.9, 27.8 and 31.3\ncase-sensitive BLEU scores on English->Chinese, English->Japanese,\nJapanese->English and English->German, respectively. The BLEU scores of\nEnglish->Chinese, English->Japanese and Japanese->English are the highest among\nall submissions, and that of English->German is the highest among all\nconstrained submissions.",
          "link": "http://arxiv.org/abs/2108.02401",
          "publishedOn": "2021-08-06T00:51:44.470Z",
          "wordCount": 560,
          "title": "WeChat Neural Machine Translation Systems for WMT21. (arXiv:2108.02401v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weijiang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Lei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuejian Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_N/0/1/0/all/0/1\">Nong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>",
          "description": "The task of video-based commonsense captioning aims to generate event-wise\ncaptions and meanwhile provide multiple commonsense descriptions (e.g.,\nattribute, effect and intention) about the underlying event in the video. Prior\nworks explore the commonsense captions by using separate networks for different\ncommonsense types, which is time-consuming and lacks mining the interaction of\ndifferent commonsense. In this paper, we propose a Hybrid Reasoning Network\n(HybridNet) to endow the neural networks with the capability of semantic-level\nreasoning and word-level reasoning. Firstly, we develop multi-commonsense\nlearning for semantic-level reasoning by jointly training different commonsense\ntypes in a unified network, which encourages the interaction between the clues\nof multiple commonsense descriptions, event-wise captions and videos. Then,\nthere are two steps to achieve the word-level reasoning: (1) a memory module\nrecords the history predicted sequence from the previous generation processes;\n(2) a memory-routed multi-head attention (MMHA) module updates the word-level\nattention maps by incorporating the history information from the memory module\ninto the transformer decoder for word-level reasoning. Moreover, the multimodal\nfeatures are used to make full use of diverse knowledge for commonsense\nreasoning. Experiments and abundant analysis on the large-scale\nVideo-to-Commonsense benchmark show that our HybridNet achieves\nstate-of-the-art performance compared with other methods.",
          "link": "http://arxiv.org/abs/2108.02365",
          "publishedOn": "2021-08-06T00:51:44.457Z",
          "wordCount": 651,
          "title": "Hybrid Reasoning Network for Video-based Commonsense Captioning. (arXiv:2108.02365v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silverman_G/0/1/0/all/0/1\">Greg M. Silverman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finzel_R/0/1/0/all/0/1\">Raymond L. Finzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinz_M/0/1/0/all/0/1\">Michael V. Heinz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilakes_J/0/1/0/all/0/1\">Jake Vasilakes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solinsky_J/0/1/0/all/0/1\">Jacob C. Solinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McEwan_R/0/1/0/all/0/1\">Reed McEwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_B/0/1/0/all/0/1\">Benjamin C. Knoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tignanelli_C/0/1/0/all/0/1\">Christopher J. Tignanelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melton_G/0/1/0/all/0/1\">Genevieve B. Melton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pakhomov_S/0/1/0/all/0/1\">Serguei VS Pakhomov</a>",
          "description": "Our objective in this study is to investigate the behavior of Boolean\noperators on combining annotation output from multiple Natural Language\nProcessing (NLP) systems across multiple corpora and to assess how filtering by\naggregation of Unified Medical Language System (UMLS) Metathesaurus concepts\naffects system performance for Named Entity Recognition (NER) of UMLS concepts.\nWe used three corpora annotated for UMLS concepts: 2010 i2b2 VA challenge set\n(31,161 annotations), Multi-source Integrated Platform for Answering Clinical\nQuestions (MiPACQ) corpus (17,457 annotations including UMLS concept unique\nidentifiers), and Fairview Health Services corpus (44,530 annotations). Our\nresults showed that for UMLS concept matching, Boolean ensembling of the MiPACQ\ncorpus trended towards higher performance over individual systems. Use of an\napproximate grid-search can help optimize the precision-recall tradeoff and can\nprovide a set of heuristics for choosing an optimal set of ensembles.",
          "link": "http://arxiv.org/abs/2108.02255",
          "publishedOn": "2021-08-06T00:51:44.405Z",
          "wordCount": 605,
          "title": "An Empirical Study of UMLS Concept Extraction from Clinical Notes using Boolean Combination Ensembles. (arXiv:2108.02255v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wenjuan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yingnian Wu</a>",
          "description": "Transfer learning with large pretrained transformer-based language models\nlike BERT has become a dominating approach for most NLP tasks. Simply\nfine-tuning those large language models on downstream tasks or combining it\nwith task-specific pretraining is often not robust. In particular, the\nperformance considerably varies as the random seed changes or the number of\npretraining and/or fine-tuning iterations varies, and the fine-tuned model is\nvulnerable to adversarial attack. We propose a simple yet effective\nadapter-based approach to mitigate these issues. Specifically, we insert small\nbottleneck layers (i.e., adapter) within each layer of a pretrained model, then\nfix the pretrained layers and train the adapter layers on the downstream task\ndata, with (1) task-specific unsupervised pretraining and then (2)\ntask-specific supervised training (e.g., classification, sequence labeling).\nOur experiments demonstrate that such a training scheme leads to improved\nstability and adversarial robustness in transfer learning to various downstream\ntasks.",
          "link": "http://arxiv.org/abs/2108.02340",
          "publishedOn": "2021-08-06T00:51:44.369Z",
          "wordCount": 582,
          "title": "Robust Transfer Learning with Pretrained Language Models through Adapters. (arXiv:2108.02340v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>",
          "description": "Video captioning combines video understanding and language generation.\nDifferent from image captioning that describes a static image with details of\nalmost every object, video captioning usually considers a sequence of frames\nand biases towards focused objects, e.g., the objects that stay in focus\nregardless of the changing background. Therefore, detecting and properly\naccommodating focused objects is critical in video captioning. To enforce the\ndescription of focused objects and achieve controllable video captioning, we\npropose an Object-Oriented Non-Autoregressive approach (O2NA), which performs\ncaption generation in three steps: 1) identify the focused objects and predict\ntheir locations in the target caption; 2) generate the related attribute words\nand relation words of these focused objects to form a draft caption; and 3)\ncombine video information to refine the draft caption to a fluent final\ncaption. Since the focused objects are generated and located ahead of other\nwords, it is difficult to apply the word-by-word autoregressive generation\nprocess; instead, we adopt a non-autoregressive approach. The experiments on\ntwo benchmark datasets, i.e., MSR-VTT and MSVD, demonstrate the effectiveness\nof O2NA, which achieves results competitive with the state-of-the-arts but with\nboth higher diversity and higher inference speed.",
          "link": "http://arxiv.org/abs/2108.02359",
          "publishedOn": "2021-08-06T00:51:44.356Z",
          "wordCount": 642,
          "title": "O2NA: An Object-Oriented Non-Autoregressive Approach for Controllable Video Captioning. (arXiv:2108.02359v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2010.00502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1\">Gautam Kishore Shahi</a>",
          "description": "In this paper, we present a semi-automated framework called AMUSED for\ngathering multi-modal annotated data from the multiple social media platforms.\nThe framework is designed to mitigate the issues of collecting and annotating\nsocial media data by cohesively combining machine and human in the data\ncollection process. From a given list of the articles from professional news\nmedia or blog, AMUSED detects links to the social media posts from news\narticles and then downloads contents of the same post from the respective\nsocial media platform to gather details about that specific post. The framework\nis capable of fetching the annotated data from multiple platforms like Twitter,\nYouTube, Reddit. The framework aims to reduce the workload and problems behind\nthe data annotation from the social media platforms. AMUSED can be applied in\nmultiple application domains, as a use case, we have implemented the framework\nfor collecting COVID-19 misinformation data from different social media\nplatforms.",
          "link": "http://arxiv.org/abs/2010.00502",
          "publishedOn": "2021-08-11T01:55:21.055Z",
          "wordCount": 670,
          "title": "AMUSED: An Annotation Framework of Multi-modal Social Media Data. (arXiv:2010.00502v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04690",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Shangfeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haobin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhichen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianying Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+PinghuaGong/0/1/0/all/0/1\">PinghuaGong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>",
          "description": "Recommendation for new users, also called user cold start, has been a\nwell-recognized challenge for online recommender systems. Most existing methods\nview the crux as the lack of initial data. However, in this paper, we argue\nthat there are neglected problems: 1) New users' behaviour follows much\ndifferent distributions from regular users. 2) Although personalized features\nare involved, heavily imbalanced samples prevent the model from balancing\nnew/regular user distributions, as if the personalized features are\noverwhelmed. We name the problem as the ``submergence\" of personalization. To\ntackle this problem, we propose a novel module: Personalized COld Start MOdules\n(POSO). Considering from a model architecture perspective, POSO personalizes\nexisting modules by introducing multiple user-group-specialized sub-modules.\nThen, it fuses their outputs by personalized gates, resulting in comprehensive\nrepresentations. In such way, POSO projects imbalanced features to even\nmodules. POSO can be flexibly integrated into many existing modules and\neffectively improves their performance with negligible computational overheads.\nThe proposed method shows remarkable advantage in industrial scenario. It has\nbeen deployed on the large-scale recommender system of Kwai, and improves new\nuser Watch Time by a large margin (+7.75%). Moreover, POSO can be further\ngeneralized to regular users, inactive users and returning users (+2%-3% on\nWatch Time), as well as item cold start (+3.8% on Watch Time). Its\neffectiveness has also been verified on public dataset (MovieLens 20M). We\nbelieve such practical experience can be well generalized to other scenarios.",
          "link": "http://arxiv.org/abs/2108.04690",
          "publishedOn": "2021-08-11T01:55:20.567Z",
          "wordCount": 677,
          "title": "POSO: Personalized Cold Start Modules for Large-scale Recommender Systems. (arXiv:2108.04690v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bodigutla_P/0/1/0/all/0/1\">Praveen Kumar Bodigutla</a>",
          "description": "\"High Quality Related Search Query Suggestions\" task aims at recommending\nsearch queries which are real, accurate, diverse, relevant and engaging.\nObtaining large amounts of query-quality human annotations is expensive. Prior\nwork on supervised query suggestion models suffered from selection and exposure\nbias, and relied on sparse and noisy immediate user-feedback (e.g., clicks),\nleading to low quality suggestions. Reinforcement Learning techniques employed\nto reformulate a query using terms from search results, have limited\nscalability to large-scale industry applications. To recommend high quality\nrelated search queries, we train a Deep Reinforcement Learning model to predict\nthe query a user would enter next. The reward signal is composed of long-term\nsession-based user feedback, syntactic relatedness and estimated naturalness of\ngenerated query. Over the baseline supervised model, our proposed approach\nachieves a significant relative improvement in terms of recommendation\ndiversity (3%), down-stream user-engagement (4.2%) and per-sentence word\nrepetitions (82%).",
          "link": "http://arxiv.org/abs/2108.04452",
          "publishedOn": "2021-08-11T01:55:20.557Z",
          "wordCount": 598,
          "title": "High Quality Related Search Query Suggestions using Deep Reinforcement Learning. (arXiv:2108.04452v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04607",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fenyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>",
          "description": "Recently, Graph Convolution Network (GCN) based methods have achieved\noutstanding performance for recommendation. These methods embed users and items\nin Euclidean space, and perform graph convolution on user-item interaction\ngraphs. However, real-world datasets usually exhibit tree-like hierarchical\nstructures, which make Euclidean space less effective in capturing user-item\nrelationship. In contrast, hyperbolic space, as a continuous analogue of a\ntree-graph, provides a promising alternative. In this paper, we propose a fully\nhyperbolic GCN model for recommendation, where all operations are performed in\nhyperbolic space. Utilizing the advantage of hyperbolic space, our method is\nable to embed users/items with less distortion and capture user-item\ninteraction relationship more accurately. Extensive experiments on public\nbenchmark datasets show that our method outperforms both Euclidean and\nhyperbolic counterparts and requires far lower embedding dimensionality to\nachieve comparable performance.",
          "link": "http://arxiv.org/abs/2108.04607",
          "publishedOn": "2021-08-11T01:55:20.547Z",
          "wordCount": 567,
          "title": "Fully Hyperbolic Graph Convolution Network for Recommendation. (arXiv:2108.04607v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Viet-Anh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salha_Galvan_G/0/1/0/all/0/1\">Guillaume Salha-Galvan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennequin_R/0/1/0/all/0/1\">Romain Hennequin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussallam_M/0/1/0/all/0/1\">Manuel Moussallam</a>",
          "description": "Collaborative Metric Learning (CML) recently emerged as a powerful paradigm\nfor recommendation based on implicit feedback collaborative filtering. However,\nstandard CML methods learn fixed user and item representations, which fails to\ncapture the complex interests of users. Existing extensions of CML also either\nignore the heterogeneity of user-item relations, i.e. that a user can\nsimultaneously like very different items, or the latent item-item relations,\ni.e. that a user's preference for an item depends, not only on its intrinsic\ncharacteristics, but also on items they previously interacted with. In this\npaper, we present a hierarchical CML model that jointly captures latent\nuser-item and item-item relations from implicit data. Our approach is inspired\nby translation mechanisms from knowledge graph embedding and leverages\nmemory-based attention networks. We empirically show the relevance of this\njoint relational modeling, by outperforming existing CML models on\nrecommendation tasks on several real-world datasets. Our experiments also\nemphasize the limits of current CML relational models on very sparse datasets.",
          "link": "http://arxiv.org/abs/2108.04655",
          "publishedOn": "2021-08-11T01:55:20.511Z",
          "wordCount": 602,
          "title": "Hierarchical Latent Relation Modeling for Collaborative Metric Learning. (arXiv:2108.04655v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_C/0/1/0/all/0/1\">Changhua Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_S/0/1/0/all/0/1\">Shanshan Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1\">Junfeng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_W/0/1/0/all/0/1\">Wenwu Ou</a>",
          "description": "Click-Through Rate (CTR) prediction is one of the core tasks in recommender\nsystems (RS). It predicts a personalized click probability for each user-item\npair. Recently, researchers have found that the performance of CTR model can be\nimproved greatly by taking user behavior sequence into consideration,\nespecially long-term user behavior sequence. The report on an e-commerce\nwebsite shows that 23\\% of users have more than 1000 clicks during the past 5\nmonths. Though there are numerous works focus on modeling sequential user\nbehaviors, few works can handle long-term user behavior sequence due to the\nstrict inference time constraint in real world system. Two-stage methods are\nproposed to push the limit for better performance. At the first stage, an\nauxiliary task is designed to retrieve the top-$k$ similar items from long-term\nuser behavior sequence. At the second stage, the classical attention mechanism\nis conducted between the candidate item and $k$ items selected in the first\nstage. However, information gap happens between retrieval stage and the main\nCTR task. This goal divergence can greatly diminishing the performance gain of\nlong-term user sequence. In this paper, inspired by Reformer, we propose a\nlocality-sensitive hashing (LSH) method called ETA (End-to-end Target\nAttention) which can greatly reduce the training and inference cost and make\nthe end-to-end training with long-term user behavior sequence possible. Both\noffline and online experiments confirm the effectiveness of our model. We\ndeploy ETA into a large-scale real world E-commerce system and achieve extra\n3.1\\% improvements on GMV (Gross Merchandise Value) compared to a two-stage\nlong user sequence CTR model.",
          "link": "http://arxiv.org/abs/2108.04468",
          "publishedOn": "2021-08-11T01:55:20.482Z",
          "wordCount": 697,
          "title": "End-to-End User Behavior Retrieval in Click-Through RatePrediction Model. (arXiv:2108.04468v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaozhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Wei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>",
          "description": "User-item interactions in recommendations can be naturally de-noted as a\nuser-item bipartite graph. Given the success of graph neural networks (GNNs) in\ngraph representation learning, GNN-based C methods have been proposed to\nadvance recommender systems. These methods often make recommendations based on\nthe learned user and item embeddings. However, we found that they do not\nperform well wit sparse user-item graphs which are quite common in real-world\nrecommendations. Therefore, in this work, we introduce a novel perspective to\nbuild GNN-based CF methods for recommendations which leads to the proposed\nframework Localized Graph Collaborative Filtering (LGCF). One key advantage of\nLGCF is that it does not need to learn embeddings for each user and item, which\nis challenging in sparse scenarios.\n\nAlternatively, LGCF aims at encoding useful CF information into a localized\ngraph and making recommendations based on such graph. Extensive experiments on\nvarious datasets validate the effectiveness of LGCF especially in sparse\nscenarios. Furthermore, empirical results demonstrate that LGCF provides\ncomplementary information to the embedding-based CF model which can be utilized\nto boost recommendation performance.",
          "link": "http://arxiv.org/abs/2108.04475",
          "publishedOn": "2021-08-11T01:55:20.455Z",
          "wordCount": 607,
          "title": "Localized Graph Collaborative Filtering. (arXiv:2108.04475v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yinqiong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yixing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "Multi-stage ranking pipelines have been a practical solution in modern search\nsystems, where the first-stage retrieval is to return a subset of candidate\ndocuments, and latter stages attempt to re-rank those candidates. Unlike\nre-ranking stages going through quick technique shifts during past decades, the\nfirst-stage retrieval has long been dominated by classical term-based models.\nUnfortunately, these models suffer from the vocabulary mismatch problem, which\nmay block re-ranking stages from relevant documents at the very beginning.\nTherefore, it has been a long-term desire to build semantic models for the\nfirst-stage retrieval that can achieve high recall efficiently. Recently, we\nhave witnessed an explosive growth of research interests on the first-stage\nsemantic retrieval models. We believe it is the right time to survey current\nstatus, learn from existing methods, and gain some insights for future\ndevelopment. In this paper, we describe the current landscape of the\nfirst-stage retrieval models under a unified framework to clarify the\nconnection between classical term-based retrieval methods, early semantic\nretrieval methods and neural semantic retrieval methods. Moreover, we identify\nsome open challenges and envision some future directions, with the hope of\ninspiring more researches on these important yet less investigated topics.",
          "link": "http://arxiv.org/abs/2103.04831",
          "publishedOn": "2021-08-10T02:00:07.204Z",
          "wordCount": 668,
          "title": "Semantic Models for the First-stage Retrieval: A Comprehensive Review. (arXiv:2103.04831v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yi-Geng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Hui-Chu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wan-Lei Zhao</a>",
          "description": "Visual object localization is the key step in a series of object detection\ntasks. In the literature, high localization accuracy is achieved with the\nmainstream strongly supervised frameworks. However, such methods require\nobject-level annotations and are unable to detect objects of unknown\ncategories. Weakly supervised methods face similar difficulties. In this paper,\na self-paced learning framework is proposed to achieve accurate object\nlocalization on the rank list returned by instance search. The proposed\nframework mines the target instance gradually from the queries and their\ncorresponding top-ranked search results. Since a common instance is shared\nbetween the query and the images in the rank list, the target visual instance\ncan be accurately localized even without knowing what the object category is.\nIn addition to performing localization on instance search, the issue of\nfew-shot object detection is also addressed under the same framework. Superior\nperformance over state-of-the-art methods is observed on both tasks.",
          "link": "http://arxiv.org/abs/2107.05005",
          "publishedOn": "2021-08-10T02:00:06.907Z",
          "wordCount": 620,
          "title": "Towards Accurate Localization by Instance Search. (arXiv:2107.05005v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>",
          "description": "Recent studies unveil the vulnerabilities of deep ranking models, where an\nimperceptible perturbation can trigger dramatic changes in the ranking result.\nWhile previous attempts focus on manipulating absolute ranks of certain\ncandidates, the possibility of adjusting their relative order remains\nunder-explored. In this paper, we formulate a new adversarial attack against\ndeep ranking systems, i.e., the Order Attack, which covertly alters the\nrelative order among a selected set of candidates according to an\nattacker-specified permutation, with limited interference to other unrelated\ncandidates. Specifically, it is formulated as a triplet-style loss imposing an\ninequality chain reflecting the specified permutation. However, direct\noptimization of such white-box objective is infeasible in a real-world attack\nscenario due to various black-box limitations. To cope with them, we propose a\nShort-range Ranking Correlation metric as a surrogate objective for black-box\nOrder Attack to approximate the white-box method. The Order Attack is evaluated\non the Fashion-MNIST and Stanford-Online-Products datasets under both white-box\nand black-box threat models. The black-box attack is also successfully\nimplemented on a major e-commerce platform. Comprehensive experimental\nevaluations demonstrate the effectiveness of the proposed methods, revealing a\nnew type of ranking model vulnerability.",
          "link": "http://arxiv.org/abs/2103.05248",
          "publishedOn": "2021-08-10T02:00:06.806Z",
          "wordCount": 686,
          "title": "Practical Relative Order Attack in Deep Ranking. (arXiv:2103.05248v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.06389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Suraj Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1\">Dwaipayan Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_M/0/1/0/all/0/1\">Mandar Mitra</a>",
          "description": "Personalized Point of Interest recommendation is very helpful for satisfying\nusers' needs at new places. In this article, we propose a tag embedding based\nmethod for Personalized Recommendation of Point Of Interest. We model the\nrelationship between tags corresponding to Point Of Interest. The model\nprovides representative embedding corresponds to a tag in a way that related\ntags will be closer. We model Point of Interest-based on tag embedding and also\nmodel the users (user profile) based on the Point Of Interest rated by them.\nfinally, we rank the user's candidate Point Of Interest based on cosine\nsimilarity between user's embedding and Point of Interest's embedding. Further,\nwe find the parameters required to model user by discrete optimizing over\ndifferent measures (like ndcg@5, MRR, ...). We also analyze the result while\nconsidering the same parameters for all users and individual parameters for\neach user. Along with it we also analyze the effect on the result while\nchanging the dataset to model the relationship between tags. Our method also\nminimizes the privacy leak issue. We used TREC Contextual Suggestion 2016 Phase\n2 dataset and have significant improvement over all the measures on the state\nof the art method. It improves ndcg@5 by 12.8%, p@5 by 4.3%, and MRR by 7.8%,\nwhich shows the effectiveness of the method.",
          "link": "http://arxiv.org/abs/2004.06389",
          "publishedOn": "2021-08-10T02:00:06.784Z",
          "wordCount": 689,
          "title": "Tag Embedding Based Personalized Point Of Interest Recommendation System. (arXiv:2004.06389v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1811.00414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_E/0/1/0/all/0/1\">Ewin Tang</a>",
          "description": "A central roadblock to analyzing quantum algorithms on quantum states is the\nlack of a comparable input model for classical algorithms. Inspired by recent\nwork of the author [E. Tang, STOC'19], we introduce such a model, where we\nassume we can efficiently perform $\\ell^2$-norm samples of input data, a\nnatural analogue to quantum algorithms that assume efficient state preparation\nof classical data. Though this model produces less practical algorithms than\nthe (stronger) standard model of classical computation, it captures versions of\nmany of the features and nuances of quantum linear algebra algorithms. With\nthis model, we describe classical analogues to Lloyd, Mohseni, and Rebentrost's\nquantum algorithms for principal component analysis [Nat. Phys. 10, 631 (2014)]\nand nearest-centroid clustering [arXiv:1307.0411]. Since they are only\npolynomially slower, these algorithms suggest that the exponential speedups of\ntheir quantum counterparts are simply an artifact of state preparation\nassumptions.",
          "link": "http://arxiv.org/abs/1811.00414",
          "publishedOn": "2021-08-10T02:00:06.764Z",
          "wordCount": 673,
          "title": "Quantum principal component analysis only achieves an exponential speedup because of its state preparation assumptions. (arXiv:1811.00414v3 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08553",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>",
          "description": "We propose Generation-Augmented Retrieval (GAR) for answering open-domain\nquestions, which augments a query through text generation of heuristically\ndiscovered relevant contexts without external resources as supervision. We\ndemonstrate that the generated contexts substantially enrich the semantics of\nthe queries and GAR with sparse representations (BM25) achieves comparable or\nbetter performance than state-of-the-art dense retrieval methods such as DPR.\nWe show that generating diverse contexts for a query is beneficial as fusing\ntheir results consistently yields better retrieval accuracy. Moreover, as\nsparse and dense representations are often complementary, GAR can be easily\ncombined with DPR to achieve even better performance. GAR achieves\nstate-of-the-art performance on Natural Questions and TriviaQA datasets under\nthe extractive QA setup when equipped with an extractive reader, and\nconsistently outperforms other retrieval methods when the same generative\nreader is used.",
          "link": "http://arxiv.org/abs/2009.08553",
          "publishedOn": "2021-08-10T02:00:06.756Z",
          "wordCount": 624,
          "title": "Generation-Augmented Retrieval for Open-domain Question Answering. (arXiv:2009.08553v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ashwin Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Adriel Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Steven QH Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1\">Du Nguyen Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>",
          "description": "Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.",
          "link": "http://arxiv.org/abs/2106.14463",
          "publishedOn": "2021-08-10T02:00:06.744Z",
          "wordCount": 725,
          "title": "RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>",
          "description": "Current open-domain question answering systems often follow a\nRetriever-Reader architecture, where the retriever first retrieves relevant\npassages and the reader then reads the retrieved passages to form an answer. In\nthis paper, we propose a simple and effective passage reranking method, named\nReader-guIDEd Reranker (RIDER), which does not involve training and reranks the\nretrieved passages solely based on the top predictions of the reader before\nreranking. We show that RIDER, despite its simplicity, achieves 10 to 20\nabsolute gains in top-1 retrieval accuracy and 1 to 4 Exact Match (EM) gains\nwithout refining the retriever or reader. In addition, RIDER, without any\ntraining, outperforms state-of-the-art transformer-based supervised rerankers.\nRemarkably, RIDER achieves 48.3 EM on the Natural Questions dataset and 66.4 EM\non the TriviaQA dataset when only 1,024 tokens (7.8 passages on average) are\nused as the reader input after passage reranking.",
          "link": "http://arxiv.org/abs/2101.00294",
          "publishedOn": "2021-08-10T02:00:06.692Z",
          "wordCount": 657,
          "title": "Rider: Reader-Guided Passage Reranking for Open-Domain Question Answering. (arXiv:2101.00294v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zekai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1\">Fangtian Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pless_R/0/1/0/all/0/1\">Robert Pless</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiuzhen Cheng</a>",
          "description": "User response prediction, which aims to predict the probability that a user\nwill provide a predefined positive response in a given context such as clicking\non an ad or purchasing an item, is crucial to many industrial applications such\nas online advertising, recommender systems, and search ranking. However, due to\nthe high dimensionality and super sparsity of the data collected in these\ntasks, handcrafting cross features is inevitably time expensive. Prior studies\nin predicting user response leveraged the feature interactions by enhancing\nfeature vectors with products of features to model second-order or high-order\ncross features, either explicitly or implicitly. Nevertheless, these existing\nmethods can be hindered by not learning sufficient cross features due to model\narchitecture limitations or modeling all high-order feature interactions with\nequal weights. This work aims to fill this gap by proposing a novel\narchitecture Deep Cross Attentional Product Network (DCAP), which keeps cross\nnetwork's benefits in modeling high-order feature interactions explicitly at\nthe vector-wise level. Beyond that, it can differentiate the importance of\ndifferent cross features in each network layer inspired by the multi-head\nattention mechanism and Product Neural Network (PNN), allowing practitioners to\nperform a more in-depth analysis of user behaviors. Additionally, our proposed\nmodel can be easily implemented and train in parallel. We conduct comprehensive\nexperiments on three real-world datasets. The results have robustly\ndemonstrated that our proposed model DCAP achieves superior prediction\nperformance compared with the state-of-the-art models.",
          "link": "http://arxiv.org/abs/2105.08649",
          "publishedOn": "2021-08-10T02:00:06.683Z",
          "wordCount": 720,
          "title": "DCAP: Deep Cross Attentional Product Network for User Response Prediction. (arXiv:2105.08649v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kostic_B/0/1/0/all/0/1\">Bogdan Kosti&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risch_J/0/1/0/all/0/1\">Julian Risch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1\">Timo M&#xf6;ller</a>",
          "description": "Open-domain extractive question answering works well on textual data by first\nretrieving candidate texts and then extracting the answer from those\ncandidates. However, some questions cannot be answered by text alone but\nrequire information stored in tables. In this paper, we present an approach for\nretrieving both texts and tables relevant to a question by jointly encoding\ntexts, tables and questions into a single vector space. To this end, we create\na new multi-modal dataset based on text and table datasets from related work\nand compare the retrieval performance of different encoding schemata. We find\nthat dense vector embeddings of transformer models outperform sparse embeddings\non four out of six evaluation datasets. Comparing different dense embedding\nmodels, tri-encoders, with one encoder for each question, text and table,\nincrease retrieval performance compared to bi-encoders with one encoder for the\nquestion and one for both text and tables. We release the newly created\nmulti-modal dataset to the community so that it can be used for training and\nevaluation.",
          "link": "http://arxiv.org/abs/2108.04049",
          "publishedOn": "2021-08-10T02:00:06.635Z",
          "wordCount": 601,
          "title": "Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models. (arXiv:2108.04049v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huimin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Rongwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhuyun Qi</a>",
          "description": "In the current deep learning based recommendation system, the embedding\nmethod is generally employed to complete the conversion from the\nhigh-dimensional sparse feature vector to the low-dimensional dense feature\nvector. However, as the dimension of the input vector of the embedding layer is\ntoo large, the addition of the embedding layer significantly slows down the\nconvergence speed of the entire neural network, which is not acceptable in\nreal-world scenarios. In addition, as the interaction between users and items\nincreases and the relationship between items becomes more complicated, the\nembedding method proposed for sequence data is no longer suitable for graphic\ndata in the current real environment. Therefore, in this paper, we propose the\nDual-modal Graph Embedding Method (DGEM) to solve these problems. DGEM includes\ntwo modes, static and dynamic. We first construct the item graph to extract the\ngraph structure and use random walk of unequal probability to capture the\nhigh-order proximity between the items. Then we generate the graph embedding\nvector through the Skip-Gram model, and finally feed the downstream deep neural\nnetwork for the recommendation task. The experimental results show that DGEM\ncan mine the high-order proximity between items and enhance the expression\nability of the recommendation model. Meanwhile it also improves the\nrecommendation performance by utilizing the time dependent relationship between\nitems.",
          "link": "http://arxiv.org/abs/2108.04031",
          "publishedOn": "2021-08-10T02:00:06.621Z",
          "wordCount": 659,
          "title": "DGEM: A New Dual-modal Graph Embedding Method in Recommendation System. (arXiv:2108.04031v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Opazo_C/0/1/0/all/0/1\">Cristian Rodriguez-Opazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1\">Damien Teney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1\">Stephen Gould</a>",
          "description": "We extend the task of composed image retrieval, where an input query consists\nof an image and short textual description of how to modify the image. Existing\nmethods have only been applied to non-complex images within narrow domains,\nsuch as fashion products, thereby limiting the scope of study on in-depth\nvisual reasoning in rich image and language contexts. To address this issue, we\ncollect the Compose Image Retrieval on Real-life images (CIRR) dataset, which\nconsists of over 36,000 pairs of crowd-sourced, open-domain images with\nhuman-generated modifying text. To extend current methods to the open-domain,\nwe propose CIRPLANT, a transformer based model that leverages rich pre-trained\nvision-and-language (V&L) knowledge for modifying visual features conditioned\non natural language. Retrieval is then done by nearest neighbor lookup on the\nmodified features. We demonstrate that with a relatively simple architecture,\nCIRPLANT outperforms existing methods on open-domain images, while matching\nstate-of-the-art accuracy on the existing narrow datasets, such as fashion.\nTogether with the release of CIRR, we believe this work will inspire further\nresearch on composed image retrieval.",
          "link": "http://arxiv.org/abs/2108.04024",
          "publishedOn": "2021-08-10T02:00:06.584Z",
          "wordCount": 634,
          "title": "Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models. (arXiv:2108.04024v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+MacAvaney_S/0/1/0/all/0/1\">Sean MacAvaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macdonald_C/0/1/0/all/0/1\">Craig Macdonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_Smith_R/0/1/0/all/0/1\">Roderick Murray-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ounis_I/0/1/0/all/0/1\">Iadh Ounis</a>",
          "description": "Search result diversification is a beneficial approach to overcome\nunder-specified queries, such as those that are ambiguous or multi-faceted.\nExisting approaches often rely on massive query logs and interaction data to\ngenerate a variety of possible query intents, which then can be used to re-rank\ndocuments. However, relying on user interaction data is problematic because one\nfirst needs a massive user base to build a sufficient log; public query logs\nare insufficient on their own. Given the recent success of causal language\nmodels (such as the Text-To-Text Transformer (T5) model) at text generation\ntasks, we explore the capacity of these models to generate potential query\nintents. We find that to encourage diversity in the generated queries, it is\nbeneficial to adapt the model by including a new Distributional Causal Language\nModeling (DCLM) objective during fine-tuning and a representation replacement\nduring inference. Across six standard evaluation benchmarks, we find that our\nmethod (which we call IntenT5) improves search result diversity and attains\n(and sometimes exceeds) the diversity obtained when using query suggestions\nbased on a proprietary query log. Our analysis shows that our approach is most\neffective for multi-faceted queries and is able to generalize effectively to\nqueries that were unseen in training data.",
          "link": "http://arxiv.org/abs/2108.04026",
          "publishedOn": "2021-08-10T02:00:06.574Z",
          "wordCount": 633,
          "title": "IntenT5: Search Result Diversification using Causal Language Models. (arXiv:2108.04026v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>",
          "description": "This paper presents a three-tier modality alignment approach to learning\ntext-image joint embedding, coined as JEMA, for cross-modal retrieval of\ncooking recipes and food images. The first tier improves recipe text embedding\nby optimizing the LSTM networks with term extraction and ranking enhanced\nsequence patterns, and optimizes the image embedding by combining the\nResNeXt-101 image encoder with the category embedding using wideResNet-50 with\nword2vec. The second tier modality alignment optimizes the textual-visual joint\nembedding loss function using a double batch-hard triplet loss with soft-margin\noptimization. The third modality alignment incorporates two types of\ncross-modality alignments as the auxiliary loss regularizations to further\nreduce the alignment errors in the joint learning of the two modality-specific\nembedding functions. The category-based cross-modal alignment aims to align the\nimage category with the recipe category as a loss regularization to the joint\nembedding. The cross-modal discriminator-based alignment aims to add the\nvisual-textual embedding distribution alignment to further regularize the joint\nembedding loss. Extensive experiments with the one-million recipes benchmark\ndataset Recipe1M demonstrate that the proposed JEMA approach outperforms the\nstate-of-the-art cross-modal embedding methods for both image-to-recipe and\nrecipe-to-image retrievals.",
          "link": "http://arxiv.org/abs/2108.03788",
          "publishedOn": "2021-08-10T02:00:06.565Z",
          "wordCount": 641,
          "title": "Learning Joint Embedding with Modality Alignments for Cross-Modal Retrieval of Recipes and Food Images. (arXiv:2108.03788v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Althammer_S/0/1/0/all/0/1\">Sophia Althammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askari_A/0/1/0/all/0/1\">Arian Askari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verberne_S/0/1/0/all/0/1\">Suzan Verberne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1\">Allan Hanbury</a>",
          "description": "In this paper, we present our approaches for the case law retrieval and the\nlegal case entailment task in the Competition on Legal Information\nExtraction/Entailment (COLIEE) 2021. As first stage retrieval methods combined\nwith neural re-ranking methods using contextualized language models like BERT\nachieved great performance improvements for information retrieval in the web\nand news domain, we evaluate these methods for the legal domain. A distinct\ncharacteristic of legal case retrieval is that the query case and case\ndescription in the corpus tend to be long documents and therefore exceed the\ninput length of BERT. We address this challenge by combining lexical and dense\nretrieval methods on the paragraph-level of the cases for the first stage\nretrieval. Here we demonstrate that the retrieval on the paragraph-level\noutperforms the retrieval on the document-level. Furthermore the experiments\nsuggest that dense retrieval methods outperform lexical retrieval. For\nre-ranking we address the problem of long documents by summarizing the cases\nand fine-tuning a BERT-based re-ranker with the summaries. Overall, our best\nresults were obtained with a combination of BM25 and dense passage retrieval\nusing domain-specific embeddings.",
          "link": "http://arxiv.org/abs/2108.03937",
          "publishedOn": "2021-08-10T02:00:06.532Z",
          "wordCount": 624,
          "title": "DoSSIER@COLIEE 2021: Leveraging dense retrieval and summarization-based re-ranking for case law retrieval. (arXiv:2108.03937v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03586",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhizhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>",
          "description": "Numerous neural retrieval models have been proposed in recent years. These\nmodels learn to compute a ranking score between the given query and document.\nThe majority of existing models are trained in pairwise fashion using\nhuman-judged labels directly without further calibration. The traditional\npairwise schemes can be time-consuming and require pre-defined\npositive-negative document pairs for training, potentially leading to learning\nbias due to document distribution mismatch between training and test\nconditions. Some popular existing listwise schemes rely on the strong\npre-defined probabilistic assumptions and stark difference between relevant and\nnon-relevant documents for the given query, which may limit the model potential\ndue to the low-quality or ambiguous relevance labels. To address these\nconcerns, we turn to a physics-inspired ranking balance scheme and propose\nPoolRank, a pooling-based listwise learning framework. The proposed scheme has\nfour major advantages: (1) PoolRank extracts training information from the best\ncandidates at the local level based on model performance and relative ranking\namong abundant document candidates. (2) By combining four pooling-based loss\ncomponents in a multi-task learning fashion, PoolRank calibrates the ranking\nbalance for the partially relevant and the highly non-relevant documents\nautomatically without costly human inspection. (3) PoolRank can be easily\ngeneralized to any neural retrieval model without requiring additional\nlearnable parameters or model structure modifications. (4) Compared to pairwise\nlearning and existing listwise learning schemes, PoolRank yields better ranking\nperformance for all studied retrieval models while retaining efficient\nconvergence rates.",
          "link": "http://arxiv.org/abs/2108.03586",
          "publishedOn": "2021-08-10T02:00:06.513Z",
          "wordCount": 666,
          "title": "PoolRank: Max/Min Pooling-based Ranking Loss for Listwise Learning & Ranking Balance. (arXiv:2108.03586v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zang_T/0/1/0/all/0/1\">Tianzi Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanmin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haobing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiadi Yu</a>",
          "description": "Traditional recommendation systems are faced with two long-standing\nobstacles, namely, data sparsity and cold-start problems, which promote the\nemergence and development of Cross-Domain Recommendation (CDR). The core idea\nof CDR is to leverage information collected from other domains to alleviate the\ntwo problems in one domain. Over the last decade, many efforts have been\nengaged for cross-domain recommendation. Recently, with the development of deep\nlearning and neural networks, a large number of methods have emerged. However,\nthere is a limited number of systematic surveys on CDR, especially regarding\nthe latest proposed methods as well as the recommendation scenarios and\nrecommendation tasks they address. In this survey paper, we first proposed a\ntwo-level taxonomy of cross-domain recommendation which classifies different\nrecommendation scenarios and recommendation tasks. We then introduce and\nsummarize existing cross-domain recommendation approaches under different\nrecommendation scenarios in a structured manner. We also organize datasets\ncommonly used. We conclude this survey by providing several potential research\ndirections about this field.",
          "link": "http://arxiv.org/abs/2108.03357",
          "publishedOn": "2021-08-10T02:00:06.499Z",
          "wordCount": 600,
          "title": "A Survey on Cross-domain Recommendation: Taxonomies, Methods, and Future Directions. (arXiv:2108.03357v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mansoury_M/0/1/0/all/0/1\">Masoud Mansoury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdollahpouri_H/0/1/0/all/0/1\">Himan Abdollahpouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mobasher_B/0/1/0/all/0/1\">Bamshad Mobasher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burke_R/0/1/0/all/0/1\">Robin Burke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabouri_M/0/1/0/all/0/1\">Milad Sabouri</a>",
          "description": "Exposure bias is a well-known issue in recommender systems where items and\nsuppliers are not equally represented in the recommendation results. This is\nespecially problematic when bias is amplified over time as a few popular items\nare repeatedly over-represented in recommendation lists. This phenomenon can be\nviewed as a recommendation feedback loop: the system repeatedly recommends\ncertain items at different time points and interactions of users with those\nitems will amplify bias towards those items over time. This issue has been\nextensively studied in the literature on model-based or neighborhood-based\nrecommendation algorithms, but less work has been done on online recommendation\nmodels such as those based on multi-armed Bandit algorithms. In this paper, we\nstudy exposure bias in a class of well-known bandit algorithms known as Linear\nCascade Bandits. We analyze these algorithms on their ability to handle\nexposure bias and provide a fair representation for items and suppliers in the\nrecommendation results. Our analysis reveals that these algorithms fail to\ntreat items and suppliers fairly and do not sufficiently explore the item space\nfor each user. To mitigate this bias, we propose a discounting factor and\nincorporate it into these algorithms that controls the exposure of items at\neach time step. To show the effectiveness of the proposed discounting factor on\nmitigating exposure bias, we perform experiments on two datasets using three\ncascading bandit algorithms and our experimental results show that the proposed\nmethod improves the exposure fairness for items and suppliers.",
          "link": "http://arxiv.org/abs/2108.03440",
          "publishedOn": "2021-08-10T02:00:06.483Z",
          "wordCount": 684,
          "title": "Unbiased Cascade Bandits: Mitigating Exposure Bias in Online Learning to Rank Recommendation. (arXiv:2108.03440v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03322",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Clement_C/0/1/0/all/0/1\">Colin B. Clement</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>",
          "description": "Pre-trained transformers have recently clinched top spots in the gamut of\nnatural language tasks and pioneered solutions to software engineering tasks.\nEven information retrieval has not been immune to the charm of the transformer,\nthough their large size and cost is generally a barrier to deployment. While\nthere has been much work in streamlining, caching, and modifying transformer\narchitectures for production, here we explore a new direction: distilling a\nlarge pre-trained translation model into a lightweight bi-encoder which can be\nefficiently cached and queried. We argue from a probabilistic perspective that\nsequence-to-sequence models are a conceptually ideal---albeit highly\nimpractical---retriever. We derive a new distillation objective, implementing\nit as a data augmentation scheme. Using natural language source code search as\na case study for cross-domain search, we demonstrate the validity of this idea\nby significantly improving upon the current leader of the CodeSearchNet\nchallenge, a recent natural language code search benchmark.",
          "link": "http://arxiv.org/abs/2108.03322",
          "publishedOn": "2021-08-10T02:00:06.467Z",
          "wordCount": 593,
          "title": "Distilling Transformers for Neural Cross-Domain Search. (arXiv:2108.03322v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jyun-Yu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chia-Jung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Longqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarrafzadeh_B/0/1/0/all/0/1\">Bahareh Sarrafzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hecht_B/0/1/0/all/0/1\">Brent Hecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teevan_J/0/1/0/all/0/1\">Jaime Teevan</a>",
          "description": "Motives or goals are recognized in psychology literature as the most\nfundamental drive that explains and predicts why people do what they do,\nincluding when they browse the web. Although providing enormous value, these\nhigher-ordered goals are often unobserved, and little is known about how to\nleverage such goals to assist people's browsing activities. This paper proposes\nto take a new approach to address this problem, which is fulfilled through a\nnovel neural framework, Goal-directed Web Browsing (GoWeB). We adopt a\npsychologically-sound taxonomy of higher-ordered goals and learn to build their\nrepresentations in a structure-preserving manner. Then we incorporate the\nresulting representations for enhancing the experiences of common activities\npeople perform on the web. Experiments on large-scale data from Microsoft Edge\nweb browser show that GoWeB significantly outperforms competitive baselines for\nin-session web page recommendation, re-visitation classification, and\ngoal-based web page grouping. A follow-up analysis further characterizes how\nthe variety of human motives can affect the difference observed in human\nbehavioral patterns.",
          "link": "http://arxiv.org/abs/2108.03350",
          "publishedOn": "2021-08-10T02:00:06.432Z",
          "wordCount": 604,
          "title": "Learning to Represent Human Motives for Goal-directed Web Browsing. (arXiv:2108.03350v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1\">Sawood Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weigle_M/0/1/0/all/0/1\">Michele C. Weigle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nelson_M/0/1/0/all/0/1\">Michael L. Nelson</a>",
          "description": "Prior work on web archive profiling were focused on Archival Holdings to\ndescribe what is present in an archive. This work defines and explores Archival\nVoids to establish a means to represent portions of URI spaces that are not\npresent in a web archive. Archival Holdings and Archival Voids profiles can\nwork independently or as complements to each other to maximize the Accuracy of\nMemento Aggregators. We discuss various sources of truth that can be used to\ncreate Archival Voids profiles. We use access logs from Arquivo.pt to create\nvarious Archival Voids profiles and analyze them against our MemGator access\nlogs for evaluation. We find that we could have avoided more than 8% of\nadditional False Positives on top of the 60% Accuracy we got from profiling\nArchival Holdings in our prior work, if Arquivo.pt were to provide an Archival\nVoids profile based on URIs that were requested hundreds of times and never\nreturned any success responses.",
          "link": "http://arxiv.org/abs/2108.03311",
          "publishedOn": "2021-08-10T02:00:06.406Z",
          "wordCount": 603,
          "title": "Profiling Web Archival Voids for Memento Routing. (arXiv:2108.03311v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03374",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Adhikari_P/0/1/0/all/0/1\">Poonam Adhikari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Ritesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyengar_S/0/1/0/all/0/1\">S.R.S Iyengar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_R/0/1/0/all/0/1\">Rishemjit Kaur</a>",
          "description": "Many different technologies are used to detect pests in the crops, such as\nmanual sampling, sensors, and radar. However, these methods have scalability\nissues as they fail to cover large areas, are uneconomical and complex. This\npaper proposes a crowdsourced based method utilising the real-time farmer\nqueries gathered over telephones for pest surveillance. We developed\ndata-driven strategies by aggregating and analyzing historical data to find\npatterns and get future insights into pest occurrence. We showed that it can be\nan accurate and economical method for pest surveillance capable of enveloping a\nlarge area with high spatio-temporal granularity. Forecasting the pest\npopulation will help farmers in making informed decisions at the right time.\nThis will also help the government and policymakers to make the necessary\npreparations as and when required and may also ensure food security.",
          "link": "http://arxiv.org/abs/2108.03374",
          "publishedOn": "2021-08-10T02:00:06.386Z",
          "wordCount": 580,
          "title": "What a million Indian farmers say?: A crowdsourcing-based method for pest surveillance. (arXiv:2108.03374v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03576",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Heydari_M/0/1/0/all/0/1\">Mojtaba Heydari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cwitkowitz_F/0/1/0/all/0/1\">Frank Cwitkowitz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_Z/0/1/0/all/0/1\">Zhiyao Duan</a>",
          "description": "The online estimation of rhythmic information, such as beat positions,\ndownbeat positions, and meter, is critical for many real-time music\napplications. Musical rhythm comprises complex hierarchical relationships\nacross time, rendering its analysis intrinsically challenging and at times\nsubjective. Furthermore, systems which attempt to estimate rhythmic information\nin real-time must be causal and must produce estimates quickly and efficiently.\nIn this work, we introduce an online system for joint beat, downbeat, and meter\ntracking, which utilizes causal convolutional and recurrent layers, followed by\na pair of sequential Monte Carlo particle filters applied during inference. The\nproposed system does not need to be primed with a time signature in order to\nperform downbeat tracking, and is instead able to estimate meter and adjust the\npredictions over time. Additionally, we propose an information gate strategy to\nsignificantly decrease the computational cost of particle filtering during the\ninference step, making the system much faster than previous sampling-based\nmethods. Experiments on the GTZAN dataset, which is unseen during training,\nshow that the system outperforms various online beat and downbeat tracking\nsystems and achieves comparable performance to a baseline offline joint method.",
          "link": "http://arxiv.org/abs/2108.03576",
          "publishedOn": "2021-08-10T02:00:06.347Z",
          "wordCount": 672,
          "title": "BeatNet: CRNN and Particle Filtering for Online Joint Beat Downbeat and Meter Tracking. (arXiv:2108.03576v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2103.06523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaekeol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_E/0/1/0/all/0/1\">Euna Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suh_J/0/1/0/all/0/1\">Jangwon Suh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1\">Wonjong Rhee</a>",
          "description": "BERT-based Neural Ranking Models (NRMs) can be classified according to how\nthe query and document are encoded through BERT's self-attention layers -\nbi-encoder versus cross-encoder. Bi-encoder models are highly efficient because\nall the documents can be pre-processed before the query time, but their\nperformance is inferior compared to cross-encoder models. Both models utilize a\nranker that receives BERT representations as the input and generates a\nrelevance score as the output. In this work, we propose a method where\nmulti-teacher distillation is applied to a cross-encoder NRM and a bi-encoder\nNRM to produce a bi-encoder NRM with two rankers. The resulting student\nbi-encoder achieves an improved performance by simultaneously learning from a\ncross-encoder teacher and a bi-encoder teacher and also by combining relevance\nscores from the two rankers. We call this method TRMD (Two Rankers and\nMulti-teacher Distillation). In the experiments, TwinBERT and ColBERT are\nconsidered as baseline bi-encoders. When monoBERT is used as the cross-encoder\nteacher, together with either TwinBERT or ColBERT as the bi-encoder teacher,\nTRMD produces a student bi-encoder that performs better than the corresponding\nbaseline bi-encoder. For P@20, the maximum improvement was 11.4%, and the\naverage improvement was 6.8%. As an additional experiment, we considered\nproducing cross-encoder students with TRMD, and found that it could also\nimprove the cross-encoders.",
          "link": "http://arxiv.org/abs/2103.06523",
          "publishedOn": "2021-08-09T00:49:26.829Z",
          "wordCount": 688,
          "title": "Improving Bi-encoder Document Ranking Models with Two Rankers and Multi-teacher Distillation. (arXiv:2103.06523v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Knittel_J/0/1/0/all/0/1\">Johannes Knittel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_S/0/1/0/all/0/1\">Steffen Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yingcai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shixia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ertl_T/0/1/0/all/0/1\">Thomas Ertl</a>",
          "description": "Breaking news and first-hand reports often trend on social media platforms\nbefore traditional news outlets cover them. The real-time analysis of posts on\nsuch platforms can reveal valuable and timely insights for journalists,\npoliticians, business analysts, and first responders, but the high number and\ndiversity of new posts pose a challenge. In this work, we present an\ninteractive system that enables the visual analysis of streaming social media\ndata on a large scale in real-time. We propose an efficient and explainable\ndynamic clustering algorithm that powers a continuously updated visualization\nof the current thematic landscape as well as detailed visual summaries of\nspecific topics of interest. Our parallel clustering strategy provides an\nadaptive stream with a digestible but diverse selection of recent posts related\nto relevant topics. We also integrate familiar visual metaphors that are highly\ninterlinked for enabling both explorative and more focused monitoring tasks.\nAnalysts can gradually increase the resolution to dive deeper into particular\ntopics. In contrast to previous work, our system also works with non-geolocated\nposts and avoids extensive preprocessing such as detecting events. We evaluated\nour dynamic clustering algorithm and discuss several use cases that show the\nutility of our system.",
          "link": "http://arxiv.org/abs/2108.03052",
          "publishedOn": "2021-08-09T00:49:26.725Z",
          "wordCount": 650,
          "title": "Real-Time Visual Analysis of High-Volume Social Media Posts. (arXiv:2108.03052v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaochen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenzhong Chen</a>",
          "description": "Hybrid recommendations have recently attracted a lot of attention where user\nfeatures are utilized as auxiliary information to address the sparsity problem\ncaused by insufficient user-item interactions. However, extracted user features\ngenerally contain rich multimodal information, and most of them are irrelevant\nto the recommendation purpose. Therefore, excessive reliance on these features\nwill make the model overfit on noise and difficult to generalize. In this\narticle, we propose a variational bandwidth auto-encoder (VBAE) for\nrecommendations, aiming to address the sparsity and noise problems\nsimultaneously. VBAE first encodes user collaborative and feature information\ninto Gaussian latent variables via deep neural networks to capture non-linear\nuser similarities. Moreover, by considering the fusion of collaborative and\nfeature variables as a virtual communication channel from an\ninformation-theoretic perspective, we introduce a user-dependent channel to\ndynamically control the information allowed to be accessed from the feature\nembeddings. A quantum-inspired uncertainty measurement of the hidden rating\nembeddings is proposed accordingly to infer the channel bandwidth by\ndisentangling the uncertainty information in the ratings from the semantic\ninformation. Through this mechanism, VBAE incorporates adequate auxiliary\ninformation from user features if collaborative information is insufficient,\nwhile avoiding excessive reliance on noisy user features to improve its\ngeneralization ability to new users. Extensive experiments conducted on three\nreal-world datasets demonstrate the effectiveness of the proposed method. Codes\nand datasets are released at https://github.com/yaochenzhu/vbae.",
          "link": "http://arxiv.org/abs/2105.07597",
          "publishedOn": "2021-08-09T00:49:26.713Z",
          "wordCount": 678,
          "title": "Variational Bandwidth Auto-encoder for Hybrid Recommender Systems. (arXiv:2105.07597v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>",
          "description": "Recommender system recommends interesting items to users based on users' past\ninformation history. Researchers have been paying attention to improvement of\nalgorithmic performance such as MAE and precision@K. Major techniques such as\nmatrix factorization and learning to rank are optimized based on such\nevaluation metrics. However, the intrinsic Matthew Effect problem poses great\nthreat to the fairness of the recommender system, and the unfairness problem\ncannot be resolved by optimization of traditional metrics. In this paper, we\npropose a novel algorithm that incorporates Matthew Effect reduction with the\nmatrix factorization framework. We demonstrate that our approach can boost the\nfairness of the algorithm and enhances performance evaluated by traditional\nmetrics.",
          "link": "http://arxiv.org/abs/2106.07347",
          "publishedOn": "2021-08-06T00:51:44.435Z",
          "wordCount": 566,
          "title": "Zipf Matrix Factorization : Matrix Factorization with Matthew Effect Reduction. (arXiv:2106.07347v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06416",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petrescu_D/0/1/0/all/0/1\">Diana Petrescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>",
          "description": "Recommendations with personalized explanations have been shown to increase\nuser trust and perceived quality and help users make better decisions.\nMoreover, such explanations allow users to provide feedback by critiquing them.\nSeveral algorithms for recommender systems with multi-step critiquing have\ntherefore been developed. However, providing a user-friendly interface based on\npersonalized explanations and critiquing has not been addressed in the last\ndecade. In this paper, we introduce four different web interfaces (available\nunder https://lia.epfl.ch/critiquing/) helping users making decisions and\nfinding their ideal item. We have chosen the hotel recommendation domain as a\nuse case even though our approach is trivially adaptable for other domains.\nMoreover, our system is model-agnostic (for both recommender systems and\ncritiquing models) allowing a great flexibility and further extensions. Our\ninterfaces are above all a useful tool to help research in recommendation with\ncritiquing. They allow to test such systems on a real use case and also to\nhighlight some limitations of these approaches to find solutions to overcome\nthem.",
          "link": "http://arxiv.org/abs/2107.06416",
          "publishedOn": "2021-08-06T00:51:44.129Z",
          "wordCount": 629,
          "title": "Multi-Step Critiquing User Interface for Recommender Systems. (arXiv:2107.06416v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rafee_S/0/1/0/all/0/1\">Syed Rifat Mahmud Rafee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gyorgy Fazekas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiggins_G/0/1/0/all/0/1\">Geraint A.~Wiggins</a>",
          "description": "Music Performers have their own idiosyncratic way of interpreting a musical\npiece. A group of skilled performers playing the same piece of music would\nlikely to inject their unique artistic styles in their performances. The\nvariations of the tempo, timing, dynamics, articulation etc. from the actual\nnotated music are what make the performers unique in their performances. This\nstudy presents a dataset consisting of four movements of Schubert's ``Sonata in\nB-flat major, D.960\" performed by nine virtuoso pianists individually. We\nproposed and extracted a set of expressive features that are able to capture\nthe characteristics of an individual performer's style. We then present a\nperformer identification method based on the similarity of feature\ndistribution, given a set of piano performances. The identification is done\nconsidering each feature individually as well as a fusion of the features.\nResults show that the proposed method achieved a precision of 0.903 using\nfusion features. Moreover, the onset time deviation feature shows promising\nresult when considered individually.",
          "link": "http://arxiv.org/abs/2108.02576",
          "publishedOn": "2021-08-06T00:51:44.089Z",
          "wordCount": 610,
          "title": "Performer Identification From Symbolic Representation of Music Using Statistical Models. (arXiv:2108.02576v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02634",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuyue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wei Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yashen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haiyong Xie</a>",
          "description": "Reasoning on knowledge graph (KG) has been studied for explainable\nrecommendation due to it's ability of providing explicit explanations. However,\ncurrent KG-based explainable recommendation methods unfortunately ignore the\ntemporal information (such as purchase time, recommend time, etc.), which may\nresult in unsuitable explanations. In this work, we propose a novel Time-aware\nPath reasoning for Recommendation (TPRec for short) method, which leverages the\npotential of temporal information to offer better recommendation with plausible\nexplanations. First, we present an efficient time-aware interaction relation\nextraction component to construct collaborative knowledge graph with time-aware\ninteractions (TCKG for short), and then introduce a novel time-aware path\nreasoning method for recommendation. We conduct extensive experiments on three\nreal-world datasets. The results demonstrate that the proposed TPRec could\nsuccessfully employ TCKG to achieve substantial gains and improve the quality\nof explainable recommendation.",
          "link": "http://arxiv.org/abs/2108.02634",
          "publishedOn": "2021-08-06T00:51:44.076Z",
          "wordCount": 585,
          "title": "Time-aware Path Reasoning on Knowledge Graph for Recommendation. (arXiv:2108.02634v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demirel_E/0/1/0/all/0/1\">Emir Demirel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahlback_S/0/1/0/all/0/1\">Sven Ahlb&#xe4;ck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixon_S/0/1/0/all/0/1\">Simon Dixon</a>",
          "description": "This paper makes several contributions to automatic lyrics transcription\n(ALT) research. Our main contribution is a novel variant of the Multistreaming\nTime-Delay Neural Network (MTDNN) architecture, called MSTRE-Net, which\nprocesses the temporal information using multiple streams in parallel with\nvarying resolutions keeping the network more compact, and thus with a faster\ninference and an improved recognition rate than having identical TDNN streams.\nIn addition, two novel preprocessing steps prior to training the acoustic model\nare proposed. First, we suggest using recordings from both monophonic and\npolyphonic domains during training the acoustic model. Second, we tag\nmonophonic and polyphonic recordings with distinct labels for discriminating\nnon-vocal silence and music instances during alignment. Moreover, we present a\nnew test set with a considerably larger size and a higher musical variability\ncompared to the existing datasets used in ALT literature, while maintaining the\ngender balance of the singers. Our best performing model sets the\nstate-of-the-art in lyrics transcription by a large margin. For\nreproducibility, we publicly share the identifiers to retrieve the data used in\nthis paper.",
          "link": "http://arxiv.org/abs/2108.02625",
          "publishedOn": "2021-08-06T00:51:44.047Z",
          "wordCount": 615,
          "title": "MSTRE-Net: Multistreaming Acoustic Modeling for Automatic Lyrics Transcription. (arXiv:2108.02625v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_D/0/1/0/all/0/1\">Detao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chuanfei Xu</a>",
          "description": "Matching items for a user from a travel item pool of large cardinality have\nbeen the most important technology for increasing the business at Fliggy, one\nof the most popular online travel platforms (OTPs) in China. There are three\nmajor challenges facing OTPs: sparsity, diversity, and implicitness. In this\npaper, we present a novel Fliggy ITinerary-aware deep matching NETwork (FitNET)\nto address these three challenges. FitNET is designed based on the popular deep\nmatching network, which has been successfully employed in many industrial\nrecommendation systems, due to its effectiveness. The concept itinerary is\nfirstly proposed under the context of recommendation systems for OTPs, which is\ndefined as the list of unconsumed orders of a user. All orders in a user\nitinerary are learned as a whole, based on which the implicit travel intention\nof each user can be more accurately inferred. To alleviate the sparsity\nproblem, users' profiles are incorporated into FitNET. Meanwhile, a series of\nitinerary-aware attention mechanisms that capture the vital interactions\nbetween user's itinerary and other input categories are carefully designed.\nThese mechanisms are very helpful in inferring a user's travel intention or\npreference, and handling the diversity in a user's need. Further, two training\nobjectives, i.e., prediction accuracy of user's travel intention and prediction\naccuracy of user's click behavior, are utilized by FitNET, so that these two\nobjectives can be optimized simultaneously. An offline experiment on Fliggy\nproduction dataset with over 0.27 million users and 1.55 million travel items,\nand an online A/B test both show that FitNET effectively learns users' travel\nintentions, preferences, and diverse needs, based on their itineraries and\ngains superior performance compared with state-of-the-art methods. FitNET now\nhas been successfully deployed at Fliggy, serving major online traffic.",
          "link": "http://arxiv.org/abs/2108.02343",
          "publishedOn": "2021-08-06T00:51:44.001Z",
          "wordCount": 719,
          "title": "Itinerary-aware Personalized Deep Matching at Fliggy. (arXiv:2108.02343v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wendong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhi Jiang</a>",
          "description": "Most current recommender systems used the historical behaviour data of user\nto predict user' preference. However, it is difficult to recommend items to new\nusers accurately. To alleviate this problem, existing user cold start methods\neither apply deep learning to build a cross-domain recommender system or map\nuser attributes into the space of user behaviour. These methods are more\nchallenging when applied to online travel platform (e.g., Fliggy), because it\nis hard to find a cross-domain that user has similar behaviour with travel\nscenarios and the Location Based Services (LBS) information of users have not\nbeen paid sufficient attention. In this work, we propose a LBS-based\nHeterogeneous Relations Model (LHRM) for user cold start recommendation, which\nutilizes user's LBS information and behaviour information in related domains\nand user's behaviour information in travel platforms (e.g., Fliggy) to\nconstruct the heterogeneous relations between users and items. Moreover, an\nattention-based multi-layer perceptron is applied to extract latent factors of\nusers and items. Through this way, LHRM has better generalization performance\nthan existing methods. Experimental results on real data from Fliggy's offline\nlog illustrate the effectiveness of LHRM.",
          "link": "http://arxiv.org/abs/2108.02344",
          "publishedOn": "2021-08-06T00:51:43.933Z",
          "wordCount": 633,
          "title": "LHRM: A LBS based Heterogeneous Relations Model for User Cold Start Recommendation in Online Travel Platform. (arXiv:2108.02344v1 [cs.IR])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2108.04603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Ziwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1\">Yongkang Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>",
          "description": "This paper proposes a novel model for recognizing images with composite\nattribute-object concepts, notably for composite concepts that are unseen\nduring model training. We aim to explore the three key properties required by\nthe task --- relation-aware, consistent, and decoupled --- to learn rich and\nrobust features for primitive concepts that compose attribute-object pairs. To\nthis end, we propose the Blocked Message Passing Network (BMP-Net). The model\nconsists of two modules. The concept module generates semantically meaningful\nfeatures for primitive concepts, whereas the visual module extracts visual\nfeatures for attributes and objects from input images. A message passing\nmechanism is used in the concept module to capture the relations between\nprimitive concepts. Furthermore, to prevent the model from being biased towards\nseen composite concepts and reduce the entanglement between attributes and\nobjects, we propose a blocking mechanism that equalizes the information\navailable to the model for both seen and unseen concepts. Extensive experiments\nand ablation studies on two benchmarks show the efficacy of the proposed model.",
          "link": "http://arxiv.org/abs/2108.04603",
          "publishedOn": "2021-08-11T01:55:20.491Z",
          "wordCount": 612,
          "title": "Relation-aware Compositional Zero-shot Learning for Attribute-Object Pair Recognition. (arXiv:2108.04603v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lecci_M/0/1/0/all/0/1\">Mattia Lecci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drago_M/0/1/0/all/0/1\">Matteo Drago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanella_A/0/1/0/all/0/1\">Andrea Zanella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorzi_M/0/1/0/all/0/1\">Michele Zorzi</a>",
          "description": "Thanks to recent advancements in the technology, eXtended Reality (XR)\napplications are gaining a lot of momentum, and they will surely become\nincreasingly popular in the next decade. These new applications, however,\nrequire a step forward also in terms of models to simulate and analyze this\ntype of traffic sources in modern communication networks, in order to guarantee\nto the users state of the art performance and Quality of Experience (QoE).\nRecognizing this need, in this work, we present a novel open-source traffic\nmodel, which researchers can use as a starting point both for improvements of\nthe model itself and for the design of optimized algorithms for the\ntransmission of these peculiar data flows. Along with the mathematical model\nand the code, we also share with the community the traces that we gathered for\nour study, collected from freely available applications such as Minecraft VR,\nGoogle Earth VR, and Virus Popper. Finally, we propose a roadmap for the\nconstruction of an end-to-end framework that fills this gap in the current\nstate of the art.",
          "link": "http://arxiv.org/abs/2108.04577",
          "publishedOn": "2021-08-11T01:55:20.435Z",
          "wordCount": 676,
          "title": "An Open Framework for Analyzing and Modeling XR Network Traffic. (arXiv:2108.04577v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1\">Alejandro Pardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1\">Juan Le&#xf3;n Alc&#xe1;zar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Video content creation keeps growing at an incredible pace; yet, creating\nengaging stories remains challenging and requires non-trivial video editing\nexpertise. Many video editing components are astonishingly hard to automate\nprimarily due to the lack of raw video materials. This paper focuses on a new\ntask for computational video editing, namely the task of raking cut\nplausibility. Our key idea is to leverage content that has already been edited\nto learn fine-grained audiovisual patterns that trigger cuts. To do this, we\nfirst collected a data source of more than 10K videos, from which we extract\nmore than 255K cuts. We devise a model that learns to discriminate between real\nand artificial cuts via contrastive learning. We set up a new task and a set of\nbaselines to benchmark video cut generation. We observe that our proposed model\noutperforms the baselines by large margins. To demonstrate our model in\nreal-world applications, we conduct human studies in a collection of unedited\nvideos. The results show that our model does a better job at cutting than\nrandom and alternative baselines.",
          "link": "http://arxiv.org/abs/2108.04294",
          "publishedOn": "2021-08-11T01:55:20.416Z",
          "wordCount": 624,
          "title": "Learning to Cut by Watching Movies. (arXiv:2108.04294v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.13689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1\">Xianbo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shunquan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiwu Huang</a>",
          "description": "Recent research has shown that non-additive image steganographic frameworks\neffectively improve security performance through adjusting distortion\ndistribution. However, as far as we know, all of the existing non-additive\nproposals are based on handcrafted policies, and can only be applied to a\nspecific image domain, which heavily prevent non-additive steganography from\nreleasing its full potentiality. In this paper, we propose an automatic\nnon-additive steganographic distortion learning framework called MCTSteg to\nremove the above restrictions. Guided by the reinforcement learning paradigm,\nwe combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental\nmodel to build MCTSteg. MCTS makes sequential decisions to adjust distortion\ndistribution without human intervention. Our proposed environmental model is\nused to obtain feedbacks from each decision. Due to its self-learning\ncharacteristic and domain-independent reward function, MCTSteg has become the\nfirst reported universal non-additive steganographic framework which can work\nin both spatial and JPEG domains. Extensive experimental results show that\nMCTSteg can effectively withstand the detection of both hand-crafted\nfeature-based and deep-learning-based steganalyzers. In both spatial and JPEG\ndomains, the security performance of MCTSteg steadily outperforms the state of\nthe art by a clear margin under different scenarios.",
          "link": "http://arxiv.org/abs/2103.13689",
          "publishedOn": "2021-08-11T01:55:20.299Z",
          "wordCount": 670,
          "title": "MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning Framework for Universal Non-additive Steganography. (arXiv:2103.13689v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zekai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1\">Fangtian Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pless_R/0/1/0/all/0/1\">Robert Pless</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiuzhen Cheng</a>",
          "description": "User response prediction, which aims to predict the probability that a user\nwill provide a predefined positive response in a given context such as clicking\non an ad or purchasing an item, is crucial to many industrial applications such\nas online advertising, recommender systems, and search ranking. However, due to\nthe high dimensionality and super sparsity of the data collected in these\ntasks, handcrafting cross features is inevitably time expensive. Prior studies\nin predicting user response leveraged the feature interactions by enhancing\nfeature vectors with products of features to model second-order or high-order\ncross features, either explicitly or implicitly. Nevertheless, these existing\nmethods can be hindered by not learning sufficient cross features due to model\narchitecture limitations or modeling all high-order feature interactions with\nequal weights. This work aims to fill this gap by proposing a novel\narchitecture Deep Cross Attentional Product Network (DCAP), which keeps cross\nnetwork's benefits in modeling high-order feature interactions explicitly at\nthe vector-wise level. Beyond that, it can differentiate the importance of\ndifferent cross features in each network layer inspired by the multi-head\nattention mechanism and Product Neural Network (PNN), allowing practitioners to\nperform a more in-depth analysis of user behaviors. Additionally, our proposed\nmodel can be easily implemented and train in parallel. We conduct comprehensive\nexperiments on three real-world datasets. The results have robustly\ndemonstrated that our proposed model DCAP achieves superior prediction\nperformance compared with the state-of-the-art models.",
          "link": "http://arxiv.org/abs/2105.08649",
          "publishedOn": "2021-08-10T02:00:07.351Z",
          "wordCount": 720,
          "title": "DCAP: Deep Cross Attentional Product Network for User Response Prediction. (arXiv:2105.08649v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Ziyu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Youfang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhiyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiangheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Caijie Chen</a>",
          "description": "The research on human emotion under multimedia stimulation based on\nphysiological signals is an emerging field, and important progress has been\nachieved for emotion recognition based on multi-modal signals. However, it is\nchallenging to make full use of the complementarity among\nspatial-spectral-temporal domain features for emotion recognition, as well as\nmodel the heterogeneity and correlation among multi-modal signals. In this\npaper, we propose a novel two-stream heterogeneous graph recurrent neural\nnetwork, named HetEmotionNet, fusing multi-modal physiological signals for\nemotion recognition. Specifically, HetEmotionNet consists of the\nspatial-temporal stream and the spatial-spectral stream, which can fuse\nspatial-spectral-temporal domain features in a unified framework. Each stream\nis composed of the graph transformer network for modeling the heterogeneity,\nthe graph convolutional network for modeling the correlation, and the gated\nrecurrent unit for capturing the temporal domain or spectral domain dependency.\nExtensive experiments on two real-world datasets demonstrate that our proposed\nmodel achieves better performance than state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2108.03354",
          "publishedOn": "2021-08-10T02:00:06.814Z",
          "wordCount": 620,
          "title": "HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent Neural Network for Multi-modal Emotion Recognition. (arXiv:2108.03354v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hui Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>",
          "description": "Hashing learns compact binary codes to store and retrieve massive data\nefficiently. Particularly, unsupervised deep hashing is supported by powerful\ndeep neural networks and has the desirable advantage of label independence. It\nis a promising technique for scalable image retrieval. However, deep models\nintroduce a large number of parameters, which is hard to optimize due to the\nlack of explicit semantic labels and brings considerable training cost. As a\nresult, the retrieval accuracy and training efficiency of existing unsupervised\ndeep hashing are still limited. To tackle the problems, in this paper, we\npropose a simple and efficient \\emph{Lightweight Augmented Graph Network\nHashing} (LAGNH) method with a two-pronged strategy. For one thing, we extract\nthe inner structure of the image as the auxiliary semantics to enhance the\nsemantic supervision of the unsupervised hash learning process. For another, we\ndesign a lightweight network structure with the assistance of the auxiliary\nsemantics, which greatly reduces the number of network parameters that needs to\nbe optimized and thus greatly accelerates the training process. Specifically,\nwe design a cross-modal attention module based on the auxiliary semantic\ninformation to adaptively mitigate the adverse effects in the deep image\nfeatures. Besides, the hash codes are learned by multi-layer message passing\nwithin an adversarial regularized graph convolutional network. Simultaneously,\nthe semantic representation capability of hash codes is further enhanced by\nreconstructing the similarity graph.",
          "link": "http://arxiv.org/abs/2108.03914",
          "publishedOn": "2021-08-10T02:00:06.795Z",
          "wordCount": 661,
          "title": "Two-pronged Strategy: Lightweight Augmented Graph Network Hashing for Scalable Image Retrieval. (arXiv:2108.03914v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>",
          "description": "This paper introduces a two-phase deep feature calibration framework for\nefficient learning of semantics enhanced text-image cross-modal joint\nembedding, which clearly separates the deep feature calibration in data\npreprocessing from training the joint embedding model. We use the Recipe1M\ndataset for the technical description and empirical validation. In\npreprocessing, we perform deep feature calibration by combining deep feature\nengineering with semantic context features derived from raw text-image input\ndata. We leverage LSTM to identify key terms, NLP methods to produce ranking\nscores for key terms before generating the key term feature. We leverage\nwideResNet50 to extract and encode the image category semantics to help\nsemantic alignment of the learned recipe and image embeddings in the joint\nlatent space. In joint embedding learning, we perform deep feature calibration\nby optimizing the batch-hard triplet loss function with soft-margin and double\nnegative sampling, also utilizing the category-based alignment loss and\ndiscriminator-based alignment loss. Extensive experiments demonstrate that our\nSEJE approach with the deep feature calibration significantly outperforms the\nstate-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.00705",
          "publishedOn": "2021-08-10T02:00:06.733Z",
          "wordCount": 634,
          "title": "Efficient Deep Feature Calibration for Cross-Modal Joint Embedding Learning. (arXiv:2108.00705v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khojasteh_H/0/1/0/all/0/1\">Hassan Keshvari Khojasteh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadzade_H/0/1/0/all/0/1\">Hoda Mohammadzade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behroozi_H/0/1/0/all/0/1\">Hamid Behroozi</a>",
          "description": "Temporal Action Localization (TAL) task in which the aim is to predict the\nstart and end of each action and its class label has many applications in the\nreal world. But due to its complexity, researchers have not reached great\nresults compared to the action recognition task. The complexity is related to\npredicting precise start and end times for different actions in any video. In\nthis paper, we propose a new network based on Gated Recurrent Unit (GRU) and\ntwo novel post-processing ideas for TAL task. Specifically, we propose a new\ndesign for the output layer of the GRU resulting in the so-called GRU-Splitted\nmodel. Moreover, linear interpolation is used to generate the action proposals\nwith precise start and end times. Finally, to rank the generated proposals\nappropriately, we use a Learn to Rank (LTR) approach. We evaluated the\nperformance of the proposed method on Thumos14 dataset. Results show the\nsuperiority of the performance of the proposed method compared to\nstate-of-the-art. Especially in the mean Average Precision (mAP) metric at\nIntersection over Union (IoU) 0.7, we get 27.52% which is 5.12% better than\nthat of state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.03375",
          "publishedOn": "2021-08-10T02:00:06.666Z",
          "wordCount": 628,
          "title": "Temporal Action Localization Using Gated Recurrent Units. (arXiv:2108.03375v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03538",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinru Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Menghan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>",
          "description": "Cough is a common symptom of respiratory and lung diseases. Cough detection\nis important to prevent, assess and control epidemic, such as COVID-19. This\npaper proposes a model to detect cough events from cough audio signals. The\nmodels are trained by the dataset combined ESC-50 dataset with self-recorded\ncough recordings. The test dataset contains inpatient cough recordings\ncollected from inpatients of the respiratory disease department in Ruijin\nHospital. We totally build 15 cough detection models based on different feature\nnumbers selected by Random Frog, Uninformative Variable Elimination (UVE), and\nVariable influence on projection (VIP) algorithms respectively. The optimal\nmodel is based on 20 features selected from Mel Frequency Cepstral Coefficients\n(MFCC) features by UVE algorithm and classified with Support Vector Machine\n(SVM) linear two-class classifier. The best cough detection model realizes the\naccuracy, recall, precision and F1-score with 94.9%, 97.1%, 93.1% and 0.95\nrespectively. Its excellent performance with fewer dimensionality of the\nfeature vector shows the potential of being applied to mobile devices, such as\nsmartphones, thus making cough detection remote and non-contact.",
          "link": "http://arxiv.org/abs/2108.03538",
          "publishedOn": "2021-08-10T02:00:06.608Z",
          "wordCount": 657,
          "title": "Cough Detection Using Selected Informative Features from Audio Signals. (arXiv:2108.03538v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wanqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming Yang</a>",
          "description": "This paper investigates a valuable setting called few-shot unsupervised\ndomain adaptation (FS-UDA), which has not been sufficiently studied in the\nliterature. In this setting, the source domain data are labelled, but with\nfew-shot per category, while the target domain data are unlabelled. To address\nthe FS-UDA setting, we develop a general UDA model to solve the following two\nkey issues: the few-shot labeled data per category and the domain adaptation\nbetween support and query sets. Our model is general in that once trained it\nwill be able to be applied to various FS-UDA tasks from the same source and\ntarget domains. Inspired by the recent local descriptor based few-shot learning\n(FSL), our general UDA model is fully built upon local descriptors (LDs) for\nimage classification and domain adaptation. By proposing a novel concept called\nsimilarity patterns (SPs), our model not only effectively considers the spatial\nrelationship of LDs that was ignored in previous FSL methods, but also makes\nthe learned image similarity better serve the required domain alignment.\nSpecifically, we propose a novel IMage-to-class sparse Similarity Encoding\n(IMSE) method. It learns SPs to extract the local discriminative information\nfor classification and meanwhile aligns the covariance matrix of the SPs for\ndomain adaptation. Also, domain adversarial training and multi-scale local\nfeature matching are performed upon LDs. Extensive experiments conducted on a\nmulti-domain benchmark dataset DomainNet demonstrates the state-of-the-art\nperformance of our IMSE for the novel setting of FS-UDA. In addition, for FSL,\nour IMSE can also show better performance than most of recent FSL methods on\nminiImageNet.",
          "link": "http://arxiv.org/abs/2108.02953",
          "publishedOn": "2021-08-09T00:49:26.032Z",
          "wordCount": 713,
          "title": "Few-shot Unsupervised Domain Adaptation with Image-to-class Sparse Similarity Encoding. (arXiv:2108.02953v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02607",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Susan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilin Chen</a>",
          "description": "We introduce a new efficient framework, the Unified Context Network (UniCon),\nfor robust active speaker detection (ASD). Traditional methods for ASD usually\noperate on each candidate's pre-cropped face track separately and do not\nsufficiently consider the relationships among the candidates. This potentially\nlimits performance, especially in challenging scenarios with low-resolution\nfaces, multiple candidates, etc. Our solution is a novel, unified framework\nthat focuses on jointly modeling multiple types of contextual information:\nspatial context to indicate the position and scale of each candidate's face,\nrelational context to capture the visual relationships among the candidates and\ncontrast audio-visual affinities with each other, and temporal context to\naggregate long-term information and smooth out local uncertainties. Based on\nsuch information, our model optimizes all candidates in a unified process for\nrobust and reliable ASD. A thorough ablation study is performed on several\nchallenging ASD benchmarks under different settings. In particular, our method\noutperforms the state-of-the-art by a large margin of about 15% mean Average\nPrecision (mAP) absolute on two challenging subsets: one with three candidate\nspeakers, and the other with faces smaller than 64 pixels. Together, our UniCon\nachieves 92.0% mAP on the AVA-ActiveSpeaker validation set, surpassing 90% for\nthe first time on this challenging dataset at the time of submission. Project\nwebsite: https://unicon-asd.github.io/.",
          "link": "http://arxiv.org/abs/2108.02607",
          "publishedOn": "2021-08-06T00:51:44.662Z",
          "wordCount": 688,
          "title": "UniCon: Unified Context Network for Robust Active Speaker Detection. (arXiv:2108.02607v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02515",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verde_S/0/1/0/all/0/1\">Sebastiano Verde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasquini_C/0/1/0/all/0/1\">Cecilia Pasquini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lago_F/0/1/0/all/0/1\">Federica Lago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goller_A/0/1/0/all/0/1\">Alessandro Goller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natale_F/0/1/0/all/0/1\">Francesco GB De Natale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piva_A/0/1/0/all/0/1\">Alessandro Piva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boato_G/0/1/0/all/0/1\">Giulia Boato</a>",
          "description": "The amount of multimedia content shared everyday, combined with the level of\nrealism reached by recent fake-generating technologies, threatens to impair the\ntrustworthiness of online information sources. The process of uploading and\nsharing data tends to hinder standard media forensic analyses, since multiple\nre-sharing steps progressively hide the traces of past manipulations. At the\nsame time though, new traces are introduced by the platforms themselves,\nenabling the reconstruction of the sharing history of digital objects, with\npossible applications in information flow monitoring and source identification.\nIn this work, we propose a supervised framework for the reconstruction of image\nsharing chains on social media platforms. The system is structured as a cascade\nof backtracking blocks, each of them tracing back one step of the sharing chain\nat a time. Blocks are designed as ensembles of classifiers trained to analyse\nthe input image independently from one another by leveraging different feature\nrepresentations that describe both content and container of the media object.\nIndividual decisions are then properly combined by a late fusion strategy.\nResults highlight the advantages of employing multiple clues, which allow\naccurately tracing back up to three steps along the sharing chain.",
          "link": "http://arxiv.org/abs/2108.02515",
          "publishedOn": "2021-08-06T00:51:44.632Z",
          "wordCount": 629,
          "title": "Multi-clue reconstruction of sharing chains for social media images. (arXiv:2108.02515v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1\">Shahroz Tariq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Simon S. Woo</a>",
          "description": "Over the last few decades, artificial intelligence research has made\ntremendous strides, but it still heavily relies on fixed datasets in stationary\nenvironments. Continual learning is a growing field of research that examines\nhow AI systems can learn sequentially from a continuous stream of linked data\nin the same way that biological systems do. Simultaneously, fake media such as\ndeepfakes and synthetic face images have emerged as significant to current\nmultimedia technologies. Recently, numerous method has been proposed which can\ndetect deepfakes with high accuracy. However, they suffer significantly due to\ntheir reliance on fixed datasets in limited evaluation settings. Therefore, in\nthis work, we apply continuous learning to neural networks' learning dynamics,\nemphasizing its potential to increase data efficiency significantly. We propose\nContinual Representation using Distillation (CoReD) method that employs the\nconcept of Continual Learning (CL), Representation Learning (RL), and Knowledge\nDistillation (KD). We design CoReD to perform sequential domain adaptation\ntasks on new deepfake and GAN-generated synthetic face datasets, while\neffectively minimizing the catastrophic forgetting in a teacher-student model\nsetting. Our extensive experimental results demonstrate that our method is\nefficient at domain adaptation to detect low-quality deepfakes videos and\nGAN-generated images from several datasets, outperforming the-state-of-art\nbaseline methods.",
          "link": "http://arxiv.org/abs/2107.02408",
          "publishedOn": "2021-08-06T00:51:44.416Z",
          "wordCount": 710,
          "title": "CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation. (arXiv:2107.02408v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02481",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Javaheri_A/0/1/0/all/0/1\">Alireza Javaheri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brites_C/0/1/0/all/0/1\">Catarina Brites</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pereira_F/0/1/0/all/0/1\">Fernando Pereira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ascenso_J/0/1/0/all/0/1\">Jo&#xe3;o Ascenso</a>",
          "description": "Point cloud coding solutions have been recently standardized to address the\nneeds of multiple application scenarios. The design and assessment of point\ncloud coding methods require reliable objective quality metrics to evaluate the\nlevel of degradation introduced by compression or any other type of processing.\nSeveral point cloud objective quality metrics has been recently proposed to\nreliable estimate human perceived quality, including the so-called\nprojection-based metrics. In this context, this paper proposes a joint geometry\nand color projection-based point cloud objective quality metric which solves\nthe critical weakness of this type of quality metrics, i.e., the misalignment\nbetween the reference and degraded projected images. Moreover, the proposed\npoint cloud quality metric exploits the best performing 2D quality metrics in\nthe literature to assess the quality of the projected images. The experimental\nresults show that the proposed projection-based quality metric offers the best\nsubjective-objective correlation performance in comparison with other metrics\nin the literature. The Pearson correlation gains regarding D1-PSNR and D2-PSNR\nmetrics are 17% and 14.2 when data with all coding degradations is considered.",
          "link": "http://arxiv.org/abs/2108.02481",
          "publishedOn": "2021-08-06T00:51:44.395Z",
          "wordCount": 627,
          "title": "Joint Geometry and Color Projection-based Point Cloud Quality Metric. (arXiv:2108.02481v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yanbin Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>",
          "description": "Transformer achieves remarkable successes in understanding 1 and\n2-dimensional signals (e.g., NLP and Image Content Understanding). As a\npotential alternative to convolutional neural networks, it shares merits of\nstrong interpretability, high discriminative power on hyper-scale data, and\nflexibility in processing varying length inputs. However, its encoders\nnaturally contain computational intensive operations such as pair-wise\nself-attention, incurring heavy computational burden when being applied on the\ncomplex 3-dimensional video signals.\n\nThis paper presents Token Shift Module (i.e., TokShift), a novel,\nzero-parameter, zero-FLOPs operator, for modeling temporal relations within\neach transformer encoder. Specifically, the TokShift barely temporally shifts\npartial [Class] token features back-and-forth across adjacent frames. Then, we\ndensely plug the module into each encoder of a plain 2D vision transformer for\nlearning 3D video representation. It is worth noticing that our TokShift\ntransformer is a pure convolutional-free video transformer pilot with\ncomputational efficiency for video understanding. Experiments on standard\nbenchmarks verify its robustness, effectiveness, and efficiency. Particularly,\nwith input clips of 8/12 frames, the TokShift transformer achieves SOTA\nprecision: 79.83%/80.40% on the Kinetics-400, 66.56% on EGTEA-Gaze+, and 96.80%\non UCF-101 datasets, comparable or better than existing SOTA convolutional\ncounterparts. Our code is open-sourced in:\nhttps://github.com/VideoNetworks/TokShift-Transformer.",
          "link": "http://arxiv.org/abs/2108.02432",
          "publishedOn": "2021-08-06T00:51:43.978Z",
          "wordCount": 640,
          "title": "Token Shift Transformer for Video Classification. (arXiv:2108.02432v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2108.04444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_P/0/1/0/all/0/1\">Peng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yan-Pei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1\">Pengfei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>",
          "description": "Point cloud completion aims to predict a complete shape in high accuracy from\nits partial observation. However, previous methods usually suffered from\ndiscrete nature of point cloud and unstructured prediction of points in local\nregions, which makes it hard to reveal fine local geometric details on the\ncomplete shape. To resolve this issue, we propose SnowflakeNet with Snowflake\nPoint Deconvolution (SPD) to generate the complete point clouds. The\nSnowflakeNet models the generation of complete point clouds as the\nsnowflake-like growth of points in 3D space, where the child points are\nprogressively generated by splitting their parent points after each SPD. Our\ninsight of revealing detailed geometry is to introduce skip-transformer in SPD\nto learn point splitting patterns which can fit local regions the best.\nSkip-transformer leverages attention mechanism to summarize the splitting\npatterns used in the previous SPD layer to produce the splitting in the current\nSPD layer. The locally compact and structured point cloud generated by SPD is\nable to precisely capture the structure characteristic of 3D shape in local\npatches, which enables the network to predict highly detailed geometries, such\nas smooth regions, sharp edges and corners. Our experimental results outperform\nthe state-of-the-art point cloud completion methods under widely used\nbenchmarks. Code will be available at\nhttps://github.com/AllenXiangX/SnowflakeNet.",
          "link": "http://arxiv.org/abs/2108.04444",
          "publishedOn": "2021-08-11T01:55:24.416Z",
          "wordCount": 675,
          "title": "SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer. (arXiv:2108.04444v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04521",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingkai Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Baocai Yin</a>",
          "description": "Jointly exploiting multiple different yet complementary domain information\nhas been proven to be an effective way to perform robust object tracking. This\npaper focuses on effectively representing and utilizing complementary features\nfrom the frame domain and event domain for boosting object tracking performance\nin challenge scenarios. Specifically, we propose Common Features Extractor\n(CFE) to learn potential common representations from the RGB domain and event\ndomain. For learning the unique features of the two domains, we utilize a\nUnique Extractor for Event (UEE) based on Spiking Neural Networks to extract\nedge cues in the event domain which may be missed in RGB in some challenging\nconditions, and a Unique Extractor for RGB (UER) based on Deep Convolutional\nNeural Networks to extract texture and semantic information in RGB domain.\nExtensive experiments on standard RGB benchmark and real event tracking dataset\ndemonstrate the effectiveness of the proposed approach. We show our approach\noutperforms all compared state-of-the-art tracking algorithms and verify\nevent-based data is a powerful cue for tracking in challenging scenes.",
          "link": "http://arxiv.org/abs/2108.04521",
          "publishedOn": "2021-08-11T01:55:24.411Z",
          "wordCount": 612,
          "title": "Multi-domain Collaborative Feature Representation for Robust Visual Object Tracking. (arXiv:2108.04521v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joung_S/0/1/0/all/0/1\">Sunghun Joung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1\">Ig-Jae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kwanghoon Sohn</a>",
          "description": "We propose a novel framework for fine-grained object recognition that learns\nto recover object variation in 3D space from a single image, trained on an\nimage collection without using any ground-truth 3D annotation. We accomplish\nthis by representing an object as a composition of 3D shape and its appearance,\nwhile eliminating the effect of camera viewpoint, in a canonical configuration.\nUnlike conventional methods modeling spatial variation in 2D images only, our\nmethod is capable of reconfiguring the appearance feature in a canonical 3D\nspace, thus enabling the subsequent object classifier to be invariant under 3D\ngeometric variation. Our representation also allows us to go beyond existing\nmethods, by incorporating 3D shape variation as an additional cue for object\nrecognition. To learn the model without ground-truth 3D annotation, we deploy a\ndifferentiable renderer in an analysis-by-synthesis framework. By incorporating\n3D shape and appearance jointly in a deep representation, our method learns the\ndiscriminative representation of the object and achieves competitive\nperformance on fine-grained image recognition and vehicle re-identification. We\nalso demonstrate that the performance of 3D shape reconstruction is improved by\nlearning fine-grained shape deformation in a boosting manner.",
          "link": "http://arxiv.org/abs/2108.04628",
          "publishedOn": "2021-08-11T01:55:24.406Z",
          "wordCount": 628,
          "title": "Learning Canonical 3D Object Representation for Fine-Grained Recognition. (arXiv:2108.04628v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Styles_O/0/1/0/all/0/1\">Olly Styles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_T/0/1/0/all/0/1\">Tanaya Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1\">Victor Sanchez</a>",
          "description": "We introduce the problem of multi-camera trajectory forecasting (MCTF), which\ninvolves predicting the trajectory of a moving object across a network of\ncameras. While multi-camera setups are widespread for applications such as\nsurveillance and traffic monitoring, existing trajectory forecasting methods\ntypically focus on single-camera trajectory forecasting (SCTF), limiting their\nuse for such applications. Furthermore, using a single camera limits the\nfield-of-view available, making long-term trajectory forecasting impossible. We\naddress these shortcomings of SCTF by developing an MCTF framework that\nsimultaneously uses all estimated relative object locations from several\nviewpoints and predicts the object's future location in all possible\nviewpoints. Our framework follows a Which-When-Where approach that predicts in\nwhich camera(s) the objects appear and when and where within the camera views\nthey appear. To this end, we propose the concept of trajectory tensors: a new\ntechnique to encode trajectories across multiple camera views and the\nassociated uncertainties. We develop several encoder-decoder MCTF models for\ntrajectory tensors and present extensive experiments on our own database\n(comprising 600 hours of video data from 15 camera views) created particularly\nfor the MCTF task. Results show that our trajectory tensor models outperform\ncoordinate trajectory-based MCTF models and existing SCTF methods adapted for\nMCTF. Code is available from: https://github.com/olly-styles/Trajectory-Tensors",
          "link": "http://arxiv.org/abs/2108.04694",
          "publishedOn": "2021-08-11T01:55:24.395Z",
          "wordCount": 639,
          "title": "Multi-Camera Trajectory Forecasting with Trajectory Tensors. (arXiv:2108.04694v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Ziwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1\">Yongkang Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>",
          "description": "This paper proposes a novel model for recognizing images with composite\nattribute-object concepts, notably for composite concepts that are unseen\nduring model training. We aim to explore the three key properties required by\nthe task --- relation-aware, consistent, and decoupled --- to learn rich and\nrobust features for primitive concepts that compose attribute-object pairs. To\nthis end, we propose the Blocked Message Passing Network (BMP-Net). The model\nconsists of two modules. The concept module generates semantically meaningful\nfeatures for primitive concepts, whereas the visual module extracts visual\nfeatures for attributes and objects from input images. A message passing\nmechanism is used in the concept module to capture the relations between\nprimitive concepts. Furthermore, to prevent the model from being biased towards\nseen composite concepts and reduce the entanglement between attributes and\nobjects, we propose a blocking mechanism that equalizes the information\navailable to the model for both seen and unseen concepts. Extensive experiments\nand ablation studies on two benchmarks show the efficacy of the proposed model.",
          "link": "http://arxiv.org/abs/2108.04603",
          "publishedOn": "2021-08-11T01:55:24.388Z",
          "wordCount": 612,
          "title": "Relation-aware Compositional Zero-shot Learning for Attribute-Object Pair Recognition. (arXiv:2108.04603v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kummen_A/0/1/0/all/0/1\">Ashild Kummen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_A/0/1/0/all/0/1\">Ali Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganeva_T/0/1/0/all/0/1\">Teodora Ganeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qianying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_R/0/1/0/all/0/1\">Robert Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratwatte_C/0/1/0/all/0/1\">Chenuka Ratwatte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almazov_E/0/1/0/all/0/1\">Emil Almazov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visram_S/0/1/0/all/0/1\">Sheena Visram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_A/0/1/0/all/0/1\">Andrew Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebire_N/0/1/0/all/0/1\">Neil J Sebire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stott_L/0/1/0/all/0/1\">Lee Stott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_Y/0/1/0/all/0/1\">Yvonne Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_G/0/1/0/all/0/1\">Graham Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamedally_D/0/1/0/all/0/1\">Dean Mohamedally</a>",
          "description": "Touchless computer interaction has become an important consideration during\nthe COVID-19 pandemic period. Despite progress in machine learning and computer\nvision that allows for advanced gesture recognition, an integrated collection\nof such open-source methods and a user-customisable approach to utilising them\nin a low-cost solution for touchless interaction in existing software is still\nmissing. In this paper, we introduce the MotionInput v2.0 application. This\napplication utilises published open-source libraries and additional gesture\ndefinitions developed to take the video stream from a standard RGB webcam as\ninput. It then maps human motion gestures to input operations for existing\napplications and games. The user can choose their own preferred way of\ninteracting from a series of motion types, including single and bi-modal hand\ngesturing, full-body repetitive or extremities-based exercises, head and facial\nmovements, eye tracking, and combinations of the above. We also introduce a\nseries of bespoke gesture recognition classifications as DirectInput triggers,\nincluding gestures for idle states, auto calibration, depth capture from a 2D\nRGB webcam stream and tracking of facial motions such as mouth motions,\nwinking, and head direction with rotation. Three use case areas assisted the\ndevelopment of the modules: creativity software, office and clinical software,\nand gaming software. A collection of open-source libraries has been integrated\nand provide a layer of modular gesture mapping on top of existing mouse and\nkeyboard controls in Windows via DirectX. With ease of access to webcams\nintegrated into most laptops and desktop computers, touchless computing becomes\nmore available with MotionInput v2.0, in a federated and locally processed\nmethod.",
          "link": "http://arxiv.org/abs/2108.04357",
          "publishedOn": "2021-08-11T01:55:24.373Z",
          "wordCount": 818,
          "title": "MotionInput v2.0 supporting DirectX: A modular library of open-source gesture-based machine learning and computer vision methods for interacting and controlling existing software with a webcam. (arXiv:2108.04357v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cen_J/0/1/0/all/0/1\">Jun Cen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_P/0/1/0/all/0/1\">Peng Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Junhao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Michael Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "Classical close-set semantic segmentation networks have limited ability to\ndetect out-of-distribution (OOD) objects, which is important for\nsafety-critical applications such as autonomous driving. Incrementally learning\nthese OOD objects with few annotations is an ideal way to enlarge the knowledge\nbase of the deep learning models. In this paper, we propose an open world\nsemantic segmentation system that includes two modules: (1) an open-set\nsemantic segmentation module to detect both in-distribution and OOD objects.\n(2) an incremental few-shot learning module to gradually incorporate those OOD\nobjects into its existing knowledge base. This open world semantic segmentation\nsystem behaves like a human being, which is able to identify OOD objects and\ngradually learn them with corresponding supervision. We adopt the Deep Metric\nLearning Network (DMLNet) with contrastive clustering to implement open-set\nsemantic segmentation. Compared to other open-set semantic segmentation\nmethods, our DMLNet achieves state-of-the-art performance on three challenging\nopen-set semantic segmentation datasets without using additional data or\ngenerative models. On this basis, two incremental few-shot learning methods are\nfurther proposed to progressively improve the DMLNet with the annotations of\nOOD objects.",
          "link": "http://arxiv.org/abs/2108.04562",
          "publishedOn": "2021-08-11T01:55:24.366Z",
          "wordCount": 621,
          "title": "Deep Metric Learning for Open World Semantic Segmentation. (arXiv:2108.04562v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bi_X/0/1/0/all/0/1\">Xiaopeng Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dehao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Ran Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zheng Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>",
          "description": "This report describes Megvii-3D team's approach towards CVPR 2021 Image\nMatching Workshop.",
          "link": "http://arxiv.org/abs/2108.04453",
          "publishedOn": "2021-08-11T01:55:24.361Z",
          "wordCount": 458,
          "title": "Method Towards CVPR 2021 Image Matching Challenge. (arXiv:2108.04453v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chaoda Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiantao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weibing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>",
          "description": "Current 3D single object tracking approaches track the target based on a\nfeature comparison between the target template and the search area. However,\ndue to the common occlusion in LiDAR scans, it is non-trivial to conduct\naccurate feature comparisons on severe sparse and incomplete shapes. In this\nwork, we exploit the ground truth bounding box given in the first frame as a\nstrong cue to enhance the feature description of the target object, enabling a\nmore accurate feature comparison in a simple yet effective way. In particular,\nwe first propose the BoxCloud, an informative and robust representation, to\ndepict an object using the point-to-box relation. We further design an\nefficient box-aware feature fusion module, which leverages the aforementioned\nBoxCloud for reliable feature matching and embedding. Integrating the proposed\ngeneral components into an existing model P2B, we construct a superior\nbox-aware tracker (BAT). Experiments confirm that our proposed BAT outperforms\nthe previous state-of-the-art by a large margin on both KITTI and NuScenes\nbenchmarks, achieving a 12.8% improvement in terms of precision while running\n~20% faster.",
          "link": "http://arxiv.org/abs/2108.04728",
          "publishedOn": "2021-08-11T01:55:24.352Z",
          "wordCount": 625,
          "title": "Box-Aware Feature Enhancement for Single Object Tracking on Point Clouds. (arXiv:2108.04728v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Botach_A/0/1/0/all/0/1\">Adam Botach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_Y/0/1/0/all/0/1\">Yuri Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miron_Y/0/1/0/all/0/1\">Yakov Miron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapiro_Y/0/1/0/all/0/1\">Yoel Shapiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Dotan Di Castro</a>",
          "description": "We introduce BIDCD - the Bosch Industrial Depth Completion Dataset. BIDCD is\na new RGBD dataset of metallic industrial objects, collected with a depth\ncamera mounted on a robotic manipulator. The main purpose of this dataset is to\nfacilitate the training of domain-specific depth completion models, to be used\nin logistics and manufacturing tasks. We trained a State-of-the-Art depth\ncompletion model on this dataset, and report the results, setting an initial\nbenchmark.",
          "link": "http://arxiv.org/abs/2108.04706",
          "publishedOn": "2021-08-11T01:55:24.347Z",
          "wordCount": 511,
          "title": "BIDCD - Bosch Industrial Depth Completion Dataset. (arXiv:2108.04706v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostler_H/0/1/0/all/0/1\">Harald K&#xf6;stler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heisig_M/0/1/0/all/0/1\">Marco Heisig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauss_P/0/1/0/all/0/1\">Patrick Krauss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seung Hee Yang</a>",
          "description": "In this article, we perform a review of the state-of-the-art of hybrid\nmachine learning in medical imaging. We start with a short summary of the\ngeneral developments of the past in machine learning and how general and\nspecialized approaches have been in competition in the past decades. A\nparticular focus will be the theoretical and experimental evidence pro and\ncontra hybrid modelling. Next, we inspect several new developments regarding\nhybrid machine learning with a particular focus on so-called known operator\nlearning and how hybrid approaches gain more and more momentum across\nessentially all applications in medical imaging and medical image analysis. As\nwe will point out by numerous examples, hybrid models are taking over in image\nreconstruction and analysis. Even domains such as physical simulation and\nscanner and acquisition design are being addressed using machine learning grey\nbox modelling approaches. Towards the end of the article, we will investigate a\nfew future directions and point out relevant areas in which hybrid modelling,\nmeta learning, and other domains will likely be able to drive the\nstate-of-the-art ahead.",
          "link": "http://arxiv.org/abs/2108.04543",
          "publishedOn": "2021-08-11T01:55:24.342Z",
          "wordCount": 657,
          "title": "Known Operator Learning and Hybrid Machine Learning in Medical Imaging --- A Review of the Past, the Present, and the Future. (arXiv:2108.04543v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04663",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thapa_A/0/1/0/all/0/1\">Ashu Thapa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alsadoon_A/0/1/0/all/0/1\">Abeer Alsadoon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasad_P/0/1/0/all/0/1\">P.W.C. Prasad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bajaj_S/0/1/0/all/0/1\">Simi Bajaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alsadoon_O/0/1/0/all/0/1\">Omar Hisham Alsadoon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rashid_T/0/1/0/all/0/1\">Tarik A. Rashid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_R/0/1/0/all/0/1\">Rasha S. Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jerew_O/0/1/0/all/0/1\">Oday D. Jerew</a>",
          "description": "Background and Aim: Recently, deep learning using convolutional neural\nnetwork has been used successfully to classify the images of breast cells\naccurately. However, the accuracy of manual classification of those\nhistopathological images is comparatively low. This research aims to increase\nthe accuracy of the classification of breast cancer images by utilizing a\nPatch-Based Classifier (PBC) along with deep learning architecture.\nMethodology: The proposed system consists of a Deep Convolutional Neural\nNetwork (DCNN) that helps in enhancing and increasing the accuracy of the\nclassification process. This is done by the use of the Patch-based Classifier\n(PBC). CNN has completely different layers where images are first fed through\nconvolutional layers using hyperbolic tangent function together with the\nmax-pooling layer, drop out layers, and SoftMax function for classification.\nFurther, the output obtained is fed to a patch-based classifier that consists\nof patch-wise classification output followed by majority voting. Results: The\nresults are obtained throughout the classification stage for breast cancer\nimages that are collected from breast-histology datasets. The proposed solution\nimproves the accuracy of classification whether or not the images had normal,\nbenign, in-situ, or invasive carcinoma from 87% to 94% with a decrease in\nprocessing time from 0.45 s to 0.2s on average. Conclusion: The proposed\nsolution focused on increasing the accuracy of classifying cancer in the breast\nby enhancing the image contrast and reducing the vanishing gradient. Finally,\nthis solution for the implementation of the Contrast Limited Adaptive Histogram\nEqualization (CLAHE) technique and modified tangent function helps in\nincreasing the accuracy.",
          "link": "http://arxiv.org/abs/2108.04663",
          "publishedOn": "2021-08-11T01:55:24.326Z",
          "wordCount": 723,
          "title": "Deep Learning for Breast Cancer Classification: Enhanced Tangent Function. (arXiv:2108.04663v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1\">Po-Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_B/0/1/0/all/0/1\">Bing-Chen Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wen-Feng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Ke-Jyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>",
          "description": "Understanding and comprehending video content is crucial for many real-world\napplications such as search and recommendation systems. While recent progress\nof deep learning has boosted performance on various tasks using visual cues,\ndeep cognition to reason intentions, motivation, or causality remains\nchallenging. Existing datasets that aim to examine video reasoning capability\nfocus on visual signals such as actions, objects, relations, or could be\nanswered utilizing text bias. Observing this, we propose a novel task, along\nwith a new dataset: Trope Understanding in Movies and Animations (TrUMAn),\nintending to evaluate and develop learning systems beyond visual signals.\nTropes are frequently used storytelling devices for creative works. By coping\nwith the trope understanding task and enabling the deep cognition skills of\nmachines, we are optimistic that data mining applications and algorithms could\nbe taken to the next level. To tackle the challenging TrUMAn dataset, we\npresent a Trope Understanding and Storytelling (TrUSt) with a new Conceptual\nStoryteller module, which guides the video encoder by performing video\nstorytelling on a latent space. The generated story embedding is then fed into\nthe trope understanding model to provide further signals. Experimental results\ndemonstrate that state-of-the-art learning systems on existing tasks reach only\n12.01% of accuracy with raw input signals. Also, even in the oracle case with\nhuman-annotated descriptions, BERT contextual embedding achieves at most 28% of\naccuracy. Our proposed TrUSt boosts the model performance and reaches 13.94%\nperformance. We also provide detailed analysis topave the way for future\nresearch. TrUMAn is publicly available\nat:https://www.cmlab.csie.ntu.edu.tw/project/trope",
          "link": "http://arxiv.org/abs/2108.04542",
          "publishedOn": "2021-08-11T01:55:24.283Z",
          "wordCount": 702,
          "title": "TrUMAn: Trope Understanding in Movies and Animations. (arXiv:2108.04542v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1\">Rashmi Bakshi</a>",
          "description": "In this work, deep learning models are applied to a segment of a robust\nhand-washing dataset that has been created with the help of 30 volunteers. This\nwork demonstrates the classification of presence of one hand, two hands and no\nhand in the scene based on transfer learning. The pre-trained model; simplest\nNN from Keras library is utilized to train the network with 704 images of hand\ngestures and the predictions are carried out for the input image. Due to the\ncontrolled and restricted dataset, 100% accuracy is achieved during the\ntraining with correct predictions for the input image. Complete handwashing\ndataset with dense models such as AlexNet for video classification for hand\nhygiene stages will be used in the future work.",
          "link": "http://arxiv.org/abs/2108.04529",
          "publishedOn": "2021-08-11T01:55:24.277Z",
          "wordCount": 548,
          "title": "Hand Pose Classification Based on Neural Networks. (arXiv:2108.04529v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kemiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Q/0/1/0/all/0/1\">Qi Hao</a>",
          "description": "Multi-object tracking (MOT) with camera-LiDAR fusion demands accurate results\nof object detection, affinity computation and data association in real time.\nThis paper presents an efficient multi-modal MOT framework with online joint\ndetection and tracking schemes and robust data association for autonomous\ndriving applications. The novelty of this work includes: (1) development of an\nend-to-end deep neural network for joint object detection and correlation using\n2D and 3D measurements; (2) development of a robust affinity computation module\nto compute occlusion-aware appearance and motion affinities in 3D space; (3)\ndevelopment of a comprehensive data association module for joint optimization\namong detection confidences, affinities and start-end probabilities. The\nexperiment results on the KITTI tracking benchmark demonstrate the superior\nperformance of the proposed method in terms of both tracking accuracy and\nprocessing speed.",
          "link": "http://arxiv.org/abs/2108.04602",
          "publishedOn": "2021-08-11T01:55:24.272Z",
          "wordCount": 571,
          "title": "Joint Multi-Object Detection and Tracking with Camera-LiDAR Fusion for Autonomous Driving. (arXiv:2108.04602v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_B/0/1/0/all/0/1\">Boseung Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jicheol Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>",
          "description": "Attribute-based person search is the task of finding person images that are\nbest matched with a set of text attributes given as query. The main challenge\nof this task is the large modality gap between attributes and images. To reduce\nthe gap, we present a new loss for learning cross-modal embeddings in the\ncontext of attribute-based person search. We regard a set of attributes as a\ncategory of people sharing the same traits. In a joint embedding space of the\ntwo modalities, our loss pulls images close to their person categories for\nmodality alignment. More importantly, it pushes apart a pair of person\ncategories by a margin determined adaptively by their semantic distance, where\nthe distance metric is learned end-to-end so that the loss considers importance\nof each attribute when relating person categories. Our loss guided by the\nadaptive semantic margin leads to more discriminative and semantically\nwell-arranged distributions of person images. As a consequence, it enables a\nsimple embedding model to achieve state-of-the-art records on public benchmarks\nwithout bells and whistles.",
          "link": "http://arxiv.org/abs/2108.04533",
          "publishedOn": "2021-08-11T01:55:24.256Z",
          "wordCount": 613,
          "title": "ASMR: Learning Attribute-Based Person Search with Adaptive Semantic Margin Regularizer. (arXiv:2108.04533v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Praveen_N/0/1/0/all/0/1\">Nalla Praveen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "Machine learning has endless applications in the health care industry. White\nblood cell classification is one of the interesting and promising area of\nresearch. The classification of the white blood cells plays an important part\nin the medical diagnosis. In practise white blood cell classification is\nperformed by the haematologist by taking a small smear of blood and careful\nexamination under the microscope. The current procedures to identify the white\nblood cell subtype is more time taking and error-prone. The computer aided\ndetection and diagnosis of the white blood cells tend to avoid the human error\nand reduce the time taken to classify the white blood cells. In the recent\nyears several deep learning approaches have been developed in the context of\nclassification of the white blood cells that are able to identify but are\nunable to localize the positions of white blood cells in the blood cell image.\nFollowing this, the present research proposes to utilize YOLOv3 object\ndetection technique to localize and classify the white blood cells with\nbounding boxes. With exhaustive experimental analysis, the proposed work is\nfound to detect the white blood cell with 99.2% accuracy and classify with 90%\naccuracy.",
          "link": "http://arxiv.org/abs/2108.04614",
          "publishedOn": "2021-08-11T01:55:24.249Z",
          "wordCount": 631,
          "title": "White blood cell subtype detection and classification. (arXiv:2108.04614v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sodani_A/0/1/0/all/0/1\">Abhigya Sodani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_M/0/1/0/all/0/1\">Michael Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anirudh Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1\">Meher Anand Kasam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>",
          "description": "Researchers often spend weeks sifting through decades of unlabeled satellite\nimagery(on NASA Worldview) in order to develop datasets on which they can start\nconducting research. We developed an interactive, scalable and fast image\nsimilarity search engine (which can take one or more images as the query image)\nthat automatically sifts through the unlabeled dataset reducing dataset\ngeneration time from weeks to minutes. In this work, we describe key components\nof the end to end pipeline. Our similarity search system was created to be able\nto identify similar images from a potentially petabyte scale database that are\nsimilar to an input image, and for this we had to break down each query image\ninto its features, which were generated by a classification layer stripped CNN\ntrained in a supervised manner. To store and search these features efficiently,\nwe had to make several scalability improvements. To improve the speed, reduce\nthe storage, and shrink memory requirements for embedding search, we add a\nfully connected layer to our CNN make all images into a 128 length vector\nbefore entering the classification layers. This helped us compress the size of\nour image features from 2048 (for ResNet, which was initially tried as our\nfeaturizer) to 128 for our new custom model. Additionally, we utilize existing\napproximate nearest neighbor search libraries to significantly speed up\nembedding search. Our system currently searches over our entire database of\nimages at 5 seconds per query on a single virtual machine in the cloud. In the\nfuture, we would like to incorporate a SimCLR based featurizing model which\ncould be trained without any labelling by a human (since the classification\naspect of the model is irrelevant to this use case).",
          "link": "http://arxiv.org/abs/2108.04479",
          "publishedOn": "2021-08-11T01:55:24.244Z",
          "wordCount": 730,
          "title": "Scalable Reverse Image Search Engine for NASAWorldview. (arXiv:2108.04479v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04454",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Youngsaeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">David Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hanseok Ko</a>",
          "description": "Anomaly detection in video streams is a challengingproblem because of the\nscarcity of abnormal events andthe difficulty of accurately annotating them.To\nallevi-ate these issues, unsupervised learning-based predictionmethods have\nbeen previously applied. These approachestrain the model with only normal\nevents and predict a fu-ture frame from a sequence of preceding frames by use\nofencoder-decoder architectures so that they result in smallprediction errors\non normal events but large errors on ab-normal events. The architecture,\nhowever, comes with thecomputational burden as some anomaly detection tasks\nre-quire low computational cost without sacrificing perfor-mance. In this\npaper, Cross-Parallel Network (CPNet) forefficient anomaly detection is\nproposed here to minimizecomputations without performance drops. It consists\nofNsmaller parallel U-Net, each of which is designed to handlea single input\nframe, to make the calculations significantlymore efficient. Additionally, an\ninter-network shift moduleis incorporated to capture temporal relationships\namong se-quential frames to enable more accurate future predictions.The\nquantitative results show that our model requires lesscomputational cost than\nthe baseline U-Net while deliver-ing equivalent performance in anomaly\ndetection.",
          "link": "http://arxiv.org/abs/2108.04454",
          "publishedOn": "2021-08-11T01:55:24.238Z",
          "wordCount": 617,
          "title": "CPNet: Cross-Parallel Network for Efficient Anomaly Detection. (arXiv:2108.04454v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1\">Ting Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jizhong Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>",
          "description": "Iterative self-consistent parallel imaging reconstruction (SPIRiT) is an\neffective self-calibrated reconstruction model for parallel magnetic resonance\nimaging (PMRI). The joint L1 norm of wavelet coefficients and joint total\nvariation (TV) regularization terms are incorporated into the SPIRiT model to\nimprove the reconstruction performance. The simultaneous two-directional\nlow-rankness (STDLR) in k-space data is incorporated into SPIRiT to realize\nimproved reconstruction. Recent methods have exploited the nonlocal\nself-similarity (NSS) of images by imposing nonlocal low-rankness of similar\npatches to achieve a superior performance. To fully utilize both the NSS in\nMagnetic resonance (MR) images and calibration consistency in the k-space\ndomain, we propose a nonlocal low-rank (NLR)-SPIRiT model by incorporating NLR\nregularization into the SPIRiT model. We apply the weighted nuclear norm (WNN)\nas a surrogate of the rank and employ the Nash equilibrium (NE) formulation and\nalternating direction method of multipliers (ADMM) to efficiently solve the\nNLR-SPIRiT model. The experimental results demonstrate the superior performance\nof NLR-SPIRiT over the state-of-the-art methods via three objective metrics and\nvisual comparison.",
          "link": "http://arxiv.org/abs/2108.04517",
          "publishedOn": "2021-08-11T01:55:24.233Z",
          "wordCount": 616,
          "title": "Iterative Self-consistent Parallel Magnetic Resonance Imaging Reconstruction based on Nonlocal Low-Rank Regularization. (arXiv:2108.04517v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Unnikrishnan_B/0/1/0/all/0/1\">Balagopal Unnikrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaram_S/0/1/0/all/0/1\">Shafa Balaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1\">Chuan Sheng Foo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_P/0/1/0/all/0/1\">Pavitra Krishnaswamy</a>",
          "description": "Deep learning models achieve strong performance for radiology image\nclassification, but their practical application is bottlenecked by the need for\nlarge labeled training datasets. Semi-supervised learning (SSL) approaches\nleverage small labeled datasets alongside larger unlabeled datasets and offer\npotential for reducing labeling cost. In this work, we introduce NoTeacher, a\nnovel consistency-based SSL framework which incorporates probabilistic\ngraphical models. Unlike Mean Teacher which maintains a teacher network updated\nvia a temporal ensemble, NoTeacher employs two independent networks, thereby\neliminating the need for a teacher network. We demonstrate how NoTeacher can be\ncustomized to handle a range of challenges in radiology image classification.\nSpecifically, we describe adaptations for scenarios with 2D and 3D inputs, uni\nand multi-label classification, and class distribution mismatch between labeled\nand unlabeled portions of the training data. In realistic empirical evaluations\non three public benchmark datasets spanning the workhorse modalities of\nradiology (X-Ray, CT, MRI), we show that NoTeacher achieves over 90-95% of the\nfully supervised AUROC with less than 5-15% labeling budget. Further, NoTeacher\noutperforms established SSL methods with minimal hyperparameter tuning, and has\nimplications as a principled and practical option for semisupervised learning\nin radiology applications.",
          "link": "http://arxiv.org/abs/2108.04423",
          "publishedOn": "2021-08-11T01:55:24.226Z",
          "wordCount": 669,
          "title": "Semi-supervised classification of radiology images with NoTeacher: A Teacher that is not Mean. (arXiv:2108.04423v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qiang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1\">Weiqing Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1\">Sujuan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuanjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuqiang Jiang</a>",
          "description": "Food logo detection plays an important role in the multimedia for its wide\nreal-world applications, such as food recommendation of the self-service shop\nand infringement detection on e-commerce platforms. A large-scale food logo\ndataset is urgently needed for developing advanced food logo detection\nalgorithms. However, there are no available food logo datasets with food brand\ninformation. To support efforts towards food logo detection, we introduce the\ndataset FoodLogoDet-1500, a new large-scale publicly available food logo\ndataset, which has 1,500 categories, about 100,000 images and about 150,000\nmanually annotated food logo objects. We describe the collection and annotation\nprocess of FoodLogoDet-1500, analyze its scale and diversity, and compare it\nwith other logo datasets. To the best of our knowledge, FoodLogoDet-1500 is the\nfirst largest publicly available high-quality dataset for food logo detection.\nThe challenge of food logo detection lies in the large-scale categories and\nsimilarities between food logo categories. For that, we propose a novel food\nlogo detection method Multi-scale Feature Decoupling Network (MFDNet), which\ndecouples classification and regression into two branches and focuses on the\nclassification branch to solve the problem of distinguishing multiple food logo\ncategories. Specifically, we introduce the feature offset module, which\nutilizes the deformation-learning for optimal classification offset and can\neffectively obtain the most representative features of classification in\ndetection. In addition, we adopt a balanced feature pyramid in MFDNet, which\npays attention to global information, balances the multi-scale feature maps,\nand enhances feature extraction capability. Comprehensive experiments on\nFoodLogoDet-1500 and other two benchmark logo datasets demonstrate the\neffectiveness of the proposed method. The FoodLogoDet-1500 can be found at this\nhttps URL.",
          "link": "http://arxiv.org/abs/2108.04644",
          "publishedOn": "2021-08-11T01:55:24.211Z",
          "wordCount": 733,
          "title": "FoodLogoDet-1500: A Dataset for Large-Scale Food Logo Detection via Multi-Scale Feature Decoupling Network. (arXiv:2108.04644v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1910.06056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thummel_M/0/1/0/all/0/1\">Martin Th&#xfc;mmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sickert_S/0/1/0/all/0/1\">Sven Sickert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>",
          "description": "The human face has a high potential for biometric identification due to its\nmany individual traits. At the same time, such identification is vulnerable to\nbiometric copies. These presentation attacks pose a great challenge in\nunsupervised authentication settings. As a countermeasure, we propose a method\nthat automatically analyzes the plausibility of facial behavior based on a\nsequence of 3D face scans. A compact feature representation measures facial\nbehavior using the temporal curvature change. Finally, we train our method only\non genuine faces in an anomaly detection scenario. Our method can detect\npresentation attacks using elastic 3D masks, bent photographs with eye holes,\nand monitor replay-attacks. For evaluation, we recorded a challenging database\ncontaining such cases using a high-quality 3D sensor. It features 109 4D face\nscans including eleven different types of presentation attacks. We achieve\nerror rates of 11% and 6% for APCER and BPCER, respectively.",
          "link": "http://arxiv.org/abs/1910.06056",
          "publishedOn": "2021-08-11T01:55:24.204Z",
          "wordCount": 644,
          "title": "Facial Behavior Analysis using 4D Curvature Statistics for Presentation Attack Detection. (arXiv:1910.06056v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tailin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Desen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yu Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>",
          "description": "The task of skeleton-based action recognition remains a core challenge in\nhuman-centred scene understanding due to the multiple granularities and large\nvariation in human motion. Existing approaches typically employ a single neural\nrepresentation for different motion patterns, which has difficulty in capturing\nfine-grained action classes given limited training data. To address the\naforementioned problems, we propose a novel multi-granular spatio-temporal\ngraph network for skeleton-based action classification that jointly models the\ncoarse- and fine-grained skeleton motion patterns. To this end, we develop a\ndual-head graph network consisting of two interleaved branches, which enables\nus to extract features at two spatio-temporal resolutions in an effective and\nefficient manner. Moreover, our network utilises a cross-head communication\nstrategy to mutually enhance the representations of both heads. We conducted\nextensive experiments on three large-scale datasets, namely NTU RGB+D 60, NTU\nRGB+D 120, and Kinetics-Skeleton, and achieves the state-of-the-art performance\non all the benchmarks, which validates the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2108.04536",
          "publishedOn": "2021-08-11T01:55:24.199Z",
          "wordCount": 607,
          "title": "Learning Multi-Granular Spatio-Temporal Graph Network for Skeleton-based Action Recognition. (arXiv:2108.04536v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1910.14442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">William B. Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasimbeg_P/0/1/0/all/0/1\">Priya Kasimbeg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1\">Micael Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toshev_A/0/1/0/all/0/1\">Alexander Toshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>",
          "description": "We present Interactive Gibson Benchmark, the first comprehensive benchmark\nfor training and evaluating Interactive Navigation: robot navigation strategies\nwhere physical interaction with objects is allowed and even encouraged to\naccomplish a task. For example, the robot can move objects if needed in order\nto clear a path leading to the goal location. Our benchmark comprises two novel\nelements: 1) a new experimental setup, the Interactive Gibson Environment\n(iGibson 0.5), which simulates high fidelity visuals of indoor scenes, and high\nfidelity physical dynamics of the robot and common objects found in these\nscenes; 2) a set of Interactive Navigation metrics which allows one to study\nthe interplay between navigation and physical interaction. We present and\nevaluate multiple learning-based baselines in Interactive Gibson, and provide\ninsights into regimes of navigation with different trade-offs between\nnavigation path efficiency and disturbance of surrounding objects. We make our\nbenchmark publicly\navailable(https://sites.google.com/view/interactivegibsonenv) and encourage\nresearchers from all disciplines in robotics (e.g. planning, learning, control)\nto propose, evaluate, and compare their Interactive Navigation solutions in\nInteractive Gibson.",
          "link": "http://arxiv.org/abs/1910.14442",
          "publishedOn": "2021-08-11T01:55:24.187Z",
          "wordCount": 707,
          "title": "Interactive Gibson Benchmark (iGibson 0.5): A Benchmark for Interactive Navigation in Cluttered Environments. (arXiv:1910.14442v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Panday_A/0/1/0/all/0/1\">Aishwarza Panday</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Muhammad Ashad Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1\">Nihad Karim Chowdhury</a>",
          "description": "Due to the limited availability and high cost of the reverse\ntranscription-polymerase chain reaction (RT-PCR) test, many studies have\nproposed machine learning techniques for detecting COVID-19 from medical\nimaging. The purpose of this study is to systematically review, assess, and\nsynthesize research articles that have used different machine learning\ntechniques to detect and diagnose COVID-19 from chest X-ray and CT scan images.\nA structured literature search was conducted in the relevant bibliographic\ndatabases to ensure that the survey solely centered on reproducible and\nhigh-quality research. We selected papers based on our inclusion criteria. In\nthis survey, we reviewed $98$ articles that fulfilled our inclusion criteria.\nWe have surveyed a complete pipeline of chest imaging analysis techniques\nrelated to COVID-19, including data collection, pre-processing, feature\nextraction, classification, and visualization. We have considered CT scans and\nX-rays as both are widely used to describe the latest developments in medical\nimaging to detect COVID-19. This survey provides researchers with valuable\ninsights into different machine learning techniques and their performance in\nthe detection and diagnosis of COVID-19 from chest imaging. At the end, the\nchallenges and limitations in detecting COVID-19 using machine learning\ntechniques and the future direction of research are discussed.",
          "link": "http://arxiv.org/abs/2108.04344",
          "publishedOn": "2021-08-11T01:55:24.170Z",
          "wordCount": 714,
          "title": "A Survey of Machine Learning Techniques for Detecting and Diagnosing COVID-19 from Imaging. (arXiv:2108.04344v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stadnick_B/0/1/0/all/0/1\">Benjamin Stadnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witowski_J/0/1/0/all/0/1\">Jan Witowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajiv_V/0/1/0/all/0/1\">Vishwaesh Rajiv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chledowski_J/0/1/0/all/0/1\">Jakub Ch&#x142;&#x119;dowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamout_F/0/1/0/all/0/1\">Farah E. Shamout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1\">Krzysztof J. Geras</a>",
          "description": "Artificial intelligence (AI) is transforming medicine and showing promise in\nimproving clinical diagnosis. In breast cancer screening, several recent\nstudies show that AI has the potential to improve radiologists' accuracy,\nsubsequently helping in early cancer diagnosis and reducing unnecessary workup.\nAs the number of proposed models and their complexity grows, it is becoming\nincreasingly difficult to re-implement them in order to reproduce the results\nand to compare different approaches. To enable reproducibility of research in\nthis application area and to enable comparison between different methods, we\nrelease a meta-repository containing deep learning models for classification of\nscreening mammograms. This meta-repository creates a framework that enables the\nevaluation of machine learning models on any private or public screening\nmammography data set. At its inception, our meta-repository contains five\nstate-of-the-art models with open-source implementations and cross-platform\ncompatibility. We compare their performance on five international data sets:\ntwo private New York University breast cancer screening data sets as well as\nthree public (DDSM, INbreast and Chinese Mammography Database) data sets. Our\nframework has a flexible design that can be generalized to other medical image\nanalysis tasks. The meta-repository is available at\nhttps://www.github.com/nyukat/mammography_metarepository.",
          "link": "http://arxiv.org/abs/2108.04800",
          "publishedOn": "2021-08-11T01:55:24.165Z",
          "wordCount": 641,
          "title": "Meta-repository of screening mammography classifiers. (arXiv:2108.04800v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kers_J/0/1/0/all/0/1\">Jesper Kers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassol_C/0/1/0/all/0/1\">Clarissa A. Cassol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_J/0/1/0/all/0/1\">Joris J. Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Idrees_N/0/1/0/all/0/1\">Najia Idrees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farber_A/0/1/0/all/0/1\">Alik Farber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haroon_S/0/1/0/all/0/1\">Samir Haroon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daly_K/0/1/0/all/0/1\">Kevin P. Daly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Suvranu Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitalia_V/0/1/0/all/0/1\">Vipul C. Chitalia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolachalama_V/0/1/0/all/0/1\">Vijaya B. Kolachalama</a>",
          "description": "Development of deep learning systems for biomedical segmentation often\nrequires access to expert-driven, manually annotated datasets. If more than a\nsingle expert is involved in the annotation of the same images, then the\ninter-expert agreement is not necessarily perfect, and no single expert\nannotation can precisely capture the so-called ground truth of the regions of\ninterest on all images. Also, it is not trivial to generate a reference\nestimate using annotations from multiple experts. Here we present a deep neural\nnetwork, defined as U-Net-and-a-half, which can simultaneously learn from\nannotations performed by multiple experts on the same set of images.\nU-Net-and-a-half contains a convolutional encoder to generate features from the\ninput images, multiple decoders that allow simultaneous learning from image\nmasks obtained from annotations that were independently generated by multiple\nexperts, and a shared low-dimensional feature space. To demonstrate the\napplicability of our framework, we used two distinct datasets from digital\npathology and radiology, respectively. Specifically, we trained two separate\nmodels using pathologist-driven annotations of glomeruli on whole slide images\nof human kidney biopsies (10 patients), and radiologist-driven annotations of\nlumen cross-sections of human arteriovenous fistulae obtained from\nintravascular ultrasound images (10 patients), respectively. The models based\non U-Net-and-a-half exceeded the performance of the traditional U-Net models\ntrained on single expert annotations alone, thus expanding the scope of\nmultitask learning in the context of biomedical image segmentation.",
          "link": "http://arxiv.org/abs/2108.04658",
          "publishedOn": "2021-08-11T01:55:24.159Z",
          "wordCount": 697,
          "title": "U-Net-and-a-half: Convolutional network for biomedical image segmentation using multiple expert-driven annotations. (arXiv:2108.04658v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04327",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Mikula_K/0/1/0/all/0/1\">Karol Mikula</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kollar_M/0/1/0/all/0/1\">Michal Kollar</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ozvat_A/0/1/0/all/0/1\">Aneta A. Ozvat</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ambroz_M/0/1/0/all/0/1\">Martin Ambroz</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cahojova_L/0/1/0/all/0/1\">Lucia Cahojova</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jarolimek_I/0/1/0/all/0/1\">Ivan Jarolimek</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sibik_J/0/1/0/all/0/1\">Jozef Sibik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sibikova_M/0/1/0/all/0/1\">Maria Sibikova</a>",
          "description": "Natural numerical networks are introduced as a new classification algorithm\nbased on the numerical solution of nonlinear partial differential equations of\nforward-backward diffusion type on complete graphs. The proposed natural\nnumerical network is applied to open important environmental and nature\nconservation task, the automated identification of protected habitats by using\nsatellite images. In the natural numerical network, the forward diffusion\ncauses the movement of points in a feature space toward each other. The\nopposite effect, keeping the points away from each other, is caused by backward\ndiffusion. This yields the desired classification. The natural numerical\nnetwork contains a few parameters that are optimized in the learning phase of\nthe method. After learning parameters and optimizing the topology of the\nnetwork graph, classification necessary for habitat identification is\nperformed. A relevancy map for each habitat is introduced as a tool for\nvalidating the classification and finding new Natura 2000 habitat appearances.",
          "link": "http://arxiv.org/abs/2108.04327",
          "publishedOn": "2021-08-11T01:55:24.154Z",
          "wordCount": 615,
          "title": "Natural Numerical Networks for Natura 2000 habitats classification by satellite images. (arXiv:2108.04327v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baltatzis_V/0/1/0/all/0/1\">Vasileios Baltatzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Folgoc_L/0/1/0/all/0/1\">Loic Le Folgoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_S/0/1/0/all/0/1\">Sam Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzanera_O/0/1/0/all/0/1\">Octavio E. Martinez Manzanera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bintsi_K/0/1/0/all/0/1\">Kyriaki-Margarita Bintsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1\">Arjun Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Sujal Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1\">Julia A. Schnabel</a>",
          "description": "Convolutional Neural Networks (CNNs) are widely used for image classification\nin a variety of fields, including medical imaging. While most studies deploy\ncross-entropy as the loss function in such tasks, a growing number of\napproaches have turned to a family of contrastive learning-based losses. Even\nthough performance metrics such as accuracy, sensitivity and specificity are\nregularly used for the evaluation of CNN classifiers, the features that these\nclassifiers actually learn are rarely identified and their effect on the\nclassification performance on out-of-distribution test samples is\ninsufficiently explored. In this paper, motivated by the real-world task of\nlung nodule classification, we investigate the features that a CNN learns when\ntrained and tested on different distributions of a synthetic dataset with\ncontrolled modes of variation. We show that different loss functions lead to\ndifferent features being learned and consequently affect the generalization\nability of the classifier on unseen data. This study provides some important\ninsights into the design of deep learning solutions for medical imaging tasks.",
          "link": "http://arxiv.org/abs/2108.04815",
          "publishedOn": "2021-08-11T01:55:24.133Z",
          "wordCount": 632,
          "title": "The Effect of the Loss on Generalization: Empirical Study on Synthetic Lung Nodule Data. (arXiv:2108.04815v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gasperini_S/0/1/0/all/0/1\">Stefano Gasperini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1\">Patrick Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dallabetta_V/0/1/0/all/0/1\">Vinzenz Dallabetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "While self-supervised monocular depth estimation in driving scenarios has\nachieved comparable performance to supervised approaches, violations of the\nstatic world assumption can still lead to erroneous depth predictions of\ntraffic participants, posing a potential safety issue. In this paper, we\npresent R4Dyn, a novel set of techniques to use cost-efficient radar data on\ntop of a self-supervised depth estimation framework. In particular, we show how\nradar can be used during training as weak supervision signal, as well as an\nextra input to enhance the estimation robustness at inference time. Since\nautomotive radars are readily available, this allows to collect training data\nfrom a variety of existing vehicles. Moreover, by filtering and expanding the\nsignal to make it compatible with learning-based approaches, we address radar\ninherent issues, such as noise and sparsity. With R4Dyn we are able to overcome\na major limitation of self-supervised depth estimation, i.e. the prediction of\ntraffic participants. We substantially improve the estimation on dynamic\nobjects, such as cars by 37% on the challenging nuScenes dataset, hence\ndemonstrating that radar is a valuable additional sensor for monocular depth\nestimation in autonomous vehicles. Additionally, we plan on making the code\npublicly available.",
          "link": "http://arxiv.org/abs/2108.04814",
          "publishedOn": "2021-08-11T01:55:24.124Z",
          "wordCount": 651,
          "title": "R4Dyn: Exploring Radar for Self-Supervised Monocular Depth Estimation of Dynamic Scenes. (arXiv:2108.04814v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2002.09674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Ziwen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>",
          "description": "Gait recognition is widely used in social security applications due to its\nadvantages in long-distance human identification. Recently, sequence-based\nmethods have achieved high accuracy by learning abundant temporal and spatial\ninformation. However, their robustness under adversarial attacks has not been\nclearly explored. In this paper, we demonstrate that the state-of-the-art gait\nrecognition model is vulnerable to such attacks. To this end, we propose a\nnovel temporal sparse adversarial attack method. Different from previous\nadditive noise models which add perturbations on original samples, we employ a\ngenerative adversarial network based architecture to semantically generate\nadversarial high-quality gait silhouettes or video frames. Moreover, by\nsparsely substituting or inserting a few adversarial gait silhouettes, the\nproposed method ensures its imperceptibility and achieves a high attack success\nrate. The experimental results show that if only one-fortieth of the frames are\nattacked, the accuracy of the target model drops dramatically.",
          "link": "http://arxiv.org/abs/2002.09674",
          "publishedOn": "2021-08-11T01:55:24.113Z",
          "wordCount": 618,
          "title": "Temporal Sparse Adversarial Attack on Sequence-based Gait Recognition. (arXiv:2002.09674v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rabbi_M/0/1/0/all/0/1\">Md Fazle Rabbi</a>",
          "description": "Derivative compressive sampling (DCS) is a signal reconstruction method from\nmeasurements of the spatial gradient with sub-Nyquist sampling rate.\nApplications of DCS include optical image reconstruction, photometric stereo,\nand shape-from-shading. In this work, we study the sensitivity of DCS with\nrespect to algorithmic hyperparameters using a brute-force search algorithm. We\nperform experiments on a dataset of surface images and deduce guidelines for\nthe user to setup values for the hyperparameters for improved signal recovery\nperformance.",
          "link": "http://arxiv.org/abs/2108.04355",
          "publishedOn": "2021-08-11T01:55:24.107Z",
          "wordCount": 502,
          "title": "Hyperparameter Analysis for Derivative Compressive Sampling. (arXiv:2108.04355v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this paper, we present a new network named Attention Aware Network (AASeg)\nfor real time semantic image segmentation. Our network incorporates spatial and\nchannel information using Spatial Attention (SA) and Channel Attention (CA)\nmodules respectively. It also uses dense local multi-scale context information\nusing Multi Scale Context (MSC) module. The feature maps are concatenated\nindividually to produce the final segmentation map. We demonstrate the\neffectiveness of our method using a comprehensive analysis, quantitative\nexperimental results and ablation study using Cityscapes, ADE20K and Camvid\ndatasets. Our network performs better than most previous architectures with a\n74.4\\% Mean IOU on Cityscapes test dataset while running at 202.7 FPS.",
          "link": "http://arxiv.org/abs/2108.04349",
          "publishedOn": "2021-08-11T01:55:24.089Z",
          "wordCount": 556,
          "title": "AASeg: Attention Aware Network for Real Time Semantic Segmentation. (arXiv:2108.04349v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1\">Bin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingyi He</a>",
          "description": "The vast majority of modern consumer-grade cameras employ a rolling shutter\nmechanism, leading to image distortions if the camera moves during image\nacquisition. In this paper, we present a novel deep network to solve the\ngeneric rolling shutter correction problem with two consecutive frames. Our\npipeline is symmetrically designed to predict the global shutter image\ncorresponding to the intermediate time of these two frames, which is difficult\nfor existing methods because it corresponds to a camera pose that differs most\nfrom the two frames. First, two time-symmetric dense undistortion flows are\nestimated by using well-established principles: pyramidal construction,\nwarping, and cost volume processing. Then, both rolling shutter images are\nwarped into a common global shutter one in the feature space, respectively.\nFinally, a symmetric consistency constraint is constructed in the image decoder\nto effectively aggregate the contextual cues of two rolling shutter images,\nthereby recovering the high-quality global shutter image. Extensive experiments\nwith both synthetic and real data from public benchmarks demonstrate the\nsuperiority of our proposed approach over the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.04775",
          "publishedOn": "2021-08-11T01:55:24.084Z",
          "wordCount": 620,
          "title": "SUNet: Symmetric Undistortion Network for Rolling Shutter Correction. (arXiv:2108.04775v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Amey Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1\">Mega Satish</a>",
          "description": "This paper aims to demonstrate the efficiency of the Adversarial Open Domain\nAdaption framework for sketch-to-photo synthesis. The unsupervised open domain\nadaption for generating realistic photos from a hand-drawn sketch is\nchallenging as there is no such sketch of that class for training data. The\nabsence of learning supervision and the huge domain gap between both the\nfreehand drawing and picture domains make it hard. We present an approach that\nlearns both sketch-to-photo and photo-to-sketch generation to synthesise the\nmissing freehand drawings from pictures. Due to the domain gap between\nsynthetic sketches and genuine ones, the generator trained on false drawings\nmay produce unsatisfactory results when dealing with drawings of lacking\nclasses. To address this problem, we offer a simple but effective open-domain\nsampling and optimization method that tricks the generator into considering\nfalse drawings as genuine. Our approach generalises the learnt sketch-to-photo\nand photo-to-sketch mappings from in-domain input to open-domain categories. On\nthe Scribble and SketchyCOCO datasets, we compared our technique to the most\ncurrent competing methods. For many types of open-domain drawings, our model\noutperforms impressive results in synthesising accurate colour, substance, and\nretaining the structural layout.",
          "link": "http://arxiv.org/abs/2108.04351",
          "publishedOn": "2021-08-11T01:55:24.027Z",
          "wordCount": 633,
          "title": "Adversarial Open Domain Adaption Framework (AODA): Sketch-to-Photo Synthesis. (arXiv:2108.04351v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Delong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zewen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Feng Xu</a>",
          "description": "In this demo, we present VirtualConductor, a system that can generate\nconducting video from any given music and a single user's image. First, a\nlarge-scale conductor motion dataset is collected and constructed. Then, we\npropose Audio Motion Correspondence Network (AMCNet) and adversarial-perceptual\nlearning to learn the cross-modal relationship and generate diverse, plausible,\nmusic-synchronized motion. Finally, we combine 3D animation rendering and a\npose transfer model to synthesize conducting video from a single given user's\nimage. Therefore, any user can become a virtual conductor through the system.",
          "link": "http://arxiv.org/abs/2108.04350",
          "publishedOn": "2021-08-11T01:55:24.016Z",
          "wordCount": 537,
          "title": "VirtualConductor: Music-driven Conducting Video Generation System. (arXiv:2108.04350v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinsheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qicong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jihua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharenborg/0/1/0/all/0/1\">Scharenborg</a>",
          "description": "Automatically generating videos in which synthesized speech is synchronized\nwith lip movements in a talking head has great potential in many human-computer\ninteraction scenarios. In this paper, we present an automatic method to\ngenerate synchronized speech and talking-head videos on the basis of text and a\nsingle face image of an arbitrary person as input. In contrast to previous\ntext-driven talking head generation methods, which can only synthesize the\nvoice of a specific person, the proposed method is capable of synthesizing\nspeech for any person that is inaccessible in the training stage. Specifically,\nthe proposed method decomposes the generation of synchronized speech and\ntalking head videos into two stages, i.e., a text-to-speech (TTS) stage and a\nspeech-driven talking head generation stage. The proposed TTS module is a\nface-conditioned multi-speaker TTS model that gets the speaker identity\ninformation from face images instead of speech, which allows us to synthesize a\npersonalized voice on the basis of the input face image. To generate the\ntalking head videos from the face images, a facial landmark-based method that\ncan predict both lip movements and head rotations is proposed. Extensive\nexperiments demonstrate that the proposed method is able to generate\nsynchronized speech and talking head videos for arbitrary persons and\nnon-persons. Synthesized speech shows consistency with the given face regarding\nto the synthesized voice's timbre and one's appearance in the image, and the\nproposed landmark-based talking head method outperforms the state-of-the-art\nlandmark-based method on generating natural talking head videos.",
          "link": "http://arxiv.org/abs/2108.04325",
          "publishedOn": "2021-08-11T01:55:24.001Z",
          "wordCount": 689,
          "title": "AnyoneNet: Synchronized Speech and Talking Head Generation for arbitrary person. (arXiv:2108.04325v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04267",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Estrada_S/0/1/0/all/0/1\">Santiago Estrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Ran Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diers_K/0/1/0/all/0/1\">Kersten Diers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Weiyi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehses_P/0/1/0/all/0/1\">Philipp Ehses</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stocker_T/0/1/0/all/0/1\">Tony St&#xf6;cker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breteler_M/0/1/0/all/0/1\">Monique M.B Breteler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuter_M/0/1/0/all/0/1\">Martin Reuter</a>",
          "description": "The neuroimage analysis community has neglected the automated segmentation of\nthe olfactory bulb (OB) despite its crucial role in olfactory function. The\nlack of an automatic processing method for the OB can be explained by its\nchallenging properties. Nonetheless, recent advances in MRI acquisition\ntechniques and resolution have allowed raters to generate more reliable manual\nannotations. Furthermore, the high accuracy of deep learning methods for\nsolving semantic segmentation problems provides us with an option to reliably\nassess even small structures. In this work, we introduce a novel, fast, and\nfully automated deep learning pipeline to accurately segment OB tissue on\nsub-millimeter T2-weighted (T2w) whole-brain MR images. To this end, we\ndesigned a three-stage pipeline: (1) Localization of a region containing both\nOBs using FastSurferCNN, (2) Segmentation of OB tissue within the localized\nregion through four independent AttFastSurferCNN - a novel deep learning\narchitecture with a self-attention mechanism to improve modeling of contextual\ninformation, and (3) Ensemble of the predicted label maps. The OB pipeline\nexhibits high performance in terms of boundary delineation, OB localization,\nand volume estimation across a wide range of ages in 203 participants of the\nRhineland Study. Moreover, it also generalizes to scans of an independent\ndataset never encountered during training, the Human Connectome Project (HCP),\nwith different acquisition parameters and demographics, evaluated in 30 cases\nat the native 0.7mm HCP resolution, and the default 0.8mm pipeline resolution.\nWe extensively validated our pipeline not only with respect to segmentation\naccuracy but also to known OB volume effects, where it can sensitively\nreplicate age effects.",
          "link": "http://arxiv.org/abs/2108.04267",
          "publishedOn": "2021-08-11T01:55:23.966Z",
          "wordCount": 707,
          "title": "Automated Olfactory Bulb Segmentation on High Resolutional T2-Weighted MRI. (arXiv:2108.04267v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_F/0/1/0/all/0/1\">Fangwen Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yaxu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambach_J/0/1/0/all/0/1\">Jason Rambach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagani_A/0/1/0/all/0/1\">Alain Pagani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>",
          "description": "This paper presents a semantic planar SLAM system that improves pose\nestimation and mapping using cues from an instance planar segmentation network.\nWhile the mainstream approaches are using RGB-D sensors, employing a monocular\ncamera with such a system still faces challenges such as robust data\nassociation and precise geometric model fitting. In the majority of existing\nwork, geometric model estimation problems such as homography estimation and\npiece-wise planar reconstruction (PPR) are usually solved by standard (greedy)\nRANSAC separately and sequentially. However, setting the inlier-outlier\nthreshold is difficult in absence of information about the scene (i.e. the\nscale). In this work, we revisit these problems and argue that two mentioned\ngeometric models (homographies/3D planes) can be solved by minimizing an energy\nfunction that exploits the spatial coherence, i.e. with graph-cut optimization,\nwhich also tackles the practical issue when the output of a trained CNN is\ninaccurate. Moreover, we propose an adaptive parameter setting strategy based\non our experiments, and report a comprehensive evaluation on various\nopen-source datasets.",
          "link": "http://arxiv.org/abs/2108.04281",
          "publishedOn": "2021-08-11T01:55:23.961Z",
          "wordCount": 609,
          "title": "Visual SLAM with Graph-Cut Optimized Multi-Plane Reconstruction. (arXiv:2108.04281v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04324",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bensaid_E/0/1/0/all/0/1\">Eden Bensaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_M/0/1/0/all/0/1\">Mauro Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1\">Benjamin Hoover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1\">Hendrik Strobelt</a>",
          "description": "Storytelling is an open-ended task that entails creative thinking and\nrequires a constant flow of ideas. Natural language generation (NLG) for\nstorytelling is especially challenging because it requires the generated text\nto follow an overall theme while remaining creative and diverse to engage the\nreader. In this work, we introduce a system and a web-based demo, FairyTailor,\nfor human-in-the-loop visual story co-creation. Users can create a cohesive\nchildren's fairytale by weaving generated texts and retrieved images with their\ninput. FairyTailor adds another modality and modifies the text generation\nprocess to produce a coherent and creative sequence of text and images. To our\nknowledge, this is the first dynamic tool for multimodal story generation that\nallows interactive co-formation of both texts and images. It allows users to\ngive feedback on co-created stories and share their results.",
          "link": "http://arxiv.org/abs/2108.04324",
          "publishedOn": "2021-08-11T01:55:23.955Z",
          "wordCount": 592,
          "title": "FairyTailor: A Multimodal Generative Framework for Storytelling. (arXiv:2108.04324v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1907.12975",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kromp_F/0/1/0/all/0/1\">Florian Kromp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_L/0/1/0/all/0/1\">Lukas Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozsaky_E/0/1/0/all/0/1\">Eva Bozsaky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambros_I/0/1/0/all/0/1\">Inge Ambros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doerr_W/0/1/0/all/0/1\">Wolfgang Doerr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taschner_Mandl_S/0/1/0/all/0/1\">Sabine Taschner-Mandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambros_P/0/1/0/all/0/1\">Peter Ambros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1\">Allan Hanbury</a>",
          "description": "Separating and labeling each instance of a nucleus (instance-aware\nsegmentation) is the key challenge in segmenting single cell nuclei on\nfluorescence microscopy images. Deep Neural Networks can learn the implicit\ntransformation of a nuclear image into a probability map indicating the class\nmembership of each pixel (nucleus or background), but the use of\npost-processing steps to turn the probability map into a labeled object mask is\nerror-prone. This especially accounts for nuclear images of tissue sections and\nnuclear images across varying tissue preparations. In this work, we aim to\nevaluate the performance of state-of-the-art deep learning architectures to\nsegment nuclei in fluorescence images of various tissue origins and sample\npreparation types without post-processing. We compare architectures that\noperate on pixel to pixel translation and an architecture that operates on\nobject detection and subsequent locally applied segmentation. In addition, we\npropose a novel strategy to create artificial images to extend the training\nset. We evaluate the influence of ground truth annotation quality, image scale\nand segmentation complexity on segmentation performance. Results show that\nthree out of four deep learning architectures (U-Net, U-Net with ResNet34\nbackbone, Mask R-CNN) can segment fluorescent nuclear images on most of the\nsample preparation types and tissue origins with satisfactory segmentation\nperformance. Mask R-CNN, an architecture designed to address instance aware\nsegmentation tasks, outperforms other architectures. Equal nuclear mean size,\nconsistent nuclear annotations and the use of artificially generated images\nresult in overall acceptable precision and recall across different tissues and\nsample preparation types.",
          "link": "http://arxiv.org/abs/1907.12975",
          "publishedOn": "2021-08-11T01:55:22.683Z",
          "wordCount": 725,
          "title": "Deep Learning architectures for generalized immunofluorescence based nuclear image segmentation. (arXiv:1907.12975v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07770",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Topiwala_P/0/1/0/all/0/1\">Pankaj Topiwala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pian_J/0/1/0/all/0/1\">Jiangfeng Pian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biondi_K/0/1/0/all/0/1\">Katalina Biondi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krovvidi_A/0/1/0/all/0/1\">Arvind Krovvidi</a>",
          "description": "Video quality assessment (VQA) is now a fastgrowing subject, beginning to\nmature in the full reference (FR) case, while the burgeoning no reference (NR)\ncase remains challenging. We investigate variants of the popular VMAF video\nquality assessment algorithm for the FR case, using support vector regression\nand feedforward neural networks, and extend it to the NR case, using the same\nlearning architectures, to develop a partially unified framework for VQA. When\nheavily trained, algorithms such as VMAF perform well on test datasets, with\n90%+ match; but predicting performance in the wild is better done by\ntraining/testing from scratch, as we do. Even from scratch, we achieve 90%+\nperformance in FR, with gains over VMAF. And we greatly reduce complexity vs.\nleading recent NR algorithms, VIDEVAL, RAPIQUE, yet exceed 80% in SRCC. In our\npreliminary testing, we find the improvements in trainability, while also\nconstraining computational complexity, as quite encouraging, suggesting further\nstudy and analysis.",
          "link": "http://arxiv.org/abs/2103.07770",
          "publishedOn": "2021-08-11T01:55:22.677Z",
          "wordCount": 634,
          "title": "VMAF And Variants: Towards A Unified VQA. (arXiv:2103.07770v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_Y/0/1/0/all/0/1\">Yawar Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1\">Justus Thies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fangchang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Q/0/1/0/all/0/1\">Qi Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>",
          "description": "3D reconstruction of large scenes is a challenging problem due to the\nhigh-complexity nature of the solution space, in particular for generative\nneural networks. In contrast to traditional generative learned models which\nencode the full generative process into a neural network and can struggle with\nmaintaining local details at the scene level, we introduce a new method that\ndirectly leverages scene geometry from the training database. First, we learn\nto synthesize an initial estimate for a 3D scene, constructed by retrieving a\ntop-k set of volumetric chunks from the scene database. These candidates are\nthen refined to a final scene generation with an attention-based refinement\nthat can effectively select the most consistent set of geometry from the\ncandidates and combine them together to create an output scene, facilitating\ntransfer of coherent structures and local detail from train scene geometry. We\ndemonstrate our neural scene reconstruction with a database for the tasks of 3D\nsuper resolution and surface reconstruction from sparse point clouds, showing\nthat our approach enables generation of more coherent, accurate 3D scenes,\nimproving on average by over 8% in IoU over state-of-the-art scene\nreconstruction.",
          "link": "http://arxiv.org/abs/2104.00024",
          "publishedOn": "2021-08-11T01:55:22.672Z",
          "wordCount": 663,
          "title": "RetrievalFuse: Neural 3D Scene Reconstruction with a Database. (arXiv:2104.00024v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1\">Zhenyu Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuesheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruixin Liu</a>",
          "description": "Weighted Hamming distance, as a similarity measure between binary codes and\nbinary queries, provides superior accuracy in search tasks than Hamming\ndistance. However, how to efficiently and accurately find $K$ binary codes that\nhave the smallest weighted Hamming distance to the query remains an open issue.\nIn this paper, a fast search algorithm is proposed to perform the\nnon-exhaustive search for $K$ nearest binary codes by weighted Hamming\ndistance. By using binary codes as direct bucket indices in a hash table, the\nsearch algorithm generates a sequence to probe the buckets based on the\nindependence characteristic of the weights for each bit. Furthermore, a fast\nsearch framework based on the proposed search algorithm is designed to solve\nthe problem of long binary codes. Specifically, long binary codes are split\ninto substrings and multiple hash tables are built on them. Then, the search\nalgorithm probes the buckets to obtain candidates according to the generated\nsubstring indices, and a merging algorithm is proposed to find the nearest\nbinary codes by merging the candidates. Theoretical analysis and experimental\nresults demonstrate that the search algorithm improves the search accuracy\ncompared to other non-exhaustive algorithms and provides orders-of-magnitude\nfaster search than the linear scan baseline.",
          "link": "http://arxiv.org/abs/2009.08591",
          "publishedOn": "2021-08-11T01:55:22.666Z",
          "wordCount": 666,
          "title": "Fast Search on Binary Codes by Weighted Hamming Distance. (arXiv:2009.08591v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1\">Xianbo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shunquan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiwu Huang</a>",
          "description": "Recent research has shown that non-additive image steganographic frameworks\neffectively improve security performance through adjusting distortion\ndistribution. However, as far as we know, all of the existing non-additive\nproposals are based on handcrafted policies, and can only be applied to a\nspecific image domain, which heavily prevent non-additive steganography from\nreleasing its full potentiality. In this paper, we propose an automatic\nnon-additive steganographic distortion learning framework called MCTSteg to\nremove the above restrictions. Guided by the reinforcement learning paradigm,\nwe combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental\nmodel to build MCTSteg. MCTS makes sequential decisions to adjust distortion\ndistribution without human intervention. Our proposed environmental model is\nused to obtain feedbacks from each decision. Due to its self-learning\ncharacteristic and domain-independent reward function, MCTSteg has become the\nfirst reported universal non-additive steganographic framework which can work\nin both spatial and JPEG domains. Extensive experimental results show that\nMCTSteg can effectively withstand the detection of both hand-crafted\nfeature-based and deep-learning-based steganalyzers. In both spatial and JPEG\ndomains, the security performance of MCTSteg steadily outperforms the state of\nthe art by a clear margin under different scenarios.",
          "link": "http://arxiv.org/abs/2103.13689",
          "publishedOn": "2021-08-11T01:55:22.652Z",
          "wordCount": 670,
          "title": "MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning Framework for Universal Non-additive Steganography. (arXiv:2103.13689v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04547",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weilun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>",
          "description": "Contrastive learning shows great potential in unpaired image-to-image\ntranslation, but sometimes the translated results are in poor quality and the\ncontents are not preserved consistently. In this paper, we uncover that the\nnegative examples play a critical role in the performance of contrastive\nlearning for image translation. The negative examples in previous methods are\nrandomly sampled from the patches of different positions in the source image,\nwhich are not effective to push the positive examples close to the query\nexamples. To address this issue, we present instance-wise hard Negative Example\nGeneration for Contrastive learning in Unpaired image-to-image\nTranslation~(NEGCUT). Specifically, we train a generator to produce negative\nexamples online. The generator is novel from two perspectives: 1) it is\ninstance-wise which means that the generated examples are based on the input\nimage, and 2) it can generate hard negative examples since it is trained with\nan adversarial loss. With the generator, the performance of unpaired\nimage-to-image translation is significantly improved. Experiments on three\nbenchmark datasets demonstrate that the proposed NEGCUT framework achieves\nstate-of-the-art performance compared to previous methods.",
          "link": "http://arxiv.org/abs/2108.04547",
          "publishedOn": "2021-08-11T01:55:22.646Z",
          "wordCount": 623,
          "title": "Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation. (arXiv:2108.04547v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12664",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shi-Xue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaobin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xu-Cheng Yin</a>",
          "description": "Arbitrary shape text detection is a challenging task due to the high\ncomplexity and variety of scene texts. In this work, we propose a novel\nadaptive boundary proposal network for arbitrary shape text detection, which\ncan learn to directly produce accurate boundary for arbitrary shape text\nwithout any post-processing. Our method mainly consists of a boundary proposal\nmodel and an innovative adaptive boundary deformation model. The boundary\nproposal model constructed by multi-layer dilated convolutions is adopted to\nproduce prior information (including classification map, distance field, and\ndirection field) and coarse boundary proposals. The adaptive boundary\ndeformation model is an encoder-decoder network, in which the encoder mainly\nconsists of a Graph Convolutional Network (GCN) and a Recurrent Neural Network\n(RNN). It aims to perform boundary deformation in an iterative way for\nobtaining text instance shape guided by prior information from the boundary\nproposal model. In this way, our method can directly and efficiently generate\naccurate text boundaries without complex post-processing. Extensive experiments\non publicly available datasets demonstrate the state-of-the-art performance of\nour method.",
          "link": "http://arxiv.org/abs/2107.12664",
          "publishedOn": "2021-08-11T01:55:22.641Z",
          "wordCount": 669,
          "title": "Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection. (arXiv:2107.12664v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_W/0/1/0/all/0/1\">Weili Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>",
          "description": "Finding tampered regions in images is a hot research topic in machine\nlearning and computer vision. Although many image manipulation location\nalgorithms have been proposed, most of them only focus on the RGB images with\ndifferent color spaces, and the frequency information that contains the\npotential tampering clues is often ignored. In this work, a novel end-to-end\ntwo-stream boundary-aware network (abbreviated as TBNet) is proposed for\ngeneric image manipulation localization in which the RGB stream, the frequency\nstream, and the boundary artifact location are explored in a unified framework.\nSpecifically, we first design an adaptive frequency selection module (AFS) to\nadaptively select the appropriate frequency to mine inconsistent statistics and\neliminate the interference of redundant statistics. Then, an adaptive\ncross-attention fusion module (ACF) is proposed to adaptively fuse the RGB\nfeature and the frequency feature. Finally, the boundary artifact location\nnetwork (BAL) is designed to locate the boundary artifacts for which the\nparameters are jointly updated by the outputs of the ACF, and its results are\nfurther fed into the decoder. Thus, the parameters of the RGB stream, the\nfrequency stream, and the boundary artifact location network are jointly\noptimized, and their latent complementary relationships are fully mined. The\nresults of extensive experiments performed on four public benchmarks of the\nimage manipulation localization task, namely, CASIA1.0, COVER, Carvalho, and\nIn-The-Wild, demonstrate that the proposed TBNet can significantly outperform\nstate-of-the-art generic image manipulation localization methods in terms of\nboth MCC and F1.",
          "link": "http://arxiv.org/abs/2108.04508",
          "publishedOn": "2021-08-11T01:55:22.635Z",
          "wordCount": 680,
          "title": "TBNet:Two-Stream Boundary-aware Network for Generic Image Manipulation Localization. (arXiv:2108.04508v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+li_Y/0/1/0/all/0/1\">Yuan li</a>",
          "description": "Among 2D convolutional networks on point clouds, point-based approaches\nconsume point clouds of fixed size directly. By analysis of PointNet, a pioneer\nin introducing deep learning into point sets, we reveal that current\npoint-based methods are essentially spatial relationship processing networks.\nIn this paper, we take a different approach. Our architecture, named PE-Net,\nlearns the representation of point clouds in high-dimensional space, and\nencodes the unordered input points to feature vectors, which standard 2D CNNs\ncan be applied to. The recommended network can adapt to changes in the number\nof input points which is the limit of current methods. Experiments show that in\nthe tasks of classification and part segmentation, PE-Net achieves the\nstate-of-the-art performance in multiple challenging datasets, such as ModelNet\nand ShapeNetPart.",
          "link": "http://arxiv.org/abs/2107.08565",
          "publishedOn": "2021-08-11T01:55:22.630Z",
          "wordCount": 578,
          "title": "Learning point embedding for 3D data processing. (arXiv:2107.08565v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shaoqing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dingfu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Junbo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bin_Z/0/1/0/all/0/1\">Zhou Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjun Zhang</a>",
          "description": "Accurate detection of obstacles in 3D is an essential task for autonomous\ndriving and intelligent transportation. In this work, we propose a general\nmultimodal fusion framework FusionPainting to fuse the 2D RGB image and 3D\npoint clouds at a semantic level for boosting the 3D object detection task.\nEspecially, the FusionPainting framework consists of three main modules: a\nmulti-modal semantic segmentation module, an adaptive attention-based semantic\nfusion module, and a 3D object detector. First, semantic information is\nobtained for 2D images and 3D Lidar point clouds based on 2D and 3D\nsegmentation approaches. Then the segmentation results from different sensors\nare adaptively fused based on the proposed attention-based semantic fusion\nmodule. Finally, the point clouds painted with the fused semantic label are\nsent to the 3D detector for obtaining the 3D objection results. The\neffectiveness of the proposed framework has been verified on the large-scale\nnuScenes detection benchmark by comparing it with three different baselines.\nThe experimental results show that the fusion strategy can significantly\nimprove the detection performance compared to the methods using only point\nclouds, and the methods using point clouds only painted with 2D segmentation\ninformation. Furthermore, the proposed approach outperforms other\nstate-of-the-art methods on the nuScenes testing benchmark.",
          "link": "http://arxiv.org/abs/2106.12449",
          "publishedOn": "2021-08-11T01:55:22.624Z",
          "wordCount": 678,
          "title": "FusionPainting: Multimodal Fusion with Adaptive Attention for 3D Object Detection. (arXiv:2106.12449v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.07588",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "Deep learning motivated by convolutional neural networks has been highly\nsuccessful in a range of medical imaging problems like image classification,\nimage segmentation, image synthesis etc. However for validation and\ninterpretability, not only do we need the predictions made by the model but\nalso how confident it is while making those predictions. This is important in\nsafety critical applications for the people to accept it. In this work, we used\nan encoder decoder architecture based on variational inference techniques for\nsegmenting brain tumour images. We evaluate our work on the publicly available\nBRATS dataset using Dice Similarity Coefficient (DSC) and Intersection Over\nUnion (IOU) as the evaluation metrics. Our model is able to segment brain\ntumours while taking into account both aleatoric uncertainty and epistemic\nuncertainty in a principled bayesian manner.",
          "link": "http://arxiv.org/abs/2008.07588",
          "publishedOn": "2021-08-11T01:55:22.618Z",
          "wordCount": 602,
          "title": "Uncertainty Quantification using Variational Inference for Biomedical Image Segmentation. (arXiv:2008.07588v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>",
          "description": "Recent vision-language (VL) studies have shown remarkable progress by\nlearning generic representations from massive image-text pairs with transformer\nmodels and then fine-tuning on downstream VL tasks. While existing research has\nbeen focused on achieving high accuracy with large pre-trained models, building\na lightweight model is of great value in practice but is less explored. In this\npaper, we propose a smaller and faster VL model, MiniVLM, which can be\nfinetuned with good performance on various downstream tasks like its larger\ncounterpart. MiniVLM consists of two modules, a vision feature extractor and a\ntransformer-based vision-language fusion module. We design a Two-stage\nEfficient feature Extractor (TEE), inspired by the one-stage EfficientDet\nnetwork, to significantly reduce the time cost of visual feature extraction by\n$95\\%$, compared to a baseline model. We adopt the MiniLM structure to reduce\nthe computation cost of the transformer module after comparing different\ncompact BERT models. In addition, we improve the MiniVLM pre-training by adding\n$7M$ Open Images data, which are pseudo-labeled by a state-of-the-art\ncaptioning model. We also pre-train with high-quality image tags obtained from\na strong tagging model to enhance cross-modality alignment. The large models\nare used offline without adding any overhead in fine-tuning and inference. With\nthe above design choices, our MiniVLM reduces the model size by $73\\%$ and the\ninference time cost by $94\\%$ while being able to retain $94-97\\%$ of the\naccuracy on multiple VL tasks. We hope that MiniVLM helps ease the use of the\nstate-of-the-art VL research for on-the-edge applications.",
          "link": "http://arxiv.org/abs/2012.06946",
          "publishedOn": "2021-08-11T01:55:22.612Z",
          "wordCount": 721,
          "title": "MiniVLM: A Smaller and Faster Vision-Language Model. (arXiv:2012.06946v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeGendre_C/0/1/0/all/0/1\">Chloe LeGendre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yun-Ta Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bleibel_F/0/1/0/all/0/1\">Francois Bleibel</a>",
          "description": "We present \"Cross-Camera Convolutional Color Constancy\" (C5), a\nlearning-based method, trained on images from multiple cameras, that accurately\nestimates a scene's illuminant color from raw images captured by a new camera\npreviously unseen during training. C5 is a hypernetwork-like extension of the\nconvolutional color constancy (CCC) approach: C5 learns to generate the weights\nof a CCC model that is then evaluated on the input image, with the CCC weights\ndynamically adapted to different input content. Unlike prior cross-camera color\nconstancy models, which are usually designed to be agnostic to the spectral\nproperties of test-set images from unobserved cameras, C5 approaches this\nproblem through the lens of transductive inference: additional unlabeled images\nare provided as input to the model at test time, which allows the model to\ncalibrate itself to the spectral properties of the test-set camera during\ninference. C5 achieves state-of-the-art accuracy for cross-camera color\nconstancy on several datasets, is fast to evaluate (~7 and ~90 ms per image on\na GPU or CPU, respectively), and requires little memory (~2 MB), and, thus, is\na practical solution to the problem of calibration-free automatic white balance\nfor mobile photography.",
          "link": "http://arxiv.org/abs/2011.11890",
          "publishedOn": "2021-08-11T01:55:22.587Z",
          "wordCount": 653,
          "title": "Cross-Camera Convolutional Color Constancy. (arXiv:2011.11890v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.04264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dongliang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1\">Ayan Kumar Bhunia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Ming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>",
          "description": "Key for solving fine-grained image categorization is finding discriminate and\nlocal regions that correspond to subtle visual traits. Great strides have been\nmade, with complex networks designed specifically to learn part-level\ndiscriminate feature representations. In this paper, we show it is possible to\ncultivate subtle details without the need for overly complicated network\ndesigns or training mechanisms -- a single loss is all it takes. The main trick\nlies with how we delve into individual feature channels early on, as opposed to\nthe convention of starting from a consolidated feature map. The proposed loss\nfunction, termed as mutual-channel loss (MC-Loss), consists of two\nchannel-specific components: a discriminality component and a diversity\ncomponent. The discriminality component forces all feature channels belonging\nto the same class to be discriminative, through a novel channel-wise attention\nmechanism. The diversity component additionally constraints channels so that\nthey become mutually exclusive on spatial-wise. The end result is therefore a\nset of feature channels that each reflects different locally discriminative\nregions for a specific class. The MC-Loss can be trained end-to-end, without\nthe need for any bounding-box/part annotations, and yields highly\ndiscriminative regions during inference. Experimental results show our MC-Loss\nwhen implemented on top of common base networks can achieve state-of-the-art\nperformance on all four fine-grained categorization datasets (CUB-Birds,\nFGVC-Aircraft, Flowers-102, and Stanford-Cars). Ablative studies further\ndemonstrate the superiority of MC-Loss when compared with other recently\nproposed general-purpose losses for visual classification, on two different\nbase networks. Code available at\nhttps://github.com/dongliangchang/Mutual-Channel-Loss",
          "link": "http://arxiv.org/abs/2002.04264",
          "publishedOn": "2021-08-11T01:55:22.580Z",
          "wordCount": 752,
          "title": "The Devil is in the Channels: Mutual-Channel Loss for Fine-Grained Image Classification. (arXiv:2002.04264v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abrahamyan_L/0/1/0/all/0/1\">Lusine Abrahamyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziatchin_V/0/1/0/all/0/1\">Valentin Ziatchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1\">Nikos Deligiannis</a>",
          "description": "Compact convolutional neural networks (CNNs) have witnessed exceptional\nimprovements in performance in recent years. However, they still fail to\nprovide the same predictive power as CNNs with a large number of parameters.\nThe diverse and even abundant features captured by the layers is an important\ncharacteristic of these successful CNNs. However, differences in this\ncharacteristic between large CNNs and their compact counterparts have rarely\nbeen investigated. In compact CNNs, due to the limited number of parameters,\nabundant features are unlikely to be obtained, and feature diversity becomes an\nessential characteristic. Diverse features present in the activation maps\nderived from a data point during model inference may indicate the presence of a\nset of unique descriptors necessary to distinguish between objects of different\nclasses. In contrast, data points with low feature diversity may not provide a\nsufficient amount of unique descriptors to make a valid prediction; we refer to\nthem as random predictions. Random predictions can negatively impact the\noptimization process and harm the final performance. This paper proposes\naddressing the problem raised by random predictions by reshaping the standard\ncross-entropy to make it biased toward data points with a limited number of\nunique descriptive features. Our novel Bias Loss focuses the training on a set\nof valuable data points and prevents the vast number of samples with poor\nlearning features from misleading the optimization process. Furthermore, to\nshow the importance of diversity, we present a family of SkipNet models whose\narchitectures are brought to boost the number of unique descriptors in the last\nlayers. Our Skipnet-M can achieve 1% higher classification accuracy than\nMobileNetV3 Large.",
          "link": "http://arxiv.org/abs/2107.11170",
          "publishedOn": "2021-08-11T01:55:22.557Z",
          "wordCount": 744,
          "title": "Bias Loss for Mobile Neural Networks. (arXiv:2107.11170v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rong_T/0/1/0/all/0/1\">Tianze Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongxiang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yichao Xiong</a>",
          "description": "We proposed a new modeling method to promote the performance of prohibited\nitems recognition via X-ray image. We analyzed the characteristics of\nprohibited items and X-ray images. We found the fact that the scales of some\nitems are too small to be recognized which encumber the model performance. Then\nwe adopted a set of data augmentation and modified the model to adapt the field\nof prohibited items recognition. The Convolutional Block Attention Module(CBAM)\nand rescoring mechanism has been assembled into the model. By the modification,\nour model achieved a mAP of 89.9% on SIXray10, mAP of 74.8%.",
          "link": "http://arxiv.org/abs/2102.12256",
          "publishedOn": "2021-08-11T01:55:22.550Z",
          "wordCount": 553,
          "title": "An Enhanced Prohibited Items Recognition Model. (arXiv:2102.12256v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.11946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rong_T/0/1/0/all/0/1\">Tianze Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanjia Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongxiang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yichao Xiong</a>",
          "description": "This work is a solution to densely packed scenes dataset SKU-110k. Our work\nis modified from Cascade R-CNN. To solve the problem, we proposed a random crop\nstrategy to ensure both the sampling rate and input scale is relatively\nsufficient as a contrast to the regular random crop. And we adopted some of\ntrick and optimized the hyper-parameters. To grasp the essential feature of the\ndensely packed scenes, we analysis the stages of a detector and investigate the\nbottleneck which limits the performance. As a result, our method obtains 58.7\nmAP on test set of SKU-110k.",
          "link": "http://arxiv.org/abs/2007.11946",
          "publishedOn": "2021-08-11T01:55:22.539Z",
          "wordCount": 572,
          "title": "A Solution to Product detection in Densely Packed Scenes. (arXiv:2007.11946v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honglei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuanzhouhan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yidong Li</a>",
          "description": "Data hiding is the procedure of encoding desired information into an image to\nresist potential noises while ensuring the embedded image has little perceptual\nperturbations from the original image. Recently, with the tremendous successes\ngained by deep neural networks in various fields, data hiding areas have\nattracted increasing number of attentions. The neglect of considering the pixel\nsensitivity within the cover image of deep neural methods will inevitably\naffect the model robustness for information hiding. Targeting at the problem,\nin this paper, we propose a novel deep data hiding scheme with Inverse Gradient\nAttention (IGA), combing the ideas of adversarial learning and attention\nmechanism to endow different sensitivity to different pixels. With the proposed\ncomponent, the model can spotlight pixels with more robustness for embedding\ndata. Empirically, extensive experiments show that the proposed model\noutperforms the state-of-the-art methods on two prevalent datasets under\nmultiple settings. Besides, we further identify and discuss the connections\nbetween the proposed inverse gradient attention and high-frequency regions\nwithin images.",
          "link": "http://arxiv.org/abs/2011.10850",
          "publishedOn": "2021-08-11T01:55:22.524Z",
          "wordCount": 649,
          "title": "Robust Data Hiding Using Inverse Gradient Attention. (arXiv:2011.10850v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sudipan Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lichao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahzad_M/0/1/0/all/0/1\">Muhammad Shahzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>",
          "description": "Semantic segmentation is a crucial step in many Earth observation tasks.\nLarge quantity of pixel-level annotation is required to train deep networks for\nsemantic segmentation. Earth observation techniques are applied to varieties of\napplications and since classes vary widely depending on the applications,\ntherefore, domain knowledge is often required to label Earth observation\nimages, impeding availability of labeled training data in many Earth\nobservation applications. To tackle these challenges, in this paper we propose\nan unsupervised semantic segmentation method that can be trained using just a\nsingle unlabeled scene. Remote sensing scenes are generally large. The proposed\nmethod exploits this property to sample smaller patches from the larger scene\nand uses deep clustering and contrastive learning to refine the weights of a\nlightweight deep model composed of a series of the convolution layers along\nwith an embedded channel attention. After unsupervised training on the target\nimage/scene, the model automatically segregates the major classes present in\nthe scene and produces the segmentation map. Experimental results on the\nVaihingen dataset demonstrate the efficacy of the proposed method.",
          "link": "http://arxiv.org/abs/2108.04222",
          "publishedOn": "2021-08-11T01:55:22.513Z",
          "wordCount": 629,
          "title": "Segmentation of VHR EO Images using Unsupervised Learning. (arXiv:2108.04222v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lingyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Chenqi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hanwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhu Li</a>",
          "description": "In this paper, we propose a no-reference (NR) image quality assessment (IQA)\nmethod via feature level pseudo-reference (PR) hallucination. The proposed\nquality assessment framework is grounded on the prior models of natural image\nstatistical behaviors and rooted in the view that the perceptually meaningful\nfeatures could be well exploited to characterize the visual quality. Herein,\nthe PR features from the distorted images are learned by a mutual learning\nscheme with the pristine reference as the supervision, and the discriminative\ncharacteristics of PR features are further ensured with the triplet\nconstraints. Given a distorted image for quality inference, the feature level\ndisentanglement is performed with an invertible neural layer for final quality\nprediction, leading to the PR and the corresponding distortion features for\ncomparison. The effectiveness of our proposed method is demonstrated on four\npopular IQA databases, and superior performance on cross-database evaluation\nalso reveals the high generalization capability of our method. The\nimplementation of our method is publicly available on\nhttps://github.com/Baoliang93/FPR.",
          "link": "http://arxiv.org/abs/2108.04165",
          "publishedOn": "2021-08-11T01:55:22.499Z",
          "wordCount": 619,
          "title": "No-Reference Image Quality Assessment by Hallucinating Pristine Features. (arXiv:2108.04165v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaemin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minseok Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jongchan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dong-Geol Choi</a>",
          "description": "Deep convolutional neural networks (CNNs) have shown state-of-the-art\nperformances in various computer vision tasks. Advances on CNN architectures\nhave focused mainly on designing convolutional blocks of the feature\nextractors, but less on the classifiers that exploit extracted features. In\nthis work, we propose Split-and-Share Module (SSM),a classifier that splits a\ngiven feature into parts, which are partially shared by multiple\nsub-classifiers. Our intuition is that the more the features are shared, the\nmore common they will become, and SSM can encourage such structural\ncharacteristics in the split features. SSM can be easily integrated into any\narchitecture without bells and whistles. We have extensively validated the\nefficacy of SSM on ImageNet-1K classification task, andSSM has shown consistent\nand significant improvements over baseline architectures. In addition, we\nanalyze the effect of SSM using the Grad-CAM visualization.",
          "link": "http://arxiv.org/abs/2108.04500",
          "publishedOn": "2021-08-11T01:55:22.493Z",
          "wordCount": 561,
          "title": "Exploiting Featureswith Split-and-Share Module. (arXiv:2108.04500v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taherkhani_F/0/1/0/all/0/1\">Fariborz Taherkhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1\">Ali Dabouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1\">Sobhan Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "Visual perception of a person is easily influenced by many factors such as\ncamera parameters, pose and viewpoint variations. These variations make person\nRe-Identification (ReID) a challenging problem. Nevertheless, human attributes\nusually stand as robust visual properties to such variations. In this paper, we\npropose a new method to leverage features from human attributes for person\nReID. Our model uses a tensor to non-linearly fuse identity and attribute\nfeatures, and then forces the parameters of the tensor in the loss function to\ngenerate discriminative fused features for ReID. Since tensor-based methods\nusually contain a large number of parameters, training all of these parameters\nbecomes very slow, and the chance of overfitting increases as well. To address\nthis issue, we propose two new techniques based on Structural Sparsity Learning\n(SSL) and Tensor Decomposition (TD) methods to create an accurate and stable\nlearning problem. We conducted experiments on several standard pedestrian\ndatasets, and experimental results indicate that our tensor-based approach\nsignificantly improves person ReID baselines and also outperforms state of the\nart methods.",
          "link": "http://arxiv.org/abs/2108.04352",
          "publishedOn": "2021-08-11T01:55:22.482Z",
          "wordCount": 609,
          "title": "Attribute Guided Sparse Tensor-Based Model for Person Re-Identification. (arXiv:2108.04352v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1\">Daochen Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_Z/0/1/0/all/0/1\">Zaid Pervaiz Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Sirui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anmoll Kumar Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_M/0/1/0/all/0/1\">Mohammad Qazim Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1\">Kwei-Herng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaben Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_N/0/1/0/all/0/1\">Na Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>",
          "description": "Action recognition is a crucial task for video understanding. In this paper,\nwe present AutoVideo, a Python system for automated video action recognition.\nIt currently supports seven action recognition algorithms and various\npre-processing modules. Unlike the existing libraries that only provide model\nzoos, AutoVideo is built with the standard pipeline language. The basic\nbuilding block is primitive, which wraps a pre-processing module or an\nalgorithm with some hyperparameters. AutoVideo is highly modular and\nextendable. It can be easily combined with AutoML searchers. The pipeline\nlanguage is quite general so that we can easily enrich AutoVideo with\nalgorithms for various other video-related tasks in the future. AutoVideo is\nreleased under MIT license at https://github.com/datamllab/autovideo",
          "link": "http://arxiv.org/abs/2108.04212",
          "publishedOn": "2021-08-11T01:55:22.464Z",
          "wordCount": 598,
          "title": "AutoVideo: An Automated Video Action Recognition System. (arXiv:2108.04212v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>",
          "description": "Semi-supervised machine learning models learn from a (small) set of labeled\ntraining examples, and a (large) set of unlabeled training examples.\nState-of-the-art models can reach within a few percentage points of\nfully-supervised training, while requiring 100x less labeled data.\n\nWe study a new class of vulnerabilities: poisoning attacks that modify the\nunlabeled dataset. In order to be useful, unlabeled datasets are given strictly\nless review than labeled datasets, and adversaries can therefore poison them\neasily. By inserting maliciously-crafted unlabeled examples totaling just 0.1%\nof the dataset size, we can manipulate a model trained on this poisoned dataset\nto misclassify arbitrary examples at test time (as any desired label). Our\nattacks are highly effective across datasets and semi-supervised learning\nmethods.\n\nWe find that more accurate methods (thus more likely to be used) are\nsignificantly more vulnerable to poisoning attacks, and as such better training\nmethods are unlikely to prevent this attack. To counter this we explore the\nspace of defenses, and propose two methods that mitigate our attack.",
          "link": "http://arxiv.org/abs/2105.01622",
          "publishedOn": "2021-08-11T01:55:22.458Z",
          "wordCount": 627,
          "title": "Poisoning the Unlabeled Dataset of Semi-Supervised Learning. (arXiv:2105.01622v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Linxi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_DArpino_C/0/1/0/all/0/1\">Claudia P&#xe9;rez-D&#x27;Arpino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1\">Shyamal Buch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_L/0/1/0/all/0/1\">Lyne P. Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1\">Micael E. Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1\">Kent Vainio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Josiah Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>",
          "description": "We present iGibson 1.0, a novel simulation environment to develop robotic\nsolutions for interactive tasks in large-scale realistic scenes. Our\nenvironment contains 15 fully interactive home-sized scenes with 108 rooms\npopulated with rigid and articulated objects. The scenes are replicas of\nreal-world homes, with distribution and the layout of objects aligned to those\nof the real world. iGibson 1.0 integrates several key features to facilitate\nthe study of interactive tasks: i) generation of high-quality virtual sensor\nsignals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain\nrandomization to change the materials of the objects (both visual and physical)\nand/or their shapes, iii) integrated sampling-based motion planners to generate\ncollision-free trajectories for robot bases and arms, and iv) intuitive\nhuman-iGibson interface that enables efficient collection of human\ndemonstrations. Through experiments, we show that the full interactivity of the\nscenes enables agents to learn useful visual representations that accelerate\nthe training of downstream manipulation tasks. We also show that iGibson 1.0\nfeatures enable the generalization of navigation agents, and that the\nhuman-iGibson interface and integrated motion planners facilitate efficient\nimitation learning of human demonstrated (mobile) manipulation behaviors.\niGibson 1.0 is open-source, equipped with comprehensive examples and\ndocumentation. For more information, visit our project website:\nthis http URL",
          "link": "http://arxiv.org/abs/2012.02924",
          "publishedOn": "2021-08-11T01:55:22.440Z",
          "wordCount": 757,
          "title": "IGibson 1.0: a Simulation Environment for Interactive Tasks in Large Realistic Scenes. (arXiv:2012.02924v6 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taherkhani_F/0/1/0/all/0/1\">Fariborz Taherkhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1\">Ali Dabouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1\">Sobhan Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "The great success of Convolutional Neural Networks (CNN) for facial attribute\nprediction relies on a large amount of labeled images. Facial image datasets\nare usually annotated by some commonly used attributes (e.g., gender), while\nlabels for the other attributes (e.g., big nose) are limited which causes their\nprediction challenging. To address this problem, we use a new Multi-Task\nLearning (MTL) paradigm in which a facial attribute predictor uses the\nknowledge of other related attributes to obtain a better generalization\nperformance. Here, we leverage MLT paradigm in two problem settings. First, it\nis assumed that the structure of the tasks (e.g., grouping pattern of facial\nattributes) is known as a prior knowledge, and parameters of the tasks (i.e.,\npredictors) within the same group are represented by a linear combination of a\nlimited number of underlying basis tasks. Here, a sparsity constraint on the\ncoefficients of this linear combination is also considered such that each task\nis represented in a more structured and simpler manner. Second, it is assumed\nthat the structure of the tasks is unknown, and then structure and parameters\nof the tasks are learned jointly by using a Laplacian regularization framework.\nOur MTL methods are compared with competing methods for facial attribute\nprediction to show its effectiveness.",
          "link": "http://arxiv.org/abs/2108.04353",
          "publishedOn": "2021-08-11T01:55:22.424Z",
          "wordCount": 652,
          "title": "Tasks Structure Regularization in Multi-Task Learning for Improving Facial Attribute Prediction. (arXiv:2108.04353v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Besler_B/0/1/0/all/0/1\">Bryce A Besler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemp_T/0/1/0/all/0/1\">Tannis D. Kemp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalski_A/0/1/0/all/0/1\">Andrew S. Michalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forkert_N/0/1/0/all/0/1\">Nils D. Forkert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_S/0/1/0/all/0/1\">Steven K. Boyd</a>",
          "description": "Anatomical structures such as the hippocampus, liver, and bones can be\nanalyzed as orientable, closed surfaces. This permits the computation of\nvolume, surface area, mean curvature, Gaussian curvature, and the\nEuler-Poincar\\'e characteristic as well as comparison of these morphometrics\nbetween structures of different topology. The structures are commonly\nrepresented implicitly in curve evolution problems as the zero level set of an\nembedding. Practically, binary images of anatomical structures are embedded\nusing a signed distance transform. However, quantization prevents the accurate\ncomputation of curvatures, leading to considerable errors in morphometry. This\npaper presents a fast, simple embedding procedure for accurate local\nmorphometry as the zero crossing of the Gaussian blurred binary image. The\nproposed method was validated based on the femur and fourth lumbar vertebrae of\n50 clinical computed tomography datasets. The results show that the signed\ndistance transform leads to large quantization errors in the computed local\ncurvature. Global validation of morphometry using regression and Bland-Altman\nanalysis revealed that the coefficient of determination for the average mean\ncurvature is improved from 93.8% with the signed distance transform to 100%\nwith the proposed method. For the surface area, the proportional bias is\nimproved from -5.0% for the signed distance transform to +0.6% for the proposed\nmethod. The Euler-Poincar\\'e characteristic is improved from unusable in the\nsigned distance transform to 98% accuracy for the proposed method. The proposed\nmethod enables an improved local and global evaluation of curvature for\npurposes of morphometry on closed, implicit surfaces.",
          "link": "http://arxiv.org/abs/2108.04354",
          "publishedOn": "2021-08-11T01:55:22.418Z",
          "wordCount": 682,
          "title": "Local Morphometry of Closed, Implicit Surfaces. (arXiv:2108.04354v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04584",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gurulingan_N/0/1/0/all/0/1\">NareshKumar Gurulingan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>",
          "description": "Scene understanding is crucial for autonomous systems which intend to operate\nin the real world. Single task vision networks extract information only based\non some aspects of the scene. In multi-task learning (MTL), on the other hand,\nthese single tasks are jointly learned, thereby providing an opportunity for\ntasks to share information and obtain a more comprehensive understanding. To\nthis end, we develop UniNet, a unified scene understanding network that\naccurately and efficiently infers vital vision tasks including object\ndetection, semantic segmentation, instance segmentation, monocular depth\nestimation, and monocular instance depth prediction. As these tasks look at\ndifferent semantic and geometric information, they can either complement or\nconflict with each other. Therefore, understanding inter-task relationships can\nprovide useful cues to enable complementary information sharing. We evaluate\nthe task relationships in UniNet through the lens of adversarial attacks based\non the notion that they can exploit learned biases and task interactions in the\nneural network. Extensive experiments on the Cityscapes dataset, using\nuntargeted and targeted attacks reveal that semantic tasks strongly interact\namongst themselves, and the same holds for geometric tasks. Additionally, we\nshow that the relationship between semantic and geometric tasks is asymmetric\nand their interaction becomes weaker as we move towards higher-level\nrepresentations.",
          "link": "http://arxiv.org/abs/2108.04584",
          "publishedOn": "2021-08-11T01:55:22.413Z",
          "wordCount": 668,
          "title": "UniNet: A Unified Scene Understanding Network and Exploring Multi-Task Relationships through the Lens of Adversarial Attacks. (arXiv:2108.04584v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.05568",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aman Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothyari_M/0/1/0/all/0/1\">Mayank Kothyari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vishwajeet Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Soumen Chakrabarti</a>",
          "description": "Multimodal IR, spanning text corpus, knowledge graph and images, called\noutside knowledge visual question answering (OKVQA), is of much recent\ninterest. However, the popular data set has serious limitations. A surprisingly\nlarge fraction of queries do not assess the ability to integrate cross-modal\ninformation. Instead, some are independent of the image, some depend on\nspeculation, some require OCR or are otherwise answerable from the image alone.\nTo add to the above limitations, frequency-based guessing is very effective\nbecause of (unintended) widespread answer overlaps between the train and test\nfolds. Overall, it is hard to determine when state-of-the-art systems exploit\nthese weaknesses rather than really infer the answers, because they are opaque\nand their 'reasoning' process is uninterpretable. An equally important\nlimitation is that the dataset is designed for the quantitative assessment only\nof the end-to-end answer retrieval task, with no provision for assessing the\ncorrect(semantic) interpretation of the input query. In response, we identify a\nkey structural idiom in OKVQA ,viz., S3 (select, substitute and search), and\nbuild a new data set and challenge around it. Specifically, the questioner\nidentifies an entity in the image and asks a question involving that entity\nwhich can be answered only by consulting a knowledge graph or corpus passage\nmentioning the entity. Our challenge consists of (i)OKVQAS3, a subset of OKVQA\nannotated based on the structural idiom and (ii)S3VQA, a new dataset built from\nscratch. We also present a neural but structurally transparent OKVQA system,\nS3, that explicitly addresses our challenge dataset, and outperforms recent\ncompetitive baselines.",
          "link": "http://arxiv.org/abs/2103.05568",
          "publishedOn": "2021-08-11T01:55:22.405Z",
          "wordCount": 749,
          "title": "Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering. (arXiv:2103.05568v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shuchang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Ting-Bing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>",
          "description": "In real applications, different computation-resource devices need\ndifferent-depth networks (e.g., ResNet-18/34/50) with high-accuracy. Usually,\nexisting methods either design multiple networks and train them independently,\nor construct depth-level/width-level dynamic neural networks which is hard to\nprove the accuracy of each sub-net. In this article, we propose an elegant\nDepth-Level Dynamic Neural Network (DDNN) integrated different-depth sub-nets\nof similar architectures. To improve the generalization of sub-nets, we design\nthe Embedded-Knowledge-Distillation (EKD) training mechanism for the DDNN to\nimplement knowledge transfer from the teacher (full-net) to multiple students\n(sub-nets). Specifically, the Kullback-Leibler (KL) divergence is introduced to\nconstrain the posterior class probability consistency between full-net and\nsub-nets, and self-attention distillation on the same resolution feature of\ndifferent depth is addressed to drive more abundant feature representations of\nsub-nets. Thus, we can obtain multiple high-accuracy sub-nets simultaneously in\na DDNN via the online knowledge distillation in each training iteration without\nextra computation cost. Extensive experiments on CIFAR-10/100, and ImageNet\ndatasets demonstrate that sub-nets in DDNN with EKD training achieve better\nperformance than individually training networks while preserving the original\nperformance of full-nets.",
          "link": "http://arxiv.org/abs/2103.00793",
          "publishedOn": "2021-08-11T01:55:22.389Z",
          "wordCount": 674,
          "title": "Embedded Knowledge Distillation in Depth-Level Dynamic Neural Network. (arXiv:2103.00793v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shimei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shangyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_M/0/1/0/all/0/1\">Miao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiaofang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chuangxue Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kunyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shuxin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yuer Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1\">Ting Zhong</a>",
          "description": "We presented an optical system to perform imaging interested objects in\ncomplex scenes, like the creature easy see the interested prey in the hunt for\ncomplex environments. It utilized Deep-learning network to learn the interested\nobjects's vision features and designed the corresponding \"imaging matrices\",\nfurthermore the learned matrixes act as the measurement matrix to complete\ncompressive imaging with a single-pixel camera, finally we can using the\ncompressed image data to only image the interested objects without the rest\nobjects and backgrounds of the scenes with the previous Deep-learning network.\nOur results demonstrate that no matter interested object is single feature or\nrich details, the interference can be successfully filtered out and this idea\ncan be applied in some common applications that effectively improve the\nperformance. This bio-inspired optical system can act as the creature eye to\nachieve success on interested-based object imaging, object detection, object\nrecognition and object tracking, etc.",
          "link": "http://arxiv.org/abs/2108.04236",
          "publishedOn": "2021-08-11T01:55:22.361Z",
          "wordCount": 607,
          "title": "An optical biomimetic eyes with interested object imaging. (arXiv:2108.04236v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1\">Chongyang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1\">Xiaoxue Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkara_S/0/1/0/all/0/1\">Srinivas Sunkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Abhinav Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jindong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arcas_B/0/1/0/all/0/1\">Blaise Aguera y Arcas</a>",
          "description": "To improve the accessibility of smart devices and to simplify their usage,\nbuilding models which understand user interfaces (UIs) and assist users to\ncomplete their tasks is critical. However, unique challenges are proposed by\nUI-specific characteristics, such as how to effectively leverage multimodal UI\nfeatures that involve image, text, and structural metadata and how to achieve\ngood performance when high-quality labeled data is unavailable. To address such\nchallenges we introduce UIBert, a transformer-based joint image-text model\ntrained through novel pre-training tasks on large-scale unlabeled UI data to\nlearn generic feature representations for a UI and its components. Our key\nintuition is that the heterogeneous features in a UI are self-aligned, i.e.,\nthe image and text features of UI components, are predictive of each other. We\npropose five pretraining tasks utilizing this self-alignment among different\nfeatures of a UI component and across various components in the same UI. We\nevaluate our method on nine real-world downstream UI tasks where UIBert\noutperforms strong multimodal baselines by up to 9.26% accuracy.",
          "link": "http://arxiv.org/abs/2107.13731",
          "publishedOn": "2021-08-11T01:55:22.350Z",
          "wordCount": 637,
          "title": "UIBert: Learning Generic Multimodal Representations for UI Understanding. (arXiv:2107.13731v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rasaee_H/0/1/0/all/0/1\">Hamza Rasaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivaz_H/0/1/0/all/0/1\">Hassan Rivaz</a>",
          "description": "Ultrasound is a non-invasive imaging modality that can be conveniently used\nto classify suspicious breast nodules and potentially detect the onset of\nbreast cancer. Recently, Convolutional Neural Networks (CNN) techniques have\nshown promising results in classifying ultrasound images of the breast into\nbenign or malignant. However, CNN inference acts as a black-box model, and as\nsuch, its decision-making is not interpretable. Therefore, increasing effort\nhas been dedicated to explaining this process, most notably through GRAD-CAM\nand other techniques that provide visual explanations into inner workings of\nCNNs. In addition to interpretation, these methods provide clinically important\ninformation, such as identifying the location for biopsy or treatment. In this\nwork, we analyze how adversarial assaults that are practically undetectable may\nbe devised to alter these importance maps dramatically. Furthermore, we will\nshow that this change in the importance maps can come with or without altering\nthe classification result, rendering them even harder to detect. As such, care\nmust be taken when using these importance maps to shed light on the inner\nworkings of deep learning. Finally, we utilize Multi-Task Learning (MTL) and\npropose a new network based on ResNet-50 to improve the classification\naccuracies. Our sensitivity and specificity is comparable to the state of the\nart results.",
          "link": "http://arxiv.org/abs/2108.04345",
          "publishedOn": "2021-08-11T01:55:22.345Z",
          "wordCount": 667,
          "title": "Explainable AI and susceptibility to adversarial attacks: a case study in classification of breast ultrasound images. (arXiv:2108.04345v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wencong_C/0/1/0/all/0/1\">CHENG Wencong</a> (1) ((1) Beijing Aviation Meteorological Institute)",
          "description": "Meteorology satellite visible light images is critical for meteorology\nsupport and forecast. However, there is no such kind of data during night time.\nTo overcome this, we propose a method based on deep learning to create\nsynthetic satellite visible light images during night. Specifically, to produce\nmore realistic products, we train a Generative Adversarial Networks (GAN) model\nto generate visible light images given the corresponding satellite infrared\nimages and numerical weather prediction(NWP) products. To better model the\nnonlinear relationship from infrared data and NWP products to visible light\nimages, we propose to use the channel-wise attention mechanics, e.g., SEBlock\nto quantitative weight the input channels. The experiments based on the ECMWF\nNWP products and FY-4A meteorology satellite visible light and infrared\nchannels date show that the proposed methods can be effective to create\nrealistic synthetic satellite visible light images during night.",
          "link": "http://arxiv.org/abs/2108.04330",
          "publishedOn": "2021-08-11T01:55:22.330Z",
          "wordCount": 586,
          "title": "Creating synthetic meteorology satellite visible light images during night based on GAN method. (arXiv:2108.04330v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1\">Ron Shmelkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1\">Tomer Friedlander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "A master face is a face image that passes face-based identity-authentication\nfor a large portion of the population. These faces can be used to impersonate,\nwith a high probability of success, any user, without having access to any\nuser-information. We optimize these faces, by using an evolutionary algorithm\nin the latent embedding space of the StyleGAN face generator. Multiple\nevolutionary strategies are compared, and we propose a novel approach that\nemploys a neural network in order to direct the search in the direction of\npromising samples, without adding fitness evaluations. The results we present\ndemonstrate that it is possible to obtain a high coverage of the LFW identities\n(over 40%) with less than 10 master faces, for three leading deep face\nrecognition systems.",
          "link": "http://arxiv.org/abs/2108.01077",
          "publishedOn": "2021-08-11T01:55:22.324Z",
          "wordCount": 607,
          "title": "Generating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution. (arXiv:2108.01077v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yunpei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1\">Tao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>",
          "description": "In recent years, research on adversarial attacks has become a hot spot.\nAlthough current literature on the transfer-based adversarial attack has\nachieved promising results for improving the transferability to unseen\nblack-box models, it still leaves a long way to go. Inspired by the idea of\nmeta-learning, this paper proposes a novel architecture called Meta Gradient\nAdversarial Attack (MGAA), which is plug-and-play and can be integrated with\nany existing gradient-based attack method for improving the cross-model\ntransferability. Specifically, we randomly sample multiple models from a model\nzoo to compose different tasks and iteratively simulate a white-box attack and\na black-box attack in each task. By narrowing the gap between the gradient\ndirections in white-box and black-box attacks, the transferability of\nadversarial examples on the black-box setting can be improved. Extensive\nexperiments on the CIFAR10 and ImageNet datasets show that our architecture\noutperforms the state-of-the-art methods for both black-box and white-box\nattack settings.",
          "link": "http://arxiv.org/abs/2108.04204",
          "publishedOn": "2021-08-11T01:55:22.318Z",
          "wordCount": 610,
          "title": "Meta Gradient Adversarial Attack. (arXiv:2108.04204v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yen_Chen_L/0/1/0/all/0/1\">Lin Yen-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1\">Pete Florence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1\">Alberto Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>",
          "description": "We present iNeRF, a framework that performs mesh-free pose estimation by\n\"inverting\" a Neural RadianceField (NeRF). NeRFs have been shown to be\nremarkably effective for the task of view synthesis - synthesizing\nphotorealistic novel views of real-world scenes or objects. In this work, we\ninvestigate whether we can apply analysis-by-synthesis via NeRF for mesh-free,\nRGB-only 6DoF pose estimation - given an image, find the translation and\nrotation of a camera relative to a 3D object or scene. Our method assumes that\nno object mesh models are available during either training or test time.\nStarting from an initial pose estimate, we use gradient descent to minimize the\nresidual between pixels rendered from a NeRF and pixels in an observed image.\nIn our experiments, we first study 1) how to sample rays during pose refinement\nfor iNeRF to collect informative gradients and 2) how different batch sizes of\nrays affect iNeRF on a synthetic dataset. We then show that for complex\nreal-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating\nthe camera poses of novel images and using these images as additional training\ndata for NeRF. Finally, we show iNeRF can perform category-level object pose\nestimation, including object instances not seen during training, with RGB\nimages by inverting a NeRF model inferred from a single view.",
          "link": "http://arxiv.org/abs/2012.05877",
          "publishedOn": "2021-08-11T01:55:22.312Z",
          "wordCount": 708,
          "title": "INeRF: Inverting Neural Radiance Fields for Pose Estimation. (arXiv:2012.05877v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11962",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hung_A/0/1/0/all/0/1\">Alex Ling Yu Hung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Galeotti_J/0/1/0/all/0/1\">John Galeotti</a>",
          "description": "Ultrasound 3D compounding is important for volumetric reconstruction, but as\nof yet there is no consensus on best practices for compounding. Ultrasound\nimages depend on probe direction and the path sound waves pass through, so when\nmultiple intersecting B-scans of the same spot from different perspectives\nyield different pixel values, there is not a single, ideal representation for\ncompounding (i.e. combining) the overlapping pixel values. Current popular\nmethods inevitably suppress or altogether leave out bright or dark regions that\nare useful, and potentially introduce new artifacts. In this work, we establish\na new algorithm to compound the overlapping pixels from different view points\nin ultrasound. We uniquely leverage Laplacian and Gaussian Pyramids to preserve\nthe maximum boundary contrast without overemphasizing noise and speckle. We\nevaluate our algorithm by comparing ours with previous algorithms, and we show\nthat our approach not only preserves both light and dark details, but also\nsomewhat suppresses artifacts, rather than amplifying them.",
          "link": "http://arxiv.org/abs/2011.11962",
          "publishedOn": "2021-08-11T01:55:22.307Z",
          "wordCount": 662,
          "title": "Good and Bad Boundaries in Ultrasound Compounding: Preserving Anatomic Boundaries While Suppressing Artifacts. (arXiv:2011.11962v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1\">Alejandro Pardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1\">Juan Le&#xf3;n Alc&#xe1;zar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Video content creation keeps growing at an incredible pace; yet, creating\nengaging stories remains challenging and requires non-trivial video editing\nexpertise. Many video editing components are astonishingly hard to automate\nprimarily due to the lack of raw video materials. This paper focuses on a new\ntask for computational video editing, namely the task of raking cut\nplausibility. Our key idea is to leverage content that has already been edited\nto learn fine-grained audiovisual patterns that trigger cuts. To do this, we\nfirst collected a data source of more than 10K videos, from which we extract\nmore than 255K cuts. We devise a model that learns to discriminate between real\nand artificial cuts via contrastive learning. We set up a new task and a set of\nbaselines to benchmark video cut generation. We observe that our proposed model\noutperforms the baselines by large margins. To demonstrate our model in\nreal-world applications, we conduct human studies in a collection of unedited\nvideos. The results show that our model does a better job at cutting than\nrandom and alternative baselines.",
          "link": "http://arxiv.org/abs/2108.04294",
          "publishedOn": "2021-08-11T01:55:22.267Z",
          "wordCount": 624,
          "title": "Learning to Cut by Watching Movies. (arXiv:2108.04294v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scheuerman_M/0/1/0/all/0/1\">Morgan Klaus Scheuerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denton_E/0/1/0/all/0/1\">Emily Denton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanna_A/0/1/0/all/0/1\">Alex Hanna</a>",
          "description": "Data is a crucial component of machine learning. The field is reliant on data\nto train, validate, and test models. With increased technical capabilities,\nmachine learning research has boomed in both academic and industry settings,\nand one major focus has been on computer vision. Computer vision is a popular\ndomain of machine learning increasingly pertinent to real-world applications,\nfrom facial recognition in policing to object detection for autonomous\nvehicles. Given computer vision's propensity to shape machine learning research\nand impact human life, we seek to understand disciplinary practices around\ndataset documentation - how data is collected, curated, annotated, and packaged\ninto datasets for computer vision researchers and practitioners to use for\nmodel tuning and development. Specifically, we examine what dataset\ndocumentation communicates about the underlying values of vision data and the\nlarger practices and goals of computer vision as a field. To conduct this\nstudy, we collected a corpus of about 500 computer vision datasets, from which\nwe sampled 114 dataset publications across different vision tasks. Through both\na structured and thematic content analysis, we document a number of values\naround accepted data practices, what makes desirable data, and the treatment of\nhumans in the dataset construction process. We discuss how computer vision\ndatasets authors value efficiency at the expense of care; universality at the\nexpense of contextuality; impartiality at the expense of positionality; and\nmodel work at the expense of data work. Many of the silenced values we identify\nsit in opposition with social computing practices. We conclude with suggestions\non how to better incorporate silenced values into the dataset creation and\ncuration process.",
          "link": "http://arxiv.org/abs/2108.04308",
          "publishedOn": "2021-08-11T01:55:22.254Z",
          "wordCount": 747,
          "title": "Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development. (arXiv:2108.04308v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Haocheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Caleb Chen Cao</a>",
          "description": "Explanation of AI, as well as fairness of algorithms' decisions and the\ntransparency of the decision model, are becoming more and more important. And\nit is crucial to design effective and human-friendly techniques when opening\nthe black-box model. Counterfactual conforms to the human way of thinking and\nprovides a human-friendly explanation, and its corresponding explanation\nalgorithm refers to a strategic alternation of a given data point so that its\nmodel output is \"counter-facted\", i.e. the prediction is reverted. In this\npaper, we adapt counterfactual explanation over fine-grained image\nclassification problem. We demonstrated an adaptive method that could give a\ncounterfactual explanation by showing the composed counterfactual feature map\nusing top-down layer searching algorithm (TDLS). We have proved that our TDLS\nalgorithm could provide more flexible counterfactual visual explanation in an\nefficient way using VGG-16 model on Caltech-UCSD Birds 200 dataset. At the end,\nwe discussed several applicable scenarios of counterfactual visual\nexplanations.",
          "link": "http://arxiv.org/abs/2108.04238",
          "publishedOn": "2021-08-11T01:55:22.232Z",
          "wordCount": 608,
          "title": "TDLS: A Top-Down Layer Searching Algorithm for Generating Counterfactual Visual Explanation. (arXiv:2108.04238v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1\">Gang Mei</a>",
          "description": "Geohazards such as landslides have caused great losses to the safety of\npeople's lives and property, which is often accompanied with surface cracks. If\nsuch surface cracks could be identified in time, it is of great significance\nfor the monitoring and early warning of geohazards. Currently, the most common\nmethod for crack identification is manual detection, which is with low\nefficiency and accuracy. In this paper, a deep transfer learning framework is\nproposed to effectively and efficiently identify slope surface cracks for the\nsake of fast monitoring and early warning of geohazards such as landslides. The\nessential idea is to employ transfer learning by training (a) the large sample\ndataset of concrete cracks and (b) the small sample dataset of soil and rock\nmasses cracks. In the proposed framework, (1) pretrained cracks identification\nmodels are constructed based on the large sample dataset of concrete cracks;\n(2) refined cracks identification models are further constructed based on the\nsmall sample dataset of soil and rock masses cracks. The proposed framework\ncould be applied to conduct UAV surveys on high-steep slopes to realize the\nmonitoring and early warning of landslides to ensure the safety of people's\nlives and property.",
          "link": "http://arxiv.org/abs/2108.04235",
          "publishedOn": "2021-08-11T01:55:22.146Z",
          "wordCount": 632,
          "title": "Deep Transfer Learning for Identifications of Slope Surface Cracks. (arXiv:2108.04235v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bi_X/0/1/0/all/0/1\">Xiaopeng Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Ran Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zheng Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>",
          "description": "This report describes Megvii-3D team's approach to-wards SimLocMatch\nChallenge @ CVPR 2021 Image Matching Workshop.",
          "link": "http://arxiv.org/abs/2108.04466",
          "publishedOn": "2021-08-11T01:55:22.125Z",
          "wordCount": 450,
          "title": "Method Towards CVPR 2021 SimLocMatch Challenge. (arXiv:2108.04466v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Otte_M/0/1/0/all/0/1\">Maximilian Otte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delfosse_Q/0/1/0/all/0/1\">Quentin Delfosse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czech_J/0/1/0/all/0/1\">Johannes Czech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>",
          "description": "Motivated by the interaction between cells, the recently introduced concept\nof Neural Cellular Automata shows promising results in a variety of tasks. So\nfar, this concept was mostly used to generate images for a single scenario. As\neach scenario requires a new model, this type of generation seems contradictory\nto the adaptability of cells in nature. To address this contradiction, we\nintroduce a concept using different initial environments as input while using a\nsingle Neural Cellular Automata to produce several outputs. Additionally, we\nintroduce GANCA, a novel algorithm that combines Neural Cellular Automata with\nGenerative Adversarial Networks, allowing for more generalization through\nadversarial training. The experiments show that a single model is capable of\nlearning several images when presented with different inputs, and that the\nadversarially trained model improves drastically on out-of-distribution data\ncompared to a supervised trained model.",
          "link": "http://arxiv.org/abs/2108.04328",
          "publishedOn": "2021-08-11T01:55:22.120Z",
          "wordCount": 582,
          "title": "Generative Adversarial Neural Cellular Automata. (arXiv:2108.04328v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04740",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rainbow_B/0/1/0/all/0/1\">Ben A. Rainbow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_Q/0/1/0/all/0/1\">Qianhui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Hubert P. H. Shum</a>",
          "description": "Predicting the movement trajectories of multiple classes of road users in\nreal-world scenarios is a challenging task due to the diverse trajectory\npatterns. While recent works of pedestrian trajectory prediction successfully\nmodelled the influence of surrounding neighbours based on the relative\ndistances, they are ineffective on multi-class trajectory prediction. This is\nbecause they ignore the impact of the implicit correlations between different\ntypes of road users on the trajectory to be predicted - for example, a nearby\npedestrian has a different level of influence from a nearby car. In this paper,\nwe propose to introduce class information into a graph convolutional neural\nnetwork to better predict the trajectory of an individual. We embed the class\nlabels of the surrounding objects into the label adjacency matrix (LAM), which\nis combined with the velocity-based adjacency matrix (VAM) comprised of the\nobjects' velocity, thereby generating a semantics-guided graph adjacency (SAM).\nSAM effectively models semantic information with trainable parameters to\nautomatically learn the embedded label features that will contribute to the\nfixed velocity-based trajectory. Such information of spatial and temporal\ndependencies is passed to a graph convolutional and temporal convolutional\nnetwork to estimate the predicted trajectory distributions. We further propose\nnew metrics, known as Average2 Displacement Error (aADE) and Average Final\nDisplacement Error (aFDE), that assess network accuracy more accurately. We\ncall our framework Semantics-STGCNN. It consistently shows superior performance\nto the state-of-the-arts in existing and the newly proposed metrics.",
          "link": "http://arxiv.org/abs/2108.04740",
          "publishedOn": "2021-08-11T01:55:22.084Z",
          "wordCount": 681,
          "title": "Semantics-STGCNN: A Semantics-guided Spatial-Temporal Graph Convolutional Network for Multi-class Trajectory Prediction. (arXiv:2108.04740v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hongwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_W/0/1/0/all/0/1\">Weili Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1\">Weizhi Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Meng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>",
          "description": "Person reidentification (ReID) is a very hot research topic in machine\nlearning and computer vision, and many person ReID approaches have been\nproposed; however, most of these methods assume that the same person has the\nsame clothes within a short time interval, and thus their visual appearance\nmust be similar. However, in an actual surveillance environment, a given person\nhas a great probability of changing clothes after a long time span, and they\nalso often take different personal belongings with them. When the existing\nperson ReID methods are applied in this type of case, almost all of them fail.\nTo date, only a few works have focused on the cloth-changing person ReID task,\nbut since it is very difficult to extract generalized and robust features for\nrepresenting people with different clothes, their performances need to be\nimproved. Moreover, visual-semantic information is often ignored. To solve\nthese issues, in this work, a novel multigranular visual-semantic embedding\nalgorithm (MVSE) is proposed for cloth-changing person ReID, where visual\nsemantic information and human attributes are embedded into the network, and\nthe generalized features of human appearance can be well learned to effectively\nsolve the problem of clothing changes. Specifically, to fully represent a\nperson with clothing changes, a multigranular feature representation scheme\n(MGR) is employed to focus on the unchanged part of the human, and then a cloth\ndesensitization network (CDN) is designed to improve the feature robustness of\nthe approach for the person with different clothing, where different high-level\nhuman attributes are fully utilized. Moreover, to further solve the issue of\npose changes and occlusion under different camera perspectives, a partially\nsemantically aligned network (PSA) is proposed to obtain the visual-semantic\ninformation that is used to align the human attributes.",
          "link": "http://arxiv.org/abs/2108.04527",
          "publishedOn": "2021-08-11T01:55:22.079Z",
          "wordCount": 724,
          "title": "Multigranular Visual-Semantic Embedding for Cloth-Changing Person Re-identification. (arXiv:2108.04527v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhaoyang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1\">Hongyang Chao</a>",
          "description": "The defect detection task can be regarded as a realistic scenario of object\ndetection in the computer vision field and it is widely used in the industrial\nfield. Directly applying vanilla object detector to defect detection task can\nachieve promising results, while there still exists challenging issues that\nhave not been solved. The first issue is the texture shift which means a\ntrained defect detector model will be easily affected by unseen texture, and\nthe second issue is partial visual confusion which indicates that a partial\ndefect box is visually similar with a complete box. To tackle these two\nproblems, we propose a Reference-based Defect Detection Network (RDDN).\nSpecifically, we introduce template reference and context reference to against\nthose two problems, respectively. Template reference can reduce the texture\nshift from image, feature or region levels, and encourage the detectors to\nfocus more on the defective area as a result. We can use either well-aligned\ntemplate images or the outputs of a pseudo template generator as template\nreferences in this work, and they are jointly trained with detectors by the\nsupervision of normal samples. To solve the partial visual confusion issue, we\npropose to leverage the carried context information of context reference, which\nis the concentric bigger box of each region proposal, to perform more accurate\nregion classification and regression. Experiments on two defect detection\ndatasets demonstrate the effectiveness of our proposed approach.",
          "link": "http://arxiv.org/abs/2108.04456",
          "publishedOn": "2021-08-11T01:55:22.073Z",
          "wordCount": 674,
          "title": "Reference-based Defect Detection Network. (arXiv:2108.04456v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saeed_S/0/1/0/all/0/1\">Shaheer U. Saeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yunguan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stavrinides_V/0/1/0/all/0/1\">Vasilis Stavrinides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baum_Z/0/1/0/all/0/1\">Zachary M. C. Baum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qianye Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusu_M/0/1/0/all/0/1\">Mirabela Rusu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Richard E. Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonn_G/0/1/0/all/0/1\">Geoffrey A. Sonn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1\">J. Alison Noble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barratt_D/0/1/0/all/0/1\">Dean C. Barratt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>",
          "description": "The performance of many medical image analysis tasks are strongly associated\nwith image data quality. When developing modern deep learning algorithms,\nrather than relying on subjective (human-based) image quality assessment (IQA),\ntask amenability potentially provides an objective measure of task-specific\nimage quality. To predict task amenability, an IQA agent is trained using\nreinforcement learning (RL) with a simultaneously optimised task predictor,\nsuch as a classification or segmentation neural network. In this work, we\ndevelop transfer learning or adaptation strategies to increase the adaptability\nof both the IQA agent and the task predictor so that they are less dependent on\nhigh-quality, expert-labelled training data. The proposed transfer learning\nstrategy re-formulates the original RL problem for task amenability in a\nmeta-reinforcement learning (meta-RL) framework. The resulting algorithm\nfacilitates efficient adaptation of the agent to different definitions of image\nquality, each with its own Markov decision process environment including\ndifferent images, labels and an adaptable task predictor. Our work demonstrates\nthat the IQA agents pre-trained on non-expert task labels can be adapted to\npredict task amenability as defined by expert task labels, using only a small\nset of expert labels. Using 6644 clinical ultrasound images from 249 prostate\ncancer patients, our results for image classification and segmentation tasks\nshow that the proposed IQA method can be adapted using data with as few as\nrespective 19.7% and 29.6% expert-reviewed consensus labels and still achieve\ncomparable IQA and task performance, which would otherwise require a training\ndataset with 100% expert labels.",
          "link": "http://arxiv.org/abs/2108.04359",
          "publishedOn": "2021-08-11T01:55:22.067Z",
          "wordCount": 727,
          "title": "Adaptable image quality assessment using meta-reinforcement learning of task amenability. (arXiv:2108.04359v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Abraham Noah Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1\">Filip Biljecki</a>",
          "description": "We present a new method to create spatial data using a generative adversarial\nnetwork (GAN). Our contribution uses coarse and widely available geospatial\ndata to create maps of less available features at the finer scale in the built\nenvironment, bypassing their traditional acquisition techniques (e.g. satellite\nimagery or land surveying). In the work, we employ land use data and road\nnetworks as input to generate building footprints, and conduct experiments in 9\ncities around the world. The method, which we implement in a tool we release\nopenly, enables generating approximate maps of the urban form, and it is\ngeneralisable to augment other types of geoinformation, enhancing the\ncompleteness and quality of spatial data infrastructure. It may be especially\nuseful in locations missing detailed and high-resolution data and those that\nare mapped with uncertain or heterogeneous quality, such as much of\nOpenStreetMap. The quality of the results is influenced by the urban form and\nscale. In most cases, experiments suggest promising performance as the method\ntends to truthfully indicate the locations, amount, and shape of buildings. The\nwork has the potential to support several applications, such as energy,\nclimate, and urban morphology studies in areas previously lacking required\ndata.",
          "link": "http://arxiv.org/abs/2108.04232",
          "publishedOn": "2021-08-11T01:55:22.034Z",
          "wordCount": 628,
          "title": "GANmapper: geographical content filling. (arXiv:2108.04232v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1\">Wenshan Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">Dong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kou_K/0/1/0/all/0/1\">Kit Ian Kou</a>",
          "description": "In this article, a robust color-edge feature extraction method based on the\nQuaternion Hardy filter is proposed. The Quaternion Hardy filter is an emerging\nedge detection theory. It is along with the Poisson and conjugate Poisson\nsmoothing kernels to handle various types of noise. Combining with the\nQuaternion Hardy filter, Jin's color gradient operator and Hough transform, the\ncolor-edge feature detection algorithm is proposed and applied to the lane\nmarking detection. Experiments are presented to demonstrate the validity of the\nproposed algorithm. The results are accurate and robust with respect to the\ncomplex environment lane markings.",
          "link": "http://arxiv.org/abs/2108.04356",
          "publishedOn": "2021-08-11T01:55:22.029Z",
          "wordCount": 532,
          "title": "A Robust Lane Detection Associated with Quaternion Hardy Filter. (arXiv:2108.04356v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdary_G/0/1/0/all/0/1\">G Jignesh Chowdary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_S/0/1/0/all/0/1\">Suganya G</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M_P/0/1/0/all/0/1\">Premalatha M</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1\">Karunamurthy K</a>",
          "description": "Tuberculosis is an infectious disease that is leading to the death of\nmillions of people across the world. The mortality rate of this disease is high\nin patients suffering from immuno-compromised disorders. The early diagnosis of\nthis disease can save lives and can avoid further complications. But the\ndiagnosis of TB is a very complex task. The standard diagnostic tests still\nrely on traditional procedures developed in the last century. These procedures\nare slow and expensive. So this paper presents an automatic approach for the\ndiagnosis of TB from posteroanterior chest x-rays. This is a two-step approach,\nwhere in the first step the lung regions are segmented from the chest x-rays\nusing the graph cut method, and then in the second step the transfer learning\nof VGG16 combined with Bi-directional LSTM is used for extracting high-level\ndiscriminative features from the segmented lung regions and then classification\nis performed using a fully connected layer. The proposed model is evaluated\nusing data from two publicly available databases namely Montgomery Country set\nand Schezien set. The proposed model achieved accuracy and sensitivity of\n97.76%, 97.01% and 96.42%, 94.11% on Schezien and Montgomery county datasets.\nThis model enhanced the diagnostic accuracy of TB by 0.7% and 11.68% on\nSchezien and Montgomery county datasets.",
          "link": "http://arxiv.org/abs/2108.04329",
          "publishedOn": "2021-08-11T01:55:22.022Z",
          "wordCount": 672,
          "title": "Class dependency based learning using Bi-LSTM coupled with the transfer learning of VGG16 for the diagnosis of Tuberculosis from chest x-rays. (arXiv:2108.04329v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04441",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1\">Kibeom Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1\">Seogkyu Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1\">Hyeran Byun</a>",
          "description": "Style transfer aims to reproduce content images with the styles from\nreference images. Existing universal style transfer methods successfully\ndeliver arbitrary styles to original images either in an artistic or a\nphoto-realistic way. However, the range of 'arbitrary style' defined by\nexisting works is bounded in the particular domain due to their structural\nlimitation. Specifically, the degrees of content preservation and stylization\nare established according to a predefined target domain. As a result, both\nphoto-realistic and artistic models have difficulty in performing the desired\nstyle transfer for the other domain. To overcome this limitation, we propose a\nunified architecture, Domain-aware Style Transfer Networks (DSTN) that transfer\nnot only the style but also the property of domain (i.e., domainness) from a\ngiven reference image. To this end, we design a novel domainness indicator that\ncaptures the domainness value from the texture and structural features of\nreference images. Moreover, we introduce a unified framework with domain-aware\nskip connection to adaptively transfer the stroke and palette to the input\ncontents guided by the domainness indicator. Our extensive experiments validate\nthat our model produces better qualitative results and outperforms previous\nmethods in terms of proxy metrics on both artistic and photo-realistic\nstylizations.",
          "link": "http://arxiv.org/abs/2108.04441",
          "publishedOn": "2021-08-11T01:55:21.935Z",
          "wordCount": 633,
          "title": "Domain-Aware Universal Style Transfer. (arXiv:2108.04441v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04476",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruihui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1\">Ka-Hei Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>",
          "description": "We present SP-GAN, a new unsupervised sphere-guided generative model for\ndirect synthesis of 3D shapes in the form of point clouds. Compared with\nexisting models, SP-GAN is able to synthesize diverse and high-quality shapes\nwith fine details and promote controllability for part-aware shape generation\nand manipulation, yet trainable without any parts annotations. In SP-GAN, we\nincorporate a global prior (uniform points on a sphere) to spatially guide the\ngenerative process and attach a local prior (a random latent code) to each\nsphere point to provide local details. The key insight in our design is to\ndisentangle the complex 3D shape generation task into a global shape modeling\nand a local structure adjustment, to ease the learning process and enhance the\nshape generation quality. Also, our model forms an implicit dense\ncorrespondence between the sphere points and points in every generated shape,\nenabling various forms of structure-aware shape manipulations such as part\nediting, part-wise shape interpolation, and multi-shape part composition, etc.,\nbeyond the existing generative models. Experimental results, which include both\nvisual and quantitative evaluations, demonstrate that our model is able to\nsynthesize diverse point clouds with fine details and less noise, as compared\nwith the state-of-the-art models.",
          "link": "http://arxiv.org/abs/2108.04476",
          "publishedOn": "2021-08-11T01:55:21.930Z",
          "wordCount": 653,
          "title": "SP-GAN: Sphere-Guided 3D Shape Generation and Manipulation. (arXiv:2108.04476v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tatsunami_Y/0/1/0/all/0/1\">Yuki Tatsunami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taki_M/0/1/0/all/0/1\">Masato Taki</a>",
          "description": "For the past ten years, CNN has reigned supreme in the world of computer\nvision, but recently, Transformer is on the rise. However, the quadratic\ncomputational cost of self-attention has become a severe problem of practice.\nThere has been much research on architectures without CNN and self-attention in\nthis context. In particular, MLP-Mixer is a simple idea designed using MLPs and\nhit an accuracy comparable to the Vision Transformer. However, the only\ninductive bias in this architecture is the embedding of tokens. Thus, there is\nstill a possibility to build a non-convolutional inductive bias into the\narchitecture itself, and we built in an inductive bias using two simple ideas.\nA way is to divide the token-mixing block vertically and horizontally. Another\nway is to make spatial correlations denser among some channels of token-mixing.\nWith this approach, we were able to improve the accuracy of the MLP-Mixer while\nreducing its parameters and computational complexity. Compared to other\nMLP-based models, the proposed model, named RaftMLP has a good balance of\ncomputational complexity, the number of parameters, and actual memory usage. In\naddition, our work indicates that MLP-based models have the potential to\nreplace CNNs by adopting inductive bias. The source code in PyTorch version is\navailable at \\url{https://github.com/okojoalg/raft-mlp}.",
          "link": "http://arxiv.org/abs/2108.04384",
          "publishedOn": "2021-08-11T01:55:21.919Z",
          "wordCount": 651,
          "title": "RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?. (arXiv:2108.04384v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ralekar_C/0/1/0/all/0/1\">Chetan Ralekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1\">Shubham Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_T/0/1/0/all/0/1\">Tapan Kumar Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhury_S/0/1/0/all/0/1\">Santanu Chaudhury</a>",
          "description": "Human observers engage in selective information uptake when classifying\nvisual patterns. The same is true of deep neural networks, which currently\nconstitute the best performing artificial vision systems. Our goal is to\nexamine the congruence, or lack thereof, in the information-gathering\nstrategies of the two systems. We have operationalized our investigation as a\ncharacter recognition task. We have used eye-tracking to assay the spatial\ndistribution of information hotspots for humans via fixation maps and an\nactivation mapping technique for obtaining analogous distributions for deep\nnetworks through visualization maps. Qualitative comparison between\nvisualization maps and fixation maps reveals an interesting correlate of\ncongruence. The deep learning model considered similar regions in character,\nwhich humans have fixated in the case of correctly classified characters. On\nthe other hand, when the focused regions are different for humans and deep\nnets, the characters are typically misclassified by the latter. Hence, we\npropose to use the visual fixation maps obtained from the eye-tracking\nexperiment as a supervisory input to align the model's focus on relevant\ncharacter regions. We find that such supervision improves the model's\nperformance significantly and does not require any additional parameters. This\napproach has the potential to find applications in diverse domains such as\nmedical analysis and surveillance in which explainability helps to determine\nsystem fidelity.",
          "link": "http://arxiv.org/abs/2108.04558",
          "publishedOn": "2021-08-11T01:55:21.913Z",
          "wordCount": 664,
          "title": "Understanding Character Recognition using Visual Explanations Derived from the Human Visual System and Deep Networks. (arXiv:2108.04558v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiaoyang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Huilin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wancheng Ge</a>",
          "description": "Deep Neural Networks (DNNs) are vulnerable to adversarial examples which\nwould inveigle neural networks to make prediction errors with small per-\nturbations on the input images. Researchers have been devoted to promoting the\nresearch on the universal adversarial perturbations (UAPs) which are\ngradient-free and have little prior knowledge on data distributions. Procedural\nadversarial noise at- tack is a data-free universal perturbation generation\nmethod. In this paper, we propose two universal adversarial perturbation (UAP)\ngeneration methods based on procedural noise functions: Simplex noise and\nWorley noise. In our framework, the shading which disturbs visual\nclassification is generated with rendering technology. Without changing the\nsemantic representations, the adversarial examples generated via our methods\nshow superior performance on the attack.",
          "link": "http://arxiv.org/abs/2108.04409",
          "publishedOn": "2021-08-11T01:55:21.899Z",
          "wordCount": 560,
          "title": "On Procedural Adversarial Noise Attack And Defense. (arXiv:2108.04409v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoxiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>",
          "description": "Blind face inpainting refers to the task of reconstructing visual contents\nwithout explicitly indicating the corrupted regions in a face image.\nInherently, this task faces two challenges: (1) how to detect various mask\npatterns of different shapes and contents; (2) how to restore visually\nplausible and pleasing contents in the masked regions. In this paper, we\npropose a novel two-stage blind face inpainting method named Frequency-guided\nTransformer and Top-Down Refinement Network (FT-TDR) to tackle these\nchallenges. Specifically, we first use a transformer-based network to detect\nthe corrupted regions to be inpainted as masks by modeling the relation among\ndifferent patches. We also exploit the frequency modality as complementary\ninformation for improved detection results and capture the local contextual\nincoherence to enhance boundary consistency. Then a top-down refinement network\nis proposed to hierarchically restore features at different levels and generate\ncontents that are semantically consistent with the unmasked face regions.\nExtensive experiments demonstrate that our method outperforms current\nstate-of-the-art blind and non-blind face inpainting methods qualitatively and\nquantitatively.",
          "link": "http://arxiv.org/abs/2108.04424",
          "publishedOn": "2021-08-11T01:55:21.867Z",
          "wordCount": 608,
          "title": "FT-TDR: Frequency-guided Transformer and Top-Down Refinement Network for Blind Face Inpainting. (arXiv:2108.04424v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Minhao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaocheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>",
          "description": "Differentiable Neural Architecture Search is one of the most popular Neural\nArchitecture Search (NAS) methods for its search efficiency and simplicity,\naccomplished by jointly optimizing the model weight and architecture parameters\nin a weight-sharing supernet via gradient-based algorithms. At the end of the\nsearch phase, the operations with the largest architecture parameters will be\nselected to form the final architecture, with the implicit assumption that the\nvalues of architecture parameters reflect the operation strength. While much\nhas been discussed about the supernet's optimization, the architecture\nselection process has received little attention. We provide empirical and\ntheoretical analysis to show that the magnitude of architecture parameters does\nnot necessarily indicate how much the operation contributes to the supernet's\nperformance. We propose an alternative perturbation-based architecture\nselection that directly measures each operation's influence on the supernet. We\nre-evaluate several differentiable NAS methods with the proposed architecture\nselection and find that it is able to extract significantly improved\narchitectures from the underlying supernets consistently. Furthermore, we find\nthat several failure modes of DARTS can be greatly alleviated with the proposed\nselection method, indicating that much of the poor generalization observed in\nDARTS can be attributed to the failure of magnitude-based architecture\nselection rather than entirely the optimization of its supernet.",
          "link": "http://arxiv.org/abs/2108.04392",
          "publishedOn": "2021-08-11T01:55:21.840Z",
          "wordCount": 656,
          "title": "Rethinking Architecture Selection in Differentiable NAS. (arXiv:2108.04392v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1\">Ayaan Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutradhar_I/0/1/0/all/0/1\">Ipsita Sutradhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mahziba Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mehedi Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1\">Malabika Sarker</a>",
          "description": "Diabetes is one of the most prevalent chronic diseases in Bangladesh, and as\na result, Diabetic Retinopathy (DR) is widespread in the population. DR, an eye\nillness caused by diabetes, can lead to blindness if it is not identified and\ntreated in its early stages. Unfortunately, diagnosis of DR requires medically\ntrained professionals, but Bangladesh has limited specialists in comparison to\nits population. Moreover, the screening process is often expensive, prohibiting\nmany from receiving timely and proper diagnosis. To address the problem, we\nintroduce a deep learning algorithm which screens for different stages of DR.\nWe use a state-of-the-art CNN architecture to diagnose patients based on\nretinal fundus imagery. This paper is an experimental evaluation of the\nalgorithm we developed for DR diagnosis and screening specifically for\nBangladeshi patients. We perform this validation study using separate pools of\nretinal image data of real patients from a hospital and field studies in\nBangladesh. Our results show that the algorithm is effective at screening\nBangladeshi eyes even when trained on a public dataset which is out of domain,\nand can accurately determine the stage of DR as well, achieving an overall\naccuracy of 92.27\\% and 93.02\\% on two validation sets of Bangladeshi eyes. The\nresults confirm the ability of the algorithm to be used in real clinical\nsettings and applications due to its high accuracy and classwise metrics. Our\nalgorithm is implemented in the application Drishti, which is used to screen\nfor DR in patients living in rural areas in Bangladesh, where access to\nprofessional screening is limited.",
          "link": "http://arxiv.org/abs/2108.04358",
          "publishedOn": "2021-08-11T01:55:21.827Z",
          "wordCount": 706,
          "title": "Convolutional Nets for Diabetic Retinopathy Screening in Bangladeshi Patients. (arXiv:2108.04358v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miyauchi_R/0/1/0/all/0/1\">Ryoma Miyauchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukusato_T/0/1/0/all/0/1\">Tsukasa Fukusato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haoran Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyata_K/0/1/0/all/0/1\">Kazunori Miyata</a>",
          "description": "Constructing stroke correspondences between keyframes is one of the most\nimportant processes in the production pipeline of hand-drawn inbetweening\nframes. This process requires time-consuming manual work imposing a tremendous\nburden on the animators. We propose a method to estimate stroke correspondences\nbetween raster character images (keyframes) without vectorization processes.\nFirst, the proposed system separates the closed areas in each keyframe and\nestimates the correspondences between closed areas by using the characteristics\nof shape, depth, and closed area connection. Second, the proposed system\nestimates stroke correspondences from the estimated closed area\ncorrespondences. We demonstrate the effectiveness of our method by performing a\nuser study and comparing the proposed system with conventional approaches.",
          "link": "http://arxiv.org/abs/2108.04393",
          "publishedOn": "2021-08-11T01:55:21.785Z",
          "wordCount": 551,
          "title": "Stroke Correspondence by Labeling Closed Areas. (arXiv:2108.04393v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12023",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koot_R/0/1/0/all/0/1\">Raivo Koot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1\">Tao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haiping Lu</a>",
          "description": "This report describes the technical details of our submission to the\nEPIC-Kitchens 2021 Unsupervised Domain Adaptation Challenge for Action\nRecognition. The EPIC-Kitchens dataset is more difficult than other video\ndomain adaptation datasets due to multi-tasks with more modalities. Firstly, to\nparticipate in the challenge, we employ a transformer to capture the spatial\ninformation from each modality. Secondly, we employ a temporal attention module\nto model temporal-wise inter-dependency. Thirdly, we employ the adversarial\ndomain adaptation network to learn the general features between labeled source\nand unlabeled target domain. Finally, we incorporate multiple modalities to\nimprove the performance by a three-stream network with late fusion. Our network\nachieves the comparable performance with the state-of-the-art baseline T$A^3$N\nand outperforms the baseline on top-1 accuracy for verb class and top-5\naccuracies for all three tasks which are verb, noun and action. Under the team\nname xy9, our submission achieved 5th place in terms of top-1 accuracy for verb\nclass and all top-5 accuracies.",
          "link": "http://arxiv.org/abs/2106.12023",
          "publishedOn": "2021-08-10T02:00:11.789Z",
          "wordCount": 657,
          "title": "Team PyKale (xy9) Submission to the EPIC-Kitchens 2021 Unsupervised Domain Adaptation Challenge for Action Recognition. (arXiv:2106.12023v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>",
          "description": "This paper introduces a two-phase deep feature calibration framework for\nefficient learning of semantics enhanced text-image cross-modal joint\nembedding, which clearly separates the deep feature calibration in data\npreprocessing from training the joint embedding model. We use the Recipe1M\ndataset for the technical description and empirical validation. In\npreprocessing, we perform deep feature calibration by combining deep feature\nengineering with semantic context features derived from raw text-image input\ndata. We leverage LSTM to identify key terms, NLP methods to produce ranking\nscores for key terms before generating the key term feature. We leverage\nwideResNet50 to extract and encode the image category semantics to help\nsemantic alignment of the learned recipe and image embeddings in the joint\nlatent space. In joint embedding learning, we perform deep feature calibration\nby optimizing the batch-hard triplet loss function with soft-margin and double\nnegative sampling, also utilizing the category-based alignment loss and\ndiscriminator-based alignment loss. Extensive experiments demonstrate that our\nSEJE approach with the deep feature calibration significantly outperforms the\nstate-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.00705",
          "publishedOn": "2021-08-10T02:00:11.652Z",
          "wordCount": 634,
          "title": "Efficient Deep Feature Calibration for Cross-Modal Joint Embedding Learning. (arXiv:2108.00705v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Assine_J/0/1/0/all/0/1\">Juliano S. Assine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filho_J/0/1/0/all/0/1\">J. C. S. Santos Filho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valle_E/0/1/0/all/0/1\">Eduardo Valle</a>",
          "description": "In the past few years, mobile deep-learning deployment progressed by leaps\nand bounds, but solutions still struggle to accommodate its severe and\nfluctuating operational restrictions, which include bandwidth, latency,\ncomputation, and energy. In this work, we help to bridge that gap, introducing\nthe first configurable solution for object detection that manages the triple\ncommunication-computation-accuracy trade-off with a single set of weights. Our\nsolution shows state-of-the-art results on COCO-2017, adding only a minor\npenalty on the base EfficientDet-D2 architecture. Our design is robust to the\nchoice of base architecture and compressor and should adapt well for future\narchitectures.",
          "link": "http://arxiv.org/abs/2105.00591",
          "publishedOn": "2021-08-10T02:00:11.645Z",
          "wordCount": 568,
          "title": "Single-Training Collaborative Object Detectors Adaptive to Bandwidth and Computation. (arXiv:2105.00591v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngjoo Kim</a>",
          "description": "This paper proposes a novel approach to map-based navigation system for\nunmanned aircraft. The proposed system attempts label-to-label matching, not\nimage-to-image matching, between aerial images and a map database. By using\nsemantic segmentation, the ground objects are labelled and the configuration of\nthe objects is used to find the corresponding location in the map database. The\nuse of the deep learning technique as a tool for extracting high-level features\nreduces the image-based localization problem to a pattern matching problem.\nThis paper proposes a pattern matching algorithm which does not require\naltitude information or a camera model to estimate the absolute horizontal\nposition. The feasibility analysis with simulated images shows the proposed\nmap-based navigation can be realized with the proposed pattern matching\nalgorithm and it is able to provide positions given the labelled objects.",
          "link": "http://arxiv.org/abs/2107.00689",
          "publishedOn": "2021-08-10T02:00:11.466Z",
          "wordCount": null,
          "title": "Aerial Map-Based Navigation Using Semantic Segmentation and Pattern Matching. (arXiv:2107.00689v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04881",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shoeibi_A/0/1/0/all/0/1\">Afshin Shoeibi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khodatars_M/0/1/0/all/0/1\">Marjane Khodatars</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jafari_M/0/1/0/all/0/1\">Mahboobeh Jafari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moridian_P/0/1/0/all/0/1\">Parisa Moridian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rezaei_M/0/1/0/all/0/1\">Mitra Rezaei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khozeimeh_F/0/1/0/all/0/1\">Fahime Khozeimeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gorriz_J/0/1/0/all/0/1\">Juan Manuel Gorriz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heras_J/0/1/0/all/0/1\">J&#xf3;nathan Heras</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Panahiazar_M/0/1/0/all/0/1\">Maryam Panahiazar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Acharya_U/0/1/0/all/0/1\">U. Rajendra Acharya</a>",
          "description": "Multiple Sclerosis (MS) is a type of brain disease which causes visual,\nsensory, and motor problems for people with a detrimental effect on the\nfunctioning of the nervous system. In order to diagnose MS, multiple screening\nmethods have been proposed so far; among them, magnetic resonance imaging (MRI)\nhas received considerable attention among physicians. MRI modalities provide\nphysicians with fundamental information about the structure and function of the\nbrain, which is crucial for the rapid diagnosis of MS lesions. Diagnosing MS\nusing MRI is time-consuming, tedious, and prone to manual errors. Hence,\ncomputer aided diagnosis systems (CADS) based on artificial intelligence (AI)\nmethods have been proposed in recent years for accurate diagnosis of MS using\nMRI neuroimaging modalities. In the AI field, automated MS diagnosis is being\nconducted using (i) conventional machine learning and (ii) deep learning (DL)\ntechniques. The conventional machine learning approach is based on feature\nextraction and selection by trial and error. In DL, these steps are performed\nby the DL model itself. In this paper, a complete review of automated MS\ndiagnosis methods performed using DL techniques with MRI neuroimaging\nmodalities are discussed. Also, each work is thoroughly reviewed and discussed.\nFinally, the most important challenges and future directions in the automated\nMS diagnosis using DL techniques coupled with MRI modalities are presented in\ndetail.",
          "link": "http://arxiv.org/abs/2105.04881",
          "publishedOn": "2021-08-10T02:00:11.465Z",
          "wordCount": null,
          "title": "Applications of Deep Learning Techniques for Automated Multiple Sclerosis Detection Using Magnetic Resonance Imaging: A Review. (arXiv:2105.04881v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tsung-Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueh-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Ping-Chia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>",
          "description": "Despite the success of deep learning on supervised point cloud semantic\nsegmentation, obtaining large-scale point-by-point manual annotations is still\na significant challenge. To reduce the huge annotation burden, we propose a\nRegion-based and Diversity-aware Active Learning (ReDAL), a general framework\nfor many deep learning approaches, aiming to automatically select only\ninformative and diverse sub-scene regions for label acquisition. Observing that\nonly a small portion of annotated regions are sufficient for 3D scene\nunderstanding with deep learning, we use softmax entropy, color discontinuity,\nand structural complexity to measure the information of sub-scene regions. A\ndiversity-aware selection algorithm is also developed to avoid redundant\nannotations resulting from selecting informative but similar regions in a\nquerying batch. Extensive experiments show that our method highly outperforms\nprevious active learning strategies, and we achieve the performance of 90%\nfully supervised learning, while less than 15% and 5% annotations are required\non S3DIS and SemanticKITTI datasets, respectively.",
          "link": "http://arxiv.org/abs/2107.11769",
          "publishedOn": "2021-08-10T02:00:11.465Z",
          "wordCount": null,
          "title": "ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation. (arXiv:2107.11769v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12666",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zefeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhiyin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Text-to-image person re-identification (ReID) aims to search for images\ncontaining a person of interest using textual descriptions. However, due to the\nsignificant modality gap and the large intra-class variance in textual\ndescriptions, text-to-image ReID remains a challenging problem. Accordingly, in\nthis paper, we propose a Semantically Self-Aligned Network (SSAN) to handle the\nabove problems. First, we propose a novel method that automatically extracts\nsemantically aligned part-level features from the two modalities. Second, we\ndesign a multi-view non-local network that captures the relationships between\nbody parts, thereby establishing better correspondences between body parts and\nnoun phrases. Third, we introduce a Compound Ranking (CR) loss that makes use\nof textual descriptions for other images of the same identity to provide extra\nsupervision, thereby effectively reducing the intra-class variance in textual\nfeatures. Finally, to expedite future research in text-to-image ReID, we build\na new database named ICFG-PEDES. Extensive experiments demonstrate that SSAN\noutperforms state-of-the-art approaches by significant margins. Both the new\nICFG-PEDES database and the SSAN code are available at\nhttps://github.com/zifyloo/SSAN.",
          "link": "http://arxiv.org/abs/2107.12666",
          "publishedOn": "2021-08-10T02:00:11.462Z",
          "wordCount": null,
          "title": "Semantically Self-Aligned Network for Text-to-Image Part-aware Person Re-identification. (arXiv:2107.12666v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zixin Zhu</a> (Xi&#x27;an jiaotong University), <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wei Tang</a> (University of Illinois at Chicago), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a> (Xi&#x27;an Jiaotong University), <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a> (Xi&#x27;an Jiaotong University), <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a> (Wormpex AI Research)",
          "description": "Effectively tackling the problem of temporal action localization (TAL)\nnecessitates a visual representation that jointly pursues two confounding\ngoals, i.e., fine-grained discrimination for temporal localization and\nsufficient visual invariance for action classification. We address this\nchallenge by enriching both the local and global contexts in the popular\ntwo-stage temporal localization framework, where action proposals are first\ngenerated followed by action classification and temporal boundary regression.\nOur proposed model, dubbed ContextLoc, can be divided into three sub-networks:\nL-Net, G-Net and P-Net. L-Net enriches the local context via fine-grained\nmodeling of snippet-level features, which is formulated as a\nquery-and-retrieval process. G-Net enriches the global context via higher-level\nmodeling of the video-level representation. In addition, we introduce a novel\ncontext adaptation module to adapt the global context to different proposals.\nP-Net further models the context-aware inter-proposal relations. We explore two\nexisting models to be the P-Net in our experiments. The efficacy of our\nproposed method is validated by experimental results on the THUMOS14 (54.3\\% at\ntIoU@0.5) and ActivityNet v1.3 (56.01\\% at tIoU@0.5) datasets, which\noutperforms recent states of the art. Code is available at\nhttps://github.com/buxiangzhiren/ContextLoc.",
          "link": "http://arxiv.org/abs/2107.12960",
          "publishedOn": "2021-08-10T02:00:11.461Z",
          "wordCount": null,
          "title": "Enriching Local and Global Contexts for Temporal Action Localization. (arXiv:2107.12960v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xunlin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yangxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Minlong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>",
          "description": "Nowadays, customer's demands for E-commerce are more diversified, which\nintroduces more complications to the product retrieval industry. Previous\nmethods are either subject to single-modal input or perform supervised\nimage-level product retrieval, thus fail to accommodate real-life scenarios\nwhere enormous weakly annotated multi-modal data are present. In this paper, we\ninvestigate a more realistic setting that aims to perform weakly-supervised\nmulti-modal instance-level product retrieval among fine-grained product\ncategories. To promote the study of this challenging task, we contribute\nProduct1M, one of the largest multi-modal cosmetic datasets for real-world\ninstance-level retrieval. Notably, Product1M contains over 1 million\nimage-caption pairs and consists of two sample types, i.e., single-product and\nmulti-product samples, which encompass a wide variety of cosmetics brands. In\naddition to the great diversity, Product1M enjoys several appealing\ncharacteristics including fine-grained categories, complex combinations, and\nfuzzy correspondence that well mimic the real-world scenes. Moreover, we\npropose a novel model named Cross-modal contrAstive Product Transformer for\ninstance-level prodUct REtrieval (CAPTURE), that excels in capturing the\npotential synergy between multi-modal inputs via a hybrid-stream transformer in\na self-supervised manner.CAPTURE generates discriminative instance features via\nmasked multi-modal learning as well as cross-modal contrastive pretraining and\nit outperforms several SOTA cross-modal baselines. Extensive ablation studies\nwell demonstrate the effectiveness and the generalization capacity of our\nmodel. Dataset and codes are available at https:\n//github.com/zhanxlin/Product1M.",
          "link": "http://arxiv.org/abs/2107.14572",
          "publishedOn": "2021-08-10T02:00:11.461Z",
          "wordCount": null,
          "title": "Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-modal Pretraining. (arXiv:2107.14572v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.08296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Granland_K/0/1/0/all/0/1\">Keenan Granland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newbury_R/0/1/0/all/0/1\">Rhys Newbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting_D/0/1/0/all/0/1\">David Ting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>",
          "description": "Training of convolutional neural networks for semantic segmentation requires\naccurate pixel-wise labeling which requires large amounts of human effort. The\nhuman-in-the-loop method reduces labeling effort; however, it requires human\nintervention for each image. This paper describes a general iterative training\nmethodology for semantic segmentation, Automating-the-Loop. This aims to\nreplicate the manual adjustments of the human-in-the-loop method with an\nautomated process, hence, drastically reducing labeling effort. Using the\napplication of detecting partially occluded apple tree segmentation, we compare\nmanually labeled annotations, self-training, human-in-the-loop, and\nAutomating-the-Loop methods in both the quality of the trained convolutional\nneural networks, and the effort needed to create them. The convolutional neural\nnetwork (U-Net) performance is analyzed using traditional metrics and a new\nmetric, Complete Grid Scan, which promotes connectivity and low noise. It is\nshown that in our application, the new Automating-the-Loop method greatly\nreduces the labeling effort while producing comparable performance to both\nhuman-in-the-loop and complete manual labeling methods.",
          "link": "http://arxiv.org/abs/2010.08296",
          "publishedOn": "2021-08-10T02:00:11.455Z",
          "wordCount": null,
          "title": "Minimizing Labeling Effort for Tree Skeleton Segmentation using an Automated Iterative Training Methodology. (arXiv:2010.08296v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12746",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qingyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengkai Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>",
          "description": "Localizing individuals in crowds is more in accordance with the practical\ndemands of subsequent high-level crowd analysis tasks than simply counting.\nHowever, existing localization based methods relying on intermediate\nrepresentations (\\textit{i.e.}, density maps or pseudo boxes) serving as\nlearning targets are counter-intuitive and error-prone. In this paper, we\npropose a purely point-based framework for joint crowd counting and individual\nlocalization. For this framework, instead of merely reporting the absolute\ncounting error at image level, we propose a new metric, called density\nNormalized Average Precision (nAP), to provide more comprehensive and more\nprecise performance evaluation. Moreover, we design an intuitive solution under\nthis framework, which is called Point to Point Network (P2PNet). P2PNet\ndiscards superfluous steps and directly predicts a set of point proposals to\nrepresent heads in an image, being consistent with the human annotation\nresults. By thorough analysis, we reveal the key step towards implementing such\na novel idea is to assign optimal learning targets for these proposals.\nTherefore, we propose to conduct this crucial association in an one-to-one\nmatching manner using the Hungarian algorithm. The P2PNet not only\nsignificantly surpasses state-of-the-art methods on popular counting\nbenchmarks, but also achieves promising localization accuracy. The codes will\nbe available at: https://github.com/TencentYoutuResearch/CrowdCounting-P2PNet.",
          "link": "http://arxiv.org/abs/2107.12746",
          "publishedOn": "2021-08-10T02:00:11.455Z",
          "wordCount": null,
          "title": "Rethinking Counting and Localization in Crowds:A Purely Point-Based Framework. (arXiv:2107.12746v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06534",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Immidisetti_R/0/1/0/all/0/1\">Rakhil Immidisetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shuowen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Existing thermal-to-visible face verification approaches expect the thermal\nand visible face images to be of similar resolution. This is unlikely in\nreal-world long-range surveillance systems, since humans are distant from the\ncameras. To address this issue, we introduce the task of thermal-to-visible\nface verification from low-resolution thermal images. Furthermore, we propose\nAxial-Generative Adversarial Network (Axial-GAN) to synthesize high-resolution\nvisible images for matching. In the proposed approach we augment the GAN\nframework with axial-attention layers which leverage the recent advances in\ntransformers for modelling long-range dependencies. We demonstrate the\neffectiveness of the proposed method by evaluating on two different\nthermal-visible face datasets. When compared to related state-of-the-art works,\nour results show significant improvements in both image quality and face\nverification performance, and are also much more efficient.",
          "link": "http://arxiv.org/abs/2104.06534",
          "publishedOn": "2021-08-10T02:00:11.454Z",
          "wordCount": null,
          "title": "Simultaneous Face Hallucination and Translation for Thermal to Visible Face Verification using Axial-GAN. (arXiv:2104.06534v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yi-Geng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Hui-Chu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wan-Lei Zhao</a>",
          "description": "Visual object localization is the key step in a series of object detection\ntasks. In the literature, high localization accuracy is achieved with the\nmainstream strongly supervised frameworks. However, such methods require\nobject-level annotations and are unable to detect objects of unknown\ncategories. Weakly supervised methods face similar difficulties. In this paper,\na self-paced learning framework is proposed to achieve accurate object\nlocalization on the rank list returned by instance search. The proposed\nframework mines the target instance gradually from the queries and their\ncorresponding top-ranked search results. Since a common instance is shared\nbetween the query and the images in the rank list, the target visual instance\ncan be accurately localized even without knowing what the object category is.\nIn addition to performing localization on instance search, the issue of\nfew-shot object detection is also addressed under the same framework. Superior\nperformance over state-of-the-art methods is observed on both tasks.",
          "link": "http://arxiv.org/abs/2107.05005",
          "publishedOn": "2021-08-10T02:00:11.449Z",
          "wordCount": null,
          "title": "Towards Accurate Localization by Instance Search. (arXiv:2107.05005v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patrikar_D/0/1/0/all/0/1\">Devashree R. Patrikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parate_M/0/1/0/all/0/1\">Mayur Rajram Parate</a>",
          "description": "The current concept of Smart Cities influences urban planners and researchers\nto provide modern, secured and sustainable infrastructure and give a decent\nquality of life to its residents. To fulfill this need video surveillance\ncameras have been deployed to enhance the safety and well-being of the\ncitizens. Despite technical developments in modern science, abnormal event\ndetection in surveillance video systems is challenging and requires exhaustive\nhuman efforts. In this paper, we surveyed various methodologies developed to\ndetect anomalies in intelligent video surveillance. Firstly, we revisit the\nsurveys on anomaly detection in the last decade. We then present a systematic\ncategorization of methodologies developed for ease of understanding.\nConsidering the notion of anomaly depends on context, we identify different\nobjects-of-interest and publicly available datasets in anomaly detection. Since\nanomaly detection is considered a time-critical application of computer vision,\nour emphasis is on anomaly detection using edge devices and approaches\nexplicitly designed for them. Further, we discuss the challenges and\nopportunities involved in anomaly detection at the edge.",
          "link": "http://arxiv.org/abs/2107.02778",
          "publishedOn": "2021-08-10T02:00:11.439Z",
          "wordCount": null,
          "title": "Anomaly Detection using Edge Computing in Video Surveillance System: Review. (arXiv:2107.02778v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03799",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jadhav_S/0/1/0/all/0/1\">Shreeraj Jadhav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_G/0/1/0/all/0/1\">Gaofeng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zawin_M/0/1/0/all/0/1\">Marlene Zawin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufman_A/0/1/0/all/0/1\">Arie E. Kaufman</a>",
          "description": "Significant work has been done towards deep learning (DL) models for\nautomatic lung and lesion segmentation and classification of COVID-19 on chest\nCT data. However, comprehensive visualization systems focused on supporting the\ndual visual+DL diagnosis of COVID-19 are non-existent. We present COVID-view, a\nvisualization application specially tailored for radiologists to diagnose\nCOVID-19 from chest CT data. The system incorporates a complete pipeline of\nautomatic lungs segmentation, localization/ isolation of lung abnormalities,\nfollowed by visualization, visual and DL analysis, and\nmeasurement/quantification tools. Our system combines the traditional 2D\nworkflow of radiologists with newer 2D and 3D visualization techniques with DL\nsupport for a more comprehensive diagnosis. COVID-view incorporates a novel DL\nmodel for classifying the patients into positive/negative COVID-19 cases, which\nacts as a reading aid for the radiologist using COVID-view and provides the\nattention heatmap as an explainable DL for the model output. We designed and\nevaluated COVID-view through suggestions, close feedback and conducting case\nstudies of real-world patient data by expert radiologists who have substantial\nexperience diagnosing chest CT scans for COVID-19, pulmonary embolism, and\nother forms of lung infections. We present requirements and task analysis for\nthe diagnosis of COVID-19 that motivate our design choices and results in a\npractical system which is capable of handling real-world patient cases.",
          "link": "http://arxiv.org/abs/2108.03799",
          "publishedOn": "2021-08-10T02:00:11.405Z",
          "wordCount": null,
          "title": "COVID-view: Diagnosis of COVID-19 using Chest CT. (arXiv:2108.03799v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1904.07768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yu-Min Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawson_A/0/1/0/all/0/1\">Austin Lawson</a>",
          "description": "Persistence diagrams are one of the main tools in the field of Topological\nData Analysis (TDA). They contain fruitful information about the shape of data.\nThe use of machine learning algorithms on the space of persistence diagrams\nproves to be challenging as the space lacks an inner product. For that reason,\ntransforming these diagrams in a way that is compatible with machine learning\nis an important topic currently researched in TDA. In this paper, our main\ncontribution consists of three components. First, we develop a general and\nunifying framework of vectorizing diagrams that we call the \\textit{Persistence\nCurves} (PCs), and show that several well-known summaries, such as Persistence\nLandscapes, fall under the PC framework. Second, we propose several new\nsummaries based on PC framework and provide a theoretical foundation for their\nstability analysis. Finally, we apply proposed PCs to two\napplications---texture classification and determining the parameters of a\ndiscrete dynamical system; their performances are competitive with other TDA\nmethods.",
          "link": "http://arxiv.org/abs/1904.07768",
          "publishedOn": "2021-08-10T02:00:11.405Z",
          "wordCount": null,
          "title": "Persistence Curves: A canonical framework for summarizing persistence diagrams. (arXiv:1904.07768v4 [cs.CG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>",
          "description": "Many unsupervised domain adaptation (UDA) methods exploit domain adversarial\ntraining to align the features to reduce domain gap, where a feature extractor\nis trained to fool a domain discriminator in order to have aligned feature\ndistributions. The discrimination capability of the domain classifier w.r.t the\nincreasingly aligned feature distributions deteriorates as training goes on,\nthus cannot effectively further drive the training of feature extractor. In\nthis work, we propose an efficient optimization strategy named Re-enforceable\nAdversarial Domain Adaptation (RADA) which aims to re-energize the domain\ndiscriminator during the training by using dynamic domain labels. Particularly,\nwe relabel the well aligned target domain samples as source domain samples on\nthe fly. Such relabeling makes the less separable distributions more separable,\nand thus leads to a more powerful domain classifier w.r.t. the new data\ndistributions, which in turn further drives feature alignment. Extensive\nexperiments on multiple UDA benchmarks demonstrate the effectiveness and\nsuperiority of our RADA.",
          "link": "http://arxiv.org/abs/2103.11661",
          "publishedOn": "2021-08-10T02:00:11.405Z",
          "wordCount": null,
          "title": "Re-energizing Domain Discriminator with Sample Relabeling for Adversarial Domain Adaptation. (arXiv:2103.11661v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03068",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bao_Y/0/1/0/all/0/1\">Yiming Bao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1\">Tong Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Linyan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jianwei Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Juan Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_D/0/1/0/all/0/1\">Dahong Qian</a>",
          "description": "The early diagnosis and screening of glaucoma are important for patients to\nreceive treatment in time and maintain eyesight. Nowadays, deep learning (DL)\nbased models have been successfully used for computer-aided diagnosis (CAD) of\nglaucoma from retina fundus images. However, a DL model pre-trained using a\ndataset from one hospital center may have poor performance on a dataset from\nanother new hospital center and therefore its applications in the real scene\nare limited. In this paper, we propose a self-adaptive transfer learning (SATL)\nstrategy to fill the domain gap between multicenter datasets. Specifically, the\nencoder of a DL model that is pre-trained on the source domain is used to\ninitialize the encoder of a reconstruction model. Then, the reconstruction\nmodel is trained using only unlabeled image data from the target domain, which\nmakes the encoder in the model adapt itself to extract useful high-level\nfeatures both for target domain images encoding and glaucoma classification,\nsimultaneously. Experimental results demonstrate that the proposed SATL\nstrategy is effective in the domain adaptation task between one private and two\npublic glaucoma diagnosis datasets, i.e. pri-RFG, REFUGE, and LAG. Moreover,\nthe proposed strategy is completely independent of the source domain data,\nwhich meets the real scene application and the privacy protection policy.",
          "link": "http://arxiv.org/abs/2105.03068",
          "publishedOn": "2021-08-10T02:00:11.404Z",
          "wordCount": null,
          "title": "Self-Adaptive Transfer Learning for Multicenter Glaucoma Classification in Fundus Retina Images. (arXiv:2105.03068v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.00909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Lingfei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dongrui Wu</a>",
          "description": "Transfer learning (TL) utilizes data or knowledge from one or more source\ndomains to facilitate the learning in a target domain. It is particularly\nuseful when the target domain has very few or no labeled data, due to\nannotation expense, privacy concerns, etc. Unfortunately, the effectiveness of\nTL is not always guaranteed. Negative transfer (NT), i.e., leveraging source\ndomain data/knowledge undesirably reduces the learning performance in the\ntarget domain, has been a long-standing and challenging problem in TL. Various\napproaches have been proposed in the literature to handle it. However, there\ndoes not exist a systematic survey on the formulation of NT, the factors\nleading to NT, and the algorithms that mitigate NT. This paper fills this gap,\nby first introducing the definition of NT and its factors, then reviewing about\nfifty representative approaches for overcoming NT, according to four\ncategories: secure transfer, domain similarity estimation, distant transfer,\nand NT mitigation. NT in related fields, e.g., multi-task learning, lifelong\nlearning, and adversarial attacks, are also discussed.",
          "link": "http://arxiv.org/abs/2009.00909",
          "publishedOn": "2021-08-10T02:00:11.403Z",
          "wordCount": null,
          "title": "A Survey on Negative Transfer. (arXiv:2009.00909v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Cheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1\">Xiya Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifeng Zhang</a>",
          "description": "Face anti-spoofing is an important task to protect the security of face\nrecognition. Most of previous work either struggle to capture discriminative\nand generalizable feature or rely on auxiliary information which is unavailable\nfor most of industrial product. Inspired by the video classification work, we\npropose an efficient two-stream model to capture the key differences between\nlive and spoof faces, which takes multi-frames and RGB difference as input\nrespectively. Feature pyramid modules with two opposite fusion directions and\npyramid pooling modules are applied to enhance feature representation. We\nevaluate the proposed method on the datasets of Siw, Oulu-NPU, CASIA-MFSD and\nReplay-Attack. The results show that our model achieves the state-of-the-art\nresults on most of datasets' protocol with much less parameter size.",
          "link": "http://arxiv.org/abs/2108.04032",
          "publishedOn": "2021-08-10T02:00:11.402Z",
          "wordCount": null,
          "title": "Two-stream Convolutional Networks for Multi-frame Face Anti-spoofing. (arXiv:2108.04032v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>",
          "description": "To mitigate the inspector's workload and improve the quality of the product,\ncomputer vision-based anomaly detection (AD) techniques are gradually deployed\nin real-world industrial scenarios. Recent anomaly analysis benchmarks progress\nto generative models. The aim is to model the defect-free distribution so that\nanomalies can be classified as out-of-distribution samples. Nevertheless, there\nare two disturbing factors that need researchers and deployers to prioritize:\n(i) the simplistic prior latent distribution inducing limited expressive\ncapability; (ii) the collapsed mutual-dependent features resulting in poor\ngeneralization. In this paper, we propose a novel Patch-wise Wasserstein\nAutoEncoder (P-WAE) architecture to alleviate those challenges. In particular,\na patch-wise variational inference model coupled with solving the jigsaw puzzle\nis designed, which is a simple yet effective way to increase the expressiveness\nand complexity of the latent manifold. This alleviates the blurry\nreconstruction problem. In addition, the Hilbert-Schmidt Independence Criterion\n(HSIC) bottleneck is introduced to constrain the over-regularization\nrepresentation. Comprehensive experiments, conducted on the MVTec AD dataset,\ndemonstrate the superior performance of our propo",
          "link": "http://arxiv.org/abs/2108.03815",
          "publishedOn": "2021-08-10T02:00:11.401Z",
          "wordCount": null,
          "title": "P-WAE: Generalized Patch-Wasserstein Autoencoder for Anomaly Screening. (arXiv:2108.03815v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yihao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juntao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Peng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianjiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qi Feng</a>",
          "description": "With the continuous improvement of the performance of object detectors via\nadvanced model architectures, imbalance problems in the training process have\nreceived more attention. It is a common paradigm in object detection frameworks\nto perform multi-scale detection. However, each scale is treated equally during\ntraining. In this paper, we carefully study the objective imbalance of\nmulti-scale detector training. We argue that the loss in each scale level is\nneither equally important nor independent. Different from the existing\nsolutions of setting multi-task weights, we dynamically optimize the loss\nweight of each scale level in the training process. Specifically, we propose an\nAdaptive Variance Weighting (AVW) to balance multi-scale loss according to the\nstatistical variance. Then we develop a novel Reinforcement Learning\nOptimization (RLO) to decide the weighting scheme probabilistically during\ntraining. The proposed dynamic methods make better utilization of multi-scale\ntraining loss without extra computational complexity and learnable parameters\nfor backpropagation. Experiments show that our approaches can consistently\nboost the performance over various baseline detectors on Pascal VOC and MS COCO\nbenchmark.",
          "link": "http://arxiv.org/abs/2108.04014",
          "publishedOn": "2021-08-10T02:00:11.401Z",
          "wordCount": null,
          "title": "Dynamic Multi-Scale Loss Optimization for Object Detection. (arXiv:2108.04014v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tianwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1\">Ruifeng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>",
          "description": "Neural painting refers to the procedure of producing a series of strokes for\na given image and non-photo-realistically recreating it using neural networks.\nWhile reinforcement learning (RL) based agents can generate a stroke sequence\nstep by step for this task, it is not easy to train a stable RL agent. On the\nother hand, stroke optimization methods search for a set of stroke parameters\niteratively in a large search space; such low efficiency significantly limits\ntheir prevalence and practicality. Different from previous methods, in this\npaper, we formulate the task as a set prediction problem and propose a novel\nTransformer-based framework, dubbed Paint Transformer, to predict the\nparameters of a stroke set with a feed forward network. This way, our model can\ngenerate a set of strokes in parallel and obtain the final painting of size 512\n* 512 in near real time. More importantly, since there is no dataset available\nfor training the Paint Transformer, we devise a self-training pipeline such\nthat it can be trained without any off-the-shelf dataset while still achieving\nexcellent generalization capability. Experiments demonstrate that our method\nachieves better painting performance than previous ones with cheaper training\nand inference costs. Codes and models are available.",
          "link": "http://arxiv.org/abs/2108.03798",
          "publishedOn": "2021-08-10T02:00:11.400Z",
          "wordCount": null,
          "title": "Paint Transformer: Feed Forward Neural Painting with Stroke Prediction. (arXiv:2108.03798v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.16563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Planche_B/0/1/0/all/0/1\">Benjamin Planche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rajat Vikram Singh</a>",
          "description": "Gradient-based algorithms are crucial to modern computer-vision and graphics\napplications, enabling learning-based optimization and inverse problems. For\nexample, photorealistic differentiable rendering pipelines for color images\nhave been proven highly valuable to applications aiming to map 2D and 3D\ndomains. However, to the best of our knowledge, no effort has been made so far\ntowards extending these gradient-based methods to the generation of depth\n(2.5D) images, as simulating structured-light depth sensors implies solving\ncomplex light transport and stereo-matching problems. In this paper, we\nintroduce a novel end-to-end differentiable simulation pipeline for the\ngeneration of realistic 2.5D scans, built on physics-based 3D rendering and\ncustom block-matching algorithms. Each module can be differentiated w.r.t\nsensor and scene parameters; e.g., to automatically tune the simulation for new\ndevices over some provided scans or to leverage the pipeline as a 3D-to-2.5D\ntransformer within larger computer-vision applications. Applied to the training\nof deep-learning methods for various depth-based recognition tasks\n(classification, pose estimation, semantic segmentation), our simulation\ngreatly improves the performance of the resulting models on real scans, thereby\ndemonstrating the fidelity and value of its synthetic depth data compared to\nprevious static simulations and learning-based domain adaptation schemes.",
          "link": "http://arxiv.org/abs/2103.16563",
          "publishedOn": "2021-08-10T02:00:11.400Z",
          "wordCount": null,
          "title": "Physics-based Differentiable Depth Sensor Simulation. (arXiv:2103.16563v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiaojiao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guizhong Liu</a>",
          "description": "Self-supervised deep learning-based 3D scene understanding methods can\novercome the difficulty of acquiring the densely labeled ground-truth and have\nmade a lot of advances. However, occlusions and moving objects are still some\nof the major limitations. In this paper, we explore the learnable occlusion\naware optical flow guided self-supervised depth and camera pose estimation by\nan adaptive cross weighted loss to address the above limitations. Firstly, we\nexplore to train the learnable occlusion mask fused optical flow network by an\nocclusion-aware photometric loss with the temporally supplemental information\nand backward-forward consistency of adjacent views. And then, we design an\nadaptive cross-weighted loss between the depth-pose and optical flow loss of\nthe geometric and photometric error to distinguish the moving objects which\nviolate the static scene assumption. Our method shows promising results on\nKITTI, Make3D, and Cityscapes datasets under multiple tasks. We also show good\ngeneralization ability under a variety of challenging scenarios.",
          "link": "http://arxiv.org/abs/2108.03893",
          "publishedOn": "2021-08-10T02:00:11.398Z",
          "wordCount": null,
          "title": "Self-supervised Learning of Occlusion Aware Flow Guided 3D Geometry Perception with Adaptive Cross Weighted Loss from Monocular Videos. (arXiv:2108.03893v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taskynov_A/0/1/0/all/0/1\">Anuar Taskynov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korviakov_V/0/1/0/all/0/1\">Vladimir Korviakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazurenko_I/0/1/0/all/0/1\">Ivan Mazurenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yepan Xiong</a>",
          "description": "Nowadays Deep Learning became widely used in many economic, technical and\nscientific areas of human interest. It is clear that efficiency of solutions\nbased on Deep Neural Networks should consider not only quality metric for the\ntarget task, but also latency and constraints of target platform design should\nbe taken into account. In this paper we present novel hardware-friendly\nTensor-Train decomposition implementation for Convolutional Neural Networks\ntogether with Tensor Yard - one-shot training algorithm which optimizes an\norder of decomposition of network layers. These ideas allow to accelerate\nResNet models on Ascend 310 NPU devices without significant loss of accuracy.\nFor example we accelerate ResNet-101 by 14.6% with drop by 0.5 of top-1\nImageNet accuracy.",
          "link": "http://arxiv.org/abs/2108.04029",
          "publishedOn": "2021-08-10T02:00:11.345Z",
          "wordCount": null,
          "title": "Tensor Yard: One-Shot Algorithm of Hardware-Friendly Tensor-Train Decomposition for Convolutional Neural Networks. (arXiv:2108.04029v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1\">Shan An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiajie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Haogang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsintotas_K/0/1/0/all/0/1\">Konstantinos A. Tsintotas</a>",
          "description": "Hand pose estimation is a fundamental task in many human-robot\ninteraction-related applications. However, previous approaches suffer from\nunsatisfying hand landmark predictions in real-world scenes and high\ncomputation burden. In this paper, we propose a fast and accurate framework for\nhand pose estimation, dubbed as \"FastHand\". Using a lightweight encoder-decoder\nnetwork architecture, FastHand fulfills the requirements of practical\napplications running on embedded devices. The encoder consists of deep layers\nwith a small number of parameters, while the decoder makes use of spatial\nlocation information to obtain more accurate results. The evaluation took place\non two publicly available datasets demonstrating the improved performance of\nthe proposed pipeline compared to other state-of-the-art approaches. FastHand\noffers high accuracy scores while reaching a speed of 25 frames per second on\nan NVIDIA Jetson TX2 graphics processing unit.",
          "link": "http://arxiv.org/abs/2102.07067",
          "publishedOn": "2021-08-10T02:00:11.337Z",
          "wordCount": null,
          "title": "Fast Monocular Hand Pose Estimation on Embedded Systems. (arXiv:2102.07067v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhar_P/0/1/0/all/0/1\">Prithviraj Dhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleason_J/0/1/0/all/0/1\">Joshua Gleason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Aniket Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castillo_C/0/1/0/all/0/1\">Carlos D. Castillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>",
          "description": "Face recognition networks encode information about sensitive attributes while\nbeing trained for identity classification. Such encoding has two major issues:\n(a) it makes the face representations susceptible to privacy leakage (b) it\nappears to contribute to bias in face recognition. However, existing bias\nmitigation approaches generally require end-to-end training and are unable to\nachieve high verification accuracy. Therefore, we present a descriptor-based\nadversarial de-biasing approach called `Protected Attribute Suppression System\n(PASS)'. PASS can be trained on top of descriptors obtained from any previously\ntrained high-performing network to classify identities and simultaneously\nreduce encoding of sensitive attributes. This eliminates the need for\nend-to-end training. As a component of PASS, we present a novel discriminator\ntraining strategy that discourages a network from encoding protected attribute\ninformation. We show the efficacy of PASS to reduce gender and skintone\ninformation in descriptors from SOTA face recognition networks like Arcface. As\na result, PASS descriptors outperform existing baselines in reducing gender and\nskintone bias on the IJB-C dataset, while maintaining a high verification\naccuracy.",
          "link": "http://arxiv.org/abs/2108.03764",
          "publishedOn": "2021-08-10T02:00:11.335Z",
          "wordCount": null,
          "title": "PASS: Protected Attribute Suppression System for Mitigating Bias in Face Recognition. (arXiv:2108.03764v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Opazo_C/0/1/0/all/0/1\">Cristian Rodriguez-Opazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1\">Damien Teney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1\">Stephen Gould</a>",
          "description": "We extend the task of composed image retrieval, where an input query consists\nof an image and short textual description of how to modify the image. Existing\nmethods have only been applied to non-complex images within narrow domains,\nsuch as fashion products, thereby limiting the scope of study on in-depth\nvisual reasoning in rich image and language contexts. To address this issue, we\ncollect the Compose Image Retrieval on Real-life images (CIRR) dataset, which\nconsists of over 36,000 pairs of crowd-sourced, open-domain images with\nhuman-generated modifying text. To extend current methods to the open-domain,\nwe propose CIRPLANT, a transformer based model that leverages rich pre-trained\nvision-and-language (V&L) knowledge for modifying visual features conditioned\non natural language. Retrieval is then done by nearest neighbor lookup on the\nmodified features. We demonstrate that with a relatively simple architecture,\nCIRPLANT outperforms existing methods on open-domain images, while matching\nstate-of-the-art accuracy on the existing narrow datasets, such as fashion.\nTogether with the release of CIRR, we believe this work will inspire further\nresearch on composed image retrieval.",
          "link": "http://arxiv.org/abs/2108.04024",
          "publishedOn": "2021-08-10T02:00:11.335Z",
          "wordCount": null,
          "title": "Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models. (arXiv:2108.04024v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Souri_Y/0/1/0/all/0/1\">Yaser Souri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farha_Y/0/1/0/all/0/1\">Yazan Abu Farha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Despinoy_F/0/1/0/all/0/1\">Fabien Despinoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francesca_G/0/1/0/all/0/1\">Gianpiero Francesca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>",
          "description": "We introduce FIFA, a fast approximate inference method for action\nsegmentation and alignment. Unlike previous approaches, FIFA does not rely on\nexpensive dynamic programming for inference. Instead, it uses an approximate\ndifferentiable energy function that can be minimized using gradient-descent.\nFIFA is a general approach that can replace exact inference improving its speed\nby more than 5 times while maintaining its performance. FIFA is an anytime\ninference algorithm that provides a better speed vs. accuracy trade-off\ncompared to exact inference. We apply FIFA on top of state-of-the-art\napproaches for weakly supervised action segmentation and alignment as well as\nfully supervised action segmentation. FIFA achieves state-of-the-art results on\nmost metrics on two action segmentation datasets.",
          "link": "http://arxiv.org/abs/2108.03894",
          "publishedOn": "2021-08-10T02:00:11.289Z",
          "wordCount": null,
          "title": "FIFA: Fast Inference Approximation for Action Segmentation. (arXiv:2108.03894v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>",
          "description": "This paper presents a three-tier modality alignment approach to learning\ntext-image joint embedding, coined as JEMA, for cross-modal retrieval of\ncooking recipes and food images. The first tier improves recipe text embedding\nby optimizing the LSTM networks with term extraction and ranking enhanced\nsequence patterns, and optimizes the image embedding by combining the\nResNeXt-101 image encoder with the category embedding using wideResNet-50 with\nword2vec. The second tier modality alignment optimizes the textual-visual joint\nembedding loss function using a double batch-hard triplet loss with soft-margin\noptimization. The third modality alignment incorporates two types of\ncross-modality alignments as the auxiliary loss regularizations to further\nreduce the alignment errors in the joint learning of the two modality-specific\nembedding functions. The category-based cross-modal alignment aims to align the\nimage category with the recipe category as a loss regularization to the joint\nembedding. The cross-modal discriminator-based alignment aims to add the\nvisual-textual embedding distribution alignment to further regularize the joint\nembedding loss. Extensive experiments with the one-million recipes benchmark\ndataset Recipe1M demonstrate that the proposed JEMA approach outperforms the\nstate-of-the-art cross-modal embedding methods for both image-to-recipe and\nrecipe-to-image retrievals.",
          "link": "http://arxiv.org/abs/2108.03788",
          "publishedOn": "2021-08-10T02:00:11.285Z",
          "wordCount": null,
          "title": "Learning Joint Embedding with Modality Alignments for Cross-Modal Retrieval of Recipes and Food Images. (arXiv:2108.03788v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Baobei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>",
          "description": "Monocular depth estimation aims at predicting depth from a single image or\nvideo. Recently, self-supervised methods draw much attention, due to their free\nof depth annotations and impressive performance on several daytime benchmarks,\nsuch as KITTI and Cityscapes. However, they produce weird outputs in more\nchallenging nighttime scenarios because of low visibility and varying\nilluminations, which bring weak textures and break brightness-consistency\nassumption, respectively. To address these problems, in this paper we propose a\nnovel framework with several improvements: (1) we introduce Priors-Based\nRegularization to learn distribution knowledge from unpaired depth maps and\nprevent model from being incorrectly trained; (2) we leverage\nMapping-Consistent Image Enhancement module to enhance image visibility and\ncontrast while maintaining brightness consistency; and (3) we present\nStatistics-Based Mask strategy to tune the number of removed pixels within\ntextureless regions, using dynamic statistics. Experimental results demonstrate\nthe effectiveness of each component. Meanwhile, our framework achieves\nremarkable improvements and state-of-the-art results on two nighttime datasets.",
          "link": "http://arxiv.org/abs/2108.03830",
          "publishedOn": "2021-08-10T02:00:11.285Z",
          "wordCount": null,
          "title": "Regularizing Nighttime Weirdness: Efficient Self-supervised Monocular Depth Estimation in the Dark. (arXiv:2108.03830v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>",
          "description": "To mitigate the radiologist's workload, computer-aided diagnosis with the\ncapability to review and analyze medical images is gradually deployed. Deep\nlearning-based region of interest segmentation is among the most exciting use\ncases. However, this paradigm is restricted in real-world clinical applications\ndue to poor robustness and generalization. The issue is more sinister with a\nlack of training data. In this paper, we address the challenge from the\nrepresentation learning point of view. We investigate that the collapsed\nrepresentations, as one of the main reasons which caused poor robustness and\ngeneralization, could be avoided through transfer learning. Therefore, we\npropose a novel two-stage framework for robust generalized segmentation. In\nparticular, an unsupervised Tile-wise AutoEncoder (T-AE) pretraining\narchitecture is coined to learn meaningful representation for improving the\ngeneralization and robustness of the downstream tasks. Furthermore, the learned\nknowledge is transferred to the segmentation benchmark. Coupled with an image\nreconstruction network, the representation keeps to be decoded, encouraging the\nmodel to capture more semantic features. Experiments of lung segmentation on\nmulti chest X-ray datasets are conducted. Empirically, the related experimental\nresults demonstrate the superior generalization capability of the proposed\nframework on unseen domains in terms of high performance and robustness to\ncorruption, especially under the scenario of the limited training data.",
          "link": "http://arxiv.org/abs/2108.03823",
          "publishedOn": "2021-08-10T02:00:11.284Z",
          "wordCount": null,
          "title": "Towards to Robust and Generalized Medical Image Segmentation Framework. (arXiv:2108.03823v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosu_R/0/1/0/all/0/1\">Radu Alexandru Rosu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutt_P/0/1/0/all/0/1\">Peer Sch&#xfc;tt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quenzel_J/0/1/0/all/0/1\">Jan Quenzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>",
          "description": "Deep convolutional neural networks (CNNs) have shown outstanding performance\nin the task of semantically segmenting images. Applying the same methods on 3D\ndata still poses challenges due to the heavy memory requirements and the lack\nof structured data. Here, we propose LatticeNet, a novel approach for 3D\nsemantic segmentation, which takes raw point clouds as input. A PointNet\ndescribes the local geometry which we embed into a sparse permutohedral\nlattice. The lattice allows for fast convolutions while keeping a low memory\nfootprint. Further, we introduce DeformSlice, a novel learned data-dependent\ninterpolation for projecting lattice features back onto the point cloud. We\npresent results of 3D segmentation on multiple datasets where our method\nachieves state-of-the-art performance. We also extend and evaluate our network\nfor instance and dynamic object segmentation.",
          "link": "http://arxiv.org/abs/2108.03917",
          "publishedOn": "2021-08-10T02:00:11.282Z",
          "wordCount": null,
          "title": "LatticeNet: Fast Spatio-Temporal Point Cloud Segmentation Using Permutohedral Lattices. (arXiv:2108.03917v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1\">Matthias Zwicker</a>",
          "description": "Unsupervised learning of global features for 3D shape analysis is an\nimportant research challenge because it avoids manual effort for supervised\ninformation collection. In this paper, we propose a view-based deep learning\nmodel called Hierarchical View Predictor (HVP) to learn 3D shape features from\nunordered views in an unsupervised manner. To mine highly discriminative\ninformation from unordered views, HVP performs a novel hierarchical view\nprediction over a view pair, and aggregates the knowledge learned from the\npredictions in all view pairs into a global feature. In a view pair, we pose\nhierarchical view prediction as the task of hierarchically predicting a set of\nimage patches in a current view from its complementary set of patches, and in\naddition, completing the current view and its opposite from any one of the two\nsets of patches. Hierarchical prediction, in patches to patches, patches to\nview and view to view, facilitates HVP to effectively learn the structure of 3D\nshapes from the correlation between patches in the same view and the\ncorrelation between a pair of complementary views. In addition, the employed\nimplicit aggregation over all view pairs enables HVP to learn global features\nfrom unordered views. Our results show that HVP can outperform state-of-the-art\nmethods under large-scale 3D shape benchmarks in shape classification and\nretrieval.",
          "link": "http://arxiv.org/abs/2108.03743",
          "publishedOn": "2021-08-10T02:00:11.280Z",
          "wordCount": null,
          "title": "Hierarchical View Predictor: Unsupervised 3D Global Feature Learning through Hierarchical Prediction among Unordered Views. (arXiv:2108.03743v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>",
          "description": "We introduce the task of open-vocabulary visual instance search (OVIS). Given\nan arbitrary textual search query, Open-vocabulary Visual Instance Search\n(OVIS) aims to return a ranked list of visual instances, i.e., image patches,\nthat satisfies the search intent from an image database. The term \"open\nvocabulary\" means that there are neither restrictions to the visual instance to\nbe searched nor restrictions to the word that can be used to compose the\ntextual search query. We propose to address such a search challenge via\nvisual-semantic aligned representation learning (ViSA). ViSA leverages massive\nimage-caption pairs as weak image-level (not instance-level) supervision to\nlearn a rich cross-modal semantic space where the representations of visual\ninstances (not images) and those of textual queries are aligned, thus allowing\nus to measure the similarities between any visual instance and an arbitrary\ntextual query. To evaluate the performance of ViSA, we build two datasets named\nOVIS40 and OVIS1600 and also introduce a pipeline for error analysis. Through\nextensive experiments on the two datasets, we demonstrate ViSA's ability to\nsearch for visual instances in images not available during training given a\nwide range of textual queries including those composed of uncommon words.\nExperimental results show that ViSA achieves an mAP@50 of 21.9% on OVIS40 under\nthe most challenging setting and achieves an mAP@6 of 14.9% on OVIS1600\ndataset.",
          "link": "http://arxiv.org/abs/2108.03704",
          "publishedOn": "2021-08-10T02:00:11.279Z",
          "wordCount": null,
          "title": "OVIS: Open-Vocabulary Visual Instance Search via Visual-Semantic Aligned Representation Learning. (arXiv:2108.03704v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haochen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanwen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>",
          "description": "Video-and-Language Inference is a recently proposed task for joint\nvideo-and-language understanding. This new task requires a model to draw\ninference on whether a natural language statement entails or contradicts a\ngiven video clip. In this paper, we study how to address three critical\nchallenges for this task: judging the global correctness of the statement\ninvolved multiple semantic meanings, joint reasoning over video and subtitles,\nand modeling long-range relationships and complex social interactions. First,\nwe propose an adaptive hierarchical graph network that achieves in-depth\nunderstanding of the video over complex interactions. Specifically, it performs\njoint reasoning over video and subtitles in three hierarchies, where the graph\nstructure is adaptively adjusted according to the semantic structures of the\nstatement. Secondly, we introduce semantic coherence learning to explicitly\nencourage the semantic coherence of the adaptive hierarchical graph network\nfrom three hierarchies. The semantic coherence learning can further improve the\nalignment between vision and linguistics, and the coherence across a sequence\nof video segments. Experimental results show that our method significantly\noutperforms the baseline by a large margin.",
          "link": "http://arxiv.org/abs/2107.12270",
          "publishedOn": "2021-08-10T02:00:11.272Z",
          "wordCount": null,
          "title": "Adaptive Hierarchical Graph Reasoning with Semantic Coherence for Video-and-Language Inference. (arXiv:2107.12270v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.15007",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ayoobi_N/0/1/0/all/0/1\">Nooshin Ayoobi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifrazi_D/0/1/0/all/0/1\">Danial Sharifrazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeibi_A/0/1/0/all/0/1\">Afshin Shoeibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorriz_J/0/1/0/all/0/1\">Juan M. Gorriz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosaei_H/0/1/0/all/0/1\">Hossein Moosaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chofreh_A/0/1/0/all/0/1\">Abdoulmohammad Gholamzadeh Chofreh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goni_F/0/1/0/all/0/1\">Feybi Ariani Goni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klemes_J/0/1/0/all/0/1\">Jiri Jaromir Klemes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosavi_A/0/1/0/all/0/1\">Amir Mosavi</a>",
          "description": "The first known case of Coronavirus disease 2019 (COVID-19) was identified in\nDecember 2019. It has spread worldwide, leading to an ongoing pandemic, imposed\nrestrictions and costs to many countries. Predicting the number of new cases\nand deaths during this period can be a useful step in predicting the costs and\nfacilities required in the future. The purpose of this study is to predict new\ncases and deaths rate one, three and seven-day ahead during the next 100 days.\nThe motivation for predicting every n days (instead of just every day) is the\ninvestigation of the possibility of computational cost reduction and still\nachieving reasonable performance. Such a scenario may be encountered real-time\nforecasting of time series. Six different deep learning methods are examined on\nthe data adopted from the WHO website. Three methods are LSTM, Convolutional\nLSTM, and GRU. The bidirectional extension is then considered for each method\nto forecast the rate of new cases and new deaths in Australia and Iran\ncountries.",
          "link": "http://arxiv.org/abs/2104.15007",
          "publishedOn": "2021-08-10T02:00:11.223Z",
          "wordCount": null,
          "title": "Time Series Forecasting of New Cases and New Deaths Rate for COVID-19 using Deep Learning Methods. (arXiv:2104.15007v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jakob_P/0/1/0/all/0/1\">Peter Jakob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_M/0/1/0/all/0/1\">Manav Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_Schirling_T/0/1/0/all/0/1\">Tobias Schmid-Schirling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>",
          "description": "Anomaly detection is a critical problem in the manufacturing industry. In\nmany applications, images of objects to be analyzed are captured from multiple\nperspectives which can be exploited to improve the robustness of anomaly\ndetection. In this work, we build upon the deep support vector data description\nalgorithm and address multi-perspective anomaly detection using three different\nfusion techniques, i.e., early fusion, late fusion, and late fusion with\nmultiple decoders. We employ different augmentation techniques with a denoising\nprocess to deal with scarce one-class data, which further improves the\nperformance (ROC AUC $= 80\\%$). Furthermore, we introduce the dices dataset,\nwhich consists of over 2000 grayscale images of falling dices from multiple\nperspectives, with 5\\% of the images containing rare anomalies (e.g., drill\nholes, sawing, or scratches). We evaluate our approach on the new dices dataset\nusing images from two different perspectives and also benchmark on the standard\nMNIST dataset. Extensive experiments demonstrate that our proposed\n{multi-perspective} approach exceeds the state-of-the-art {single-perspective\nanomaly detection on both the MNIST and dices datasets}. To the best of our\nknowledge, this is the first work that focuses on addressing multi-perspective\nanomaly detection in images by jointly using different perspectives together\nwith one single objective function for anomaly detection.",
          "link": "http://arxiv.org/abs/2105.09903",
          "publishedOn": "2021-08-10T02:00:11.146Z",
          "wordCount": null,
          "title": "Multi-Perspective Anomaly Detection. (arXiv:2105.09903v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaobo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>",
          "description": "In this paper, we study the task of hallucinating an authentic\nhigh-resolution (HR) face from an occluded thumbnail. We propose a multi-stage\nProgressive Upsampling and Inpainting Generative Adversarial Network, dubbed\nPro-UIGAN, which exploits facial geometry priors to replenish and upsample (8*)\nthe occluded and tiny faces (16*16 pixels). Pro-UIGAN iteratively (1) estimates\nfacial geometry priors for low-resolution (LR) faces and (2) acquires\nnon-occluded HR face images under the guidance of the estimated priors. Our\nmulti-stage hallucination network super-resolves and inpaints occluded LR faces\nin a coarse-to-fine manner, thus reducing unwanted blurriness and artifacts\nsignificantly. Specifically, we design a novel cross-modal transformer module\nfor facial priors estimation, in which an input face and its landmark features\nare formulated as queries and keys, respectively. Such a design encourages\njoint feature learning across the input facial and landmark features, and deep\nfeature correspondences will be discovered by attention. Thus, facial\nappearance features and facial geometry priors are learned in a mutual\npromotion manner. Extensive experiments demonstrate that our Pro-UIGAN achieves\nvisually pleasing HR faces, reaching superior performance in downstream tasks,\ni.e., face alignment, face parsing, face recognition and expression\nclassification, compared with other state-of-the-art (SotA) methods.",
          "link": "http://arxiv.org/abs/2108.00602",
          "publishedOn": "2021-08-10T02:00:11.146Z",
          "wordCount": null,
          "title": "Pro-UIGAN: Progressive Face Hallucination from Occluded Thumbnails. (arXiv:2108.00602v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peixia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baopu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Ming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+yan_J/0/1/0/all/0/1\">Junjie yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>",
          "description": "We introduce the first Neural Architecture Search (NAS) method to find a\nbetter transformer architecture for image recognition. Recently, transformers\nwithout CNN-based backbones are found to achieve impressive performance for\nimage recognition. However, the transformer is designed for NLP tasks and thus\ncould be sub-optimal when directly used for image recognition. In order to\nimprove the visual representation ability for transformers, we propose a new\nsearch space and searching algorithm. Specifically, we introduce a locality\nmodule that models the local correlations in images explicitly with fewer\ncomputational cost. With the locality module, our search space is defined to\nlet the search algorithm freely trade off between global and local information\nas well as optimizing the low-level design choice in each module. To tackle the\nproblem caused by huge search space, a hierarchical neural architecture search\nmethod is proposed to search the optimal vision transformer from two levels\nseparately with the evolutionary algorithm. Extensive experiments on the\nImageNet dataset demonstrate that our method can find more discriminative and\nefficient transformer variants than the ResNet family (e.g., ResNet101) and the\nbaseline ViT for image classification.",
          "link": "http://arxiv.org/abs/2107.02960",
          "publishedOn": "2021-08-10T02:00:11.137Z",
          "wordCount": null,
          "title": "GLiT: Neural Architecture Search for Global and Local Image Transformer. (arXiv:2107.02960v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vaishnav_M/0/1/0/all/0/1\">Mohit Vaishnav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadene_R/0/1/0/all/0/1\">Remi Cadene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alamia_A/0/1/0/all/0/1\">Andrea Alamia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linsley_D/0/1/0/all/0/1\">Drew Linsley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1\">Rufin VanRullen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1\">Thomas Serre</a>",
          "description": "Visual understanding requires comprehending complex visual relations between\nobjects within a scene. Here, we seek to characterize the computational demands\nfor abstract visual reasoning. We do this by systematically assessing the\nability of modern deep convolutional neural networks (CNNs) to learn to solve\nthe Synthetic Visual Reasoning Test (SVRT) challenge, a collection of\ntwenty-three visual reasoning problems. Our analysis leads to a novel taxonomy\nof visual reasoning tasks, which can be primarily explained by both the type of\nrelations (same-different vs. spatial-relation judgments) and the number of\nrelations used to compose the underlying rules. Prior cognitive neuroscience\nwork suggests that attention plays a key role in human's visual reasoning\nability. To test this, we extended the CNNs with spatial and feature-based\nattention mechanisms. In a second series of experiments, we evaluated the\nability of these attention networks to learn to solve the SVRT challenge and\nfound the resulting architectures to be much more efficient at solving the\nhardest of these visual reasoning tasks. Most importantly, the corresponding\nimprovements on individual tasks partially explained the taxonomy. Overall,\nthis work advances our understanding of visual reasoning and yields testable\nNeuroscience predictions regarding the need for feature-based vs. spatial\nattention in visual reasoning.",
          "link": "http://arxiv.org/abs/2108.03603",
          "publishedOn": "2021-08-10T02:00:11.133Z",
          "wordCount": 641,
          "title": "Understanding the computational demands underlying visual reasoning. (arXiv:2108.03603v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tianwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meiling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhengxing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>",
          "description": "Fast arbitrary neural style transfer has attracted widespread attention from\nacademic, industrial and art communities due to its flexibility in enabling\nvarious applications. Existing solutions either attentively fuse deep style\nfeature into deep content feature without considering feature distributions, or\nadaptively normalize deep content feature according to the style such that\ntheir global statistics are matched. Although effective, leaving shallow\nfeature unexplored and without locally considering feature statistics, they are\nprone to unnatural output with unpleasing local distortions. To alleviate this\nproblem, in this paper, we propose a novel attention and normalization module,\nnamed Adaptive Attention Normalization (AdaAttN), to adaptively perform\nattentive normalization on per-point basis. Specifically, spatial attention\nscore is learnt from both shallow and deep features of content and style\nimages. Then per-point weighted statistics are calculated by regarding a style\nfeature point as a distribution of attention-weighted output of all style\nfeature points. Finally, the content feature is normalized so that they\ndemonstrate the same local feature statistics as the calculated per-point\nweighted style feature statistics. Besides, a novel local feature loss is\nderived based on AdaAttN to enhance local visual quality. We also extend\nAdaAttN to be ready for video style transfer with slight modifications.\nExperiments demonstrate that our method achieves state-of-the-art arbitrary\nimage/video style transfer. Codes and models are available.",
          "link": "http://arxiv.org/abs/2108.03647",
          "publishedOn": "2021-08-10T02:00:11.126Z",
          "wordCount": 680,
          "title": "AdaAttN: Revisit Attention Mechanism in Arbitrary Neural Style Transfer. (arXiv:2108.03647v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.06170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Samyak Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarlagadda_P/0/1/0/all/0/1\">Pradeep Yarlagadda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyoti_S/0/1/0/all/0/1\">Shreyank Jyoti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karthik_S/0/1/0/all/0/1\">Shyamgopal Karthik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_R/0/1/0/all/0/1\">Ramanathan Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_V/0/1/0/all/0/1\">Vineet Gandhi</a>",
          "description": "We propose the ViNet architecture for audio-visual saliency prediction. ViNet\nis a fully convolutional encoder-decoder architecture. The encoder uses visual\nfeatures from a network trained for action recognition, and the decoder infers\na saliency map via trilinear interpolation and 3D convolutions, combining\nfeatures from multiple hierarchies. The overall architecture of ViNet is\nconceptually simple; it is causal and runs in real-time (60 fps). ViNet does\nnot use audio as input and still outperforms the state-of-the-art audio-visual\nsaliency prediction models on nine different datasets (three visual-only and\nsix audio-visual datasets). ViNet also surpasses human performance on the CC,\nSIM and AUC metrics for the AVE dataset, and to our knowledge, it is the first\nnetwork to do so. We also explore a variation of ViNet architecture by\naugmenting audio features into the decoder. To our surprise, upon sufficient\ntraining, the network becomes agnostic to the input audio and provides the same\noutput irrespective of the input. Interestingly, we also observe similar\nbehaviour in the previous state-of-the-art models \\cite{tsiami2020stavis} for\naudio-visual saliency prediction. Our findings contrast with previous works on\ndeep learning-based audio-visual saliency prediction, suggesting a clear avenue\nfor future explorations incorporating audio in a more effective manner. The\ncode and pre-trained models are available at\nhttps://github.com/samyak0210/ViNet.",
          "link": "http://arxiv.org/abs/2012.06170",
          "publishedOn": "2021-08-10T02:00:11.097Z",
          "wordCount": 710,
          "title": "ViNet: Pushing the limits of Visual Modality for Audio-Visual Saliency Prediction. (arXiv:2012.06170v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shipeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiale Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiangwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>",
          "description": "Incremental learning of semantic segmentation has emerged as a promising\nstrategy for visual scene interpretation in the open- world setting. However,\nit remains challenging to acquire novel classes in an online fashion for the\nsegmentation task, mainly due to its continuously-evolving semantic label\nspace, partial pixelwise ground-truth annotations, and constrained data\navailability. To ad- dress this, we propose an incremental learning strategy\nthat can fast adapt deep segmentation models without catastrophic forgetting,\nusing a streaming input data with pixel annotations on the novel classes only.\nTo this end, we develop a uni ed learning strategy based on the\nExpectation-Maximization (EM) framework, which integrates an iterative\nrelabeling strategy that lls in the missing labels and a rehearsal-based\nincremental learning step that balances the stability-plasticity of the model.\nMoreover, our EM algorithm adopts an adaptive sampling method to select\ninformative train- ing data and a class-balancing training strategy in the\nincremental model updates, both improving the e cacy of model learning. We\nvalidate our approach on the PASCAL VOC 2012 and ADE20K datasets, and the\nresults demonstrate its superior performance over the existing incremental\nmethods.",
          "link": "http://arxiv.org/abs/2108.03613",
          "publishedOn": "2021-08-10T02:00:11.072Z",
          "wordCount": 634,
          "title": "An EM Framework for Online Incremental Learning of Semantic Segmentation. (arXiv:2108.03613v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demir_G/0/1/0/all/0/1\">Gozdenur Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cekmis_A/0/1/0/all/0/1\">Asli Cekmis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yesilkaynak_V/0/1/0/all/0/1\">Vahit Bugra Yesilkaynak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_G/0/1/0/all/0/1\">Gozde Unal</a>",
          "description": "Visual design is associated with the use of some basic design elements and\nprinciples. Those are applied by the designers in the various disciplines for\naesthetic purposes, relying on an intuitive and subjective process. Thus,\nnumerical analysis of design visuals and disclosure of the aesthetic value\nembedded in them are considered as hard. However, it has become possible with\nemerging artificial intelligence technologies. This research aims at a neural\nnetwork model, which recognizes and classifies the design principles over\ndifferent domains. The domains include artwork produced since the late 20th\ncentury; professional photos; and facade pictures of contemporary buildings.\nThe data collection and curation processes, including the production of\ncomputationally-based synthetic dataset, is genuine. The proposed model learns\nfrom the knowledge of myriads of original designs, by capturing the underlying\nshared patterns. It is expected to consolidate design processes by providing an\naesthetic evaluation of the visual compositions with objectivity.",
          "link": "http://arxiv.org/abs/2108.04048",
          "publishedOn": "2021-08-10T02:00:11.064Z",
          "wordCount": 603,
          "title": "Detecting Visual Design Principles in Art and Architecture through Deep Convolutional Neural Networks. (arXiv:2108.04048v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1\">Bingzhang Hul Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagnucco_M/0/1/0/all/0/1\">Maurice Pagnucco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yu Guan</a>",
          "description": "Video captioning aims to automatically generate natural language sentences\nthat can describe the visual contents of a given video. Existing generative\nmodels like encoder-decoder frameworks cannot explicitly explore the\nobject-level interactions and frame-level information from complex\nspatio-temporal data to generate semantic-rich captions. Our main contribution\nis to identify three key problems in a joint framework for future video\nsummarization tasks. 1) Enhanced Object Proposal: we propose a novel\nConditional Graph that can fuse spatio-temporal information into latent object\nproposal. 2) Visual Knowledge: Latent Proposal Aggregation is proposed to\ndynamically extract visual words with higher semantic levels. 3) Sentence\nValidation: A novel Discriminative Language Validator is proposed to verify\ngenerated captions so that key semantic concepts can be effectively preserved.\nOur experiments on two public datasets (MVSD and MSR-VTT) manifest significant\nimprovements over state-of-the-art approaches on all metrics, especially for\nBLEU-4 and CIDEr. Our code is available at\nhttps://github.com/baiyang4/D-LSG-Video-Caption.",
          "link": "http://arxiv.org/abs/2108.03662",
          "publishedOn": "2021-08-10T02:00:11.055Z",
          "wordCount": 598,
          "title": "Discriminative Latent Semantic Graph for Video Captioning. (arXiv:2108.03662v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1\">Haozhe Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoteng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1\">Guixiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1\">Liang Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yong Xia</a>",
          "description": "Automated and accurate segmentation of the infected regions in computed\ntomography (CT) images is critical for the prediction of the pathological stage\nand treatment response of COVID-19. Several deep convolutional neural networks\n(DCNNs) have been designed for this task, whose performance, however, tends to\nbe suppressed by their limited local receptive fields and insufficient global\nreasoning ability. In this paper, we propose a pixel-wise sparse graph\nreasoning (PSGR) module and insert it into a segmentation network to enhance\nthe modeling of long-range dependencies for COVID-19 infected region\nsegmentation in CT images. In the PSGR module, a graph is first constructed by\nprojecting each pixel on a node based on the features produced by the\nsegmentation backbone, and then converted into a sparsely-connected graph by\nkeeping only K strongest connections to each uncertain pixel. The long-range\ninformation reasoning is performed on the sparsely-connected graph to generate\nenhanced features. The advantages of this module are two-fold: (1) the\npixel-wise mapping strategy not only avoids imprecise pixel-to-node projections\nbut also preserves the inherent information of each pixel for global reasoning;\nand (2) the sparsely-connected graph construction results in effective\ninformation retrieval and reduction of the noise propagation. The proposed\nsolution has been evaluated against four widely-used segmentation models on\nthree public datasets. The results show that the segmentation model equipped\nwith our PSGR module can effectively segment COVID-19 infected regions in CT\nimages, outperforming all other competing models.",
          "link": "http://arxiv.org/abs/2108.03809",
          "publishedOn": "2021-08-10T02:00:11.048Z",
          "wordCount": 730,
          "title": "PSGR: Pixel-wise Sparse Graph Reasoning for COVID-19 Pneumonia Segmentation in CT Images. (arXiv:2108.03809v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sejersen_J/0/1/0/all/0/1\">Jonas le Fevre Sejersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figueiredo_R/0/1/0/all/0/1\">Rui Pimentel de Figueiredo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kayacan_E/0/1/0/all/0/1\">Erdal Kayacan</a>",
          "description": "In the maritime sector, safe vessel navigation is of great importance,\nparticularly in congested harbors and waterways. The focus of this work is to\nestimate the distance between an object of interest and potential obstacles\nusing a companion UAV. The proposed approach fuses GPS data with long-range\naerial images. First, we employ semantic segmentation DNN for discriminating\nthe vessel of interest, water, and potential solid objects using raw image\ndata. The network is trained with both real and images generated and\nautomatically labeled from a realistic AirSim simulation environment. Then, the\ndistances between the extracted vessel and non-water obstacle blobs are\ncomputed using a novel GSD estimation algorithm. To the best of our knowledge,\nthis work is the first attempt to detect and estimate distances to unknown\nobjects from long-range visual data captured with conventional RGB cameras and\nauxiliary absolute positioning systems (e.g. GPS). The simulation results\nillustrate the accuracy and efficacy of the proposed method for visually aided\nnavigation of vessels assisted by UAV.",
          "link": "http://arxiv.org/abs/2108.03862",
          "publishedOn": "2021-08-10T02:00:11.038Z",
          "wordCount": 634,
          "title": "Safe Vessel Navigation Visually Aided by Autonomous Unmanned Aerial Vehicles in Congested Harbors and Waterways. (arXiv:2108.03862v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qingyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boshen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiayi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>",
          "description": "Recently, the problem of inaccurate learning targets in crowd counting draws\nincreasing attention. Inspired by a few pioneering work, we solve this problem\nby trying to predict the indices of pre-defined interval bins of counts instead\nof the count values themselves. However, an inappropriate interval setting\nmight make the count error contributions from different intervals extremely\nimbalanced, leading to inferior counting performance. Therefore, we propose a\nnovel count interval partition criterion called Uniform Error Partition (UEP),\nwhich always keeps the expected counting error contributions equal for all\nintervals to minimize the prediction risk. Then to mitigate the inevitably\nintroduced discretization errors in the count quantization process, we propose\nanother criterion called Mean Count Proxies (MCP). The MCP criterion selects\nthe best count proxy for each interval to represent its count value during\ninference, making the overall expected discretization error of an image nearly\nnegligible. As far as we are aware, this work is the first to delve into such a\nclassification task and ends up with a promising solution for count interval\npartition. Following the above two theoretically demonstrated criterions, we\npropose a simple yet effective model termed Uniform Error Partition Network\n(UEPNet), which achieves state-of-the-art performance on several challenging\ndatasets. The codes will be available at:\nhttps://github.com/TencentYoutuResearch/CrowdCounting-UEPNet.",
          "link": "http://arxiv.org/abs/2107.12619",
          "publishedOn": "2021-08-10T02:00:11.008Z",
          "wordCount": 705,
          "title": "Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting. (arXiv:2107.12619v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Duo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>",
          "description": "Domain adaptation is critical for success when confronting with the lack of\nannotations in a new domain. As the huge time consumption of labeling process\non 3D point cloud, domain adaptation for 3D semantic segmentation is of great\nexpectation. With the rise of multi-modal datasets, large amount of 2D images\nare accessible besides 3D point clouds. In light of this, we propose to further\nleverage 2D data for 3D domain adaptation by intra and inter domain cross modal\nlearning. As for intra-domain cross modal learning, most existing works sample\nthe dense 2D pixel-wise features into the same size with sparse 3D point-wise\nfeatures, resulting in the abandon of numerous useful 2D features. To address\nthis problem, we propose Dynamic sparse-to-dense Cross Modal Learning (DsCML)\nto increase the sufficiency of multi-modality information interaction for\ndomain adaptation. For inter-domain cross modal learning, we further advance\nCross Modal Adversarial Learning (CMAL) on 2D and 3D data which contains\ndifferent semantic content aiming to promote high-level modal complementarity.\nWe evaluate our model under various multi-modality domain adaptation settings\nincluding day-to-night, country-to-country and dataset-to-dataset, brings large\nimprovements over both uni-modal and multi-modal domain adaptation methods on\nall settings.",
          "link": "http://arxiv.org/abs/2107.14724",
          "publishedOn": "2021-08-10T02:00:11.000Z",
          "wordCount": null,
          "title": "Sparse-to-dense Feature Matching: Intra and Inter domain Cross-modal Learning in Domain Adaptation for 3D Semantic Segmentation. (arXiv:2107.14724v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yicheng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Cheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiahui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yongqi Sun</a>",
          "description": "Recovering 3D human pose from 2D joints is still a challenging problem,\nespecially without any 3D annotation, video information, or multi-view\ninformation. In this paper, we present an unsupervised GAN-based model\nconsisting of multiple weight-sharing generators to estimate a 3D human pose\nfrom a single image without 3D annotations. In our model, we introduce\nsingle-view-multi-angle consistency (SVMAC) to significantly improve the\nestimation performance. With 2D joint locations as input, our model estimates a\n3D pose and a camera simultaneously. During training, the estimated 3D pose is\nrotated by random angles and the estimated camera projects the rotated 3D poses\nback to 2D. The 2D reprojections will be fed into weight-sharing generators to\nestimate the corresponding 3D poses and cameras, which are then mixed to impose\nSVMAC constraints to self-supervise the training process. The experimental\nresults show that our method outperforms the state-of-the-art unsupervised\nmethods by 2.6% on Human 3.6M and 15.0% on MPI-INF-3DHP. Moreover, qualitative\nresults on MPII and LSP show that our method can generalize well to unknown\ndata.",
          "link": "http://arxiv.org/abs/2106.05616",
          "publishedOn": "2021-08-10T02:00:10.993Z",
          "wordCount": 648,
          "title": "SVMAC: Unsupervised 3D Human Pose Estimation from a Single Image with Single-view-multi-angle Consistenty. (arXiv:2106.05616v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Han Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cen_J/0/1/0/all/0/1\">Jun Cen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ningzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Dong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>",
          "description": "The semantic representation of deep features is essential for image context\nunderstanding, and effective fusion of features with different semantic\nrepresentations can significantly improve the model's performance on salient\nobject detection. In this paper, a novel method called MPI is proposed for\nsalient object detection. Firstly, a multi-receptive enhancement module (MRE)\nis designed to effectively expand the receptive fields of features from\ndifferent layers and generate features with different receptive fields. MRE can\nenhance the semantic representation and improve the model's perception of the\nimage context, which enables the model to locate the salient object accurately.\nSecondly, in order to reduce the reuse of redundant information in the complex\ntop-down fusion method and weaken the differences between semantic features, a\nrelatively simple but effective parallel fusion strategy (PFS) is proposed. It\nallows multi-scale features to better interact with each other, thus improving\nthe overall performance of the model. Experimental results on multiple datasets\ndemonstrate that the proposed method outperforms state-of-the-art methods under\ndifferent evaluation metrics.",
          "link": "http://arxiv.org/abs/2108.03618",
          "publishedOn": "2021-08-10T02:00:10.985Z",
          "wordCount": 617,
          "title": "MPI: Multi-receptive and Parallel Integration for Salient Object Detection. (arXiv:2108.03618v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zizhuang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qingtian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_C/0/1/0/all/0/1\">Chen Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yisong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoping Wang</a>",
          "description": "In this paper, we present a novel recurrent multi-view stereo network based\non long short-term memory (LSTM) with adaptive aggregation, namely AA-RMVSNet.\nWe firstly introduce an intra-view aggregation module to adaptively extract\nimage features by using context-aware convolution and multi-scale aggregation,\nwhich efficiently improves the performance on challenging regions, such as thin\nobjects and large low-textured surfaces. To overcome the difficulty of varying\nocclusion in complex scenes, we propose an inter-view cost volume aggregation\nmodule for adaptive pixel-wise view aggregation, which is able to preserve\nbetter-matched pairs among all views. The two proposed adaptive aggregation\nmodules are lightweight, effective and complementary regarding improving the\naccuracy and completeness of 3D reconstruction. Instead of conventional 3D\nCNNs, we utilize a hybrid network with recurrent structure for cost volume\nregularization, which allows high-resolution reconstruction and finer\nhypothetical plane sweep. The proposed network is trained end-to-end and\nachieves excellent performance on various datasets. It ranks $1^{st}$ among all\nsubmissions on Tanks and Temples benchmark and achieves competitive results on\nDTU dataset, which exhibits strong generalizability and robustness.\nImplementation of our method is available at\nhttps://github.com/QT-Zhu/AA-RMVSNet.",
          "link": "http://arxiv.org/abs/2108.03824",
          "publishedOn": "2021-08-10T02:00:10.977Z",
          "wordCount": 618,
          "title": "AA-RMVSNet: Adaptive Aggregation Recurrent Multi-view Stereo Network. (arXiv:2108.03824v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parate_M/0/1/0/all/0/1\">Mayur R. Parate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhurchandi_K/0/1/0/all/0/1\">Kishor M. Bhurchandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1\">Ashwin G. Kothari</a>",
          "description": "Intelligent resident surveillance is one of the most essential smart\ncommunity services. The increasing demand for security needs surveillance\nsystems to be able to detect anomalies in surveillance scenes. Employing\nhigh-capacity computational devices for intelligent surveillance in residential\nsocieties is costly and not feasible. Therefore, we propose anomaly detection\nfor intelligent surveillance using CPU-only edge devices. A modular framework\nto capture object-level inferences and tracking is developed. To cope with\npartial occlusions, posture deformations, and complex scenes, we employed\nfeature encoding and trajectory association governed by two metrices\ncomplementing to each other. The elements of an anomaly detection framework are\noptimized to run on CPU-only edge devices with sufficient frames per second\n(FPS). The experimental results indicate the proposed method is feasible and\nachieves satisfactory results in real-life scenarios.",
          "link": "http://arxiv.org/abs/2107.04767",
          "publishedOn": "2021-08-10T02:00:10.969Z",
          "wordCount": null,
          "title": "Anomaly Detection in Residential Video Surveillance on Edge Devices in IoT Framework. (arXiv:2107.04767v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maracani_A/0/1/0/all/0/1\">Andrea Maracani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michieli_U/0/1/0/all/0/1\">Umberto Michieli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toldo_M/0/1/0/all/0/1\">Marco Toldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanuttigh_P/0/1/0/all/0/1\">Pietro Zanuttigh</a>",
          "description": "Deep networks allow to obtain outstanding results in semantic segmentation,\nhowever they need to be trained in a single shot with a large amount of data.\nContinual learning settings where new classes are learned in incremental steps\nand previous training data is no longer available are challenging due to the\ncatastrophic forgetting phenomenon. Existing approaches typically fail when\nseveral incremental steps are performed or in presence of a distribution shift\nof the background class. We tackle these issues by recreating no longer\navailable data for the old classes and outlining a content inpainting scheme on\nthe background class. We propose two sources for replay data. The first resorts\nto a generative adversarial network to sample from the class space of past\nlearning steps. The second relies on web-crawled data to retrieve images\ncontaining examples of old classes from online databases. In both scenarios no\nsamples of past steps are stored, thus avoiding privacy concerns. Replay data\nare then blended with new samples during the incremental steps. Our approach,\nRECALL, outperforms state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.03673",
          "publishedOn": "2021-08-10T02:00:10.961Z",
          "wordCount": 611,
          "title": "RECALL: Replay-based Continual Learning in Semantic Segmentation. (arXiv:2108.03673v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Cheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Abhishek Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzey_J/0/1/0/all/0/1\">Joseph Linzey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Rushikesh Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sung Jik Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Sudharsan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alber_D/0/1/0/all/0/1\">Daniel Alber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondepudi_A/0/1/0/all/0/1\">Akhil Kondepudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urias_E/0/1/0/all/0/1\">Esteban Urias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandian_B/0/1/0/all/0/1\">Balaji Pandian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Holou_W/0/1/0/all/0/1\">Wajd Al-Holou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_S/0/1/0/all/0/1\">Steve Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_B/0/1/0/all/0/1\">B. Gregory Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heth_J/0/1/0/all/0/1\">Jason Heth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freudiger_C/0/1/0/all/0/1\">Chris Freudiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalsa_S/0/1/0/all/0/1\">Siri Khalsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacione_D/0/1/0/all/0/1\">Donato Pacione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golfinos_J/0/1/0/all/0/1\">John G. Golfinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camelo_Piragua_S/0/1/0/all/0/1\">Sandra Camelo-Piragua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orringer_D/0/1/0/all/0/1\">Daniel A. Orringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollon_T/0/1/0/all/0/1\">Todd Hollon</a>",
          "description": "Background: Accurate diagnosis of skull base tumors is essential for\nproviding personalized surgical treatment strategies. Intraoperative diagnosis\ncan be challenging due to tumor diversity and lack of intraoperative pathology\nresources.\n\nObjective: To develop an independent and parallel intraoperative pathology\nworkflow that can provide rapid and accurate skull base tumor diagnoses using\nlabel-free optical imaging and artificial intelligence (AI).\n\nMethod: We used a fiber laser-based, label-free, non-consumptive,\nhigh-resolution microscopy method ($<$ 60 sec per 1 $\\times$ 1 mm$^\\text{2}$),\ncalled stimulated Raman histology (SRH), to image a consecutive, multicenter\ncohort of skull base tumor patients. SRH images were then used to train a\nconvolutional neural network (CNN) model using three representation learning\nstrategies: cross-entropy, self-supervised contrastive learning, and supervised\ncontrastive learning. Our trained CNN models were tested on a held-out,\nmulticenter SRH dataset.\n\nResults: SRH was able to image the diagnostic features of both benign and\nmalignant skull base tumors. Of the three representation learning strategies,\nsupervised contrastive learning most effectively learned the distinctive and\ndiagnostic SRH image features for each of the skull base tumor types. In our\nmulticenter testing set, cross-entropy achieved an overall diagnostic accuracy\nof 91.5%, self-supervised contrastive learning 83.9%, and supervised\ncontrastive learning 96.6%. Our trained model was able to identify tumor-normal\nmargins and detect regions of microscopic tumor infiltration in whole-slide SRH\nimages.\n\nConclusion: SRH with AI models trained using contrastive representation\nlearning can provide rapid and accurate intraoperative diagnosis of skull base\ntumors.",
          "link": "http://arxiv.org/abs/2108.03555",
          "publishedOn": "2021-08-10T02:00:10.940Z",
          "wordCount": 738,
          "title": "Contrastive Representation Learning for Rapid Intraoperative Diagnosis of Skull Base Tumors Imaged Using Stimulated Raman Histology. (arXiv:2108.03555v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Surace_L/0/1/0/all/0/1\">Luca Surace</a> (Universit&#xe0; della Svizzera italiana), <a href=\"http://arxiv.org/find/cs/1/au:+Wernikowski_M/0/1/0/all/0/1\">Marek Wernikowski</a> (West Pomeranian University of Technology), <a href=\"http://arxiv.org/find/cs/1/au:+Tursun_O/0/1/0/all/0/1\">Okan Tursun</a> (Universit&#xe0; della Svizzera italiana), <a href=\"http://arxiv.org/find/cs/1/au:+Myszkowski_K/0/1/0/all/0/1\">Karol Myszkowski</a> (Max Planck Institute for Informatics), <a href=\"http://arxiv.org/find/cs/1/au:+Mantiuk_R/0/1/0/all/0/1\">Rados&#x142;aw Mantiuk</a> (West Pomeranian University of Technology), <a href=\"http://arxiv.org/find/cs/1/au:+Didyk_P/0/1/0/all/0/1\">Piotr Didyk</a> (Universit&#xe0; della Svizzera italiana)",
          "description": "Foveated image reconstruction recovers full image from a sparse set of\nsamples distributed according to the human visual system's retinal sensitivity\nthat rapidly drops with eccentricity. Recently, the use of Generative\nAdversarial Networks was shown to be a promising solution for such a task as\nthey can successfully hallucinate missing image information. Like for other\nsupervised learning approaches, also for this one, the definition of the loss\nfunction and training strategy heavily influences the output quality. In this\nwork, we pose the question of how to efficiently guide the training of foveated\nreconstruction techniques such that they are fully aware of the human visual\nsystem's capabilities and limitations, and therefore, reconstruct visually\nimportant image features. Due to the nature of GAN-based solutions, we\nconcentrate on the human's sensitivity to hallucination for different input\nsample densities. We present new psychophysical experiments, a dataset, and a\nprocedure for training foveated image reconstruction. The strategy provides\nflexibility to the generator network by penalizing only perceptually important\ndeviations in the output. As a result, the method aims to preserve perceived\nimage statistics rather than natural image statistics. We evaluate our strategy\nand compare it to alternative solutions using a newly trained objective metric\nand user experiments.",
          "link": "http://arxiv.org/abs/2108.03499",
          "publishedOn": "2021-08-10T02:00:10.930Z",
          "wordCount": 666,
          "title": "Learning Foveated Reconstruction to Preserve Perceived Image Statistics. (arXiv:2108.03499v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuewei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_L/0/1/0/all/0/1\">Liangli Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1\">Rick Siow Mong Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingen Liu</a>",
          "description": "In this paper, we propose to utilize Automated Machine Learning to adaptively\nsearch a neural architecture for deepfake detection. This is the first time to\nemploy automated machine learning for deepfake detection. Based on our explored\nsearch space, our proposed method achieves competitive prediction accuracy\ncompared to previous methods. To improve the generalizability of our method,\nespecially when training data and testing data are manipulated by different\nmethods, we propose a simple yet effective strategy in our network learning\nprocess: making it to estimate potential manipulation regions besides\npredicting the real/fake labels. Unlike previous works manually design neural\nnetworks, our method can relieve us from the high labor cost in network\nconstruction. More than that, compared to previous works, our method depends\nmuch less on prior knowledge, e.g., which manipulation method is utilized or\nwhere exactly the fake image is manipulated. Extensive experimental results on\ntwo benchmark datasets demonstrate the effectiveness of our proposed method for\ndeepfake detection.",
          "link": "http://arxiv.org/abs/2106.10705",
          "publishedOn": "2021-08-10T02:00:10.920Z",
          "wordCount": 621,
          "title": "Automated Deepfake Detection. (arXiv:2106.10705v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1\">Sakib Shahriar</a>",
          "description": "\"Art is the lie that enables us to realize the truth.\" - Pablo Picasso. For\ncenturies, humans have dedicated themselves to producing arts to convey their\nimagination. The advancement in technology and deep learning in particular, has\ncaught the attention of many researchers trying to investigate whether art\ngeneration is possible by computers and algorithms. Using generative\nadversarial networks (GANs), applications such as synthesizing photorealistic\nhuman faces and creating captions automatically from images were realized. This\nsurvey takes a comprehensive look at the recent works using GANs for generating\nvisual arts, music, and literary text. A performance comparison and description\nof the various GAN architecture are also presented. Finally, some of the key\nchallenges in art generation using GANs are highlighted along with\nrecommendations for future work.",
          "link": "http://arxiv.org/abs/2108.03857",
          "publishedOn": "2021-08-10T02:00:10.910Z",
          "wordCount": 597,
          "title": "GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network. (arXiv:2108.03857v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaoning Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yuanqi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuehu Liu</a>",
          "description": "Fundamental machine learning theory shows that different samples contribute\nunequally both in learning and testing processes. Contemporary studies on DNN\nimply that such sample di?erence is rooted on the distribution of intrinsic\npattern information, namely sample regularity. Motivated by the recent\ndiscovery on network memorization and generalization, we proposed a pair of\nsample regularity measures for both processes with a formulation-consistent\nrepresentation. Specifically, cumulative binary training/generalizing loss\n(CBTL/CBGL), the cumulative number of correct classi?cations of the\ntraining/testing sample within training stage, is proposed to quantize the\nstability in memorization-generalization process; while\nforgetting/mal-generalizing events, i.e., the mis-classification of previously\nlearned or generalized sample, are utilized to represent the uncertainty of\nsample regularity with respect to optimization dynamics. Experiments validated\nthe effectiveness and robustness of the proposed approaches for mini-batch SGD\noptimization. Further applications on training/testing sample selection show\nthe proposed measures sharing the uni?ed computing procedure could benefit for\nboth tasks.",
          "link": "http://arxiv.org/abs/2108.03913",
          "publishedOn": "2021-08-10T02:00:10.876Z",
          "wordCount": 598,
          "title": "Unified Regularity Measures for Sample-wise Learning and Generalization. (arXiv:2108.03913v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03642",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Toader_B/0/1/0/all/0/1\">Bogdan Toader</a>, <a href=\"http://arxiv.org/find/math/1/au:+Boulanger_J/0/1/0/all/0/1\">Jerome Boulanger</a>, <a href=\"http://arxiv.org/find/math/1/au:+Korolev_Y/0/1/0/all/0/1\">Yury Korolev</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lenz_M/0/1/0/all/0/1\">Martin O. Lenz</a>, <a href=\"http://arxiv.org/find/math/1/au:+Manton_J/0/1/0/all/0/1\">James Manton</a>, <a href=\"http://arxiv.org/find/math/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Schonlieb</a>, <a href=\"http://arxiv.org/find/math/1/au:+Muresan_L/0/1/0/all/0/1\">Leila Muresan</a>",
          "description": "We study the problem of deconvolution for light-sheet microscopy, where the\ndata is corrupted by spatially varying blur and a combination of Poisson and\nGaussian noise. The spatial variation of the point spread function (PSF) of a\nlight-sheet microscope is determined by the interaction between the excitation\nsheet and the detection objective PSF. First, we introduce a model of the image\nformation process that incorporates this interaction, therefore capturing the\nmain characteristics of this imaging modality. Then, we formulate a variational\nmodel that accounts for the combination of Poisson and Gaussian noise through a\ndata fidelity term consisting of the infimal convolution of the single noise\nfidelities, first introduced in L. Calatroni et al. \"Infimal convolution of\ndata discrepancies for mixed noise removal\", SIAM Journal on Imaging Sciences\n10.3 (2017), 1196-1233. We establish convergence rates in a Bregman distance\nunder a source condition for the infimal convolution fidelity and a discrepancy\nprinciple for choosing the value of the regularisation parameter. The inverse\nproblem is solved by applying the primal-dual hybrid gradient (PDHG) algorithm\nin a novel way. Finally, numerical experiments performed on both simulated and\nreal data show superior reconstruction results in comparison with other\nmethods.",
          "link": "http://arxiv.org/abs/2108.03642",
          "publishedOn": "2021-08-10T02:00:10.868Z",
          "wordCount": 656,
          "title": "Image reconstruction in light-sheet microscopy: spatially varying deconvolution and mixed noise. (arXiv:2108.03642v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abouelnaga_Y/0/1/0/all/0/1\">Yehya Abouelnaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_M/0/1/0/all/0/1\">Mai Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilic_S/0/1/0/all/0/1\">Slobodan Ilic</a>",
          "description": "We propose a lightweight retrieval-based pipeline to predict 6DOF camera\nposes from RGB images. Our pipeline uses a convolutional neural network (CNN)\nto encode a query image as a feature vector. A nearest neighbor lookup finds\nthe pose-wise nearest database image. A siamese convolutional neural network\nregresses the relative pose from the nearest neighboring database image to the\nquery image. The relative pose is then applied to the nearest neighboring\nabsolute pose to obtain the query image's final absolute pose prediction. Our\nmodel is a distilled version of NN-Net that reduces its parameters by 98.87%,\ninformation retrieval feature vector size by 87.5%, and inference time by\n89.18% without a significant decrease in localization accuracy.",
          "link": "http://arxiv.org/abs/2108.03819",
          "publishedOn": "2021-08-10T02:00:10.862Z",
          "wordCount": 545,
          "title": "DistillPose: Lightweight Camera Localization Using Auxiliary Learning. (arXiv:2108.03819v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12476",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Meina Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>",
          "description": "Recent studies on Generative Adversarial Network (GAN) reveal that different\nlayers of a generative CNN hold different semantics of the synthesized images.\nHowever, few GAN models have explicit dimensions to control the semantic\nattributes represented in a specific layer. This paper proposes EigenGAN which\nis able to unsupervisedly mine interpretable and controllable dimensions from\ndifferent generator layers. Specifically, EigenGAN embeds one linear subspace\nwith orthogonal basis into each generator layer. Via generative adversarial\ntraining to learn a target distribution, these layer-wise subspaces\nautomatically discover a set of \"eigen-dimensions\" at each layer corresponding\nto a set of semantic attributes or interpretable variations. By traversing the\ncoefficient of a specific eigen-dimension, the generator can produce samples\nwith continuous changes corresponding to a specific semantic attribute. Taking\nthe human face for example, EigenGAN can discover controllable dimensions for\nhigh-level concepts such as pose and gender in the subspace of deep layers, as\nwell as low-level concepts such as hue and color in the subspace of shallow\nlayers. Moreover, in the linear case, we theoretically prove that our algorithm\nderives the principal components as PCA does. Codes can be found in\nhttps://github.com/LynnHo/EigenGAN-Tensorflow.",
          "link": "http://arxiv.org/abs/2104.12476",
          "publishedOn": "2021-08-10T02:00:10.852Z",
          "wordCount": null,
          "title": "EigenGAN: Layer-Wise Eigen-Learning for GANs. (arXiv:2104.12476v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoteng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1\">Haozhe Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1\">Liang Zhan</a>",
          "description": "In this paper, we propose a Boundary-aware Graph Reasoning (BGR) module to\nlearn long-range contextual features for semantic segmentation. Rather than\ndirectly construct the graph based on the backbone features, our BGR module\nexplores a reasonable way to combine segmentation erroneous regions with the\ngraph construction scenario. Motivated by the fact that most hard-to-segment\npixels broadly distribute on boundary regions, our BGR module uses the boundary\nscore map as prior knowledge to intensify the graph node connections and\nthereby guide the graph reasoning focus on boundary regions. In addition, we\nemploy an efficient graph convolution implementation to reduce the\ncomputational cost, which benefits the integration of our BGR module into\ncurrent segmentation backbones. Extensive experiments on three challenging\nsegmentation benchmarks demonstrate the effectiveness of our proposed BGR\nmodule for semantic segmentation.",
          "link": "http://arxiv.org/abs/2108.03791",
          "publishedOn": "2021-08-10T02:00:10.846Z",
          "wordCount": 567,
          "title": "Boundary-aware Graph Reasoning for Semantic Segmentation. (arXiv:2108.03791v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zikun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1\">Wenjie Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenyu He</a>",
          "description": "Most existing trackers based on deep learning perform tracking in a holistic\nstrategy, which aims to learn deep representations of the whole target for\nlocalizing the target. It is arduous for such methods to track targets with\nvarious appearance variations. To address this limitation, another type of\nmethods adopts a part-based tracking strategy which divides the target into\nequal patches and tracks all these patches in parallel. The target state is\ninferred by summarizing the tracking results of these patches. A potential\nlimitation of such trackers is that not all patches are equally informative for\ntracking. Some patches that are not discriminative may have adverse effects. In\nthis paper, we propose to track the salient local parts of the target that are\ndiscriminative for tracking. In particular, we propose a fine-grained saliency\nmining module to capture the local saliencies. Further, we design a\nsaliency-association modeling module to associate the captured saliencies\ntogether to learn effective correlation representations between the exemplar\nand the search image for state estimation. Extensive experiments on five\ndiverse datasets demonstrate that the proposed method performs favorably\nagainst state-of-the-art trackers.",
          "link": "http://arxiv.org/abs/2108.03637",
          "publishedOn": "2021-08-10T02:00:10.790Z",
          "wordCount": 617,
          "title": "Saliency-Associated Object Tracking. (arXiv:2108.03637v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_R/0/1/0/all/0/1\">Rui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srijan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Francois Bremond</a>",
          "description": "In video understanding, most cross-modal knowledge distillation (KD) methods\nare tailored for classification tasks, focusing on the discriminative\nrepresentation of the trimmed videos. However, action detection requires not\nonly categorizing actions, but also localizing them in untrimmed videos.\nTherefore, transferring knowledge pertaining to temporal relations is critical\nfor this task which is missing in the previous cross-modal KD frameworks. To\nthis end, we aim at learning an augmented RGB representation for action\ndetection, taking advantage of additional modalities at training time through\nKD. We propose a KD framework consisting of two levels of distillation. On one\nhand, atomic-level distillation encourages the RGB student to learn the\nsub-representation of the actions from the teacher in a contrastive manner. On\nthe other hand, sequence-level distillation encourages the student to learn the\ntemporal knowledge from the teacher, which consists of transferring the Global\nContextual Relations and the Action Boundary Saliency. The result is an\nAugmented-RGB stream that can achieve competitive performance as the two-stream\nnetwork while using only RGB at inference time. Extensive experimental analysis\nshows that our proposed distillation framework is generic and outperforms other\npopular cross-modal distillation methods in action detection task.",
          "link": "http://arxiv.org/abs/2108.03619",
          "publishedOn": "2021-08-10T02:00:10.765Z",
          "wordCount": null,
          "title": "Learning an Augmented RGB Representation with Cross-Modal Knowledge Distillation for Action Detection. (arXiv:2108.03619v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03746",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chao_C/0/1/0/all/0/1\">Chen Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1\">Matthias Zwicker</a>",
          "description": "Learning to generate 3D point clouds without 3D supervision is an important\nbut challenging problem. Current solutions leverage various differentiable\nrenderers to project the generated 3D point clouds onto a 2D image plane, and\ntrain deep neural networks using the per-pixel difference with 2D ground truth\nimages. However, these solutions are still struggling to fully recover fine\nstructures of 3D shapes, such as thin tubes or planes. To resolve this issue,\nwe propose an unsupervised approach for 3D point cloud generation with fine\nstructures. Specifically, we cast 3D point cloud learning as a 2D projection\nmatching problem. Rather than using entire 2D silhouette images as a regular\npixel supervision, we introduce structure adaptive sampling to randomly sample\n2D points within the silhouettes as an irregular point supervision, which\nalleviates the consistency issue of sampling from different view angles. Our\nmethod pushes the neural network to generate a 3D point cloud whose 2D\nprojections match the irregular point supervision from different view angles.\nOur 2D projection matching approach enables the neural network to learn more\naccurate structure information than using the per-pixel difference, especially\nfor fine and thin 3D structures. Our method can recover fine 3D structures from\n2D silhouette images at different resolutions, and is robust to different\nsampling methods and point number in irregular point supervision. Our method\noutperforms others under widely used benchmarks. Our code, data and models are\navailable at https://github.com/chenchao15/2D\\_projection\\_matching.",
          "link": "http://arxiv.org/abs/2108.03746",
          "publishedOn": "2021-08-10T02:00:10.765Z",
          "wordCount": null,
          "title": "Unsupervised Learning of Fine Structure Generation for 3D Point Clouds by 2D Projection Matching. (arXiv:2108.03746v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gammulle_H/0/1/0/all/0/1\">Harshala Gammulle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_T/0/1/0/all/0/1\">Tharindu Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>",
          "description": "This paper presents a novel lightweight COVID-19 diagnosis framework using CT\nscans. Our system utilises a novel two-stage approach to generate robust and\nefficient diagnoses across heterogeneous patient level inputs. We use a\npowerful backbone network as a feature extractor to capture discriminative\nslice-level features. These features are aggregated by a lightweight network to\nobtain a patient level diagnosis. The aggregation network is carefully designed\nto have a small number of trainable parameters while also possessing sufficient\ncapacity to generalise to diverse variations within different CT volumes and to\nadapt to noise introduced during the data acquisition. We achieve a significant\nperformance increase over the baselines when benchmarked on the SPGC COVID-19\nRadiomics Dataset, despite having only 2.5 million trainable parameters and\nrequiring only 0.623 seconds on average to process a single patient's CT volume\nusing an Nvidia-GeForce RTX 2080 GPU.",
          "link": "http://arxiv.org/abs/2108.03786",
          "publishedOn": "2021-08-10T02:00:10.757Z",
          "wordCount": null,
          "title": "Multi-Slice Net: A novel light weight framework for COVID-19 Diagnosis. (arXiv:2108.03786v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jungang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1\">Wei An</a>",
          "description": "Camera arrays provide spatial and angular information within a single\nsnapshot. With refocusing methods, focal planes can be altered after exposure.\nIn this letter, we propose a light field refocusing method to improve the\nimaging quality of camera arrays. In our method, the disparity is first\nestimated. Then, the unfocused region (bokeh) is rendered by using a\ndepth-based anisotropic filter. Finally, the refocused image is produced by a\nreconstruction-based superresolution approach where the bokeh image is used as\na regularization term. Our method can selectively refocus images with focused\nregion being superresolved and bokeh being aesthetically rendered. Our method\nalso enables postadjustment of depth of field. We conduct experiments on both\npublic and self-developed datasets. Our method achieves superior visual\nperformance with acceptable computational cost as compared to other\nstate-of-the-art methods. Code is available at\nhttps://github.com/YingqianWang/Selective-LF-Refocusing.",
          "link": "http://arxiv.org/abs/2108.03918",
          "publishedOn": "2021-08-10T02:00:10.749Z",
          "wordCount": null,
          "title": "Selective Light Field Refocusing for Camera Arrays Using Bokeh Rendering and Superresolution. (arXiv:2108.03918v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lizarraga_A/0/1/0/all/0/1\">Andrew Lizarraga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">David Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubicki_A/0/1/0/all/0/1\">Antoni Kubicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahib_A/0/1/0/all/0/1\">Ashish Sahib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nunez_E/0/1/0/all/0/1\">Elvis Nunez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narr_K/0/1/0/all/0/1\">Katherine Narr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Shantanu H. Joshi</a>",
          "description": "We present a geometric framework for aligning white matter fiber tracts. By\nregistering fiber tracts between brains, one expects to see overlap of\nanatomical structures that often provide meaningful comparisons across\nsubjects. However, the geometry of white matter tracts is highly heterogeneous,\nand finding direct tract-correspondence across multiple individuals remains a\nchallenging problem. We present a novel deformation metric between tracts that\nallows one to compare tracts while simultaneously obtaining a registration. To\naccomplish this, fiber tracts are represented by an intrinsic mean along with\nthe deformation fields represented by tangent vectors from the mean. In this\nsetting, one can determine a parallel transport between tracts and then\nregister corresponding tangent vectors. We present the results of bundle\nalignment on a population of 43 healthy adult subjects.",
          "link": "http://arxiv.org/abs/2108.03697",
          "publishedOn": "2021-08-10T02:00:10.746Z",
          "wordCount": null,
          "title": "Alignment of Tractography Streamlines using Deformation Transfer via Parallel Transport. (arXiv:2108.03697v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yecong Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuanshuo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1\">Mingwen Shao</a>",
          "description": "Rain removal plays an important role in the restoration of degraded images.\nRecently, data-driven methods have achieved remarkable success. However, these\napproaches neglect that the appearance of rain is often accompanied by low\nlight conditions, which will further degrade the image quality. Therefore, it\nis very indispensable to jointly remove the rain and enhance the light for\nreal-world rain image restoration. In this paper, we aim to address this\nproblem from two aspects. First, we proposed a novel entangled network, namely\nEMNet, which can remove the rain and enhance illumination in one go.\nSpecifically, two encoder-decoder networks interact complementary information\nthrough entanglement structure, and parallel rain removal and illumination\nenhancement. Considering that the encoder-decoder structure is unreliable in\npreserving spatial details, we employ a detail recovery network to restore the\ndesired fine texture. Second, we present a new synthetic dataset, namely\nDarkRain, to boost the development of rain image restoration algorithms in\npractical scenarios. DarkRain not only contains different degrees of rain, but\nalso considers different lighting conditions, and more realistically simulates\nthe rainfall in the real world. EMNet is extensively evaluated on the proposed\nbenchmark and achieves state-of-the-art results. In addition, after a simple\ntransformation, our method outshines existing methods in both rain removal and\nlow-light image enhancement. The source code and dataset will be made publicly\navailable later.",
          "link": "http://arxiv.org/abs/2108.03873",
          "publishedOn": "2021-08-10T02:00:10.743Z",
          "wordCount": null,
          "title": "Rain Removal and Illumination Enhancement Done in One Go. (arXiv:2108.03873v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thoker_F/0/1/0/all/0/1\">Fida Mohammad Thoker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doughty_H/0/1/0/all/0/1\">Hazel Doughty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G.M. Snoek</a>",
          "description": "This paper strives for self-supervised learning of a feature space suitable\nfor skeleton-based action recognition. Our proposal is built upon learning\ninvariances to input skeleton representations and various skeleton\naugmentations via a noise contrastive estimation. In particular, we propose\ninter-skeleton contrastive learning, which learns from multiple different input\nskeleton representations in a cross-contrastive manner. In addition, we\ncontribute several skeleton-specific spatial and temporal augmentations which\nfurther encourage the model to learn the spatio-temporal dynamics of skeleton\ndata. By learning similarities between different skeleton representations as\nwell as augmented views of the same sequence, the network is encouraged to\nlearn higher-level semantics of the skeleton data than when only using the\naugmented views. Our approach achieves state-of-the-art performance for\nself-supervised learning from skeleton data on the challenging PKU and NTU\ndatasets with multiple downstream tasks, including action recognition, action\nretrieval and semi-supervised learning. Code is available at\nhttps://github.com/fmthoker/skeleton-contrast.",
          "link": "http://arxiv.org/abs/2108.03656",
          "publishedOn": "2021-08-10T02:00:10.683Z",
          "wordCount": null,
          "title": "Skeleton-Contrastive 3D Action Representation Learning. (arXiv:2108.03656v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosu_R/0/1/0/all/0/1\">Radu Alexandru Rosu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>",
          "description": "Multi-View Stereo (MVS) is a core task in 3D computer vision. With the surge\nof novel deep learning methods, learned MVS has surpassed the accuracy of\nclassical approaches, but still relies on building a memory intensive dense\ncost volume. Novel View Synthesis (NVS) is a parallel line of research and has\nrecently seen an increase in popularity with Neural Radiance Field (NeRF)\nmodels, which optimize a per scene radiance field. However, NeRF methods do not\ngeneralize to novel scenes and are slow to train and test. We propose to bridge\nthe gap between these two methodologies with a novel network that can recover\n3D scene geometry as a distance function, together with high-resolution color\nimages. Our method uses only a sparse set of images as input and can generalize\nwell to novel scenes. Additionally, we propose a coarse-to-fine sphere tracing\napproach in order to significantly increase speed. We show on various datasets\nthat our method reaches comparable accuracy to per-scene optimized methods\nwhile being able to generalize and running significantly faster.",
          "link": "http://arxiv.org/abs/2108.03880",
          "publishedOn": "2021-08-10T02:00:09.795Z",
          "wordCount": 605,
          "title": "NeuralMVS: Bridging Multi-View Stereo and Novel View Synthesis. (arXiv:2108.03880v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>",
          "description": "In this paper, we introduce a novel task, referred to as Weakly-Supervised\nSpatio-Temporal Anomaly Detection (WSSTAD) in surveillance video. Specifically,\ngiven an untrimmed video, WSSTAD aims to localize a spatio-temporal tube (i.e.,\na sequence of bounding boxes at consecutive times) that encloses the abnormal\nevent, with only coarse video-level annotations as supervision during training.\nTo address this challenging task, we propose a dual-branch network which takes\nas input the proposals with multi-granularities in both spatial-temporal\ndomains. Each branch employs a relationship reasoning module to capture the\ncorrelation between tubes/videolets, which can provide rich contextual\ninformation and complex entity relationships for the concept learning of\nabnormal behaviors. Mutually-guided Progressive Refinement framework is set up\nto employ dual-path mutual guidance in a recurrent manner, iteratively sharing\nauxiliary supervision information across branches. It impels the learned\nconcepts of each branch to serve as a guide for its counterpart, which\nprogressively refines the corresponding branch and the whole framework.\nFurthermore, we contribute two datasets, i.e., ST-UCF-Crime and STRA,\nconsisting of videos containing spatio-temporal abnormal annotations to serve\nas the benchmarks for WSSTAD. We conduct extensive qualitative and quantitative\nevaluations to demonstrate the effectiveness of the proposed approach and\nanalyze the key factors that contribute more to handle this task.",
          "link": "http://arxiv.org/abs/2108.03825",
          "publishedOn": "2021-08-10T02:00:09.667Z",
          "wordCount": null,
          "title": "Weakly-Supervised Spatio-Temporal Anomaly Detection in Surveillance Video. (arXiv:2108.03825v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Williams_W/0/1/0/all/0/1\">Walt Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_J/0/1/0/all/0/1\">Jimmy Hall</a>",
          "description": "Breast Cancer is the most prevalent cancer in the world. The World Health\nOrganization reports that the disease still affects a significant portion of\nthe developing world citing increased mortality rates in the majority of low to\nmiddle income countries. The most popular protocol pathologists use for\ndiagnosing breast cancer is the Nottingham grading system which grades the\nproliferation of tumors based on 3 major criteria, the most important of them\nbeing mitotic cell count. The way in which pathologists evaluate mitotic cell\ncount is to subjectively and qualitatively analyze cells present in stained\nslides of tissue and make a decision on its mitotic state i.e. is it mitotic or\nnot?This process is extremely inefficient and tiring for pathologists and so an\nefficient, accurate, and fully automated tool to aid with the diagnosis is\nextremely desirable. Fortunately, creating such a tool is made significantly\neasier with the AutoML tool available from Microsoft Azure, however to the best\nof our knowledge the AutoML tool has never been formally evaluated for use in\nmitotic cell detection in histopathology images. This paper serves as an\nevaluation of the AutoML tool for this purpose and will provide a first look on\nhow the tool handles this challenging problem. All code is available\nathttps://github.com/WaltAFWilliams/AMDet",
          "link": "http://arxiv.org/abs/2108.03676",
          "publishedOn": "2021-08-10T02:00:09.481Z",
          "wordCount": null,
          "title": "AMDet: A Tool for Mitotic Cell Detection in Histopathology Slides. (arXiv:2108.03676v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.01041",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ekanayake_E/0/1/0/all/0/1\">E.M.M.B. Ekanayake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weerasooriya_H/0/1/0/all/0/1\">H.M.H.K. Weerasooriya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ranasinghe_D/0/1/0/all/0/1\">D.Y.L. Ranasinghe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herath_S/0/1/0/all/0/1\">S. Herath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rathnayake_B/0/1/0/all/0/1\">B. Rathnayake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Godaliyadda_G/0/1/0/all/0/1\">G.M.R.I. Godaliyadda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ekanayake_M/0/1/0/all/0/1\">M.P.B. Ekanayake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herath_H/0/1/0/all/0/1\">H.M.V.R. Herath</a>",
          "description": "Hyperspectral unmixing (HU) has become an important technique in exploiting\nhyperspectral data since it decomposes a mixed pixel into a collection of\nendmembers weighted by fractional abundances. The endmembers of a hyperspectral\nimage (HSI) are more likely to be generated by independent sources and be mixed\nin a macroscopic degree before arriving at the sensor element of the imaging\nspectrometer as mixed spectra. Over the past few decades, many attempts have\nfocused on imposing auxiliary constraints on the conventional nonnegative\nmatrix factorization (NMF) framework in order to effectively unmix these mixed\nspectra. As a promising step toward finding an optimum constraint to extract\nendmembers, this paper presents a novel blind HU algorithm, referred to as\nKurtosis-based Smooth Nonnegative Matrix Factorization (KbSNMF) which\nincorporates a novel constraint based on the statistical independence of the\nprobability density functions of endmember spectra. Imposing this constraint on\nthe conventional NMF framework promotes the extraction of independent\nendmembers while further enhancing the parts-based representation of data.\nExperiments conducted on diverse synthetic HSI datasets (with numerous numbers\nof endmembers, spectral bands, pixels, and noise levels) and three standard\nreal HSI datasets demonstrate the validity of the proposed KbSNMF algorithm\ncompared to several state-of-the-art NMF-based HU baselines. The proposed\nalgorithm exhibits superior performance especially in terms of extracting\nendmember spectra from hyperspectral data; therefore, it could uplift the\nperformance of recent deep learning HU methods which utilize the endmember\nspectra as supervisory input data for abundance extraction.",
          "link": "http://arxiv.org/abs/2003.01041",
          "publishedOn": "2021-08-10T02:00:09.276Z",
          "wordCount": 750,
          "title": "Constrained Nonnegative Matrix Factorization for Blind Hyperspectral Unmixing incorporating Endmember Independence. (arXiv:2003.01041v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rasouli_A/0/1/0/all/0/1\">Amir Rasouli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohani_M/0/1/0/all/0/1\">Mohsen Rohani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jun Luo</a>",
          "description": "Pedestrian behavior prediction is one of the major challenges for intelligent\ndriving systems. Pedestrians often exhibit complex behaviors influenced by\nvarious contextual elements. To address this problem, we propose BiPed, a\nmultitask learning framework that simultaneously predicts trajectories and\nactions of pedestrians by relying on multimodal data. Our method benefits from\n1) a bifold encoding approach where different data modalities are processed\nindependently allowing them to develop their own representations, and jointly\nto produce a representation for all modalities using shared parameters; 2) a\nnovel interaction modeling technique that relies on categorical semantic\nparsing of the scenes to capture interactions between target pedestrians and\ntheir surroundings; and 3) a bifold prediction mechanism that uses both\nindependent and shared decoding of multimodal representations. Using public\npedestrian behavior benchmark datasets for driving, PIE and JAAD, we highlight\nthe benefits of the proposed method for behavior prediction and show that our\nmodel achieves state-of-the-art performance and improves trajectory and action\nprediction by up to 22% and 9% respectively. We further investigate the\ncontributions of the proposed reasoning techniques via extensive ablation\nstudies.",
          "link": "http://arxiv.org/abs/2012.03298",
          "publishedOn": "2021-08-10T02:00:09.250Z",
          "wordCount": 653,
          "title": "Bifold and Semantic Reasoning for Pedestrian Behavior Prediction. (arXiv:2012.03298v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Mingfeng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhi_M/0/1/0/all/0/1\">Minghao Zhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Liying Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaocheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jucheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiahao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>",
          "description": "High-resolution magnetic resonance images can provide fine-grained anatomical\ninformation, but acquiring such data requires a long scanning time. In this\npaper, a framework called the Fused Attentive Generative Adversarial\nNetworks(FA-GAN) is proposed to generate the super-resolution MR image from\nlow-resolution magnetic resonance images, which can reduce the scanning time\neffectively but with high resolution MR images. In the framework of the FA-GAN,\nthe local fusion feature block, consisting of different three-pass networks by\nusing different convolution kernels, is proposed to extract image features at\ndifferent scales. And the global feature fusion module, including the channel\nattention module, the self-attention module, and the fusion operation, is\ndesigned to enhance the important features of the MR image. Moreover, the\nspectral normalization process is introduced to make the discriminator network\nstable. 40 sets of 3D magnetic resonance images (each set of images contains\n256 slices) are used to train the network, and 10 sets of images are used to\ntest the proposed method. The experimental results show that the PSNR and SSIM\nvalues of the super-resolution magnetic resonance image generated by the\nproposed FA-GAN method are higher than the state-of-the-art reconstruction\nmethods.",
          "link": "http://arxiv.org/abs/2108.03920",
          "publishedOn": "2021-08-10T02:00:09.244Z",
          "wordCount": 649,
          "title": "FA-GAN: Fused Attentive Generative Adversarial Networks for MRI Image Super-Resolution. (arXiv:2108.03920v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.09124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuting Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jia-Xin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaowei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>",
          "description": "While self-supervised representation learning (SSL) has received widespread\nattention from the community, recent research argue that its performance will\nsuffer a cliff fall when the model size decreases. The current method mainly\nrelies on contrastive learning to train the network and in this work, we\npropose a simple yet effective Distilled Contrastive Learning (DisCo) to ease\nthe issue by a large margin. Specifically, we find the final embedding obtained\nby the mainstream SSL methods contains the most fruitful information, and\npropose to distill the final embedding to maximally transmit a teacher's\nknowledge to a lightweight model by constraining the last embedding of the\nstudent to be consistent with that of the teacher. In addition, in the\nexperiment, we find that there exists a phenomenon termed Distilling BottleNeck\nand present to enlarge the embedding dimension to alleviate this problem. Our\nmethod does not introduce any extra parameter to lightweight models during\ndeployment. Experimental results demonstrate that our method achieves the\nstate-of-the-art on all lightweight models. Particularly, when\nResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear\nresult of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50,\nbut the number of parameters of EfficientNet-B0 is only 9.4\\%/16.3\\% of\nResNet-101/ResNet-50. Code is available at https://github.\ncom/Yuting-Gao/DisCo-pytorch.",
          "link": "http://arxiv.org/abs/2104.09124",
          "publishedOn": "2021-08-10T02:00:09.199Z",
          "wordCount": 698,
          "title": "DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning. (arXiv:2104.09124v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lihe Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1\">Wei Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>",
          "description": "Few-shot segmentation (FSS) aims to segment unseen classes given only a few\nannotated samples. Existing methods suffer the problem of feature undermining,\ni.e. potential novel classes are treated as background during training phase.\nOur method aims to alleviate this problem and enhance the feature embedding on\nlatent novel classes. In our work, we propose a novel joint-training framework.\nBased on conventional episodic training on support-query pairs, we add an\nadditional mining branch that exploits latent novel classes via transferable\nsub-clusters, and a new rectification technique on both background and\nforeground categories to enforce more stable prototypes. Over and above that,\nour transferable sub-cluster has the ability to leverage extra unlabeled data\nfor further feature enhancement. Extensive experiments on two FSS benchmarks\ndemonstrate that our method outperforms previous state-of-the-art by a large\nmargin of 3.7% mIOU on PASCAL-5i and 7.0% mIOU on COCO-20i at the cost of 74%\nfewer parameters and 2.5x faster inference speed. The source code is available\nat https://github.com/LiheYoung/MiningFSS.",
          "link": "http://arxiv.org/abs/2103.15402",
          "publishedOn": "2021-08-10T02:00:09.187Z",
          "wordCount": 632,
          "title": "Mining Latent Classes for Few-shot Segmentation. (arXiv:2103.15402v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.05690",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+B_L/0/1/0/all/0/1\">Lalith Bharadwaj B</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boddeda_R/0/1/0/all/0/1\">Rohit Boddeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1\">Sai Vardhan K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_M/0/1/0/all/0/1\">Madhu G</a>",
          "description": "The issue of COVID-19, increasing with a massive mortality rate. This led to\nthe WHO declaring it as a pandemic. In this situation, it is crucial to perform\nefficient and fast diagnosis. The reverse transcript polymerase chain reaction\n(RTPCR) test is conducted to detect the presence of SARS-CoV-2. This test is\ntime-consuming and instead chest CT (or Chest X-ray) can be used for a fast and\naccurate diagnosis. Automated diagnosis is considered to be important as it\nreduces human effort and provides accurate and low-cost tests. The\ncontributions of our research are three-fold. First, it is aimed to analyse the\nbehaviour and performance of variant vision models ranging from Inception to\nNAS networks with the appropriate fine-tuning procedure. Second, the behaviour\nof these models is visually analysed by plotting CAMs for individual networks\nand determining classification performance with AUCROC curves. Thirdly, stacked\nensembles techniques are imparted to provide higher generalisation on combining\nthe fine-tuned models, in which six ensemble neural networks are designed by\ncombining the existing fine-tuned networks. Implying these stacked ensembles\nprovides a great generalization to the models. The ensemble model designed by\ncombining all the fine-tuned networks obtained a state-of-the-art accuracy\nscore of 99.17%. The precision and recall for the COVID-19 class are 99.99% and\n89.79% respectively, which resembles the robustness of the stacked ensembles.",
          "link": "http://arxiv.org/abs/2010.05690",
          "publishedOn": "2021-08-10T02:00:09.179Z",
          "wordCount": 769,
          "title": "COVID-19 Classification Using Staked Ensembles: A Comprehensive Analysis. (arXiv:2010.05690v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.09128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eliasof_M/0/1/0/all/0/1\">Moshe Eliasof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ephrath_J/0/1/0/all/0/1\">Jonathan Ephrath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruthotto_L/0/1/0/all/0/1\">Lars Ruthotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treister_E/0/1/0/all/0/1\">Eran Treister</a>",
          "description": "We present a multigrid-in-channels (MGIC) approach that tackles the quadratic\ngrowth of the number of parameters with respect to the number of channels in\nstandard convolutional neural networks (CNNs). Thereby our approach addresses\nthe redundancy in CNNs that is also exposed by the recent success of\nlightweight CNNs. Lightweight CNNs can achieve comparable accuracy to standard\nCNNs with fewer parameters; however, the number of weights still scales\nquadratically with the CNN's width. Our MGIC architectures replace each CNN\nblock with an MGIC counterpart that utilizes a hierarchy of nested grouped\nconvolutions of small group size to address this.\n\nHence, our proposed architectures scale linearly with respect to the\nnetwork's width while retaining full coupling of the channels as in standard\nCNNs.\n\nOur extensive experiments on image classification, segmentation, and point\ncloud classification show that applying this strategy to different\narchitectures like ResNet and MobileNetV3 reduces the number of parameters\nwhile obtaining similar or better accuracy.",
          "link": "http://arxiv.org/abs/2011.09128",
          "publishedOn": "2021-08-10T02:00:09.172Z",
          "wordCount": 638,
          "title": "MGIC: Multigrid-in-Channels Neural Network Architectures. (arXiv:2011.09128v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1707.08148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Afsheen Rafaqat Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohsen Ali</a>",
          "description": "Current image transformation and recoloring algorithms try to introduce\nartistic effects in the photographed images, based on user input of target\nimage(s) or selection of pre-designed filters. These manipulations, although\nintended to enhance the impact of an image on the viewer, do not include the\noption of image transformation by specifying the affect information. In this\npaper we present an automatic image-transformation method that transforms the\nsource image such that it can induce an emotional affect on the viewer, as\ndesired by the user. Our proposed novel image emotion transfer algorithm does\nnot require a user-specified target image. The proposed algorithm uses features\nextracted from top layers of deep convolutional neural network and the\nuser-specified emotion distribution to select multiple target images from an\nimage database for color transformation, such that the resultant image has\ndesired emotional impact. Our method can handle more diverse set of photographs\nthan the previous methods. We conducted a detailed user study showing the\neffectiveness of our proposed method. A discussion and reasoning of failure\ncases has also been provided, indicating inherent limitation of color-transfer\nbased methods in the use of emotion assignment.\n\nProject Page: this http URL",
          "link": "http://arxiv.org/abs/1707.08148",
          "publishedOn": "2021-08-10T02:00:09.140Z",
          "wordCount": 671,
          "title": "Automatic Image Transformation for Inducing Affect. (arXiv:1707.08148v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04016",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lalande_A/0/1/0/all/0/1\">Alain Lalande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pommier_T/0/1/0/all/0/1\">Thibaut Pommier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decourselle_T/0/1/0/all/0/1\">Thomas Decourselle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qayyum_A/0/1/0/all/0/1\">Abdul Qayyum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salomon_M/0/1/0/all/0/1\">Michel Salomon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginhac_D/0/1/0/all/0/1\">Dominique Ginhac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skandarani_Y/0/1/0/all/0/1\">Youssef Skandarani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boucher_A/0/1/0/all/0/1\">Arnaud Boucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahim_K/0/1/0/all/0/1\">Khawla Brahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruijne_M/0/1/0/all/0/1\">Marleen de Bruijne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camarasa_R/0/1/0/all/0/1\">Robin Camarasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correia_T/0/1/0/all/0/1\">Teresa M. Correia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girum_K/0/1/0/all/0/1\">Kibrom B. Girum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennemuth_A/0/1/0/all/0/1\">Anja Hennemuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huellebrand_M/0/1/0/all/0/1\">Markus Huellebrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_R/0/1/0/all/0/1\">Raabid Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivantsits_M/0/1/0/all/0/1\">Matthias Ivantsits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_C/0/1/0/all/0/1\">Craig Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rishabh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jixi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsekos_N/0/1/0/all/0/1\">Nikolaos V. Tsekos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varela_M/0/1/0/all/0/1\">Marta Varela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hannu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuncheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couturier_R/0/1/0/all/0/1\">Raphael Couturier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meriaudeau_F/0/1/0/all/0/1\">Fabrice Meriaudeau</a>",
          "description": "A key factor for assessing the state of the heart after myocardial infarction\n(MI) is to measure whether the myocardium segment is viable after reperfusion\nor revascularization therapy. Delayed enhancement-MRI or DE-MRI, which is\nperformed several minutes after injection of the contrast agent, provides high\ncontrast between viable and nonviable myocardium and is therefore a method of\nchoice to evaluate the extent of MI. To automatically assess myocardial status,\nthe results of the EMIDEC challenge that focused on this task are presented in\nthis paper. The challenge's main objectives were twofold. First, to evaluate if\ndeep learning methods can distinguish between normal and pathological cases.\nSecond, to automatically calculate the extent of myocardial infarction. The\npublicly available database consists of 150 exams divided into 50 cases with\nnormal MRI after injection of a contrast agent and 100 cases with myocardial\ninfarction (and then with a hyperenhanced area on DE-MRI), whatever their\ninclusion in the cardiac emergency department. Along with MRI, clinical\ncharacteristics are also provided. The obtained results issued from several\nworks show that the automatic classification of an exam is a reachable task\n(the best method providing an accuracy of 0.92), and the automatic segmentation\nof the myocardium is possible. However, the segmentation of the diseased area\nneeds to be improved, mainly due to the small size of these areas and the lack\nof contrast with the surrounding structures.",
          "link": "http://arxiv.org/abs/2108.04016",
          "publishedOn": "2021-08-10T02:00:09.133Z",
          "wordCount": 741,
          "title": "Deep Learning methods for automatic evaluation of delayed enhancement-MRI. The results of the EMIDEC challenge. (arXiv:2108.04016v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1\">Cherie Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jong_A/0/1/0/all/0/1\">Andrew Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_H/0/1/0/all/0/1\">Harry Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1\">Rohan Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonatti_R/0/1/0/all/0/1\">Rogerio Bonatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1\">Sebastian Scherer</a>",
          "description": "Aerial vehicles are revolutionizing applications that require capturing the\n3D structure of dynamic targets in the wild, such as sports, medicine, and\nentertainment. The core challenges in developing a motion-capture system that\noperates in outdoors environments are: (1) 3D inference requires multiple\nsimultaneous viewpoints of the target, (2) occlusion caused by obstacles is\nfrequent when tracking moving targets, and (3) the camera and vehicle state\nestimation is noisy. We present a real-time aerial system for multi-camera\ncontrol that can reconstruct human motions in natural environments without the\nuse of special-purpose markers. We develop a multi-robot coordination scheme\nthat maintains the optimal flight formation for target reconstruction quality\namongst obstacles. We provide studies evaluating system performance in\nsimulation, and validate real-world performance using two drones while a target\nperforms activities such as jogging and playing soccer. Supplementary video:\nhttps://youtu.be/jxt91vx0cns",
          "link": "http://arxiv.org/abs/2108.03936",
          "publishedOn": "2021-08-10T02:00:09.116Z",
          "wordCount": 590,
          "title": "3D Human Reconstruction in the Wild with Collaborative Aerial Cameras. (arXiv:2108.03936v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tongzhou Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1\">Fanbo Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Derek Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Stone Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhiwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>",
          "description": "Learning generalizable manipulation skills is central for robots to achieve\ntask automation in environments with endless scene and object variations.\nHowever, existing robot learning environments are limited in both scale and\ndiversity of 3D assets (especially of articulated objects), making it difficult\nto train and evaluate the generalization ability of agents over novel objects.\nIn this work, we focus on object-level generalization and propose SAPIEN\nManipulation Skill Benchmark (abbreviated as ManiSkill), a large-scale\nlearning-from-demonstrations benchmark for articulated object manipulation with\n3D visual input (point cloud and RGB-D image). ManiSkill supports object-level\nvariations by utilizing a rich and diverse set of articulated objects, and each\ntask is carefully designed for learning manipulations on a single category of\nobjects. We equip ManiSkill with a large number of high-quality demonstrations\nto facilitate learning-from-demonstrations approaches and perform evaluations\non baseline algorithms. We believe that ManiSkill can encourage the robot\nlearning community to explore more on learning generalizable object\nmanipulation skills.",
          "link": "http://arxiv.org/abs/2107.14483",
          "publishedOn": "2021-08-10T02:00:09.110Z",
          "wordCount": 629,
          "title": "ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills. (arXiv:2107.14483v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khozeimeh_F/0/1/0/all/0/1\">Fahime Khozeimeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifrazi_D/0/1/0/all/0/1\">Danial Sharifrazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izadi_N/0/1/0/all/0/1\">Navid Hoseini Izadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joloudari_J/0/1/0/all/0/1\">Javad Hassannataj Joloudari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeibi_A/0/1/0/all/0/1\">Afshin Shoeibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorriz_J/0/1/0/all/0/1\">Juan M. Gorriz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_S/0/1/0/all/0/1\">Sadiq Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sani_Z/0/1/0/all/0/1\">Zahra Alizadeh Sani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosaei_H/0/1/0/all/0/1\">Hossein Moosaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Sheikh Mohammed Shariful Islam</a>",
          "description": "COVID-19 has caused many deaths worldwide. The automation of the diagnosis of\nthis virus is highly desired. Convolutional neural networks (CNNs) have shown\noutstanding classification performance on image datasets. To date, it appears\nthat COVID computer-aided diagnosis systems based on CNNs and clinical\ninformation have not yet been analysed or explored. We propose a novel method,\nnamed the CNN-AE, to predict the survival chance of COVID-19 patients using a\nCNN trained with clinical information. Notably, the required resources to\nprepare CT images are expensive and limited compared to those required to\ncollect clinical data, such as blood pressure, liver disease, etc. We evaluated\nour method using a publicly available clinical dataset that we collected. The\ndataset properties were carefully analysed to extract important features and\ncompute the correlations of features. A data augmentation procedure based on\nautoencoders (AEs) was proposed to balance the dataset. The experimental\nresults revealed that the average accuracy of the CNN-AE (96.05%) was higher\nthan that of the CNN (92.49%). To demonstrate the generality of our\naugmentation method, we trained some existing mortality risk prediction methods\non our dataset (with and without data augmentation) and compared their\nperformances. We also evaluated our method using another dataset for further\ngenerality verification. To show that clinical data can be used for COVID-19\nsurvival chance prediction, the CNN-AE was compared with multiple pre-trained\ndeep models that were tuned based on CT images.",
          "link": "http://arxiv.org/abs/2104.08954",
          "publishedOn": "2021-08-10T02:00:09.091Z",
          "wordCount": 816,
          "title": "Combining a Convolutional Neural Network with Autoencoders to Predict the Survival Chance of COVID-19 Patients. (arXiv:2104.08954v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1\">Yuren Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wentong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ackermann_H/0/1/0/all/0/1\">Hanno Ackermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Michael Ying Yang</a>",
          "description": "Dynamic scene graph generation aims at generating a scene graph of the given\nvideo. Compared to the task of scene graph generation from images, it is more\nchallenging because of the dynamic relationships between objects and the\ntemporal dependencies between frames allowing for a richer semantic\ninterpretation. In this paper, we propose Spatial-temporal Transformer\n(STTran), a neural network that consists of two core modules: (1) a spatial\nencoder that takes an input frame to extract spatial context and reason about\nthe visual relationships within a frame, and (2) a temporal decoder which takes\nthe output of the spatial encoder as input in order to capture the temporal\ndependencies between frames and infer the dynamic relationships. Furthermore,\nSTTran is flexible to take varying lengths of videos as input without clipping,\nwhich is especially important for long videos. Our method is validated on the\nbenchmark dataset Action Genome (AG). The experimental results demonstrate the\nsuperior performance of our method in terms of dynamic scene graphs. Moreover,\na set of ablative studies is conducted and the effect of each proposed module\nis justified. Code available at: https://github.com/yrcong/STTran.",
          "link": "http://arxiv.org/abs/2107.12309",
          "publishedOn": "2021-08-10T02:00:09.068Z",
          "wordCount": 659,
          "title": "Spatial-Temporal Transformer for Dynamic Scene Graph Generation. (arXiv:2107.12309v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhengzheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Bin Tang</a>",
          "description": "Salient object detection is the pixel-level dense prediction task which can\nhighlight the prominent object in the scene. Recently U-Net framework is widely\nused, and continuous convolution and pooling operations generate multi-level\nfeatures which are complementary with each other. In view of the more\ncontribution of high-level features for the performance, we propose a triplet\ntransformer embedding module to enhance them by learning long-range\ndependencies across layers. It is the first to use three transformer encoders\nwith shared weights to enhance multi-level features. By further designing scale\nadjustment module to process the input, devising three-stream decoder to\nprocess the output and attaching depth features to color features for the\nmulti-modal fusion, the proposed triplet transformer embedding network\n(TriTransNet) achieves the state-of-the-art performance in RGB-D salient object\ndetection, and pushes the performance to a new level. Experimental results\ndemonstrate the effectiveness of the proposed modules and the competition of\nTriTransNet.",
          "link": "http://arxiv.org/abs/2108.03990",
          "publishedOn": "2021-08-10T02:00:09.062Z",
          "wordCount": 593,
          "title": "TriTransNet: RGB-D Salient Object Detection with a Triplet Transformer Embedding Network. (arXiv:2108.03990v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.02193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1\">Abulikemu Abuduweili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingjian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng-Zhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>",
          "description": "While recent studies on semi-supervised learning have shown remarkable\nprogress in leveraging both labeled and unlabeled data, most of them presume a\nbasic setting of the model is randomly initialized. In this work, we consider\nsemi-supervised learning and transfer learning jointly, leading to a more\npractical and competitive paradigm that can utilize both powerful pre-trained\nmodels from source domain as well as labeled/unlabeled data in the target\ndomain. To better exploit the value of both pre-trained weights and unlabeled\ntarget examples, we introduce adaptive consistency regularization that consists\nof two complementary components: Adaptive Knowledge Consistency (AKC) on the\nexamples between the source and target model, and Adaptive Representation\nConsistency (ARC) on the target model between labeled and unlabeled examples.\nExamples involved in the consistency regularization are adaptively selected\naccording to their potential contributions to the target task. We conduct\nextensive experiments on popular benchmarks including CIFAR-10, CUB-200, and\nMURA, by fine-tuning the ImageNet pre-trained ResNet-50 model. Results show\nthat our proposed adaptive consistency regularization outperforms\nstate-of-the-art semi-supervised learning techniques such as Pseudo Label, Mean\nTeacher, and FixMatch. Moreover, our algorithm is orthogonal to existing\nmethods and thus able to gain additional improvements on top of MixMatch and\nFixMatch. Our code is available at\nhttps://github.com/SHI-Labs/Semi-Supervised-Transfer-Learning.",
          "link": "http://arxiv.org/abs/2103.02193",
          "publishedOn": "2021-08-10T02:00:09.040Z",
          "wordCount": 678,
          "title": "Adaptive Consistency Regularization for Semi-Supervised Transfer Learning. (arXiv:2103.02193v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04023",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Maosheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuangjie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tongyi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "We present a novel and flexible architecture for point cloud segmentation\nwith dual-representation iterative learning. In point cloud processing,\ndifferent representations have their own pros and cons. Thus, finding suitable\nways to represent point cloud data structure while keeping its own internal\nphysical property such as permutation and scale-invariant is a fundamental\nproblem. Therefore, we propose our work, DRINet, which serves as the basic\nnetwork structure for dual-representation learning with great flexibility at\nfeature transferring and less computation cost, especially for large-scale\npoint clouds. DRINet mainly consists of two modules called Sparse Point-Voxel\nFeature Extraction and Sparse Voxel-Point Feature Extraction. By utilizing\nthese two modules iteratively, features can be propagated between two different\nrepresentations. We further propose a novel multi-scale pooling layer for\npointwise locality learning to improve context information propagation. Our\nnetwork achieves state-of-the-art results for point cloud classification and\nsegmentation tasks on several datasets while maintaining high runtime\nefficiency. For large-scale outdoor scenarios, our method outperforms\nstate-of-the-art methods with a real-time inference speed of 62ms per frame.",
          "link": "http://arxiv.org/abs/2108.04023",
          "publishedOn": "2021-08-10T02:00:09.026Z",
          "wordCount": 616,
          "title": "DRINet: A Dual-Representation Iterative Learning Network for Point Cloud Segmentation. (arXiv:2108.04023v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03995",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_W/0/1/0/all/0/1\">Wei-Chang Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yi-Ping Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yun-Chia Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Chyh-Ming Lai</a>",
          "description": "Convolutional neural networks (CNNs) are widely used in image recognition.\nNumerous CNN models, such as LeNet, AlexNet, VGG, ResNet, and GoogLeNet, have\nbeen proposed by increasing the number of layers, to improve the performance of\nCNNs. However, performance deteriorates beyond a certain number of layers.\nHence, hyperparameter optimisation is a more efficient way to improve CNNs. To\nvalidate this concept, a new algorithm based on simplified swarm optimisation\nis proposed to optimise the hyperparameters of the simplest CNN model, which is\nLeNet. The results of experiments conducted on the MNIST, Fashion MNIST, and\nCifar10 datasets showed that the accuracy of the proposed algorithm is higher\nthan the original LeNet model and PSO-LeNet and that it has a high potential to\nbe extended to more complicated models, such as AlexNet.",
          "link": "http://arxiv.org/abs/2103.03995",
          "publishedOn": "2021-08-10T02:00:09.016Z",
          "wordCount": 601,
          "title": "Convolution Neural Network Hyperparameter Optimization Using Simplified Swarm Optimization. (arXiv:2103.03995v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.00959",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Tirer_T/0/1/0/all/0/1\">Tom Tirer</a>, <a href=\"http://arxiv.org/find/math/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>",
          "description": "Ill-posed linear inverse problems appear in many scientific setups, and are\ntypically addressed by solving optimization problems, which are composed of\ndata fidelity and prior terms. Recently, several works have considered a\nback-projection (BP) based fidelity term as an alternative to the common least\nsquares (LS), and demonstrated excellent results for popular inverse problems.\nThese works have also empirically shown that using the BP term, rather than the\nLS term, requires fewer iterations of optimization algorithms. In this paper,\nwe examine the convergence rate of the projected gradient descent (PGD)\nalgorithm for the BP objective. Our analysis allows to identify an inherent\nsource for its faster convergence compared to using the LS objective, while\nmaking only mild assumptions. We also analyze the more general proximal\ngradient method under a relaxed contraction condition on the proximal mapping\nof the prior. This analysis further highlights the advantage of BP when the\nlinear measurement operator is badly conditioned. Numerical experiments with\nboth $\\ell_1$-norm and GAN-based priors corroborate our theoretical results.",
          "link": "http://arxiv.org/abs/2005.00959",
          "publishedOn": "2021-08-10T02:00:08.991Z",
          "wordCount": 661,
          "title": "On the Convergence Rate of Projected Gradient Descent for a Back-Projection based Objective. (arXiv:2005.00959v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Son_W/0/1/0/all/0/1\">Wonchul Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_J/0/1/0/all/0/1\">Jaemin Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Junyong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonjun Hwang</a>",
          "description": "With the success of deep neural networks, knowledge distillation which guides\nthe learning of a small student network from a large teacher network is being\nactively studied for model compression and transfer learning. However, few\nstudies have been performed to resolve the poor learning issue of the student\nnetwork when the student and teacher model sizes significantly differ. In this\npaper, we propose a densely guided knowledge distillation using multiple\nteacher assistants that gradually decreases the model size to efficiently\nbridge the large gap between the teacher and student networks. To stimulate\nmore efficient learning of the student network, we guide each teacher assistant\nto every other smaller teacher assistants iteratively. Specifically, when\nteaching a smaller teacher assistant at the next step, the existing larger\nteacher assistants from the previous step are used as well as the teacher\nnetwork. Moreover, we design stochastic teaching where, for each mini-batch, a\nteacher or teacher assistants are randomly dropped. This acts as a regularizer\nto improve the efficiency of teaching of the student network. Thus, the student\ncan always learn salient distilled knowledge from the multiple sources. We\nverified the effectiveness of the proposed method for a classification task\nusing CIFAR-10, CIFAR-100, and ImageNet. We also achieved significant\nperformance improvements with various backbone architectures such as ResNet,\nWideResNet, and VGG.",
          "link": "http://arxiv.org/abs/2009.08825",
          "publishedOn": "2021-08-10T02:00:08.977Z",
          "wordCount": 693,
          "title": "Densely Guided Knowledge Distillation using Multiple Teacher Assistants. (arXiv:2009.08825v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Makansi_O/0/1/0/all/0/1\">Osama Makansi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cicek_O/0/1/0/all/0/1\">&#xd6;zg&#xfc;n Cicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marrakchi_Y/0/1/0/all/0/1\">Yassine Marrakchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1\">Thomas Brox</a>",
          "description": "Predicting the states of dynamic traffic actors into the future is important\nfor autonomous systems to operate safelyand efficiently. Remarkably, the most\ncritical scenarios aremuch less frequent and more complex than the\nuncriticalones. Therefore, uncritical cases dominate the prediction. In this\npaper, we address specifically the challenging scenarios at the long tail of\nthe dataset distribution. Our analysis shows that the common losses tend to\nplace challenging cases suboptimally in the embedding space. As a consequence,\nwe propose to supplement the usual loss with aloss that places challenging\ncases closer to each other. This triggers sharing information among challenging\ncases andlearning specific predictive features. We show on four public datasets\nthat this leads to improved performance on the challenging scenarios while the\noverall performance stays stable. The approach is agnostic w.r.t. the used\nnetwork architecture, input modality or viewpoint, and can be integrated into\nexisting solutions easily. Code is available at\nhttps://github.com/lmb-freiburg/Contrastive-Future-Trajectory-Prediction",
          "link": "http://arxiv.org/abs/2103.12474",
          "publishedOn": "2021-08-10T02:00:08.964Z",
          "wordCount": 635,
          "title": "On Exposing the Challenging Long Tail in Future Prediction of Traffic Actors. (arXiv:2103.12474v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05237",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_H/0/1/0/all/0/1\">Hao Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zifan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1\">Chenyang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_K/0/1/0/all/0/1\">Ka Lung Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "We present a controllable camera simulator based on deep neural networks to\nsynthesize raw image data under different camera settings, including exposure\ntime, ISO, and aperture. The proposed simulator includes an exposure module\nthat utilizes the principle of modern lens designs for correcting the luminance\nlevel. It also contains a noise module using the noise level function and an\naperture module with adaptive attention to simulate the side effects on noise\nand defocus blur. To facilitate the learning of a simulator model, we collect a\ndataset of the 10,000 raw images of 450 scenes with different exposure\nsettings. Quantitative experiments and qualitative comparisons show that our\napproach outperforms relevant baselines in raw data synthesize on multiple\ncameras. Furthermore, the camera simulator enables various applications,\nincluding large-aperture enhancement, HDR, auto exposure, and data augmentation\nfor training local feature detectors. Our work represents the first attempt to\nsimulate a camera sensor's behavior leveraging both the advantage of\ntraditional raw sensor features and the power of data-driven deep learning.",
          "link": "http://arxiv.org/abs/2104.05237",
          "publishedOn": "2021-08-10T02:00:08.957Z",
          "wordCount": 636,
          "title": "Neural Camera Simulators. (arXiv:2104.05237v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azimi_S/0/1/0/all/0/1\">Shiva Azimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadhawan_R/0/1/0/all/0/1\">Rohan Wadhawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_T/0/1/0/all/0/1\">Tapan K. Gandhi</a>",
          "description": "In the recent decade, high-throughput plant phenotyping techniques, which\ncombine non-invasive image analysis and machine learning, have been\nsuccessfully applied to identify and quantify plant health and diseases.\nHowever, these techniques usually do not consider the progressive nature of\nplant stress and often require images showing severe signs of stress to ensure\nhigh confidence detection, thereby reducing the feasibility for early detection\nand recovery of plants under stress. To overcome the problem mentioned above,\nwe propose a deep learning pipeline for the temporal analysis of the visual\nchanges induced in the plant due to stress and apply it for the specific case\nof water stress identification in Chickpea plant shoot images. For this, we\nhave considered an image dataset of two chickpea varieties JG-62 and Pusa-372,\nunder three water stress conditions; control, young seedling, and before\nflowering, captured over five months. We have employed a variant of the\nLong-term Recurrent Convolutional Network (LRCN) to learn spatio-temporal\npatterns from the chickpea plant dataset and use them for water stress\nclassification. Our model has achieved ceiling level classification performance\nof 98.52% on JG-62 and 97.78% on Pusa-372 chickpea plant data and has\noutperformed the state-of-the-art time-invariant technique by at least 14% for\nboth JG-62 and Pusa-372 species, to the best of our knowledge. Furthermore, our\nLRCN model has demonstrated robustness to noisy input, with a less than 2.5%\ndip in average model accuracy and a small standard deviation about the mean for\nboth species. Lastly, we have performed an ablation study to analyze the\nperformance of the LRCN model by decreasing the number of temporal session data\nused for training.",
          "link": "http://arxiv.org/abs/2104.07911",
          "publishedOn": "2021-08-10T02:00:08.921Z",
          "wordCount": 741,
          "title": "Intelligent Monitoring of Stress Induced by Water Deficiency in Plants using Deep Learning. (arXiv:2104.07911v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.05443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1\">Alejandro Pardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengmeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1\">Pablo Arbelaez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "We study the problem of object detection from a novel perspective in which\nannotation budget constraints are taken into consideration, appropriately\ncoined Budget Aware Object Detection (BAOD). When provided with a fixed budget,\nwe propose a strategy for building a diverse and informative dataset that can\nbe used to optimally train a robust detector. We investigate both optimization\nand learning-based methods to sample which images to annotate and what type of\nannotation (strongly or weakly supervised) to annotate them with. We adopt a\nhybrid supervised learning framework to train the object detector from both\nthese types of annotation. We conduct a comprehensive empirical study showing\nthat a handcrafted optimization method outperforms other selection techniques\nincluding random sampling, uncertainty sampling and active learning. By\ncombining an optimal image/annotation selection scheme with hybrid supervised\nlearning to solve the BAOD problem, we show that one can achieve the\nperformance of a strongly supervised detector on PASCAL-VOC 2007 while saving\n12.8% of its original annotation budget. Furthermore, when $100\\%$ of the\nbudget is used, it surpasses this performance by 2.0 mAP percentage points.",
          "link": "http://arxiv.org/abs/1904.05443",
          "publishedOn": "2021-08-10T02:00:08.914Z",
          "wordCount": 640,
          "title": "BAOD: Budget-Aware Object Detection. (arXiv:1904.05443v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ashesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1\">Luca Del Pero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1\">Hugo Grimmett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1\">Peter Ondruska</a>",
          "description": "Despite the numerous successes of machine learning over the past decade\n(image recognition, decision-making, NLP, image synthesis), self-driving\ntechnology has not yet followed the same trend. In this paper, we study the\nhistory, composition, and development bottlenecks of the modern self-driving\nstack. We argue that the slow progress is caused by approaches that require too\nmuch hand-engineering, an over-reliance on road testing, and high fleet\ndeployment costs. We observe that the classical stack has several bottlenecks\nthat preclude the necessary scale needed to capture the long tail of rare\nevents. To resolve these problems, we outline the principles of Autonomy 2.0,\nan ML-first approach to self-driving, as a viable alternative to the currently\nadopted state-of-the-art. This approach is based on (i) a fully differentiable\nAV stack trainable from human demonstrations, (ii) closed-loop data-driven\nreactive simulation, and (iii) large-scale, low-cost data collections as\ncritical solutions towards scalability issues. We outline the general\narchitecture, survey promising works in this direction and propose key\nchallenges to be addressed by the community in the future.",
          "link": "http://arxiv.org/abs/2107.08142",
          "publishedOn": "2021-08-10T02:00:08.895Z",
          "wordCount": 654,
          "title": "Autonomy 2.0: Why is self-driving always 5 years away?. (arXiv:2107.08142v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.08719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H Ang Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>",
          "description": "Point clouds are often sparse and incomplete, which imposes difficulties for\nreal-world applications. Existing shape completion methods tend to generate\nrough shapes without fine-grained details. Considering this, we introduce a\ntwo-branch network for shape completion. The first branch is a cascaded shape\ncompletion sub-network to synthesize complete objects, where we propose to use\nthe partial input together with the coarse output to preserve the object\ndetails during the dense point reconstruction. The second branch is an\nauto-encoder to reconstruct the original partial input. The two branches share\na same feature extractor to learn an accurate global feature for shape\ncompletion. Furthermore, we propose two strategies to enable the training of\nour network when ground truth data are not available. This is to mitigate the\ndependence of existing approaches on large amounts of ground truth training\ndata that are often difficult to obtain in real-world applications.\nAdditionally, our proposed strategies are also able to improve the\nreconstruction quality for fully supervised learning. We verify our approach in\nself-supervised, semi-supervised and fully supervised settings with superior\nperformances. Quantitative and qualitative results on different datasets\ndemonstrate that our method achieves more realistic outputs than\nstate-of-the-art approaches on the point cloud completion task.",
          "link": "http://arxiv.org/abs/2010.08719",
          "publishedOn": "2021-08-10T02:00:08.885Z",
          "wordCount": 673,
          "title": "Cascaded Refinement Network for Point Cloud Completion with Self-supervision. (arXiv:2010.08719v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guodong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Huimin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhaohui Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuzhao Li</a>",
          "description": "Few-shot learning (FSL) attempts to learn with limited data. In this work, we\nperform the feature extraction in the Euclidean space and the geodesic distance\nmetric on the Oblique Manifold (OM). Specially, for better feature extraction,\nwe propose a non-parametric Region Self-attention with Spatial Pyramid Pooling\n(RSSPP), which realizes a trade-off between the generalization and the\ndiscriminative ability of the single image feature. Then, we embed the feature\nto OM as a point. Furthermore, we design an Oblique Distance-based Classifier\n(ODC) that achieves classification in the tangent spaces which better\napproximate OM locally by learnable tangency points. Finally, we introduce a\nnew method for parameters initialization and a novel loss function in the\ntransductive settings. Extensive experiments demonstrate the effectiveness of\nour algorithm and it outperforms state-of-the-art methods on the popular\nbenchmarks: mini-ImageNet, tiered-ImageNet, and Caltech-UCSD Birds-200-2011\n(CUB).",
          "link": "http://arxiv.org/abs/2108.04009",
          "publishedOn": "2021-08-10T02:00:08.877Z",
          "wordCount": 574,
          "title": "Transductive Few-Shot Classification on the Oblique Manifold. (arXiv:2108.04009v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_K/0/1/0/all/0/1\">Kenan Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xuesheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyun Yang</a>",
          "description": "Deep learning based visual trackers entail offline pre-training on large\nvolumes of video datasets with accurate bounding box annotations that are\nlabor-expensive to achieve. We present a new framework to facilitate bounding\nbox annotations for video sequences, which investigates a\nselection-and-refinement strategy to automatically improve the preliminary\nannotations generated by tracking algorithms. A temporal assessment network\n(T-Assess Net) is proposed which is able to capture the temporal coherence of\ntarget locations and select reliable tracking results by measuring their\nquality. Meanwhile, a visual-geometry refinement network (VG-Refine Net) is\nalso designed to further enhance the selected tracking results by considering\nboth target appearance and temporal geometry constraints, allowing inaccurate\ntracking results to be corrected. The combination of the above two networks\nprovides a principled approach to ensure the quality of automatic video\nannotation. Experiments on large scale tracking benchmarks demonstrate that our\nmethod can deliver highly accurate bounding box annotations and significantly\nreduce human labor by 94.0%, yielding an effective means to further boost\ntracking performance with augmented training data.",
          "link": "http://arxiv.org/abs/2108.03821",
          "publishedOn": "2021-08-10T02:00:08.871Z",
          "wordCount": null,
          "title": "Video Annotation for Visual Tracking via Selection and Refinement. (arXiv:2108.03821v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Jing Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shicai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Di Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>",
          "description": "Nowadays advanced image editing tools and technical skills produce tampered\nimages more realistically, which can easily evade image forensic systems and\nmake authenticity verification of images more difficult. To tackle this\nchallenging problem, we introduce TransForensics, a novel image forgery\nlocalization method inspired by Transformers. The two major components in our\nframework are dense self-attention encoders and dense correction modules. The\nformer is to model global context and all pairwise interactions between local\npatches at different scales, while the latter is used for improving the\ntransparency of the hidden layers and correcting the outputs from different\nbranches. Compared to previous traditional and deep learning methods,\nTransForensics not only can capture discriminative representations and obtain\nhigh-quality mask predictions but is also not limited by tampering types and\npatch sequence orders. By conducting experiments on main benchmarks, we show\nthat TransForensics outperforms the stateof-the-art methods by a large margin.",
          "link": "http://arxiv.org/abs/2108.03871",
          "publishedOn": "2021-08-10T02:00:08.868Z",
          "wordCount": 586,
          "title": "TransForensics: Image Forgery Localization with Dense Self-Attention. (arXiv:2108.03871v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.08027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>",
          "description": "Close your eyes and listen to music, one can easily imagine an actor dancing\nrhythmically along with the music. These dance movements are usually made up of\ndance movements you have seen before. In this paper, we propose to reproduce\nsuch an inherent capability of the human-being within a computer vision system.\nThe proposed system consists of three modules. To explore the relationship\nbetween music and dance movements, we propose a cross-modal alignment module\nthat focuses on dancing video clips, accompanied on pre-designed music, to\nlearn a system that can judge the consistency between the visual features of\npose sequences and the acoustic features of music. The learned model is then\nused in the imagination module to select a pose sequence for the given music.\nSuch pose sequence selected from the music, however, is usually discontinuous.\nTo solve this problem, in the spatial-temporal alignment module we develop a\nspatial alignment algorithm based on the tendency and periodicity of dance\nmovements to predict dance movements between discontinuous fragments. In\naddition, the selected pose sequence is often misaligned with the music beat.\nTo solve this problem, we further develop a temporal alignment algorithm to\nalign the rhythm of music and dance. Finally, the processed pose sequence is\nused to synthesize realistic dancing videos in the imagination module. The\ngenerated dancing videos match the content and rhythm of the music.\nExperimental results and subjective evaluations show that the proposed approach\ncan perform the function of generating promising dancing videos by inputting\nmusic.",
          "link": "http://arxiv.org/abs/2009.08027",
          "publishedOn": "2021-08-10T02:00:08.834Z",
          "wordCount": 731,
          "title": "DanceIt: Music-inspired Dancing Video Synthesis. (arXiv:2009.08027v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1\">Chaochen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>",
          "description": "Weakly Supervised Semantic Segmentation (WSSS) based on image-level labels\nhas been greatly advanced by exploiting the outputs of Class Activation Map\n(CAM) to generate the pseudo labels for semantic segmentation. However, CAM\nmerely discovers seeds from a small number of regions, which may be\ninsufficient to serve as pseudo masks for semantic segmentation. In this paper,\nwe formulate the expansion of object regions in CAM as an increase in\ninformation. From the perspective of information theory, we propose a novel\nComplementary Patch (CP) Representation and prove that the information of the\nsum of the CAMs by a pair of input images with complementary hidden (patched)\nparts, namely CP Pair, is greater than or equal to the information of the\nbaseline CAM. Therefore, a CAM with more information related to object seeds\ncan be obtained by narrowing down the gap between the sum of CAMs generated by\nthe CP Pair and the original CAM. We propose a CP Network (CPN) implemented by\na triplet network and three regularization functions. To further improve the\nquality of the CAMs, we propose a Pixel-Region Correlation Module (PRCM) to\naugment the contextual information by using object-region relations between the\nfeature maps and the CAMs. Experimental results on the PASCAL VOC 2012 datasets\nshow that our proposed method achieves a new state-of-the-art in WSSS,\nvalidating the effectiveness of our CP Representation and CPN.",
          "link": "http://arxiv.org/abs/2108.03852",
          "publishedOn": "2021-08-10T02:00:08.824Z",
          "wordCount": 667,
          "title": "Complementary Patch for Weakly Supervised Semantic Segmentation. (arXiv:2108.03852v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.02432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jing_T/0/1/0/all/0/1\">Taotao Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhengming Ding</a>",
          "description": "Open-set domain adaptation (OSDA) considers that the target domain contains\nsamples from novel categories unobserved in external source domain.\nUnfortunately, existing OSDA methods always ignore the demand for the\ninformation of unseen categories and simply recognize them as \"unknown\" set\nwithout further explanation. This motivates us to understand the unknown\ncategories more specifically by exploring the underlying structures and\nrecovering their interpretable semantic attributes. In this paper, we propose a\nnovel framework to accurately identify the seen categories in target domain,\nand effectively recover the semantic attributes for unseen categories.\nSpecifically, structure preserving partial alignment is developed to recognize\nthe seen categories through domain-invariant feature learning. Attribute\npropagation over visual graph is designed to smoothly transit attributes from\nseen to unseen categories via visual-semantic mapping. Moreover, two new\ncross-main benchmarks are constructed to evaluate the proposed framework in the\nnovel and practical challenge. Experimental results on open-set recognition and\nsemantic recovery demonstrate the superiority of the proposed method over other\ncompared baselines.",
          "link": "http://arxiv.org/abs/2105.02432",
          "publishedOn": "2021-08-10T02:00:08.815Z",
          "wordCount": 633,
          "title": "Towards Novel Target Discovery Through Open-Set Domain Adaptation. (arXiv:2105.02432v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saleh_K/0/1/0/all/0/1\">Khaled Saleh</a>",
          "description": "Forecasting the trajectory of pedestrians in shared urban traffic\nenvironments is still considered one of the challenging problems facing the\ndevelopment of autonomous vehicles (AVs). In the literature, this problem is\noften tackled using recurrent neural networks (RNNs). Despite the powerful\ncapabilities of RNNs in capturing the temporal dependency in the pedestrians'\nmotion trajectories, they were argued to be challenged when dealing with longer\nsequential data. Thus, in this work, we are introducing a framework based on\nthe transformer networks that were shown recently to be more efficient and\noutperformed RNNs in many sequential-based tasks. We relied on a fusion of the\npast positional information, agent interactions information and scene physical\nsemantics information as an input to our framework in order to provide a robust\ntrajectory prediction of pedestrians. We have evaluated our framework on two\nreal-life datasets of pedestrians in shared urban traffic environments and it\nhas outperformed the compared baseline approaches in both short-term and\nlong-term prediction horizons.",
          "link": "http://arxiv.org/abs/2012.01757",
          "publishedOn": "2021-08-10T02:00:08.807Z",
          "wordCount": 622,
          "title": "Pedestrian Trajectory Prediction using Context-Augmented Transformer Networks. (arXiv:2012.01757v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>",
          "description": "Recent studies unveil the vulnerabilities of deep ranking models, where an\nimperceptible perturbation can trigger dramatic changes in the ranking result.\nWhile previous attempts focus on manipulating absolute ranks of certain\ncandidates, the possibility of adjusting their relative order remains\nunder-explored. In this paper, we formulate a new adversarial attack against\ndeep ranking systems, i.e., the Order Attack, which covertly alters the\nrelative order among a selected set of candidates according to an\nattacker-specified permutation, with limited interference to other unrelated\ncandidates. Specifically, it is formulated as a triplet-style loss imposing an\ninequality chain reflecting the specified permutation. However, direct\noptimization of such white-box objective is infeasible in a real-world attack\nscenario due to various black-box limitations. To cope with them, we propose a\nShort-range Ranking Correlation metric as a surrogate objective for black-box\nOrder Attack to approximate the white-box method. The Order Attack is evaluated\non the Fashion-MNIST and Stanford-Online-Products datasets under both white-box\nand black-box threat models. The black-box attack is also successfully\nimplemented on a major e-commerce platform. Comprehensive experimental\nevaluations demonstrate the effectiveness of the proposed methods, revealing a\nnew type of ranking model vulnerability.",
          "link": "http://arxiv.org/abs/2103.05248",
          "publishedOn": "2021-08-10T02:00:08.790Z",
          "wordCount": 686,
          "title": "Practical Relative Order Attack in Deep Ranking. (arXiv:2103.05248v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kimura_T/0/1/0/all/0/1\">Takumi Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_T/0/1/0/all/0/1\">Takashi Matsubara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uehara_K/0/1/0/all/0/1\">Kuniaki Uehara</a>",
          "description": "A point cloud serves as a representation of the surface of a\nthree-dimensional (3D) shape. Deep generative models have been adapted to model\ntheir variations typically using a map from a ball-like set of latent\nvariables. However, previous approaches did not pay much attention to the\ntopological structure of a point cloud, despite that a continuous map cannot\nexpress the varying numbers of holes and intersections. Moreover, a point cloud\nis often composed of multiple subparts, and it is also difficult to express. In\nthis study, we propose ChartPointFlow, a flow-based generative model with\nmultiple latent labels for 3D point clouds. Each label is assigned to points in\nan unsupervised manner. Then, a map conditioned on a label is assigned to a\ncontinuous subset of a point cloud, similar to a chart of a manifold. This\nenables our proposed model to preserve the topological structure with clear\nboundaries, whereas previous approaches tend to generate blurry point clouds\nand fail to generate holes. The experimental results demonstrate that\nChartPointFlow achieves state-of-the-art performance in terms of generation and\nreconstruction compared with other point cloud generators. Moreover,\nChartPointFlow divides an object into semantic subparts using charts, and it\ndemonstrates superior performance in case of unsupervised segmentation.",
          "link": "http://arxiv.org/abs/2012.02346",
          "publishedOn": "2021-08-10T02:00:08.760Z",
          "wordCount": 692,
          "title": "ChartPointFlow for Topology-Aware 3D Point Cloud Generation. (arXiv:2012.02346v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuesong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Guinan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>",
          "description": "Generative Adversarial Networks (GANs) are formulated as minimax game\nproblems, whereby generators attempt to approach real data distributions by\nvirtue of adversarial learning against discriminators. The intrinsic problem\ncomplexity poses the challenge to enhance the performance of generative\nnetworks. In this work, we aim to boost model learning from the perspective of\nnetwork architectures, by incorporating recent progress on automated\narchitecture search into GANs. To this end, we propose a fully differentiable\nsearch framework for generative adversarial networks, dubbed alphaGAN. The\nsearching process is formalized as solving a bi-level minimax optimization\nproblem, in which the outer-level objective aims for seeking a suitable network\narchitecture towards pure Nash Equilibrium conditioned on the generator and the\ndiscriminator network parameters optimized with a traditional GAN loss in the\ninner level. The entire optimization performs a first-order method by\nalternately minimizing the two-level objective in a fully differentiable\nmanner, enabling architecture search to be completed in an enormous search\nspace. Extensive experiments on CIFAR-10 and STL-10 datasets show that our\nalgorithm can obtain high-performing architectures only with 3-GPU hours on a\nsingle GPU in the search space comprised of approximate 2 ? 1011 possible\nconfigurations. We also provide a comprehensive analysis on the behavior of the\nsearching process and the properties of searched architectures, which would\nbenefit further research on architectures for generative models. Pretrained\nmodels and codes are available at https://github.com/yuesongtian/AlphaGAN.",
          "link": "http://arxiv.org/abs/2006.09134",
          "publishedOn": "2021-08-10T02:00:08.752Z",
          "wordCount": 730,
          "title": "AlphaGAN: Fully Differentiable Architecture Search for Generative Adversarial Networks. (arXiv:2006.09134v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03470",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bandyopadhyay_H/0/1/0/all/0/1\">Hmrishav Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dastidar_S/0/1/0/all/0/1\">Shuvayan Ghosh Dastidar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_B/0/1/0/all/0/1\">Bisakh Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1\">Biplab Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1\">Nibaran Das</a>",
          "description": "Presently, Covid-19 is a serious threat to the world at large. Efforts are\nbeing made to reduce disease screening times and in the development of a\nvaccine to resist this disease, even as thousands succumb to it everyday. We\npropose a novel method of automated screening of diseases like Covid-19 and\npneumonia from Chest X-Ray images with the help of Computer Vision. Unlike\ncomputer vision classification algorithms which come with heavy computational\ncosts, we propose a knowledge distillation based approach which allows us to\nbring down the model depth, while preserving the accuracy. We make use of an\naugmentation of the standard distillation module with an auxiliary intermediate\nassistant network that aids in the continuity of the flow of information.\nFollowing this approach, we are able to build an extremely light student\nnetwork, consisting of just 3 convolutional blocks without any compromise on\naccuracy. We thus propose a method of classification of diseases which can not\nonly lead to faster screening, but can also operate seamlessly on low-end\ndevices.",
          "link": "http://arxiv.org/abs/2108.03470",
          "publishedOn": "2021-08-10T02:00:08.348Z",
          "wordCount": 655,
          "title": "A distillation based approach for the diagnosis of diseases. (arXiv:2108.03470v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03287",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mejri_M/0/1/0/all/0/1\">Mohamed Mejri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mejri_A/0/1/0/all/0/1\">Aymen Mejri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mejri_O/0/1/0/all/0/1\">Oumayma Mejri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fekih_C/0/1/0/all/0/1\">Chiraz Fekih</a>",
          "description": "Breast cancer is one of the factors that cause the increase of mortality of\nwomen. The most widely used method for diagnosing this geological disease i.e.\nbreast cancer is the ultrasound scan. Several key features such as the\nsmoothness and the texture of the tumor captured through ultrasound scans\nencode the abnormality of the breast tumors (malignant from benign). However,\nultrasound scans are often noisy and include irrelevant parts of the breast\nthat may bias the segmentation of eventual tumors. In this paper, we are going\nto extract the region of interest ( i.e, bounding boxes of the tumors) and\nfeed-forward them to one semantic segmentation encoder-decoder structure based\non its classification (i.e, malignant or benign). the whole process aims to\nbuild an instance-based segmenter from a semantic segmenter and an object\ndetector.",
          "link": "http://arxiv.org/abs/2108.03287",
          "publishedOn": "2021-08-10T02:00:08.341Z",
          "wordCount": 575,
          "title": "Semantic Segmentation and Object Detection Towards Instance Segmentation: Breast Tumor Identification. (arXiv:2108.03287v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1\">Wei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hongchen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Affordance detection refers to identifying the potential action possibilities\nof objects in an image, which is a crucial ability for robot perception and\nmanipulation. To empower robots with this ability in unseen scenarios, we first\nstudy the challenging one-shot affordance detection problem in this paper,\ni.e., given a support image that depicts the action purpose, all objects in a\nscene with the common affordance should be detected. To this end, we devise a\nOne-Shot Affordance Detection Network (OSAD-Net) that firstly estimates the\nhuman action purpose and then transfers it to help detect the common affordance\nfrom all candidate images. Through collaboration learning, OSAD-Net can capture\nthe common characteristics between objects having the same underlying\naffordance and learn a good adaptation capability for perceiving unseen\naffordances. Besides, we build a large-scale Purpose-driven Affordance Dataset\nv2 (PADv2) by collecting and labeling 30k images from 39 affordance and 103\nobject categories. With complex scenes and rich annotations, our PADv2 dataset\ncan be used as a test bed to benchmark affordance detection methods and may\nalso facilitate downstream vision tasks, such as scene understanding, action\nrecognition, and robot manipulation. Specifically, we conducted comprehensive\nexperiments on PADv2 dataset by including 11 advanced models from several\nrelated research fields. Experimental results demonstrate the superiority of\nour model over previous representative ones in terms of both objective metrics\nand visual quality. The benchmark suite is available at\nhttps://github.com/lhc1224/OSAD Net.",
          "link": "http://arxiv.org/abs/2108.03658",
          "publishedOn": "2021-08-10T02:00:08.330Z",
          "wordCount": 669,
          "title": "One-Shot Object Affordance Detection in the Wild. (arXiv:2108.03658v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03690",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yueqi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Ka Leong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "Although deep learning based image compression methods have achieved\npromising progress these days, the performance of these methods still cannot\nmatch the latest compression standard Versatile Video Coding (VVC). Most of the\nrecent developments focus on designing a more accurate and flexible entropy\nmodel that can better parameterize the distributions of the latent features.\nHowever, few efforts are devoted to structuring a better transformation between\nthe image space and the latent feature space. In this paper, instead of\nemploying previous autoencoder style networks to build this transformation, we\npropose an enhanced Invertible Encoding Network with invertible neural networks\n(INNs) to largely mitigate the information loss problem for better compression.\nExperimental results on the Kodak, CLIC, and Tecnick datasets show that our\nmethod outperforms the existing learned image compression methods and\ncompression standards, including VVC (VTM 12.1), especially for high-resolution\nimages. Our source code is available at https://github.com/xyq7/InvCompress.",
          "link": "http://arxiv.org/abs/2108.03690",
          "publishedOn": "2021-08-10T02:00:08.308Z",
          "wordCount": 591,
          "title": "Enhanced Invertible Encoding for Learned Image Compression. (arXiv:2108.03690v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiale Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yong Ding</a>",
          "description": "In this paper, we present an Intersection-over-Union (IoU) guided two-stage\n3D object detector with a voxel-to-point decoder. To preserve the necessary\ninformation from all raw points and maintain the high box recall in voxel based\nRegion Proposal Network (RPN), we propose a residual voxel-to-point decoder to\nextract the point features in addition to the map-view features from the voxel\nbased RPN. We use a 3D Region of Interest (RoI) alignment to crop and align the\nfeatures with the proposal boxes for accurately perceiving the object position.\nThe RoI-Aligned features are finally aggregated with the corner geometry\nembeddings that can provide the potentially missing corner information in the\nbox refinement stage. We propose a simple and efficient method to align the\nestimated IoUs to the refined proposal boxes as a more relevant localization\nconfidence. The comprehensive experiments on KITTI and Waymo Open Dataset\ndemonstrate that our method achieves significant improvements with novel\narchitectures against the existing methods. The code is available on Github\nURL\\footnote{\\url{https://github.com/jialeli1/From-Voxel-to-Point}}.",
          "link": "http://arxiv.org/abs/2108.03648",
          "publishedOn": "2021-08-10T02:00:08.281Z",
          "wordCount": 633,
          "title": "From Voxel to Point: IoU-guided 3D Object Detection for Point Cloud with Voxel-to-Point Decoder. (arXiv:2108.03648v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03596",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1\">Qi Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bingfeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yi Yuan</a>",
          "description": "Chinese character style transfer is a very challenging problem because of the\ncomplexity of the glyph shapes or underlying structures and large numbers of\nexisted characters, when comparing with English letters. Moreover, the\nhandwriting of calligraphy masters has a more irregular stroke and is difficult\nto obtain in real-world scenarios. Recently, several GAN-based methods have\nbeen proposed for font synthesis, but some of them require numerous reference\ndata and the other part of them have cumbersome preprocessing steps to divide\nthe character into different parts to be learned and transferred separately. In\nthis paper, we propose a simple but powerful end-to-end Chinese calligraphy\nfont generation framework ZiGAN, which does not require any manual operation or\nredundant preprocessing to generate fine-grained target-style characters with\nfew-shot references. To be specific, a few paired samples from different\ncharacter styles are leveraged to attain a fine-grained correlation between\nstructures underlying different glyphs. To capture valuable style knowledge in\ntarget and strengthen the coarse-grained understanding of character content, we\nutilize multiple unpaired samples to align the feature distributions belonging\nto different character styles. By doing so, only a few target Chinese\ncalligraphy characters are needed to generated expected style transferred\ncharacters. Experiments demonstrate that our method has a state-of-the-art\ngeneralization ability in few-shot Chinese character style transfer.",
          "link": "http://arxiv.org/abs/2108.03596",
          "publishedOn": "2021-08-10T02:00:08.251Z",
          "wordCount": 664,
          "title": "ZiGAN: Fine-grained Chinese Calligraphy Font Generation via a Few-shot Style Transfer Approach. (arXiv:2108.03596v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhengyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Qiqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiangmiao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>",
          "description": "Unsupervised domain adaptation (UDA) aims to adapt a model of the labeled\nsource domain to an unlabeled target domain. Although the domain shifts may\nexist in various dimensions such as appearance, textures, etc, the contextual\ndependency, which is generally shared across different domains, is neglected by\nrecent methods. In this paper, we utilize this important clue as explicit prior\nknowledge and propose end-to-end Context-Aware Mixup (CAMix) for domain\nadaptive semantic segmentation. Firstly, we design a contextual mask generation\nstrategy by leveraging accumulated spatial distributions and contextual\nrelationships. The generated contextual mask is critical in this work and will\nguide the domain mixup. In addition, we define the significance mask to\nindicate where the pixels are credible. To alleviate the over-alignment (e.g.,\nearly performance degradation), the source and target significance masks are\nmixed based on the contextual mask into the mixed significance mask, and we\nintroduce a significance-reweighted consistency loss on it. Experimental\nresults show that the proposed method outperforms the state-of-the-art methods\nby a large margin on two widely-used domain adaptation benchmarks, i.e., GTAV\n$\\rightarrow $ Cityscapes and SYNTHIA $\\rightarrow $ Cityscapes.",
          "link": "http://arxiv.org/abs/2108.03557",
          "publishedOn": "2021-08-10T02:00:08.243Z",
          "wordCount": 624,
          "title": "Context-Aware Mixup for Domain Adaptive Semantic Segmentation. (arXiv:2108.03557v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ganz_R/0/1/0/all/0/1\">Roy Ganz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1\">Michael Elad</a>",
          "description": "The interest of the machine learning community in image synthesis has grown\nsignificantly in recent years, with the introduction of a wide range of deep\ngenerative models and means for training them. Such machines' ultimate goal is\nto match the distributions of the given training images and the synthesized\nones. In this work, we propose a general model-agnostic technique for improving\nthe image quality and the distribution fidelity of generated images, obtained\nby any generative model. Our method, termed BIGRoC (boosting image generation\nvia a robust classifier), is based on a post-processing procedure via the\nguidance of a given robust classifier and without a need for additional\ntraining of the generative model. Given a synthesized image, we propose to\nupdate it through projected gradient steps over the robust classifier, in an\nattempt to refine its recognition. We demonstrate this post-processing\nalgorithm on various image synthesis methods and show a significant improvement\nof the generated images, both quantitatively and qualitatively.",
          "link": "http://arxiv.org/abs/2108.03702",
          "publishedOn": "2021-08-10T02:00:08.221Z",
          "wordCount": 591,
          "title": "BIGRoC: Boosting Image Generation via a Robust Classifier. (arXiv:2108.03702v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03634",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiale Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yong Ding</a>",
          "description": "Most of the existing single-stage and two-stage 3D object detectors are\nanchor-based methods, while the efficient but challenging anchor-free\nsingle-stage 3D object detection is not well investigated. Recent studies on 2D\nobject detection show that the anchor-free methods also are of great potential.\nHowever, the unordered and sparse properties of point clouds prevent us from\ndirectly leveraging the advanced 2D methods on 3D point clouds. We overcome\nthis by converting the voxel-based sparse 3D feature volumes into the sparse 2D\nfeature maps. We propose an attentive module to fit the sparse feature maps to\ndense mostly on the object regions through the deformable convolution tower and\nthe supervised mask-guided attention. By directly regressing the 3D bounding\nbox from the enhanced and dense feature maps, we construct a novel single-stage\n3D detector for point clouds in an anchor-free manner. We propose an IoU-based\ndetection confidence re-calibration scheme to improve the correlation between\nthe detection confidence score and the accuracy of the bounding box regression.\nOur code is publicly available at \\url{https://github.com/jialeli1/MGAF-3DSSD}.",
          "link": "http://arxiv.org/abs/2108.03634",
          "publishedOn": "2021-08-10T02:00:08.212Z",
          "wordCount": 634,
          "title": "Anchor-free 3D Single Stage Detector with Mask-Guided Attention for Point Cloud. (arXiv:2108.03634v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deepshikha_K/0/1/0/all/0/1\">Kumari Deepshikha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yelleni_S/0/1/0/all/0/1\">Sai Harsha Yelleni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srijith_P/0/1/0/all/0/1\">P.K. Srijith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_C/0/1/0/all/0/1\">C Krishna Mohan</a>",
          "description": "With the advancements made in deep learning, computer vision problems like\nobject detection and segmentation have seen a great improvement in performance.\nHowever, in many real-world applications such as autonomous driving vehicles,\nthe risk associated with incorrect predictions of objects is very high.\nStandard deep learning models for object detection such as YOLO models are\noften overconfident in their predictions and do not take into account the\nuncertainty in predictions on out-of-distribution data. In this work, we\npropose an efficient and effective approach to model uncertainty in object\ndetection and segmentation tasks using Monte-Carlo DropBlock (MC-DropBlock)\nbased inference. The proposed approach applies drop-block during training time\nand test time on the convolutional layer of the deep learning models such as\nYOLO. We show that this leads to a Bayesian convolutional neural network\ncapable of capturing the epistemic uncertainty in the model. Additionally, we\ncapture the aleatoric uncertainty using a Gaussian likelihood. We demonstrate\nthe effectiveness of the proposed approach on modeling uncertainty in object\ndetection and segmentation tasks using out-of-distribution experiments.\nExperimental results show that MC-DropBlock improves the generalization,\ncalibration, and uncertainty modeling capabilities of YOLO models in object\ndetection and segmentation.",
          "link": "http://arxiv.org/abs/2108.03614",
          "publishedOn": "2021-08-10T02:00:08.206Z",
          "wordCount": 633,
          "title": "Monte Carlo DropBlock for Modelling Uncertainty in Object Detection. (arXiv:2108.03614v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03506",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagmar_A/0/1/0/all/0/1\">Aadesh Bagmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiya_S/0/1/0/all/0/1\">Shishira R Maiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bidwalka_S/0/1/0/all/0/1\">Shruti Bidwalka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Amol Deshpande</a>",
          "description": "The vulnerability of the Lottery Ticket Hypothesis has not been studied from\nthe purview of Membership Inference Attacks. Through this work, we are the\nfirst to empirically show that the lottery ticket networks are equally\nvulnerable to membership inference attacks. A Membership Inference Attack (MIA)\nis the process of determining whether a data sample belongs to a training set\nof a trained model or not. Membership Inference Attacks could leak critical\ninformation about the training data that can be used for targeted attacks.\nRecent deep learning models often have very large memory footprints and a high\ncomputational cost associated with training and drawing inferences. Lottery\nTicket Hypothesis is used to prune the networks to find smaller sub-networks\nthat at least match the performance of the original model in terms of test\naccuracy in a similar number of iterations. We used CIFAR-10, CIFAR-100, and\nImageNet datasets to perform image classification tasks and observe that the\nattack accuracies are similar. We also see that the attack accuracy varies\ndirectly according to the number of classes in the dataset and the sparsity of\nthe network. We demonstrate that these attacks are transferable across models\nwith high accuracy.",
          "link": "http://arxiv.org/abs/2108.03506",
          "publishedOn": "2021-08-10T02:00:08.199Z",
          "wordCount": 652,
          "title": "Membership Inference Attacks on Lottery Ticket Networks. (arXiv:2108.03506v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03611",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tian Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>",
          "description": "Brain tumor is a common and fatal form of cancer which affects both adults\nand children. The classification of brain tumors into different types is hence\na crucial task, as it greatly influences the treatment that physicians will\nprescribe. In light of this, medical imaging techniques, especially those\napplying deep convolutional networks followed by a classification layer, have\nbeen developed to make possible computer-aided classification of brain tumor\ntypes. In this paper, we present a novel approach of directly learning deep\nembeddings for brain tumor types, which can be used for downstream tasks such\nas classification. Along with using triplet loss variants, our approach applies\ncontrastive learning to performing unsupervised pre-training, combined with a\nrare-case data augmentation module to effectively ameliorate the lack of data\nproblem in the brain tumor imaging analysis domain. We evaluate our method on\nan extensive brain tumor dataset which consists of 27 different tumor classes,\nout of which 13 are defined as rare. With a common encoder during all the\nexperiments, we compare our approach with a baseline classification-layer based\nmodel, and the results well prove the effectiveness of our approach across all\nmeasured metrics.",
          "link": "http://arxiv.org/abs/2108.03611",
          "publishedOn": "2021-08-10T02:00:08.192Z",
          "wordCount": 622,
          "title": "Triplet Contrastive Learning for Brain Tumor Classification. (arXiv:2108.03611v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+J_P/0/1/0/all/0/1\">Pawan S J</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rishi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_H/0/1/0/all/0/1\">Hemanth Sai Ram Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vani_M/0/1/0/all/0/1\">M Vani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_J/0/1/0/all/0/1\">Jeny Rajan</a>",
          "description": "The capsule network is a distinct and promising segment of the neural network\nfamily that drew attention due to its unique ability to maintain the\nequivariance property by preserving the spatial relationship amongst the\nfeatures. The capsule network has attained unprecedented success over image\nclassification tasks with datasets such as MNIST and affNIST by encoding the\ncharacteristic features into the capsules and building the parse-tree\nstructure. However, on the datasets involving complex foreground and background\nregions such as CIFAR-10, the performance of the capsule network is sub-optimal\ndue to its naive data routing policy and incompetence towards extracting\ncomplex features. This paper proposes a new design strategy for capsule network\narchitecture for efficiently dealing with complex images. The proposed method\nincorporates wide bottleneck residual modules and the Squeeze and Excitation\nattention blocks upheld by the modified FM routing algorithm to address the\ndefined problem. A wide bottleneck residual module facilitates extracting\ncomplex features followed by the squeeze and excitation attention block to\nenable channel-wise attention by suppressing the trivial features. This setup\nallows channel inter-dependencies at almost no computational cost, thereby\nenhancing the representation ability of capsules on complex images. We\nextensively evaluate the performance of the proposed model on three publicly\navailable datasets, namely CIFAR-10, Fashion MNIST, and SVHN, to outperform the\ntop-5 performance on CIFAR-10 and Fashion MNIST with highly competitive\nperformance on the SVHN dataset.",
          "link": "http://arxiv.org/abs/2108.03627",
          "publishedOn": "2021-08-10T02:00:08.185Z",
          "wordCount": 685,
          "title": "WideCaps: A Wide Attention based Capsule Network for Image Classification. (arXiv:2108.03627v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yunyao Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>",
          "description": "Semi-supervised video object segmentation is a task of segmenting the target\nobject in a video sequence given only a mask annotation in the first frame. The\nlimited information available makes it an extremely challenging task. Most\nprevious best-performing methods adopt matching-based transductive reasoning or\nonline inductive learning. Nevertheless, they are either less discriminative\nfor similar instances or insufficient in the utilization of spatio-temporal\ninformation. In this work, we propose to integrate transductive and inductive\nlearning into a unified framework to exploit the complementarity between them\nfor accurate and robust video object segmentation. The proposed approach\nconsists of two functional branches. The transduction branch adopts a\nlightweight transformer architecture to aggregate rich spatio-temporal cues\nwhile the induction branch performs online inductive learning to obtain\ndiscriminative target information. To bridge these two diverse branches, a\ntwo-head label encoder is introduced to learn the suitable target prior for\neach of them. The generated mask encodings are further forced to be\ndisentangled to better retain their complementarity. Extensive experiments on\nseveral prevalent benchmarks show that, without the need of synthetic training\ndata, the proposed approach sets a series of new state-of-the-art records. Code\nis available at https://github.com/maoyunyao/JOINT.",
          "link": "http://arxiv.org/abs/2108.03679",
          "publishedOn": "2021-08-10T02:00:08.164Z",
          "wordCount": 639,
          "title": "Joint Inductive and Transductive Learning for Video Object Segmentation. (arXiv:2108.03679v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03332",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lingelbach_M/0/1/0/all/0/1\">Michael Lingelbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1\">Kent Vainio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zheng Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokmen_C/0/1/0/all/0/1\">Cem Gokmen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1\">Shyamal Buch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">C. Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gweon_H/0/1/0/all/0/1\">Hyowon Gweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>",
          "description": "We introduce BEHAVIOR, a benchmark for embodied AI with 100 activities in\nsimulation, spanning a range of everyday household chores such as cleaning,\nmaintenance, and food preparation. These activities are designed to be\nrealistic, diverse, and complex, aiming to reproduce the challenges that agents\nmust face in the real world. Building such a benchmark poses three fundamental\ndifficulties for each activity: definition (it can differ by time, place, or\nperson), instantiation in a simulator, and evaluation. BEHAVIOR addresses these\nwith three innovations. First, we propose an object-centric, predicate\nlogic-based description language for expressing an activity's initial and goal\nconditions, enabling generation of diverse instances for any activity. Second,\nwe identify the simulator-agnostic features required by an underlying\nenvironment to support BEHAVIOR, and demonstrate its realization in one such\nsimulator. Third, we introduce a set of metrics to measure task progress and\nefficiency, absolute and relative to human demonstrators. We include 500 human\ndemonstrations in virtual reality (VR) to serve as the human ground truth. Our\nexperiments demonstrate that even state of the art embodied AI solutions\nstruggle with the level of realism, diversity, and complexity imposed by the\nactivities in our benchmark. We make BEHAVIOR publicly available at\nbehavior.stanford.edu to facilitate and calibrate the development of new\nembodied AI solutions.",
          "link": "http://arxiv.org/abs/2108.03332",
          "publishedOn": "2021-08-10T02:00:08.158Z",
          "wordCount": 679,
          "title": "BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments. (arXiv:2108.03332v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Rongrong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_N/0/1/0/all/0/1\">Na Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "We present a novel approach to joint depth and normal estimation for\ntime-of-flight (ToF) sensors. Our model learns to predict the high-quality\ndepth and normal maps jointly from ToF raw sensor data. To achieve this, we\nmeticulously constructed the first large-scale dataset (named ToF-100) with\npaired raw ToF data and ground-truth high-resolution depth maps provided by an\nindustrial depth camera. In addition, we also design a simple but effective\nframework for joint depth and normal estimation, applying a robust Chamfer loss\nvia jittering to improve the performance of our model. Our experiments\ndemonstrate that our proposed method can efficiently reconstruct\nhigh-resolution depth and normal maps and significantly outperforms\nstate-of-the-art approaches. Our code and data will be available at\n\\url{https://github.com/hkustVisionRr/JointlyDepthNormalEstimation}",
          "link": "http://arxiv.org/abs/2108.03649",
          "publishedOn": "2021-08-10T02:00:08.148Z",
          "wordCount": 573,
          "title": "Joint Depth and Normal Estimation from Real-world Time-of-flight Raw Data. (arXiv:2108.03649v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03635",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zexi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_H/0/1/0/all/0/1\">Henry Wing Fung Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yuk Ying Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haisheng Li</a>",
          "description": "As an image sensing instrument, light field images can supply extra angular\ninformation compared with monocular images and have facilitated a wide range of\nmeasurement applications. Light field image capturing devices usually suffer\nfrom the inherent trade-off between the angular and spatial resolutions. To\ntackle this problem, several methods, such as light field reconstruction and\nlight field super-resolution, have been proposed but leaving two problems\nunaddressed, namely domain asymmetry and efficient information flow. In this\npaper, we propose an end-to-end Spatio-Angular Dense Network (SADenseNet) for\nlight field reconstruction with two novel components, namely correlation blocks\nand spatio-angular dense skip connections to address them. The former performs\neffective modeling of the correlation information in a way that conforms with\nthe domain asymmetry. And the latter consists of three kinds of connections\nenhancing the information flow within two domains. Extensive experiments on\nboth real-world and synthetic datasets have been conducted to demonstrate that\nthe proposed SADenseNet's state-of-the-art performance at significantly reduced\ncosts in memory and computation. The qualitative results show that the\nreconstructed light field images are sharp with correct details and can serve\nas pre-processing to improve the accuracy of related measurement applications.",
          "link": "http://arxiv.org/abs/2108.03635",
          "publishedOn": "2021-08-10T02:00:08.138Z",
          "wordCount": 633,
          "title": "Efficient Light Field Reconstruction via Spatio-Angular Dense Network. (arXiv:2108.03635v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wilms_C/0/1/0/all/0/1\">Christian Wilms</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frintrop_S/0/1/0/all/0/1\">Simone Frintrop</a>",
          "description": "Class-agnostic object proposal generation is an important first step in many\nobject detection pipelines. However, object proposals of modern systems are\nrather inaccurate in terms of segmentation and only roughly adhere to object\nboundaries. Since typical refinement steps are usually not applicable to\nthousands of proposals, we propose a superpixel-based refinement system for\nobject proposal generation systems. Utilizing precise superpixels and\nsuperpixel pooling on deep features, we refine initial coarse proposals in an\nend-to-end learned system. Furthermore, we propose a novel DeepFH segmentation,\nwhich enriches the classic Felzenszwalb and Huttenlocher (FH) segmentation with\ndeep features leading to improved segmentation results and better object\nproposal refinements. On the COCO dataset with LVIS annotations, we show that\nour refinement based on DeepFH superpixels outperforms state-of-the-art methods\nand leads to more precise object proposals.",
          "link": "http://arxiv.org/abs/2108.03503",
          "publishedOn": "2021-08-10T02:00:08.114Z",
          "wordCount": 564,
          "title": "DeepFH Segmentations for Superpixel-based Object Proposal Refinement. (arXiv:2108.03503v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Isobe_T/0/1/0/all/0/1\">Takashi Isobe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Lu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Yi Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengjin Wang</a>",
          "description": "In this work, we address the problem of unsupervised domain adaptation for\nperson re-ID where annotations are available for the source domain but not for\ntarget. Previous methods typically follow a two-stage optimization pipeline,\nwhere the network is first pre-trained on source and then fine-tuned on target\nwith pseudo labels created by feature clustering. Such methods sustain two main\nlimitations. (1) The label noise may hinder the learning of discriminative\nfeatures for recognizing target classes. (2) The domain gap may hinder\nknowledge transferring from source to target. We propose three types of\ntechnical schemes to alleviate these issues. First, we propose a cluster-wise\ncontrastive learning algorithm (CCL) by iterative optimization of feature\nlearning and cluster refinery to learn noise-tolerant representations in the\nunsupervised manner. Second, we adopt a progressive domain adaptation (PDA)\nstrategy to gradually mitigate the domain gap between source and target data.\nThird, we propose Fourier augmentation (FA) for further maximizing the class\nseparability of re-ID models by imposing extra constraints in the Fourier\nspace. We observe that these proposed schemes are capable of facilitating the\nlearning of discriminative feature representations. Experiments demonstrate\nthat our method consistently achieves notable improvements over the\nstate-of-the-art unsupervised re-ID methods on multiple benchmarks, e.g.,\nsurpassing MMT largely by 8.1\\%, 9.9\\%, 11.4\\% and 11.1\\% mAP on the\nMarket-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT tasks,\nrespectively.",
          "link": "http://arxiv.org/abs/2108.03439",
          "publishedOn": "2021-08-10T02:00:08.091Z",
          "wordCount": 668,
          "title": "Towards Discriminative Representation Learning for Unsupervised Person Re-identification. (arXiv:2108.03439v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1\">Fengjun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_T/0/1/0/all/0/1\">Teng Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>",
          "description": "Superimposing visible watermarks on images provides a powerful weapon to cope\nwith the copyright issue. Watermark removal techniques, which can strengthen\nthe robustness of visible watermarks in an adversarial way, have attracted\nincreasing research interest. Modern watermark removal methods perform\nwatermark localization and background restoration simultaneously, which could\nbe viewed as a multi-task learning problem. However, existing approaches suffer\nfrom incomplete detected watermark and degraded texture quality of restored\nbackground. Therefore, we design a two-stage multi-task network to address the\nabove issues. The coarse stage consists of a watermark branch and a background\nbranch, in which the watermark branch self-calibrates the roughly estimated\nmask and passes the calibrated mask to background branch to reconstruct the\nwatermarked area. In the refinement stage, we integrate multi-level features to\nimprove the texture quality of watermarked area. Extensive experiments on two\ndatasets demonstrate the effectiveness of our proposed method.",
          "link": "http://arxiv.org/abs/2108.03581",
          "publishedOn": "2021-08-10T02:00:08.077Z",
          "wordCount": 588,
          "title": "Visible Watermark Removal via Self-calibrated Localization and Background Refinement. (arXiv:2108.03581v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03418",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lai_Q/0/1/0/all/0/1\">Qiuxia Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Ailing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hanqiu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiang Xu</a>",
          "description": "The selective visual attention mechanism in the human visual system (HVS)\nrestricts the amount of information to reach visual awareness for perceiving\nnatural scenes, allowing near real-time information processing with limited\ncomputational capacity [Koch and Ullman, 1987]. This kind of selectivity acts\nas an 'Information Bottleneck (IB)', which seeks a trade-off between\ninformation compression and predictive accuracy. However, such information\nconstraints are rarely explored in the attention mechanism for deep neural\nnetworks (DNNs). In this paper, we propose an IB-inspired spatial attention\nmodule for DNN structures built for visual recognition. The module takes as\ninput an intermediate representation of the input image, and outputs a\nvariational 2D attention map that minimizes the mutual information (MI) between\nthe attention-modulated representation and the input, while maximizing the MI\nbetween the attention-modulated representation and the task label. To further\nrestrict the information bypassed by the attention map, we quantize the\ncontinuous attention scores to a set of learnable anchor values during\ntraining. Extensive experiments show that the proposed IB-inspired spatial\nattention mechanism can yield attention maps that neatly highlight the regions\nof interest while suppressing backgrounds, and bootstrap standard DNN\nstructures for visual recognition tasks (e.g., image classification,\nfine-grained recognition, cross-domain classification). The attention maps are\ninterpretable for the decision making of the DNNs as verified in the\nexperiments. Our code is available at https://github.com/ashleylqx/AIB.git.",
          "link": "http://arxiv.org/abs/2108.03418",
          "publishedOn": "2021-08-10T02:00:08.065Z",
          "wordCount": 670,
          "title": "Information Bottleneck Approach to Spatial Attention Learning. (arXiv:2108.03418v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Haoyu Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>",
          "description": "Lane segmentation is a challenging issue in autonomous driving system\ndesigning because lane marks show weak textural consistency due to occlusion or\nextreme illumination but strong geometric continuity in traffic images, from\nwhich general convolution neural networks (CNNs) are not capable of learning\nsemantic objects. To empower conventional CNNs in learning geometric clues of\nlanes, we propose a deep network named ContinuityLearner to better learn\ngeometric prior within lane. Specifically, our proposed CNN-based paradigm\ninvolves a novel Context-encoding image feature learning network to generate\nclass-dependent image feature maps and a new encoding layer to exploit the\ngeometric continuity feature representation by fusing both spatial and visual\ninformation of lane together. The ContinuityLearner, performing on the\ngeometric continuity feature of lanes, is trained to directly predict the lane\nin traffic scenarios with integrated and continuous instance semantic. The\nexperimental results on the CULane dataset and the Tusimple benchmark\ndemonstrate that our ContinuityLearner has superior performance over other\nstate-of-the-art techniques in lane segmentation.",
          "link": "http://arxiv.org/abs/2108.03507",
          "publishedOn": "2021-08-10T02:00:08.055Z",
          "wordCount": 597,
          "title": "ContinuityLearner: Geometric Continuity Feature Learning for Lane Segmentation. (arXiv:2108.03507v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03551",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Lv Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mofei Song</a>",
          "description": "Aiming at discovering and locating most distinctive objects from visual\nscenes, salient object detection (SOD) plays an essential role in various\ncomputer vision systems. Coming to the era of high resolution, SOD methods are\nfacing new challenges. The major limitation of previous methods is that they\ntry to identify the salient regions and estimate the accurate objects\nboundaries simultaneously with a single regression task at low-resolution. This\npractice ignores the inherent difference between the two difficult problems,\nresulting in poor detection quality. In this paper, we propose a novel deep\nlearning framework for high-resolution SOD task, which disentangles the task\ninto a low-resolution saliency classification network (LRSCN) and a\nhigh-resolution refinement network (HRRN). As a pixel-wise classification task,\nLRSCN is designed to capture sufficient semantics at low-resolution to identify\nthe definite salient, background and uncertain image regions. HRRN is a\nregression task, which aims at accurately refining the saliency value of pixels\nin the uncertain region to preserve a clear object boundary at high-resolution\nwith limited GPU memory. It is worth noting that by introducing uncertainty\ninto the training process, our HRRN can well address the high-resolution\nrefinement task without using any high-resolution training data. Extensive\nexperiments on high-resolution saliency datasets as well as some widely used\nsaliency benchmarks show that the proposed method achieves superior performance\ncompared to the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.03551",
          "publishedOn": "2021-08-10T02:00:08.046Z",
          "wordCount": 658,
          "title": "Disentangled High Quality Salient Object Detection. (arXiv:2108.03551v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Al_Saffar_A/0/1/0/all/0/1\">A. Al-Saffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stancombe_A/0/1/0/all/0/1\">A. Stancombe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_A/0/1/0/all/0/1\">A. Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbosh_A/0/1/0/all/0/1\">A. Abbosh</a>",
          "description": "Incorporating boundaries of the imaging object as a priori information to\nimaging algorithms can significantly improve the performance of electromagnetic\nmedical imaging systems. To avoid overly complicating the system by using\ndifferent sensors and the adverse effect of the subject's movement, a\nlearning-based method is proposed to estimate the boundary (external contour)\nof the imaged object using the same electromagnetic imaging data. While imaging\ntechniques may discard the reflection coefficients for being dominant and\nuninformative for imaging, these parameters are made use of for boundary\ndetection. The learned model is verified through independent clinical human\ntrials by using a head imaging system with a 16-element antenna array that\nworks across the band 0.7-1.6 GHz. The evaluation demonstrated that the model\nachieves average dissimilarity of 0.012 in Hu-moment while detecting head\nboundary. The model enables fast scan and image creation while eliminating the\nneed for additional devices for accurate boundary estimation.",
          "link": "http://arxiv.org/abs/2108.03233",
          "publishedOn": "2021-08-10T02:00:08.019Z",
          "wordCount": 597,
          "title": "Operational Learning-based Boundary Estimation in Electromagnetic Medical Imaging. (arXiv:2108.03233v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yifan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiahao_T/0/1/0/all/0/1\">Tom Z.Jiahao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiancong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yushkevich_P/0/1/0/all/0/1\">Paul A.Yushkevich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1\">James C.Gee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_M/0/1/0/all/0/1\">M.Ani Hsieh</a>",
          "description": "Deformable image registration, aiming to find spatial correspondence between\na given image pair, is one of the most critical problems in the domain of\nmedical image analysis. In this paper, we present a generic, fast, and accurate\ndiffeomorphic image registration framework that leverages neural ordinary\ndifferential equations (NODEs). We model each voxel as a moving particle and\nconsider the set of all voxels in a 3D image as a high-dimensional dynamical\nsystem whose trajectory determines the targeted deformation field. Compared\nwith traditional optimization-based methods, our framework reduces the running\ntime from tens of minutes to tens of seconds. Compared with recent data-driven\ndeep learning methods, our framework is more accessible since it does not\nrequire large amounts of training data. Our experiments show that the\nregistration results of our method outperform state-of-the-arts under various\nmetrics, indicating that our modeling approach is well fitted for the task of\ndeformable image registration.",
          "link": "http://arxiv.org/abs/2108.03443",
          "publishedOn": "2021-08-10T02:00:08.013Z",
          "wordCount": 586,
          "title": "Deformable Image Registration using Neural ODEs. (arXiv:2108.03443v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yingqing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yazhou Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "Portrait images often suffer from undesirable shadows cast by casual objects\nor even the face itself. While existing methods for portrait shadow removal\nrequire training on a large-scale synthetic dataset, we propose the first\nunsupervised method for portrait shadow removal without any training data. Our\nkey idea is to leverage the generative facial priors embedded in the\noff-the-shelf pretrained StyleGAN2. To achieve this, we formulate the shadow\nremoval task as a layer decomposition problem: a shadowed portrait image is\nconstructed by the blending of a shadow image and a shadow-free image. We\npropose an effective progressive optimization algorithm to learn the\ndecomposition process. Our approach can also be extended to portrait tattoo\nremoval and watermark removal. Qualitative and quantitative experiments on a\nreal-world portrait shadow dataset demonstrate that our approach achieves\ncomparable performance with supervised shadow removal methods. Our source code\nis available at\nhttps://github.com/YingqingHe/Shadow-Removal-via-Generative-Priors.",
          "link": "http://arxiv.org/abs/2108.03466",
          "publishedOn": "2021-08-10T02:00:07.999Z",
          "wordCount": 601,
          "title": "Unsupervised Portrait Shadow Removal via Generative Priors. (arXiv:2108.03466v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_M/0/1/0/all/0/1\">Md. Tareq Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohammed Eunus Ali</a>",
          "description": "Reconstructing a layout of indoor spaces has been a crucial part of growing\nindoor location based services. One of the key challenges in the proliferation\nof indoor location based services is the unavailability of indoor spatial maps\ndue to the complex nature of capturing an indoor space model (e.g., floor plan)\nof an existing building. In this paper, we propose a system to automatically\ngenerate floor plans that can recognize rooms from the point-clouds obtained\nthrough smartphones like Google's Tango. In particular, we propose two\napproaches - a Recurrent Neural Network based approach using Pointer Network\nand a Convolutional Neural Network based approach using Mask-RCNN to identify\nrooms (and thereby floor plans) from point-clouds. Experimental results on\ndifferent datasets demonstrate approximately 0.80-0.90 Intersection-over-Union\nscores, which show that our models can effectively identify the rooms and\nregenerate the shapes of the rooms in heterogeneous environment.",
          "link": "http://arxiv.org/abs/2108.03378",
          "publishedOn": "2021-08-10T02:00:07.987Z",
          "wordCount": 578,
          "title": "Learning Indoor Layouts from Simple Point-Clouds. (arXiv:2108.03378v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03489",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_C/0/1/0/all/0/1\">Cristina Vasconcelos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1\">Hugo Larochelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1\">Vincent Dumoulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romijnders_R/0/1/0/all/0/1\">Rob Romijnders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1\">Nicolas Le Roux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goroshin_R/0/1/0/all/0/1\">Ross Goroshin</a>",
          "description": "We investigate the impact of aliasing on generalization in Deep Convolutional\nNetworks and show that data augmentation schemes alone are unable to prevent it\ndue to structural limitations in widely used architectures. Drawing insights\nfrom frequency analysis theory, we take a closer look at ResNet and\nEfficientNet architectures and review the trade-off between aliasing and\ninformation loss in each of their major components. We show how to mitigate\naliasing by inserting non-trainable low-pass filters at key locations,\nparticularly where networks lack the capacity to learn them. These simple\narchitectural changes lead to substantial improvements in generalization on\ni.i.d. and even more on out-of-distribution conditions, such as image\nclassification under natural corruptions on ImageNet-C [11] and few-shot\nlearning on Meta-Dataset [26]. State-of-the art results are achieved on both\ndatasets without introducing additional trainable parameters and using the\ndefault hyper-parameters of open source codebases.",
          "link": "http://arxiv.org/abs/2108.03489",
          "publishedOn": "2021-08-10T02:00:07.884Z",
          "wordCount": 594,
          "title": "Impact of Aliasing on Generalization in Deep Convolutional Networks. (arXiv:2108.03489v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_N/0/1/0/all/0/1\">Ning Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gengwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chuanlong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>",
          "description": "Fine-tuning from pre-trained ImageNet models has been a simple, effective,\nand popular approach for various computer vision tasks. The common practice of\nfine-tuning is to adopt a default hyperparameter setting with a fixed\npre-trained model, while both of them are not optimized for specific tasks and\ntime constraints. Moreover, in cloud computing or GPU clusters where the tasks\narrive sequentially in a stream, faster online fine-tuning is a more desired\nand realistic strategy for saving money, energy consumption, and CO2 emission.\nIn this paper, we propose a joint Neural Architecture Search and Online\nAdaption framework named NASOA towards a faster task-oriented fine-tuning upon\nthe request of users. Specifically, NASOA first adopts an offline NAS to\nidentify a group of training-efficient networks to form a pretrained model zoo.\nWe propose a novel joint block and macro-level search space to enable a\nflexible and efficient search. Then, by estimating fine-tuning performance via\nan adaptive model by accumulating experience from the past tasks, an online\nschedule generator is proposed to pick up the most suitable model and generate\na personalized training regime with respect to each desired task in a one-shot\nfashion. The resulting model zoo is more training efficient than SOTA models,\ne.g. 6x faster than RegNetY-16GF, and 1.7x faster than EfficientNetB3.\nExperiments on multiple datasets also show that NASOA achieves much better\nfine-tuning results, i.e. improving around 2.1% accuracy than the best\nperformance in RegNet series under various constraints and tasks; 40x faster\ncompared to the BOHB.",
          "link": "http://arxiv.org/abs/2108.03434",
          "publishedOn": "2021-08-10T02:00:07.850Z",
          "wordCount": 701,
          "title": "NASOA: Towards Faster Task-oriented Online Fine-tuning with a Zoo of Models. (arXiv:2108.03434v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03457",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zifan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_N/0/1/0/all/0/1\">Na Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1\">Dit-Yan Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "Existing vision systems for autonomous driving or robots are sensitive to\nwaterdrops adhered to windows or camera lenses. Most recent waterdrop removal\napproaches take a single image as input and often fail to recover the missing\ncontent behind waterdrops faithfully. Thus, we propose a learning-based model\nfor waterdrop removal with stereo images. To better detect and remove\nwaterdrops from stereo images, we propose a novel row-wise dilated attention\nmodule to enlarge attention's receptive field for effective information\npropagation between the two stereo images. In addition, we propose an attention\nconsistency loss between the ground-truth disparity map and attention scores to\nenhance the left-right consistency in stereo images. Because of related\ndatasets' unavailability, we collect a real-world dataset that contains stereo\nimages with and without waterdrops. Extensive experiments on our dataset\nsuggest that our model outperforms state-of-the-art methods both quantitatively\nand qualitatively. Our source code and the stereo waterdrop dataset are\navailable at\n\\href{https://github.com/VivianSZF/Stereo-Waterdrop-Removal}{https://github.com/VivianSZF/Stereo-Waterdrop-Removal}",
          "link": "http://arxiv.org/abs/2108.03457",
          "publishedOn": "2021-08-10T02:00:07.716Z",
          "wordCount": 601,
          "title": "Stereo Waterdrop Removal with Row-wise Dilated Attention. (arXiv:2108.03457v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03568",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruohao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Liao Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Dantong Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_J/0/1/0/all/0/1\">Jun Yue</a>",
          "description": "Leaf segmentation is the most direct and effective way for high-throughput\nplant phenotype data analysis and quantitative researches of complex traits.\nCurrently, the primary goal of plant phenotyping is to raise the accuracy of\nthe autonomous phenotypic measurement. In this work, we present the LeafMask\nneural network, a new end-to-end model to delineate each leaf region and count\nthe number of leaves, with two main components: 1) the mask assembly module\nmerging position-sensitive bases of each predicted box after non-maximum\nsuppression (NMS) and corresponding coefficients to generate original masks; 2)\nthe mask refining module elaborating leaf boundaries from the mask assembly\nmodule by the point selection strategy and predictor. In addition, we also\ndesign a novel and flexible multi-scale attention module for the dual\nattention-guided mask (DAG-Mask) branch to effectively enhance information\nexpression and produce more accurate bases. Our main contribution is to\ngenerate the final improved masks by combining the mask assembly module with\nthe mask refining module under the anchor-free instance segmentation paradigm.\nWe validate our LeafMask through extensive experiments on Leaf Segmentation\nChallenge (LSC) dataset. Our proposed model achieves the 90.09% BestDice score\noutperforming other state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.03568",
          "publishedOn": "2021-08-10T02:00:07.688Z",
          "wordCount": 631,
          "title": "LeafMask: Towards Greater Accuracy on Leaf Segmentation. (arXiv:2108.03568v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03553",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Qiqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiangmiao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhengyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>",
          "description": "Domain adaptation aims to bridge the domain shifts between the source and\ntarget domains. These shifts may span different dimensions such as fog,\nrainfall, etc. However, recent methods typically do not consider explicit prior\nknowledge on a specific dimension, thus leading to less desired adaptation\nperformance. In this paper, we study a practical setting called Specific Domain\nAdaptation (SDA) that aligns the source and target domains in a\ndemanded-specific dimension. Within this setting, we observe the intra-domain\ngap induced by different domainness (i.e., numerical magnitudes of this\ndimension) is crucial when adapting to a specific domain. To address the\nproblem, we propose a novel Self-Adversarial Disentangling (SAD) framework. In\nparticular, given a specific dimension, we first enrich the source domain by\nintroducing a domainness creator with providing additional supervisory signals.\nGuided by the created domainness, we design a self-adversarial regularizer and\ntwo loss functions to jointly disentangle the latent representations into\ndomainness-specific and domainness-invariant features, thus mitigating the\nintra-domain gap. Our method can be easily taken as a plug-and-play framework\nand does not introduce any extra costs in the inference time. We achieve\nconsistent improvements over state-of-the-art methods in both object detection\nand semantic segmentation tasks.",
          "link": "http://arxiv.org/abs/2108.03553",
          "publishedOn": "2021-08-10T02:00:07.681Z",
          "wordCount": 636,
          "title": "Self-Adversarial Disentangling for Specific Domain Adaptation. (arXiv:2108.03553v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elashmawy_S/0/1/0/all/0/1\">Shahd Elashmawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramsis_M/0/1/0/all/0/1\">Marian Ramsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eraqi_H/0/1/0/all/0/1\">Hesham M. Eraqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldeshnawy_F/0/1/0/all/0/1\">Farah Eldeshnawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mabrouk_H/0/1/0/all/0/1\">Hadeel Mabrouk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abugabal_O/0/1/0/all/0/1\">Omar Abugabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakr_N/0/1/0/all/0/1\">Nourhan Sakr</a>",
          "description": "Despite the advancement in the domain of audio and audio-visual speech\nrecognition, visual speech recognition systems are still quite under-explored\ndue to the visual ambiguity of some phonemes. In this work, we propose a new\nlip-reading model that combines three contributions. First, the model front-end\nadopts a spatio-temporal attention mechanism to help extract the informative\ndata from the input visual frames. Second, the model back-end utilizes a\nsequence-level and frame-level Knowledge Distillation (KD) techniques that\nallow leveraging audio data during the visual model training. Third, a data\npreprocessing pipeline is adopted that includes facial landmarks\ndetection-based lip-alignment. On LRW lip-reading dataset benchmark, a\nnoticeable accuracy improvement is demonstrated; the spatio-temporal attention,\nKnowledge Distillation, and lip-alignment contributions achieved 88.43%,\n88.64%, and 88.37% respectively.",
          "link": "http://arxiv.org/abs/2108.03543",
          "publishedOn": "2021-08-10T02:00:07.662Z",
          "wordCount": 571,
          "title": "Spatio-Temporal Attention Mechanism and Knowledge Distillation for Lip Reading. (arXiv:2108.03543v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agostinho_S/0/1/0/all/0/1\">S&#xe9;rgio Agostinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osep_A/0/1/0/all/0/1\">Aljo&#x161;a O&#x161;ep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bue_A/0/1/0/all/0/1\">Alessio Del Bue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taix&#xe9;</a>",
          "description": "We tackle data-driven 3D point cloud registration. Given point\ncorrespondences, the standard Kabsch algorithm provides an optimal rotation\nestimate. This allows to train registration models in an end-to-end manner by\ndifferentiating the SVD operation. However, given the initial rotation estimate\nsupplied by Kabsch, we show we can improve point correspondence learning during\nmodel training by extending the original optimization problem. In particular,\nwe linearize the governing constraints of the rotation matrix and solve the\nresulting linear system of equations. We then iteratively produce new solutions\nby updating the initial estimate. Our experiments show that, by plugging our\ndifferentiable layer to existing learning-based registration methods, we\nimprove the correspondence matching quality. This yields up to a 7% decrease in\nrotation error for correspondence-based data-driven registration methods.",
          "link": "http://arxiv.org/abs/2108.03257",
          "publishedOn": "2021-08-10T02:00:07.656Z",
          "wordCount": 572,
          "title": "(Just) A Spoonful of Refinements Helps the Registration Error Go Down. (arXiv:2108.03257v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_E/0/1/0/all/0/1\">Eric Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tu Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swaminathan_V/0/1/0/all/0/1\">Vishy Swaminathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>",
          "description": "Images tell powerful stories but cannot always be trusted. Matching images\nback to trusted sources (attribution) enables users to make a more informed\njudgment of the images they encounter online. We propose a robust image hashing\nalgorithm to perform such matching. Our hash is sensitive to manipulation of\nsubtle, salient visual details that can substantially change the story told by\nan image. Yet the hash is invariant to benign transformations (changes in\nquality, codecs, sizes, shapes, etc.) experienced by images during online\nredistribution. Our key contribution is OSCAR-Net (Object-centric Scene Graph\nAttention for Image Attribution Network); a robust image hashing model inspired\nby recent successes of Transformers in the visual domain. OSCAR-Net constructs\na scene graph representation that attends to fine-grained changes of every\nobject's visual appearance and their spatial relationships. The network is\ntrained via contrastive learning on a dataset of original and manipulated\nimages yielding a state of the art image hash for content fingerprinting that\nscales to millions of images.",
          "link": "http://arxiv.org/abs/2108.03541",
          "publishedOn": "2021-08-10T02:00:07.648Z",
          "wordCount": 606,
          "title": "OSCAR-Net: Object-centric Scene Graph Attention for Image Attribution. (arXiv:2108.03541v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1\">Chenyang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuhua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Chenyang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yankun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wenxiu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "Due to the lack of a large-scale reflection removal dataset with diverse\nreal-world scenes, many existing reflection removal methods are trained on\nsynthetic data plus a small amount of real-world data, which makes it difficult\nto evaluate the strengths or weaknesses of different reflection removal methods\nthoroughly. Furthermore, existing real-world benchmarks and datasets do not\ncategorize image data based on the types and appearances of reflection (e.g.,\nsmoothness, intensity), making it hard to analyze reflection removal methods.\nHence, we construct a new reflection removal dataset that is categorized,\ndiverse, and real-world (CDR). A pipeline based on RAW data is used to capture\nperfectly aligned input images and transmission images. The dataset is\nconstructed using diverse glass types under various environments to ensure\ndiversity. By analyzing several reflection removal methods and conducting\nextensive experiments on our dataset, we show that state-of-the-art reflection\nremoval methods generally perform well on blurry reflection but fail in\nobtaining satisfying performance on other types of real-world reflection. We\nbelieve our dataset can help develop novel methods to remove real-world\nreflection better. Our dataset is available at\nhttps://alexzhao-hugga.github.io/Real-World-Reflection-Removal/.",
          "link": "http://arxiv.org/abs/2108.03380",
          "publishedOn": "2021-08-10T02:00:07.641Z",
          "wordCount": 627,
          "title": "A Categorized Reflection Removal Dataset with Diverse Real-world Scenes. (arXiv:2108.03380v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jia-Ren Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yong-Sheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1\">Wei-Chen Chiu</a>",
          "description": "Faces manifest large variations in many aspects, such as identity,\nexpression, pose, and face styling. Therefore, it is a great challenge to\ndisentangle and extract these characteristics from facial images, especially in\nan unsupervised manner. In this work, we introduce cycle-consistency in facial\ncharacteristics as free supervisory signal to learn facial representations from\nunlabeled facial images. The learning is realized by superimposing the facial\nmotion cycle-consistency and identity cycle-consistency constraints. The main\nidea of the facial motion cycle-consistency is that, given a face with\nexpression, we can perform de-expression to a neutral face via the removal of\nfacial motion and further perform re-expression to reconstruct back to the\noriginal face. The main idea of the identity cycle-consistency is to exploit\nboth de-identity into mean face by depriving the given neutral face of its\nidentity via feature re-normalization and re-identity into neutral face by\nadding the personal attributes to the mean face. At training time, our model\nlearns to disentangle two distinct facial representations to be useful for\nperforming cycle-consistent face reconstruction. At test time, we use the\nlinear protocol scheme for evaluating facial representations on various tasks,\nincluding facial expression recognition and head pose regression. We also can\ndirectly apply the learnt facial representations to person recognition,\nfrontalization and image-to-image translation. Our experiments show that the\nresults of our approach is competitive with those of existing methods,\ndemonstrating the rich and unique information embedded in the disentangled\nrepresentations. Code is available at https://github.com/JiaRenChang/FaceCycle .",
          "link": "http://arxiv.org/abs/2108.03427",
          "publishedOn": "2021-08-10T02:00:07.635Z",
          "wordCount": 683,
          "title": "Learning Facial Representations from the Cycle-consistency of Face. (arXiv:2108.03427v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dube_S/0/1/0/all/0/1\">Simant Dube</a>",
          "description": "The goals of this paper are two-fold. The first goal is to serve as an\nexpository tutorial on the working of deep learning models which emphasizes\ngeometrical intuition about the reasons for success of deep learning. The\nsecond goal is to complement the current results on the expressive power of\ndeep learning models and their loss surfaces with novel insights and results.\nIn particular, we describe how deep neural networks carve out manifolds\nespecially when the multiplication neurons are introduced. Multiplication is\nused in dot products and the attention mechanism and it is employed in capsule\nnetworks and self-attention based transformers. We also describe how random\npolynomial, random matrix, spin glass and computational complexity perspectives\non the loss surfaces are interconnected.",
          "link": "http://arxiv.org/abs/2108.03579",
          "publishedOn": "2021-08-10T02:00:07.627Z",
          "wordCount": 567,
          "title": "Expressive Power and Loss Surfaces of Deep Learning Models. (arXiv:2108.03579v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Thanh-Dat Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_C/0/1/0/all/0/1\">Chi Nhan Duong</a>, The <a href=\"http://arxiv.org/find/cs/1/au:+Vu_D/0/1/0/all/0/1\">De Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hoang Anh Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Khoa Luu</a>",
          "description": "Turn-taking has played an essential role in structuring the regulation of a\nconversation. The task of identifying the main speaker (who is properly taking\nhis/her turn of speaking) and the interrupters (who are interrupting or\nreacting to the main speaker's utterances) remains a challenging task. Although\nsome prior methods have partially addressed this task, there still remain some\nlimitations. Firstly, a direct association of Audio and Visual features may\nlimit the correlations to be extracted due to different modalities. Secondly,\nthe relationship across temporal segments helping to maintain the consistency\nof localization, separation, and conversation contexts is not effectively\nexploited. Finally, the interactions between speakers that usually contain the\ntracking and anticipatory decisions about the transition to a new speaker are\nusually ignored. Therefore, this work introduces a new Audio-Visual Transformer\napproach to the problem of localization and highlighting the main speaker in\nboth audio and visual channels of a multi-speaker conversation video in the\nwild. The proposed method exploits different types of correlations presented in\nboth visual and audio signals. The temporal audio-visual relationships across\nspatial-temporal space are anticipated and optimized via the self-attention\nmechanism in a Transformerstructure. Moreover, a newly collected dataset is\nintroduced for the main speaker detection. To the best of our knowledge, it is\none of the first studies that is able to automatically localize and highlight\nthe main speaker in both visual and audio channels in multi-speaker\nconversation videos.",
          "link": "http://arxiv.org/abs/2108.03256",
          "publishedOn": "2021-08-10T02:00:07.606Z",
          "wordCount": 685,
          "title": "The Right to Talk: An Audio-Visual Transformer Approach. (arXiv:2108.03256v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thoker_F/0/1/0/all/0/1\">Fida Mohammad Thoker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>",
          "description": "This paper strives for action recognition and detection in video modalities\nlike RGB, depth maps or 3D-skeleton sequences when only limited\nmodality-specific labeled examples are available. For the RGB, and derived\noptical-flow, modality many large-scale labeled datasets have been made\navailable. They have become the de facto pre-training choice when recognizing\nor detecting new actions from RGB datasets that have limited amounts of labeled\nexamples available. Unfortunately, large-scale labeled action datasets for\nother modalities are unavailable for pre-training. In this paper, our goal is\nto recognize actions from limited examples in non-RGB video modalities, by\nlearning from large-scale labeled RGB data. To this end, we propose a two-step\ntraining process: (i) we extract action representation knowledge from an\nRGB-trained teacher network and adapt it to a non-RGB student network. (ii) we\nthen fine-tune the transfer model with available labeled examples of the target\nmodality. For the knowledge transfer we introduce feature-supervision\nstrategies, which rely on unlabeled pairs of two modalities (the RGB and the\ntarget modality) to transfer feature level representations from the teacher to\nthe student network. Ablations and generalizations with two RGB source datasets\nand two non-RGB target datasets demonstrate that an optical-flow teacher\nprovides better action transfer features than RGB for both depth maps and\n3D-skeletons, even when evaluated on a different target domain, or for a\ndifferent task. Compared to alternative cross-modal action transfer methods we\nshow a good improvement in performance especially when labeled non-RGB examples\nto learn from are scarce",
          "link": "http://arxiv.org/abs/2108.03329",
          "publishedOn": "2021-08-10T02:00:07.599Z",
          "wordCount": 683,
          "title": "Feature-Supervised Action Modality Transfer. (arXiv:2108.03329v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khojasteh_H/0/1/0/all/0/1\">Hassan Keshvari Khojasteh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadzade_H/0/1/0/all/0/1\">Hoda Mohammadzade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behroozi_H/0/1/0/all/0/1\">Hamid Behroozi</a>",
          "description": "Temporal Action Localization (TAL) task in which the aim is to predict the\nstart and end of each action and its class label has many applications in the\nreal world. But due to its complexity, researchers have not reached great\nresults compared to the action recognition task. The complexity is related to\npredicting precise start and end times for different actions in any video. In\nthis paper, we propose a new network based on Gated Recurrent Unit (GRU) and\ntwo novel post-processing ideas for TAL task. Specifically, we propose a new\ndesign for the output layer of the GRU resulting in the so-called GRU-Splitted\nmodel. Moreover, linear interpolation is used to generate the action proposals\nwith precise start and end times. Finally, to rank the generated proposals\nappropriately, we use a Learn to Rank (LTR) approach. We evaluated the\nperformance of the proposed method on Thumos14 dataset. Results show the\nsuperiority of the performance of the proposed method compared to\nstate-of-the-art. Especially in the mean Average Precision (mAP) metric at\nIntersection over Union (IoU) 0.7, we get 27.52% which is 5.12% better than\nthat of state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.03375",
          "publishedOn": "2021-08-10T02:00:07.591Z",
          "wordCount": 628,
          "title": "Temporal Action Localization Using Gated Recurrent Units. (arXiv:2108.03375v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lingelbach_M/0/1/0/all/0/1\">Michael Lingelbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1\">Kent Vainio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokmen_C/0/1/0/all/0/1\">Cem Gokmen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharan_G/0/1/0/all/0/1\">Gokul Dharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_T/0/1/0/all/0/1\">Tanish Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurenkov_A/0/1/0/all/0/1\">Andrey Kurenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gweon_H/0/1/0/all/0/1\">Hyowon Gweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>",
          "description": "Recent research in embodied AI has been boosted by the use of simulation\nenvironments to develop and train robot learning approaches. However, the use\nof simulation has skewed the attention to tasks that only require what robotics\nsimulators can simulate: motion and physical contact. We present iGibson 2.0,\nan open-source simulation environment that supports the simulation of a more\ndiverse set of household tasks through three key innovations. First, iGibson\n2.0 supports object states, including temperature, wetness level, cleanliness\nlevel, and toggled and sliced states, necessary to cover a wider range of\ntasks. Second, iGibson 2.0 implements a set of predicate logic functions that\nmap the simulator states to logic states like Cooked or Soaked. Additionally,\ngiven a logic state, iGibson 2.0 can sample valid physical states that satisfy\nit. This functionality can generate potentially infinite instances of tasks\nwith minimal effort from the users. The sampling mechanism allows our scenes to\nbe more densely populated with small objects in semantically meaningful\nlocations. Third, iGibson 2.0 includes a virtual reality (VR) interface to\nimmerse humans in its scenes to collect demonstrations. As a result, we can\ncollect demonstrations from humans on these new types of tasks, and use them\nfor imitation learning. We evaluate the new capabilities of iGibson 2.0 to\nenable robot learning of novel tasks, in the hope of demonstrating the\npotential of this new simulator to support new research in embodied AI. iGibson\n2.0 and its new dataset will be publicly available at\nthis http URL",
          "link": "http://arxiv.org/abs/2108.03272",
          "publishedOn": "2021-08-10T02:00:07.575Z",
          "wordCount": 731,
          "title": "IGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks. (arXiv:2108.03272v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1\">Cheng Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Huaqi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarroni_G/0/1/0/all/0/1\">Giacomo Tarroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>",
          "description": "The success of neural networks on medical image segmentation tasks typically\nrelies on large labeled datasets for model training. However, acquiring and\nmanually labeling a large medical image set is resource-intensive, expensive,\nand sometimes impractical due to data sharing and privacy issues. To address\nthis challenge, we propose an adversarial data augmentation approach to improve\nthe efficiency in utilizing training data and to enlarge the dataset via\nsimulated but realistic transformations. Specifically, we present a generic\ntask-driven learning framework, which jointly optimizes a data augmentation\nmodel and a segmentation network during training, generating informative\nexamples to enhance network generalizability for the downstream task. The data\naugmentation model utilizes a set of photometric and geometric image\ntransformations and chains them to simulate realistic complex imaging\nvariations that could exist in magnetic resonance (MR) imaging. The proposed\nadversarial data augmentation does not rely on generative networks and can be\nused as a plug-in module in general segmentation networks. It is\ncomputationally efficient and applicable for both supervised and\nsemi-supervised learning. We analyze and evaluate the method on two MR image\nsegmentation tasks: cardiac segmentation and prostate segmentation. Results\nshow that the proposed approach can alleviate the need for labeled data while\nimproving model generalization ability, indicating its practical value in\nmedical imaging applications.",
          "link": "http://arxiv.org/abs/2108.03429",
          "publishedOn": "2021-08-10T02:00:07.555Z",
          "wordCount": 673,
          "title": "Enhancing MR Image Segmentation with Realistic Adversarial Data Augmentation. (arXiv:2108.03429v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03534",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Haonan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_D/0/1/0/all/0/1\">Daniel King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yun-Hsuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bly_R/0/1/0/all/0/1\">Randall A. Bly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moe_K/0/1/0/all/0/1\">Kris S. Moe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hannaford_B/0/1/0/all/0/1\">Blake Hannaford</a>",
          "description": "Accurate instrument segmentation in endoscopic vision of robot-assisted\nsurgery is challenging due to reflection on the instruments and frequent\ncontacts with tissue. Deep neural networks (DNN) show competitive performance\nand are in favor in recent years. However, the hunger of DNN for labeled data\nposes a huge workload of annotation. Motivated by alleviating this workload, we\npropose a general embeddable method to decrease the usage of labeled real\nimages, using active generated synthetic images. In each active learning\niteration, the most informative unlabeled images are first queried by active\nlearning and then labeled. Next, synthetic images are generated based on these\nselected images. The instruments and backgrounds are cropped out and randomly\ncombined with each other with blending and fusion near the boundary. The\neffectiveness of the proposed method is validated on 2 sinus surgery datasets\nand 1 intraabdominal surgery dataset. The results indicate a considerable\nimprovement in performance, especially when the budget for annotation is small.\nThe effectiveness of different types of synthetic images, blending methods, and\nexternal background are also studied. All the code is open-sourced at:\nhttps://github.com/HaonanPeng/active_syn_generator.",
          "link": "http://arxiv.org/abs/2108.03534",
          "publishedOn": "2021-08-10T02:00:07.529Z",
          "wordCount": 634,
          "title": "Reducing Annotating Load: Active Learning with Synthetic Images in Surgical Instrument Segmentation. (arXiv:2108.03534v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Redekop_E/0/1/0/all/0/1\">Ekaterina Redekop</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chernyavskiy_A/0/1/0/all/0/1\">Alexey Chernyavskiy</a>",
          "description": "The development of high quality medical image segmentation algorithms depends\non the availability of large datasets with pixel-level labels. The challenges\nof collecting such datasets, especially in case of 3D volumes, motivate to\ndevelop approaches that can learn from other types of labels that are cheap to\nobtain, e.g. bounding boxes. We focus on 3D medical images with their\ncorresponding 3D bounding boxes which are considered as series of per-slice\nnon-tight 2D bounding boxes. While current weakly-supervised approaches that\nuse 2D bounding boxes as weak labels can be applied to medical image\nsegmentation, we show that their success is limited in cases when the\nassumption about the tightness of the bounding boxes breaks. We propose a new\nbounding box correction framework which is trained on a small set of\npixel-level annotations to improve the tightness of a larger set of non-tight\nbounding box annotations. The effectiveness of our solution is demonstrated by\nevaluating a known weakly-supervised segmentation approach with and without the\nproposed bounding box correction algorithm. When the tightness is improved by\nour solution, the results of the weakly-supervised segmentation become much\ncloser to those of the fully-supervised one.",
          "link": "http://arxiv.org/abs/2108.03300",
          "publishedOn": "2021-08-10T02:00:07.522Z",
          "wordCount": 627,
          "title": "Medical image segmentation with imperfect 3D bounding boxes. (arXiv:2108.03300v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03267",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Thanh-Dat Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_C/0/1/0/all/0/1\">Chi Nhan Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_S/0/1/0/all/0/1\">Son Lam Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rainwater_C/0/1/0/all/0/1\">Chase Rainwater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Khoa Luu</a>",
          "description": "Semantic segmentation aims to predict pixel-level labels. It has become a\npopular task in various computer vision applications. While fully supervised\nsegmentation methods have achieved high accuracy on large-scale vision\ndatasets, they are unable to generalize on a new test environment or a new\ndomain well. In this work, we first introduce a new Un-aligned Domain Score to\nmeasure the efficiency of a learned model on a new target domain in\nunsupervised manner. Then, we present the new Bijective Maximum\nLikelihood(BiMaL) loss that is a generalized form of the Adversarial Entropy\nMinimization without any assumption about pixel independence. We have evaluated\nthe proposed BiMaL on two domains. The proposed BiMaL approach consistently\noutperforms the SOTA methods on empirical experiments on \"SYNTHIA to\nCityscapes\", \"GTA5 to Cityscapes\", and \"SYNTHIA to Vistas\".",
          "link": "http://arxiv.org/abs/2108.03267",
          "publishedOn": "2021-08-10T02:00:07.495Z",
          "wordCount": 585,
          "title": "BiMaL: Bijective Maximum Likelihood Approach to Domain Adaptation in Semantic Scene Segmentation. (arXiv:2108.03267v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peixia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baopu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Ming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>",
          "description": "In this paper, we observe two levels of redundancies when applying vision\ntransformers (ViT) for image recognition. First, fixing the number of tokens\nthrough the whole network produces redundant features at the spatial level.\nSecond, the attention maps among different transformer layers are redundant.\nBased on the observations above, we propose a PSViT: a ViT with token Pooling\nand attention Sharing to reduce the redundancy, effectively enhancing the\nfeature representation ability, and achieving a better speed-accuracy\ntrade-off. Specifically, in our PSViT, token pooling can be defined as the\noperation that decreases the number of tokens at the spatial level. Besides,\nattention sharing will be built between the neighboring transformer layers for\nreusing the attention maps having a strong correlation among adjacent layers.\nThen, a compact set of the possible combinations for different token pooling\nand attention sharing mechanisms are constructed. Based on the proposed compact\nset, the number of tokens in each layer and the choices of layers sharing\nattention can be treated as hyper-parameters that are learned from data\nautomatically. Experimental results show that the proposed scheme can achieve\nup to 6.6% accuracy improvement in ImageNet classification compared with the\nDeiT.",
          "link": "http://arxiv.org/abs/2108.03428",
          "publishedOn": "2021-08-10T02:00:07.487Z",
          "wordCount": 641,
          "title": "PSViT: Better Vision Transformer via Token Pooling and Attention Sharing. (arXiv:2108.03428v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03372",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengsen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yihang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+YanBai/0/1/0/all/0/1\">YanBai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Tao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1\">Minghua Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Lingyu Duan</a>",
          "description": "In object re-identification (ReID), the development of deep learning\ntechniques often involves model update and deployment. It is unbearable to\nre-extract image features of the large-scale gallery when deploying new models.\nTherefore, backward-compatible representation is proposed to enable the \"new\"\nfeatures compatible with \"old\"' features, free from the re-extracting process.\nThe existing backward-compatible methods simply conduct constraints in the\nembedding space or discriminative space and ignore the intra-class variance of\nthe old embeddings, resulting in a risk of damaging the discriminability of new\nembeddings.\n\nIn this work, we propose a Neighborhood Consensus Contrastive Learning (NCCL)\nmethod, which learns backward-compatible representation from a neighborhood\nconsensus perspective with both embedding structures and discriminative\nknowledge. With NCCL, the new embeddings are aligned and improved with old\nembeddings in a multi-cluster view. Besides, we also propose a scheme to filter\nthe old embeddings with low credibility, which can further improve the\ncompatibility robustness. Our method ensures backward compatibility without\nimpairing the accuracy of the new model. And it can even improve the new\nmodel's accuracy in most scenarios.",
          "link": "http://arxiv.org/abs/2108.03372",
          "publishedOn": "2021-08-10T02:00:07.455Z",
          "wordCount": 613,
          "title": "Neighborhood Consensus Contrastive Learning for Backward-Compatible Representation. (arXiv:2108.03372v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiangyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_M/0/1/0/all/0/1\">Mark W. Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sreenath_K/0/1/0/all/0/1\">Koushil Sreenath</a>",
          "description": "The capabilities of autonomous flight with unmanned aerial vehicles (UAVs)\nhave significantly increased in recent times. However, basic problems such as\nfast and robust geo-localization in GPS-denied environments still remain\nunsolved. Existing research has primarily concentrated on improving the\naccuracy of localization at the cost of long and varying computation time in\nvarious situations, which often necessitates the use of powerful ground station\nmachines. In order to make image-based geo-localization online and pragmatic\nfor lightweight embedded systems on UAVs, we propose a framework that is\nreliable in changing scenes, flexible about computing resource allocation and\nadaptable to common camera placements. The framework is comprised of two\nstages: offline database preparation and online inference. At the first stage,\ncolor images and depth maps are rendered as seen from potential vehicle poses\nquantized over the satellite and topography maps of anticipated flying areas. A\ndatabase is then populated with the global and local descriptors of the\nrendered images. At the second stage, for each captured real-world query image,\ntop global matches are retrieved from the database and the vehicle pose is\nfurther refined via local descriptor matching. We present field experiments of\nimage-based localization on two different UAV platforms to validate our\nresults.",
          "link": "http://arxiv.org/abs/2108.03344",
          "publishedOn": "2021-08-10T02:00:07.413Z",
          "wordCount": 656,
          "title": "Real-time Geo-localization Using Satellite Imagery and Topography for Unmanned Aerial Vehicles. (arXiv:2108.03344v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengde Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fangyun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>",
          "description": "This paper presents an end-to-end semi-supervised object detection approach,\nin contrast to previous more complex multi-stage methods. The end-to-end\ntraining gradually improves pseudo label qualities during the curriculum, and\nthe more and more accurate pseudo labels in turn benefit object detection\ntraining. We also propose two simple yet effective techniques within this\nframework: a soft teacher mechanism where the classification loss of each\nunlabeled bounding box is weighed by the classification score produced by the\nteacher network; a box jittering approach to select reliable pseudo boxes for\nthe learning of box regression. On the COCO benchmark, the proposed approach\noutperforms previous methods by a large margin under various labeling ratios,\ni.e. 1\\%, 5\\% and 10\\%. Moreover, our approach proves to perform also well when\nthe amount of labeled data is relatively large. For example, it can improve a\n40.9 mAP baseline detector trained using the full COCO training set by +3.6\nmAP, reaching 44.5 mAP, by leveraging the 123K unlabeled images of COCO. On the\nstate-of-the-art Swin Transformer based object detector (58.9 mAP on test-dev),\nit can still significantly improve the detection accuracy by +1.5 mAP, reaching\n60.4 mAP, and improve the instance segmentation accuracy by +1.2 mAP, reaching\n52.4 mAP. Further incorporating with the Object365 pre-trained model, the\ndetection accuracy reaches 61.3 mAP and the instance segmentation accuracy\nreaches 53.0 mAP, pushing the new state-of-the-art.",
          "link": "http://arxiv.org/abs/2106.09018",
          "publishedOn": "2021-08-09T00:49:28.100Z",
          "wordCount": 715,
          "title": "End-to-End Semi-Supervised Object Detection with Soft Teacher. (arXiv:2106.09018v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alonso_I/0/1/0/all/0/1\">Inigo Alonso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabater_A/0/1/0/all/0/1\">Alberto Sabater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferstl_D/0/1/0/all/0/1\">David Ferstl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1\">Luis Montesano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1\">Ana C. Murillo</a>",
          "description": "This work presents a novel approach for semi-supervised semantic\nsegmentation. The key element of this approach is our contrastive learning\nmodule that enforces the segmentation network to yield similar pixel-level\nfeature representations for same-class samples across the whole dataset. To\nachieve this, we maintain a memory bank continuously updated with relevant and\nhigh-quality feature vectors from labeled data. In an end-to-end training, the\nfeatures from both labeled and unlabeled data are optimized to be similar to\nsame-class samples from the memory bank. Our approach outperforms the current\nstate-of-the-art for semi-supervised semantic segmentation and semi-supervised\ndomain adaptation on well-known public benchmarks, with larger improvements on\nthe most challenging scenarios, i.e., less available labeled data.\nhttps://github.com/Shathe/SemiSeg-Contrastive",
          "link": "http://arxiv.org/abs/2104.13415",
          "publishedOn": "2021-08-09T00:49:28.028Z",
          "wordCount": 610,
          "title": "Semi-Supervised Semantic Segmentation with Pixel-Level Contrastive Learning from a Class-wise Memory Bank. (arXiv:2104.13415v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.01034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kuangyan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yiming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Liao Zhu</a>",
          "description": "This paper has proposed a new baseline deep learning model of more benefits\nfor image classification. Different from the convolutional neural network(CNN)\npractice where filters are trained by back propagation to represent different\npatterns of an image, we are inspired by a method called \"PCANet\" in \"PCANet: A\nSimple Deep Learning Baseline for Image Classification?\" to choose filter\nvectors from basis vectors in frequency domain like Fourier coefficients or\nwavelets without back propagation. Researchers have demonstrated that those\nbasis in frequency domain can usually provide physical insights, which adds to\nthe interpretability of the model by analyzing the frequencies selected.\nBesides, the training process will also be more time efficient, mathematically\nclear and interpretable compared with the \"black-box\" training process of CNN.",
          "link": "http://arxiv.org/abs/2001.01034",
          "publishedOn": "2021-08-09T00:49:27.986Z",
          "wordCount": 606,
          "title": "FrequentNet : A New Interpretable Deep Learning Baseline for Image Classification. (arXiv:2001.01034v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08637",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Rajpal_S/0/1/0/all/0/1\">Sheetal Rajpal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_M/0/1/0/all/0/1\">Manoj Agarwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpal_A/0/1/0/all/0/1\">Ankit Rajpal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lakhyani_N/0/1/0/all/0/1\">Navin Lakhyani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saggar_A/0/1/0/all/0/1\">Arpita Saggar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_N/0/1/0/all/0/1\">Naveen Kumar</a>",
          "description": "Coronaviruses constitute a family of viruses that gives rise to respiratory\ndiseases. As COVID-19 is highly contagious, early diagnosis of COVID-19 is\ncrucial for an effective treatment strategy. However, the RT-PCR test which is\nconsidered to be a gold standard in the diagnosis of COVID-19 suffers from a\nhigh false-negative rate. Chest X-ray (CXR) image analysis has emerged as a\nfeasible and effective diagnostic technique towards this objective. In this\nwork, we propose the COVID-19 classification problem as a three-class\nclassification problem to distinguish between COVID-19, normal, and pneumonia\nclasses. We propose a three-stage framework, named COV-ELM. Stage one deals\nwith preprocessing and transformation while stage two deals with feature\nextraction. These extracted features are passed as an input to the ELM at the\nthird stage, resulting in the identification of COVID-19. The choice of ELM in\nthis work has been motivated by its faster convergence, better generalization\ncapability, and shorter training time in comparison to the conventional\ngradient-based learning algorithms. As bigger and diverse datasets become\navailable, ELM can be quickly retrained as compared to its gradient-based\ncompetitor models. The proposed model achieved a macro average F1-score of 0.95\nand the overall sensitivity of ${0.94 \\pm 0.02} at a 95% confidence interval.\nWhen compared to state-of-the-art machine learning algorithms, the COV-ELM is\nfound to outperform its competitors in this three-class classification\nscenario. Further, LIME has been integrated with the proposed COV-ELM model to\ngenerate annotated CXR images. The annotations are based on the superpixels\nthat have contributed to distinguish between the different classes. It was\nobserved that the superpixels correspond to the regions of the human lungs that\nare clinically observed in COVID-19 and Pneumonia cases.",
          "link": "http://arxiv.org/abs/2007.08637",
          "publishedOn": "2021-08-09T00:49:27.980Z",
          "wordCount": 831,
          "title": "COV-ELM classifier: An Extreme Learning Machine based identification of COVID-19 using Chest X-Ray Images. (arXiv:2007.08637v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1\">Chaitanya Devaguptapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Devansh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_G/0/1/0/all/0/1\">Gaurav Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>",
          "description": "Adversarial robustness of deep learning models has gained much traction in\nthe last few years. Various attacks and defenses are proposed to improve the\nadversarial robustness of modern-day deep learning architectures. While all\nthese approaches help improve the robustness, one promising direction for\nimproving adversarial robustness is un-explored, i.e., the complex topology of\nthe neural network architecture. In this work, we answer the following\nquestion: \"Can the complex topology of a neural network give adversarial\nrobustness without any form of adversarial training?\" empirically by\nexperimenting with different hand-crafted and NAS based architectures. Our\nfindings show that, for small-scale attacks, NAS-based architectures are more\nrobust for small-scale datasets and simple tasks than hand-crafted\narchitectures. However, as the dataset's size or the task's complexity\nincrease, hand-crafted architectures are more robust than NAS-based\narchitectures. We perform the first large scale study to understand adversarial\nrobustness purely from an architectural perspective. Our results show that\nrandom sampling in the search space of DARTS (a popular NAS method) with simple\nensembling can improve the robustness to PGD attack by nearly ~12\\%. We show\nthat NAS, which is popular for SoTA accuracy, can provide adversarial accuracy\nas a free add-on without any form of adversarial training. Our results show\nthat leveraging the power of neural network topology with methods like\nensembles can be an excellent way to achieve adversarial robustness without any\nform of adversarial training. We also introduce a metric that can be used to\ncalculate the trade-off between clean accuracy and adversarial robustness.",
          "link": "http://arxiv.org/abs/2007.08428",
          "publishedOn": "2021-08-09T00:49:27.964Z",
          "wordCount": 753,
          "title": "On Adversarial Robustness: A Neural Architecture Search perspective. (arXiv:2007.08428v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garibaldi_J/0/1/0/all/0/1\">Jonathan M Garibaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>",
          "description": "The goal of representation learning is different from the ultimate objective\nof machine learning such as decision making, it is therefore very difficult to\nestablish clear and direct objectives for training representation learning\nmodels. It has been argued that a good representation should disentangle the\nunderlying variation factors, yet how to translate this into training\nobjectives remains unknown. This paper presents an attempt to establish direct\ntraining criterions and design principles for developing good representation\nlearning models. We propose that a good representation learning model should be\nmaximally expressive, i.e., capable of distinguishing the maximum number of\ninput configurations. We formally define expressiveness and introduce the\nmaximum expressiveness (MEXS) theorem of a general learning model. We propose\nto train a model by maximizing its expressiveness while at the same time\nincorporating general priors such as model smoothness. We present a conscience\ncompetitive learning algorithm which encourages the model to reach its MEXS\nwhilst at the same time adheres to model smoothness prior. We also introduce a\nlabel consistent training (LCT) technique to boost model smoothness by\nencouraging it to assign consistent labels to similar samples. We present\nextensive experimental results to show that our method can indeed design\nrepresentation learning models capable of developing representations that are\nas good as or better than state of the art. We also show that our technique is\ncomputationally efficient, robust against different parameter settings and can\nwork effectively on a variety of datasets. Code available at\nhttps://github.com/qlilx/odgrlm.git",
          "link": "http://arxiv.org/abs/2107.05948",
          "publishedOn": "2021-08-09T00:49:27.890Z",
          "wordCount": 716,
          "title": "On Designing Good Representation Learning Models. (arXiv:2107.05948v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muralikrishnan_S/0/1/0/all/0/1\">Sanjeev Muralikrishnan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Siddhartha Chaudhuri</a> (2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1\">Noam Aigerman</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1\">Vladimir Kim</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1\">Matthew Fisher</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy Mitra</a> (1 and 2) ((1) University College London, (2) Adobe Research, (3) IIT Bombay)",
          "description": "We investigate the problem of training generative models on a very sparse\ncollection of 3D models. We use geometrically motivated energies to augment and\nthus boost a sparse collection of example (training) models. We analyze the\nHessian of the as-rigid-as-possible (ARAP) energy to sample from and project to\nthe underlying (local) shape space, and use the augmented dataset to train a\nvariational autoencoder (VAE). We iterate the process of building latent spaces\nof VAE and augmenting the associated dataset, to progressively reveal a richer\nand more expressive generative space for creating geometrically and\nsemantically valid samples. Our framework allows us to train generative 3D\nmodels even with a small set of good quality 3D models, which are typically\nhard to curate. We extensively evaluate our method against a set of strong\nbaselines, provide ablation studies and demonstrate application towards\nestablishing shape correspondences. We present multiple examples of interesting\nand meaningful shape variations even when starting from as few as 3-10 training\nshapes.",
          "link": "http://arxiv.org/abs/2108.03225",
          "publishedOn": "2021-08-09T00:49:27.882Z",
          "wordCount": 620,
          "title": "GLASS: Geometric Latent Augmentation for Shape Spaces. (arXiv:2108.03225v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1904.02601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_A/0/1/0/all/0/1\">Anqi Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xui_L/0/1/0/all/0/1\">Lan Xui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>",
          "description": "In this paper, we present TightCap, a data-driven scheme to capture both the\nhuman shape and dressed garments accurately with only a single 3D human scan,\nwhich enables numerous applications such as virtual try-on, biometrics and body\nevaluation. To break the severe variations of the human poses and garments, we\npropose to model the clothing tightness - the displacements from the garments\nto the human shape implicitly in the global UV texturing domain. To this end,\nwe utilize an enhanced statistical human template and an effective multi-stage\nalignment scheme to map the 3D scan into a hybrid 2D geometry image. Based on\nthis 2D representation, we propose a novel framework to predicted clothing\ntightness via a novel tightness formulation, as well as an effective\noptimization scheme to further reconstruct multi-layer human shape and garments\nunder various clothing categories and human postures. We further propose a new\nclothing tightness dataset (CTD) of human scans with a large variety of\nclothing styles, poses and corresponding ground-truth human shapes to stimulate\nfurther research. Extensive experiments demonstrate the effectiveness of our\nTightCap to achieve high-quality human shape and dressed garments\nreconstruction, as well as the further applications for clothing segmentation,\nretargeting and animation.",
          "link": "http://arxiv.org/abs/1904.02601",
          "publishedOn": "2021-08-09T00:49:27.874Z",
          "wordCount": 693,
          "title": "TightCap: 3D Human Shape Capture with Clothing Tightness Field. (arXiv:1904.02601v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1\">Prashant Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pai_A/0/1/0/all/0/1\">Ajey Pai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_N/0/1/0/all/0/1\">Nisarg Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Prasenjit Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makharia_G/0/1/0/all/0/1\">Govind Makharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1\">Prathosh AP</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>",
          "description": "Contrastive Learning (CL) is a recent representation learning approach, which\nencourages inter-class separability and intra-class compactness in learned\nimage representations. Since medical images often contain multiple semantic\nclasses in an image, using CL to learn representations of local features (as\nopposed to global) is important. In this work, we present a novel\nsemi-supervised 2D medical segmentation solution that applies CL on image\npatches, instead of full images. These patches are meaningfully constructed\nusing the semantic information of different classes obtained via pseudo\nlabeling. We also propose a novel consistency regularization (CR) scheme, which\nworks in synergy with CL. It addresses the problem of confirmation bias, and\nencourages better clustering in the feature space. We evaluate our method on\nfour public medical segmentation datasets and a novel histopathology dataset\nthat we introduce. Our method obtains consistent improvements over\nstate-of-the-art semi-supervised segmentation approaches for all datasets.",
          "link": "http://arxiv.org/abs/2106.06801",
          "publishedOn": "2021-08-09T00:49:27.859Z",
          "wordCount": 683,
          "title": "Contrastive Semi-Supervised Learning for 2D Medical Image Segmentation. (arXiv:2106.06801v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songtao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>",
          "description": "In this report, we present some experienced improvements to YOLO series,\nforming a new high-performance detector -- YOLOX. We switch the YOLO detector\nto an anchor-free manner and conduct other advanced detection techniques, i.e.,\na decoupled head and the leading label assignment strategy SimOTA to achieve\nstate-of-the-art results across a large scale range of models: For YOLO-Nano\nwith only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing\nNanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in\nindustry, we boost it to 47.3% AP on COCO, outperforming the current best\npractice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as\nYOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on\nTesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on\nStreaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021)\nusing a single YOLOX-L model. We hope this report can provide useful experience\nfor developers and researchers in practical scenes, and we also provide deploy\nversions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at\nhttps://github.com/Megvii-BaseDetection/YOLOX.",
          "link": "http://arxiv.org/abs/2107.08430",
          "publishedOn": "2021-08-09T00:49:27.834Z",
          "wordCount": 654,
          "title": "YOLOX: Exceeding YOLO Series in 2021. (arXiv:2107.08430v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+VS_V/0/1/0/all/0/1\">Vibashan VS</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oza_P/0/1/0/all/0/1\">Poojan Oza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "In image fusion, images obtained from different sensors are fused to generate\na single image with enhanced information. In recent years, state-of-the-art\nmethods have adopted Convolution Neural Networks (CNNs) to encode meaningful\nfeatures for image fusion. Specifically, CNN-based methods perform image fusion\nby fusing local features. However, they do not consider long-range dependencies\nthat are present in the image. Transformer-based models are designed to\novercome this by modeling the long-range dependencies with the help of\nself-attention mechanism. This motivates us to propose a novel Image Fusion\nTransformer (IFT) where we develop a transformer-based multi-scale fusion\nstrategy that attends to both local and long-range information (or global\ncontext). The proposed method follows a two-stage training approach. In the\nfirst stage, we train an auto-encoder to extract deep features at multiple\nscales. In the second stage, multi-scale features are fused using a\nSpatio-Transformer (ST) fusion strategy. The ST fusion blocks are comprised of\na CNN and a transformer branch which capture local and long-range features,\nrespectively. Extensive experiments on multiple benchmark datasets show that\nthe proposed method performs better than many competitive fusion algorithms.\nFurthermore, we show the effectiveness of the proposed ST fusion strategy with\nan ablation analysis. The source code is available at:\nhttps://github.com/Vibashan/Image-Fusion-Transformer.",
          "link": "http://arxiv.org/abs/2107.09011",
          "publishedOn": "2021-08-09T00:49:27.815Z",
          "wordCount": 673,
          "title": "Image Fusion Transformer. (arXiv:2107.09011v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lakshika_J/0/1/0/all/0/1\">Jayani P. G. Lakshika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talagala_T/0/1/0/all/0/1\">Thiyanga S. Talagala</a>",
          "description": "Plant species identification is time consuming, costly, and requires lots of\nefforts, and expertise knowledge. In recent, many researchers use deep learning\nmethods to classify plants directly using plant images. While deep learning\nmodels have achieved a great success, the lack of interpretability limit their\nwidespread application. To overcome this, we explore the use of interpretable,\nmeasurable and computer-aided features extracted from plant leaf images. Image\nprocessing is one of the most challenging, and crucial steps in\nfeature-extraction. The purpose of image processing is to improve the leaf\nimage by removing undesired distortion. The main image processing steps of our\nalgorithm involves: i) Convert original image to RGB (Red-Green-Blue) image,\nii) Gray scaling, iii) Gaussian smoothing, iv) Binary thresholding, v) Remove\nstalk, vi) Closing holes, and vii) Resize image. The next step after image\nprocessing is to extract features from plant leaf images. We introduced 52\ncomputationally efficient features to classify plant species. These features\nare mainly classified into four groups as: i) shape-based features, ii)\ncolor-based features, iii) texture-based features, and iv) scagnostic features.\nLength, width, area, texture correlation, monotonicity and scagnostics are to\nname few of them. We explore the ability of features to discriminate the\nclasses of interest under supervised learning and unsupervised learning\nsettings. For that, supervised dimensionality reduction technique, Linear\nDiscriminant Analysis (LDA), and unsupervised dimensionality reduction\ntechnique, Principal Component Analysis (PCA) are used to convert and visualize\nthe images from digital-image space to feature space. The results show that the\nfeatures are sufficient to discriminate the classes of interest under both\nsupervised and unsupervised learning settings.",
          "link": "http://arxiv.org/abs/2106.08077",
          "publishedOn": "2021-08-09T00:49:27.797Z",
          "wordCount": 730,
          "title": "Computer-aided Interpretable Features for Leaf Image Classification. (arXiv:2106.08077v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltanian_Zadeh_S/0/1/0/all/0/1\">Somayyeh Soltanian-Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farsiu_S/0/1/0/all/0/1\">Sina Farsiu</a>",
          "description": "Salient object detection (SOD) is viewed as a pixel-wise saliency modeling\ntask by traditional deep learning-based methods. A limitation of current SOD\nmodels is insufficient utilization of inter-pixel information, which usually\nresults in imperfect segmentation near edge regions and low spatial coherence.\nAs we demonstrate, using a saliency mask as the only label is suboptimal. To\naddress this limitation, we propose a connectivity-based approach called\nbilateral connectivity network (BiconNet), which uses connectivity masks\ntogether with saliency masks as labels for effective modeling of inter-pixel\nrelationships and object saliency. Moreover, we propose a bilateral voting\nmodule to enhance the output connectivity map, and a novel edge feature\nenhancement method that efficiently utilizes edge-specific features. Through\ncomprehensive experiments on five benchmark datasets, we demonstrate that our\nproposed method can be plugged into any existing state-of-the-art\nsaliency-based SOD framework to improve its performance with negligible\nparameter increase.",
          "link": "http://arxiv.org/abs/2103.00334",
          "publishedOn": "2021-08-09T00:49:27.768Z",
          "wordCount": 656,
          "title": "BiconNet: An Edge-preserved Connectivity-based Approach for Salient Object Detection. (arXiv:2103.00334v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gosala_N/0/1/0/all/0/1\">Nikhil Gosala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>",
          "description": "Bird's-Eye-View (BEV) maps have emerged as one of the most powerful\nrepresentations for scene understanding due to their ability to provide rich\nspatial context while being easy to interpret and process. However, generating\nBEV maps requires complex multi-stage paradigms that encapsulate a series of\ndistinct tasks such as depth estimation, ground plane estimation, and semantic\nsegmentation. These sub-tasks are often learned in a disjoint manner which\nprevents the model from holistic reasoning and results in erroneous BEV maps.\nMoreover, existing algorithms only predict the semantics in the BEV space,\nwhich limits their use in applications where the notion of object instances is\ncritical. In this work, we present the first end-to-end learning approach for\ndirectly predicting dense panoptic segmentation maps in the BEV, given a single\nmonocular image in the frontal view (FV). Our architecture follows the top-down\nparadigm and incorporates a novel dense transformer module consisting of two\ndistinct transformers that learn to independently map vertical and flat regions\nin the input image from the FV to the BEV. Additionally, we derive a\nmathematical formulation for the sensitivity of the FV-BEV transformation which\nallows us to intelligently weight pixels in the BEV space to account for the\nvarying descriptiveness across the FV image. Extensive evaluations on the\nKITTI-360 and nuScenes datasets demonstrate that our approach exceeds the\nstate-of-the-art in the PQ metric by 3.61 pp and 4.93 pp respectively.",
          "link": "http://arxiv.org/abs/2108.03227",
          "publishedOn": "2021-08-09T00:49:27.753Z",
          "wordCount": 671,
          "title": "Bird's-Eye-View Panoptic Segmentation Using Monocular Frontal View Images. (arXiv:2108.03227v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Athey_T/0/1/0/all/0/1\">Thomas L. Athey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tward_D/0/1/0/all/0/1\">Daniel J. Tward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_U/0/1/0/all/0/1\">Ulrich Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_M/0/1/0/all/0/1\">Michael I. Miller</a>",
          "description": "Recent advances in brain clearing and imaging have made it possible to image\nentire mammalian brains at sub-micron resolution. These images offer the\npotential to assemble brain-wide atlases of projection neuron morphology, but\nmanual neuron reconstruction remains a bottleneck. In this paper we present a\nprobabilistic method which combines a hidden Markov state process that encodes\nneuron geometric properties with a random field appearance model of the\nflourescence process. Our method utilizes dynamic programming to efficiently\ncompute the global maximizers of what we call the \"most probable\" neuron path.\nWe applied our algorithm to the output of image segmentation models where false\nnegatives severed neuronal processes, and showed that it can follow axons in\nthe presence of noise or nearby neurons. Our method has the potential to be\nintegrated into a semi or fully automated reconstruction pipeline.\nAdditionally, it creates a framework for conditioning the probability to fixed\nstart and endpoints through which users can intervene with hard constraints to,\nfor example, rule out certain reconstructions, or assign axons to particular\ncell bodies.",
          "link": "http://arxiv.org/abs/2106.02701",
          "publishedOn": "2021-08-09T00:49:27.746Z",
          "wordCount": 638,
          "title": "Hidden Markov Modeling for Maximum Likelihood Neuron Reconstruction. (arXiv:2106.02701v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kaidong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathan_M/0/1/0/all/0/1\">Mohammad I. Fathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1\">Krushi Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Cuncong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Ajay Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Amit Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jean S. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>",
          "description": "Colorectal cancer (CRC) is one of the most common types of cancer with a high\nmortality rate. Colonoscopy is the preferred procedure for CRC screening and\nhas proven to be effective in reducing CRC mortality. Thus, a reliable\ncomputer-aided polyp detection and classification system can significantly\nincrease the effectiveness of colonoscopy. In this paper, we create an\nendoscopic dataset collected from various sources and annotate the ground truth\nof polyp location and classification results with the help of experienced\ngastroenterologists. The dataset can serve as a benchmark platform to train and\nevaluate the machine learning models for polyp classification. We have also\ncompared the performance of eight state-of-the-art deep learning-based object\ndetection models. The results demonstrate that deep CNN models are promising in\nCRC screening. This work can serve as a baseline for future research in polyp\ndetection and classification.",
          "link": "http://arxiv.org/abs/2104.10824",
          "publishedOn": "2021-08-09T00:49:27.739Z",
          "wordCount": 622,
          "title": "Colonoscopy Polyp Detection and Classification: Dataset Creation and Comparative Evaluations. (arXiv:2104.10824v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Chengxiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhengping Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1\">Zheng Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Gangyi Ding</a>",
          "description": "Deep generative models have made great progress in synthesizing images with\narbitrary human poses and transferring poses of one person to others. However,\nmost existing approaches explicitly leverage the pose information extracted\nfrom the source images as a conditional input for the generative networks.\nMeanwhile, they usually focus on the visual fidelity of the synthesized images\nbut neglect the inherent consistency, which further confines their performance\nof pose transfer. To alleviate the current limitations and improve the quality\nof the synthesized images, we propose a pose transfer network with Disentangled\nFeature Consistency (DFC-Net) to facilitate human pose transfer. Given a pair\nof images containing the source and target person, DFC-Net extracts pose and\nstatic information from the source and target respectively, then synthesizes an\nimage of the target person with the desired pose from the source. Moreover,\nDFC-Net leverages disentangled feature consistency losses in the adversarial\ntraining to strengthen the transfer coherence and integrates the keypoint\namplifier to enhance the pose feature extraction. Additionally, an unpaired\nsupport dataset Mixamo-Sup providing more extra pose information has been\nfurther utilized during the training to improve the generality and robustness\nof DFC-Net. Extensive experimental results on Mixamo-Pose and EDN-10k have\ndemonstrated DFC-Net achieves state-of-the-art performance on pose transfer.",
          "link": "http://arxiv.org/abs/2107.10984",
          "publishedOn": "2021-08-09T00:49:27.720Z",
          "wordCount": 680,
          "title": "Human Pose Transfer with Disentangled Feature Consistency. (arXiv:2107.10984v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yajun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arai_S/0/1/0/all/0/1\">Shogo Arai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Diyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fangzhou Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosuge_K/0/1/0/all/0/1\">Kazuhiro Kosuge</a>",
          "description": "Instance segmentation is an important pre-processing task in numerous\nreal-world applications, such as robotics, autonomous vehicles, and\nhuman-computer interaction. Compared with the rapid development of deep\nlearning for two-dimensional (2D) image tasks, deep learning-based instance\nsegmentation of 3D point cloud still has a lot of room for development. In\nparticular, distinguishing a large number of occluded objects of the same class\nis a highly challenging problem, which is seen in a robotic bin-picking. In a\nusual bin-picking scene, many indentical objects are stacked together and the\nmodel of the objects is known. Thus, the semantic information can be ignored;\ninstead, the focus in the bin-picking is put on the segmentation of instances.\nBased on this task requirement, we propose a Fast Point Cloud Clustering (FPCC)\nfor instance segmentation of bin-picking scene. FPCC includes a network named\nFPCC-Net and a fast clustering algorithm. FPCC-net has two subnets, one for\ninferring the geometric centers for clustering and the other for describing\nfeatures of each point. FPCC-Net extracts features of each point and infers\ngeometric center points of each instance simultaneously. After that, the\nproposed clustering algorithm clusters the remaining points to the closest\ngeometric center in feature embedding space. Experiments show that FPCC also\nsurpasses the existing works in bin-picking scenes and is more computationally\nefficient. Our code and data are available at https://github.com/xyjbaal/FPCC.",
          "link": "http://arxiv.org/abs/2012.14618",
          "publishedOn": "2021-08-09T00:49:27.712Z",
          "wordCount": 708,
          "title": "FPCC: Fast Point Cloud Clustering for Instance Segmentation. (arXiv:2012.14618v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Windsor_R/0/1/0/all/0/1\">Rhydian Windsor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamaludin_A/0/1/0/all/0/1\">Amir Jamaludin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadir_T/0/1/0/all/0/1\">Timor Kadir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>",
          "description": "This paper explores the use of self-supervised deep learning in medical\nimaging in cases where two scan modalities are available for the same subject.\nSpecifically, we use a large publicly-available dataset of over 20,000 subjects\nfrom the UK Biobank with both whole body Dixon technique magnetic resonance\n(MR) scans and also dual-energy x-ray absorptiometry (DXA) scans. We make three\ncontributions: (i) We introduce a multi-modal image-matching contrastive\nframework, that is able to learn to match different-modality scans of the same\nsubject with high accuracy. (ii) Without any adaption, we show that the\ncorrespondences learnt during this contrastive training step can be used to\nperform automatic cross-modal scan registration in a completely unsupervised\nmanner. (iii) Finally, we use these registrations to transfer segmentation maps\nfrom the DXA scans to the MR scans where they are used to train a network to\nsegment anatomical regions without requiring ground-truth MR examples. To aid\nfurther research, our code will be made publicly available.",
          "link": "http://arxiv.org/abs/2107.06652",
          "publishedOn": "2021-08-09T00:49:27.704Z",
          "wordCount": 641,
          "title": "Self-Supervised Multi-Modal Alignment for Whole Body Medical Imaging. (arXiv:2107.06652v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1\">Nathanael L. Baisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zheheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_R/0/1/0/all/0/1\">Ritesh Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Bryan Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelov_P/0/1/0/all/0/1\">Plamen Angelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1\">Sue Black</a>",
          "description": "In cases of serious crime, including sexual abuse, often the only available\ninformation with demonstrated potential for identification is images of the\nhands. Since this evidence is captured in uncontrolled situations, it is\ndifficult to analyse. As global approaches to feature comparison are limited in\nthis case, it is important to extend to consider local information. In this\nwork, we propose hand-based person identification by learning both global and\nlocal deep feature representation. Our proposed method, Global and Part-Aware\nNetwork (GPA-Net), creates global and local branches on the conv-layer for\nlearning robust discriminative global and part-level features. For learning the\nlocal (part-level) features, we perform uniform partitioning on the conv-layer\nin both horizontal and vertical directions. We retrieve the parts by conducting\na soft partition without explicitly partitioning the images or requiring\nexternal cues such as pose estimation. We make extensive evaluations on two\nlarge multi-ethnic and publicly available hand datasets, demonstrating that our\nproposed method significantly outperforms competing approaches.",
          "link": "http://arxiv.org/abs/2101.05260",
          "publishedOn": "2021-08-09T00:49:27.687Z",
          "wordCount": 666,
          "title": "Hand-Based Person Identification using Global and Part-Aware Deep Feature Representation Learning. (arXiv:2101.05260v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1\">Bolin Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arridge_S/0/1/0/all/0/1\">Simon R. Arridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucka_F/0/1/0/all/0/1\">Felix Lucka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cox_B/0/1/0/all/0/1\">Ben T. Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_N/0/1/0/all/0/1\">Nam Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beard_P/0/1/0/all/0/1\">Paul C. Beard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1\">Edward Z. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Betcke_M/0/1/0/all/0/1\">Marta M. Betcke</a>",
          "description": "Curvelet frame is of special significance for photoacoustic tomography (PAT)\ndue to its sparsifying and microlocalisation properties. We derive a one-to-one\nmap between wavefront directions in image and data spaces in PAT which suggests\nnear equivalence between the recovery of the initial pressure and PAT data from\ncompressed/subsampled measurements when assuming sparsity in Curvelet frame. As\nthe latter is computationally more tractable, investigation to which extent\nthis equivalence holds conducted in this paper is of immediate practical\nsignificance. To this end we formulate and compare DR, a two step approach\nbased on the recovery of the complete volume of the photoacoustic data from the\nsubsampled data followed by the acoustic inversion, and p0R, a one step\napproach where the photoacoustic image (the initial pressure, p0) is directly\nrecovered from the subsampled data. Effective representation of the\nphotoacoustic data requires basis defined on the range of the photoacoustic\nforward operator. To this end we propose a novel wedge-restriction of Curvelet\ntransform which enables us to construct such basis. Both recovery problems are\nformulated in a variational framework. As the Curvelet frame is heavily\noverdetermined, we use reweighted l1 norm penalties to enhance the sparsity of\nthe solution. The data reconstruction problem DR is a standard compressed\nsensing recovery problem, which we solve using an ADMMtype algorithm, SALSA.\nSubsequently, the initial pressure is recovered using time reversal as\nimplemented in the k-Wave Toolbox. The p0 reconstruction problem, p0R, aims to\nrecover the photoacoustic image directly via FISTA, or ADMM when in addition\nincluding a non-negativity constraint. We compare and discuss the relative\nmerits of the two approaches and illustrate them on 2D simulated and 3D real\ndata in a fair and rigorous manner.",
          "link": "http://arxiv.org/abs/2011.13080",
          "publishedOn": "2021-08-09T00:49:27.679Z",
          "wordCount": 782,
          "title": "Photoacoustic Reconstruction Using Sparsity in Curvelet Frame: Image versus Data Domain. (arXiv:2011.13080v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rajvinder Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verjans_J/0/1/0/all/0/1\">Johan W. Verjans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>",
          "description": "Anomaly detection with weakly supervised video-level labels is typically\nformulated as a multiple instance learning (MIL) problem, in which we aim to\nidentify snippets containing abnormal events, with each video represented as a\nbag of video snippets. Although current methods show effective detection\nperformance, their recognition of the positive instances, i.e., rare abnormal\nsnippets in the abnormal videos, is largely biased by the dominant negative\ninstances, especially when the abnormal events are subtle anomalies that\nexhibit only small differences compared with normal events. This issue is\nexacerbated in many methods that ignore important video temporal dependencies.\nTo address this issue, we introduce a novel and theoretically sound method,\nnamed Robust Temporal Feature Magnitude learning (RTFM), which trains a feature\nmagnitude learning function to effectively recognise the positive instances,\nsubstantially improving the robustness of the MIL approach to the negative\ninstances from abnormal videos. RTFM also adapts dilated convolutions and\nself-attention mechanisms to capture long- and short-range temporal\ndependencies to learn the feature magnitude more faithfully. Extensive\nexperiments show that the RTFM-enabled MIL model (i) outperforms several\nstate-of-the-art methods by a large margin on four benchmark data sets\n(ShanghaiTech, UCF-Crime, XD-Violence and UCSD-Peds) and (ii) achieves\nsignificantly improved subtle anomaly discriminability and sample efficiency.\nCode is available at https://github.com/tianyu0207/RTFM.",
          "link": "http://arxiv.org/abs/2101.10030",
          "publishedOn": "2021-08-09T00:49:27.654Z",
          "wordCount": 697,
          "title": "Weakly-supervised Video Anomaly Detection with Robust Temporal Feature Magnitude Learning. (arXiv:2101.10030v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.03780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anpei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Ling Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>",
          "description": "Recently, Generative Adversarial Networks (GANs)} have been widely used for\nportrait image generation. However, in the latent space learned by GANs,\ndifferent attributes, such as pose, shape, and texture style, are generally\nentangled, making the explicit control of specific attributes difficult. To\naddress this issue, we propose a SofGAN image generator to decouple the latent\nspace of portraits into two subspaces: a geometry space and a texture space.\nThe latent codes sampled from the two subspaces are fed to two network branches\nseparately, one to generate the 3D geometry of portraits with canonical pose,\nand the other to generate textures. The aligned 3D geometries also come with\nsemantic part segmentation, encoded as a semantic occupancy field (SOF). The\nSOF allows the rendering of consistent 2D semantic segmentation maps at\narbitrary views, which are then fused with the generated texture maps and\nstylized to a portrait photo using our semantic instance-wise (SIW) module.\nThrough extensive experiments, we show that our system can generate high\nquality portrait images with independently controllable geometry and texture\nattributes. The method also generalizes well in various applications such as\nappearance-consistent facial animation and dynamic styling.",
          "link": "http://arxiv.org/abs/2007.03780",
          "publishedOn": "2021-08-09T00:49:27.635Z",
          "wordCount": 672,
          "title": "SofGAN: A Portrait Image Generator with Dynamic Styling. (arXiv:2007.03780v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gojcic_Z/0/1/0/all/0/1\">Zan Gojcic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usvyatsov_M/0/1/0/all/0/1\">Mikhail Usvyatsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieser_A/0/1/0/all/0/1\">Andreas Wieser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>",
          "description": "We introduce PREDATOR, a model for pairwise point-cloud registration with\ndeep attention to the overlap region. Different from previous work, our model\nis specifically designed to handle (also) point-cloud pairs with low overlap.\nIts key novelty is an overlap-attention block for early information exchange\nbetween the latent encodings of the two point clouds. In this way the\nsubsequent decoding of the latent representations into per-point features is\nconditioned on the respective other point cloud, and thus can predict which\npoints are not only salient, but also lie in the overlap region between the two\npoint clouds. The ability to focus on points that are relevant for matching\ngreatly improves performance: PREDATOR raises the rate of successful\nregistrations by more than 20% in the low-overlap scenario, and also sets a new\nstate of the art for the 3DMatch benchmark with 89% registration recall.",
          "link": "http://arxiv.org/abs/2011.13005",
          "publishedOn": "2021-08-09T00:49:27.628Z",
          "wordCount": 637,
          "title": "PREDATOR: Registration of 3D Point Clouds with Low Overlap. (arXiv:2011.13005v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.09872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kisuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Ran Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luther_K/0/1/0/all/0/1\">Kyle Luther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seung_H/0/1/0/all/0/1\">H. Sebastian Seung</a>",
          "description": "We show dense voxel embeddings learned via deep metric learning can be\nemployed to produce a highly accurate segmentation of neurons from 3D electron\nmicroscopy images. A \"metric graph\" on a set of edges between voxels is\nconstructed from the dense voxel embeddings generated by a convolutional\nnetwork. Partitioning the metric graph with long-range edges as repulsive\nconstraints yields an initial segmentation with high precision, with\nsubstantial accuracy gain for very thin objects. The convolutional embedding\nnet is reused without any modification to agglomerate the systematic splits\ncaused by complex \"self-contact\" motifs. Our proposed method achieves\nstate-of-the-art accuracy on the challenging problem of 3D neuron\nreconstruction from the brain images acquired by serial section electron\nmicroscopy. Our alternative, object-centered representation could be more\ngenerally useful for other computational tasks in automated neural circuit\nreconstruction.",
          "link": "http://arxiv.org/abs/1909.09872",
          "publishedOn": "2021-08-09T00:49:27.585Z",
          "wordCount": 605,
          "title": "Learning and Segmenting Dense Voxel Embeddings for 3D Neuron Reconstruction. (arXiv:1909.09872v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1901.10233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Volkhonskiy_D/0/1/0/all/0/1\">Denis Volkhonskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muravleva_E/0/1/0/all/0/1\">Ekaterina Muravleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudakov_O/0/1/0/all/0/1\">Oleg Sudakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orlov_D/0/1/0/all/0/1\">Denis Orlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belozerov_B/0/1/0/all/0/1\">Boris Belozerov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koroteev_D/0/1/0/all/0/1\">Dmitry Koroteev</a>",
          "description": "In many branches of earth sciences, the problem of rock study on the\nmicro-level arises. However, a significant number of representative samples is\nnot always feasible. Thus the problem of the generation of samples with similar\nproperties becomes actual. In this paper, we propose a novel deep learning\narchitecture for three-dimensional porous media reconstruction from\ntwo-dimensional slices. We fit a distribution on all possible three-dimensional\nstructures of a specific type based on the given dataset of samples. Then,\ngiven partial information (central slices), we recover the three-dimensional\nstructure around such slices as the most probable one according to that\nconstructed distribution. Technically, we implement this in the form of a deep\nneural network with encoder, generator and discriminator modules. Numerical\nexperiments show that this method provides a good reconstruction in terms of\nMinkowski functionals.",
          "link": "http://arxiv.org/abs/1901.10233",
          "publishedOn": "2021-08-09T00:49:27.551Z",
          "wordCount": 626,
          "title": "Reconstruction of 3D Porous Media From 2D Slices. (arXiv:1901.10233v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03168",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sribhashyam_S/0/1/0/all/0/1\">Sidharth Srivatsav Sribhashyam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salekin_M/0/1/0/all/0/1\">Md Sirajus Salekin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goldgof_D/0/1/0/all/0/1\">Dmitry Goldgof</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>",
          "description": "Spectrograms visualize the frequency components of a given signal which may\nbe an audio signal or even a time-series signal. Audio signals have higher\nsampling rate and high variability of frequency with time. Spectrograms can\ncapture such variations well. But, vital signs which are time-series signals\nhave less sampling frequency and low-frequency variability due to which,\nspectrograms fail to express variations and patterns. In this paper, we propose\na novel solution to introduce frequency variability using frequency modulation\non vital signs. Then we apply spectrograms on frequency modulated signals to\ncapture the patterns. The proposed approach has been evaluated on 4 different\nmedical datasets across both prediction and classification tasks. Significant\nresults are found showing the efficacy of the approach for vital sign signals.\nThe results from the proposed approach are promising with an accuracy of 91.55%\nand 91.67% in prediction and classification tasks respectively.",
          "link": "http://arxiv.org/abs/2108.03168",
          "publishedOn": "2021-08-09T00:49:27.474Z",
          "wordCount": 612,
          "title": "Pattern Recognition in Vital Signs Using Spectrograms. (arXiv:2108.03168v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Unnikrishnan_A/0/1/0/all/0/1\">Aishwarya Unnikrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_J/0/1/0/all/0/1\">Joseph Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1\">Lu Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capodieci_A/0/1/0/all/0/1\">Andrew Capodieci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayakumar_P/0/1/0/all/0/1\">Paramsothy Jayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barton_K/0/1/0/all/0/1\">Kira Barton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1\">Maani Ghaffari</a>",
          "description": "This paper reports on a dynamic semantic mapping framework that incorporates\n3D scene flow measurements into a closed-form Bayesian inference model.\nExistence of dynamic objects in the environment cause artifacts and traces in\ncurrent mapping algorithms, leading to an inconsistent map posterior. We\nleverage state-of-the-art semantic segmentation and 3D flow estimation using\ndeep learning to provide measurements for map inference. We develop a\ncontinuous (i.e., can be queried at arbitrary resolution) Bayesian model that\npropagates the scene with flow and infers a 3D semantic occupancy map with\nbetter performance than its static counterpart. Experimental results using\npublicly available data sets show that the proposed framework generalizes its\npredecessors and improves over direct measurements from deep neural networks\nconsistently.",
          "link": "http://arxiv.org/abs/2108.03180",
          "publishedOn": "2021-08-09T00:49:27.297Z",
          "wordCount": 567,
          "title": "Dynamic Semantic Occupancy Mapping using 3D Scene Flow and Closed-Form Bayesian Inference. (arXiv:2108.03180v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kudisthalert_W/0/1/0/all/0/1\">Wasu Kudisthalert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasupa_K/0/1/0/all/0/1\">Kitsuchart Pasupa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>",
          "description": "Extreme Learning Machine is a powerful classification method very competitive\nexisting classification methods. It is extremely fast at training.\nNevertheless, it cannot perform face verification tasks properly because face\nverification tasks require comparison of facial images of two individuals at\nthe same time and decide whether the two faces identify the same person. The\nstructure of Extreme Leaning Machine was not designed to feed two input data\nstreams simultaneously, thus, in 2-input scenarios Extreme Learning Machine\nmethods are normally applied using concatenated inputs. However, this setup\nconsumes two times more computational resources and it is not optimized for\nrecognition tasks where learning a separable distance metric is critical. For\nthese reasons, we propose and develop a Siamese Extreme Learning Machine\n(SELM). SELM was designed to be fed with two data streams in parallel\nsimultaneously. It utilizes a dual-stream Siamese condition in the extra\nSiamese layer to transform the data before passing it along to the hidden\nlayer. Moreover, we propose a Gender-Ethnicity-Dependent triplet feature\nexclusively trained on a variety of specific demographic groups. This feature\nenables learning and extracting of useful facial features of each group.\nExperiments were conducted to evaluate and compare the performances of SELM,\nExtreme Learning Machine, and DCNN. The experimental results showed that the\nproposed feature was able to perform correct classification at 97.87% accuracy\nand 99.45% AUC. They also showed that using SELM in conjunction with the\nproposed feature provided 98.31% accuracy and 99.72% AUC. They outperformed the\nwell-known DCNN and Extreme Leaning Machine methods by a wide margin.",
          "link": "http://arxiv.org/abs/2108.03140",
          "publishedOn": "2021-08-09T00:49:27.273Z",
          "wordCount": 703,
          "title": "SELM: Siamese Extreme Learning Machine with Application to Face Biometrics. (arXiv:2108.03140v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chartsias_A/0/1/0/all/0/1\">Agisilaos Chartsias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mumith_A/0/1/0/all/0/1\">Angela Mumith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_J/0/1/0/all/0/1\">Jorge Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_K/0/1/0/all/0/1\">Kanwal Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beqiri_A/0/1/0/all/0/1\">Arian Beqiri</a>",
          "description": "Analysis of cardiac ultrasound images is commonly performed in routine\nclinical practice for quantification of cardiac function. Its increasing\nautomation frequently employs deep learning networks that are trained to\npredict disease or detect image features. However, such models are extremely\ndata-hungry and training requires labelling of many thousands of images by\nexperienced clinicians. Here we propose the use of contrastive learning to\nmitigate the labelling bottleneck. We train view classification models for\nimbalanced cardiac ultrasound datasets and show improved performance for\nviews/classes for which minimal labelled data is available. Compared to a naive\nbaseline model, we achieve an improvement in F1 score of up to 26% in those\nviews while maintaining state-of-the-art performance for the views with\nsufficiently many labelled training observations.",
          "link": "http://arxiv.org/abs/2108.03124",
          "publishedOn": "2021-08-09T00:49:27.253Z",
          "wordCount": 570,
          "title": "Contrastive Learning for View Classification of Echocardiograms. (arXiv:2108.03124v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bateson_M/0/1/0/all/0/1\">Mathilde Bateson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kervadec_H/0/1/0/all/0/1\">Hoel Kervadec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lombaert_H/0/1/0/all/0/1\">Herv&#xe9; Lombaert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>",
          "description": "Domain adaptation (DA) has drawn high interest for its capacity to adapt a\nmodel trained on labeled source data to perform well on unlabeled or weakly\nlabeled target data from a different domain. Most common DA techniques require\nconcurrent access to the input images of both the source and target domains.\nHowever, in practice, privacy concerns often impede the availability of source\nimages in the adaptation phase. This is a very frequent DA scenario in medical\nimaging, where, for instance, the source and target images could come from\ndifferent clinical sites. We introduce a source-free domain adaptation for\nimage segmentation. Our formulation is based on minimizing a label-free entropy\nloss defined over target-domain data, which we further guide with a\ndomain-invariant prior on the segmentation regions. Many priors can be derived\nfrom anatomical information. Here, a class ratio prior is estimated from\nanatomical knowledge and integrated in the form of a Kullback Leibler (KL)\ndivergence in our overall loss function. Furthermore, we motivate our overall\nloss with an interesting link to maximizing the mutual information between the\ntarget images and their label predictions. We show the effectiveness of our\nprior aware entropy minimization in a variety of domain-adaptation scenarios,\nwith different modalities and applications, including spine, prostate, and\ncardiac segmentation. Our method yields comparable results to several state of\nthe art adaptation techniques, despite having access to much less information,\nas the source images are entirely absent in our adaptation phase. Our\nstraightforward adaptation strategy uses only one network, contrary to popular\nadversarial techniques, which are not applicable to a source-free DA setting.\nOur framework can be readily used in a breadth of segmentation problems, and\nour code is publicly available: https://github.com/mathilde-b/SFDA",
          "link": "http://arxiv.org/abs/2108.03152",
          "publishedOn": "2021-08-09T00:49:27.245Z",
          "wordCount": 729,
          "title": "Source-Free Domain Adaptation for Image Segmentation. (arXiv:2108.03152v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jooyoung Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yonghyun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwon_Y/0/1/0/all/0/1\">Youngjune Gwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "Denoising diffusion probabilistic models (DDPM) have shown remarkable\nperformance in unconditional image generation. However, due to the\nstochasticity of the generative process in DDPM, it is challenging to generate\nimages with the desired semantics. In this work, we propose Iterative Latent\nVariable Refinement (ILVR), a method to guide the generative process in DDPM to\ngenerate high-quality images based on a given reference image. Here, the\nrefinement of the generative process in DDPM enables a single DDPM to sample\nimages from various sets directed by the reference image. The proposed ILVR\nmethod generates high-quality images while controlling the generation. The\ncontrollability of our method allows adaptation of a single DDPM without any\nadditional learning in various image generation tasks, such as generation from\nvarious downsampling factors, multi-domain image translation, paint-to-image,\nand editing with scribbles.",
          "link": "http://arxiv.org/abs/2108.02938",
          "publishedOn": "2021-08-09T00:49:27.238Z",
          "wordCount": 574,
          "title": "ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models. (arXiv:2108.02938v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suarez_I/0/1/0/all/0/1\">Iago Su&#xe1;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buenaposada_J/0/1/0/all/0/1\">Jos&#xe9; M. Buenaposada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumela_L/0/1/0/all/0/1\">Luis Baumela</a>",
          "description": "Detecting local features, such as corners, segments or blobs, is the first\nstep in the pipeline of many Computer Vision applications. Its speed is crucial\nfor real time applications. In this paper we present ELSED, the fastest line\nsegment detector in the literature. The key for its efficiency is a local\nsegment growing algorithm that connects gradient aligned pixels in presence of\nsmall discontinuities. The proposed algorithm not only runs in devices with\nvery low end hardware, but may also be parametrized to foster the detection of\nshort or longer segments, depending on the task at hand. We also introduce new\nmetrics to evaluate the accuracy and repeatability of segment detectors. In our\nexperiments with different public benchmarks we prove that our method is the\nmost efficient in the literature and quantify the accuracy traded for such\ngain.",
          "link": "http://arxiv.org/abs/2108.03144",
          "publishedOn": "2021-08-09T00:49:27.231Z",
          "wordCount": 566,
          "title": "ELSED: Enhanced Line SEgment Drawing. (arXiv:2108.03144v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03131",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+MacLean_A/0/1/0/all/0/1\">Alexander MacLean</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abbasi_S/0/1/0/all/0/1\">Saad Abbasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebadi_A/0/1/0/all/0/1\">Ashkan Ebadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_A/0/1/0/all/0/1\">Andy Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pavlova_M/0/1/0/all/0/1\">Maya Pavlova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gunraj_H/0/1/0/all/0/1\">Hayden Gunraj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xi_P/0/1/0/all/0/1\">Pengcheng Xi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kohli_S/0/1/0/all/0/1\">Sonny Kohli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>",
          "description": "The Coronavirus Disease 2019 (COVID-19) pandemic has impacted many aspects of\nlife globally, and a critical factor in mitigating its effects is screening\nindividuals for infections, thereby allowing for both proper treatment for\nthose individuals as well as action to be taken to prevent further spread of\nthe virus. Point-of-care ultrasound (POCUS) imaging has been proposed as a\nscreening tool as it is a much cheaper and easier to apply imaging modality\nthan others that are traditionally used for pulmonary examinations, namely\nchest x-ray and computed tomography. Given the scarcity of expert radiologists\nfor interpreting POCUS examinations in many highly affected regions around the\nworld, low-cost deep learning-driven clinical decision support solutions can\nhave a large impact during the on-going pandemic. Motivated by this, we\nintroduce COVID-Net US, a highly efficient, self-attention deep convolutional\nneural network design tailored for COVID-19 screening from lung POCUS images.\nExperimental results show that the proposed COVID-Net US can achieve an AUC of\nover 0.98 while achieving 353X lower architectural complexity, 62X lower\ncomputational complexity, and 14.3X faster inference times on a Raspberry Pi.\nClinical validation was also conducted, where select cases were reviewed and\nreported on by a practicing clinician (20 years of clinical practice)\nspecializing in intensive care (ICU) and 15 years of expertise in POCUS\ninterpretation. To advocate affordable healthcare and artificial intelligence\nfor resource-constrained environments, we have made COVID-Net US open source\nand publicly available as part of the COVID-Net open source initiative.",
          "link": "http://arxiv.org/abs/2108.03131",
          "publishedOn": "2021-08-09T00:49:27.223Z",
          "wordCount": 781,
          "title": "COVID-Net US: A Tailored, Highly Efficient, Self-Attention Deep Convolutional Neural Network Design for Detection of COVID-19 Patient Cases from Point-of-care Ultrasound Imaging. (arXiv:2108.03131v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1\">Ge-Peng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Keren Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianbing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Appearance and motion are two important sources of information in video\nobject segmentation (VOS). Previous methods mainly focus on using simplex\nsolutions, lowering the upper bound of feature collaboration among and across\nthese two cues. In this paper, we study a novel framework, termed the FSNet\n(Full-duplex Strategy Network), which designs a relational cross-attention\nmodule (RCAM) to achieve the bidirectional message propagation across embedding\nsubspaces. Furthermore, the bidirectional purification module (BPM) is\nintroduced to update the inconsistent features between the spatial-temporal\nembeddings, effectively improving the model robustness. By considering the\nmutual restraint within the full-duplex strategy, our FSNet performs the\ncross-modal feature-passing (i.e., transmission and receiving) simultaneously\nbefore the fusion and decoding stage, making it robust to various challenging\nscenarios (e.g., motion blur, occlusion) in VOS. Extensive experiments on five\npopular benchmarks (i.e., DAVIS$_{16}$, FBMS, MCL, SegTrack-V2, and\nDAVSOD$_{19}$) show that our FSNet outperforms other state-of-the-arts for both\nthe VOS and video salient object detection tasks.",
          "link": "http://arxiv.org/abs/2108.03151",
          "publishedOn": "2021-08-09T00:49:27.203Z",
          "wordCount": 600,
          "title": "Full-Duplex Strategy for Video Object Segmentation. (arXiv:2108.03151v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhiqing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fengkai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuo Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yangyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Huici Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhiyong Feng</a>",
          "description": "With autonomous driving developing in a booming stage, accurate object\ndetection in complex scenarios attract wide attention to ensure the safety of\nautonomous driving. Millimeter wave (mmWave) radar and vision fusion is a\nmainstream solution for accurate obstacle detection. This article presents a\ndetailed survey on mmWave radar and vision fusion based obstacle detection\nmethods. Firstly, we introduce the tasks, evaluation criteria and datasets of\nobject detection for autonomous driving. Then, the process of mmWave radar and\nvision fusion is divided into three parts: sensor deployment, sensor\ncalibration and sensor fusion, which are reviewed comprehensively. Especially,\nwe classify the fusion methods into data level, decision level and feature\nlevel fusion methods. Besides, we introduce the fusion of lidar and vision in\nautonomous driving in the aspects of obstacle detection, object classification\nand road segmentation, which is promising in the future. Finally, we summarize\nthis article.",
          "link": "http://arxiv.org/abs/2108.03004",
          "publishedOn": "2021-08-09T00:49:27.196Z",
          "wordCount": 595,
          "title": "MmWave Radar and Vision Fusion based Object Detection for Autonomous Driving: A Survey. (arXiv:2108.03004v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenglong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Wen Chen</a>",
          "description": "Contrastive learning, which aims at minimizing the distance between positive\npairs while maximizing that of negative ones, has been widely and successfully\napplied in unsupervised feature learning, where the design of positive and\nnegative (pos/neg) pairs is one of its keys. In this paper, we attempt to\ndevise a feature-level data manipulation, differing from data augmentation, to\nenhance the generic contrastive self-supervised learning. To this end, we first\ndesign a visualization scheme for pos/neg score (Pos/neg score indicates cosine\nsimilarity of pos/neg pair.) distribution, which enables us to analyze,\ninterpret and understand the learning process. To our knowledge, this is the\nfirst attempt of its kind. More importantly, leveraging this tool, we gain some\nsignificant observations, which inspire our novel Feature Transformation\nproposals including the extrapolation of positives. This operation creates\nharder positives to boost the learning because hard positives enable the model\nto be more view-invariant. Besides, we propose the interpolation among\nnegatives, which provides diversified negatives and makes the model more\ndiscriminative. It is the first attempt to deal with both challenges\nsimultaneously. Experiment results show that our proposed Feature\nTransformation can improve at least 6.0% accuracy on ImageNet-100 over MoCo\nbaseline, and about 2.0% accuracy on ImageNet-1K over the MoCoV2 baseline.\nTransferring to the downstream tasks successfully demonstrate our model is less\ntask-bias. Visualization tools and codes\nhttps://github.com/DTennant/CL-Visualizing-Feature-Transformation .",
          "link": "http://arxiv.org/abs/2108.02982",
          "publishedOn": "2021-08-09T00:49:27.189Z",
          "wordCount": 674,
          "title": "Improving Contrastive Learning by Visualizing Feature Transformation. (arXiv:2108.02982v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pepe_A/0/1/0/all/0/1\">Antonio Pepe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1\">Christina Gsaxner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fen-hua Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1\">Jens Kleesiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F. Frangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1\">Jan Egger</a>",
          "description": "The aortic vessel tree is composed of the aorta and its branching arteries,\nand plays a key role in supplying the whole body with blood. Aortic diseases,\nlike aneurysms or dissections, can lead to an aortic rupture, whose treatment\nwith open surgery is highly risky. Therefore, patients commonly undergo drug\ntreatment under constant monitoring, which requires regular inspections of the\nvessels through imaging. The standard imaging modality for diagnosis and\nmonitoring is computed tomography (CT), which can provide a detailed picture of\nthe aorta and its branching vessels if combined with a contrast agent,\nresulting in a CT angiography (CTA). Optimally, the whole aortic vessel tree\ngeometry from consecutive CTAs, are overlaid and compared. This allows to not\nonly detect changes in the aorta, but also more peripheral vessel tree changes,\ncaused by the primary pathology or newly developed. When performed manually,\nthis reconstruction requires slice by slice contouring, which could easily take\na whole day for a single aortic vessel tree and, hence, is not feasible in\nclinical practice. Automatic or semi-automatic vessel tree segmentation\nalgorithms, on the other hand, can complete this task in a fraction of the\nmanual execution time and run in parallel to the clinical routine of the\nclinicians. In this paper, we systematically review computing techniques for\nthe automatic and semi-automatic segmentation of the aortic vessel tree. The\nreview concludes with an in-depth discussion on how close these\nstate-of-the-art approaches are to an application in clinical practice and how\nactive this research field is, taking into account the number of publications,\ndatasets and challenges.",
          "link": "http://arxiv.org/abs/2108.02998",
          "publishedOn": "2021-08-09T00:49:27.181Z",
          "wordCount": 744,
          "title": "AI-based Aortic Vessel Tree Segmentation for Cardiovascular Diseases Treatment: Status Quo. (arXiv:2108.02998v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03116",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1\">Kai Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weixing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Feng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Dongdong Zheng</a>",
          "description": "Rotating object detection has wide applications in aerial photographs, remote\nsensing images, UAVs, etc. At present, most of the rotating object detection\ndatasets focus on the field of remote sensing, and these images are usually\nshot in high-altitude scenes. However, image datasets captured at low-altitude\nareas also should be concerned, such as drone-based datasets. So we present a\nlow-altitude dronebased dataset, named UAV-ROD, aiming to promote the research\nand development in rotating object detection and UAV applications. The UAV-ROD\nconsists of 1577 images and 30,090 instances of car category annotated by\noriented bounding boxes. In particular, The UAV-ROD can be utilized for the\nrotating object detection, vehicle orientation recognition and object counting\ntasks. Compared with horizontal object detection, the regression stage of the\nrotation detection is a tricky problem. In this paper, we propose a rotating\nobject detector TS4Net, which contains anchor refinement module (ARM) and\ntwo-stage sample selective strategy (TS4). The ARM can convert preseted\nhorizontal anchors into high-quality rotated anchors through twostage anchor\nrefinement. The TS4 module utilizes different constrained sample selective\nstrategies to allocate positive and negative samples, which is adaptive to the\nregression task in different stages. Benefiting from the ARM and TS4, the\nTS4Net can achieve superior performance for rotating object detection solely\nwith one preseted horizontal anchor. Extensive experimental results on UAV-ROD\ndataset and three remote sensing datasets DOTA, HRSC2016 and UCAS-AOD\ndemonstrate that our method achieves competitive performance against most\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.03116",
          "publishedOn": "2021-08-09T00:49:27.173Z",
          "wordCount": 683,
          "title": "TS4Net: Two-Stage Sample Selective Strategy for Rotating Object Detection. (arXiv:2108.03116v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demir_U/0/1/0/all/0/1\">Ufuk Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozer_A/0/1/0/all/0/1\">Atahan Ozer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahin_Y/0/1/0/all/0/1\">Yusuf H. Sahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_G/0/1/0/all/0/1\">Gozde Unal</a>",
          "description": "In recent years, deep learning based methods have shown success in essential\nmedical image analysis tasks such as segmentation. Post-processing and refining\nthe results of segmentation is a common practice to decrease the\nmisclassifications originating from the segmentation network. In addition to\nwidely used methods like Conditional Random Fields (CRFs) which focus on the\nstructure of the segmented volume/area, a graph-based recent approach makes use\nof certain and uncertain points in a graph and refines the segmentation\naccording to a small graph convolutional network (GCN). However, there are two\ndrawbacks of the approach: most of the edges in the graph are assigned randomly\nand the GCN is trained independently from the segmentation network. To address\nthese issues, we define a new neighbor-selection mechanism according to feature\ndistances and combine the two networks in the training procedure. According to\nthe experimental results on pancreas segmentation from Computed Tomography (CT)\nimages, we demonstrate improvement in the quantitative measures. Also,\nexamining the dynamic neighbors created by our method, edges between\nsemantically similar image parts are observed. The proposed method also shows\nqualitative enhancements in the segmentation maps, as demonstrated in the\nvisual results.",
          "link": "http://arxiv.org/abs/2108.03117",
          "publishedOn": "2021-08-09T00:49:27.153Z",
          "wordCount": 651,
          "title": "Uncertainty-Based Dynamic Graph Neighborhoods For Medical Segmentation. (arXiv:2108.03117v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boxer_L/0/1/0/all/0/1\">Laurence Boxer</a>",
          "description": "Two objects may be close in the Hausdor? metric, yet have very different\ngeometric and topological properties. We examine other methods of comparing\ndigital images such that objects close in each of these measures have some\nsimilar geometric or topological property. Such measures may be combined with\nthe Hausdorff metric to yield a metric in which close images are similar with\nrespect to multiple properties.",
          "link": "http://arxiv.org/abs/2108.03114",
          "publishedOn": "2021-08-09T00:49:27.144Z",
          "wordCount": 502,
          "title": "Beyond the Hausdorff Metric in Digital Topology. (arXiv:2108.03114v1 [cs.CG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03002",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhang_H/0/1/0/all/0/1\">HongBing Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1\">XinYi Liu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fan_H/0/1/0/all/0/1\">HongTao Fan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1\">YaJing Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ye_Y/0/1/0/all/0/1\">Yinlin Ye</a>",
          "description": "More recently, an Approximate SVD Based on Qatar Riyal (QR) Decomposition\n(CSVD-QR) method for matrix complete problem is presented, whose computational\ncomplexity is $O(r^2(m+n))$, which is mainly due to that $r$ is far less than\n$\\min\\{m,n\\}$, where $r$ represents the largest number of singular values of\nmatrix $X$. What is particularly interesting is that after replacing the\nnuclear norm with the $L_{2,1}$ norm proposed based on this decomposition, as\nthe upper bound of the nuclear norm, when the intermediate matrix $D$ in its\ndecomposition is close to the diagonal matrix, it will converge to the nuclear\nnorm, and is exactly equal, when the $D$ matrix is equal to the diagonal\nmatrix, to the nuclear norm, which ingeniously avoids the calculation of the\nsingular value of the matrix. To the best of our knowledge, there is no\nliterature to generalize and apply it to solve tensor complete problems.\nInspired by this, in this paper we propose a class of tensor minimization model\nbased on $L_{2,1}$ norm and CSVD-QR method for the tensor complete problem,\nwhich is convex and therefore has a global minimum solution.",
          "link": "http://arxiv.org/abs/2108.03002",
          "publishedOn": "2021-08-09T00:49:27.136Z",
          "wordCount": 640,
          "title": "Fast and Accurate Low-Rank Tensor Completion Methods Based on QR Decomposition and $L_{2,1}$ Norm Minimization. (arXiv:2108.03002v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Shuvendu Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>",
          "description": "We propose a self-supervised contrastive learning approach for facial\nexpression recognition (FER) in videos. We propose a novel temporal\nsampling-based augmentation scheme to be utilized in addition to standard\nspatial augmentations used for contrastive learning. Our proposed temporal\naugmentation scheme randomly picks from one of three temporal sampling\ntechniques: (1) pure random sampling, (2) uniform sampling, and (3) sequential\nsampling. This is followed by a combination of up to three standard spatial\naugmentations. We then use a deep R(2+1)D network for FER, which we train in a\nself-supervised fashion based on the augmentations and subsequently fine-tune.\nExperiments are performed on the Oulu-CASIA dataset and the performance is\ncompared to other works in FER. The results indicate that our method achieves\nan accuracy of 89.4%, setting a new state-of-the-art by outperforming other\nworks. Additional experiments and analysis confirm the considerable\ncontribution of the proposed temporal augmentation versus the existing spatial\nones.",
          "link": "http://arxiv.org/abs/2108.03064",
          "publishedOn": "2021-08-09T00:49:27.128Z",
          "wordCount": 599,
          "title": "Spatiotemporal Contrastive Learning of Facial Expressions in Videos. (arXiv:2108.03064v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03138",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mason_H/0/1/0/all/0/1\">Harry Mason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristoni_L/0/1/0/all/0/1\">Lorenzo Cristoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walden_A/0/1/0/all/0/1\">Andrew Walden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazzari_R/0/1/0/all/0/1\">Roberto Lazzari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulimood_T/0/1/0/all/0/1\">Thomas Pulimood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandjean_L/0/1/0/all/0/1\">Louis Grandjean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wheeler_Kingshott_C/0/1/0/all/0/1\">Claudia AM Gandini Wheeler-Kingshott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baum_Z/0/1/0/all/0/1\">Zachary MC Baum</a>",
          "description": "Lung ultrasound imaging has been shown effective in detecting typical\npatterns for interstitial pneumonia, as a point-of-care tool for both patients\nwith COVID-19 and other community-acquired pneumonia (CAP). In this work, we\nfocus on the hyperechoic B-line segmentation task. Using deep neural networks,\nwe automatically outline the regions that are indicative of pathology-sensitive\nartifacts and their associated sonographic patterns. With a real-world\ndata-scarce scenario, we investigate approaches to utilize both COVID-19 and\nCAP lung ultrasound data to train the networks; comparing fine-tuning and\nunsupervised domain adaptation. Segmenting either type of lung condition at\ninference may support a range of clinical applications during evolving epidemic\nstages, but also demonstrates value in resource-constrained clinical scenarios.\nAdapting real clinical data acquired from COVID-19 patients to those from CAP\npatients significantly improved Dice scores from 0.60 to 0.87 (p < 0.001) and\nfrom 0.43 to 0.71 (p < 0.001), on independent COVID-19 and CAP test cases,\nrespectively. It is of practical value that the improvement was demonstrated\nwith only a small amount of data in both training and adaptation data sets, a\ncommon constraint for deploying machine learning models in clinical practice.\nInterestingly, we also report that the inverse adaptation, from labelled CAP\ndata to unlabeled COVID-19 data, did not demonstrate an improvement when tested\non either condition. Furthermore, we offer a possible explanation that\ncorrelates the segmentation performance to label consistency and data domain\ndiversity in this point-of-care lung ultrasound application.",
          "link": "http://arxiv.org/abs/2108.03138",
          "publishedOn": "2021-08-09T00:49:27.121Z",
          "wordCount": 740,
          "title": "Lung Ultrasound Segmentation and Adaptation between COVID-19 and Community-Acquired Pneumonia. (arXiv:2108.03138v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiangxi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+lin_G/0/1/0/all/0/1\">Guosheng lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>",
          "description": "Currently, the state-of-the-art methods treat few-shot semantic segmentation\ntask as a conditional foreground-background segmentation problem, assuming each\nclass is independent. In this paper, we introduce the concept of meta-class,\nwhich is the meta information (e.g. certain middle-level features) shareable\namong all classes. To explicitly learn meta-class representations in few-shot\nsegmentation task, we propose a novel Meta-class Memory based few-shot\nsegmentation method (MM-Net), where we introduce a set of learnable memory\nembeddings to memorize the meta-class information during the base class\ntraining and transfer to novel classes during the inference stage. Moreover,\nfor the $k$-shot scenario, we propose a novel image quality measurement module\nto select images from the set of support images. A high-quality class prototype\ncould be obtained with the weighted sum of support image features based on the\nquality measure. Experiments on both PASCAL-$5^i$ and COCO dataset shows that\nour proposed method is able to achieve state-of-the-art results in both 1-shot\nand 5-shot settings. Particularly, our proposed MM-Net achieves 37.5\\% mIoU on\nthe COCO dataset in 1-shot setting, which is 5.1\\% higher than the previous\nstate-of-the-art.",
          "link": "http://arxiv.org/abs/2108.02958",
          "publishedOn": "2021-08-09T00:49:27.112Z",
          "wordCount": 615,
          "title": "Learning Meta-class Memory for Few-Shot Semantic Segmentation. (arXiv:2108.02958v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1\">Shunda Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1\">Liang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>",
          "description": "Single image dehazing is a challenging task, for which the domain shift\nbetween synthetic training data and real-world testing images usually leads to\ndegradation of existing methods. To address this issue, we propose a novel\nimage dehazing framework collaborating with unlabeled real data. First, we\ndevelop a disentangled image dehazing network (DID-Net), which disentangles the\nfeature representations into three component maps, i.e. the latent haze-free\nimage, the transmission map, and the global atmospheric light estimate,\nrespecting the physical model of a haze process. Our DID-Net predicts the three\ncomponent maps by progressively integrating features across scales, and refines\neach map by passing an independent refinement network. Then a\ndisentangled-consistency mean-teacher network (DMT-Net) is employed to\ncollaborate unlabeled real data for boosting single image dehazing.\nSpecifically, we encourage the coarse predictions and refinements of each\ndisentangled component to be consistent between the student and teacher\nnetworks by using a consistency loss on unlabeled real data. We make comparison\nwith 13 state-of-the-art dehazing methods on a new collected dataset (Haze4K)\nand two widely-used dehazing datasets (i.e., SOTS and HazeRD), as well as on\nreal-world hazy images. Experimental results demonstrate that our method has\nobvious quantitative and qualitative improvements over the existing methods.",
          "link": "http://arxiv.org/abs/2108.02934",
          "publishedOn": "2021-08-09T00:49:27.086Z",
          "wordCount": 657,
          "title": "From Synthetic to Real: Image Dehazing Collaborating with Unlabeled Real Data. (arXiv:2108.02934v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuejiao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_K/0/1/0/all/0/1\">Kea Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1\">Tyler Derr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1\">Eirini Ntoutsi</a>",
          "description": "While image understanding on recognition-level has achieved remarkable\nadvancements, reliable visual scene understanding requires comprehensive image\nunderstanding on recognition-level but also cognition-level, which calls for\nexploiting the multi-source information as well as learning different levels of\nunderstanding and extensive commonsense knowledge. In this paper, we propose a\nnovel Cognitive Attention Network (CAN) for visual commonsense reasoning to\nachieve interpretable visual understanding. Specifically, we first introduce an\nimage-text fusion module to fuse information from images and text collectively.\nSecond, a novel inference module is designed to encode commonsense among image,\nquery and response. Extensive experiments on large-scale Visual Commonsense\nReasoning (VCR) benchmark dataset demonstrate the effectiveness of our\napproach. The implementation is publicly available at\nhttps://github.com/tanjatang/CAN",
          "link": "http://arxiv.org/abs/2108.02924",
          "publishedOn": "2021-08-09T00:49:27.078Z",
          "wordCount": 562,
          "title": "Interpretable Visual Understanding with Cognitive Attention Network. (arXiv:2108.02924v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1\">Liangyu Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanjie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>",
          "description": "Labeling is onerous for crowd counting as it should annotate each individual\nin crowd images. Recently, several methods have been proposed for\nsemi-supervised crowd counting to reduce the labeling efforts. Given a limited\nlabeling budget, they typically select a few crowd images and densely label all\nindividuals in each of them. Despite the promising results, we argue the\nNone-or-All labeling strategy is suboptimal as the densely labeled individuals\nin each crowd image usually appear similar while the massive unlabeled crowd\nimages may contain entirely diverse individuals. To this end, we propose to\nbreak the labeling chain of previous methods and make the first attempt to\nreduce spatial labeling redundancy for semi-supervised crowd counting. First,\ninstead of annotating all the regions in each crowd image, we propose to\nannotate the representative ones only. We analyze the region representativeness\nfrom both vertical and horizontal directions, and formulate them as cluster\ncenters of Gaussian Mixture Models. Additionally, to leverage the rich\nunlabeled regions, we exploit the similarities among individuals in each crowd\nimage to directly supervise the unlabeled regions via feature propagation\ninstead of the error-prone label propagation employed in the previous methods.\nIn this way, we can transfer the original spatial labeling redundancy caused by\nindividual similarities to effective supervision signals on the unlabeled\nregions. Extensive experiments on the widely-used benchmarks demonstrate that\nour method can outperform previous best approaches by a large margin.",
          "link": "http://arxiv.org/abs/2108.02970",
          "publishedOn": "2021-08-09T00:49:27.071Z",
          "wordCount": 678,
          "title": "Reducing Spatial Labeling Redundancy for Semi-supervised Crowd Counting. (arXiv:2108.02970v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wen-Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Min-Chun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chu-Song Chen</a>",
          "description": "Geometry-aware modules are widely applied in recent deep learning\narchitectures for scene representation and rendering. However, these modules\nrequire intrinsic camera information that might not be obtained accurately. In\nthis paper, we propose a Spatial Transformation Routing (STR) mechanism to\nmodel the spatial properties without applying any geometric prior. The STR\nmechanism treats the spatial transformation as the message passing process, and\nthe relation between the view poses and the routing weights is modeled by an\nend-to-end trainable neural network. Besides, an Occupancy Concept Mapping\n(OCM) framework is proposed to provide explainable rationals for scene-fusion\nprocesses. We conducted experiments on several datasets and show that the\nproposed STR mechanism improves the performance of the Generative Query Network\n(GQN). The visualization results reveal that the routing process can pass the\nobserved information from one location of some view to the associated location\nin the other view, which demonstrates the advantage of the proposed model in\nterms of spatial cognition.",
          "link": "http://arxiv.org/abs/2108.03072",
          "publishedOn": "2021-08-09T00:49:27.063Z",
          "wordCount": 603,
          "title": "STR-GQN: Scene Representation and Rendering for Unknown Cameras Based on Spatial Transformation Routing. (arXiv:2108.03072v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+lu_Z/0/1/0/all/0/1\">Zhihe lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Sen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>",
          "description": "A few-shot semantic segmentation model is typically composed of a CNN\nencoder, a CNN decoder and a simple classifier (separating foreground and\nbackground pixels). Most existing methods meta-learn all three model components\nfor fast adaptation to a new class. However, given that as few as a single\nsupport set image is available, effective model adaption of all three\ncomponents to the new class is extremely challenging. In this work we propose\nto simplify the meta-learning task by focusing solely on the simplest\ncomponent, the classifier, whilst leaving the encoder and decoder to\npre-training. We hypothesize that if we pre-train an off-the-shelf segmentation\nmodel over a set of diverse training classes with sufficient annotations, the\nencoder and decoder can capture rich discriminative features applicable for any\nunseen classes, rendering the subsequent meta-learning stage unnecessary. For\nthe classifier meta-learning, we introduce a Classifier Weight Transformer\n(CWT) designed to dynamically adapt the supportset trained classifier's weights\nto each query image in an inductive way. Extensive experiments on two standard\nbenchmarks show that despite its simplicity, our method outperforms the\nstate-of-the-art alternatives, often by a large margin.Code is available on\nhttps://github.com/zhiheLu/CWTfor-FSS.",
          "link": "http://arxiv.org/abs/2108.03032",
          "publishedOn": "2021-08-09T00:49:27.056Z",
          "wordCount": 637,
          "title": "Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer. (arXiv:2108.03032v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03001",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chenqian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanlu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Lyna Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiaotian Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuqing Yang</a>",
          "description": "Architecture performance predictors have been widely used in neural\narchitecture search (NAS). Although they are shown to be simple and effective,\nthe optimization objectives in previous arts (e.g., precise accuracy estimation\nor perfect ranking of all architectures in the space) did not capture the\nranking nature of NAS. In addition, a large number of ground-truth\narchitecture-accuracy pairs are usually required to build a reliable predictor,\nmaking the process too computationally expensive. To overcome these, in this\npaper, we look at NAS from a novel point of view and introduce Learning to Rank\n(LTR) methods to select the best (ace) architectures from a space.\nSpecifically, we propose to use Normalized Discounted Cumulative Gain (NDCG) as\nthe target metric and LambdaRank as the training algorithm. We also propose to\nleverage weak supervision from weight sharing by pretraining architecture\nrepresentation on weak labels obtained from the super-net and then finetuning\nthe ranking model using a small number of architectures trained from scratch.\nExtensive experiments on NAS benchmarks and large-scale search spaces\ndemonstrate that our approach outperforms SOTA with a significantly reduced\nsearch cost.",
          "link": "http://arxiv.org/abs/2108.03001",
          "publishedOn": "2021-08-09T00:49:27.038Z",
          "wordCount": 642,
          "title": "AceNAS: Learning to Rank Ace Neural Architectures with Weak Supervision of Weight Sharing. (arXiv:2108.03001v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barbato_F/0/1/0/all/0/1\">Francesco Barbato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michieli_U/0/1/0/all/0/1\">Umberto Michieli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toldo_M/0/1/0/all/0/1\">Marco Toldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanuttigh_P/0/1/0/all/0/1\">Pietro Zanuttigh</a>",
          "description": "Deep learning models achieve outstanding accuracy in semantic segmentation,\nhowever they require a huge amount of labeled data for their optimization.\nHence, domain adaptation approaches have come into play to transfer knowledge\nacquired on a label-abundant source domain to a related label-scarce target\ndomain. However, such models do not generalize well to data with statistical\nproperties not perfectly matching the ones of the training samples. In this\nwork, we design and carefully analyze multiple latent space-shaping\nregularization strategies that work in conjunction to reduce the domain\ndiscrepancy in semantic segmentation. In particular, we devise a feature\nclustering strategy to increase domain alignment, a feature perpendicularity\nconstraint to space apart feature belonging to different semantic classes,\nincluding those not present in the current batch, and a feature norm alignment\nstrategy to separate active and inactive channels. Additionally, we propose a\nnovel performance metric to capture the relative efficacy of an adaptation\nstrategy compared to supervised training. We verify the effectiveness of our\nframework in synthetic-to-real and real-to-real adaptation scenarios,\noutperforming previous state-of-the-art methods on multiple road scenes\nbenchmarks and using different backbones.",
          "link": "http://arxiv.org/abs/2108.03021",
          "publishedOn": "2021-08-09T00:49:27.029Z",
          "wordCount": 637,
          "title": "Adapting Segmentation Networks to New Domains by Disentangling Latent Representations. (arXiv:2108.03021v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1\">Weiqing Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haisheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuqiang Jiang</a>",
          "description": "Background: Maintaining a healthy diet is vital to avoid health-related\nissues, e.g., undernutrition, obesity and many non-communicable diseases. An\nindispensable part of the health diet is dietary assessment. Traditional manual\nrecording methods are burdensome and contain substantial biases and errors.\nRecent advances in Artificial Intelligence, especially computer vision\ntechnologies, have made it possible to develop automatic dietary assessment\nsolutions, which are more convenient, less time-consuming and even more\naccurate to monitor daily food intake.\n\nScope and approach: This review presents one unified Vision-Based Dietary\nAssessment (VBDA) framework, which generally consists of three stages: food\nimage analysis, volume estimation and nutrient derivation. Vision-based food\nanalysis methods, including food recognition, detection and segmentation, are\nsystematically summarized, and methods of volume estimation and nutrient\nderivation are also given. The prosperity of deep learning makes VBDA gradually\nmove to an end-to-end implementation, which applies food images to a single\nnetwork to directly estimate the nutrition. The recently proposed end-to-end\nmethods are also discussed. We further analyze existing dietary assessment\ndatasets, indicating that one large-scale benchmark is urgently needed, and\nfinally highlight key challenges and future trends for VBDA.\n\nKey findings and conclusions: After thorough exploration, we find that\nmulti-task end-to-end deep learning approaches are one important trend of VBDA.\nDespite considerable research progress, many challenges remain for VBDA due to\nthe meal complexity. We also provide the latest ideas for future development of\nVBDA, e.g., fine-grained food analysis and accurate volume estimation. This\nsurvey aims to encourage researchers to propose more practical solutions for\nVBDA.",
          "link": "http://arxiv.org/abs/2108.02947",
          "publishedOn": "2021-08-09T00:49:27.021Z",
          "wordCount": 689,
          "title": "Vision-Based Food Analysis for Automatic Dietary Assessment. (arXiv:2108.02947v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Che_K/0/1/0/all/0/1\">Kaiwei Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chengwei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yibing Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1\">Nachuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiankun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1\">Max Q.-H. Meng</a>",
          "description": "Colonoscopy is a standard imaging tool for visualizing the entire\ngastrointestinal (GI) tract of patients to capture lesion areas. However, it\ntakes the clinicians excessive time to review a large number of images\nextracted from colonoscopy videos. Thus, automatic detection of biological\nanatomical landmarks within the colon is highly demanded, which can help reduce\nthe burden of clinicians by providing guidance information for the locations of\nlesion areas. In this article, we propose a novel deep learning-based approach\nto detect biological anatomical landmarks in colonoscopy videos. First, raw\ncolonoscopy video sequences are pre-processed to reject interference frames.\nSecond, a ResNet-101 based network is used to detect three biological\nanatomical landmarks separately to obtain the intermediate detection results.\nThird, to achieve more reliable localization of the landmark periods within the\nwhole video period, we propose to post-process the intermediate detection\nresults by identifying the incorrectly predicted frames based on their temporal\ndistribution and reassigning them back to the correct class. Finally, the\naverage detection accuracy reaches 99.75\\%. Meanwhile, the average IoU of 0.91\nshows a high degree of similarity between our predicted landmark periods and\nground truth. The experimental results demonstrate that our proposed model is\ncapable of accurately detecting and localizing biological anatomical landmarks\nfrom colonoscopy videos.",
          "link": "http://arxiv.org/abs/2108.02948",
          "publishedOn": "2021-08-09T00:49:27.013Z",
          "wordCount": 657,
          "title": "Deep Learning-based Biological Anatomical Landmark Detection in Colonoscopy Videos. (arXiv:2108.02948v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1\">Rashmi Bakshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courtney_J/0/1/0/all/0/1\">Jane Courtney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berry_D/0/1/0/all/0/1\">Damon Berry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavin_G/0/1/0/all/0/1\">Graham Gavin</a>",
          "description": "The process of hand washing involves complex hand movements. There are six\nprincipal sequential steps for washing hands as per the World Health\nOrganisation (WHO) guidelines. In this work, a detailed description of an\naluminium rig construction for creating a robust hand-washing dataset is\ndiscussed. The preliminary results with the help of image processing and\ncomputer vision algorithms for hand pose extraction and feature detection such\nas Harris detector, Shi-Tomasi and SIFT are demonstrated. The hand hygiene\npose- Rub hands palm to palm was captured as an input image for running all the\nexperiments. The future work will focus upon processing the video recordings of\nhand movements captured and applying deep-learning solutions for the\nclassification of hand-hygiene stages.",
          "link": "http://arxiv.org/abs/2108.03015",
          "publishedOn": "2021-08-09T00:49:27.004Z",
          "wordCount": 548,
          "title": "Feature Detection for Hand Hygiene Stages. (arXiv:2108.03015v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tokieda_K/0/1/0/all/0/1\">Kodai Tokieda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwaguchi_T/0/1/0/all/0/1\">Takafumi Iwaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawasaki_H/0/1/0/all/0/1\">Hiroshi Kawasaki</a>",
          "description": "Importance of structured-light based one-shot scanning technique is\nincreasing because of its simple system configuration and ability of capturing\nmoving objects. One severe limitation of the technique is that it can capture\nonly sparse shape, but not high frequency shapes, because certain area of\nprojection pattern is required to encode spatial information. In this paper, we\npropose a technique to recover high-frequency shapes by using shading\ninformation, which is captured by one-shot RGB-D sensor based on structured\nlight with single camera. Since color image comprises shading information of\nobject surface, high-frequency shapes can be recovered by shape from shading\ntechniques. Although multiple images with different lighting positions are\nrequired for shape from shading techniques, we propose a learning based\napproach to recover shape from a single image. In addition, to overcome the\nproblem of preparing sufficient amount of data for training, we propose a new\ndata augmentation method for high-frequency shapes using synthetic data and\ndomain adaptation. Experimental results are shown to confirm the effectiveness\nof the proposed method.",
          "link": "http://arxiv.org/abs/2108.02937",
          "publishedOn": "2021-08-09T00:49:26.997Z",
          "wordCount": 607,
          "title": "High-frequency shape recovery from shading by CNN and domain adaptation. (arXiv:2108.02937v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosinol_A/0/1/0/all/0/1\">Antoni Rosinol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>",
          "description": "Meshes are commonly used as 3D maps since they encode the topology of the\nscene while being lightweight.\n\nUnfortunately, 3D meshes are mathematically difficult to handle directly\nbecause of their combinatorial and discrete nature.\n\nTherefore, most approaches generate 3D meshes of a scene after fusing depth\ndata using volumetric or other representations.\n\nNevertheless, volumetric fusion remains computationally expensive both in\nterms of speed and memory.\n\nIn this paper, we leapfrog these intermediate representations and build a 3D\nmesh directly from a depth map and the sparse landmarks triangulated with\nvisual odometry.\n\nTo this end, we formulate a non-smooth convex optimization problem that we\nsolve using a primal-dual method.\n\nOur approach generates a smooth and accurate 3D mesh that substantially\nimproves the state-of-the-art on direct mesh reconstruction while running in\nreal-time.",
          "link": "http://arxiv.org/abs/2108.02957",
          "publishedOn": "2021-08-09T00:49:26.978Z",
          "wordCount": 571,
          "title": "Smooth Mesh Estimation from Depth Data using Non-Smooth Convex Optimization. (arXiv:2108.02957v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xinxin Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haotian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruigang Yang</a>",
          "description": "This paper presents a novel framework to recover \\emph{detailed} avatar from\na single image. It is a challenging task due to factors such as variations in\nhuman shapes, body poses, texture, and viewpoints. Prior methods typically\nattempt to recover the human body shape using a parametric-based template that\nlacks the surface details. As such resulting body shape appears to be without\nclothing. In this paper, we propose a novel learning-based framework that\ncombines the robustness of the parametric model with the flexibility of\nfree-form 3D deformation. We use the deep neural networks to refine the 3D\nshape in a Hierarchical Mesh Deformation (HMD) framework, utilizing the\nconstraints from body joints, silhouettes, and per-pixel shading information.\nOur method can restore detailed human body shapes with complete textures beyond\nskinned models. Experiments demonstrate that our method has outperformed\nprevious state-of-the-art approaches, achieving better accuracy in terms of\nboth 2D IoU number and 3D metric distance.",
          "link": "http://arxiv.org/abs/2108.02931",
          "publishedOn": "2021-08-09T00:49:26.970Z",
          "wordCount": 593,
          "title": "Detailed Avatar Recovery from Single Image. (arXiv:2108.02931v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yue Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhuoming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honggang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Deqiang Xu</a>",
          "description": "mmWave radar has been shown as an effective sensing technique in low\nvisibility, smoke, dusty, and dense fog environment. However tapping the\npotential of radar sensing to reconstruct 3D object shapes remains a great\nchallenge, due to the characteristics of radar data such as sparsity, low\nresolution, specularity, high noise, and multi-path induced shadow reflections\nand artifacts. In this paper we propose 3D Reconstruction and Imaging via\nmmWave Radar (3DRIMR), a deep learning based architecture that reconstructs 3D\nshape of an object in dense detailed point cloud format, based on sparse raw\nmmWave radar intensity data. The architecture consists of two back-to-back\nconditional GAN deep neural networks: the first generator network generates 2D\ndepth images based on raw radar intensity data, and the second generator\nnetwork outputs 3D point clouds based on the results of the first generator.\nThe architecture exploits both convolutional neural network's convolutional\noperation (that extracts local structure neighborhood information) and the\nefficiency and detailed geometry capture capability of point clouds (other than\ncostly voxelization of 3D space or distance fields). Our experiments have\ndemonstrated 3DRIMR's effectiveness in reconstructing 3D objects, and its\nperformance improvement over standard techniques.",
          "link": "http://arxiv.org/abs/2108.02858",
          "publishedOn": "2021-08-09T00:49:26.962Z",
          "wordCount": 642,
          "title": "3DRIMR: 3D Reconstruction and Imaging via mmWave Radar based on Deep Learning. (arXiv:2108.02858v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fontinele_J/0/1/0/all/0/1\">Jefferson Fontinele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lefundes_G/0/1/0/all/0/1\">Gabriel Lefundes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_L/0/1/0/all/0/1\">Luciano Oliveira</a>",
          "description": "This paper introduces a method for image semantic segmentation grounded on a\nnovel fusion scheme, which takes place inside a deep convolutional neural\nnetwork. The main goal of our proposal is to explore object boundary\ninformation to improve the overall segmentation performance. Unlike previous\nworks that combine boundary and segmentation features, or those that use\nboundary information to regularize semantic segmentation, we instead propose a\nnovel approach that embodies boundary information onto segmentation. For that,\nour semantic segmentation method uses two streams, which are combined through\nan attention gate, forming an end-to-end Y-model. To the best of our knowledge,\nours is the first work to show that boundary detection can improve semantic\nsegmentation when fused through a semantic fusion gate (attention model). We\nperformed an extensive evaluation of our method over public data sets. We found\ncompetitive results on all data sets after comparing our proposed model with\nother twelve state-of-the-art segmenters, considering the same training\nconditions. Our proposed model achieved the best mIoU on the CityScapes,\nCamVid, and Pascal Context data sets, and the second best on Mapillary Vistas.",
          "link": "http://arxiv.org/abs/2108.02840",
          "publishedOn": "2021-08-09T00:49:26.954Z",
          "wordCount": 621,
          "title": "Attention-based fusion of semantic boundary and non-boundary information to improve semantic segmentation. (arXiv:2108.02840v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jile Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengsen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yihang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xuetao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Ling-Yu Duan</a>",
          "description": "Visual retrieval system faces frequent model update and deployment. It is a\nheavy workload to re-extract features of the whole database every time.Feature\ncompatibility enables the learned new visual features to be directly compared\nwith the old features stored in the database. In this way, when updating the\ndeployed model, we can bypass the inflexible and time-consuming feature\nre-extraction process. However, the old feature space that needs to be\ncompatible is not ideal and faces the distribution discrepancy problem with the\nnew space caused by different supervision losses. In this work, we propose a\nglobal optimization Dual-Tuning method to obtain feature compatibility against\ndifferent networks and losses. A feature-level prototype loss is proposed to\nexplicitly align two types of embedding features, by transferring global\nprototype information. Furthermore, we design a component-level mutual\nstructural regularization to implicitly optimize the feature intrinsic\nstructure. Experimental results on million-scale datasets demonstrate that our\nDual-Tuning is able to obtain feature compatibility without sacrificing\nperformance. (Our code will be avaliable at\nhttps://github.com/yanbai1993/Dual-Tuning)",
          "link": "http://arxiv.org/abs/2108.02959",
          "publishedOn": "2021-08-09T00:49:26.935Z",
          "wordCount": 616,
          "title": "Dual-Tuning: Joint Prototype Transfer and Structure Regularization for Compatible Feature Learning. (arXiv:2108.02959v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wanqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming Yang</a>",
          "description": "This paper investigates a valuable setting called few-shot unsupervised\ndomain adaptation (FS-UDA), which has not been sufficiently studied in the\nliterature. In this setting, the source domain data are labelled, but with\nfew-shot per category, while the target domain data are unlabelled. To address\nthe FS-UDA setting, we develop a general UDA model to solve the following two\nkey issues: the few-shot labeled data per category and the domain adaptation\nbetween support and query sets. Our model is general in that once trained it\nwill be able to be applied to various FS-UDA tasks from the same source and\ntarget domains. Inspired by the recent local descriptor based few-shot learning\n(FSL), our general UDA model is fully built upon local descriptors (LDs) for\nimage classification and domain adaptation. By proposing a novel concept called\nsimilarity patterns (SPs), our model not only effectively considers the spatial\nrelationship of LDs that was ignored in previous FSL methods, but also makes\nthe learned image similarity better serve the required domain alignment.\nSpecifically, we propose a novel IMage-to-class sparse Similarity Encoding\n(IMSE) method. It learns SPs to extract the local discriminative information\nfor classification and meanwhile aligns the covariance matrix of the SPs for\ndomain adaptation. Also, domain adversarial training and multi-scale local\nfeature matching are performed upon LDs. Extensive experiments conducted on a\nmulti-domain benchmark dataset DomainNet demonstrates the state-of-the-art\nperformance of our IMSE for the novel setting of FS-UDA. In addition, for FSL,\nour IMSE can also show better performance than most of recent FSL methods on\nminiImageNet.",
          "link": "http://arxiv.org/abs/2108.02953",
          "publishedOn": "2021-08-09T00:49:26.927Z",
          "wordCount": 713,
          "title": "Few-shot Unsupervised Domain Adaptation with Image-to-class Sparse Similarity Encoding. (arXiv:2108.02953v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanjie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongmin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>",
          "description": "Existing domain adaptation methods for crowd counting view each crowd image\nas a whole and reduce domain discrepancies on crowds and backgrounds\nsimultaneously. However, we argue that these methods are suboptimal, as crowds\nand backgrounds have quite different characteristics and backgrounds may vary\ndramatically in different crowd scenes (see Fig.~\\ref{teaser}). This makes\ncrowds not well aligned across domains together with backgrounds in a holistic\nmanner. To this end, we propose to untangle crowds and backgrounds from crowd\nimages and design fine-grained domain adaption methods for crowd counting.\nDifferent from other tasks which possess region-based fine-grained annotations\n(e.g., segments or bounding boxes), crowd counting only annotates one point on\neach human head, which impedes the implementation of fine-grained adaptation\nmethods. To tackle this issue, we propose a novel and effective schema to learn\ncrowd segmentation from point-level crowd counting annotations in the context\nof Multiple Instance Learning. We further leverage the derived segments to\npropose a crowd-aware fine-grained domain adaptation framework for crowd\ncounting, which consists of two novel adaptation modules, i.e., Crowd Region\nTransfer (CRT) and Crowd Density Alignment (CDA). Specifically, the CRT module\nis designed to guide crowd features transfer across domains beyond background\ndistractions, and the CDA module dedicates to constraining the target-domain\ncrowd density distributions. Extensive experiments on multiple cross-domain\nsettings (i.e., Synthetic $\\rightarrow$ Real, Fixed $\\rightarrow$ Fickle,\nNormal $\\rightarrow$ BadWeather) demonstrate the superiority of the proposed\nmethod compared with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.02980",
          "publishedOn": "2021-08-09T00:49:26.920Z",
          "wordCount": 679,
          "title": "Fine-grained Domain Adaptive Crowd Counting via Point-derived Segmentation. (arXiv:2108.02980v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sambaturu_B/0/1/0/all/0/1\">Bhavani Sambaturu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ashutosh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C.V. Jawahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1\">Chetan Arora</a>",
          "description": "Semantic segmentation of medical images is an essential first step in\ncomputer-aided diagnosis systems for many applications. However, given many\ndisparate imaging modalities and inherent variations in the patient data, it is\ndifficult to consistently achieve high accuracy using modern deep neural\nnetworks (DNNs). This has led researchers to propose interactive image\nsegmentation techniques where a medical expert can interactively correct the\noutput of a DNN to the desired accuracy. However, these techniques often need\nseparate training data with the associated human interactions, and do not\ngeneralize to various diseases, and types of medical images. In this paper, we\nsuggest a novel conditional inference technique for DNNs which takes the\nintervention by a medical expert as test time constraints and performs\ninference conditioned upon these constraints. Our technique is generic can be\nused for medical images from any modality. Unlike other methods, our approach\ncan correct multiple structures simultaneously and add structures missed at\ninitial segmentation. We report an improvement of 13.3, 12.5, 17.8, 10.2, and\n12.4 times in user annotation time than full human annotation for the nucleus,\nmultiple cells, liver and tumor, organ, and brain segmentation respectively. We\nreport a time saving of 2.8, 3.0, 1.9, 4.4, and 8.6 fold compared to other\ninteractive segmentation techniques. Our method can be useful to clinicians for\ndiagnosis and post-surgical follow-up with minimal intervention from the\nmedical expert. The source-code and the detailed results are available here\n[1].",
          "link": "http://arxiv.org/abs/2108.02996",
          "publishedOn": "2021-08-09T00:49:26.910Z",
          "wordCount": 705,
          "title": "Efficient and Generic Interactive Segmentation Framework to Correct Mispredictions during Clinical Evaluation of Medical Images. (arXiv:2108.02996v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sandhini Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krueger_G/0/1/0/all/0/1\">Gretchen Krueger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jack Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radford_A/0/1/0/all/0/1\">Alec Radford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong Wook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brundage_M/0/1/0/all/0/1\">Miles Brundage</a>",
          "description": "Recently, there have been breakthroughs in computer vision (\"CV\") models that\nare more generalizable with the advent of models such as CLIP and ALIGN. In\nthis paper, we analyze CLIP and highlight some of the challenges such models\npose. CLIP reduces the need for task specific training data, potentially\nopening up many niche tasks to automation. CLIP also allows its users to\nflexibly specify image classification classes in natural language, which we\nfind can shift how biases manifest. Additionally, through some preliminary\nprobes we find that CLIP can inherit biases found in prior computer vision\nsystems. Given the wide and unpredictable domain of uses for such models, this\nraises questions regarding what sufficiently safe behaviour for such systems\nmay look like. These results add evidence to the growing body of work calling\nfor a change in the notion of a 'better' model--to move beyond simply looking\nat higher accuracy at task-oriented capability evaluations, and towards a\nbroader 'better' that takes into account deployment-critical features such as\ndifferent use contexts, and people who interact with the model when thinking\nabout model deployment.",
          "link": "http://arxiv.org/abs/2108.02818",
          "publishedOn": "2021-08-09T00:49:26.901Z",
          "wordCount": 643,
          "title": "Evaluating CLIP: Towards Characterization of Broader Capabilities and Downstream Implications. (arXiv:2108.02818v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thuan Trong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thuan Q. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1\">Dung Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Ngoc Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_N/0/1/0/all/0/1\">Nguyen D. Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khang Nguyen</a>",
          "description": "Vietnam is such an attractive tourist destination with its stunning and\npristine landscapes and its top-rated unique food and drink. Among thousands of\nVietnamese dishes, foreigners and native people are interested in easy-to-eat\ntastes and easy-to-do recipes, along with reasonable prices, mouthwatering\nflavors, and popularity. Due to the diversity and almost all the dishes have\nsignificant similarities and the lack of quality Vietnamese food datasets, it\nis hard to implement an auto system to classify Vietnamese food, therefore,\nmake people easier to discover Vietnamese food. This paper introduces a new\nVietnamese food dataset named VinaFood21, which consists of 13,950 images\ncorresponding to 21 dishes. We use 10,044 images for model training and 6,682\ntest images to classify each food in the VinaFood21 dataset and achieved an\naverage accuracy of 74.81% when fine-tuning CNN EfficientNet-B0.\n(https://github.com/nguyenvd-uit/uit-together-dataset)",
          "link": "http://arxiv.org/abs/2108.02929",
          "publishedOn": "2021-08-09T00:49:26.874Z",
          "wordCount": 586,
          "title": "VinaFood21: A Novel Dataset for Evaluating Vietnamese Food Recognition. (arXiv:2108.02929v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02923",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yulin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuxi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yuchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiameng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Kun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>",
          "description": "Structured text understanding on Visually Rich Documents (VRDs) is a crucial\npart of Document Intelligence. Due to the complexity of content and layout in\nVRDs, structured text understanding has been a challenging task. Most existing\nstudies decoupled this problem into two sub-tasks: entity labeling and entity\nlinking, which require an entire understanding of the context of documents at\nboth token and segment levels. However, little work has been concerned with the\nsolutions that efficiently extract the structured data from different levels.\nThis paper proposes a unified framework named StrucTexT, which is flexible and\neffective for handling both sub-tasks. Specifically, based on the transformer,\nwe introduce a segment-token aligned encoder to deal with the entity labeling\nand entity linking tasks at different levels of granularity. Moreover, we\ndesign a novel pre-training strategy with three self-supervised tasks to learn\na richer representation. StrucTexT uses the existing Masked Visual Language\nModeling task and the new Sentence Length Prediction and Paired Boxes Direction\ntasks to incorporate the multi-modal information across text, image, and\nlayout. We evaluate our method for structured text understanding at\nsegment-level and token-level and show it outperforms the state-of-the-art\ncounterparts with significantly superior performance on the FUNSD, SROIE, and\nEPHOIE datasets.",
          "link": "http://arxiv.org/abs/2108.02923",
          "publishedOn": "2021-08-09T00:49:26.844Z",
          "wordCount": 654,
          "title": "StrucTexT: Structured Text Understanding with Multi-Modal Transformers. (arXiv:2108.02923v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>",
          "description": "The growing number of action classes has posed a new challenge for video\nunderstanding, making Zero-Shot Action Recognition (ZSAR) a thriving direction.\nThe ZSAR task aims to recognize target (unseen) actions without training\nexamples by leveraging semantic representations to bridge seen and unseen\nactions. However, due to the complexity and diversity of actions, it remains\nchallenging to semantically represent action classes and transfer knowledge\nfrom seen data. In this work, we propose an ER-enhanced ZSAR model inspired by\nan effective human memory technique Elaborative Rehearsal (ER), which involves\nelaborating a new concept and relating it to known concepts. Specifically, we\nexpand each action class as an Elaborative Description (ED) sentence, which is\nmore discriminative than a class name and less costly than manual-defined\nattributes. Besides directly aligning class semantics with videos, we\nincorporate objects from the video as Elaborative Concepts (EC) to improve\nvideo semantics and generalization from seen actions to unseen actions. Our\nER-enhanced ZSAR model achieves state-of-the-art results on three existing\nbenchmarks. Moreover, we propose a new ZSAR evaluation protocol on the Kinetics\ndataset to overcome limitations of current benchmarks and demonstrate the first\ncase where ZSAR performance is comparable to few-shot learning baselines on\nthis more realistic setting. We will release our codes and collected EDs at\nhttps://github.com/DeLightCMU/ElaborativeRehearsal.",
          "link": "http://arxiv.org/abs/2108.02833",
          "publishedOn": "2021-08-09T00:49:26.836Z",
          "wordCount": 645,
          "title": "Elaborative Rehearsal for Zero-shot Action Recognition. (arXiv:2108.02833v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jindi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kejie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaohua Jia</a>",
          "description": "In recent years, many deep learning models have been adopted in autonomous\ndriving. At the same time, these models introduce new vulnerabilities that may\ncompromise the safety of autonomous vehicles. Specifically, recent studies have\ndemonstrated that adversarial attacks can cause a significant decline in\ndetection precision of deep learning-based 3D object detection models. Although\ndriving safety is the ultimate concern for autonomous driving, there is no\ncomprehensive study on the linkage between the performance of deep learning\nmodels and the driving safety of autonomous vehicles under adversarial attacks.\nIn this paper, we investigate the impact of two primary types of adversarial\nattacks, perturbation attacks and patch attacks, on the driving safety of\nvision-based autonomous vehicles rather than the detection precision of deep\nlearning models. In particular, we consider two state-of-the-art models in\nvision-based 3D object detection, Stereo R-CNN and DSGN. To evaluate driving\nsafety, we propose an end-to-end evaluation framework with a set of driving\nsafety performance metrics. By analyzing the results of our extensive\nevaluation experiments, we find that (1) the attack's impact on the driving\nsafety of autonomous vehicles and the attack's impact on the precision of 3D\nobject detectors are decoupled, and (2) the DSGN model demonstrates stronger\nrobustness to adversarial attacks than the Stereo R-CNN model. In addition, we\nfurther investigate the causes behind the two findings with an ablation study.\nThe findings of this paper provide a new perspective to evaluate adversarial\nattacks and guide the selection of deep learning models in autonomous driving.",
          "link": "http://arxiv.org/abs/2108.02940",
          "publishedOn": "2021-08-09T00:49:26.809Z",
          "wordCount": 695,
          "title": "Evaluating Adversarial Attacks on Driving Safety in Vision-Based Autonomous Vehicles. (arXiv:2108.02940v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_F/0/1/0/all/0/1\">Fengchun Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>",
          "description": "We study a worst-case scenario in generalization: Out-of-domain\ngeneralization from a single source. The goal is to learn a robust model from a\nsingle source and expect it to generalize over many unknown distributions. This\nchallenging problem has been seldom investigated while existing solutions\nsuffer from various limitations such as the ignorance of uncertainty assessment\nand label augmentation. In this paper, we propose uncertainty-guided domain\ngeneralization to tackle the aforementioned limitations. The key idea is to\naugment the source capacity in both feature and label spaces, while the\naugmentation is guided by uncertainty assessment. To the best of our knowledge,\nthis is the first work to (1) quantify the generalization uncertainty from a\nsingle source and (2) leverage it to guide both feature and label augmentation\nfor robust generalization. The model training and deployment are effectively\norganized in a Bayesian meta-learning framework. We conduct extensive\ncomparisons and ablation study to validate our approach. The results prove our\nsuperior performance in a wide scope of tasks including image classification,\nsemantic segmentation, text classification, and speech recognition.",
          "link": "http://arxiv.org/abs/2108.02888",
          "publishedOn": "2021-08-09T00:49:26.797Z",
          "wordCount": 629,
          "title": "Out-of-domain Generalization from a Single Source: A Uncertainty Quantification Approach. (arXiv:2108.02888v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Ken C. L. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashyap_S/0/1/0/all/0/1\">Satyananda Kashyap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Mehdi Moradi</a>",
          "description": "Transfer learning allows the reuse of deep learning features on new datasets\nwith limited data. However, the resulting models could be unnecessarily large\nand thus inefficient. Although network pruning can be applied to improve\ninference efficiency, existing algorithms usually require fine-tuning and may\nnot be suitable for small datasets. In this paper, we propose an algorithm that\ntransforms the convolutional weights into the subspaces of orthonormal bases\nwhere a model is pruned. Using singular value decomposition, we decompose a\nconvolutional layer into two layers: a convolutional layer with the orthonormal\nbasis vectors as the filters, and a layer that we name \"BasisScalingConv\",\nwhich is responsible for rescaling the features and transforming them back to\nthe original space. As the filters in each transformed layer are linearly\nindependent with known relative importance, pruning can be more effective and\nstable, and fine tuning individual weights is unnecessary. Furthermore, as the\nnumbers of input and output channels of the original convolutional layer remain\nunchanged, basis pruning is applicable to virtually all network architectures.\nBasis pruning can also be combined with existing pruning algorithms for double\npruning to further increase the pruning capability. With less than 1% reduction\nin the classification accuracy, we can achieve pruning ratios up to 98.9% in\nparameters and 98.6% in FLOPs.",
          "link": "http://arxiv.org/abs/2108.02893",
          "publishedOn": "2021-08-09T00:49:26.790Z",
          "wordCount": 652,
          "title": "Basis Scaling and Double Pruning for Efficient Transfer Learning. (arXiv:2108.02893v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1\">Miao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Baorong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xuetong Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jizhou Huang</a>",
          "description": "Image Retrieval is a fundamental task of obtaining images similar to the\nquery one from a database. A common image retrieval practice is to firstly\nretrieve candidate images via similarity search using global image features and\nthen re-rank the candidates by leveraging their local features. Previous\nlearning-based studies mainly focus on either global or local image\nrepresentation learning to tackle the retrieval task. In this paper, we abandon\nthe two-stage paradigm and seek to design an effective single-stage solution by\nintegrating local and global information inside images into compact image\nrepresentations. Specifically, we propose a Deep Orthogonal Local and Global\n(DOLG) information fusion framework for end-to-end image retrieval. It\nattentively extracts representative local information with multi-atrous\nconvolutions and self-attention at first. Components orthogonal to the global\nimage representation are then extracted from the local information. At last,\nthe orthogonal components are concatenated with the global representation as a\ncomplementary, and then aggregation is performed to generate the final\nrepresentation. The whole framework is end-to-end differentiable and can be\ntrained with image-level labels. Extensive experimental results validate the\neffectiveness of our solution and show that our model achieves state-of-the-art\nimage retrieval performances on Revisited Oxford and Paris datasets.",
          "link": "http://arxiv.org/abs/2108.02927",
          "publishedOn": "2021-08-09T00:49:26.782Z",
          "wordCount": 653,
          "title": "DOLG: Single-Stage Image Retrieval with Deep Orthogonal Fusion of Local and Global Features. (arXiv:2108.02927v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02832",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1\">Akash Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jonnalagedda_P/0/1/0/all/0/1\">Padmaja Jonnalagedda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhanu_B/0/1/0/all/0/1\">Bir Bhanu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>",
          "description": "Most of the existing works in supervised spatio-temporal video\nsuper-resolution (STVSR) heavily rely on a large-scale external dataset\nconsisting of paired low-resolution low-frame rate (LR-LFR)and high-resolution\nhigh-frame-rate (HR-HFR) videos. Despite their remarkable performance, these\nmethods make a prior assumption that the low-resolution video is obtained by\ndown-scaling the high-resolution video using a known degradation kernel, which\ndoes not hold in practical settings. Another problem with these methods is that\nthey cannot exploit instance-specific internal information of video at testing\ntime. Recently, deep internal learning approaches have gained attention due to\ntheir ability to utilize the instance-specific statistics of a video. However,\nthese methods have a large inference time as they require thousands of gradient\nupdates to learn the intrinsic structure of the data. In this work, we\npresentAdaptiveVideoSuper-Resolution (Ada-VSR) which leverages external, as\nwell as internal, information through meta-transfer learning and internal\nlearning, respectively. Specifically, meta-learning is employed to obtain\nadaptive parameters, using a large-scale external dataset, that can adapt\nquickly to the novel condition (degradation model) of the given test video\nduring the internal learning task, thereby exploiting external and internal\ninformation of a video for super-resolution. The model trained using our\napproach can quickly adapt to a specific video condition with only a few\ngradient updates, which reduces the inference time significantly. Extensive\nexperiments on standard datasets demonstrate that our method performs favorably\nagainst various state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.02832",
          "publishedOn": "2021-08-09T00:49:26.762Z",
          "wordCount": 670,
          "title": "Ada-VSR: Adaptive Video Super-Resolution with Meta-Learning. (arXiv:2108.02832v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Sen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wentong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Michael Ying Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>",
          "description": "A lifespan face synthesis (LFS) model aims to generate a set of\nphoto-realistic face images of a person's whole life, given only one snapshot\nas reference. The generated face image given a target age code is expected to\nbe age-sensitive reflected by bio-plausible transformations of shape and\ntexture, while being identity preserving. This is extremely challenging because\nthe shape and texture characteristics of a face undergo separate and highly\nnonlinear transformations w.r.t. age. Most recent LFS models are based on\ngenerative adversarial networks (GANs) whereby age code conditional\ntransformations are applied to a latent face representation. They benefit\ngreatly from the recent advancements of GANs. However, without explicitly\ndisentangling their latent representations into the texture, shape and identity\nfactors, they are fundamentally limited in modeling the nonlinear age-related\ntransformation on texture and shape whilst preserving identity. In this work, a\nnovel LFS model is proposed to disentangle the key face characteristics\nincluding shape, texture and identity so that the unique shape and texture age\ntransformations can be modeled effectively. This is achieved by extracting\nshape, texture and identity features separately from an encoder. Critically,\ntwo transformation modules, one conditional convolution based and the other\nchannel attention based, are designed for modeling the nonlinear shape and\ntexture feature transformations respectively. This is to accommodate their\nrather distinct aging processes and ensure that our synthesized images are both\nage-sensitive and identity preserving. Extensive experiments show that our LFS\nmodel is clearly superior to the state-of-the-art alternatives. Codes and demo\nare available on our project website:\n\\url{https://senhe.github.io/projects/iccv_2021_lifespan_face}.",
          "link": "http://arxiv.org/abs/2108.02874",
          "publishedOn": "2021-08-09T00:49:26.755Z",
          "wordCount": 694,
          "title": "Disentangled Lifespan Face Synthesis. (arXiv:2108.02874v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1\">Shagufta Henna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reji_A/0/1/0/all/0/1\">Aparna Reji</a>",
          "description": "Covid-19 detection at an early stage can aid in an effective treatment and\nisolation plan to prevent its spread. Recently, transfer learning has been used\nfor Covid-19 detection using X-ray, ultrasound, and CT scans. One of the major\nlimitations inherent to these proposed methods is limited labeled dataset size\nthat affects the reliability of Covid-19 diagnosis and disease progression. In\nthis work, we demonstrate that how we can augment limited X-ray images data by\nusing Contrast limited adaptive histogram equalization (CLAHE) to train the\nlast layer of the pre-trained deep learning models to mitigate the bias of\ntransfer learning for Covid-19 detection. We transfer learned various\npre-trained deep learning models including AlexNet, ZFNet, VGG-16, ResNet-18,\nand GoogLeNet, and fine-tune the last layer by using CLAHE-augmented dataset.\nThe experiment results reveal that the CLAHE-based augmentation to various\npre-trained deep learning models significantly improves the model efficiency.\nThe pre-trained VCG-16 model with CLAHEbased augmented images achieves a\nsensitivity of 95% using 15 epochs. AlexNet works show good sensitivity when\ntrained on non-augmented data. Other models demonstrate a value of less than\n60% when trained on non-augmented data. Our results reveal that the sample bias\ncan negatively impact the performance of transfer learning which is\nsignificantly improved by using CLAHE-based augmentation.",
          "link": "http://arxiv.org/abs/2108.02870",
          "publishedOn": "2021-08-09T00:49:26.704Z",
          "wordCount": 692,
          "title": "A Data Augmented Approach to Transfer Learning for Covid-19 Detection. (arXiv:2108.02870v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ningli Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Debao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1\">Xiao Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strasbaugh_C/0/1/0/all/0/1\">Chris Strasbaugh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_A/0/1/0/all/0/1\">Alper Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sezen_H/0/1/0/all/0/1\">Halil Sezen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>",
          "description": "In this paper, we present a case study that performs an unmanned aerial\nvehicle (UAV) based fine-scale 3D change detection and monitoring of\nprogressive collapse performance of a building during a demolition event.\nMulti-temporal oblique photogrammetry images are collected with 3D point clouds\ngenerated at different stages of the demolition. The geometric accuracy of the\ngenerated point clouds has been evaluated against both airborne and terrestrial\nLiDAR point clouds, achieving an average distance of 12 cm and 16 cm for roof\nand facade respectively. We propose a hierarchical volumetric change detection\nframework that unifies multi-temporal UAV images for pose estimation (free of\nground control points), reconstruction, and a coarse-to-fine 3D density change\nanalysis. This work has provided a solution capable of addressing change\ndetection on full 3D time-series datasets where dramatic scene content changes\nare presented progressively. Our change detection results on the building\ndemolition event have been evaluated against the manually marked ground-truth\nchanges and have achieved an F-1 score varying from 0.78 to 0.92, with\nconsistently high precision (0.92 - 0.99). Volumetric changes through the\ndemolition progress are derived from change detection and have shown to\nfavorably reflect the qualitative and quantitative building demolition\nprogression.",
          "link": "http://arxiv.org/abs/2108.02800",
          "publishedOn": "2021-08-09T00:49:26.684Z",
          "wordCount": 669,
          "title": "A volumetric change detection framework using UAV oblique photogrammetry - A case study of ultra-high-resolution monitoring of progressive building collapse. (arXiv:2108.02800v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kukacka_J/0/1/0/all/0/1\">Jan Kuka&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zenz_A/0/1/0/all/0/1\">Anja Zenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kollovieh_M/0/1/0/all/0/1\">Marcel Kollovieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Justel_D/0/1/0/all/0/1\">Dominik J&#xfc;stel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntziachristos_V/0/1/0/all/0/1\">Vasilis Ntziachristos</a>",
          "description": "Fundus photography is the primary method for retinal imaging and essential\nfor diabetic retinopathy prevention. Automated segmentation of fundus\nphotographs would improve the quality, capacity, and cost-effectiveness of eye\ncare screening programs. However, current segmentation methods are not robust\ntowards the diversity in imaging conditions and pathologies typical for\nreal-world clinical applications. To overcome these limitations, we utilized\ncontrastive self-supervised learning to exploit the large variety of unlabeled\nfundus images in the publicly available EyePACS dataset. We pre-trained an\nencoder of a U-Net, which we later fine-tuned on several retinal vessel and\nlesion segmentation datasets. We demonstrate for the first time that by using\ncontrastive self-supervised learning, the pre-trained network can recognize\nblood vessels, optic disc, fovea, and various lesions without being provided\nany labels. Furthermore, when fine-tuned on a downstream blood vessel\nsegmentation task, such pre-trained networks achieve state-of-the-art\nperformance on images from different datasets. Additionally, the pre-training\nalso leads to shorter training times and an improved few-shot performance on\nboth blood vessel and lesion segmentation tasks. Altogether, our results\nshowcase the benefits of contrastive self-supervised pre-training which can\nplay a crucial role in real-world clinical applications requiring robust models\nable to adapt to new devices with only a few annotated samples.",
          "link": "http://arxiv.org/abs/2108.02798",
          "publishedOn": "2021-08-09T00:49:26.664Z",
          "wordCount": 650,
          "title": "Self-Supervised Learning from Unlabeled Fundus Photographs Improves Segmentation of the Retina. (arXiv:2108.02798v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cheng-Ju Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>",
          "description": "Human-robot collaboration is an essential research topic in artificial\nintelligence (AI), enabling researchers to devise cognitive AI systems and\naffords an intuitive means for users to interact with the robot. Of note,\ncommunication plays a central role. To date, prior studies in embodied agent\nnavigation have only demonstrated that human languages facilitate communication\nby instructions in natural languages. Nevertheless, a plethora of other forms\nof communication is left unexplored. In fact, human communication originated in\ngestures and oftentimes is delivered through multimodal cues, e.g. \"go there\"\nwith a pointing gesture. To bridge the gap and fill in the missing dimension of\ncommunication in embodied agent navigation, we propose investigating the\neffects of using gestures as the communicative interface instead of verbal\ncues. Specifically, we develop a VR-based 3D simulation environment, named\nGes-THOR, based on AI2-THOR platform. In this virtual environment, a human\nplayer is placed in the same virtual scene and shepherds the artificial agent\nusing only gestures. The agent is tasked to solve the navigation problem guided\nby natural gestures with unknown semantics; we do not use any predefined\ngestures due to the diversity and versatile nature of human gestures. We argue\nthat learning the semantics of natural gestures is mutually beneficial to\nlearning the navigation task--learn to communicate and communicate to learn. In\na series of experiments, we demonstrate that human gesture cues, even without\npredefined semantics, improve the object-goal navigation for an embodied agent,\noutperforming various state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.02846",
          "publishedOn": "2021-08-09T00:49:26.621Z",
          "wordCount": 701,
          "title": "Communicative Learning with Natural Gestures for Embodied Navigation Agents with Human-in-the-Scene. (arXiv:2108.02846v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zohourianshahzadi_Z/0/1/0/all/0/1\">Zanyar Zohourianshahzadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1\">Jugal K. Kalita</a>",
          "description": "Inspired by how the human brain employs a higher number of neural pathways\nwhen describing a highly focused subject, we show that deep attentive models\nused for the main vision-language task of image captioning, could be extended\nto achieve better performance. Image captioning bridges a gap between computer\nvision and natural language processing. Automated image captioning is used as a\ntool to eliminate the need for human agent for creating descriptive captions\nfor unseen images.Automated image captioning is challenging and yet\ninteresting. One reason is that AI based systems capable of generating\nsentences that describe an input image could be used in a wide variety of tasks\nbeyond generating captions for unseen images found on web or uploaded to social\nmedia. For example, in biology and medical sciences, these systems could\nprovide researchers and physicians with a brief linguistic description of\nrelevant images, potentially expediting their work.",
          "link": "http://arxiv.org/abs/2108.02807",
          "publishedOn": "2021-08-09T00:49:26.609Z",
          "wordCount": 628,
          "title": "Neural Twins Talk & Alternative Calculations. (arXiv:2108.02807v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Shuquan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Songfang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>",
          "description": "Point cloud segmentation is a fundamental task in 3D. Despite recent progress\non point cloud segmentation with the power of deep networks, current deep\nlearning methods based on the clean label assumptions may fail with noisy\nlabels. Yet, object class labels are often mislabeled in real-world point cloud\ndatasets. In this work, we take the lead in solving this issue by proposing a\nnovel Point Noise-Adaptive Learning (PNAL) framework. Compared to existing\nnoise-robust methods on image tasks, our PNAL is noise-rate blind, to cope with\nthe spatially variant noise rate problem specific to point clouds.\nSpecifically, we propose a novel point-wise confidence selection to obtain\nreliable labels based on the historical predictions of each point. A novel\ncluster-wise label correction is proposed with a voting strategy to generate\nthe best possible label taking the neighbor point correlations into\nconsideration. We conduct extensive experiments to demonstrate the\neffectiveness of PNAL on both synthetic and real-world noisy datasets. In\nparticular, even with $60\\%$ symmetric noisy labels, our proposed method\nproduces much better results than its baseline counterpart without PNAL and is\ncomparable to the ideal upper bound trained on a completely clean dataset.\nMoreover, we fully re-labeled the validation set of a popular but noisy\nreal-world scene dataset ScanNetV2 to make it clean, for rigorous experiment\nand future research. Our code and data are available at\n\\url{https://shuquanye.com/PNAL_website/}.",
          "link": "http://arxiv.org/abs/2107.14230",
          "publishedOn": "2021-08-06T01:58:38.210Z",
          "wordCount": 701,
          "title": "Learning with Noisy Labels for Robust Point Cloud Segmentation. (arXiv:2107.14230v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02602",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Condat_L/0/1/0/all/0/1\">Laurent Condat</a>",
          "description": "It is common to have to process signals or images whose values are cyclic and\ncan be represented as points on the complex circle, like wrapped phases,\nangles, orientations, or color hues. We consider a Tikhonov-type regularization\nmodel to smoothen or interpolate circle-valued signals defined on arbitrary\ngraphs. We propose a convex relaxation of this nonconvex problem as a\nsemidefinite program, and an efficient algorithm to solve it.",
          "link": "http://arxiv.org/abs/2108.02602",
          "publishedOn": "2021-08-06T00:51:47.662Z",
          "wordCount": 501,
          "title": "Tikhonov Regularization of Circle-Valued Signals. (arXiv:2108.02602v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1\">Shahroz Tariq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Simon S. Woo</a>",
          "description": "Over the last few decades, artificial intelligence research has made\ntremendous strides, but it still heavily relies on fixed datasets in stationary\nenvironments. Continual learning is a growing field of research that examines\nhow AI systems can learn sequentially from a continuous stream of linked data\nin the same way that biological systems do. Simultaneously, fake media such as\ndeepfakes and synthetic face images have emerged as significant to current\nmultimedia technologies. Recently, numerous method has been proposed which can\ndetect deepfakes with high accuracy. However, they suffer significantly due to\ntheir reliance on fixed datasets in limited evaluation settings. Therefore, in\nthis work, we apply continuous learning to neural networks' learning dynamics,\nemphasizing its potential to increase data efficiency significantly. We propose\nContinual Representation using Distillation (CoReD) method that employs the\nconcept of Continual Learning (CL), Representation Learning (RL), and Knowledge\nDistillation (KD). We design CoReD to perform sequential domain adaptation\ntasks on new deepfake and GAN-generated synthetic face datasets, while\neffectively minimizing the catastrophic forgetting in a teacher-student model\nsetting. Our extensive experimental results demonstrate that our method is\nefficient at domain adaptation to detect low-quality deepfakes videos and\nGAN-generated images from several datasets, outperforming the-state-of-art\nbaseline methods.",
          "link": "http://arxiv.org/abs/2107.02408",
          "publishedOn": "2021-08-06T00:51:47.654Z",
          "wordCount": 710,
          "title": "CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation. (arXiv:2107.02408v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1\">Juan P. Vigueras-Guill&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1\">Frank Seeliger</a>",
          "description": "Regularization in convolutional neural networks (CNNs) is usually addressed\nwith dropout layers. However, dropout is sometimes detrimental in the\nconvolutional part of a CNN as it simply sets to zero a percentage of pixels in\nthe feature maps, adding unrepresentative examples during training. Here, we\npropose a CNN layer that performs regularization by applying random rotations\nof reflections to a small percentage of feature maps after every convolutional\nlayer. We prove how this concept is beneficial for images with orientational\nsymmetries, such as in medical images, as it provides a certain degree of\nrotational invariance. We tested this method in two datasets, a patch-based set\nof histopathology images (PatchCamelyon) to perform classification using a\ngeneric DenseNet, and a set of specular microscopy images of the corneal\nendothelium to perform segmentation using a tailored U-net, improving the\nperformance in both cases.",
          "link": "http://arxiv.org/abs/2108.02704",
          "publishedOn": "2021-08-06T00:51:47.648Z",
          "wordCount": 615,
          "title": "Rotaflip: A New CNN Layer for Regularization and Rotational Invariance in Medical Images. (arXiv:2108.02704v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.13611",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Mu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Huijuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Q/0/1/0/all/0/1\">Qichuan Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>",
          "description": "Image-to-image translation has been revolutionized with GAN-based methods.\nHowever, existing methods lack the ability to preserve the identity of the\nsource domain. As a result, synthesized images can often over-adapt to the\nreference domain, losing important structural characteristics and suffering\nfrom suboptimal visual quality. To solve these challenges, we propose a novel\nfrequency domain image translation (FDIT) framework, exploiting frequency\ninformation for enhancing the image generation process. Our key idea is to\ndecompose the image into low-frequency and high-frequency components, where the\nhigh-frequency feature captures object structure akin to the identity. Our\ntraining objective facilitates the preservation of frequency information in\nboth pixel space and Fourier spectral space. We broadly evaluate FDIT across\nfive large-scale datasets and multiple tasks including image translation and\nGAN inversion. Extensive experiments and ablations show that FDIT effectively\npreserves the identity of the source image, and produces photo-realistic\nimages. FDIT establishes state-of-the-art performance, reducing the average FID\nscore by 5.6% compared to the previous best method.",
          "link": "http://arxiv.org/abs/2011.13611",
          "publishedOn": "2021-08-06T00:51:47.641Z",
          "wordCount": 641,
          "title": "Frequency Domain Image Translation: More Photo-realistic, Better Identity-preserving. (arXiv:2011.13611v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hornauer_J/0/1/0/all/0/1\">Julia Hornauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nalpantidis_L/0/1/0/all/0/1\">Lazaros Nalpantidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>",
          "description": "Real-world perception systems in many cases build on hardware with limited\nresources to adhere to cost and power limitations of their carrying system.\nDeploying deep neural networks on resource-constrained hardware became possible\nwith model compression techniques, as well as efficient and hardware-aware\narchitecture design. However, model adaptation is additionally required due to\nthe diverse operation environments. In this work, we address the problem of\ntraining deep neural networks on resource-constrained hardware in the context\nof visual domain adaptation. We select the task of monocular depth estimation\nwhere our goal is to transform a pre-trained model to the target's domain data.\nWhile the source domain includes labels, we assume an unlabelled target domain,\nas it happens in real-world applications. Then, we present an adversarial\nlearning approach that is adapted for training on the device with limited\nresources. Since visual domain adaptation, i.e. neural network training, has\nnot been previously explored for resource-constrained hardware, we present the\nfirst feasibility study for image-based depth estimation. Our experiments show\nthat visual domain adaptation is relevant only for efficient network\narchitectures and training sets at the order of a few hundred samples. Models\nand code are publicly available.",
          "link": "http://arxiv.org/abs/2108.02671",
          "publishedOn": "2021-08-06T00:51:47.635Z",
          "wordCount": 650,
          "title": "Visual Domain Adaptation for Monocular Depth Estimation on Resource-Constrained Hardware. (arXiv:2108.02671v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Rongchang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>",
          "description": "Semi-supervised learning aims to boost the accuracy of a model by exploring\nunlabeled images. The state-of-the-art methods are consistency-based which\nlearn about unlabeled images by encouraging the model to give consistent\npredictions for images under different augmentations. However, when applied to\npose estimation, the methods degenerate and predict every pixel in unlabeled\nimages as background. This is because contradictory predictions are gradually\npushed to the background class due to highly imbalanced class distribution. But\nthis is not an issue in supervised learning because it has accurate labels.\nThis inspires us to stabilize the training by obtaining reliable pseudo labels.\nSpecifically, we learn two networks to mutually teach each other. In\nparticular, for each image, we compose an easy-hard pair by applying different\naugmentations and feed them to both networks. The more reliable predictions on\neasy images in each network are used to teach the other network to learn about\nthe corresponding hard images. The approach successfully avoids degeneration\nand achieves promising results on public datasets. The source code will be\nreleased.",
          "link": "http://arxiv.org/abs/2011.12498",
          "publishedOn": "2021-08-06T00:51:47.538Z",
          "wordCount": 646,
          "title": "An Empirical Study of the Collapsing Problem in Semi-Supervised 2D Human Pose Estimation. (arXiv:2011.12498v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samak_T/0/1/0/all/0/1\">Tanmay Vilas Samak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samak_C/0/1/0/all/0/1\">Chinmay Vilas Samak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandhasamy_S/0/1/0/all/0/1\">Sivanathan Kandhasamy</a>",
          "description": "In this work, we present a lightweight pipeline for robust behavioral cloning\nof a human driver using end-to-end imitation learning. The proposed pipeline\nwas employed to train and deploy three distinct driving behavior models onto a\nsimulated vehicle. The training phase comprised of data collection, balancing,\naugmentation, preprocessing and training a neural network, following which, the\ntrained model was deployed onto the ego vehicle to predict steering commands\nbased on the feed from an onboard camera. A novel coupled control law was\nformulated to generate longitudinal control commands on-the-go based on the\npredicted steering angle and other parameters such as actual speed of the ego\nvehicle and the prescribed constraints for speed and steering. We analyzed\ncomputational efficiency of the pipeline and evaluated robustness of the\ntrained models through exhaustive experimentation during the deployment phase.\nWe also compared our approach against state-of-the-art implementation in order\nto comment on its validity.",
          "link": "http://arxiv.org/abs/2010.04767",
          "publishedOn": "2021-08-06T00:51:47.406Z",
          "wordCount": 651,
          "title": "Robust Behavioral Cloning for Autonomous Vehicles using End-to-End Imitation Learning. (arXiv:2010.04767v4 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sanguesa_M/0/1/0/all/0/1\">Marta Girones Sanguesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutnar_D/0/1/0/all/0/1\">Denis Kutnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velden_B/0/1/0/all/0/1\">Bas H.M. van der Velden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijf_H/0/1/0/all/0/1\">Hugo J. Kuijf</a>",
          "description": "Cerebral microbleeds are small, dark, round lesions that can be visualised on\nT2*-weighted MRI or other sequences sensitive to susceptibility effects. In\nthis work, we propose a multi-stage approach to both microbleed detection and\nsegmentation. First, possible microbleed locations are detected with a Mask\nR-CNN technique. Second, at each possible microbleed location, a simple U-Net\nperforms the final segmentation. This work used the 72 subjects as training\ndata provided by the \"Where is VALDO?\" challenge of MICCAI 2021.",
          "link": "http://arxiv.org/abs/2108.02482",
          "publishedOn": "2021-08-06T00:51:47.386Z",
          "wordCount": 528,
          "title": "MixMicrobleed: Multi-stage detection and segmentation of cerebral microbleeds. (arXiv:2108.02482v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jingyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1\">Qi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1\">Hongyang Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "Unsupervised learning is just at a tipping point where it could really take\noff. Among these approaches, contrastive learning has seen tremendous progress\nand led to state-of-the-art performance. In this paper, we construct a novel\nprobabilistic graphical model that effectively incorporates the low rank\npromoting prior into the framework of contrastive learning, referred to as\nLORAC. In contrast to the existing conventional self-supervised approaches that\nonly considers independent learning, our hypothesis explicitly requires that\nall the samples belonging to the same instance class lie on the same subspace\nwith small dimension. This heuristic poses particular joint learning\nconstraints to reduce the degree of freedom of the problem during the search of\nthe optimal network parameterization. Most importantly, we argue that the low\nrank prior employed here is not unique, and many different priors can be\ninvoked in a similar probabilistic way, corresponding to different hypotheses\nabout underlying truth behind the contrastive features. Empirical evidences\nshow that the proposed algorithm clearly surpasses the state-of-the-art\napproaches on multiple benchmarks, including image classification, object\ndetection, instance segmentation and keypoint detection.",
          "link": "http://arxiv.org/abs/2108.02696",
          "publishedOn": "2021-08-06T00:51:47.359Z",
          "wordCount": 629,
          "title": "A Low Rank Promoting Prior for Unsupervised Contrastive Learning. (arXiv:2108.02696v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.02443",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pomponi_J/0/1/0/all/0/1\">Jary Pomponi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scardapane_S/0/1/0/all/0/1\">Simone Scardapane</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Uncini_A/0/1/0/all/0/1\">Aurelio Uncini</a>",
          "description": "Catastrophic forgetting (CF) happens whenever a neural network overwrites\npast knowledge while being trained on new tasks. Common techniques to handle CF\ninclude regularization of the weights (using, e.g., their importance on past\ntasks), and rehearsal strategies, where the network is constantly re-trained on\npast data. Generative models have also been applied for the latter, in order to\nhave endless sources of data. In this paper, we propose a novel method that\ncombines the strengths of regularization and generative-based rehearsal\napproaches. Our generative model consists of a normalizing flow (NF), a\nprobabilistic and invertible neural network, trained on the internal embeddings\nof the network. By keeping a single NF conditioned on the task, we show that\nour memory overhead remains constant. In addition, exploiting the invertibility\nof the NF, we propose a simple approach to regularize the network's embeddings\nwith respect to past tasks. We show that our method performs favorably with\nrespect to state-of-the-art approaches in the literature, with bounded\ncomputational power and memory overheads.",
          "link": "http://arxiv.org/abs/2007.02443",
          "publishedOn": "2021-08-06T00:51:47.351Z",
          "wordCount": 655,
          "title": "Pseudo-Rehearsal for Continual Learning with Normalizing Flows. (arXiv:2007.02443v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.03948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Q/0/1/0/all/0/1\">Qi Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_F/0/1/0/all/0/1\">Fuhong Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jun He</a>",
          "description": "Recently, convolutional neural networks (CNNs) have set latest\nstate-of-the-art on various human activity recognition (HAR) datasets. However,\ndeep CNNs often require more computing resources, which limits their\napplications in embedded HAR. Although many successful methods have been\nproposed to reduce memory and FLOPs of CNNs, they often involve special network\narchitectures designed for visual tasks, which are not suitable for deep HAR\ntasks with time series sensor signals, due to remarkable discrepancy.\nTherefore, it is necessary to develop lightweight deep models to perform HAR.\nAs filter is the basic unit in constructing CNNs, it deserves further research\nwhether re-designing smaller filters is applicable for deep HAR. In the paper,\ninspired by the idea, we proposed a lightweight CNN using Lego filters for HAR.\nA set of lower-dimensional filters is used as Lego bricks to be stacked for\nconventional filters, which does not rely on any special network structure. The\nlocal loss function is used to train model. To our knowledge, this is the first\npaper that proposes lightweight CNN for HAR in ubiquitous and wearable\ncomputing arena. The experiment results on five public HAR datasets, UCI-HAR\ndataset, OPPORTUNITY dataset, UNIMIB-SHAR dataset, PAMAP2 dataset, and WISDM\ndataset collected from either smartphones or multiple sensor nodes, indicate\nthat our novel Lego CNN with local loss can greatly reduce memory and\ncomputation cost over CNN, while achieving higher accuracy. That is to say, the\nproposed model is smaller, faster and more accurate. Finally, we evaluate the\nactual performance on an Android smartphone.",
          "link": "http://arxiv.org/abs/2005.03948",
          "publishedOn": "2021-08-06T00:51:47.336Z",
          "wordCount": 743,
          "title": "Layer-wise training convolutional neural networks with smaller filters for human activity recognition using wearable sensors. (arXiv:2005.03948v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Ke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>",
          "description": "Multi-label image recognition is a challenging computer vision task of\npractical use. Progresses in this area, however, are often characterized by\ncomplicated methods, heavy computations, and lack of intuitive explanations. To\neffectively capture different spatial regions occupied by objects from\ndifferent categories, we propose an embarrassingly simple module, named\nclass-specific residual attention (CSRA). CSRA generates class-specific\nfeatures for every category by proposing a simple spatial attention score, and\nthen combines it with the class-agnostic average pooling feature. CSRA achieves\nstate-of-the-art results on multilabel recognition, and at the same time is\nmuch simpler than them. Furthermore, with only 4 lines of code, CSRA also leads\nto consistent improvement across many diverse pretrained models and datasets\nwithout any extra training. CSRA is both easy to implement and light in\ncomputations, which also enjoys intuitive explanations and visualizations.",
          "link": "http://arxiv.org/abs/2108.02456",
          "publishedOn": "2021-08-06T00:51:47.323Z",
          "wordCount": 574,
          "title": "Residual Attention: A Simple but Effective Method for Multi-Label Recognition. (arXiv:2108.02456v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.06107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoman Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shixiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>",
          "description": "Automatic and accurate tumor segmentation on medical images is in high demand\nto assist physicians with diagnosis and treatment. However, it is difficult to\nobtain massive amounts of annotated training data required by the deep-learning\nmodels as the manual delineation process is often tedious and expertise\nrequired. Although self-supervised learning (SSL) scheme has been widely\nadopted to address this problem, most SSL methods focus only on global\nstructure information, ignoring the key distinguishing features of tumor\nregions: local intensity variation and large size distribution. In this paper,\nwe propose Scale-Aware Restoration (SAR), a SSL method for 3D tumor\nsegmentation. Specifically, a novel proxy task, i.e. scale discrimination, is\nformulated to pre-train the 3D neural network combined with the\nself-restoration task. Thus, the pre-trained model learns multi-level local\nrepresentations through multi-scale inputs. Moreover, an adversarial learning\nmodule is further introduced to learn modality invariant representations from\nmultiple unlabeled source datasets. We demonstrate the effectiveness of our\nmethods on two downstream tasks: i) Brain tumor segmentation, ii) Pancreas\ntumor segmentation. Compared with the state-of-the-art 3D SSL methods, our\nproposed approach can significantly improve the segmentation accuracy. Besides,\nwe analyze its advantages from multiple perspectives such as data efficiency,\nperformance, and convergence speed.",
          "link": "http://arxiv.org/abs/2010.06107",
          "publishedOn": "2021-08-06T00:51:47.316Z",
          "wordCount": 672,
          "title": "SAR: Scale-Aware Restoration Learning for 3D Tumor Segmentation. (arXiv:2010.06107v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>",
          "description": "We present VoxelTrack for multi-person 3D pose estimation and tracking from a\nfew cameras which are separated by wide baselines. It employs a multi-branch\nnetwork to jointly estimate 3D poses and re-identification (Re-ID) features for\nall people in the environment. In contrast to previous efforts which require to\nestablish cross-view correspondence based on noisy 2D pose estimates, it\ndirectly estimates and tracks 3D poses from a 3D voxel-based representation\nconstructed from multi-view images. We first discretize the 3D space by regular\nvoxels and compute a feature vector for each voxel by averaging the body joint\nheatmaps that are inversely projected from all views. We estimate 3D poses from\nthe voxel representation by predicting whether each voxel contains a particular\nbody joint. Similarly, a Re-ID feature is computed for each voxel which is used\nto track the estimated 3D poses over time. The main advantage of the approach\nis that it avoids making any hard decisions based on individual images. The\napproach can robustly estimate and track 3D poses even when people are severely\noccluded in some cameras. It outperforms the state-of-the-art methods by a\nlarge margin on three public datasets including Shelf, Campus and CMU Panoptic.",
          "link": "http://arxiv.org/abs/2108.02452",
          "publishedOn": "2021-08-06T00:51:47.276Z",
          "wordCount": 640,
          "title": "VoxelTrack: Multi-Person 3D Human Pose Estimation and Tracking in the Wild. (arXiv:2108.02452v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1912.05949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1\">Nathanael L. Baisa</a>",
          "description": "We propose a novel online multi-object visual tracker using a Gaussian\nmixture Probability Hypothesis Density (GM-PHD) filter and deep appearance\nlearning. The GM-PHD filter has a linear complexity with the number of objects\nand observations while estimating the states and cardinality of time-varying\nnumber of objects, however, it is susceptible to miss-detections and does not\ninclude the identity of objects. We use visual-spatio-temporal information\nobtained from object bounding boxes and deeply learned appearance\nrepresentations to perform estimates-to-tracks data association for target\nlabeling as well as formulate an augmented likelihood and then integrate into\nthe update step of the GM-PHD filter. We also employ additional unassigned\ntracks prediction after the data association step to overcome the\nsusceptibility of the GM-PHD filter towards miss-detections caused by\nocclusion. Extensive evaluations on MOT16, MOT17 and HiEve benchmark datasets\nshow that our tracker significantly outperforms several state-of-the-art\ntrackers in terms of tracking accuracy and identification.",
          "link": "http://arxiv.org/abs/1912.05949",
          "publishedOn": "2021-08-06T00:51:47.265Z",
          "wordCount": 661,
          "title": "Occlusion-Robust Online Multi-Object Visual Tracking using a GM-PHD Filter with CNN-Based Re-Identification. (arXiv:1912.05949v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02667",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shubao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke-Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Taiping Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_M/0/1/0/all/0/1\">Mingwei Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>",
          "description": "With various face presentation attacks arising under unseen scenarios, face\nanti-spoofing (FAS) based on domain generalization (DG) has drawn growing\nattention due to its robustness. Most existing methods utilize DG frameworks to\nalign the features to seek a compact and generalized feature space. However,\nlittle attention has been paid to the feature extraction process for the FAS\ntask, especially the influence of normalization, which also has a great impact\non the generalization of the learned representation. To address this issue, we\npropose a novel perspective of face anti-spoofing that focuses on the\nnormalization selection in the feature extraction process. Concretely, an\nAdaptive Normalized Representation Learning (ANRL) framework is devised, which\nadaptively selects feature normalization methods according to the inputs,\naiming to learn domain-agnostic and discriminative representation. Moreover, to\nfacilitate the representation learning, Dual Calibration Constraints are\ndesigned, including Inter-Domain Compatible loss and Inter-Class Separable\nloss, which provide a better optimization direction for generalizable\nrepresentation. Extensive experiments and visualizations are presented to\ndemonstrate the effectiveness of our method against the SOTA competitors.",
          "link": "http://arxiv.org/abs/2108.02667",
          "publishedOn": "2021-08-06T00:51:47.250Z",
          "wordCount": 621,
          "title": "Adaptive Normalized Representation Learning for Generalizable Face Anti-Spoofing. (arXiv:2108.02667v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02455",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junyu Dong</a>",
          "description": "Ocean fronts can cause the accumulation of nutrients and affect the\npropagation of underwater sound, so high-precision ocean front detection is of\ngreat significance to the marine fishery and national defense fields. However,\nthe current ocean front detection methods either have low detection accuracy or\nmost can only detect the occurrence of ocean front by binary classification,\nrarely considering the differences of the characteristics of multiple ocean\nfronts in different sea areas. In order to solve the above problems, we propose\na semantic segmentation network called location and seasonality enhanced\nnetwork (LSENet) for multi-class ocean fronts detection at pixel level. In this\nnetwork, we first design a channel supervision unit structure, which integrates\nthe seasonal characteristics of the ocean front itself and the contextual\ninformation to improve the detection accuracy. We also introduce a location\nattention mechanism to adaptively assign attention weights to the fronts\naccording to their frequently occurred sea area, which can further improve the\naccuracy of multi-class ocean front detection. Compared with other semantic\nsegmentation methods and current representative ocean front detection method,\nthe experimental results demonstrate convincingly that our method is more\neffective.",
          "link": "http://arxiv.org/abs/2108.02455",
          "publishedOn": "2021-08-06T00:51:47.189Z",
          "wordCount": 630,
          "title": "LSENet: Location and Seasonality Enhanced Network for Multi-Class Ocean Front Detection. (arXiv:2108.02455v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Wen Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongfang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1\">Chang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yu-Ling Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yun Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xueli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yanhong Tai</a>",
          "description": "Objective: We develop a computer-aided diagnosis (CAD) system using deep\nlearning approaches for lesion detection and classification on whole-slide\nimages (WSIs) with breast cancer. The deep features being distinguishing in\nclassification from the convolutional neural networks (CNN) are demonstrated in\nthis study to provide comprehensive interpretability for the proposed CAD\nsystem using pathological knowledge. Methods: In the experiment, a total of 186\nslides of WSIs were collected and classified into three categories:\nNon-Carcinoma, Ductal Carcinoma in Situ (DCIS), and Invasive Ductal Carcinoma\n(IDC). Instead of conducting pixel-wise classification into three classes\ndirectly, we designed a hierarchical framework with the multi-view scheme that\nperforms lesion detection for region proposal at higher magnification first and\nthen conducts lesion classification at lower magnification for each detected\nlesion. Results: The slide-level accuracy rate for three-category\nclassification reaches 90.8% (99/109) through 5-fold cross-validation and\nachieves 94.8% (73/77) on the testing set. The experimental results show that\nthe morphological characteristics and co-occurrence properties learned by the\ndeep learning models for lesion classification are accordant with the clinical\nrules in diagnosis. Conclusion: The pathological interpretability of the deep\nfeatures not only enhances the reliability of the proposed CAD system to gain\nacceptance from medical specialists, but also facilitates the development of\ndeep learning frameworks for various tasks in pathology. Significance: This\npaper presents a CAD system for pathological image analysis, which fills the\nclinical requirements and can be accepted by medical specialists with providing\nits interpretability from the pathological perspective.",
          "link": "http://arxiv.org/abs/2108.02656",
          "publishedOn": "2021-08-06T00:51:47.169Z",
          "wordCount": 710,
          "title": "A Computer-Aided Diagnosis System for Breast Pathology: A Deep Learning Approach with Model Interpretability from Pathological Perspective. (arXiv:2108.02656v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1\">Juan P. Vigueras-Guill&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1\">Frank Seeliger</a>",
          "description": "The automated segmentation of cancer tissue in histopathology images can help\nclinicians to detect, diagnose, and analyze such disease. Different from other\nnatural images used in many convolutional networks for benchmark,\nhistopathology images can be extremely large, and the cancerous patterns can\nreach beyond 1000 pixels. Therefore, the well-known networks in the literature\nwere never conceived to handle these peculiarities. In this work, we propose a\nFully Convolutional DenseUNet that is particularly designed to solve\nhistopathology problems. We evaluated our network in two public pathology\ndatasets published as challenges in the recent MICCAI 2019: binary segmentation\nin colon cancer images (DigestPath2019), and multi-class segmentation in\nprostate cancer images (Gleason2019), achieving similar and better results than\nthe winners of the challenges, respectively. Furthermore, we discussed some\ngood practices in the training setup to yield the best performance and the main\nchallenges in these histopathology datasets.",
          "link": "http://arxiv.org/abs/2108.02676",
          "publishedOn": "2021-08-06T00:51:47.121Z",
          "wordCount": 609,
          "title": "Redesigning Fully Convolutional DenseUNets for Large Histopathology Images. (arXiv:2108.02676v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yanbin Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>",
          "description": "Transformer achieves remarkable successes in understanding 1 and\n2-dimensional signals (e.g., NLP and Image Content Understanding). As a\npotential alternative to convolutional neural networks, it shares merits of\nstrong interpretability, high discriminative power on hyper-scale data, and\nflexibility in processing varying length inputs. However, its encoders\nnaturally contain computational intensive operations such as pair-wise\nself-attention, incurring heavy computational burden when being applied on the\ncomplex 3-dimensional video signals.\n\nThis paper presents Token Shift Module (i.e., TokShift), a novel,\nzero-parameter, zero-FLOPs operator, for modeling temporal relations within\neach transformer encoder. Specifically, the TokShift barely temporally shifts\npartial [Class] token features back-and-forth across adjacent frames. Then, we\ndensely plug the module into each encoder of a plain 2D vision transformer for\nlearning 3D video representation. It is worth noticing that our TokShift\ntransformer is a pure convolutional-free video transformer pilot with\ncomputational efficiency for video understanding. Experiments on standard\nbenchmarks verify its robustness, effectiveness, and efficiency. Particularly,\nwith input clips of 8/12 frames, the TokShift transformer achieves SOTA\nprecision: 79.83%/80.40% on the Kinetics-400, 66.56% on EGTEA-Gaze+, and 96.80%\non UCF-101 datasets, comparable or better than existing SOTA convolutional\ncounterparts. Our code is open-sourced in:\nhttps://github.com/VideoNetworks/TokShift-Transformer.",
          "link": "http://arxiv.org/abs/2108.02432",
          "publishedOn": "2021-08-06T00:51:47.114Z",
          "wordCount": 640,
          "title": "Token Shift Transformer for Video Classification. (arXiv:2108.02432v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bennequin_E/0/1/0/all/0/1\">Etienne Bennequin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouvier_V/0/1/0/all/0/1\">Victor Bouvier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tami_M/0/1/0/all/0/1\">Myriam Tami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toubhans_A/0/1/0/all/0/1\">Antoine Toubhans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1\">C&#xe9;line Hudelot</a>",
          "description": "Few-Shot Learning (FSL) algorithms have made substantial progress in learning\nnovel concepts with just a handful of labelled data. To classify query\ninstances from novel classes encountered at test-time, they only require a\nsupport set composed of a few labelled samples. FSL benchmarks commonly assume\nthat those queries come from the same distribution as instances in the support\nset. However, in a realistic set-ting, data distribution is plausibly subject\nto change, a situation referred to as Distribution Shift (DS). The present work\naddresses the new and challenging problem of Few-Shot Learning under\nSupport/Query Shift (FSQS) i.e., when support and query instances are sampled\nfrom related but different distributions. Our contributions are the following.\nFirst, we release a testbed for FSQS, including datasets, relevant baselines\nand a protocol for a rigorous and reproducible evaluation. Second, we observe\nthat well-established FSL algorithms unsurprisingly suffer from a considerable\ndrop in accuracy when facing FSQS, stressing the significance of our study.\nFinally, we show that transductive algorithms can limit the inopportune effect\nof DS. In particular, we study both the role of Batch-Normalization and Optimal\nTransport (OT) in aligning distributions, bridging Unsupervised Domain\nAdaptation with FSL. This results in a new method that efficiently combines OT\nwith the celebrated Prototypical Networks. We bring compelling experiments\ndemonstrating the advantage of our method. Our work opens an exciting line of\nresearch by providing a testbed and strong baselines. Our code is available at\nhttps://github.com/ebennequin/meta-domain-shift.",
          "link": "http://arxiv.org/abs/2105.11804",
          "publishedOn": "2021-08-06T00:51:46.615Z",
          "wordCount": 713,
          "title": "Bridging Few-Shot Learning and Adaptation: New Challenges of Support-Query Shift. (arXiv:2105.11804v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akan_A/0/1/0/all/0/1\">Adil Kaan Akan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1\">Erkut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1\">Aykut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guney_F/0/1/0/all/0/1\">Fatma G&#xfc;ney</a>",
          "description": "Motion is an important cue for video prediction and often utilized by\nseparating video content into static and dynamic components. Most of the\nprevious work utilizing motion is deterministic but there are stochastic\nmethods that can model the inherent uncertainty of the future. Existing\nstochastic models either do not reason about motion explicitly or make limiting\nassumptions about the static part. In this paper, we reason about appearance\nand motion in the video stochastically by predicting the future based on the\nmotion history. Explicit reasoning about motion without history already reaches\nthe performance of current stochastic models. The motion history further\nimproves the results by allowing to predict consistent dynamics several frames\ninto the future. Our model performs comparably to the state-of-the-art models\non the generic video prediction datasets, however, significantly outperforms\nthem on two challenging real-world autonomous driving datasets with complex\nmotion and dynamic background.",
          "link": "http://arxiv.org/abs/2108.02760",
          "publishedOn": "2021-08-06T00:51:46.567Z",
          "wordCount": 584,
          "title": "SLAMP: Stochastic Latent Appearance and Motion Prediction. (arXiv:2108.02760v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Haotian Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhengyang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>",
          "description": "This paper proposes a hypothesis for the aesthetic appreciation that\naesthetic images make a neural network strengthen salient concepts and discard\ninessential concepts. In order to verify this hypothesis, we use multi-variate\ninteractions to represent salient concepts and inessential concepts contained\nin images. Furthermore, we design a set of operations to revise images towards\nmore beautiful ones. In experiments, we find that the revised images are more\naesthetic than the original ones to some extent.",
          "link": "http://arxiv.org/abs/2108.02646",
          "publishedOn": "2021-08-06T00:51:46.527Z",
          "wordCount": 525,
          "title": "A Hypothesis for the Aesthetic Appreciation in Neural Networks. (arXiv:2108.02646v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2001.04627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1\">Piotr Koniusz</a>",
          "description": "In this paper, we build on a concept of self-supervision by taking RGB frames\nas input to learn to predict both action concepts and auxiliary descriptors\ne.g., object descriptors. So-called hallucination streams are trained to\npredict auxiliary cues, simultaneously fed into classification layers, and then\nhallucinated at the testing stage to aid network. We design and hallucinate two\ndescriptors, one leveraging four popular object detectors applied to training\nvideos, and the other leveraging image- and video-level saliency detectors. The\nfirst descriptor encodes the detector- and ImageNet-wise class prediction\nscores, confidence scores, and spatial locations of bounding boxes and frame\nindexes to capture the spatio-temporal distribution of features per video.\nAnother descriptor encodes spatio-angular gradient distributions of saliency\nmaps and intensity patterns. Inspired by the characteristic function of the\nprobability distribution, we capture four statistical moments on the above\nintermediate descriptors. As numbers of coefficients in the mean, covariance,\ncoskewness and cokurtotsis grow linearly, quadratically, cubically and\nquartically w.r.t. the dimension of feature vectors, we describe the covariance\nmatrix by its leading n' eigenvectors (so-called subspace) and we capture\nskewness/kurtosis rather than costly coskewness/cokurtosis. We obtain state of\nthe art on five popular datasets such as Charades and EPIC-Kitchens.",
          "link": "http://arxiv.org/abs/2001.04627",
          "publishedOn": "2021-08-06T00:51:46.237Z",
          "wordCount": 664,
          "title": "Self-supervising Action Recognition by Statistical Moment and Subspace Descriptors. (arXiv:2001.04627v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00911",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_C/0/1/0/all/0/1\">Chengtao Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1\">Liying Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Huimin Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tong_R/0/1/0/all/0/1\">Ruofeng Tong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Lanfen Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jingsong Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Wei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Q/0/1/0/all/0/1\">Qingqing Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1\">Hongjie Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_Z/0/1/0/all/0/1\">Zhiyi Peng</a>",
          "description": "Multi-phase computed tomography (CT) images provide crucial complementary\ninformation for accurate liver tumor segmentation (LiTS). State-of-the-art\nmulti-phase LiTS methods usually fused cross-phase features through\nphase-weighted summation or channel-attention based concatenation. However,\nthese methods ignored the spatial (pixel-wise) relationships between different\nphases, hence leading to insufficient feature integration. In addition, the\nperformance of existing methods remains subject to the uncertainty in\nsegmentation, which is particularly acute in tumor boundary regions. In this\nwork, we propose a novel LiTS method to adequately aggregate multi-phase\ninformation and refine uncertain region segmentation. To this end, we introduce\na spatial aggregation module (SAM), which encourages per-pixel interactions\nbetween different phases, to make full use of cross-phase information.\nMoreover, we devise an uncertain region inpainting module (URIM) to refine\nuncertain pixels using neighboring discriminative features. Experiments on an\nin-house multi-phase CT dataset of focal liver lesions (MPCT-FLLs) demonstrate\nthat our method achieves promising liver tumor segmentation and outperforms\nstate-of-the-arts.",
          "link": "http://arxiv.org/abs/2108.00911",
          "publishedOn": "2021-08-06T00:51:46.221Z",
          "wordCount": 642,
          "title": "Multi-phase Liver Tumor Segmentation with Spatial Aggregation and Uncertain Region Inpainting. (arXiv:2108.00911v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhongjin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Heming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Dong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongbo Fu</a>",
          "description": "Head shapes play an important role in 3D character design. In this work, we\npropose SimpModeling, a novel sketch-based system for helping users, especially\namateur users, easily model 3D animalmorphic heads - a prevalent kind of heads\nin character design. Although sketching provides an easy way to depict desired\nshapes, it is challenging to infer dense geometric information from sparse line\ndrawings. Recently, deepnet-based approaches have been taken to address this\nchallenge and try to produce rich geometric details from very few strokes.\nHowever, while such methods reduce users' workload, they would cause less\ncontrollability of target shapes. This is mainly due to the uncertainty of the\nneural prediction. Our system tackles this issue and provides good\ncontrollability from three aspects: 1) we separate coarse shape design and\ngeometric detail specification into two stages and respectively provide\ndifferent sketching means; 2) in coarse shape designing, sketches are used for\nboth shape inference and geometric constraints to determine global geometry,\nand in geometric detail crafting, sketches are used for carving surface\ndetails; 3) in both stages, we use the advanced implicit-based shape inference\nmethods, which have strong ability to handle the domain gap between freehand\nsketches and synthetic ones used for training. Experimental results confirm the\neffectiveness of our method and the usability of our interactive system. We\nalso contribute to a dataset of high-quality 3D animal heads, which are\nmanually created by artists.",
          "link": "http://arxiv.org/abs/2108.02548",
          "publishedOn": "2021-08-06T00:51:46.148Z",
          "wordCount": 685,
          "title": "SimpModeling: Sketching Implicit Field to Guide Mesh Modeling for 3D Animalmorphic Head Design. (arXiv:2108.02548v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Garcia_F/0/1/0/all/0/1\">Fernando P&#xe9;rez-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scott_C/0/1/0/all/0/1\">Catherine Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sparks_R/0/1/0/all/0/1\">Rachel Sparks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diehl_B/0/1/0/all/0/1\">Beate Diehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1\">S&#xe9;bastien Ourselin</a>",
          "description": "Detailed analysis of seizure semiology, the symptoms and signs which occur\nduring a seizure, is critical for management of epilepsy patients. Inter-rater\nreliability using qualitative visual analysis is often poor for semiological\nfeatures. Therefore, automatic and quantitative analysis of video-recorded\nseizures is needed for objective assessment.\n\nWe present GESTURES, a novel architecture combining convolutional neural\nnetworks (CNNs) and recurrent neural networks (RNNs) to learn deep\nrepresentations of arbitrarily long videos of epileptic seizures.\n\nWe use a spatiotemporal CNN (STCNN) pre-trained on large human action\nrecognition (HAR) datasets to extract features from short snippets (approx. 0.5\ns) sampled from seizure videos. We then train an RNN to learn seizure-level\nrepresentations from the sequence of features.\n\nWe curated a dataset of seizure videos from 68 patients and evaluated\nGESTURES on its ability to classify seizures into focal onset seizures (FOSs)\n(N = 106) vs. focal to bilateral tonic-clonic seizures (TCSs) (N = 77),\nobtaining an accuracy of 98.9% using bidirectional long short-term memory\n(BLSTM) units.\n\nWe demonstrate that an STCNN trained on a HAR dataset can be used in\ncombination with an RNN to accurately represent arbitrarily long videos of\nseizures. GESTURES can provide accurate seizure classification by modeling\nsequences of semiologies.",
          "link": "http://arxiv.org/abs/2106.12014",
          "publishedOn": "2021-08-06T00:51:46.021Z",
          "wordCount": 690,
          "title": "Transfer Learning of Deep Spatiotemporal Networks to Model Arbitrarily Long Videos of Seizures. (arXiv:2106.12014v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1\">Tsang Ing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1\">Cleber Zanchettin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Adriano L. I. Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "In this paper, we argue that the unsatisfactory out-of-distribution (OOD)\ndetection performance of neural networks is mainly due to the SoftMax loss\nanisotropy and propensity to produce low entropy probability distributions in\ndisagreement with the principle of maximum entropy. Current out-of-distribution\n(OOD) detection approaches usually do not directly fix the SoftMax loss\ndrawbacks, but rather build techniques to circumvent it. Unfortunately, those\nmethods usually produce undesired side effects (e.g., classification accuracy\ndrop, additional hyperparameters, slower inferences, and collecting extra\ndata). In the opposite direction, we propose replacing SoftMax loss with a\nnovel loss function that does not suffer from the mentioned weaknesses. The\nproposed IsoMax loss is isotropic (exclusively distance-based) and provides\nhigh entropy posterior probability distributions. Replacing the SoftMax loss by\nIsoMax loss requires no model or training changes. Additionally, the models\ntrained with IsoMax loss produce as fast and energy-efficient inferences as\nthose trained using SoftMax loss. Moreover, no classification accuracy drop is\nobserved. The proposed method does not rely on outlier/background data,\nhyperparameter tuning, temperature calibration, feature extraction, metric\nlearning, adversarial training, ensemble procedures, or generative models. Our\nexperiments showed that IsoMax loss works as a seamless SoftMax loss drop-in\nreplacement that significantly improves neural networks' OOD detection\nperformance. Hence, it may be used as a baseline OOD detection approach to be\ncombined with current or future OOD detection techniques to achieve even higher\nresults.",
          "link": "http://arxiv.org/abs/2006.04005",
          "publishedOn": "2021-08-06T00:51:46.014Z",
          "wordCount": 734,
          "title": "Entropic Out-of-Distribution Detection: Seamless Detection of Unknown Examples. (arXiv:2006.04005v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guanglei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Mingli Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>",
          "description": "While convolutional neural networks have shown a tremendous impact on various\ncomputer vision tasks, they generally demonstrate limitations in explicitly\nmodeling long-range dependencies due to the intrinsic locality of the\nconvolution operation. Initially designed for natural language processing\ntasks, Transformers have emerged as alternative architectures with innate\nglobal self-attention mechanisms to capture long-range dependencies. In this\npaper, we propose TransDepth, an architecture that benefits from both\nconvolutional neural networks and transformers. To avoid the network losing its\nability to capture local-level details due to the adoption of transformers, we\npropose a novel decoder that employs attention mechanisms based on gates.\nNotably, this is the first paper that applies transformers to pixel-wise\nprediction problems involving continuous labels (i.e., monocular depth\nprediction and surface normal estimation). Extensive experiments demonstrate\nthat the proposed TransDepth achieves state-of-the-art performance on three\nchallenging datasets. Our code is available at:\nhttps://github.com/ygjwd12345/TransDepth.",
          "link": "http://arxiv.org/abs/2103.12091",
          "publishedOn": "2021-08-06T00:51:45.988Z",
          "wordCount": 613,
          "title": "Transformer-Based Attention Networks for Continuous Pixel-Wise Prediction. (arXiv:2103.12091v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12636",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Fengxiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Detection transformers have recently shown promising object detection results\nand attracted increasing attention. However, how to develop effective domain\nadaptation techniques to improve its cross-domain performance remains\nunexplored and unclear. In this paper, we delve into this topic and empirically\nfind that direct feature distribution alignment on the CNN backbone only brings\nlimited improvements, as it does not guarantee domain-invariant sequence\nfeatures in the transformer for prediction. To address this issue, we propose a\nnovel Sequence Feature Alignment (SFA) method that is specially designed for\nthe adaptation of detection transformers. Technically, SFA consists of a domain\nquery-based feature alignment (DQFA) module and a token-wise feature alignment\n(TDA) module. In DQFA, a novel domain query is used to aggregate and align\nglobal context from the token sequence of both domains. DQFA reduces the domain\ndiscrepancy in global feature representations and object relations when\ndeploying in the transformer encoder and decoder, respectively. Meanwhile, TDA\naligns token features in the sequence from both domains, which reduces the\ndomain gaps in local and instance-level feature representations in the\ntransformer encoder and decoder, respectively. Besides, a novel bipartite\nmatching consistency loss is proposed to enhance the feature discriminability\nfor robust object detection. Experiments on three challenging benchmarks show\nthat SFA outperforms state-of-the-art domain adaptive object detection methods.\nCode has been made available at: https://github.com/encounter1997/SFA.",
          "link": "http://arxiv.org/abs/2107.12636",
          "publishedOn": "2021-08-06T00:51:45.914Z",
          "wordCount": 726,
          "title": "Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.04696",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Perez_Garcia_F/0/1/0/all/0/1\">Fernando P&#xe9;rez-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sparks_R/0/1/0/all/0/1\">Rachel Sparks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">S&#xe9;bastien Ourselin</a>",
          "description": "Processing of medical images such as MRI or CT presents unique challenges\ncompared to RGB images typically used in computer vision. These include a lack\nof labels for large datasets, high computational costs, and metadata to\ndescribe the physical properties of voxels. Data augmentation is used to\nartificially increase the size of the training datasets. Training with image\npatches decreases the need for computational power. Spatial metadata needs to\nbe carefully taken into account in order to ensure a correct alignment of\nvolumes.\n\nWe present TorchIO, an open-source Python library to enable efficient\nloading, preprocessing, augmentation and patch-based sampling of medical images\nfor deep learning. TorchIO follows the style of PyTorch and integrates standard\nmedical image processing libraries to efficiently process images during\ntraining of neural networks. TorchIO transforms can be composed, reproduced,\ntraced and extended. We provide multiple generic preprocessing and augmentation\noperations as well as simulation of MRI-specific artifacts.\n\nSource code, comprehensive tutorials and extensive documentation for TorchIO\ncan be found at https://torchio.rtfd.io/. The package can be installed from the\nPython Package Index running 'pip install torchio'. It includes a command-line\ninterface which allows users to apply transforms to image files without using\nPython. Additionally, we provide a graphical interface within a TorchIO\nextension in 3D Slicer to visualize the effects of transforms.\n\nTorchIO was developed to help researchers standardize medical image\nprocessing pipelines and allow them to focus on the deep learning experiments.\nIt encourages open science, as it supports reproducibility and is version\ncontrolled so that the software can be cited precisely. Due to its modularity,\nthe library is compatible with other frameworks for deep learning with medical\nimages.",
          "link": "http://arxiv.org/abs/2003.04696",
          "publishedOn": "2021-08-06T00:51:45.884Z",
          "wordCount": 819,
          "title": "TorchIO: A Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning. (arXiv:2003.04696v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1\">Tal Ridnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1\">Emanuel Ben-Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1\">Lihi Zelnik-Manor</a>",
          "description": "ImageNet-1K serves as the primary dataset for pretraining deep learning\nmodels for computer vision tasks. ImageNet-21K dataset, which is bigger and\nmore diverse, is used less frequently for pretraining, mainly due to its\ncomplexity, low accessibility, and underestimation of its added value. This\npaper aims to close this gap, and make high-quality efficient pretraining on\nImageNet-21K available for everyone. Via a dedicated preprocessing stage,\nutilization of WordNet hierarchical structure, and a novel training scheme\ncalled semantic softmax, we show that various models significantly benefit from\nImageNet-21K pretraining on numerous datasets and tasks, including small\nmobile-oriented models. We also show that we outperform previous ImageNet-21K\npretraining schemes for prominent new models like ViT and Mixer. Our proposed\npretraining pipeline is efficient, accessible, and leads to SoTA reproducible\nresults, from a publicly available dataset. The training code and pretrained\nmodels are available at: https://github.com/Alibaba-MIIL/ImageNet21K",
          "link": "http://arxiv.org/abs/2104.10972",
          "publishedOn": "2021-08-06T00:51:45.876Z",
          "wordCount": 632,
          "title": "ImageNet-21K Pretraining for the Masses. (arXiv:2104.10972v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00531",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pourreza_R/0/1/0/all/0/1\">Reza Pourreza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_T/0/1/0/all/0/1\">Taco S Cohen</a>",
          "description": "While most neural video codecs address P-frame coding (predicting each frame\nfrom past ones), in this paper we address B-frame compression (predicting\nframes using both past and future reference frames). Our B-frame solution is\nbased on the existing P-frame methods. As a result, B-frame coding capability\ncan easily be added to an existing neural codec. The basic idea of our B-frame\ncoding method is to interpolate the two reference frames to generate a single\nreference frame and then use it together with an existing P-frame codec to\nencode the input B-frame. Our studies show that the interpolated frame is a\nmuch better reference for the P-frame codec compared to using the previous\nframe as is usually done. Our results show that using the proposed method with\nan existing P-frame codec can lead to 28.5%saving in bit-rate on the UVG\ndataset compared to the P-frame codec while generating the same video quality.",
          "link": "http://arxiv.org/abs/2104.00531",
          "publishedOn": "2021-08-06T00:51:45.858Z",
          "wordCount": 614,
          "title": "Extending Neural P-frame Codecs for B-frame Coding. (arXiv:2104.00531v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sim_H/0/1/0/all/0/1\">Hyeonjun Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jihyong Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Munchurl Kim</a>",
          "description": "In this paper, we firstly present a dataset (X4K1000FPS) of 4K videos of 1000\nfps with the extreme motion to the research community for video frame\ninterpolation (VFI), and propose an extreme VFI network, called XVFI-Net, that\nfirst handles the VFI for 4K videos with large motion. The XVFI-Net is based on\na recursive multi-scale shared structure that consists of two cascaded modules\nfor bidirectional optical flow learning between two input frames (BiOF-I) and\nfor bidirectional optical flow learning from target to input frames (BiOF-T).\nThe optical flows are stably approximated by a complementary flow reversal\n(CFR) proposed in BiOF-T module. During inference, the BiOF-I module can start\nat any scale of input while the BiOF-T module only operates at the original\ninput scale so that the inference can be accelerated while maintaining highly\naccurate VFI performance. Extensive experimental results show that our XVFI-Net\ncan successfully capture the essential information of objects with extremely\nlarge motions and complex textures while the state-of-the-art methods exhibit\npoor performance. Furthermore, our XVFI-Net framework also performs comparably\non the previous lower resolution benchmark dataset, which shows a robustness of\nour algorithm as well. All source codes, pre-trained models, and proposed\nX4K1000FPS datasets are publicly available at\nhttps://github.com/JihyongOh/XVFI.",
          "link": "http://arxiv.org/abs/2103.16206",
          "publishedOn": "2021-08-06T00:51:45.850Z",
          "wordCount": 675,
          "title": "XVFI: eXtreme Video Frame Interpolation. (arXiv:2103.16206v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Osco_L/0/1/0/all/0/1\">Lucas Prado Osco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_J/0/1/0/all/0/1\">Jos&#xe9; Marcato Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramos_A/0/1/0/all/0/1\">Ana Paula Marques Ramos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorge_L/0/1/0/all/0/1\">L&#xfa;cio Andr&#xe9; de Castro Jorge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatholahi_S/0/1/0/all/0/1\">Sarah Narges Fatholahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1\">Jonathan de Andrade Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_E/0/1/0/all/0/1\">Edson Takashi Matsubara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pistori_H/0/1/0/all/0/1\">Hemerson Pistori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_W/0/1/0/all/0/1\">Wesley Nunes Gon&#xe7;alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jonathan Li</a>",
          "description": "Deep Neural Networks (DNNs) learn representation from data with an impressive\ncapability, and brought important breakthroughs for processing images,\ntime-series, natural language, audio, video, and many others. In the remote\nsensing field, surveys and literature revisions specifically involving DNNs\nalgorithms' applications have been conducted in an attempt to summarize the\namount of information produced in its subfields. Recently, Unmanned Aerial\nVehicles (UAV) based applications have dominated aerial sensing research.\nHowever, a literature revision that combines both \"deep learning\" and \"UAV\nremote sensing\" thematics has not yet been conducted. The motivation for our\nwork was to present a comprehensive review of the fundamentals of Deep Learning\n(DL) applied in UAV-based imagery. We focused mainly on describing\nclassification and regression techniques used in recent applications with\nUAV-acquired data. For that, a total of 232 papers published in international\nscientific journal databases was examined. We gathered the published material\nand evaluated their characteristics regarding application, sensor, and\ntechnique used. We relate how DL presents promising results and has the\npotential for processing tasks associated with UAV-based image data. Lastly, we\nproject future perspectives, commentating on prominent DL paths to be explored\nin the UAV remote sensing field. Our revision consists of a friendly-approach\nto introduce, commentate, and summarize the state-of-the-art in UAV-based image\napplications with DNNs algorithms in diverse subfields of remote sensing,\ngrouping it in the environmental, urban, and agricultural contexts.",
          "link": "http://arxiv.org/abs/2101.10861",
          "publishedOn": "2021-08-06T00:51:45.844Z",
          "wordCount": 729,
          "title": "A Review on Deep Learning in UAV Remote Sensing. (arXiv:2101.10861v2 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Canyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eschweiler_D/0/1/0/all/0/1\">Dennis Eschweiler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stegmaier_J/0/1/0/all/0/1\">Johannes Stegmaier</a>",
          "description": "Recent developments in fluorescence microscopy allow capturing\nhigh-resolution 3D images over time for living model organisms. To be able to\nimage even large specimens, techniques like multi-view light-sheet imaging\nrecord different orientations at each time point that can then be fused into a\nsingle high-quality volume. Based on measured point spread functions (PSF),\ndeconvolution and content fusion are able to largely revert the inevitable\ndegradation occurring during the imaging process. Classical multi-view\ndeconvolution and fusion methods mainly use iterative procedures and\ncontent-based averaging. Lately, Convolutional Neural Networks (CNNs) have been\ndeployed to approach 3D single-view deconvolution microscopy, but the\nmulti-view case waits to be studied. We investigated the efficacy of CNN-based\nmulti-view deconvolution and fusion with two synthetic data sets that mimic\ndeveloping embryos and involve either two or four complementary 3D views.\nCompared with classical state-of-the-art methods, the proposed semi- and\nself-supervised models achieve competitive and superior deconvolution and\nfusion quality in the two-view and quad-view cases, respectively.",
          "link": "http://arxiv.org/abs/2108.02743",
          "publishedOn": "2021-08-06T00:51:45.837Z",
          "wordCount": 620,
          "title": "Semi- and Self-Supervised Multi-View Fusion of 3D Microscopy Images using Generative Adversarial Networks. (arXiv:2108.02743v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1\">Shuyi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protter_M/0/1/0/all/0/1\">Matan Protter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimerman_G/0/1/0/all/0/1\">Gadi Zimerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghui Xu</a>",
          "description": "Recent deep generative models have achieved promising performance in image\ninpainting. However, it is still very challenging for a neural network to\ngenerate realistic image details and textures, due to its inherent spectral\nbias. By our understanding of how artists work, we suggest to adopt a\n`structure first detail next' workflow for image inpainting. To this end, we\npropose to build a Pyramid Generator by stacking several sub-generators, where\nlower-layer sub-generators focus on restoring image structures while the\nhigher-layer sub-generators emphasize image details. Given an input image, it\nwill be gradually restored by going through the entire pyramid in a bottom-up\nfashion. Particularly, our approach has a learning scheme of progressively\nincreasing hole size, which allows it to restore large-hole images. In\naddition, our method could fully exploit the benefits of learning with\nhigh-resolution images, and hence is suitable for high-resolution image\ninpainting. Extensive experimental results on benchmark datasets have validated\nthe effectiveness of our approach compared with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.08905",
          "publishedOn": "2021-08-06T00:51:45.826Z",
          "wordCount": 634,
          "title": "Structure First Detail Next: Image Inpainting with Pyramid Generator. (arXiv:2106.08905v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">HanQin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamm_K/0/1/0/all/0/1\">Keaton Hamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Longxiu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Needell_D/0/1/0/all/0/1\">Deanna Needell</a>",
          "description": "This paper considers the use of Robust PCA in a CUR decomposition framework\nand applications thereof. Our main algorithms produce a robust version of\ncolumn-row factorizations of matrices $\\mathbf{D}=\\mathbf{L}+\\mathbf{S}$ where\n$\\mathbf{L}$ is low-rank and $\\mathbf{S}$ contains sparse outliers. These\nmethods yield interpretable factorizations at low computational cost, and\nprovide new CUR decompositions that are robust to sparse outliers, in contrast\nto previous methods. We consider two key imaging applications of Robust PCA:\nvideo foreground-background separation and face modeling. This paper examines\nthe qualitative behavior of our Robust CUR decompositions on the benchmark\nvideos and face datasets, and find that our method works as well as standard\nRobust PCA while being significantly faster. Additionally, we consider hybrid\nrandomized and deterministic sampling methods which produce a compact CUR\ndecomposition of a given matrix, and apply this to video sequences to produce\ncanonical frames thereof.",
          "link": "http://arxiv.org/abs/2101.05231",
          "publishedOn": "2021-08-06T00:51:45.810Z",
          "wordCount": 624,
          "title": "Robust CUR Decomposition: Theory and Imaging Applications. (arXiv:2101.05231v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1\">Stephen Bates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1\">Anastasios Angelopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1\">Lihua Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "While improving prediction accuracy has been the focus of machine learning in\nrecent years, this alone does not suffice for reliable decision-making.\nDeploying learning systems in consequential settings also requires calibrating\nand communicating the uncertainty of predictions. To convey instance-wise\nuncertainty for prediction tasks, we show how to generate set-valued\npredictions from a black-box predictor that control the expected loss on future\ntest points at a user-specified level. Our approach provides explicit\nfinite-sample guarantees for any dataset by using a holdout set to calibrate\nthe size of the prediction sets. This framework enables simple,\ndistribution-free, rigorous error control for many tasks, and we demonstrate it\nin five large-scale machine learning problems: (1) classification problems\nwhere some mistakes are more costly than others; (2) multi-label\nclassification, where each observation has multiple associated labels; (3)\nclassification problems where the labels have a hierarchical structure; (4)\nimage segmentation, where we wish to predict a set of pixels containing an\nobject of interest; and (5) protein structure prediction. Lastly, we discuss\nextensions to uncertainty quantification for ranking, metric learning and\ndistributionally robust learning.",
          "link": "http://arxiv.org/abs/2101.02703",
          "publishedOn": "2021-08-06T00:51:45.778Z",
          "wordCount": 677,
          "title": "Distribution-Free, Risk-Controlling Prediction Sets. (arXiv:2101.02703v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1\">Olivier J. H&#xe9;naff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1\">Skanda Koppula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1\">Jean-Baptiste Alayrac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oord_A/0/1/0/all/0/1\">Aaron van den Oord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Jo&#xe3;o Carreira</a>",
          "description": "Self-supervised pretraining has been shown to yield powerful representations\nfor transfer learning. These performance gains come at a large computational\ncost however, with state-of-the-art methods requiring an order of magnitude\nmore computation than supervised pretraining. We tackle this computational\nbottleneck by introducing a new self-supervised objective, contrastive\ndetection, which tasks representations with identifying object-level features\nacross augmentations. This objective extracts a rich learning signal per image,\nleading to state-of-the-art transfer accuracy on a variety of downstream tasks,\nwhile requiring up to 10x less pretraining. In particular, our strongest\nImageNet-pretrained model performs on par with SEER, one of the largest\nself-supervised systems to date, which uses 1000x more pretraining data.\nFinally, our objective seamlessly handles pretraining on more complex images\nsuch as those in COCO, closing the gap with supervised transfer learning from\nCOCO to PASCAL.",
          "link": "http://arxiv.org/abs/2103.10957",
          "publishedOn": "2021-08-06T00:51:45.764Z",
          "wordCount": 610,
          "title": "Efficient Visual Pretraining with Contrastive Detection. (arXiv:2103.10957v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunsong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "In this paper, we propose an instance similarity learning (ISL) method for\nunsupervised feature representation. Conventional methods assign close instance\npairs in the feature space with high similarity, which usually leads to wrong\npairwise relationship for large neighborhoods because the Euclidean distance\nfails to depict the true semantic similarity on the feature manifold. On the\ncontrary, our method mines the feature manifold in an unsupervised manner,\nthrough which the semantic similarity among instances is learned in order to\nobtain discriminative representations. Specifically, we employ the Generative\nAdversarial Networks (GAN) to mine the underlying feature manifold, where the\ngenerated features are applied as the proxies to progressively explore the\nfeature manifold so that the semantic similarity among instances is acquired as\nreliable pseudo supervision. Extensive experiments on image classification\ndemonstrate the superiority of our method compared with the state-of-the-art\nmethods. The code is available at https://github.com/ZiweiWangTHU/ISL.git.",
          "link": "http://arxiv.org/abs/2108.02721",
          "publishedOn": "2021-08-06T00:51:45.757Z",
          "wordCount": 588,
          "title": "Instance Similarity Learning for Unsupervised Feature Representation. (arXiv:2108.02721v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02740",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongbo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1\">Maks Ovsjanikov</a>",
          "description": "In this work, we propose UPDesc, an unsupervised method to learn point\ndescriptors for robust point cloud registration. Our work builds upon a recent\nsupervised 3D CNN-based descriptor extraction framework, namely, 3DSmoothNet,\nwhich leverages a voxel-based representation to parameterize the surrounding\ngeometry of interest points. Instead of using a predefined fixed-size local\nsupport in voxelization, which potentially limits the access of richer local\ngeometry information, we propose to learn the support size in a data-driven\nmanner. To this end, we design a differentiable voxelization module that can\nback-propagate gradients to the support size optimization. To optimize\ndescriptor similarity, the prior 3D CNN work and other supervised methods\nrequire abundant correspondence labels or pose annotations of point clouds for\ncrafting metric learning losses. Differently, we show that unsupervised\nlearning of descriptor similarity can be achieved by performing geometric\nregistration in networks. Our learning objectives consider descriptor\nsimilarity both across and within point clouds without supervision. Through\nextensive experiments on point cloud registration benchmarks, we show that our\nlearned descriptors yield superior performance over existing unsupervised\nmethods.",
          "link": "http://arxiv.org/abs/2108.02740",
          "publishedOn": "2021-08-06T00:51:45.695Z",
          "wordCount": 612,
          "title": "UPDesc: Unsupervised Point Descriptor Learning for Robust Registration. (arXiv:2108.02740v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1\">Juan P. Vigueras-Guill&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_A/0/1/0/all/0/1\">Arijit Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engkvist_O/0/1/0/all/0/1\">Ola Engkvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1\">Frank Seeliger</a>",
          "description": "Capsule Networks (CapsNets) is a machine learning architecture proposed to\novercome some of the shortcomings of convolutional neural networks (CNNs).\nHowever, CapsNets have mainly outperformed CNNs in datasets where images are\nsmall and/or the objects to identify have minimal background noise. In this\nwork, we present a new architecture, parallel CapsNets, which exploits the\nconcept of branching the network to isolate certain capsules, allowing each\nbranch to identify different entities. We applied our concept to the two\ncurrent types of CapsNet architectures, studying the performance for networks\nwith different layers of capsules. We tested our design in a public, highly\nunbalanced dataset of acute myeloid leukaemia images (15 classes). Our\nexperiments showed that conventional CapsNets show similar performance than our\nbaseline CNN (ResNeXt-50) but depict instability problems. In contrast,\nparallel CapsNets can outperform ResNeXt-50, is more stable, and shows better\nrotational invariance than both, conventional CapsNets and ResNeXt-50.",
          "link": "http://arxiv.org/abs/2108.02644",
          "publishedOn": "2021-08-06T00:51:45.676Z",
          "wordCount": 607,
          "title": "Parallel Capsule Networks for Classification of White Blood Cells. (arXiv:2108.02644v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1\">Qiang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1\">Nanxuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guoqiang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>",
          "description": "The fully convolutional network (FCN) has dominated salient object detection\nfor a long period. However, the locality of CNN requires the model deep enough\nto have a global receptive field and such a deep model always leads to the loss\nof local details. In this paper, we introduce a new attention-based encoder,\nvision transformer, into salient object detection to ensure the globalization\nof the representations from shallow to deep layers. With the global view in\nvery shallow layers, the transformer encoder preserves more local\nrepresentations to recover the spatial details in final saliency maps. Besides,\nas each layer can capture a global view of its previous layer, adjacent layers\ncan implicitly maximize the representation differences and minimize the\nredundant features, making that every output feature of transformer layers\ncontributes uniquely for final prediction. To decode features from the\ntransformer, we propose a simple yet effective deeply-transformed decoder. The\ndecoder densely decodes and upsamples the transformer features, generating the\nfinal saliency map with less noise injection. Experimental results demonstrate\nthat our method significantly outperforms other FCN-based and transformer-based\nmethods in five benchmarks by a large margin, with an average of 12.17%\nimprovement in terms of Mean Absolute Error (MAE). Code will be available at\nhttps://github.com/OliverRensu/GLSTR.",
          "link": "http://arxiv.org/abs/2108.02759",
          "publishedOn": "2021-08-06T00:51:45.669Z",
          "wordCount": 646,
          "title": "Unifying Global-Local Representations in Salient Object Detection with Transformer. (arXiv:2108.02759v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12469",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thoreau_M/0/1/0/all/0/1\">Michael Thoreau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wilson_F/0/1/0/all/0/1\">Frazer Wilson</a>",
          "description": "Access to high resolution satellite imagery has dramatically increased in\nrecent years as several new constellations have entered service. High revisit\nfrequencies as well as improved resolution has widened the use cases of\nsatellite imagery to areas such as humanitarian relief and even Search and\nRescue (SaR). We propose a novel remote sensing object detection dataset for\ndeep learning assisted SaR. This dataset contains only small objects that have\nbeen identified as potential targets as part of a live SaR response. We\nevaluate the application of popular object detection models to this dataset as\na baseline to inform further research. We also propose a novel object detection\nmetric, specifically designed to be used in a deep learning assisted SaR\nsetting.",
          "link": "http://arxiv.org/abs/2107.12469",
          "publishedOn": "2021-08-06T00:51:45.663Z",
          "wordCount": 597,
          "title": "SaRNet: A Dataset for Deep Learning Assisted Search and Rescue with Satellite Imagery. (arXiv:2107.12469v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.06610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Havasi_M/0/1/0/all/0/1\">Marton Havasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1\">Rodolphe Jenatton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jeremiah Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_J/0/1/0/all/0/1\">Jasper Snoek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Dustin Tran</a>",
          "description": "Recent approaches to efficiently ensemble neural networks have shown that\nstrong robustness and uncertainty performance can be achieved with a negligible\ngain in parameters over the original network. However, these methods still\nrequire multiple forward passes for prediction, leading to a significant\ncomputational cost. In this work, we show a surprising result: the benefits of\nusing multiple predictions can be achieved `for free' under a single model's\nforward pass. In particular, we show that, using a multi-input multi-output\n(MIMO) configuration, one can utilize a single model's capacity to train\nmultiple subnetworks that independently learn the task at hand. By ensembling\nthe predictions made by the subnetworks, we improve model robustness without\nincreasing compute. We observe a significant improvement in negative\nlog-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet,\nand their out-of-distribution variants compared to previous methods.",
          "link": "http://arxiv.org/abs/2010.06610",
          "publishedOn": "2021-08-06T00:51:45.656Z",
          "wordCount": 632,
          "title": "Training independent subnetworks for robust prediction. (arXiv:2010.06610v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02725",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shaunak Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_M/0/1/0/all/0/1\">Mikhail Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_G/0/1/0/all/0/1\">Gaurav Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sviridenko_M/0/1/0/all/0/1\">Maxim Sviridenko</a>",
          "description": "Numerous online stock image libraries offer high quality yet copyright free\nimages for use in marketing campaigns. To assist advertisers in navigating such\nthird party libraries, we study the problem of automatically fetching relevant\nad images given the ad text (via a short textual query for images). Motivated\nby our observations in logged data on ad image search queries (given ad text),\nwe formulate a keyword extraction problem, where a keyword extracted from the\nad text (or its augmented version) serves as the ad image query. In this\ncontext, we propose VisualTextRank: an unsupervised method to (i) augment input\nad text using semantically similar ads, and (ii) extract the image query from\nthe augmented ad text. VisualTextRank builds on prior work on graph based\ncontext extraction (biased TextRank in particular) by leveraging both the text\nand image of similar ads for better keyword extraction, and using advertiser\ncategory specific biasing with sentence-BERT embeddings. Using data collected\nfrom the Verizon Media Native (Yahoo Gemini) ad platform's stock image search\nfeature for onboarding advertisers, we demonstrate the superiority of\nVisualTextRank compared to competitive keyword extraction baselines (including\nan $11\\%$ accuracy lift over biased TextRank). For the case when the stock\nimage library is restricted to English queries, we show the effectiveness of\nVisualTextRank on multilingual ads (translated to English) while leveraging\nsemantically similar English ads. Online tests with a simplified version of\nVisualTextRank led to a 28.7% increase in the usage of stock image search, and\na 41.6% increase in the advertiser onboarding rate in the Verizon Media Native\nad platform.",
          "link": "http://arxiv.org/abs/2108.02725",
          "publishedOn": "2021-08-06T00:51:45.649Z",
          "wordCount": 714,
          "title": "VisualTextRank: Unsupervised Graph-based Content Extraction for Automating Ad Text to Image Search. (arXiv:2108.02725v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuang_H/0/1/0/all/0/1\">Haofei Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwertfeger_S/0/1/0/all/0/1\">S&#xf6;ren Schwertfeger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stachniss_C/0/1/0/all/0/1\">Cyrill Stachniss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>",
          "description": "Contrastive learning has revolutionized self-supervised image representation\nlearning field, and recently been adapted to video domain. One of the greatest\nadvantages of contrastive learning is that it allows us to flexibly define\npowerful loss objectives as long as we can find a reasonable way to formulate\npositive and negative samples to contrast. However, existing approaches rely\nheavily on the short-range spatiotemporal salience to form clip-level\ncontrastive signals, thus limit themselves from using global context. In this\npaper, we propose a new video-level contrastive learning method based on\nsegments to formulate positive pairs. Our formulation is able to capture global\ncontext in a video, thus robust to temporal content change. We also incorporate\na temporal order regularization term to enforce the inherent sequential\nstructure of videos. Extensive experiments show that our video-level\ncontrastive learning framework (VCLR) is able to outperform previous\nstate-of-the-arts on five video datasets for downstream action classification,\naction localization and video retrieval. Code is available at\nhttps://github.com/amazon-research/video-contrastive-learning.",
          "link": "http://arxiv.org/abs/2108.02722",
          "publishedOn": "2021-08-06T00:51:45.641Z",
          "wordCount": 618,
          "title": "Video Contrastive Learning with Global Context. (arXiv:2108.02722v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mehra_V/0/1/0/all/0/1\">Vipul Mehra</a>",
          "description": "This thesis is divided into two parts:Part I: Analysis of Fruits, Vegetables,\nCheese and Fish based on Image Processing using Computer Vision and Deep\nLearning: A Review. It consists of a comprehensive review of image processing,\ncomputer vision and deep learning techniques applied to carry out analysis of\nfruits, vegetables, cheese and fish.This part also serves as a literature\nreview for Part II.Part II: GuavaNet: A deep neural network architecture for\nautomatic sensory evaluation to predict degree of acceptability for Guava by a\nconsumer. This part introduces to an end-to-end deep neural network\narchitecture that can predict the degree of acceptability by the consumer for a\nguava based on sensory evaluation.",
          "link": "http://arxiv.org/abs/2108.02563",
          "publishedOn": "2021-08-06T00:51:45.622Z",
          "wordCount": 574,
          "title": "GuavaNet: A deep neural network architecture for automatic sensory evaluation to predict degree of acceptability for Guava by a consumer. (arXiv:2108.02563v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Palafox_P/0/1/0/all/0/1\">Pablo Palafox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozic_A/0/1/0/all/0/1\">Alja&#x17e; Bo&#x17e;i&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1\">Justus Thies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>",
          "description": "Parametric 3D models have enabled a wide variety of tasks in computer\ngraphics and vision, such as modeling human bodies, faces, and hands. However,\nthe construction of these parametric models is often tedious, as it requires\nheavy manual tweaking, and they struggle to represent additional complexity and\ndetails such as wrinkles or clothing. To this end, we propose Neural Parametric\nModels (NPMs), a novel, learned alternative to traditional, parametric 3D\nmodels, which does not require hand-crafted, object-specific constraints. In\nparticular, we learn to disentangle 4D dynamics into latent-space\nrepresentations of shape and pose, leveraging the flexibility of recent\ndevelopments in learned implicit functions. Crucially, once learned, our neural\nparametric models of shape and pose enable optimization over the learned spaces\nto fit to new observations, similar to the fitting of a traditional parametric\nmodel, e.g., SMPL. This enables NPMs to achieve a significantly more accurate\nand detailed representation of observed deformable sequences. We show that NPMs\nimprove notably over both parametric and non-parametric state of the art in\nreconstruction and tracking of monocular depth sequences of clothed humans and\nhands. Latent-space interpolation as well as shape/pose transfer experiments\nfurther demonstrate the usefulness of NPMs. Code is publicly available at\nhttps://pablopalafox.github.io/npms.",
          "link": "http://arxiv.org/abs/2104.00702",
          "publishedOn": "2021-08-06T00:51:45.614Z",
          "wordCount": 676,
          "title": "NPMs: Neural Parametric Models for 3D Deformable Shapes. (arXiv:2104.00702v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02522",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yuhang Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xingrui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calway_A/0/1/0/all/0/1\">Andrew Calway</a>",
          "description": "We propose a novel object-augmented RGB-D SLAM system that is capable of\nconstructing a consistent object map and performing relocalisation based on\ncentroids of objects in the map. The approach aims to overcome the view\ndependence of appearance-based relocalisation methods using point features or\nimages. During the map construction, we use a pre-trained neural network to\ndetect objects and estimate 6D poses from RGB-D data. An incremental\nprobabilistic model is used to aggregate estimates over time to create the\nobject map. Then in relocalisation, we use the same network to extract\nobjects-of-interest in the `lost' frames. Pairwise geometric matching finds\ncorrespondences between map and frame objects, and probabilistic absolute\norientation followed by application of iterative closest point to dense depth\nmaps and object centroids gives relocalisation. Results of experiments in\ndesktop environments demonstrate very high success rates even for frames with\nwidely different viewpoints from those used to construct the map, significantly\noutperforming two appearance-based methods.",
          "link": "http://arxiv.org/abs/2108.02522",
          "publishedOn": "2021-08-06T00:51:45.607Z",
          "wordCount": 602,
          "title": "Object-Augmented RGB-D SLAM for Wide-Disparity Relocalisation. (arXiv:2108.02522v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haobo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jianjun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>",
          "description": "Point cloud registration is a fundamental problem in 3D computer vision. In\nthis paper, we cast point cloud registration into a planning problem in\nreinforcement learning, which can seek the transformation between the source\nand target point clouds through trial and error. By modeling the point cloud\nregistration process as a Markov decision process (MDP), we develop a latent\ndynamic model of point clouds, consisting of a transformation network and\nevaluation network. The transformation network aims to predict the new\ntransformed feature of the point cloud after performing a rigid transformation\n(i.e., action) on it while the evaluation network aims to predict the alignment\nprecision between the transformed source point cloud and target point cloud as\nthe reward signal. Once the dynamic model of the point cloud is trained, we\nemploy the cross-entropy method (CEM) to iteratively update the planning policy\nby maximizing the rewards in the point cloud registration process. Thus, the\noptimal policy, i.e., the transformation between the source and target point\nclouds, can be obtained via gradually narrowing the search space of the\ntransformation. Experimental results on ModelNet40 and 7Scene benchmark\ndatasets demonstrate that our method can yield good registration performance in\nan unsupervised manner.",
          "link": "http://arxiv.org/abs/2108.02613",
          "publishedOn": "2021-08-06T00:51:45.599Z",
          "wordCount": 646,
          "title": "Planning with Learned Dynamic Model for Unsupervised Point Cloud Registration. (arXiv:2108.02613v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_H/0/1/0/all/0/1\">Harrison Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Brian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fawaz_K/0/1/0/all/0/1\">Kassem Fawaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>",
          "description": "The proliferation of automated facial recognition in various commercial and\ngovernment sectors has caused significant privacy concerns for individuals. A\nrecent and popular approach to address these privacy concerns is to employ\nevasion attacks against the metric embedding networks powering facial\nrecognition systems. Face obfuscation systems generate imperceptible\nperturbations, when added to an image, cause the facial recognition system to\nmisidentify the user. The key to these approaches is the generation of\nperturbations using a pre-trained metric embedding network followed by their\napplication to an online system, whose model might be proprietary. This\ndependence of face obfuscation on metric embedding networks, which are known to\nbe unfair in the context of facial recognition, surfaces the question of\ndemographic fairness -- \\textit{are there demographic disparities in the\nperformance of face obfuscation systems?} To address this question, we perform\nan analytical and empirical exploration of the performance of recent face\nobfuscation systems that rely on deep embedding networks. We find that metric\nembedding networks are demographically aware; they cluster faces in the\nembedding space based on their demographic attributes. We observe that this\neffect carries through to the face obfuscation systems: faces belonging to\nminority groups incur reduced utility compared to those from majority groups.\nFor example, the disparity in average obfuscation success rate on the online\nFace++ API can reach up to 20 percentage points. Further, for some demographic\ngroups, the average perturbation size increases by up to 17\\% when choosing a\ntarget identity belonging to a different demographic group versus the same\ndemographic group. Finally, we present a simple analytical model to provide\ninsights into these phenomena.",
          "link": "http://arxiv.org/abs/2108.02707",
          "publishedOn": "2021-08-06T00:51:45.592Z",
          "wordCount": 706,
          "title": "Fairness Properties of Face Recognition and Obfuscation Systems. (arXiv:2108.02707v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.06028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_Z/0/1/0/all/0/1\">Ziqi Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhichao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Naiyan Wang</a>",
          "description": "Estimating the states of surrounding traffic participants stays at the core\nof autonomous driving. In this paper, we study a novel setting of this problem:\nmodel-free single-object tracking (SOT), which takes the object state in the\nfirst frame as input, and jointly solves state estimation and tracking in\nsubsequent frames. The main purpose for this new setting is to break the strong\nlimitation of the popular \"detection and tracking\" scheme in multi-object\ntracking. Moreover, we notice that shape completion by overlaying the point\nclouds, which is a by-product of our proposed task, not only improves the\nperformance of state estimation but also has numerous applications. As no\nbenchmark for this task is available so far, we construct a new dataset\nLiDAR-SOT and corresponding evaluation protocols based on the Waymo Open\ndataset. We then propose an optimization-based algorithm called SOTracker\ninvolving point cloud registration, vehicle shapes, correspondence, and motion\npriors. Our quantitative and qualitative results prove the effectiveness of our\nSOTracker and reveal the challenging cases for SOT in point clouds, including\nthe sparsity of LiDAR data, abrupt motion variation, etc. Finally, we also\nexplore how the proposed task and algorithm may benefit other autonomous\ndriving applications, including simulating LiDAR scans, generating motion data,\nand annotating optical flow. The code and protocols for our benchmark and\nalgorithm are available at https://github.com/TuSimple/LiDAR_SOT/. A video\ndemonstration is at https://www.youtube.com/watch?v=BpHixKs91i8.",
          "link": "http://arxiv.org/abs/2103.06028",
          "publishedOn": "2021-08-06T00:51:45.576Z",
          "wordCount": 705,
          "title": "Model-free Vehicle Tracking and State Estimation in Point Cloud Sequences. (arXiv:2103.06028v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Han Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "In this paper, we propose a generalizable mixed-precision quantization (GMPQ)\nmethod for efficient inference. Conventional methods require the consistency of\ndatasets for bitwidth search and model deployment to guarantee the policy\noptimality, leading to heavy search cost on challenging largescale datasets in\nrealistic applications. On the contrary, our GMPQ searches the\nmixed-quantization policy that can be generalized to largescale datasets with\nonly a small amount of data, so that the search cost is significantly reduced\nwithout performance degradation. Specifically, we observe that locating network\nattribution correctly is general ability for accurate visual analysis across\ndifferent data distribution. Therefore, despite of pursuing higher model\naccuracy and complexity, we preserve attribution rank consistency between the\nquantized models and their full-precision counterparts via efficient\ncapacity-aware attribution imitation for generalizable mixed-precision\nquantization strategy search. Extensive experiments show that our method\nobtains competitive accuracy-complexity trade-off compared with the\nstate-of-the-art mixed-precision networks in significantly reduced search cost.\nThe code is available at https://github.com/ZiweiWangTHU/GMPQ.git.",
          "link": "http://arxiv.org/abs/2108.02720",
          "publishedOn": "2021-08-06T00:51:45.568Z",
          "wordCount": 598,
          "title": "Generalizable Mixed-Precision Quantization via Attribution Rank Preservation. (arXiv:2108.02720v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng-Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun-Yan Zhu</a>",
          "description": "Can a user create a deep generative model by sketching a single example?\nTraditionally, creating a GAN model has required the collection of a\nlarge-scale dataset of exemplars and specialized knowledge in deep learning. In\ncontrast, sketching is possibly the most universally accessible way to convey a\nvisual concept. In this work, we present a method, GAN Sketching, for rewriting\nGANs with one or more sketches, to make GANs training easier for novice users.\nIn particular, we change the weights of an original GAN model according to user\nsketches. We encourage the model's output to match the user sketches through a\ncross-domain adversarial loss. Furthermore, we explore different regularization\nmethods to preserve the original model's diversity and image quality.\nExperiments have shown that our method can mold GANs to match shapes and poses\nspecified by sketches while maintaining realism and diversity. Finally, we\ndemonstrate a few applications of the resulting GAN, including latent space\ninterpolation and image editing.",
          "link": "http://arxiv.org/abs/2108.02774",
          "publishedOn": "2021-08-06T00:51:45.561Z",
          "wordCount": 599,
          "title": "Sketch Your Own GAN. (arXiv:2108.02774v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1911.07158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fuxun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karianakis_N/0/1/0/all/0/1\">Nikolaos Karianakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Pei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lymberopoulos_D/0/1/0/all/0/1\">Dimitrios Lymberopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Sidi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weisong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>",
          "description": "Current state-of-the-art object detectors can have significant performance\ndrop when deployed in the wild due to domain gaps with training data.\nUnsupervised Domain Adaptation (UDA) is a promising approach to adapt models\nfor new domains/environments without any expensive label cost. However, without\nground truth labels, most prior works on UDA for object detection tasks can\nonly perform coarse image-level and/or feature-level adaptation by using\nadversarial learning methods. In this work, we show that such adversarial-based\nmethods can only reduce the domain style gap, but cannot address the domain\ncontent distribution gap that is shown to be important for object detectors. To\novercome this limitation, we propose the Cross-Domain Semi-Supervised Learning\n(CDSSL) framework by leveraging high-quality pseudo labels to learn better\nrepresentations from the target domain directly. To enable SSL for cross-domain\nobject detection, we propose fine-grained domain transfer,\nprogressive-confidence-based label sharpening and imbalanced sampling strategy\nto address two challenges: (i) non-identical distribution between source and\ntarget domain data, (ii) error amplification/accumulation due to noisy pseudo\nlabeling on the target domain. Experiment results show that our proposed\napproach consistently achieves new state-of-the-art performance (2.2% - 9.5%\nbetter than prior best work on mAP) under various domain gap scenarios. The\ncode will be released.",
          "link": "http://arxiv.org/abs/1911.07158",
          "publishedOn": "2021-08-06T00:51:45.555Z",
          "wordCount": 715,
          "title": "Unsupervised Domain Adaptation for Object Detection via Cross-Domain Semi-Supervised Learning. (arXiv:1911.07158v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khorrami_K/0/1/0/all/0/1\">Khazar Khorrami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>",
          "description": "Systems that can find correspondences between multiple modalities, such as\nbetween speech and images, have great potential to solve different recognition\nand data analysis tasks in an unsupervised manner. This work studies multimodal\nlearning in the context of visually grounded speech (VGS) models, and focuses\non their recently demonstrated capability to extract spatiotemporal alignments\nbetween spoken words and the corresponding visual objects without ever been\nexplicitly trained for object localization or word recognition. As the main\ncontributions, we formalize the alignment problem in terms of an audiovisual\nalignment tensor that is based on earlier VGS work, introduce systematic\nmetrics for evaluating model performance in aligning visual objects and spoken\nwords, and propose a new VGS model variant for the alignment task utilizing\ncross-modal attention layer. We test our model and a previously proposed model\nin the alignment task using SPEECH-COCO captions coupled with MSCOCO images. We\ncompare the alignment performance using our proposed evaluation metrics to the\nsemantic retrieval task commonly used to evaluate VGS models. We show that\ncross-modal attention layer not only helps the model to achieve higher semantic\ncross-modal retrieval performance, but also leads to substantial improvements\nin the alignment performance between image object and spoken words.",
          "link": "http://arxiv.org/abs/2108.02562",
          "publishedOn": "2021-08-06T00:51:45.531Z",
          "wordCount": 652,
          "title": "Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models. (arXiv:2108.02562v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aydin_A/0/1/0/all/0/1\">Ayberk Aydin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_D/0/1/0/all/0/1\">Deniz Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karli_B/0/1/0/all/0/1\">Berat Tuna Karli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanoglu_O/0/1/0/all/0/1\">Oguz Hanoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Temizel_A/0/1/0/all/0/1\">Alptekin Temizel</a>",
          "description": "Deep Neural Networks have been shown to be vulnerable to various kinds of\nadversarial perturbations. In addition to widely studied additive noise based\nperturbations, adversarial examples can also be created by applying a per pixel\nspatial drift on input images. While spatial transformation based adversarial\nexamples look more natural to human observers due to absence of additive noise,\nthey still possess visible distortions caused by spatial transformations. Since\nthe human vision is more sensitive to the distortions in the luminance compared\nto those in chrominance channels, which is one of the main ideas behind the\nlossy visual multimedia compression standards, we propose a spatial\ntransformation based perturbation method to create adversarial examples by only\nmodifying the color components of an input image. While having competitive\nfooling rates on CIFAR-10 and NIPS2017 Adversarial Learning Challenge datasets,\nexamples created with the proposed method have better scores with regards to\nvarious perceptual quality metrics. Human visual perception studies validate\nthat the examples are more natural looking and often indistinguishable from\ntheir original counterparts.",
          "link": "http://arxiv.org/abs/2108.02502",
          "publishedOn": "2021-08-06T00:51:45.519Z",
          "wordCount": 625,
          "title": "Imperceptible Adversarial Examples by Spatial Chroma-Shift. (arXiv:2108.02502v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xinxin Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Ji Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhenbo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Minglun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>",
          "description": "Given a picture of a chair, could we extract the 3-D shape of the chair,\nanimate its plausible articulations and motions, and render in-situ in its\noriginal image space? The above question prompts us to devise an automated\napproach to extract and manipulate articulated objects in single images.\nComparing with previous efforts on object manipulation, our work goes beyond\n2-D manipulation and focuses on articulable objects, thus introduces greater\nflexibility for possible object deformations. The pipeline of our approach\nstarts by reconstructing and refining a 3-D mesh representation of the object\nof interest from an input image; its control joints are predicted by exploiting\nthe semantic part segmentation information; the obtained object 3-D mesh is\nthen rigged \\& animated by non-rigid deformation, and rendered to perform\nin-situ motions in its original image space. Quantitative evaluations are\ncarried out on 3-D reconstruction from single images, an established task that\nis related to our pipeline, where our results surpass those of the SOTAs by a\nnoticeable margin. Extensive visual results also demonstrate the applicability\nof our approach.",
          "link": "http://arxiv.org/abs/2108.02708",
          "publishedOn": "2021-08-06T00:51:45.501Z",
          "wordCount": 629,
          "title": "Object Wake-up: 3-D Object Reconstruction, Animation, and in-situ Rendering from a Single Image. (arXiv:2108.02708v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02607",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Susan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilin Chen</a>",
          "description": "We introduce a new efficient framework, the Unified Context Network (UniCon),\nfor robust active speaker detection (ASD). Traditional methods for ASD usually\noperate on each candidate's pre-cropped face track separately and do not\nsufficiently consider the relationships among the candidates. This potentially\nlimits performance, especially in challenging scenarios with low-resolution\nfaces, multiple candidates, etc. Our solution is a novel, unified framework\nthat focuses on jointly modeling multiple types of contextual information:\nspatial context to indicate the position and scale of each candidate's face,\nrelational context to capture the visual relationships among the candidates and\ncontrast audio-visual affinities with each other, and temporal context to\naggregate long-term information and smooth out local uncertainties. Based on\nsuch information, our model optimizes all candidates in a unified process for\nrobust and reliable ASD. A thorough ablation study is performed on several\nchallenging ASD benchmarks under different settings. In particular, our method\noutperforms the state-of-the-art by a large margin of about 15% mean Average\nPrecision (mAP) absolute on two challenging subsets: one with three candidate\nspeakers, and the other with faces smaller than 64 pixels. Together, our UniCon\nachieves 92.0% mAP on the AVA-ActiveSpeaker validation set, surpassing 90% for\nthe first time on this challenging dataset at the time of submission. Project\nwebsite: https://unicon-asd.github.io/.",
          "link": "http://arxiv.org/abs/2108.02607",
          "publishedOn": "2021-08-06T00:51:45.493Z",
          "wordCount": 688,
          "title": "UniCon: Unified Context Network for Robust Active Speaker Detection. (arXiv:2108.02607v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiemin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>",
          "description": "Instance segmentation on point clouds is a fundamental task in 3D scene\nperception. In this work, we propose a concise clustering-based framework named\nHAIS, which makes full use of spatial relation of points and point sets.\nConsidering clustering-based methods may result in over-segmentation or\nunder-segmentation, we introduce the hierarchical aggregation to progressively\ngenerate instance proposals, i.e., point aggregation for preliminarily\nclustering points to sets and set aggregation for generating complete instances\nfrom sets. Once the complete 3D instances are obtained, a sub-network of\nintra-instance prediction is adopted for noisy points filtering and mask\nquality scoring. HAIS is fast (only 410ms per frame) and does not require\nnon-maximum suppression. It ranks 1st on the ScanNet v2 benchmark, achieving\nthe highest 69.9% AP50 and surpassing previous state-of-the-art (SOTA) methods\nby a large margin. Besides, the SOTA results on the S3DIS dataset validate the\ngood generalization ability. Code will be available at\nhttps://github.com/hustvl/HAIS.",
          "link": "http://arxiv.org/abs/2108.02350",
          "publishedOn": "2021-08-06T00:51:45.483Z",
          "wordCount": 591,
          "title": "Hierarchical Aggregation for 3D Instance Segmentation. (arXiv:2108.02350v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuxin Ding</a>",
          "description": "Single online handwritten Chinese character recognition~(single OLHCCR) has\nachieved prominent performance. However, in real application scenarios, users\nalways write multiple Chinese characters to form one complete sentence and the\ncontextual information within these characters holds the significant potential\nto improve the accuracy, robustness and efficiency of sentence-level OLHCCR. In\nthis work, we first propose a simple and straightforward end-to-end network,\nnamely vanilla compositional network~(VCN) to tackle the sentence-level OLHCCR.\nIt couples convolutional neural network with sequence modeling architecture to\nexploit the handwritten character's previous contextual information. Although\nVCN performs much better than the state-of-the-art single OLHCCR model, it\nexposes high fragility when confronting with not well written characters such\nas sloppy writing, missing or broken strokes. To improve the robustness of\nsentence-level OLHCCR, we further propose a novel deep spatial-temporal fusion\nnetwork~(DSTFN). It utilizes a pre-trained autoregresssive framework as the\nbackbone component, which projects each Chinese character into word embeddings,\nand integrates the spatial glyph features of handwritten characters and their\ncontextual information multiple times at multi-layer fusion module. We also\nconstruct a large-scale sentence-level handwriting dataset, named as CSOHD to\nevaluate models. Extensive experiment results demonstrate that DSTFN achieves\nthe state-of-the-art performance, which presents strong robustness compared\nwith VCN and exiting single OLHCCR models. The in-depth empirical analysis and\ncase studies indicate that DSTFN can significantly improve the efficiency of\nhandwriting input, with the handwritten Chinese character with incomplete\nstrokes being recognized precisely.",
          "link": "http://arxiv.org/abs/2108.02561",
          "publishedOn": "2021-08-06T00:51:45.466Z",
          "wordCount": 682,
          "title": "Sentence-level Online Handwritten Chinese Character Recognition. (arXiv:2108.02561v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02476",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1\">Hui Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Weizhen Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Li Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>",
          "description": "Differentiation of colorectal polyps is an important clinical examination. A\ncomputer-aided diagnosis system is required to assist accurate diagnosis from\ncolonoscopy images. Most previous studies at-tempt to develop models for polyp\ndifferentiation using Narrow-Band Imaging (NBI) or other enhanced images.\nHowever, the wide range of these models' applications for clinical work has\nbeen limited by the lagging of imaging techniques. Thus, we propose a novel\nframework based on a teacher-student architecture for the accurate colorectal\npolyp classification (CPC) through directly using white-light (WL) colonoscopy\nimages in the examination. In practice, during training, the auxiliary NBI\nimages are utilized to train a teacher network and guide the student network to\nacquire richer feature representation from WL images. The feature transfer is\nrealized by domain alignment and contrastive learning. Eventually the final\nstudent network has the ability to extract aligned features from only WL images\nto facilitate the CPC task. Besides, we release the first public-available\npaired CPC dataset containing WL-NBI pairs for the alignment training.\nQuantitative and qualitative evaluation indicates that the proposed method\noutperforms the previous methods in CPC, improving the accuracy by 5.6%with\nvery fast speed.",
          "link": "http://arxiv.org/abs/2108.02476",
          "publishedOn": "2021-08-06T00:51:45.458Z",
          "wordCount": 636,
          "title": "Colorectal Polyp Classification from White-light Colonoscopy Images via Domain Alignment. (arXiv:2108.02476v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02557",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Barina_D/0/1/0/all/0/1\">David Barina</a>",
          "description": "In recent years, a bag with image and video compression formats has been\ntorn. However, most of them are focused on lossy compression and only\nmarginally support the lossless mode. In this paper, I will focus on lossless\nformats and the critical question: \"Which one is the most efficient?\" It turned\nout that FLIF is currently the most efficient format for lossless image\ncompression. This finding is in contrast to that FLIF developers stopped its\ndevelopment in favor of JPEG XL.",
          "link": "http://arxiv.org/abs/2108.02557",
          "publishedOn": "2021-08-06T00:51:45.451Z",
          "wordCount": 520,
          "title": "Comparison of Lossless Image Formats. (arXiv:2108.02557v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiange Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fangfang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsui_K/0/1/0/all/0/1\">Kwok-Leung Tsui</a>",
          "description": "Automatic detection of rail track and its fasteners via using continuously\ncollected railway images is important to maintenance as it can significantly\nimprove maintenance efficiency and better ensure system safety. Dominant\ncomputer vision-based detection models typically rely on convolutional neural\nnetworks that utilize local image features and cumbersome prior settings to\ngenerate candidate boxes. In this paper, we propose a deep convolutional\ntransformer network based method to detect multi-class rail components\nincluding the rail, clip, and bolt. We effectively synergize advantages of the\nconvolutional structure on extracting latent features from raw images as well\nas advantages of transformers on selectively determining valuable latent\nfeatures to achieve an efficient and accurate performance on rail component\ndetections. Our proposed method simplifies the detection pipeline by\neliminating the need of prior settings, such as anchor box, aspect ratio,\ndefault coordinates, and post-processing, such as the threshold for non-maximum\nsuppression; as well as allows users to trade off the quality and complexity of\nthe detector with limited training data. Results of a comprehensive\ncomputational study show that our proposed method outperforms a set of existing\nstate-of-art approaches with large margins",
          "link": "http://arxiv.org/abs/2108.02423",
          "publishedOn": "2021-08-06T00:51:45.444Z",
          "wordCount": 626,
          "title": "Automatic Detection of Rail Components via A Deep Convolutional Transformer Network. (arXiv:2108.02423v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delbruck_T/0/1/0/all/0/1\">Tobi Delbruck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shih-Chii Liu</a>",
          "description": "Long Short-Term Memory (LSTM) recurrent networks are frequently used for\ntasks involving time sequential data such as speech recognition. However, it is\ndifficult to deploy these networks on hardware to achieve high throughput and\nlow latency because the fully-connected structure makes LSTM networks a\nmemory-bounded algorithm. Previous work in LSTM accelerators either exploited\nweight spatial sparsity or temporal sparsity. In this paper, we present a new\naccelerator called \"Spartus\" that exploits spatio-temporal sparsity to achieve\nultra-low latency inference. The spatial sparsity was induced using our\nproposed pruning method called Column-Balanced Targeted Dropout (CBTD) that\nleads to structured sparse weight matrices benefiting workload balance. It\nachieved up to 96% weight sparsity with negligible accuracy difference for an\nLSTM network trained on a TIMIT phone recognition task. To induce temporal\nsparsity in LSTM, we create the DeltaLSTM by extending the previous DeltaGRU\nmethod to the LSTM network. This combined sparsity saves on weight memory\naccess and associated arithmetic operations simultaneously. Spartus was\nimplemented on a Xilinx Zynq-7100 FPGA. The per-sample latency for a single\nDeltaLSTM layer of 1024 neurons running on Spartus is 1 us. Spartus achieved\n9.4 TOp/s effective batch-1 throughput and 1.1 TOp/J energy efficiency, which\nare respectively 4X and 7X higher than the previous state-of-the-art.",
          "link": "http://arxiv.org/abs/2108.02297",
          "publishedOn": "2021-08-06T00:51:45.436Z",
          "wordCount": 653,
          "title": "Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-temporal Sparsity. (arXiv:2108.02297v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "The advancements in deep learning technologies have produced immense\ncontribution to biomedical image analysis applications. With breast cancer\nbeing the common deadliest disease among women, early detection is the key\nmeans to improve survivability. Medical imaging like ultrasound presents an\nexcellent visual representation of the functioning of the organs; however, for\nany radiologist analysing such scans is challenging and time consuming which\ndelays the diagnosis process. Although various deep learning based approaches\nare proposed that achieved promising results, the present article introduces an\nefficient residual cross-spatial attention guided inception U-Net (RCA-IUnet)\nmodel with minimal training parameters for tumor segmentation using breast\nultrasound imaging to further improve the segmentation performance of varying\ntumor sizes. The RCA-IUnet model follows U-Net topology with residual inception\ndepth-wise separable convolution and hybrid pooling (max pooling and spectral\npooling) layers. In addition, cross-spatial attention filters are added to\nsuppress the irrelevant features and focus on the target structure. The\nsegmentation performance of the proposed model is validated on two publicly\navailable datasets using standard segmentation evaluation metrics, where it\noutperformed the other state-of-the-art segmentation models.",
          "link": "http://arxiv.org/abs/2108.02508",
          "publishedOn": "2021-08-06T00:51:45.429Z",
          "wordCount": 628,
          "title": "RCA-IUnet: A residual cross-spatial attention guided inception U-Net model for tumor segmentation in breast ultrasound imaging. (arXiv:2108.02508v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02560",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1\">Zhenyu Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuesheng Zhu</a>",
          "description": "Online hashing methods usually learn the hash functions online, aiming to\nefficiently adapt to the data variations in the streaming environment. However,\nwhen the hash functions are updated, the binary codes for the whole database\nhave to be updated to be consistent with the hash functions, resulting in the\ninefficiency in the online image retrieval process. In this paper, we propose a\nnovel online hashing framework without updating binary codes. In the proposed\nframework, the hash functions are fixed and a parametric similarity function\nfor the binary codes is learnt online to adapt to the streaming data.\nSpecifically, a parametric similarity function that has a bilinear form is\nadopted and a metric learning algorithm is proposed to learn the similarity\nfunction online based on the characteristics of the hashing methods. The\nexperiments on two multi-label image datasets show that our method is\ncompetitive or outperforms the state-of-the-art online hashing methods in terms\nof both accuracy and efficiency for multi-label image retrieval.",
          "link": "http://arxiv.org/abs/2108.02560",
          "publishedOn": "2021-08-06T00:51:45.421Z",
          "wordCount": 590,
          "title": "Online Hashing with Similarity Learning. (arXiv:2108.02560v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Han Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zehua Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>",
          "description": "The intellectual property (IP) of Deep neural networks (DNNs) can be easily\n``stolen'' by surrogate model attack. There has been significant progress in\nsolutions to protect the IP of DNN models in classification tasks. However,\nlittle attention has been devoted to the protection of DNNs in image processing\ntasks. By utilizing consistent invisible spatial watermarks, one recent work\nfirst considered model watermarking for deep image processing networks and\ndemonstrated its efficacy in many downstream tasks. Nevertheless, it highly\ndepends on the hypothesis that the embedded watermarks in the network outputs\nare consistent. When the attacker uses some common data augmentation attacks\n(e.g., rotate, crop, and resize) during surrogate model training, it will\ntotally fail because the underlying watermark consistency is destroyed. To\nmitigate this issue, we propose a new watermarking methodology, namely\n``structure consistency'', based on which a new deep structure-aligned model\nwatermarking algorithm is designed. Specifically, the embedded watermarks are\ndesigned to be aligned with physically consistent image structures, such as\nedges or semantic regions. Experiments demonstrate that our method is much more\nrobust than the baseline method in resisting data augmentation attacks for\nmodel IP protection. Besides that, we further test the generalization ability\nand robustness of our method to a broader range of circumvention attacks.",
          "link": "http://arxiv.org/abs/2108.02360",
          "publishedOn": "2021-08-06T00:51:45.414Z",
          "wordCount": 653,
          "title": "Exploring Structure Consistency for Deep Model Watermarking. (arXiv:2108.02360v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+zhang_J/0/1/0/all/0/1\">Jie zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qidong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>",
          "description": "Recent research shows deep neural networks are vulnerable to different types\nof attacks, such as adversarial attack, data poisoning attack and backdoor\nattack. Among them, backdoor attack is the most cunning one and can occur in\nalmost every stage of deep learning pipeline. Therefore, backdoor attack has\nattracted lots of interests from both academia and industry. However, most\nexisting backdoor attack methods are either visible or fragile to some\neffortless pre-processing such as common data transformations. To address these\nlimitations, we propose a robust and invisible backdoor attack called ``Poison\nInk''. Concretely, we first leverage the image structures as target poisoning\nareas, and fill them with poison ink (information) to generate the trigger\npattern. As the image structure can keep its semantic meaning during the data\ntransformation, such trigger pattern is inherently robust to data\ntransformations. Then we leverage a deep injection network to embed such\ntrigger pattern into the cover image to achieve stealthiness. Compared to\nexisting popular backdoor attack methods, Poison Ink outperforms both in\nstealthiness and robustness. Through extensive experiments, we demonstrate\nPoison Ink is not only general to different datasets and network architectures,\nbut also flexible for different attack scenarios. Besides, it also has very\nstrong resistance against many state-of-the-art defense techniques.",
          "link": "http://arxiv.org/abs/2108.02488",
          "publishedOn": "2021-08-06T00:51:45.389Z",
          "wordCount": 649,
          "title": "Poison Ink: Robust and Invisible Backdoor Attack. (arXiv:2108.02488v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>",
          "description": "Video captioning combines video understanding and language generation.\nDifferent from image captioning that describes a static image with details of\nalmost every object, video captioning usually considers a sequence of frames\nand biases towards focused objects, e.g., the objects that stay in focus\nregardless of the changing background. Therefore, detecting and properly\naccommodating focused objects is critical in video captioning. To enforce the\ndescription of focused objects and achieve controllable video captioning, we\npropose an Object-Oriented Non-Autoregressive approach (O2NA), which performs\ncaption generation in three steps: 1) identify the focused objects and predict\ntheir locations in the target caption; 2) generate the related attribute words\nand relation words of these focused objects to form a draft caption; and 3)\ncombine video information to refine the draft caption to a fluent final\ncaption. Since the focused objects are generated and located ahead of other\nwords, it is difficult to apply the word-by-word autoregressive generation\nprocess; instead, we adopt a non-autoregressive approach. The experiments on\ntwo benchmark datasets, i.e., MSR-VTT and MSVD, demonstrate the effectiveness\nof O2NA, which achieves results competitive with the state-of-the-arts but with\nboth higher diversity and higher inference speed.",
          "link": "http://arxiv.org/abs/2108.02359",
          "publishedOn": "2021-08-06T00:51:45.370Z",
          "wordCount": 642,
          "title": "O2NA: An Object-Oriented Non-Autoregressive Approach for Controllable Video Captioning. (arXiv:2108.02359v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02574",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_F/0/1/0/all/0/1\">Fei Wen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1\">Zeyu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ying_R/0/1/0/all/0/1\">Rendong Ying</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peilin Liu</a>",
          "description": "Recently, much progress has been made in unsupervised restoration learning.\nHowever, existing methods more or less rely on some assumptions on the signal\nand/or degradation model, which limits their practical performance. How to\nconstruct an optimal criterion for unsupervised restoration learning without\nany prior knowledge on the degradation model is still an open question. Toward\nanswering this question, this work proposes a criterion for unsupervised\nrestoration learning based on the optimal transport theory. This criterion has\nfavorable properties, e.g., approximately maximal preservation of the\ninformation of the signal, whilst achieving perceptual reconstruction.\nFurthermore, though a relaxed unconstrained formulation is used in practical\nimplementation, we show that the relaxed formulation in theory has the same\nsolution as the original constrained formulation. Experiments on synthetic and\nreal-world data, including realistic photographic, microscopy, depth, and raw\ndepth images, demonstrate that the proposed method even compares favorably with\nsupervised methods, e.g., approaching the PSNR of supervised methods while\nhaving better perceptual quality. Particularly, for spatially correlated noise\nand realistic microscopy images, the proposed method not only achieves better\nperceptual quality but also has higher PSNR than supervised methods. Besides,\nit shows remarkable superiority in harsh practical conditions with complex\nnoise, e.g., raw depth images.",
          "link": "http://arxiv.org/abs/2108.02574",
          "publishedOn": "2021-08-06T00:51:45.355Z",
          "wordCount": 652,
          "title": "Optimal Transport for Unsupervised Restoration Learning. (arXiv:2108.02574v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_K/0/1/0/all/0/1\">Keck-Voon Ling</a>",
          "description": "In this work, we aim to address the challenging task of open set recognition\n(OSR). Many recent OSR methods rely on auto-encoders to extract class-specific\nfeatures by a reconstruction strategy, requiring the network to restore the\ninput image on pixel-level. This strategy is commonly over-demanding for OSR\nsince class-specific features are generally contained in target objects, not in\nall pixels. To address this shortcoming, here we discard the pixel-level\nreconstruction strategy and pay more attention to improving the effectiveness\nof class-specific feature extraction. We propose a mutual information-based\nmethod with a streamlined architecture, Maximal Mutual Information Open Set\nRecognition (M2IOSR). The proposed M2IOSR only uses an encoder to extract\nclass-specific features by maximizing the mutual information between the given\ninput and its latent features across multiple scales. Meanwhile, to further\nreduce the open space risk, latent features are constrained to class\nconditional Gaussian distributions by a KL-divergence loss function. In this\nway, a strong function is learned to prevent the network from mapping different\nobservations to similar latent features and help the network extract\nclass-specific features with desired statistical characteristics. The proposed\nmethod significantly improves the performance of baselines and achieves new\nstate-of-the-art results on several benchmarks consistently. Source codes are\nuploaded in supplementary materials.",
          "link": "http://arxiv.org/abs/2108.02373",
          "publishedOn": "2021-08-06T00:51:45.348Z",
          "wordCount": 641,
          "title": "M2IOSR: Maximal Mutual Information Open Set Recognition. (arXiv:2108.02373v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02559",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shixiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoman Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>",
          "description": "Annotating multiple organs in 3D medical images is time-consuming and costly.\nMeanwhile, there exist many single-organ datasets with one specific organ\nannotated. This paper investigates how to learn a multi-organ segmentation\nmodel leveraging a set of binary-labeled datasets. A novel Multi-teacher\nSingle-student Knowledge Distillation (MS-KD) framework is proposed, where the\nteacher models are pre-trained single-organ segmentation networks, and the\nstudent model is a multi-organ segmentation network. Considering that each\nteacher focuses on different organs, a region-based supervision method,\nconsisting of logits-wise supervision and feature-wise supervision, is\nproposed. Each teacher supervises the student in two regions, the organ region\nwhere the teacher is considered as an expert and the background region where\nall teachers agree. Extensive experiments on three public single-organ datasets\nand a multi-organ dataset have demonstrated the effectiveness of the proposed\nMS-KD framework.",
          "link": "http://arxiv.org/abs/2108.02559",
          "publishedOn": "2021-08-06T00:51:45.325Z",
          "wordCount": 569,
          "title": "MS-KD: Multi-Organ Segmentation with Multiple Binary-Labeled Datasets. (arXiv:2108.02559v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kutnar_D/0/1/0/all/0/1\">Denis Kutnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velden_B/0/1/0/all/0/1\">Bas H.M. van der Velden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanguesa_M/0/1/0/all/0/1\">Marta Girones Sanguesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geerlings_M/0/1/0/all/0/1\">Mirjam I. Geerlings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biesbroek_J/0/1/0/all/0/1\">J. Matthijs Biesbroek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijf_H/0/1/0/all/0/1\">Hugo J. Kuijf</a>",
          "description": "Lacunes of presumed vascular origin are fluid-filled cavities of between 3 -\n15 mm in diameter, visible on T1 and FLAIR brain MRI. Quantification of lacunes\nrelies on manual annotation or semi-automatic / interactive approaches; and\nalmost no automatic methods exist for this task. In this work, we present a\ntwo-stage approach to segment lacunes of presumed vascular origin: (1)\ndetection with Mask R-CNN followed by (2) segmentation with a U-Net CNN. Data\noriginates from Task 3 of the \"Where is VALDO?\" challenge and consists of 40\ntraining subjects. We report the mean DICE on the training set of 0.83 and on\nthe validation set of 0.84. Source code is available at:\nhttps://github.com/hjkuijf/MixLacune . The docker container hjkuijf/mixlacune\ncan be pulled from https://hub.docker.com/r/hjkuijf/mixlacune .",
          "link": "http://arxiv.org/abs/2108.02483",
          "publishedOn": "2021-08-06T00:51:45.318Z",
          "wordCount": 584,
          "title": "MixLacune: Segmentation of lacunes of presumed vascular origin. (arXiv:2108.02483v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Duo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yanye Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xuejing Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>",
          "description": "The nonlocal-based blocks are designed for capturing long-range\nspatial-temporal dependencies in computer vision tasks. Although having shown\nexcellent performance, they still lack the mechanism to encode the rich,\nstructured information among elements in an image or video. In this paper, to\ntheoretically analyze the property of these nonlocal-based blocks, we provide a\nnew perspective to interpret them, where we view them as a set of graph filters\ngenerated on a fully-connected graph. Specifically, when choosing the Chebyshev\ngraph filter, a unified formulation can be derived for explaining and analyzing\nthe existing nonlocal-based blocks (e.g., nonlocal block, nonlocal stage,\ndouble attention block). Furthermore, by concerning the property of spectral,\nwe propose an efficient and robust spectral nonlocal block, which can be more\nrobust and flexible to catch long-range dependencies when inserted into deep\nneural networks than the existing nonlocal blocks. Experimental results\ndemonstrate the clear-cut improvements and practical applicabilities of our\nmethod on image classification, action recognition, semantic segmentation, and\nperson re-identification tasks.",
          "link": "http://arxiv.org/abs/2108.02451",
          "publishedOn": "2021-08-06T00:51:45.311Z",
          "wordCount": 610,
          "title": "Unifying Nonlocal Blocks for Neural Networks. (arXiv:2108.02451v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zeren Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yazhou Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiu-Shen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongshun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1\">Fumin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng-Tao Shen</a>",
          "description": "Learning from the web can ease the extreme dependence of deep learning on\nlarge-scale manually labeled datasets. Especially for fine-grained recognition,\nwhich targets at distinguishing subordinate categories, it will significantly\nreduce the labeling costs by leveraging free web data. Despite its significant\npractical and research value, the webly supervised fine-grained recognition\nproblem is not extensively studied in the computer vision community, largely\ndue to the lack of high-quality datasets. To fill this gap, in this paper we\nconstruct two new benchmark webly supervised fine-grained datasets, termed\nWebFG-496 and WebiNat-5089, respectively. In concretely, WebFG-496 consists of\nthree sub-datasets containing a total of 53,339 web training images with 200\nspecies of birds (Web-bird), 100 types of aircrafts (Web-aircraft), and 196\nmodels of cars (Web-car). For WebiNat-5089, it contains 5089 sub-categories and\nmore than 1.1 million web training images, which is the largest webly\nsupervised fine-grained dataset ever. As a minor contribution, we also propose\na novel webly supervised method (termed ``{Peer-learning}'') for benchmarking\nthese datasets.~Comprehensive experimental results and analyses on two new\nbenchmark datasets demonstrate that the proposed method achieves superior\nperformance over the competing baseline models and states-of-the-art. Our\nbenchmark datasets and the source codes of Peer-learning have been made\navailable at\n{\\url{https://github.com/NUST-Machine-Intelligence-Laboratory/weblyFG-dataset}}.",
          "link": "http://arxiv.org/abs/2108.02399",
          "publishedOn": "2021-08-06T00:51:45.264Z",
          "wordCount": 655,
          "title": "Webly Supervised Fine-Grained Recognition: Benchmark Datasets and An Approach. (arXiv:2108.02399v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weihao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Michael Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "We design a multiscopic vision system that utilizes a low-cost monocular RGB\ncamera to acquire accurate depth estimation. Unlike multi-view stereo with\nimages captured at unconstrained camera poses, the proposed system controls the\nmotion of a camera to capture a sequence of images in horizontally or\nvertically aligned positions with the same parallax. In this system, we propose\na new heuristic method and a robust learning-based method to fuse multiple cost\nvolumes between the reference image and its surrounding images. To obtain\ntraining data, we build a synthetic dataset with multiscopic images. The\nexperiments on the real-world Middlebury dataset and real robot demonstration\nshow that our multiscopic vision system outperforms traditional two-frame\nstereo matching methods in depth estimation. Our code and dataset are available\nat \\url{https://sites.google.com/view/multiscopic",
          "link": "http://arxiv.org/abs/2108.02448",
          "publishedOn": "2021-08-06T00:51:45.258Z",
          "wordCount": 589,
          "title": "MFuseNet: Robust Depth Estimation with Learned Multiscopic Fusion. (arXiv:2108.02448v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yongxing Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zekun Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Ling-Yu Duan</a>",
          "description": "Unsupervised domain adaptive person re-identification (UDA re-ID) aims at\ntransferring the labeled source domain's knowledge to improve the model's\ndiscriminability on the unlabeled target domain. From a novel perspective, we\nargue that the bridging between the source and target domains can be utilized\nto tackle the UDA re-ID task, and we focus on explicitly modeling appropriate\nintermediate domains to characterize this bridging. Specifically, we propose an\nIntermediate Domain Module (IDM) to generate intermediate domains'\nrepresentations on-the-fly by mixing the source and target domains' hidden\nrepresentations using two domain factors. Based on the \"shortest geodesic path\"\ndefinition, i.e., the intermediate domains along the shortest geodesic path\nbetween the two extreme domains can play a better bridging role, we propose two\nproperties that these intermediate domains should satisfy. To ensure these two\nproperties to better characterize appropriate intermediate domains, we enforce\nthe bridge losses on intermediate domains' prediction space and feature space,\nand enforce a diversity loss on the two domain factors. The bridge losses aim\nat guiding the distribution of appropriate intermediate domains to keep the\nright distance to the source and target domains. The diversity loss serves as a\nregularization to prevent the generated intermediate domains from being\nover-fitting to either of the source and target domains. Our proposed method\noutperforms the state-of-the-arts by a large margin in all the common UDA re-ID\ntasks, and the mAP gain is up to 7.7% on the challenging MSMT17 benchmark. Code\nis available at https://github.com/SikaStar/IDM.",
          "link": "http://arxiv.org/abs/2108.02413",
          "publishedOn": "2021-08-06T00:51:45.235Z",
          "wordCount": 694,
          "title": "IDM: An Intermediate Domain Module for Domain Adaptive Person Re-ID. (arXiv:2108.02413v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunfei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>",
          "description": "Autonomous assembly has been a desired functionality of many intelligent\nrobot systems. We study a new challenging assembly task, designing and\nconstructing a bridge without a blueprint. In this task, the robot needs to\nfirst design a feasible bridge architecture for arbitrarily wide cliffs and\nthen manipulate the blocks reliably to construct a stable bridge according to\nthe proposed design. In this paper, we propose a bi-level approach to tackle\nthis task. At the high level, the system learns a bridge blueprint policy in a\nphysical simulator using deep reinforcement learning and curriculum learning. A\npolicy is represented as an attention-based neural network with object-centric\ninput, which enables generalization to different numbers of blocks and cliff\nwidths. For low-level control, we implement a motion-planning-based policy for\nreal-robot motion control, which can be directly combined with a trained\nblueprint policy for real-world bridge construction without tuning. In our\nfield study, our bi-level robot system demonstrates the capability of\nmanipulating blocks to construct a diverse set of bridges with different\narchitectures.",
          "link": "http://arxiv.org/abs/2108.02439",
          "publishedOn": "2021-08-06T00:51:45.210Z",
          "wordCount": 615,
          "title": "Learning to Design and Construct Bridge without Blueprint. (arXiv:2108.02439v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02425",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1\">Ruihang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Grasping in cluttered scenes has always been a great challenge for robots,\ndue to the requirement of the ability to well understand the scene and object\ninformation. Previous works usually assume that the geometry information of the\nobjects is available, or utilize a step-wise, multi-stage strategy to predict\nthe feasible 6-DoF grasp poses. In this work, we propose to formalize the 6-DoF\ngrasp pose estimation as a simultaneous multi-task learning problem. In a\nunified framework, we jointly predict the feasible 6-DoF grasp poses, instance\nsemantic segmentation, and collision information. The whole framework is\njointly optimized and end-to-end differentiable. Our model is evaluated on\nlarge-scale benchmarks as well as the real robot system. On the public dataset,\nour method outperforms prior state-of-the-art methods by a large margin (+4.08\nAP). We also demonstrate the implementation of our model on a real robotic\nplatform and show that the robot can accurately grasp target objects in\ncluttered scenarios with a high success rate. Project link:\nhttps://openbyterobotics.github.io/sscl",
          "link": "http://arxiv.org/abs/2108.02425",
          "publishedOn": "2021-08-06T00:51:45.203Z",
          "wordCount": 616,
          "title": "Simultaneous Semantic and Collision Learning for 6-DoF Grasp Pose Estimation. (arXiv:2108.02425v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Guang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhiping Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1\">En Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianping Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qing Liao</a>",
          "description": "Video abnormal event detection (VAD) is a vital semi-supervised task that\nrequires learning with only roughly labeled normal videos, as anomalies are\noften practically unavailable. Although deep neural networks (DNNs) enable\ngreat progress in VAD, existing solutions typically suffer from two issues: (1)\nThe precise and comprehensive localization of video events is ignored. (2) The\nvideo semantics and temporal context are under-explored. To address those\nissues, we are motivated by the prevalent cloze test in education and propose a\nnovel approach named visual cloze completion (VCC), which performs VAD by\nlearning to complete \"visual cloze tests\" (VCTs). Specifically, VCC first\nlocalizes each video event and encloses it into a spatio-temporal cube (STC).\nTo achieve both precise and comprehensive localization, appearance and motion\nare used as mutually complementary cues to mark the object region associated\nwith each video event. For each marked region, a normalized patch sequence is\nextracted from temporally adjacent frames and stacked into the STC. By\ncomparing each patch and the patch sequence of a STC to a visual \"word\" and\n\"sentence\" respectively, we can deliberately erase a certain \"word\" (patch) to\nyield a VCT. DNNs are then trained to infer the erased patch by video\nsemantics, so as to complete the VCT. To fully exploit the temporal context,\neach patch in STC is alternatively erased to create multiple VCTs, and the\nerased patch's optical flow is also inferred to integrate richer motion clues.\nMeanwhile, a new DNN architecture is designed as a model-level solution to\nutilize video semantics and temporal context. Extensive experiments demonstrate\nthat VCC achieves state-of-the-art VAD performance. Our codes and results are\nopen at \\url{https://github.com/yuguangnudt/VEC_VAD/tree/VCC}",
          "link": "http://arxiv.org/abs/2108.02356",
          "publishedOn": "2021-08-06T00:51:45.188Z",
          "wordCount": 729,
          "title": "Video Abnormal Event Detection by Learning to Complete Visual Cloze Tests. (arXiv:2108.02356v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Minghang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>",
          "description": "The recently proposed Detection Transformer (DETR) model successfully applies\nTransformer to objects detection and achieves comparable performance with\ntwo-stage object detection frameworks, such as Faster-RCNN. However, DETR\nsuffers from its slow convergence. Training DETR from scratch needs 500 epochs\nto achieve a high accuracy. To accelerate its convergence, we propose a simple\nyet effective scheme for improving the DETR framework, namely Spatially\nModulated Co-Attention (SMCA) mechanism. The core idea of SMCA is to conduct\nlocation-aware co-attention in DETR by constraining co-attention responses to\nbe high near initially estimated bounding box locations. Our proposed SMCA\nincreases DETR's convergence speed by replacing the original co-attention\nmechanism in the decoder while keeping other operations in DETR unchanged.\nFurthermore, by integrating multi-head and scale-selection attention designs\ninto SMCA, our fully-fledged SMCA can achieve better performance compared to\nDETR with a dilated convolution-based backbone (45.6 mAP at 108 epochs vs. 43.3\nmAP at 500 epochs). We perform extensive ablation studies on COCO dataset to\nvalidate SMCA. Code is released at https://github.com/gaopengcuhk/SMCA-DETR .",
          "link": "http://arxiv.org/abs/2108.02404",
          "publishedOn": "2021-08-06T00:51:45.179Z",
          "wordCount": 611,
          "title": "Fast Convergence of DETR with Spatially Modulated Co-Attention. (arXiv:2108.02404v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02348",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanhui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1\">Xiao Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaolin Wu</a>",
          "description": "For deep learning methods of image super-resolution, the most critical issue\nis whether the paired low and high resolution images for training accurately\nreflect the sampling process of real cameras. Low and high resolution\n(LR$\\sim$HR) image pairs synthesized by existing degradation models (\\eg,\nbicubic downsampling) deviate from those in reality; thus the super-resolution\nCNN trained by these synthesized LR$\\sim$HR image pairs does not perform well\nwhen being applied to real images. In this paper, we propose a novel method to\ncapture a large set of realistic LR$\\sim$HR image pairs using real cameras.The\ndata acquisition is carried out under controllable lab conditions with minimum\nhuman intervention and at high throughput (about 500 image pairs per hour). The\nhigh level of automation makes it easy to produce a set of real LR$\\sim$HR\ntraining image pairs for each camera. Our innovation is to shoot images\ndisplayed on an ultra-high quality screen at different resolutions.There are\nthree distinctive advantages with our method that allow us to collect\nhigh-quality training datasets for image super-resolution. First, as the LR and\nHR images are taken of a 3D planar surface (the screen) the registration\nproblem fits exactly to a homography model. Second, we can display special\nmarkers on the image margin to further improve the registration\nprecision.Third, the displayed digital image file can be exploited as a\nreference to optimize the high frequency content of the restored image.\nExperimental results show that training a super-resolution CNN by our\nLR$\\sim$HR dataset has superior restoration performance than training it by\nexisting datasets on real world images at the inference stage.",
          "link": "http://arxiv.org/abs/2108.02348",
          "publishedOn": "2021-08-06T00:51:45.161Z",
          "wordCount": 699,
          "title": "Dual-reference Training Data Acquisition and CNN Construction for Image Super-Resolution. (arXiv:2108.02348v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Richard J. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Ming Y. Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_D/0/1/0/all/0/1\">Drew F. K. Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tiffany Y. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipkova_J/0/1/0/all/0/1\">Jana Lipkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaban_M/0/1/0/all/0/1\">Muhammad Shaban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shady_M/0/1/0/all/0/1\">Maha Shady</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1\">Mane Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_B/0/1/0/all/0/1\">Bumjin Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noor_Z/0/1/0/all/0/1\">Zahra Noor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1\">Faisal Mahmood</a>",
          "description": "The rapidly emerging field of deep learning-based computational pathology has\ndemonstrated promise in developing objective prognostic models from histology\nwhole slide images. However, most prognostic models are either based on\nhistology or genomics alone and do not address how histology and genomics can\nbe integrated to develop joint image-omic prognostic models. Additionally\nidentifying explainable morphological and molecular descriptors from these\nmodels that govern such prognosis is of interest. We used multimodal deep\nlearning to integrate gigapixel whole slide pathology images, RNA-seq\nabundance, copy number variation, and mutation data from 5,720 patients across\n14 major cancer types. Our interpretable, weakly-supervised, multimodal deep\nlearning algorithm is able to fuse these heterogeneous modalities for\npredicting outcomes and discover prognostic features from these modalities that\ncorroborate with poor and favorable outcomes via multimodal interpretability.\nWe compared our model with unimodal deep learning models trained on histology\nslides and molecular profiles alone, and demonstrate performance increase in\nrisk stratification on 9 out of 14 cancers. In addition, we analyze morphologic\nand molecular markers responsible for prognostic predictions across all cancer\ntypes. All analyzed data, including morphological and molecular correlates of\npatient prognosis across the 14 cancer types at a disease and patient level are\npresented in an interactive open-access database\n(this http URL) to allow for further exploration and\nprognostic biomarker discovery. To validate that these model explanations are\nprognostic, we further analyzed high attention morphological regions in WSIs,\nwhich indicates that tumor-infiltrating lymphocyte presence corroborates with\nfavorable cancer prognosis on 9 out of 14 cancer types studied.",
          "link": "http://arxiv.org/abs/2108.02278",
          "publishedOn": "2021-08-06T00:51:45.144Z",
          "wordCount": 733,
          "title": "Pan-Cancer Integrative Histology-Genomic Analysis via Interpretable Multimodal Deep Learning. (arXiv:2108.02278v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xuri Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fuhai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1\">Joemon M. Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhilong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>",
          "description": "The current state-of-the-art image-sentence retrieval methods implicitly\nalign the visual-textual fragments, like regions in images and words in\nsentences, and adopt attention modules to highlight the relevance of\ncross-modal semantic correspondences. However, the retrieval performance\nremains unsatisfactory due to a lack of consistent representation in both\nsemantics and structural spaces. In this work, we propose to address the above\nissue from two aspects: (i) constructing intrinsic structure (along with\nrelations) among the fragments of respective modalities, e.g., \"dog $\\to$ play\n$\\to$ ball\" in semantic structure for an image, and (ii) seeking explicit\ninter-modal structural and semantic correspondence between the visual and\ntextual modalities. In this paper, we propose a novel Structured Multi-modal\nFeature Embedding and Alignment (SMFEA) model for image-sentence retrieval. In\norder to jointly and explicitly learn the visual-textual embedding and the\ncross-modal alignment, SMFEA creates a novel multi-modal structured module with\na shared context-aware referral tree. In particular, the relations of the\nvisual and textual fragments are modeled by constructing Visual Context-aware\nStructured Tree encoder (VCS-Tree) and Textual Context-aware Structured Tree\nencoder (TCS-Tree) with shared labels, from which visual and textual features\ncan be jointly learned and optimized. We utilize the multi-modal tree structure\nto explicitly align the heterogeneous image-sentence data by maximizing the\nsemantic and structural similarity between corresponding inter-modal tree\nnodes. Extensive experiments on Microsoft COCO and Flickr30K benchmarks\ndemonstrate the superiority of the proposed model in comparison to the\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.02417",
          "publishedOn": "2021-08-06T00:51:45.136Z",
          "wordCount": 690,
          "title": "Structured Multi-modal Feature Embedding and Alignment for Image-Sentence Retrieval. (arXiv:2108.02417v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiange Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fangfang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsui_K/0/1/0/all/0/1\">Kwok-Leung Tsui</a>",
          "description": "Automated inspection and detection of foreign objects on railways is\nimportant for rail transportation safety as it helps prevent potential\naccidents and trains derailment. Most existing vision-based approaches focus on\nthe detection of frontal intrusion objects with prior labels, such as\ncategories and locations of the objects. In reality, foreign objects with\nunknown categories can appear anytime on railway tracks. In this paper, we\ndevelop a semi-supervised convolutional autoencoder based framework that only\nrequires railway track images without prior knowledge on the foreign objects in\nthe training process. It consists of three different modules, a bottleneck\nfeature generator as encoder, a photographic image generator as decoder, and a\nreconstruction discriminator developed via adversarial learning. In the\nproposed framework, the problem of detecting the presence, location, and shape\nof foreign objects is addressed by comparing the input and reconstructed images\nas well as setting thresholds based on reconstruction errors. The proposed\nmethod is evaluated through comprehensive studies under different performance\ncriteria. The results show that the proposed method outperforms some well-known\nbenchmarking methods. The proposed framework is useful for data analytics via\nthe train Internet-of-Things (IoT) systems",
          "link": "http://arxiv.org/abs/2108.02421",
          "publishedOn": "2021-08-06T00:51:45.129Z",
          "wordCount": 626,
          "title": "Intelligent Railway Foreign Object Detection: A Semi-supervised Convolutional Autoencoder Based Method. (arXiv:2108.02421v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weijie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang%2A_C/0/1/0/all/0/1\">Chong Wang*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haohe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shenghao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Song Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xulun Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiafei Wu</a>",
          "description": "Expensive bounding-box annotations have limited the development of object\ndetection task. Thus, it is necessary to focus on more challenging task of\nfew-shot object detection. It requires the detector to recognize objects of\nnovel classes with only a few training samples. Nowadays, many existing popular\nmethods based on meta-learning have achieved promising performance, such as\nMeta R-CNN series. However, only a single category of support data is used as\nthe attention to guide the detecting of query images each time. Their relevance\nto each other remains unexploited. Moreover, a lot of recent works treat the\nsupport data and query images as independent branch without considering the\nrelationship between them. To address this issue, we propose a dynamic\nrelevance learning model, which utilizes the relationship between all support\nimages and Region of Interest (RoI) on the query images to construct a dynamic\ngraph convolutional network (GCN). By adjusting the prediction distribution of\nthe base detector using the output of this GCN, the proposed model can guide\nthe detector to improve the class representation implicitly. Comprehensive\nexperiments have been conducted on Pascal VOC and MS-COCO dataset. The proposed\nmodel achieves the best overall performance, which shows its effectiveness of\nlearning more generalized features. Our code is available at\nhttps://github.com/liuweijie19980216/DRL-for-FSOD.",
          "link": "http://arxiv.org/abs/2108.02235",
          "publishedOn": "2021-08-06T00:51:45.120Z",
          "wordCount": 664,
          "title": "Dynamic Relevance Learning for Few-Shot Object Detection. (arXiv:2108.02235v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guolei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jingyun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "Due to the fact that fully supervised semantic segmentation methods require\nsufficient fully-labeled data to work well and can not generalize to unseen\nclasses, few-shot segmentation has attracted lots of research attention.\nPrevious arts extract features from support and query images, which are\nprocessed jointly before making predictions on query images. The whole process\nis based on convolutional neural networks (CNN), leading to the problem that\nonly local information is used. In this paper, we propose a TRansformer-based\nFew-shot Semantic segmentation method (TRFS). Specifically, our model consists\nof two modules: Global Enhancement Module (GEM) and Local Enhancement Module\n(LEM). GEM adopts transformer blocks to exploit global information, while LEM\nutilizes conventional convolutions to exploit local information, across query\nand support features. Both GEM and LEM are complementary, helping to learn\nbetter feature representations for segmenting query images. Extensive\nexperiments on PASCAL-5i and COCO datasets show that our approach achieves new\nstate-of-the-art performance, demonstrating its effectiveness.",
          "link": "http://arxiv.org/abs/2108.02266",
          "publishedOn": "2021-08-06T00:51:45.083Z",
          "wordCount": 601,
          "title": "Boosting Few-shot Semantic Segmentation with Transformers. (arXiv:2108.02266v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+he_D/0/1/0/all/0/1\">Dailan he</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yusheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Junyu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_T/0/1/0/all/0/1\">Tianrui Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaofei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aixi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>",
          "description": "Recently proposed fine-grained 3D visual grounding is an essential and\nchallenging task, whose goal is to identify the 3D object referred by a natural\nlanguage sentence from other distractive objects of the same category. Existing\nworks usually adopt dynamic graph networks to indirectly model the\nintra/inter-modal interactions, making the model difficult to distinguish the\nreferred object from distractors due to the monolithic representations of\nvisual and linguistic contents. In this work, we exploit Transformer for its\nnatural suitability on permutation-invariant 3D point clouds data and propose a\nTransRefer3D network to extract entity-and-relation aware multimodal context\namong objects for more discriminative feature learning. Concretely, we devise\nan Entity-aware Attention (EA) module and a Relation-aware Attention (RA)\nmodule to conduct fine-grained cross-modal feature matching. Facilitated by\nco-attention operation, our EA module matches visual entity features with\nlinguistic entity features while RA module matches pair-wise visual relation\nfeatures with linguistic relation features, respectively. We further integrate\nEA and RA modules into an Entity-and-Relation aware Contextual Block (ERCB) and\nstack several ERCBs to form our TransRefer3D for hierarchical multimodal\ncontext modeling. Extensive experiments on both Nr3D and Sr3D datasets\ndemonstrate that our proposed model significantly outperforms existing\napproaches by up to 10.6% and claims the new state-of-the-art. To the best of\nour knowledge, this is the first work investigating Transformer architecture\nfor fine-grained 3D visual grounding task.",
          "link": "http://arxiv.org/abs/2108.02388",
          "publishedOn": "2021-08-06T00:51:45.027Z",
          "wordCount": 676,
          "title": "TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding. (arXiv:2108.02388v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_L/0/1/0/all/0/1\">Lam Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thuc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunil Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Deokjai Choi</a>",
          "description": "Gait data captured by inertial sensors have demonstrated promising results on\nuser authentication. However, most existing approaches stored the enrolled gait\npattern insecurely for matching with the validating pattern, thus, posed\ncritical security and privacy issues. In this study, we present a gait\ncryptosystem that generates from gait data the random key for user\nauthentication, meanwhile, secures the gait pattern. First, we propose a\nrevocable and random binary string extraction method using a deep neural\nnetwork followed by feature-wise binarization. A novel loss function for\nnetwork optimization is also designed, to tackle not only the intrauser\nstability but also the inter-user randomness. Second, we propose a new\nbiometric key generation scheme, namely Irreversible Error Correct and\nObfuscate (IECO), improved from the Error Correct and Obfuscate (ECO) scheme,\nto securely generate from the binary string the random and irreversible key.\nThe model was evaluated with two benchmark datasets as OU-ISIR and whuGAIT. We\nshowed that our model could generate the key of 139 bits from 5-second data\nsequence with zero False Acceptance Rate (FAR) and False Rejection Rate (FRR)\nsmaller than 5.441%. In addition, the security and user privacy analyses showed\nthat our model was secure against existing attacks on biometric template\nprotection, and fulfilled irreversibility and unlinkability.",
          "link": "http://arxiv.org/abs/2108.02400",
          "publishedOn": "2021-08-06T00:51:45.006Z",
          "wordCount": 652,
          "title": "Security and Privacy Enhanced Gait Authentication with Random Representation Learning and Digital Lockers. (arXiv:2108.02400v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weijiang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Lei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuejian Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_N/0/1/0/all/0/1\">Nong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>",
          "description": "The task of video-based commonsense captioning aims to generate event-wise\ncaptions and meanwhile provide multiple commonsense descriptions (e.g.,\nattribute, effect and intention) about the underlying event in the video. Prior\nworks explore the commonsense captions by using separate networks for different\ncommonsense types, which is time-consuming and lacks mining the interaction of\ndifferent commonsense. In this paper, we propose a Hybrid Reasoning Network\n(HybridNet) to endow the neural networks with the capability of semantic-level\nreasoning and word-level reasoning. Firstly, we develop multi-commonsense\nlearning for semantic-level reasoning by jointly training different commonsense\ntypes in a unified network, which encourages the interaction between the clues\nof multiple commonsense descriptions, event-wise captions and videos. Then,\nthere are two steps to achieve the word-level reasoning: (1) a memory module\nrecords the history predicted sequence from the previous generation processes;\n(2) a memory-routed multi-head attention (MMHA) module updates the word-level\nattention maps by incorporating the history information from the memory module\ninto the transformer decoder for word-level reasoning. Moreover, the multimodal\nfeatures are used to make full use of diverse knowledge for commonsense\nreasoning. Experiments and abundant analysis on the large-scale\nVideo-to-Commonsense benchmark show that our HybridNet achieves\nstate-of-the-art performance compared with other methods.",
          "link": "http://arxiv.org/abs/2108.02365",
          "publishedOn": "2021-08-06T00:51:44.981Z",
          "wordCount": 651,
          "title": "Hybrid Reasoning Network for Video-based Commonsense Captioning. (arXiv:2108.02365v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Duo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>",
          "description": "Semantic segmentation is a crucial image understanding task, where each pixel\nof image is categorized into a corresponding label. Since the pixel-wise\nlabeling for ground-truth is tedious and labor intensive, in practical\napplications, many works exploit the synthetic images to train the model for\nreal-word image semantic segmentation, i.e., Synthetic-to-Real Semantic\nSegmentation (SRSS). However, Deep Convolutional Neural Networks (CNNs) trained\non the source synthetic data may not generalize well to the target real-world\ndata. In this work, we propose two simple yet effective texture randomization\nmechanisms, Global Texture Randomization (GTR) and Local Texture Randomization\n(LTR), for Domain Generalization based SRSS. GTR is proposed to randomize the\ntexture of source images into diverse unreal texture styles. It aims to\nalleviate the reliance of the network on texture while promoting the learning\nof the domain-invariant cues. In addition, we find the texture difference is\nnot always occurred in entire image and may only appear in some local areas.\nTherefore, we further propose a LTR mechanism to generate diverse local regions\nfor partially stylizing the source images. Finally, we implement a\nregularization of Consistency between GTR and LTR (CGL) aiming to harmonize the\ntwo proposed mechanisms during training. Extensive experiments on five publicly\navailable datasets (i.e., GTA5, SYNTHIA, Cityscapes, BDDS and Mapillary) with\nvarious SRSS settings (i.e., GTA5/SYNTHIA to Cityscapes/BDDS/Mapillary)\ndemonstrate that the proposed method is superior to the state-of-the-art\nmethods for domain generalization based SRSS.",
          "link": "http://arxiv.org/abs/2108.02376",
          "publishedOn": "2021-08-06T00:51:44.972Z",
          "wordCount": 686,
          "title": "Global and Local Texture Randomization for Synthetic-to-Real Semantic Segmentation. (arXiv:2108.02376v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiarui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>",
          "description": "One-stage long-tailed recognition methods improve the overall performance in\na \"seesaw\" manner, i.e., either sacrifice the head's accuracy for better tail\nclassification or elevate the head's accuracy even higher but ignore the tail.\nExisting algorithms bypass such trade-off by a multi-stage training process:\npre-training on imbalanced set and fine-tuning on balanced set. Though\nachieving promising performance, not only are they sensitive to the\ngeneralizability of the pre-trained model, but also not easily integrated into\nother computer vision tasks like detection and segmentation, where pre-training\nof classifiers solely is not applicable. In this paper, we propose a one-stage\nlong-tailed recognition scheme, ally complementary experts (ACE), where the\nexpert is the most knowledgeable specialist in a sub-set that dominates its\ntraining, and is complementary to other experts in the less-seen categories\nwithout being disturbed by what it has never seen. We design a\ndistribution-adaptive optimizer to adjust the learning pace of each expert to\navoid over-fitting. Without special bells and whistles, the vanilla ACE\noutperforms the current one-stage SOTA method by 3-10% on CIFAR10-LT,\nCIFAR100-LT, ImageNet-LT and iNaturalist datasets. It is also shown to be the\nfirst one to break the \"seesaw\" trade-off by improving the accuracy of the\nmajority and minority categories simultaneously in only one stage. Code and\ntrained models are at https://github.com/jrcai/ACE.",
          "link": "http://arxiv.org/abs/2108.02385",
          "publishedOn": "2021-08-06T00:51:44.960Z",
          "wordCount": 657,
          "title": "ACE: Ally Complementary Experts for Solving Long-Tailed Recognition in One-Shot. (arXiv:2108.02385v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xinzhi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenju Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chunxia Xiao</a>",
          "description": "Existing image captioning methods just focus on understanding the\nrelationship between objects or instances in a single image, without exploring\nthe contextual correlation existed among contextual image. In this paper, we\npropose Dual Graph Convolutional Networks (Dual-GCN) with transformer and\ncurriculum learning for image captioning. In particular, we not only use an\nobject-level GCN to capture the object to object spatial relation within a\nsingle image, but also adopt an image-level GCN to capture the feature\ninformation provided by similar images. With the well-designed Dual-GCN, we can\nmake the linguistic transformer better understand the relationship between\ndifferent objects in a single image and make full use of similar images as\nauxiliary information to generate a reasonable caption description for a single\nimage. Meanwhile, with a cross-review strategy introduced to determine\ndifficulty levels, we adopt curriculum learning as the training strategy to\nincrease the robustness and generalization of our proposed model. We conduct\nextensive experiments on the large-scale MS COCO dataset, and the experimental\nresults powerfully demonstrate that our proposed method outperforms recent\nstate-of-the-art approaches. It achieves a BLEU-1 score of 82.2 and a BLEU-2\nscore of 67.6. Our source code is available at {\\em\n\\color{magenta}{\\url{https://github.com/Unbear430/DGCN-for-image-captioning}}}.",
          "link": "http://arxiv.org/abs/2108.02366",
          "publishedOn": "2021-08-06T00:51:44.946Z",
          "wordCount": 662,
          "title": "Dual Graph Convolutional Networks with Transformer and Curriculum Learning for Image Captioning. (arXiv:2108.02366v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1\">Sen Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Richard Yi Da Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1\">Gaofeng Meng</a>",
          "description": "The vanilla GAN [5] suffers from mode collapse deeply, which usually\nmanifests as that the images generated by generators tend to have a high\nsimilarity amongst them, even though their corresponding latent vectors have\nbeen very different. In this paper, we introduce a pluggable block called\ndiversity penalty (dp) to alleviate mode collapse of GANs. It is used to reduce\nthe similarity of image pairs in feature space, i.e., if two latent vectors are\ndifferent, then we enforce the generator to generate two images with different\nfeatures. The normalized Gram Matrix is used to measure the similarity. We\ncompare the proposed method with Unrolled GAN [17], BourGAN [26], PacGAN [14],\nVEEGAN [23] and ALI [4] on 2D synthetic dataset, and results show that our\nproposed method can help GAN capture more modes of the data distribution.\nFurther, we apply this penalty term into image data augmentation on MNIST,\nFashion-MNIST and CIFAR-10, and the testing accuracy is improved by 0.24%,\n1.34% and 0.52% compared with WGAN GP [6], respectively. Finally, we\nquantitatively evaluate the proposed method with IS and FID on CelebA,\nCIFAR-10, MNIST and Fashion-MNIST. Results show that our method gets much\nhigher IS and lower FID compared with some current GAN architectures.",
          "link": "http://arxiv.org/abs/2108.02353",
          "publishedOn": "2021-08-06T00:51:44.896Z",
          "wordCount": 644,
          "title": "dp-GAN : Alleviating Mode Collapse in GAN via Diversity Penalty Module. (arXiv:2108.02353v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02234",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1\">Nathanael L. Baisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Bryan Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelov_P/0/1/0/all/0/1\">Plamen Angelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1\">Sue Black</a>",
          "description": "In this paper, we propose a novel hand-based person recognition method for\nthe purpose of criminal investigations since the hand image is often the only\navailable information in cases of serious crime such as sexual abuse. Our\nproposed method, Multi-Branch with Attention Network (MBA-Net), incorporates\nboth channel and spatial attention modules in branches in addition to a global\n(without attention) branch to capture global structural information for\ndiscriminative feature learning. The attention modules focus on the relevant\nfeatures of the hand image while suppressing the irrelevant backgrounds. In\norder to overcome the weakness of the attention mechanisms, equivariant to\npixel shuffling, we integrate relative positional encodings into the spatial\nattention module to capture the spatial positions of pixels. Extensive\nevaluations on two large multi-ethnic and publicly available hand datasets\ndemonstrate that our proposed method achieves state-of-the-art performance,\nsurpassing the existing hand-based identification methods.",
          "link": "http://arxiv.org/abs/2108.02234",
          "publishedOn": "2021-08-06T00:51:44.857Z",
          "wordCount": 583,
          "title": "Multi-Branch with Attention Network for Hand-Based Person Recognition. (arXiv:2108.02234v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02317",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Qiu_Z/0/1/0/all/0/1\">Ziheng Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1\">Xinyi Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_T/0/1/0/all/0/1\">Tianao Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_P/0/1/0/all/0/1\">Pan Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zibang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhong_J/0/1/0/all/0/1\">Jingang Zhong</a>",
          "description": "Fourier single-pixel imaging (FSI) is a branch of single-pixel imaging\ntechniques. It uses Fourier basis patterns as structured patterns for spatial\ninformation acquisition in the Fourier domain. However, the spatial resolution\nof the image reconstructed by FSI mainly depends on the number of Fourier\ncoefficients sampled. The reconstruction of a high-resolution image typically\nrequires a number of Fourier coefficients to be sampled, and therefore takes a\nlong data acquisition time. Here we propose a new sampling strategy for FSI. It\nallows FSI to reconstruct a clear and sharp image with a reduced number of\nmeasurements. The core of the proposed sampling strategy is to perform a\nvariable density sampling in the Fourier space and, more importantly, the\ndensity with respect to the importance of Fourier coefficients is subject to a\none-dimensional Gaussian function. Combined with compressive sensing, the\nproposed sampling strategy enables better reconstruction quality than\nconventional sampling strategies, especially when the sampling ratio is low. We\nexperimentally demonstrate compressive FSI combined with the proposed sampling\nstrategy is able to reconstruct a sharp and clear image of 256-by-256 pixels\nwith a sampling ratio of 10%. The proposed method enables fast single-pixel\nimaging and provides a new approach for efficient spatial information\nacquisition.",
          "link": "http://arxiv.org/abs/2108.02317",
          "publishedOn": "2021-08-06T00:51:44.849Z",
          "wordCount": 653,
          "title": "Efficient Fourier single-pixel imaging with Gaussian random sampling. (arXiv:2108.02317v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Cong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yicheng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Judish_M/0/1/0/all/0/1\">Max Judish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1\">Russell H Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armand_M/0/1/0/all/0/1\">Mehran Armand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grupp_R/0/1/0/all/0/1\">Robert Grupp</a>",
          "description": "Image-based navigation is widely considered the next frontier of minimally\ninvasive surgery. It is believed that image-based navigation will increase the\naccess to reproducible, safe, and high-precision surgery as it may then be\nperformed at acceptable costs and effort. This is because image-based\ntechniques avoid the need of specialized equipment and seamlessly integrate\nwith contemporary workflows. Further, it is expected that image-based\nnavigation will play a major role in enabling mixed reality environments and\nautonomous, robotic workflows. A critical component of image guidance is 2D/3D\nregistration, a technique to estimate the spatial relationships between 3D\nstructures, e.g., volumetric imagery or tool models, and 2D images thereof,\nsuch as fluoroscopy or endoscopy. While image-based 2D/3D registration is a\nmature technique, its transition from the bench to the bedside has been\nrestrained by well-known challenges, including brittleness of the optimization\nobjective, hyperparameter selection, and initialization, difficulties around\ninconsistencies or multiple objects, and limited single-view performance. One\nreason these challenges persist today is that analytical solutions are likely\ninadequate considering the complexity, variability, and high-dimensionality of\ngeneric 2D/3D registration problems. The recent advent of machine\nlearning-based approaches to imaging problems that, rather than specifying the\ndesired functional mapping, approximate it using highly expressive parametric\nmodels holds promise for solving some of the notorious challenges in 2D/3D\nregistration. In this manuscript, we review the impact of machine learning on\n2D/3D registration to systematically summarize the recent advances made by\nintroduction of this novel technology. Grounded in these insights, we then\noffer our perspective on the most pressing needs, significant open problems,\nand possible next steps.",
          "link": "http://arxiv.org/abs/2108.02238",
          "publishedOn": "2021-08-06T00:51:44.842Z",
          "wordCount": 727,
          "title": "The Impact of Machine Learning on 2D/3D Registration for Image-guided Interventions: A Systematic Review and Perspective. (arXiv:2108.02238v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1\">Willy Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossinelli_D/0/1/0/all/0/1\">Diego Rossinelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_G/0/1/0/all/0/1\">Georg Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenger_R/0/1/0/all/0/1\">Roland H. Wenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hieber_S/0/1/0/all/0/1\">Simone Hieber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_B/0/1/0/all/0/1\">Bert M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtcuoglu_V/0/1/0/all/0/1\">Vartan Kurtcuoglu</a>",
          "description": "The performance of machine learning algorithms used for the segmentation of\n3D biomedical images lags behind that of the algorithms employed in the\nclassification of 2D photos. This may be explained by the comparative lack of\nhigh-volume, high-quality training datasets, which require state-of-the art\nimaging facilities, domain experts for annotation and large computational and\npersonal resources to create. The HR-Kidney dataset presented in this work\nbridges this gap by providing 1.7 TB of artefact-corrected synchrotron\nradiation-based X-ray phase-contrast microtomography images of whole mouse\nkidneys and validated segmentations of 33 729 glomeruli, which represents a 1-2\norders of magnitude increase over currently available biomedical datasets. The\ndataset further contains the underlying raw data, classical segmentations of\nrenal vasculature and uriniferous tubules, as well as true 3D manual\nannotations. By removing limits currently imposed by small training datasets,\nthe provided data open up the possibility for disruptions in machine learning\nfor biomedical image analysis.",
          "link": "http://arxiv.org/abs/2108.02226",
          "publishedOn": "2021-08-06T00:51:44.814Z",
          "wordCount": 610,
          "title": "Terabyte-scale supervised 3D training and benchmarking dataset of the mouse kidney. (arXiv:2108.02226v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Quiros_A/0/1/0/all/0/1\">Adalberto Claudio Quiros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coudray_N/0/1/0/all/0/1\">Nicolas Coudray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeaton_A/0/1/0/all/0/1\">Anna Yeaton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunhem_W/0/1/0/all/0/1\">Wisuwat Sunhem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_Smith_R/0/1/0/all/0/1\">Roderick Murray-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsirigos_A/0/1/0/all/0/1\">Aristotelis Tsirigos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Ke Yuan</a>",
          "description": "Deep learning based analysis of histopathology images shows promise in\nadvancing the understanding of tumor progression, tumor micro-environment, and\ntheir underpinning biological processes. So far, these approaches have focused\non extracting information associated with annotations. In this work, we ask how\nmuch information can be learned from the tissue architecture itself.\n\nWe present an adversarial learning model to extract feature representations\nof cancer tissue, without the need for manual annotations. We show that these\nrepresentations are able to identify a variety of morphological characteristics\nacross three cancer types: Breast, colon, and lung. This is supported by 1) the\nseparation of morphologic characteristics in the latent space; 2) the ability\nto classify tissue type with logistic regression using latent representations,\nwith an AUC of 0.97 and 85% accuracy, comparable to supervised deep models; 3)\nthe ability to predict the presence of tumor in Whole Slide Images (WSIs) using\nmultiple instance learning (MIL), achieving an AUC of 0.98 and 94% accuracy.\n\nOur results show that our model captures distinct phenotypic characteristics\nof real tissue samples, paving the way for further understanding of tumor\nprogression and tumor micro-environment, and ultimately refining\nhistopathological classification for diagnosis and treatment. The code and\npretrained models are available at:\nhttps://github.com/AdalbertoCq/Adversarial-learning-of-cancer-tissue-representations",
          "link": "http://arxiv.org/abs/2108.02223",
          "publishedOn": "2021-08-06T00:51:44.800Z",
          "wordCount": 655,
          "title": "Adversarial learning of cancer tissue representations. (arXiv:2108.02223v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_N/0/1/0/all/0/1\">Nitish Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prados_D/0/1/0/all/0/1\">David Ramon Prados</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodzic_N/0/1/0/all/0/1\">Nedim Hodzic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanassios_C/0/1/0/all/0/1\">Christos Karanassios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1\">H.R. Tizhoosh</a>",
          "description": "Lung nodules are commonly missed in chest radiographs. We propose and\nevaluate P-AnoGAN, an unsupervised anomaly detection approach for lung nodules\nin radiographs. P-AnoGAN modifies the fast anomaly detection generative\nadversarial network (f-AnoGAN) by utilizing a progressive GAN and a\nconvolutional encoder-decoder-encoder pipeline. Model training uses only\nunlabelled healthy lung patches extracted from the Indiana University Chest\nX-Ray Collection. External validation and testing are performed using healthy\nand unhealthy patches extracted from the ChestX-ray14 and Japanese Society for\nRadiological Technology datasets, respectively. Our model robustly identifies\npatches containing lung nodules in external validation and test data with\nROC-AUC of 91.17% and 87.89%, respectively. These results show unsupervised\nmethods may be useful in challenging tasks such as lung nodule detection in\nradiographs.",
          "link": "http://arxiv.org/abs/2108.02233",
          "publishedOn": "2021-08-06T00:51:44.792Z",
          "wordCount": 594,
          "title": "Unsupervised Detection of Lung Nodules in Chest Radiography Using Generative Adversarial Networks. (arXiv:2108.02233v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.08598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuesi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huzhang_G/0/1/0/all/0/1\">Guangda Huzhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qianying Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Da_Q/0/1/0/all/0/1\">Qing Da</a>",
          "description": "Ensemble models in E-commerce combine predictions from multiple sub-models\nfor ranking and revenue improvement. Industrial ensemble models are typically\ndeep neural networks, following the supervised learning paradigm to infer\nconversion rate given inputs from sub-models. However, this process has the\nfollowing two problems. Firstly, the point-wise scoring approach disregards the\nrelationships between items and leads to homogeneous displayed results, while\ndiversified display benefits user experience and revenue. Secondly, the\nlearning paradigm focuses on the ranking metrics and does not directly optimize\nthe revenue. In our work, we propose a new Learning-To-Ensemble (LTE) framework\nRAEGO, which replaces the ensemble model with a contextual Rank Aggregator (RA)\nand explores the best weights of sub-models by the Evaluator-Generator\nOptimization (EGO). To achieve the best online performance, we propose a new\nrank aggregation algorithm TournamentGreedy as a refinement of classic rank\naggregators, which also produces the best average weighted Kendall Tau Distance\n(KTD) amongst all the considered algorithms with quadratic time complexity.\nUnder the assumption that the best output list should be Pareto Optimal on the\nKTD metric for sub-models, we show that our RA algorithm has higher efficiency\nand coverage in exploring the optimal weights. Combined with the idea of\nBayesian Optimization and gradient descent, we solve the online contextual\nBlack-Box Optimization task that finds the optimal weights for sub-models given\na chosen RA model. RA-EGO has been deployed in our online system and has\nimproved the revenue significantly.",
          "link": "http://arxiv.org/abs/2107.08598",
          "publishedOn": "2021-08-11T01:55:24.192Z",
          "wordCount": 697,
          "title": "Learning-To-Ensemble by Contextual Rank Aggregation in E-Commerce. (arXiv:2107.08598v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dumas_J/0/1/0/all/0/1\">Jonathan Dumas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanaspeze_A/0/1/0/all/0/1\">Antoine Wehenkel Damien Lanaspeze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornelusse_B/0/1/0/all/0/1\">Bertrand Corn&#xe9;lusse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutera_A/0/1/0/all/0/1\">Antonio Sutera</a>",
          "description": "Greater direct electrification of end-use sectors with a higher share of\nrenewables is one of the pillars to power a carbon-neutral society by 2050.\nHowever, in contrast to conventional power plants, renewable energy is subject\nto uncertainty raising challenges for their interaction with power systems.\nScenario-based probabilistic forecasting models have become an important tool\nto equip decision-makers. This paper proposes to present to the power systems\nforecasting practitioners a recent deep learning technique, the normalizing\nflows, to produce accurate scenario-based probabilistic forecasts that are\ncrucial to face the new challenges in power systems applications. The strength\nof this technique is to directly learn the stochastic multivariate distribution\nof the underlying process by maximizing the likelihood. Through comprehensive\nempirical evaluations using the open data of the Global Energy Forecasting\nCompetition 2014, we demonstrate that this methodology is competitive with\nother state-of-the-art deep learning generative models: generative adversarial\nnetworks and variational autoencoders. The models producing weather-based wind,\nsolar power, and load scenarios are properly compared both in terms of forecast\nvalue, by considering the case study of an energy retailer, and quality using\nseveral complementary metrics. The numerical experiments are simple and easily\nreproducible. Thus, we hope it will encourage other forecasting practitioners\nto test and use normalizing flows in power system applications such as bidding\non electricity markets, scheduling of power systems with high renewable energy\nsources penetration, energy management of virtual power plan or microgrids, and\nunit commitment.",
          "link": "http://arxiv.org/abs/2106.09370",
          "publishedOn": "2021-08-11T01:55:24.148Z",
          "wordCount": 741,
          "title": "A deep generative model for probabilistic energy forecasting in power systems: normalizing flows. (arXiv:2106.09370v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01289",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mok_J/0/1/0/all/0/1\">Jisoo Mok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_B/0/1/0/all/0/1\">Byunggook Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_H/0/1/0/all/0/1\">Hyeokjun Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "Deep neural networks continue to awe the world with their remarkable\nperformance. Their predictions, however, are prone to be corrupted by\nadversarial examples that are imperceptible to humans. Current efforts to\nimprove the robustness of neural networks against adversarial examples are\nfocused on developing robust training methods, which update the weights of a\nneural network in a more robust direction. In this work, we take a step beyond\ntraining of the weight parameters and consider the problem of designing an\nadversarially robust neural architecture with high intrinsic robustness. We\npropose AdvRush, a novel adversarial robustness-aware neural architecture\nsearch algorithm, based upon a finding that independent of the training method,\nthe intrinsic robustness of a neural network can be represented with the\nsmoothness of its input loss landscape. Through a regularizer that favors a\ncandidate architecture with a smoother input loss landscape, AdvRush\nsuccessfully discovers an adversarially robust neural architecture. Along with\na comprehensive theoretical motivation for AdvRush, we conduct an extensive\namount of experiments to demonstrate the efficacy of AdvRush on various\nbenchmark datasets. Notably, on CIFAR-10, AdvRush achieves 55.91% robust\naccuracy under FGSM attack after standard training and 50.04% robust accuracy\nunder AutoAttack after 7-step PGD adversarial training.",
          "link": "http://arxiv.org/abs/2108.01289",
          "publishedOn": "2021-08-11T01:55:24.118Z",
          "wordCount": 650,
          "title": "AdvRush: Searching for Adversarially Robust Neural Architectures. (arXiv:2108.01289v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1\">Daochen Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_Z/0/1/0/all/0/1\">Zaid Pervaiz Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Sirui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anmoll Kumar Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_M/0/1/0/all/0/1\">Mohammad Qazim Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1\">Kwei-Herng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaben Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_N/0/1/0/all/0/1\">Na Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>",
          "description": "Action recognition is a crucial task for video understanding. In this paper,\nwe present AutoVideo, a Python system for automated video action recognition.\nIt currently supports seven action recognition algorithms and various\npre-processing modules. Unlike the existing libraries that only provide model\nzoos, AutoVideo is built with the standard pipeline language. The basic\nbuilding block is primitive, which wraps a pre-processing module or an\nalgorithm with some hyperparameters. AutoVideo is highly modular and\nextendable. It can be easily combined with AutoML searchers. The pipeline\nlanguage is quite general so that we can easily enrich AutoVideo with\nalgorithms for various other video-related tasks in the future. AutoVideo is\nreleased under MIT license at https://github.com/datamllab/autovideo",
          "link": "http://arxiv.org/abs/2108.04212",
          "publishedOn": "2021-08-11T01:55:24.079Z",
          "wordCount": 598,
          "title": "AutoVideo: An Automated Video Action Recognition System. (arXiv:2108.04212v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.12975",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kromp_F/0/1/0/all/0/1\">Florian Kromp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_L/0/1/0/all/0/1\">Lukas Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozsaky_E/0/1/0/all/0/1\">Eva Bozsaky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambros_I/0/1/0/all/0/1\">Inge Ambros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doerr_W/0/1/0/all/0/1\">Wolfgang Doerr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taschner_Mandl_S/0/1/0/all/0/1\">Sabine Taschner-Mandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambros_P/0/1/0/all/0/1\">Peter Ambros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1\">Allan Hanbury</a>",
          "description": "Separating and labeling each instance of a nucleus (instance-aware\nsegmentation) is the key challenge in segmenting single cell nuclei on\nfluorescence microscopy images. Deep Neural Networks can learn the implicit\ntransformation of a nuclear image into a probability map indicating the class\nmembership of each pixel (nucleus or background), but the use of\npost-processing steps to turn the probability map into a labeled object mask is\nerror-prone. This especially accounts for nuclear images of tissue sections and\nnuclear images across varying tissue preparations. In this work, we aim to\nevaluate the performance of state-of-the-art deep learning architectures to\nsegment nuclei in fluorescence images of various tissue origins and sample\npreparation types without post-processing. We compare architectures that\noperate on pixel to pixel translation and an architecture that operates on\nobject detection and subsequent locally applied segmentation. In addition, we\npropose a novel strategy to create artificial images to extend the training\nset. We evaluate the influence of ground truth annotation quality, image scale\nand segmentation complexity on segmentation performance. Results show that\nthree out of four deep learning architectures (U-Net, U-Net with ResNet34\nbackbone, Mask R-CNN) can segment fluorescent nuclear images on most of the\nsample preparation types and tissue origins with satisfactory segmentation\nperformance. Mask R-CNN, an architecture designed to address instance aware\nsegmentation tasks, outperforms other architectures. Equal nuclear mean size,\nconsistent nuclear annotations and the use of artificially generated images\nresult in overall acceptable precision and recall across different tissues and\nsample preparation types.",
          "link": "http://arxiv.org/abs/1907.12975",
          "publishedOn": "2021-08-11T01:55:24.073Z",
          "wordCount": 725,
          "title": "Deep Learning architectures for generalized immunofluorescence based nuclear image segmentation. (arXiv:1907.12975v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Odetola_T/0/1/0/all/0/1\">Tolulope Odetola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalid_F/0/1/0/all/0/1\">Faiq Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandefur_T/0/1/0/all/0/1\">Travis Sandefur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_H/0/1/0/all/0/1\">Hawzhin Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1\">Syed Rafay Hasan</a>",
          "description": "To reduce the time-to-market and access to state-of-the-art techniques, CNN\nhardware mapping and deployment on embedded accelerators are often outsourced\nto untrusted third parties, which is going to be more prevalent in futuristic\nartificial intelligence of things (AIoT) systems. These AIoT systems anticipate\nhorizontal collaboration among different resource-constrained AIoT node\ndevices, where CNN layers are partitioned and these devices collaboratively\ncompute complex CNN tasks. This horizontal collaboration opens another attack\nsurface to the CNN-based application, like inserting the hardware Trojans (HT)\ninto the embedded accelerators designed for the CNN. Therefore, there is a dire\nneed to explore this attack surface for designing secure embedded hardware\naccelerators for CNNs. Towards this goal, in this paper, we exploited this\nattack surface to propose an HT-based attack called FeSHI. Since in horizontal\ncollaboration of RC AIoT devices different sections of CNN architectures are\noutsourced to different untrusted third parties, the attacker may not know the\ninput image, but it has access to the layer-by-layer output feature maps\ninformation for the assigned sections of the CNN architecture. This attack\nexploits the statistical distribution, i.e., Gaussian distribution, of the\nlayer-by-layer feature maps of the CNN to design two triggers for stealthy HT\nwith a very low probability of triggering. Also, three different novel,\nstealthy and effective trigger designs are proposed.",
          "link": "http://arxiv.org/abs/2106.06895",
          "publishedOn": "2021-08-11T01:55:24.067Z",
          "wordCount": 694,
          "title": "FeSHI: Feature Map Based Stealthy Hardware Intrinsic Attack. (arXiv:2106.06895v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bothmann_L/0/1/0/all/0/1\">Ludwig Bothmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strickroth_S/0/1/0/all/0/1\">Sven Strickroth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casalicchio_G/0/1/0/all/0/1\">Giuseppe Casalicchio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1\">David R&#xfc;gamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1\">Marius Lindauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheipl_F/0/1/0/all/0/1\">Fabian Scheipl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>",
          "description": "Education should not be a privilege but a common good. It should be openly\naccessible to everyone, with as few barriers as possible; even more so for key\ntechnologies such as Machine Learning (ML) and Data Science (DS). Open\nEducational Resources (OER) are a crucial factor for greater educational\nequity. In this paper, we describe the specific requirements for OER in ML and\nDS and argue that it is especially important for these fields to make source\nfiles publicly available, leading to Open Source Educational Resources (OSER).\nWe present our view on the collaborative development of OSER, the challenges\nthis poses, and first steps towards their solutions. We outline how OSER can be\nused for blended learning scenarios and share our experiences in university\neducation. Finally, we discuss additional challenges such as credit assignment\nor granting certificates.",
          "link": "http://arxiv.org/abs/2107.14330",
          "publishedOn": "2021-08-11T01:55:24.033Z",
          "wordCount": 605,
          "title": "Developing Open Source Educational Resources for Machine Learning and Data Science. (arXiv:2107.14330v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhi-Qin John Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hanxu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaoyu Zhang</a>",
          "description": "Studying the implicit regularization effect of the nonlinear training\ndynamics of neural networks (NNs) is important for understanding why\nover-parameterized neural networks often generalize well on real dataset.\nEmpirically, for two-layer NN, existing works have shown that input weights of\nhidden neurons (the input weight of a hidden neuron consists of the weight from\nits input layer to the hidden neuron and its bias term) condense on isolated\norientations with a small initialization. The condensation dynamics implies\nthat NNs can learn features from the training data with a network configuration\neffectively equivalent to a much smaller network during the training. In this\nwork, we show that the multiple roots of activation function at origin\n(referred as ``multiplicity'') is a key factor for understanding the\ncondensation at the initial stage of training. Our experiments of multilayer\nnetworks suggest that the maximal number of condensed orientations is twice the\nmultiplicity of the activation function used. Our theoretical analysis of\ntwo-layer networks confirms experiments for two cases, one is for the\nactivation function of multiplicity one, which contains many common activation\nfunctions, and the other is for the one-dimensional input. This work makes a\nstep towards understanding how small initialization implicitly leads NNs to\ncondensation at initial training stage, which lays a foundation for the future\nstudy of the nonlinear dynamics of NNs and its implicit regularization effect\nat a later stage of training.",
          "link": "http://arxiv.org/abs/2105.11686",
          "publishedOn": "2021-08-11T01:55:23.984Z",
          "wordCount": 712,
          "title": "Towards Understanding the Condensation of Two-layer Neural Networks at Initial Training. (arXiv:2105.11686v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shuchang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Ting-Bing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>",
          "description": "In real applications, different computation-resource devices need\ndifferent-depth networks (e.g., ResNet-18/34/50) with high-accuracy. Usually,\nexisting methods either design multiple networks and train them independently,\nor construct depth-level/width-level dynamic neural networks which is hard to\nprove the accuracy of each sub-net. In this article, we propose an elegant\nDepth-Level Dynamic Neural Network (DDNN) integrated different-depth sub-nets\nof similar architectures. To improve the generalization of sub-nets, we design\nthe Embedded-Knowledge-Distillation (EKD) training mechanism for the DDNN to\nimplement knowledge transfer from the teacher (full-net) to multiple students\n(sub-nets). Specifically, the Kullback-Leibler (KL) divergence is introduced to\nconstrain the posterior class probability consistency between full-net and\nsub-nets, and self-attention distillation on the same resolution feature of\ndifferent depth is addressed to drive more abundant feature representations of\nsub-nets. Thus, we can obtain multiple high-accuracy sub-nets simultaneously in\na DDNN via the online knowledge distillation in each training iteration without\nextra computation cost. Extensive experiments on CIFAR-10/100, and ImageNet\ndatasets demonstrate that sub-nets in DDNN with EKD training achieve better\nperformance than individually training networks while preserving the original\nperformance of full-nets.",
          "link": "http://arxiv.org/abs/2103.00793",
          "publishedOn": "2021-08-11T01:55:23.977Z",
          "wordCount": 674,
          "title": "Embedded Knowledge Distillation in Depth-Level Dynamic Neural Network. (arXiv:2103.00793v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13430",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nishida_K/0/1/0/all/0/1\">Kiheiji Nishida</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Naito_K/0/1/0/all/0/1\">Kanta Naito</a>",
          "description": "This study proposes multivariate kernel density estimation by stagewise\nminimization algorithm based on $U$-divergence and a simple dictionary. The\ndictionary consists of an appropriate scalar bandwidth matrix and a part of the\noriginal data. The resulting estimator brings us data-adaptive weighting\nparameters and bandwidth matrices, and realizes a sparse representation of\nkernel density estimation. We develop the non-asymptotic error bound of\nestimator obtained via the proposed stagewise minimization algorithm. It is\nconfirmed from simulation studies that the proposed estimator performs\ncompetitive to or sometime better than other well-known density estimators.",
          "link": "http://arxiv.org/abs/2107.13430",
          "publishedOn": "2021-08-11T01:55:23.972Z",
          "wordCount": 548,
          "title": "Kernel Density Estimation by Stagewise Algorithm with a Simple Dictionary. (arXiv:2107.13430v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03322",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lucic_A/0/1/0/all/0/1\">Ana Lucic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoeve_M/0/1/0/all/0/1\">Maartje ter Hoeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolomei_G/0/1/0/all/0/1\">Gabriele Tolomei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1\">Fabrizio Silvestri</a>",
          "description": "Given the increasing promise of Graph Neural Networks (GNNs) in real-world\napplications, several methods have been developed for explaining their\npredictions. So far, these methods have primarily focused on generating\nsubgraphs that are especially relevant for a particular prediction. However,\nsuch methods do not provide a clear opportunity for recourse: given a\nprediction, we want to understand how the prediction can be changed in order to\nachieve a more desirable outcome. In this work, we propose a method for\ngenerating counterfactual (CF) explanations for GNNs: the minimal perturbation\nto the input (graph) data such that the prediction changes. Using only edge\ndeletions, we find that our method, CF-GNNExplainer can generate CF\nexplanations for the majority of instances across three widely used datasets\nfor GNN explanations, while removing less than 3 edges on average, with at\nleast 94\\% accuracy. This indicates that CF-GNNExplainer primarily removes\nedges that are crucial for the original predictions, resulting in minimal CF\nexplanations.",
          "link": "http://arxiv.org/abs/2102.03322",
          "publishedOn": "2021-08-11T01:55:22.356Z",
          "wordCount": 647,
          "title": "CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks. (arXiv:2102.03322v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1\">Moritz Knolle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1\">Dmitrii Usynin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1\">Alexander Ziller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus R. Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>",
          "description": "The application of differential privacy to the training of deep neural\nnetworks holds the promise of allowing large-scale (decentralized) use of\nsensitive data while providing rigorous privacy guarantees to the individual.\nThe predominant approach to differentially private training of neural networks\nis DP-SGD, which relies on norm-based gradient clipping as a method for\nbounding sensitivity, followed by the addition of appropriately calibrated\nGaussian noise. In this work we propose NeuralDP, a technique for privatising\nactivations of some layer within a neural network, which by the post-processing\nproperties of differential privacy yields a differentially private network. We\nexperimentally demonstrate on two datasets (MNIST and Pediatric Pneumonia\nDataset (PPD)) that our method offers substantially improved privacy-utility\ntrade-offs compared to DP-SGD.",
          "link": "http://arxiv.org/abs/2107.14582",
          "publishedOn": "2021-08-11T01:55:22.291Z",
          "wordCount": 594,
          "title": "NeuralDP Differentially private neural networks by design. (arXiv:2107.14582v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jindi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kejie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaohua Jia</a>",
          "description": "In recent years, many deep learning models have been adopted in autonomous\ndriving. At the same time, these models introduce new vulnerabilities that may\ncompromise the safety of autonomous vehicles. Specifically, recent studies have\ndemonstrated that adversarial attacks can cause a significant decline in\ndetection precision of deep learning-based 3D object detection models. Although\ndriving safety is the ultimate concern for autonomous driving, there is no\ncomprehensive study on the linkage between the performance of deep learning\nmodels and the driving safety of autonomous vehicles under adversarial attacks.\nIn this paper, we investigate the impact of two primary types of adversarial\nattacks, perturbation attacks and patch attacks, on the driving safety of\nvision-based autonomous vehicles rather than the detection precision of deep\nlearning models. In particular, we consider two state-of-the-art models in\nvision-based 3D object detection, Stereo R-CNN and DSGN. To evaluate driving\nsafety, we propose an end-to-end evaluation framework with a set of driving\nsafety performance metrics. By analyzing the results of our extensive\nevaluation experiments, we find that (1) the attack's impact on the driving\nsafety of autonomous vehicles and the attack's impact on the precision of 3D\nobject detectors are decoupled, and (2) the DSGN model demonstrates stronger\nrobustness to adversarial attacks than the Stereo R-CNN model. In addition, we\nfurther investigate the causes behind the two findings with an ablation study.\nThe findings of this paper provide a new perspective to evaluate adversarial\nattacks and guide the selection of deep learning models in autonomous driving.",
          "link": "http://arxiv.org/abs/2108.02940",
          "publishedOn": "2021-08-11T01:55:22.260Z",
          "wordCount": 704,
          "title": "Evaluating Adversarial Attacks on Driving Safety in Vision-Based Autonomous Vehicles. (arXiv:2108.02940v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Ruoxuan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koenecke_A/0/1/0/all/0/1\">Allison Koenecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powell_M/0/1/0/all/0/1\">Michael Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1\">Joshua T. Vogelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athey_S/0/1/0/all/0/1\">Susan Athey</a>",
          "description": "Analyzing observational data from multiple sources can be useful for\nincreasing statistical power to detect a treatment effect; however, practical\nconstraints such as privacy considerations may restrict individual-level\ninformation sharing across data sets. This paper develops federated methods\nthat only utilize summary-level information from heterogeneous data sets. Our\nfederated methods provide doubly-robust point estimates of treatment effects as\nwell as variance estimates. We derive the asymptotic distributions of our\nfederated estimators, which are shown to be asymptotically equivalent to the\ncorresponding estimators from the combined, individual-level data. We show that\nto achieve these properties, federated methods should be adjusted based on\nconditions such as whether models are correctly specified and stable across\nheterogeneous data sets.",
          "link": "http://arxiv.org/abs/2107.11732",
          "publishedOn": "2021-08-11T01:55:22.248Z",
          "wordCount": 591,
          "title": "Federated Causal Inference in Heterogeneous Observational Data. (arXiv:2107.11732v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1\">Xianbo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shunquan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiwu Huang</a>",
          "description": "Recent research has shown that non-additive image steganographic frameworks\neffectively improve security performance through adjusting distortion\ndistribution. However, as far as we know, all of the existing non-additive\nproposals are based on handcrafted policies, and can only be applied to a\nspecific image domain, which heavily prevent non-additive steganography from\nreleasing its full potentiality. In this paper, we propose an automatic\nnon-additive steganographic distortion learning framework called MCTSteg to\nremove the above restrictions. Guided by the reinforcement learning paradigm,\nwe combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental\nmodel to build MCTSteg. MCTS makes sequential decisions to adjust distortion\ndistribution without human intervention. Our proposed environmental model is\nused to obtain feedbacks from each decision. Due to its self-learning\ncharacteristic and domain-independent reward function, MCTSteg has become the\nfirst reported universal non-additive steganographic framework which can work\nin both spatial and JPEG domains. Extensive experimental results show that\nMCTSteg can effectively withstand the detection of both hand-crafted\nfeature-based and deep-learning-based steganalyzers. In both spatial and JPEG\ndomains, the security performance of MCTSteg steadily outperforms the state of\nthe art by a clear margin under different scenarios.",
          "link": "http://arxiv.org/abs/2103.13689",
          "publishedOn": "2021-08-11T01:55:22.204Z",
          "wordCount": 670,
          "title": "MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning Framework for Universal Non-additive Steganography. (arXiv:2103.13689v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hatt_T/0/1/0/all/0/1\">Tobias Hatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feuerriegel_S/0/1/0/all/0/1\">Stefan Feuerriegel</a>",
          "description": "Decision-making often requires accurate estimation of treatment effects from\nobservational data. This is challenging as outcomes of alternative decisions\nare not observed and have to be estimated. Previous methods estimate outcomes\nbased on unconfoundedness but neglect any constraints that unconfoundedness\nimposes on the outcomes. In this paper, we propose a novel regularization\nframework for estimating average treatment effects that exploits\nunconfoundedness. To this end, we formalize unconfoundedness as an\northogonality constraint, which ensures that the outcomes are orthogonal to the\ntreatment assignment. This orthogonality constraint is then included in the\nloss function via a regularization. Based on our regularization framework, we\ndevelop deep orthogonal networks for unconfounded treatments (DONUT), which\nlearn outcomes that are orthogonal to the treatment assignment. Using a variety\nof benchmark datasets for estimating average treatment effects, we demonstrate\nthat DONUT outperforms the state-of-the-art substantially.",
          "link": "http://arxiv.org/abs/2101.08490",
          "publishedOn": "2021-08-11T01:55:22.198Z",
          "wordCount": 593,
          "title": "Estimating Average Treatment Effects via Orthogonal Regularization. (arXiv:2101.08490v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosel_F/0/1/0/all/0/1\">Fabian R&#xf6;sel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahrenkrog_Petersen_S/0/1/0/all/0/1\">Stephan A. Fahrenkrog-Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aa_H/0/1/0/all/0/1\">Han van der Aa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weidlich_M/0/1/0/all/0/1\">Matthias Weidlich</a>",
          "description": "To enable process analysis based on an event log without compromising the\nprivacy of individuals involved in process execution, a log may be anonymized.\nSuch anonymization strives to transform a log so that it satisfies provable\nprivacy guarantees, while largely maintaining its utility for process analysis.\nExisting techniques perform anonymization using simple, syntactic measures to\nidentify suitable transformation operations. This way, the semantics of the\nactivities referenced by the events in a trace are neglected, potentially\nleading to transformations in which events of unrelated activities are merged.\nTo avoid this and incorporate the semantics of activities during anonymization,\nwe propose to instead incorporate a distance measure based on feature learning.\nSpecifically, we show how embeddings of events enable the definition of a\ndistance measure for traces to guide event log anonymization. Our experiments\nwith real-world data indicate that anonymization using this measure, compared\nto a syntactic one, yields logs that are closer to the original log in various\ndimensions and, hence, have higher utility for process analysis.",
          "link": "http://arxiv.org/abs/2107.06578",
          "publishedOn": "2021-08-11T01:55:22.192Z",
          "wordCount": 657,
          "title": "A Distance Measure for Privacy-preserving Process Mining based on Feature Learning. (arXiv:2107.06578v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1\">Ron Shmelkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1\">Tomer Friedlander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "A master face is a face image that passes face-based identity-authentication\nfor a large portion of the population. These faces can be used to impersonate,\nwith a high probability of success, any user, without having access to any\nuser-information. We optimize these faces, by using an evolutionary algorithm\nin the latent embedding space of the StyleGAN face generator. Multiple\nevolutionary strategies are compared, and we propose a novel approach that\nemploys a neural network in order to direct the search in the direction of\npromising samples, without adding fitness evaluations. The results we present\ndemonstrate that it is possible to obtain a high coverage of the LFW identities\n(over 40%) with less than 10 master faces, for three leading deep face\nrecognition systems.",
          "link": "http://arxiv.org/abs/2108.01077",
          "publishedOn": "2021-08-11T01:55:22.185Z",
          "wordCount": 607,
          "title": "Generating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution. (arXiv:2108.01077v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaomin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaojin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loh_P/0/1/0/all/0/1\">Po-Ling Loh</a>",
          "description": "We investigate problems in penalized $M$-estimation, inspired by applications\nin machine learning debugging. Data are collected from two pools, one\ncontaining data with possibly contaminated labels, and the other which is known\nto contain only cleanly labeled points. We first formulate a general\nstatistical algorithm for identifying buggy points and provide rigorous\ntheoretical guarantees under the assumption that the data follow a linear\nmodel. We then present two case studies to illustrate the results of our\ngeneral theory and the dependence of our estimator on clean versus buggy\npoints. We further propose an algorithm for tuning parameter selection of our\nLasso-based algorithm and provide corresponding theoretical guarantees.\nFinally, we consider a two-person \"game\" played between a bug generator and a\ndebugger, where the debugger can augment the contaminated data set with cleanly\nlabeled versions of points in the original data pool. We establish a\ntheoretical result showing a sufficient condition under which the bug generator\ncan always fool the debugger. Nonetheless, we provide empirical results showing\nthat such a situation may not occur in practice, making it possible for natural\naugmentation strategies combined with our Lasso debugging algorithm to succeed.",
          "link": "http://arxiv.org/abs/2006.09009",
          "publishedOn": "2021-08-11T01:55:22.169Z",
          "wordCount": 651,
          "title": "Provable Training Set Debugging for Linear Regression. (arXiv:2006.09009v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingheng Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorbet_R/0/1/0/all/0/1\">Rob Gorbet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulic_D/0/1/0/all/0/1\">Dana Kuli&#x107;</a>",
          "description": "A promising characteristic of Deep Reinforcement Learning (DRL) is its\ncapability to learn optimal policy in an end-to-end manner without relying on\nfeature engineering. However, most approaches assume a fully observable state\nspace, i.e. fully observable Markov Decision Processes (MDPs). In real-world\nrobotics, this assumption is unpractical, because of issues such as sensor\nsensitivity limitations and sensor noise, and the lack of knowledge about\nwhether the observation design is complete or not. These scenarios lead to\nPartially Observable MDPs (POMDPs). In this paper, we propose\nLong-Short-Term-Memory-based Twin Delayed Deep Deterministic Policy Gradient\n(LSTM-TD3) by introducing a memory component to TD3, and compare its\nperformance with other DRL algorithms in both MDPs and POMDPs. Our results\ndemonstrate the significant advantages of the memory component in addressing\nPOMDPs, including the ability to handle missing and noisy observation data.",
          "link": "http://arxiv.org/abs/2102.12344",
          "publishedOn": "2021-08-11T01:55:22.163Z",
          "wordCount": 617,
          "title": "Memory-based Deep Reinforcement Learning for POMDPs. (arXiv:2102.12344v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1\">Samet Oymak</a>",
          "description": "Conventional wisdom dictates that learning rate should be in the stable\nregime so that gradient-based algorithms don't blow up. This letter introduces\na simple scenario where an unstably large learning rate scheme leads to a super\nfast convergence, with the convergence rate depending only logarithmically on\nthe condition number of the problem. Our scheme uses a Cyclical Learning Rate\n(CLR) where we periodically take one large unstable step and several small\nstable steps to compensate for the instability. These findings also help\nexplain the empirical observations of [Smith and Topin, 2019] where they show\nthat CLR with a large maximum learning rate can dramatically accelerate\nlearning and lead to so-called \"super-convergence\". We prove that our scheme\nexcels in the problems where Hessian exhibits a bimodal spectrum and the\neigenvalues can be grouped into two clusters (small and large). The unstably\nlarge step is the key to enabling fast convergence over the small\neigen-spectrum.",
          "link": "http://arxiv.org/abs/2102.10734",
          "publishedOn": "2021-08-11T01:55:22.158Z",
          "wordCount": 602,
          "title": "Provable Super-Convergence with a Large Cyclical Learning Rate. (arXiv:2102.10734v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hammar_K/0/1/0/all/0/1\">Kim Hammar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stadler_R/0/1/0/all/0/1\">Rolf Stadler</a>",
          "description": "We study automated intrusion prevention using reinforcement learning. In a\nnovel approach, we formulate the problem of intrusion prevention as an optimal\nstopping problem. This formulation allows us insight into the structure of the\noptimal policies, which turn out to be threshold based. Since the computation\nof the optimal defender policy using dynamic programming is not feasible for\npractical cases, we approximate the optimal policy through reinforcement\nlearning in a simulation environment. To define the dynamics of the simulation,\nwe emulate the target infrastructure and collect measurements. Our evaluations\nshow that the learned policies are close to optimal and that they indeed can be\nexpressed using thresholds.",
          "link": "http://arxiv.org/abs/2106.07160",
          "publishedOn": "2021-08-11T01:55:22.152Z",
          "wordCount": 591,
          "title": "Learning Intrusion Prevention Policies through Optimal Stopping. (arXiv:2106.07160v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1\">Andrei-Marius Avram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1\">Vasile Pais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1\">Dan Tufis</a>",
          "description": "EuroVoc is a multilingual thesaurus that was built for organizing the\nlegislative documentary of the European Union institutions. It contains\nthousands of categories at different levels of specificity and its descriptors\nare targeted by legal texts in almost thirty languages. In this work we propose\na unified framework for EuroVoc classification on 22 languages by fine-tuning\nmodern Transformer-based pretrained language models. We study extensively the\nperformance of our trained models and show that they significantly improve the\nresults obtained by a similar tool - JEX - on the same dataset. The code and\nthe fine-tuned models were open sourced, together with a programmatic interface\nthat eases the process of loading the weights of a trained model and of\nclassifying a new document.",
          "link": "http://arxiv.org/abs/2108.01139",
          "publishedOn": "2021-08-11T01:55:22.131Z",
          "wordCount": 594,
          "title": "PyEuroVoc: A Tool for Multilingual Legal Document Classification with EuroVoc Descriptors. (arXiv:2108.01139v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Infante_G/0/1/0/all/0/1\">Guillermo Infante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsson_A/0/1/0/all/0/1\">Anders Jonsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_V/0/1/0/all/0/1\">Vicen&#xe7; G&#xf3;mez</a>",
          "description": "In this work we present a novel approach to hierarchical reinforcement\nlearning for linearly-solvable Markov decision processes. Our approach assumes\nthat the state space is partitioned, and the subtasks consist in moving between\nthe partitions. We represent value functions on several levels of abstraction,\nand use the compositionality of subtasks to estimate the optimal values of the\nstates in each partition. The policy is implicitly defined on these optimal\nvalue estimates, rather than being decomposed among the subtasks. As a\nconsequence, our approach can learn the globally optimal policy, and does not\nsuffer from the non-stationarity of high-level decisions. If several partitions\nhave equivalent dynamics, the subtasks of those partitions can be shared. If\nthe set of boundary states is smaller than the entire state space, our approach\ncan have significantly smaller sample complexity than that of a flat learner,\nand we validate this empirically in several experiments.",
          "link": "http://arxiv.org/abs/2106.15380",
          "publishedOn": "2021-08-11T01:55:22.114Z",
          "wordCount": 610,
          "title": "Globally Optimal Hierarchical Reinforcement Learning for Linearly-Solvable Markov Decision Processes. (arXiv:2106.15380v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abrahamyan_L/0/1/0/all/0/1\">Lusine Abrahamyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziatchin_V/0/1/0/all/0/1\">Valentin Ziatchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1\">Nikos Deligiannis</a>",
          "description": "Compact convolutional neural networks (CNNs) have witnessed exceptional\nimprovements in performance in recent years. However, they still fail to\nprovide the same predictive power as CNNs with a large number of parameters.\nThe diverse and even abundant features captured by the layers is an important\ncharacteristic of these successful CNNs. However, differences in this\ncharacteristic between large CNNs and their compact counterparts have rarely\nbeen investigated. In compact CNNs, due to the limited number of parameters,\nabundant features are unlikely to be obtained, and feature diversity becomes an\nessential characteristic. Diverse features present in the activation maps\nderived from a data point during model inference may indicate the presence of a\nset of unique descriptors necessary to distinguish between objects of different\nclasses. In contrast, data points with low feature diversity may not provide a\nsufficient amount of unique descriptors to make a valid prediction; we refer to\nthem as random predictions. Random predictions can negatively impact the\noptimization process and harm the final performance. This paper proposes\naddressing the problem raised by random predictions by reshaping the standard\ncross-entropy to make it biased toward data points with a limited number of\nunique descriptive features. Our novel Bias Loss focuses the training on a set\nof valuable data points and prevents the vast number of samples with poor\nlearning features from misleading the optimization process. Furthermore, to\nshow the importance of diversity, we present a family of SkipNet models whose\narchitectures are brought to boost the number of unique descriptors in the last\nlayers. Our Skipnet-M can achieve 1% higher classification accuracy than\nMobileNetV3 Large.",
          "link": "http://arxiv.org/abs/2107.11170",
          "publishedOn": "2021-08-11T01:55:22.109Z",
          "wordCount": 744,
          "title": "Bias Loss for Mobile Neural Networks. (arXiv:2107.11170v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04682",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Sun_X/0/1/0/all/0/1\">Xiangyan Sun</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_K/0/1/0/all/0/1\">Ke Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lin_Y/0/1/0/all/0/1\">Yuquan Lin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wu_L/0/1/0/all/0/1\">Lingjie Wu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Xing_H/0/1/0/all/0/1\">Haoming Xing</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gao_M/0/1/0/all/0/1\">Minghong Gao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tan_S/0/1/0/all/0/1\">Suocheng Tan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ni_Z/0/1/0/all/0/1\">Zekun Ni</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Han_Q/0/1/0/all/0/1\">Qi Han</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wu_J/0/1/0/all/0/1\">Junqiu Wu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Fan_J/0/1/0/all/0/1\">Jie Fan</a>",
          "description": "We have developed an end-to-end, retrosynthesis system, named ChemiRise, that\ncan propose complete retrosynthesis routes for organic compounds rapidly and\nreliably. The system was trained on a processed patent database of over 3\nmillion organic reactions. Experimental reactions were atom-mapped, clustered,\nand extracted into reaction templates. We then trained a graph convolutional\nneural network-based one-step reaction proposer using template embeddings and\ndeveloped a guiding algorithm on the directed acyclic graph (DAG) of chemical\ncompounds to find the best candidate to explore. The atom-mapping algorithm and\nthe one-step reaction proposer were benchmarked against previous studies and\nshowed better results. The final product was demonstrated by retrosynthesis\nroutes reviewed and rated by human experts, showing satisfying functionality\nand a potential productivity boost in real-life use cases.",
          "link": "http://arxiv.org/abs/2108.04682",
          "publishedOn": "2021-08-11T01:55:22.102Z",
          "wordCount": 597,
          "title": "ChemiRise: a data-driven retrosynthesis engine. (arXiv:2108.04682v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06559",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Xu_W/0/1/0/all/0/1\">Winnie Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_R/0/1/0/all/0/1\">Ricky T.Q. Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1\">Xuechen Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Duvenaud_D/0/1/0/all/0/1\">David Duvenaud</a>",
          "description": "We perform scalable approximate inference in a continuous-depth Bayesian\nneural network family. In this model class, uncertainty about separate weights\nin each layer gives hidden units that follow a stochastic differential\nequation. We demonstrate gradient-based stochastic variational inference in\nthis infinite-parameter setting, producing arbitrarily-flexible approximate\nposteriors. We also derive a novel gradient estimator that approaches zero\nvariance as the approximate posterior over weights approaches the true\nposterior. This approach brings continuous-depth Bayesian neural nets to a\ncompetitive comparison against discrete-depth alternatives, while inheriting\nthe memory-efficient training and tunable precision of Neural ODEs.",
          "link": "http://arxiv.org/abs/2102.06559",
          "publishedOn": "2021-08-11T01:55:22.062Z",
          "wordCount": 555,
          "title": "Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations. (arXiv:2102.06559v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05568",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aman Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothyari_M/0/1/0/all/0/1\">Mayank Kothyari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vishwajeet Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Soumen Chakrabarti</a>",
          "description": "Multimodal IR, spanning text corpus, knowledge graph and images, called\noutside knowledge visual question answering (OKVQA), is of much recent\ninterest. However, the popular data set has serious limitations. A surprisingly\nlarge fraction of queries do not assess the ability to integrate cross-modal\ninformation. Instead, some are independent of the image, some depend on\nspeculation, some require OCR or are otherwise answerable from the image alone.\nTo add to the above limitations, frequency-based guessing is very effective\nbecause of (unintended) widespread answer overlaps between the train and test\nfolds. Overall, it is hard to determine when state-of-the-art systems exploit\nthese weaknesses rather than really infer the answers, because they are opaque\nand their 'reasoning' process is uninterpretable. An equally important\nlimitation is that the dataset is designed for the quantitative assessment only\nof the end-to-end answer retrieval task, with no provision for assessing the\ncorrect(semantic) interpretation of the input query. In response, we identify a\nkey structural idiom in OKVQA ,viz., S3 (select, substitute and search), and\nbuild a new data set and challenge around it. Specifically, the questioner\nidentifies an entity in the image and asks a question involving that entity\nwhich can be answered only by consulting a knowledge graph or corpus passage\nmentioning the entity. Our challenge consists of (i)OKVQAS3, a subset of OKVQA\nannotated based on the structural idiom and (ii)S3VQA, a new dataset built from\nscratch. We also present a neural but structurally transparent OKVQA system,\nS3, that explicitly addresses our challenge dataset, and outperforms recent\ncompetitive baselines.",
          "link": "http://arxiv.org/abs/2103.05568",
          "publishedOn": "2021-08-11T01:55:22.046Z",
          "wordCount": 749,
          "title": "Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering. (arXiv:2103.05568v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>",
          "description": "Semi-supervised machine learning models learn from a (small) set of labeled\ntraining examples, and a (large) set of unlabeled training examples.\nState-of-the-art models can reach within a few percentage points of\nfully-supervised training, while requiring 100x less labeled data.\n\nWe study a new class of vulnerabilities: poisoning attacks that modify the\nunlabeled dataset. In order to be useful, unlabeled datasets are given strictly\nless review than labeled datasets, and adversaries can therefore poison them\neasily. By inserting maliciously-crafted unlabeled examples totaling just 0.1%\nof the dataset size, we can manipulate a model trained on this poisoned dataset\nto misclassify arbitrary examples at test time (as any desired label). Our\nattacks are highly effective across datasets and semi-supervised learning\nmethods.\n\nWe find that more accurate methods (thus more likely to be used) are\nsignificantly more vulnerable to poisoning attacks, and as such better training\nmethods are unlikely to prevent this attack. To counter this we explore the\nspace of defenses, and propose two methods that mitigate our attack.",
          "link": "http://arxiv.org/abs/2105.01622",
          "publishedOn": "2021-08-11T01:55:22.041Z",
          "wordCount": 627,
          "title": "Poisoning the Unlabeled Dataset of Semi-Supervised Learning. (arXiv:2105.01622v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06035",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinkai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhixiong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guowei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xintan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongchao Liu</a>",
          "description": "Graph neural networks (GNNs) have been popularly used in analyzing\ngraph-structured data, showing promising results in various applications such\nas node classification, link prediction and network recommendation. In this\npaper, we present a new graph attention neural network, namely GIPA, for\nattributed graph data learning. GIPA consists of three key components:\nattention, feature propagation and aggregation. Specifically, the attention\ncomponent introduces a new multi-layer perceptron based multi-head to generate\nbetter non-linear feature mapping and representation than conventional\nimplementations such as dot-product. The propagation component considers not\nonly node features but also edge features, which differs from existing GNNs\nthat merely consider node features. The aggregation component uses a residual\nconnection to generate the final embedding. We evaluate the performance of GIPA\nusing the Open Graph Benchmark proteins (ogbn-proteins for short) dataset. The\nexperimental results reveal that GIPA can beat the state-of-the-art models in\nterms of prediction accuracy, e.g., GIPA achieves an average test ROC-AUC of\n$0.8700\\pm 0.0010$ and outperforms all the previous methods listed in the\nogbn-proteins leaderboard.",
          "link": "http://arxiv.org/abs/2105.06035",
          "publishedOn": "2021-08-11T01:55:21.993Z",
          "wordCount": 641,
          "title": "GIPA: General Information Propagation Algorithm for Graph Learning. (arXiv:2105.06035v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kojima_N/0/1/0/all/0/1\">Noriyuki Kojima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1\">Alane Suhr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>",
          "description": "We study continual learning for natural language instruction generation, by\nobserving human users' instruction execution. We focus on a collaborative\nscenario, where the system both acts and delegates tasks to human users using\nnatural language. We compare user execution of generated instructions to the\noriginal system intent as an indication to the system's success communicating\nits intent. We show how to use this signal to improve the system's ability to\ngenerate instructions via contextual bandit learning. In interaction with real\nusers, our system demonstrates dramatic improvements in its ability to generate\nlanguage over time.",
          "link": "http://arxiv.org/abs/2108.04812",
          "publishedOn": "2021-08-11T01:55:21.891Z",
          "wordCount": 553,
          "title": "Continual Learning for Grounded Instruction Generation by Observing Human Following Behavior. (arXiv:2108.04812v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1808.00560",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yijue Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1\">Feng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchiori_E/0/1/0/all/0/1\">Elena Marchiori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theodoridis_S/0/1/0/all/0/1\">Sergios Theodoridis</a>",
          "description": "Spectral mixture (SM) kernels comprise a powerful class of kernels for\nGaussian processes (GPs) capable of discovering structurally complex patterns\nand modeling negative covariances. Being a linear superposition of\nquasi-periodical kernel components, the state-of-the-art SM kernel does not\nconsider component compression and dependency structures between components. In\nthis paper, we investigate the benefits of component compression and modeling\nof both time and phase delay structures between basis components in the SM\nkernel. By verifying the presence of dependencies between function components\nusing Gaussian conditionals and posterior covariance, we first propose a new SM\nkernel variant with a time and phase delay dependency structure (SMD) and then\nprovide a structure adaptation (SA) algorithm for the SMD. The SMD kernel is\nconstructed in two steps: first, time delay and phase delay are incorporated\ninto each basis component; next, cross-convolution between a basis component\nand the reversed complex conjugate of another basis component is performed,\nwhich yields a complex-valued and positive definite kernel incorporating\ndependency structures between basis components. The model compression and\ndependency sparsity of the SMD kernel can be obtained by using automatic\npruning in SA. We perform a thorough comparative experimental analysis of the\nSMD on both synthetic and real-life datasets. The results corroborate the\nefficacy of the dependency structure and SA in the SMD.",
          "link": "http://arxiv.org/abs/1808.00560",
          "publishedOn": "2021-08-11T01:55:21.885Z",
          "wordCount": 745,
          "title": "Novel Compressible Adaptive Spectral Mixture Kernels for Gaussian Processes with Sparse Time and Phase Delay Structures. (arXiv:1808.00560v7 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.11890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1\">Zhaohan Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Ren Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Ting Wang</a>",
          "description": "One intriguing property of deep neural networks (DNNs) is their inherent\nvulnerability to backdoor attacks -- a trojan model responds to\ntrigger-embedded inputs in a highly predictable manner while functioning\nnormally otherwise. Despite the plethora of prior work on DNNs for continuous\ndata (e.g., images), the vulnerability of graph neural networks (GNNs) for\ndiscrete-structured data (e.g., graphs) is largely unexplored, which is highly\nconcerning given their increasing use in security-sensitive domains. To bridge\nthis gap, we present GTA, the first backdoor attack on GNNs. Compared with\nprior work, GTA departs in significant ways: graph-oriented -- it defines\ntriggers as specific subgraphs, including both topological structures and\ndescriptive features, entailing a large design spectrum for the adversary;\ninput-tailored -- it dynamically adapts triggers to individual graphs, thereby\noptimizing both attack effectiveness and evasiveness; downstream model-agnostic\n-- it can be readily launched without knowledge regarding downstream models or\nfine-tuning strategies; and attack-extensible -- it can be instantiated for\nboth transductive (e.g., node classification) and inductive (e.g., graph\nclassification) tasks, constituting severe threats for a range of\nsecurity-critical applications. Through extensive evaluation using benchmark\ndatasets and state-of-the-art models, we demonstrate the effectiveness of GTA.\nWe further provide analytical justification for its effectiveness and discuss\npotential countermeasures, pointing to several promising research directions.",
          "link": "http://arxiv.org/abs/2006.11890",
          "publishedOn": "2021-08-11T01:55:21.878Z",
          "wordCount": 699,
          "title": "Graph Backdoor. (arXiv:2006.11890v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Izzo_C/0/1/0/all/0/1\">Cosimo Izzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1\">Aldo Lipani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhrati_R/0/1/0/all/0/1\">Ramin Okhrati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medda_F/0/1/0/all/0/1\">Francesca Medda</a>",
          "description": "Deep neural networks have gained momentum based on their accuracy, but their\ninterpretability is often criticised. As a result, they are labelled as black\nboxes. In response, several methods have been proposed in the literature to\nexplain their predictions. Among the explanatory methods, Shapley values is a\nfeature attribution method favoured for its robust theoretical foundation.\nHowever, the analysis of feature attributions using Shapley values requires\nchoosing a baseline that represents the concept of missingness. An arbitrary\nchoice of baseline could negatively impact the explanatory power of the method\nand possibly lead to incorrect interpretations. In this paper, we present a\nmethod for choosing a baseline according to a neutrality value: as a parameter\nselected by decision-makers, the point at which their choices are determined by\nthe model predictions being either above or below it. Hence, the proposed\nbaseline is set based on a parameter that depends on the actual use of the\nmodel. This procedure stands in contrast to how other baselines are set, i.e.\nwithout accounting for how the model is used. We empirically validate our\nchoice of baseline in the context of binary classification tasks, using two\ndatasets: a synthetic dataset and a dataset derived from the financial domain.",
          "link": "http://arxiv.org/abs/2006.04896",
          "publishedOn": "2021-08-11T01:55:21.873Z",
          "wordCount": 679,
          "title": "A Baseline for Shapley Values in MLPs: from Missingness to Neutrality. (arXiv:2006.04896v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kers_J/0/1/0/all/0/1\">Jesper Kers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassol_C/0/1/0/all/0/1\">Clarissa A. Cassol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_J/0/1/0/all/0/1\">Joris J. Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Idrees_N/0/1/0/all/0/1\">Najia Idrees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farber_A/0/1/0/all/0/1\">Alik Farber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haroon_S/0/1/0/all/0/1\">Samir Haroon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daly_K/0/1/0/all/0/1\">Kevin P. Daly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Suvranu Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitalia_V/0/1/0/all/0/1\">Vipul C. Chitalia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolachalama_V/0/1/0/all/0/1\">Vijaya B. Kolachalama</a>",
          "description": "Development of deep learning systems for biomedical segmentation often\nrequires access to expert-driven, manually annotated datasets. If more than a\nsingle expert is involved in the annotation of the same images, then the\ninter-expert agreement is not necessarily perfect, and no single expert\nannotation can precisely capture the so-called ground truth of the regions of\ninterest on all images. Also, it is not trivial to generate a reference\nestimate using annotations from multiple experts. Here we present a deep neural\nnetwork, defined as U-Net-and-a-half, which can simultaneously learn from\nannotations performed by multiple experts on the same set of images.\nU-Net-and-a-half contains a convolutional encoder to generate features from the\ninput images, multiple decoders that allow simultaneous learning from image\nmasks obtained from annotations that were independently generated by multiple\nexperts, and a shared low-dimensional feature space. To demonstrate the\napplicability of our framework, we used two distinct datasets from digital\npathology and radiology, respectively. Specifically, we trained two separate\nmodels using pathologist-driven annotations of glomeruli on whole slide images\nof human kidney biopsies (10 patients), and radiologist-driven annotations of\nlumen cross-sections of human arteriovenous fistulae obtained from\nintravascular ultrasound images (10 patients), respectively. The models based\non U-Net-and-a-half exceeded the performance of the traditional U-Net models\ntrained on single expert annotations alone, thus expanding the scope of\nmultitask learning in the context of biomedical image segmentation.",
          "link": "http://arxiv.org/abs/2108.04658",
          "publishedOn": "2021-08-11T01:55:21.851Z",
          "wordCount": 697,
          "title": "U-Net-and-a-half: Convolutional network for biomedical image segmentation using multiple expert-driven annotations. (arXiv:2108.04658v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Moyu Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinning Zhu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunhong Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yang Ji</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Feng Pan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Changchuan Yin</a> (1) ((1) Beijing University of Posts and Telecommunications)",
          "description": "With the increasing demands of personalized learning, knowledge tracing has\nbecome important which traces students' knowledge states based on their\nhistorical practices. Factor analysis methods mainly use two kinds of factors\nwhich are separately related to students and questions to model students'\nknowledge states. These methods use the total number of attempts of students to\nmodel students' learning progress and hardly highlight the impact of the most\nrecent relevant practices. Besides, current factor analysis methods ignore rich\ninformation contained in questions. In this paper, we propose Multi-Factors\nAware Dual-Attentional model (MF-DAKT) which enriches question representations\nand utilizes multiple factors to model students' learning progress based on a\ndual-attentional mechanism. More specifically, we propose a novel\nstudent-related factor which records the most recent attempts on relevant\nconcepts of students to highlight the impact of recent exercises. To enrich\nquestions representations, we use a pre-training method to incorporate two\nkinds of question information including questions' relation and difficulty\nlevel. We also add a regularization term about questions' difficulty level to\nrestrict pre-trained question representations to fine-tuning during the process\nof predicting students' performance. Moreover, we apply a dual-attentional\nmechanism to differentiate contributions of factors and factor interactions to\nfinal prediction in different practice records. At last, we conduct experiments\non several real-world datasets and results show that MF-DAKT can outperform\nexisting knowledge tracing methods. We also conduct several studies to validate\nthe effects of each component of MF-DAKT.",
          "link": "http://arxiv.org/abs/2108.04741",
          "publishedOn": "2021-08-11T01:55:21.846Z",
          "wordCount": 691,
          "title": "Multi-Factors Aware Dual-Attentional Knowledge Tracing. (arXiv:2108.04741v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2009.05673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heaton_J/0/1/0/all/0/1\">Jeff Heaton</a>",
          "description": "Deep learning is a group of exciting new technologies for neural networks.\nThrough a combination of advanced training techniques and neural network\narchitectural components, it is now possible to create neural networks that can\nhandle tabular data, images, text, and audio as both input and output. Deep\nlearning allows a neural network to learn hierarchies of information in a way\nthat is like the function of the human brain. This course will introduce the\nstudent to classic neural network structures, Convolution Neural Networks\n(CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU),\nGeneral Adversarial Networks (GAN), and reinforcement learning. Application of\nthese architectures to computer vision, time series, security, natural language\nprocessing (NLP), and data generation will be covered. High-Performance\nComputing (HPC) aspects will demonstrate how deep learning can be leveraged\nboth on graphical processing units (GPUs), as well as grids. Focus is primarily\nupon the application of deep learning to problems, with some introduction to\nmathematical foundations. Readers will use the Python programming language to\nimplement deep learning using Google TensorFlow and Keras. It is not necessary\nto know Python prior to this book; however, familiarity with at least one\nprogramming language is assumed.",
          "link": "http://arxiv.org/abs/2009.05673",
          "publishedOn": "2021-08-11T01:55:21.821Z",
          "wordCount": 656,
          "title": "Applications of Deep Neural Networks. (arXiv:2009.05673v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haoyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhize Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1\">Peter Richt&#xe1;rik</a>",
          "description": "Federated Averaging (FedAvg, also known as Local-SGD) (McMahan et al., 2017)\nis a classical federated learning algorithm in which clients run multiple local\nSGD steps before communicating their update to an orchestrating server. We\npropose a new federated learning algorithm, FedPAGE, able to further reduce the\ncommunication complexity by utilizing the recent optimal PAGE method (Li et\nal., 2021) instead of plain SGD in FedAvg. We show that FedPAGE uses much fewer\ncommunication rounds than previous local methods for both federated convex and\nnonconvex optimization. Concretely, 1) in the convex setting, the number of\ncommunication rounds of FedPAGE is $O(\\frac{N^{3/4}}{S\\epsilon})$, improving\nthe best-known result $O(\\frac{N}{S\\epsilon})$ of SCAFFOLD (Karimireddy et\nal.,2020) by a factor of $N^{1/4}$, where $N$ is the total number of clients\n(usually is very large in federated learning), $S$ is the sampled subset of\nclients in each communication round, and $\\epsilon$ is the target error; 2) in\nthe nonconvex setting, the number of communication rounds of FedPAGE is\n$O(\\frac{\\sqrt{N}+S}{S\\epsilon^2})$, improving the best-known result\n$O(\\frac{N^{2/3}}{S^{2/3}\\epsilon^2})$ of SCAFFOLD (Karimireddy et al.,2020) by\na factor of $N^{1/6}S^{1/3}$, if the sampled clients $S\\leq \\sqrt{N}$. Note\nthat in both settings, the communication cost for each round is the same for\nboth FedPAGE and SCAFFOLD. As a result, FedPAGE achieves new state-of-the-art\nresults in terms of communication complexity for both federated convex and\nnonconvex optimization.",
          "link": "http://arxiv.org/abs/2108.04755",
          "publishedOn": "2021-08-11T01:55:21.815Z",
          "wordCount": 677,
          "title": "FedPAGE: A Fast Local Stochastic Gradient Method for Communication-Efficient Federated Learning. (arXiv:2108.04755v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04742",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_N/0/1/0/all/0/1\">Nat&#xe1;lia V. N. Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abramo_L/0/1/0/all/0/1\">L. Raul Abramo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirata_N/0/1/0/all/0/1\">Nina S. Hirata</a>",
          "description": "Errors in measurements are key to weighting the value of data, but are often\nneglected in Machine Learning (ML). We show how Convolutional Neural Networks\n(CNNs) are able to learn about the context and patterns of signal and noise,\nleading to improvements in the performance of classification methods. We\nconstruct a model whereby two classes of objects follow an underlying Gaussian\ndistribution, and where the features (the input data) have varying, but known,\nlevels of noise. This model mimics the nature of scientific data sets, where\nthe noises arise as realizations of some random processes whose underlying\ndistributions are known. The classification of these objects can then be\nperformed using standard statistical techniques (e.g., least-squares\nminimization or Markov-Chain Monte Carlo), as well as ML techniques. This\nallows us to take advantage of a maximum likelihood approach to object\nclassification, and to measure the amount by which the ML methods are\nincorporating the information in the input data uncertainties. We show that,\nwhen each data point is subject to different levels of noise (i.e., noises with\ndifferent distribution functions), that information can be learned by the CNNs,\nraising the ML performance to at least the same level of the least-squares\nmethod -- and sometimes even surpassing it. Furthermore, we show that, with\nvarying noise levels, the confidence of the ML classifiers serves as a proxy\nfor the underlying cumulative distribution function, but only if the\ninformation about specific input data uncertainties is provided to the CNNs.",
          "link": "http://arxiv.org/abs/2108.04742",
          "publishedOn": "2021-08-11T01:55:21.809Z",
          "wordCount": 702,
          "title": "The information of attribute uncertainties: what convolutional neural networks can learn about errors in input data. (arXiv:2108.04742v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.05244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_a/0/1/0/all/0/1\">and Jianjun Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jing-Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zheng-Hua Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jun Guo</a>",
          "description": "Due to lack of data, overfitting ubiquitously exists in real-world\napplications of deep neural networks (DNNs). We propose advanced dropout, a\nmodel-free methodology, to mitigate overfitting and improve the performance of\nDNNs. The advanced dropout technique applies a model-free and easily\nimplemented distribution with parametric prior, and adaptively adjusts dropout\nrate. Specifically, the distribution parameters are optimized by stochastic\ngradient variational Bayes in order to carry out an end-to-end training. We\nevaluate the effectiveness of the advanced dropout against nine dropout\ntechniques on seven computer vision datasets (five small-scale datasets and two\nlarge-scale datasets) with various base models. The advanced dropout\noutperforms all the referred techniques on all the datasets.We further compare\nthe effectiveness ratios and find that advanced dropout achieves the highest\none on most cases. Next, we conduct a set of analysis of dropout rate\ncharacteristics, including convergence of the adaptive dropout rate, the\nlearned distributions of dropout masks, and a comparison with dropout rate\ngeneration without an explicit distribution. In addition, the ability of\noverfitting prevention is evaluated and confirmed. Finally, we extend the\napplication of the advanced dropout to uncertainty inference, network pruning,\ntext classification, and regression. The proposed advanced dropout is also\nsuperior to the corresponding referred methods. Codes are available at\nhttps://github.com/PRIS-CV/AdvancedDropout.",
          "link": "http://arxiv.org/abs/2010.05244",
          "publishedOn": "2021-08-11T01:55:21.803Z",
          "wordCount": 693,
          "title": "Advanced Dropout: A Model-free Methodology for Bayesian Dropout Optimization. (arXiv:2010.05244v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.06429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lukina_A/0/1/0/all/0/1\">Anna Lukina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schilling_C/0/1/0/all/0/1\">Christian Schilling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henzinger_T/0/1/0/all/0/1\">Thomas A. Henzinger</a>",
          "description": "Neural-network classifiers achieve high accuracy when predicting the class of\nan input that they were trained to identify. Maintaining this accuracy in\ndynamic environments, where inputs frequently fall outside the fixed set of\ninitially known classes, remains a challenge. The typical approach is to detect\ninputs from novel classes and retrain the classifier on an augmented dataset.\nHowever, not only the classifier but also the detection mechanism needs to\nadapt in order to distinguish between newly learned and yet unknown input\nclasses. To address this challenge, we introduce an algorithmic framework for\nactive monitoring of a neural network. A monitor wrapped in our framework\noperates in parallel with the neural network and interacts with a human user\nvia a series of interpretable labeling queries for incremental adaptation. In\naddition, we propose an adaptive quantitative monitor to improve precision. An\nexperimental evaluation on a diverse set of benchmarks with varying numbers of\nclasses confirms the benefits of our active monitoring framework in dynamic\nscenarios.",
          "link": "http://arxiv.org/abs/2009.06429",
          "publishedOn": "2021-08-11T01:55:21.797Z",
          "wordCount": 645,
          "title": "Into the Unknown: Active Monitoring of Neural Networks. (arXiv:2009.06429v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bonassi_F/0/1/0/all/0/1\">Fabio Bonassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scattolini_R/0/1/0/all/0/1\">Riccardo Scattolini</a>",
          "description": "Owing to their superior modeling capabilities, gated Recurrent Neural\nNetworks (RNNs), such as Gated Recurrent Units (GRUs) and Long Short-Term\nMemory networks (LSTMs), have become popular tools for learning dynamical\nsystems. This paper aims to discuss how these networks can be adopted for the\nsynthesis of Internal Model Control (IMC) architectures. To this end, a first\ngated RNN is used to learn a model of the unknown input-output stable plant.\nThen, another gated RNN approximating the model inverse is trained. The\nproposed scheme is able to cope with the saturation of the control variables,\nand it can be deployed on low-power embedded controllers since it does not\nrequire any online computation. The approach is then tested on the Quadruple\nTank benchmark system, resulting in satisfactory closed-loop performances.",
          "link": "http://arxiv.org/abs/2108.04585",
          "publishedOn": "2021-08-11T01:55:21.791Z",
          "wordCount": 596,
          "title": "Recurrent neural network-based Internal Model Control of unknown nonlinear stable systems. (arXiv:2108.04585v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulanikar_A/0/1/0/all/0/1\">Abhiram Anand Gulanikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_L/0/1/0/all/0/1\">Lov Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neti_L/0/1/0/all/0/1\">Lalita Bhanu Murthy Neti</a>",
          "description": "A code smell is a surface indicator of an inherent problem in the system,\nmost often due to deviation from standard coding practices on the developers\npart during the development phase. Studies observe that code smells made the\ncode more susceptible to call for modifications and corrections than code that\ndid not contain code smells. Restructuring the code at the early stage of\ndevelopment saves the exponentially increasing amount of effort it would\nrequire to address the issues stemming from the presence of these code smells.\nInstead of using traditional features to detect code smells, we use user\ncomments to manually construct features to predict code smells. We use three\nExtreme learning machine kernels over 629 packages to identify eight code\nsmells by leveraging feature engineering aspects and using sampling techniques.\nOur findings indicate that the radial basis functional kernel performs best out\nof the three kernel methods with a mean accuracy of 98.52.",
          "link": "http://arxiv.org/abs/2108.04656",
          "publishedOn": "2021-08-11T01:55:21.780Z",
          "wordCount": 628,
          "title": "Empirical Analysis on Effectiveness of NLP Methods for Predicting Code Smell. (arXiv:2108.04656v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baier_L/0/1/0/all/0/1\">Lucas Baier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhl_N/0/1/0/all/0/1\">Niklas K&#xfc;hl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoffer_J/0/1/0/all/0/1\">Jakob Sch&#xf6;ffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satzger_G/0/1/0/all/0/1\">Gerhard Satzger</a>",
          "description": "As a reaction to the high infectiousness and lethality of the COVID-19 virus,\ncountries around the world have adopted drastic policy measures to contain the\npandemic. However, it remains unclear which effect these measures, so-called\nnon-pharmaceutical interventions (NPIs), have on the spread of the virus. In\nthis article, we use machine learning and apply drift detection methods in a\nnovel way to predict the time lag of policy interventions with respect to the\ndevelopment of daily case numbers of COVID-19 across 9 European countries and\n28 US states. Our analysis shows that there are, on average, more than two\nweeks between NPI enactment and a drift in the case numbers.",
          "link": "http://arxiv.org/abs/2012.03728",
          "publishedOn": "2021-08-11T01:55:21.757Z",
          "wordCount": 630,
          "title": "Utilizing Concept Drift for Measuring the Effectiveness of Policy Interventions: The Case of the COVID-19 Pandemic. (arXiv:2012.03728v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.02829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nakano_F/0/1/0/all/0/1\">Felipe Kenji Nakano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pliakos_K/0/1/0/all/0/1\">Konstantinos Pliakos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vens_C/0/1/0/all/0/1\">Celine Vens</a>",
          "description": "Recently, deep neural networks have expanded the state-of-art in various\nscientific fields and provided solutions to long standing problems across\nmultiple application domains. Nevertheless, they also suffer from weaknesses\nsince their optimal performance depends on massive amounts of training data and\nthe tuning of an extended number of parameters. As a countermeasure, some\ndeep-forest methods have been recently proposed, as efficient and low-scale\nsolutions. Despite that, these approaches simply employ label classification\nprobabilities as induced features and primarily focus on traditional\nclassification and regression tasks, leaving multi-output prediction\nunder-explored. Moreover, recent work has demonstrated that tree-embeddings are\nhighly representative, especially in structured output prediction. In this\ndirection, we propose a novel deep tree-ensemble (DTE) model, where every layer\nenriches the original feature set with a representation learning component\nbased on tree-embeddings. In this paper, we specifically focus on two\nstructured output prediction tasks, namely multi-label classification and\nmulti-target regression. We conducted experiments using multiple benchmark\ndatasets and the obtained results confirm that our method provides superior\nresults to state-of-the-art methods in both tasks.",
          "link": "http://arxiv.org/abs/2011.02829",
          "publishedOn": "2021-08-11T01:55:21.749Z",
          "wordCount": 626,
          "title": "Deep tree-ensembles for multi-output prediction. (arXiv:2011.02829v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04526",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1\">Qingpeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yiyuan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongle Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meihui Zhang</a>",
          "description": "Data processing and analytics are fundamental and pervasive. Algorithms play\na vital role in data processing and analytics where many algorithm designs have\nincorporated heuristics and general rules from human knowledge and experience\nto improve their effectiveness. Recently, reinforcement learning, deep\nreinforcement learning (DRL) in particular, is increasingly explored and\nexploited in many areas because it can learn better strategies in complicated\nenvironments it is interacting with than statically designed algorithms.\nMotivated by this trend, we provide a comprehensive review of recent works\nfocusing on utilizing deep reinforcement learning to improve data processing\nand analytics. First, we present an introduction to key concepts, theories, and\nmethods in deep reinforcement learning. Next, we discuss deep reinforcement\nlearning deployment on database systems, facilitating data processing and\nanalytics in various aspects, including data organization, scheduling, tuning,\nand indexing. Then, we survey the application of deep reinforcement learning in\ndata processing and analytics, ranging from data preparation, natural language\ninterface to healthcare, fintech, etc. Finally, we discuss important open\nchallenges and future research directions of using deep reinforcement learning\nin data processing and analytics.",
          "link": "http://arxiv.org/abs/2108.04526",
          "publishedOn": "2021-08-11T01:55:21.725Z",
          "wordCount": 629,
          "title": "A Survey on Deep Reinforcement Learning for Data Processing and Analytics. (arXiv:2108.04526v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.01184",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Polo_F/0/1/0/all/0/1\">Felipe Maia Polo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vicente_R/0/1/0/all/0/1\">Renato Vicente</a>",
          "description": "In supervised learning, training and test datasets are often sampled from\ndistinct distributions. Domain adaptation techniques are thus required.\nCovariate shift adaptation yields good generalization performance when domains\ndiffer only by the marginal distribution of features. Covariate shift\nadaptation is usually implemented using importance weighting, which may fail,\naccording to common wisdom, due to small effective sample sizes (ESS). Previous\nresearch argues this scenario is more common in high-dimensional settings.\nHowever, how effective sample size, dimensionality, and model\nperformance/generalization are formally related in supervised learning,\nconsidering the context of covariate shift adaptation, is still somewhat\nobscure in the literature. Thus, a main challenge is presenting a unified\ntheory connecting those points. Hence, in this paper, we focus on building a\nunified view connecting the ESS, data dimensionality, and generalization in the\ncontext of covariate shift adaptation. Moreover, we also demonstrate how\ndimensionality reduction or feature selection can increase the ESS, and argue\nthat our results support dimensionality reduction before covariate shift\nadaptation as a good practice.",
          "link": "http://arxiv.org/abs/2010.01184",
          "publishedOn": "2021-08-11T01:55:21.718Z",
          "wordCount": 634,
          "title": "Effective Sample Size, Dimensionality, and Generalization in Covariate Shift Adaptation. (arXiv:2010.01184v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10443",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Unlu_A/0/1/0/all/0/1\">Ali Unlu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1\">Laurence Aitchison</a>",
          "description": "We develop variational Laplace for Bayesian neural networks (BNNs) which\nexploits a local approximation of the curvature of the likelihood to estimate\nthe ELBO without the need for stochastic sampling of the neural-network\nweights. The Variational Laplace objective is simple to evaluate, as it is (in\nessence) the log-likelihood, plus weight-decay, plus a squared-gradient\nregularizer. Variational Laplace gave better test performance and expected\ncalibration errors than maximum a-posteriori inference and standard\nsampling-based variational inference, despite using the same variational\napproximate posterior. Finally, we emphasise care needed in benchmarking\nstandard VI as there is a risk of stopping before the variance parameters have\nconverged. We show that early-stopping can be avoided by increasing the\nlearning rate for the variance parameters.",
          "link": "http://arxiv.org/abs/2011.10443",
          "publishedOn": "2021-08-11T01:55:21.705Z",
          "wordCount": 562,
          "title": "Variational Laplace for Bayesian neural networks. (arXiv:2011.10443v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.07588",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "Deep learning motivated by convolutional neural networks has been highly\nsuccessful in a range of medical imaging problems like image classification,\nimage segmentation, image synthesis etc. However for validation and\ninterpretability, not only do we need the predictions made by the model but\nalso how confident it is while making those predictions. This is important in\nsafety critical applications for the people to accept it. In this work, we used\nan encoder decoder architecture based on variational inference techniques for\nsegmenting brain tumour images. We evaluate our work on the publicly available\nBRATS dataset using Dice Similarity Coefficient (DSC) and Intersection Over\nUnion (IOU) as the evaluation metrics. Our model is able to segment brain\ntumours while taking into account both aleatoric uncertainty and epistemic\nuncertainty in a principled bayesian manner.",
          "link": "http://arxiv.org/abs/2008.07588",
          "publishedOn": "2021-08-11T01:55:21.693Z",
          "wordCount": 602,
          "title": "Uncertainty Quantification using Variational Inference for Biomedical Image Segmentation. (arXiv:2008.07588v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.04575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dongliang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jun Guo</a>",
          "description": "Channel attention mechanisms have been commonly applied in many visual tasks\nfor effective performance improvement. It is able to reinforce the informative\nchannels as well as to suppress the useless channels. Recently, different\nchannel attention modules have been proposed and implemented in various ways.\nGenerally speaking, they are mainly based on convolution and pooling\noperations. In this paper, we propose Gaussian process embedded channel\nattention (GPCA) module and further interpret the channel attention schemes in\na probabilistic way. The GPCA module intends to model the correlations among\nthe channels, which are assumed to be captured by beta distributed variables.\nAs the beta distribution cannot be integrated into the end-to-end training of\nconvolutional neural networks (CNNs) with a mathematically tractable solution,\nwe utilize an approximation of the beta distribution to solve this problem. To\nspecify, we adapt a Sigmoid-Gaussian approximation, in which the Gaussian\ndistributed variables are transferred into the interval [0,1]. The Gaussian\nprocess is then utilized to model the correlations among different channels. In\nthis case, a mathematically tractable solution is derived. The GPCA module can\nbe efficiently implemented and integrated into the end-to-end training of the\nCNNs. Experimental results demonstrate the promising performance of the\nproposed GPCA module. Codes are available at https://github.com/PRIS-CV/GPCA.",
          "link": "http://arxiv.org/abs/2003.04575",
          "publishedOn": "2021-08-11T01:55:21.678Z",
          "wordCount": 684,
          "title": "GPCA: A Probabilistic Framework for Gaussian Process Embedded Channel Attention. (arXiv:2003.04575v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04659",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_T/0/1/0/all/0/1\">Tanmay G. Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_L/0/1/0/all/0/1\">Lov Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neti_L/0/1/0/all/0/1\">Lalita Bhanu Murthy Neti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_A/0/1/0/all/0/1\">Aneesh Krishna</a>",
          "description": "Code Smell, similar to a bad smell, is a surface indication of something\ntainted but in terms of software writing practices. This metric is an\nindication of a deeper problem lies within the code and is associated with an\nissue which is prominent to experienced software developers with acceptable\ncoding practices. Recent studies have often observed that codes having code\nsmells are often prone to a higher probability of change in the software\ndevelopment cycle. In this paper, we developed code smell prediction models\nwith the help of features extracted from source code to predict eight types of\ncode smell. Our work also presents the application of data sampling techniques\nto handle class imbalance problem and feature selection techniques to find\nrelevant feature sets. Previous studies had made use of techniques such as\nNaive - Bayes and Random forest but had not explored deep learning methods to\npredict code smell. A total of 576 distinct Deep Learning models were trained\nusing the features and datasets mentioned above. The study concluded that the\ndeep learning models which used data from Synthetic Minority Oversampling\nTechnique gave better results in terms of accuracy, AUC with the accuracy of\nsome models improving from 88.47 to 96.84.",
          "link": "http://arxiv.org/abs/2108.04659",
          "publishedOn": "2021-08-11T01:55:21.663Z",
          "wordCount": 695,
          "title": "An Empirical Study on Predictability of Software Code Smell Using Deep Learning Models. (arXiv:2108.04659v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Viet-Anh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salha_Galvan_G/0/1/0/all/0/1\">Guillaume Salha-Galvan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennequin_R/0/1/0/all/0/1\">Romain Hennequin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussallam_M/0/1/0/all/0/1\">Manuel Moussallam</a>",
          "description": "Collaborative Metric Learning (CML) recently emerged as a powerful paradigm\nfor recommendation based on implicit feedback collaborative filtering. However,\nstandard CML methods learn fixed user and item representations, which fails to\ncapture the complex interests of users. Existing extensions of CML also either\nignore the heterogeneity of user-item relations, i.e. that a user can\nsimultaneously like very different items, or the latent item-item relations,\ni.e. that a user's preference for an item depends, not only on its intrinsic\ncharacteristics, but also on items they previously interacted with. In this\npaper, we present a hierarchical CML model that jointly captures latent\nuser-item and item-item relations from implicit data. Our approach is inspired\nby translation mechanisms from knowledge graph embedding and leverages\nmemory-based attention networks. We empirically show the relevance of this\njoint relational modeling, by outperforming existing CML models on\nrecommendation tasks on several real-world datasets. Our experiments also\nemphasize the limits of current CML relational models on very sparse datasets.",
          "link": "http://arxiv.org/abs/2108.04655",
          "publishedOn": "2021-08-11T01:55:21.655Z",
          "wordCount": 602,
          "title": "Hierarchical Latent Relation Modeling for Collaborative Metric Learning. (arXiv:2108.04655v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stadnick_B/0/1/0/all/0/1\">Benjamin Stadnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witowski_J/0/1/0/all/0/1\">Jan Witowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajiv_V/0/1/0/all/0/1\">Vishwaesh Rajiv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chledowski_J/0/1/0/all/0/1\">Jakub Ch&#x142;&#x119;dowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamout_F/0/1/0/all/0/1\">Farah E. Shamout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1\">Krzysztof J. Geras</a>",
          "description": "Artificial intelligence (AI) is transforming medicine and showing promise in\nimproving clinical diagnosis. In breast cancer screening, several recent\nstudies show that AI has the potential to improve radiologists' accuracy,\nsubsequently helping in early cancer diagnosis and reducing unnecessary workup.\nAs the number of proposed models and their complexity grows, it is becoming\nincreasingly difficult to re-implement them in order to reproduce the results\nand to compare different approaches. To enable reproducibility of research in\nthis application area and to enable comparison between different methods, we\nrelease a meta-repository containing deep learning models for classification of\nscreening mammograms. This meta-repository creates a framework that enables the\nevaluation of machine learning models on any private or public screening\nmammography data set. At its inception, our meta-repository contains five\nstate-of-the-art models with open-source implementations and cross-platform\ncompatibility. We compare their performance on five international data sets:\ntwo private New York University breast cancer screening data sets as well as\nthree public (DDSM, INbreast and Chinese Mammography Database) data sets. Our\nframework has a flexible design that can be generalized to other medical image\nanalysis tasks. The meta-repository is available at\nhttps://www.github.com/nyukat/mammography_metarepository.",
          "link": "http://arxiv.org/abs/2108.04800",
          "publishedOn": "2021-08-11T01:55:21.639Z",
          "wordCount": 641,
          "title": "Meta-repository of screening mammography classifiers. (arXiv:2108.04800v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karadla_L/0/1/0/all/0/1\">Lahari Karadla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weizi Li</a>",
          "description": "The COVID-19 pandemic has resulted in significant social and economic impacts\nthroughout the world. In addition to the health consequences, the impacts on\ntraffic behaviors have also been sudden and dramatic. We have analyzed how the\nroad traffic safety of New York City, Los Angeles, and Boston in the U.S. have\nbeen impacted by the pandemic and corresponding local government orders and\nrestrictions. To be specific, we have studied the accident hotspots'\ndistributions before and after the outbreak of the pandemic and found that\ntraffic accidents have shifted in both location and time compared to previous\nyears. In addition, we have studied the road network characteristics in those\nhotspot regions with the hope to understand the underlying cause of the hotspot\nshifts.",
          "link": "http://arxiv.org/abs/2108.04787",
          "publishedOn": "2021-08-11T01:55:21.624Z",
          "wordCount": 621,
          "title": "Analyzing Effects of The COVID-19 Pandemic on Road Traffic Safety: The Cases of New York City, Los Angeles, and Boston. (arXiv:2108.04787v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.04170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khorshidi_H/0/1/0/all/0/1\">Hadi A. Khorshidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aickelin_U/0/1/0/all/0/1\">Uwe Aickelin</a>",
          "description": "Class imbalance is a substantial challenge in classifying many real-world\ncases. Synthetic over-sampling methods have been effective to improve the\nperformance of classifiers for imbalance problems. However, most synthetic\nover-sampling methods generate non-diverse synthetic instances within the\nconvex hull formed by the existing minority instances as they only concentrate\non the minority class and ignore the vast information provided by the majority\nclass. They also often do not perform well for extremely imbalanced data as the\nfewer the minority instances, the less information to generate synthetic\ninstances. Moreover, existing methods that generate synthetic instances using\nthe majority class distributional information cannot perform effectively when\nthe majority class has a multi-modal distribution. We propose a new method to\ngenerate diverse and adaptable synthetic instances using Synthetic\nOver-sampling with Minority and Majority classes (SOMM). SOMM generates\nsynthetic instances diversely within the minority data space. It updates the\ngenerated instances adaptively to the neighbourhood including both classes.\nThus, SOMM performs well for both binary and multiclass imbalance problems. We\nexamine the performance of SOMM for binary and multiclass problems using\nbenchmark data sets for different imbalance levels. The empirical results show\nthe superiority of SOMM compared to other existing methods.",
          "link": "http://arxiv.org/abs/2011.04170",
          "publishedOn": "2021-08-11T01:55:21.618Z",
          "wordCount": 672,
          "title": "A Synthetic Over-sampling method with Minority and Majority classes for imbalance problems. (arXiv:2011.04170v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mandi_J/0/1/0/all/0/1\">Jayanta Mandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canoy_R/0/1/0/all/0/1\">Rocsildes Canoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucarey_V/0/1/0/all/0/1\">V&#xed;ctor Bucarey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guns_T/0/1/0/all/0/1\">Tias Guns</a>",
          "description": "The traditional Capacitated Vehicle Routing Problem (CVRP) minimizes the\ntotal distance of the routes under the capacity constraints of the vehicles.\nBut more often, the objective involves multiple criteria including not only the\ntotal distance of the tour but also other factors such as travel costs, travel\ntime, and fuel consumption.Moreover, in reality, there are numerous implicit\npreferences ingrained in the minds of the route planners and the drivers.\nDrivers, for instance, have familiarity with certain neighborhoods and\nknowledge of the state of roads, and often consider the best places for rest\nand lunch breaks. This knowledge is difficult to formulate and balance when\noperational routing decisions have to be made. This motivates us to learn the\nimplicit preferences from past solutions and to incorporate these learned\npreferences in the optimization process. These preferences are in the form of\narc probabilities, i.e., the more preferred a route is, the higher is the joint\nprobability. The novelty of this work is the use of a neural network model to\nestimate the arc probabilities, which allows for additional features and\nautomatic parameter estimation. This first requires identifying suitable\nfeatures, neural architectures and loss functions, taking into account that\nthere is typically few data available. We investigate the difference with a\nprior weighted Markov counting approach, and study the applicability of neural\nnetworks in this setting.",
          "link": "http://arxiv.org/abs/2108.04578",
          "publishedOn": "2021-08-11T01:55:21.612Z",
          "wordCount": 666,
          "title": "Data Driven VRP: A Neural Network Model to Learn Hidden Preferences for VRP. (arXiv:2108.04578v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1910.14442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">William B. Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasimbeg_P/0/1/0/all/0/1\">Priya Kasimbeg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1\">Micael Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toshev_A/0/1/0/all/0/1\">Alexander Toshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>",
          "description": "We present Interactive Gibson Benchmark, the first comprehensive benchmark\nfor training and evaluating Interactive Navigation: robot navigation strategies\nwhere physical interaction with objects is allowed and even encouraged to\naccomplish a task. For example, the robot can move objects if needed in order\nto clear a path leading to the goal location. Our benchmark comprises two novel\nelements: 1) a new experimental setup, the Interactive Gibson Environment\n(iGibson 0.5), which simulates high fidelity visuals of indoor scenes, and high\nfidelity physical dynamics of the robot and common objects found in these\nscenes; 2) a set of Interactive Navigation metrics which allows one to study\nthe interplay between navigation and physical interaction. We present and\nevaluate multiple learning-based baselines in Interactive Gibson, and provide\ninsights into regimes of navigation with different trade-offs between\nnavigation path efficiency and disturbance of surrounding objects. We make our\nbenchmark publicly\navailable(https://sites.google.com/view/interactivegibsonenv) and encourage\nresearchers from all disciplines in robotics (e.g. planning, learning, control)\nto propose, evaluate, and compare their Interactive Navigation solutions in\nInteractive Gibson.",
          "link": "http://arxiv.org/abs/1910.14442",
          "publishedOn": "2021-08-11T01:55:21.597Z",
          "wordCount": 707,
          "title": "Interactive Gibson Benchmark (iGibson 0.5): A Benchmark for Interactive Navigation in Cluttered Environments. (arXiv:1910.14442v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.14408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muralidharan_D/0/1/0/all/0/1\">Deepak Muralidharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1\">Joel Ruben Antony Moniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Sida Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_J/0/1/0/all/0/1\">Justine Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulman_S/0/1/0/all/0/1\">Stephen Pulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1\">Atish Kothari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1\">Ray Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yinying Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaul_V/0/1/0/all/0/1\">Vivek Kaul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1\">Mubarak Seyed Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_G/0/1/0/all/0/1\">Gang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dun_N/0/1/0/all/0/1\">Nan Dun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yidan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+O_A/0/1/0/all/0/1\">Andy O</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitkara_P/0/1/0/all/0/1\">Pooja Chitkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Alkesh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tayal_K/0/1/0/all/0/1\">Kushal Tayal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Roger Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grasch_P/0/1/0/all/0/1\">Peter Grasch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1\">Jason D. Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>",
          "description": "Named Entity Recognition (NER) and Entity Linking (EL) play an essential role\nin voice assistant interaction, but are challenging due to the special\ndifficulties associated with spoken user queries. In this paper, we propose a\nnovel architecture that jointly solves the NER and EL tasks by combining them\nin a joint reranking module. We show that our proposed framework improves NER\naccuracy by up to 3.13% and EL accuracy by up to 3.6% in F1 score. The features\nused also lead to better accuracies in other natural language understanding\ntasks, such as domain classification and semantic parsing.",
          "link": "http://arxiv.org/abs/2005.14408",
          "publishedOn": "2021-08-11T01:55:21.591Z",
          "wordCount": 638,
          "title": "Noise Robust Named Entity Understanding for Voice Assistants. (arXiv:2005.14408v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fenyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>",
          "description": "Graph Neural Networks (GNNs) have achieved great success among various\ndomains. Nevertheless, most GNN methods are sensitive to the quality of graph\nstructures. To tackle this problem, some studies exploit different graph\nstructure learning strategies to refine the original graph structure. However,\nthese methods only consider feature information while ignoring available label\ninformation. In this paper, we propose a novel label-informed graph structure\nlearning framework which incorporates label information explicitly through a\nclass transition matrix. We conduct extensive experiments on seven node\nclassification benchmark datasets and the results show that our method\noutperforms or matches the state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2108.04595",
          "publishedOn": "2021-08-11T01:55:21.585Z",
          "wordCount": 533,
          "title": "Label-informed Graph Structure Learning for Node Classification. (arXiv:2108.04595v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1908.04741",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Nuske_F/0/1/0/all/0/1\">Feliks N&#xfc;ske</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gelss_P/0/1/0/all/0/1\">Patrick Gel&#xdf;</a>, <a href=\"http://arxiv.org/find/math/1/au:+Klus_S/0/1/0/all/0/1\">Stefan Klus</a>, <a href=\"http://arxiv.org/find/math/1/au:+Clementi_C/0/1/0/all/0/1\">Cecilia Clementi</a>",
          "description": "Recent years have seen rapid advances in the data-driven analysis of\ndynamical systems based on Koopman operator theory and related approaches. On\nthe other hand, low-rank tensor product approximations -- in particular the\ntensor train (TT) format -- have become a valuable tool for the solution of\nlarge-scale problems in a number of fields. In this work, we combine\nKoopman-based models and the TT format, enabling their application to\nhigh-dimensional problems in conjunction with a rich set of basis functions or\nfeatures. We derive efficient algorithms to obtain a reduced matrix\nrepresentation of the system's evolution operator starting from an appropriate\nlow-rank representation of the data. These algorithms can be applied to both\nstationary and non-stationary systems. We establish the infinite-data limit of\nthese matrix representations, and demonstrate our methods' capabilities using\nseveral benchmark data sets.",
          "link": "http://arxiv.org/abs/1908.04741",
          "publishedOn": "2021-08-11T01:55:21.580Z",
          "wordCount": 609,
          "title": "Tensor-based computation of metastable and coherent sets. (arXiv:1908.04741v3 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.05259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bontrager_P/0/1/0/all/0/1\">Philip Bontrager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1\">Julian Togelius</a>",
          "description": "Machine learning for procedural content generation has recently become an\nactive area of research. Levels vary in both form and function and are mostly\nunrelated to each other across games. This has made it difficult to assemble\nsuitably large datasets to bring machine learning to level design in the same\nway as it's been used for image generation. Here we propose Generative Playing\nNetworks which design levels for itself to play. The algorithm is built in two\nparts; an agent that learns to play game levels, and a generator that learns\nthe distribution of playable levels. As the agent learns and improves its\nability, the space of playable levels, as defined by the agent, grows. The\ngenerator targets the agent's playability estimates to then update its\nunderstanding of what constitutes a playable level. We call this process of\nlearning the distribution of data found through self-discovery with an\nenvironment, self-supervised inductive learning. Unlike previous approaches to\nprocedural content generation, Generative Playing Networks are end-to-end\ndifferentiable and do not require human-designed examples or domain knowledge.\nWe demonstrate the capability of this framework by training an agent and level\ngenerator for a 2D dungeon crawler game.",
          "link": "http://arxiv.org/abs/2002.05259",
          "publishedOn": "2021-08-11T01:55:21.565Z",
          "wordCount": 662,
          "title": "Learning to Generate Levels From Nothing. (arXiv:2002.05259v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Panagopoulos_G/0/1/0/all/0/1\">George Panagopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tziortziotis_N/0/1/0/all/0/1\">Nikolaos Tziortziotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malliaros_F/0/1/0/all/0/1\">Fragkiskos D. Malliaros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>",
          "description": "As the field of machine learning for combinatorial optimization advances,\ntraditional problems are resurfaced and readdressed through this new\nperspective. The overwhelming majority of the literature focuses on small graph\nproblems, while several real-world problems are devoted to large graphs. Here,\nwe focus on two such problems that are related: influence estimation, a\n\\#P-hard counting problem, and influence maximization, an NP-hard problem. We\ndevelop GLIE, a Graph Neural Network (GNN) that inherently parameterizes an\nupper bound of influence estimation and train it on small simulated graphs.\nExperiments show that GLIE can provide accurate predictions faster than the\nalternatives for graphs 10 times larger than the train set. More importantly,\nit can be used on arbitrary large graphs for influence maximization, as the\npredictions can rank effectively seed sets even when the accuracy deteriorates.\nTo showcase this, we propose a version of a standard Influence Maximization\n(IM) algorithm where we substitute traditional influence estimation with the\npredictions of GLIE.We also transfer GLIE into a reinforcement learning model\nthat learns how to choose seeds to maximize influence sequentially using GLIE's\nhidden representations and predictions. The final results show that the\nproposed methods surpasses a previous GNN-RL approach and perform on par with a\nstate-of-the-art IM algorithm.",
          "link": "http://arxiv.org/abs/2108.04623",
          "publishedOn": "2021-08-11T01:55:21.556Z",
          "wordCount": 638,
          "title": "Learning to Maximize Influence. (arXiv:2108.04623v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04698",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gu_S/0/1/0/all/0/1\">Shuting Gu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1\">Hongqiao Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_X/0/1/0/all/0/1\">Xiang Zhou</a>",
          "description": "The transition state (TS) calculation is a grand challenge for computational\nintensive energy function. The traditional methods need to evaluate the\ngradients of the energy function at a very large number of locations. To reduce\nthe number of expensive computations of the true gradients, we propose an\nactive learning framework consisting of a statistical surrogate model, Gaussian\nprocess regression (GPR) for the energy function, and a single-walker dynamics\nmethod, gentle accent dynamics (GAD), for the saddle-type transition states. TS\nis detected by the GAD applied to the GPR surrogate for the gradient vector and\nthe Hessian matrix. Our key ingredient for efficiency improvements is an active\nlearning method which sequentially designs the most informative locations and\ntakes evaluations of the original model at these locations to train GPR. We\nformulate this active learning task as the optimal experimental design problem\nand propose a very efficient sample-based sub-optimal criterion to construct\nthe optimal locations. We show that the new method significantly decreases the\nrequired number of energy or force evaluations of the original model.",
          "link": "http://arxiv.org/abs/2108.04698",
          "publishedOn": "2021-08-11T01:55:21.550Z",
          "wordCount": 604,
          "title": "Active Learning for Transition State Calculation. (arXiv:2108.04698v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04809",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Shin_D/0/1/0/all/0/1\">Dongil Shin</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Cupertino_A/0/1/0/all/0/1\">Andrea Cupertino</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Jong_M/0/1/0/all/0/1\">Matthijs H. J. de Jong</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Steeneken_P/0/1/0/all/0/1\">Peter G. Steeneken</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Bessa_M/0/1/0/all/0/1\">Miguel A. Bessa</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Norte_R/0/1/0/all/0/1\">Richard A. Norte</a>",
          "description": "From ultra-sensitive detectors of fundamental forces to quantum networks and\nsensors, mechanical resonators are enabling next-generation technologies to\noperate in room temperature environments. Currently, silicon nitride\nnanoresonators stand as a leading microchip platform in these advances by\nallowing for mechanical resonators whose motion is remarkably isolated from\nambient thermal noise. However, to date, human intuition has remained the\ndriving force behind design processes. Here, inspired by nature and guided by\nmachine learning, a spiderweb nanomechanical resonator is developed that\nexhibits vibration modes which are isolated from ambient thermal environments\nvia a novel \"torsional soft-clamping\" mechanism discovered by the data-driven\noptimization algorithm. This bio-inspired resonator is then fabricated;\nexperimentally confirming a new paradigm in mechanics with quality factors\nabove 1 billion in room temperature environments. In contrast to other\nstate-of-the-art resonators, this milestone is achieved with a compact design\nwhich does not require sub-micron lithographic features or complex phononic\nbandgaps, making it significantly easier and cheaper to manufacture at large\nscales. Here we demonstrate the ability of machine learning to work in tandem\nwith human intuition to augment creative possibilities and uncover new\nstrategies in computing and nanotechnology.",
          "link": "http://arxiv.org/abs/2108.04809",
          "publishedOn": "2021-08-11T01:55:21.539Z",
          "wordCount": 652,
          "title": "Spiderweb nanomechanical resonators via Bayesian optimization: inspired by nature and guided by machine learning. (arXiv:2108.04809v1 [cond-mat.mes-hall])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bodigutla_P/0/1/0/all/0/1\">Praveen Kumar Bodigutla</a>",
          "description": "\"High Quality Related Search Query Suggestions\" task aims at recommending\nsearch queries which are real, accurate, diverse, relevant and engaging.\nObtaining large amounts of query-quality human annotations is expensive. Prior\nwork on supervised query suggestion models suffered from selection and exposure\nbias, and relied on sparse and noisy immediate user-feedback (e.g., clicks),\nleading to low quality suggestions. Reinforcement Learning techniques employed\nto reformulate a query using terms from search results, have limited\nscalability to large-scale industry applications. To recommend high quality\nrelated search queries, we train a Deep Reinforcement Learning model to predict\nthe query a user would enter next. The reward signal is composed of long-term\nsession-based user feedback, syntactic relatedness and estimated naturalness of\ngenerated query. Over the baseline supervised model, our proposed approach\nachieves a significant relative improvement in terms of recommendation\ndiversity (3%), down-stream user-engagement (4.2%) and per-sentence word\nrepetitions (82%).",
          "link": "http://arxiv.org/abs/2108.04452",
          "publishedOn": "2021-08-11T01:55:21.533Z",
          "wordCount": 598,
          "title": "High Quality Related Search Query Suggestions using Deep Reinforcement Learning. (arXiv:2108.04452v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Behr_D/0/1/0/all/0/1\">David Behr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maina_C/0/1/0/all/0/1\">Ciira wa Maina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>",
          "description": "This paper is an investigation into aspects of an audio classification\npipeline that will be appropriate for the monitoring of bird species on edges\ndevices. These aspects include transfer learning, data augmentation and model\noptimization. The hope is that the resulting models will be good candidates to\ndeploy on edge devices to monitor bird populations. Two classification\napproaches will be taken into consideration, one which explores the\neffectiveness of a traditional Deep Neural Network(DNN) and another that makes\nuse of Convolutional layers.This study aims to contribute empirical evidence of\nthe merits and demerits of each approach.",
          "link": "http://arxiv.org/abs/2108.04449",
          "publishedOn": "2021-08-11T01:55:21.517Z",
          "wordCount": 548,
          "title": "An empirical investigation into audio pipeline approaches for classifying bird species. (arXiv:2108.04449v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hongwu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shanglin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weitze_S/0/1/0/all/0/1\">Scott Weitze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Sahidul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_T/0/1/0/all/0/1\">Tong Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Minghu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Mimi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Caiwen Ding</a>",
          "description": "Being able to learn from complex data with phase information is imperative\nfor many signal processing applications. Today' s real-valued deep neural\nnetworks (DNNs) have shown efficiency in latent information analysis but fall\nshort when applied to the complex domain. Deep complex networks (DCN), in\ncontrast, can learn from complex data, but have high computational costs;\ntherefore, they cannot satisfy the instant decision-making requirements of many\ndeployable systems dealing with short observations or short signal bursts.\nRecent, Binarized Complex Neural Network (BCNN), which integrates DCNs with\nbinarized neural networks (BNN), shows great potential in classifying complex\ndata in real-time. In this paper, we propose a structural pruning based\naccelerator of BCNN, which is able to provide more than 5000 frames/s inference\nthroughput on edge devices. The high performance comes from both the algorithm\nand hardware sides. On the algorithm side, we conduct structural pruning to the\noriginal BCNN models and obtain 20 $\\times$ pruning rates with negligible\naccuracy loss; on the hardware side, we propose a novel 2D convolution\noperation accelerator for the binary complex neural network. Experimental\nresults show that the proposed design works with over 90% utilization and is\nable to achieve the inference throughput of 5882 frames/s and 4938 frames/s for\ncomplex NIN-Net and ResNet-18 using CIFAR-10 dataset and Alveo U280 Board.",
          "link": "http://arxiv.org/abs/2108.04811",
          "publishedOn": "2021-08-11T01:55:21.512Z",
          "wordCount": 669,
          "title": "Binary Complex Neural Network Acceleration on FPGA. (arXiv:2108.04811v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04620",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Jentzen_A/0/1/0/all/0/1\">Arnulf Jentzen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Riekert_A/0/1/0/all/0/1\">Adrian Riekert</a>",
          "description": "Gradient descent (GD) type optimization methods are the standard instrument\nto train artificial neural networks (ANNs) with rectified linear unit (ReLU)\nactivation. Despite the great success of GD type optimization methods in\nnumerical simulations for the training of ANNs with ReLU activation, it remains\n- even in the simplest situation of the plain vanilla GD optimization method\nwith random initializations and ANNs with one hidden layer - an open problem to\nprove (or disprove) the conjecture that the risk of the GD optimization method\nconverges in the training of such ANNs to zero as the width of the ANNs, the\nnumber of independent random initializations, and the number of GD steps\nincrease to infinity. In this article we prove this conjecture in the situation\nwhere the probability distribution of the input data is equivalent to the\ncontinuous uniform distribution on a compact interval, where the probability\ndistributions for the random initializations of the ANN parameters are standard\nnormal distributions, and where the target function under consideration is\ncontinuous and piecewise affine linear. Roughly speaking, the key ingredients\nin our mathematical convergence analysis are (i) to prove that suitable sets of\nglobal minima of the risk functions are \\emph{twice continuously differentiable\nsubmanifolds of the ANN parameter spaces}, (ii) to prove that the Hessians of\nthe risk functions on these sets of global minima satisfy an appropriate\n\\emph{maximal rank condition}, and, thereafter, (iii) to apply the machinery in\n[Fehrman, B., Gess, B., Jentzen, A., Convergence rates for the stochastic\ngradient descent method for non-convex objective functions. J. Mach. Learn.\nRes. 21(136): 1--48, 2020] to establish convergence of the GD optimization\nmethod with random initializations.",
          "link": "http://arxiv.org/abs/2108.04620",
          "publishedOn": "2021-08-11T01:55:21.484Z",
          "wordCount": 750,
          "title": "A proof of convergence for the gradient descent optimization method with random initializations in the training of neural networks with ReLU activation for piecewise linear target functions. (arXiv:2108.04620v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bshouty_N/0/1/0/all/0/1\">Nader H. Bshouty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddad_Zaknoon_C/0/1/0/all/0/1\">Catherine A. Haddad-Zaknoon</a>",
          "description": "In this paper, we study learning and testing decision tree of size and depth\nthat are significantly smaller than the number of attributes $n$.\n\nOur main result addresses the problem of poly$(n,1/\\epsilon)$ time algorithms\nwith poly$(s,1/\\epsilon)$ query complexity (independent of $n$) that\ndistinguish between functions that are decision trees of size $s$ from\nfunctions that are $\\epsilon$-far from any decision tree of size\n$\\phi(s,1/\\epsilon)$, for some function $\\phi > s$. The best known result is\nthe recent one that follows from Blank, Lange and Tan,~\\cite{BlancLT20}, that\ngives $\\phi(s,1/\\epsilon)=2^{O((\\log^3s)/\\epsilon^3)}$. In this paper, we give\na new algorithm that achieves $\\phi(s,1/\\epsilon)=2^{O(\\log^2 (s/\\epsilon))}$.\n\nMoreover, we study the testability of depth-$d$ decision tree and give a {\\it\ndistribution free} tester that distinguishes between depth-$d$ decision tree\nand functions that are $\\epsilon$-far from depth-$d^2$ decision tree. In\nparticular, for decision trees of size $s$, the above result holds in the\ndistribution-free model when the tree depth is $O(\\log(s/\\epsilon))$.\n\nWe also give other new results in learning and testing of size-$s$ decision\ntrees and depth-$d$ decision trees that follow from results in the literature\nand some results we prove in this paper.",
          "link": "http://arxiv.org/abs/2108.04587",
          "publishedOn": "2021-08-11T01:55:21.479Z",
          "wordCount": 618,
          "title": "On Learning and Testing Decision Tree. (arXiv:2108.04587v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gasperini_S/0/1/0/all/0/1\">Stefano Gasperini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1\">Patrick Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dallabetta_V/0/1/0/all/0/1\">Vinzenz Dallabetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "While self-supervised monocular depth estimation in driving scenarios has\nachieved comparable performance to supervised approaches, violations of the\nstatic world assumption can still lead to erroneous depth predictions of\ntraffic participants, posing a potential safety issue. In this paper, we\npresent R4Dyn, a novel set of techniques to use cost-efficient radar data on\ntop of a self-supervised depth estimation framework. In particular, we show how\nradar can be used during training as weak supervision signal, as well as an\nextra input to enhance the estimation robustness at inference time. Since\nautomotive radars are readily available, this allows to collect training data\nfrom a variety of existing vehicles. Moreover, by filtering and expanding the\nsignal to make it compatible with learning-based approaches, we address radar\ninherent issues, such as noise and sparsity. With R4Dyn we are able to overcome\na major limitation of self-supervised depth estimation, i.e. the prediction of\ntraffic participants. We substantially improve the estimation on dynamic\nobjects, such as cars by 37% on the challenging nuScenes dataset, hence\ndemonstrating that radar is a valuable additional sensor for monocular depth\nestimation in autonomous vehicles. Additionally, we plan on making the code\npublicly available.",
          "link": "http://arxiv.org/abs/2108.04814",
          "publishedOn": "2021-08-11T01:55:21.452Z",
          "wordCount": 651,
          "title": "R4Dyn: Exploring Radar for Self-Supervised Monocular Depth Estimation of Dynamic Scenes. (arXiv:2108.04814v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04729",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chierichetti_F/0/1/0/all/0/1\">Flavio Chierichetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panconesi_A/0/1/0/all/0/1\">Alessandro Panconesi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_G/0/1/0/all/0/1\">Giuseppe Re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trevisan_L/0/1/0/all/0/1\">Luca Trevisan</a>",
          "description": "Correlation Clustering is an important clustering problem with many\napplications. We study the reconstruction version of this problem in which one\nis seeking to reconstruct a latent clustering that has been corrupted by random\nnoise and adversarial modifications.\n\nConcerning the latter, we study a standard \"post-adversarial\" model, in which\nadversarial modifications come after the noise, and also introduce and analyze\na \"pre-adversarial\" model in which adversarial modifications come before the\nnoise. Given an input coming from such a semi-adversarial generative model, the\ngoal is to reconstruct almost perfectly and with high probability the latent\nclustering.\n\nWe focus on the case where the hidden clusters have equal size and show the\nfollowing. In the pre-adversarial setting, spectral algorithms are optimal, in\nthe sense that they reconstruct all the way to the information-theoretic\nthreshold beyond which no reconstruction is possible. In contrast, in the\npost-adversarial setting their ability to restore the hidden clusters stops\nbefore the threshold, but the gap is optimally filled by SDP-based algorithms.",
          "link": "http://arxiv.org/abs/2108.04729",
          "publishedOn": "2021-08-11T01:55:21.447Z",
          "wordCount": 598,
          "title": "Correlation Clustering Reconstruction in Semi-Adversarial Models. (arXiv:2108.04729v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04584",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gurulingan_N/0/1/0/all/0/1\">NareshKumar Gurulingan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>",
          "description": "Scene understanding is crucial for autonomous systems which intend to operate\nin the real world. Single task vision networks extract information only based\non some aspects of the scene. In multi-task learning (MTL), on the other hand,\nthese single tasks are jointly learned, thereby providing an opportunity for\ntasks to share information and obtain a more comprehensive understanding. To\nthis end, we develop UniNet, a unified scene understanding network that\naccurately and efficiently infers vital vision tasks including object\ndetection, semantic segmentation, instance segmentation, monocular depth\nestimation, and monocular instance depth prediction. As these tasks look at\ndifferent semantic and geometric information, they can either complement or\nconflict with each other. Therefore, understanding inter-task relationships can\nprovide useful cues to enable complementary information sharing. We evaluate\nthe task relationships in UniNet through the lens of adversarial attacks based\non the notion that they can exploit learned biases and task interactions in the\nneural network. Extensive experiments on the Cityscapes dataset, using\nuntargeted and targeted attacks reveal that semantic tasks strongly interact\namongst themselves, and the same holds for geometric tasks. Additionally, we\nshow that the relationship between semantic and geometric tasks is asymmetric\nand their interaction becomes weaker as we move towards higher-level\nrepresentations.",
          "link": "http://arxiv.org/abs/2108.04584",
          "publishedOn": "2021-08-11T01:55:21.440Z",
          "wordCount": 668,
          "title": "UniNet: A Unified Scene Understanding Network and Exploring Multi-Task Relationships through the Lens of Adversarial Attacks. (arXiv:2108.04584v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04782",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1\">Yangyi Lu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_Z/0/1/0/all/0/1\">Ziping Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tewari_A/0/1/0/all/0/1\">Ambuj Tewari</a>",
          "description": "The Oxford English Dictionary defines precision medicine as \"medical care\ndesigned to optimize efficiency or therapeutic benefit for particular groups of\npatients, especially by using genetic or molecular profiling.\" It is not an\nentirely new idea: physicians from ancient times have recognized that medical\ntreatment needs to consider individual variations in patient characteristics.\nHowever, the modern precision medicine movement has been enabled by a\nconfluence of events: scientific advances in fields such as genetics and\npharmacology, technological advances in mobile devices and wearable sensors,\nand methodological advances in computing and data sciences.\n\nThis chapter is about bandit algorithms: an area of data science of special\nrelevance to precision medicine. With their roots in the seminal work of\nBellman, Robbins, Lai and others, bandit algorithms have come to occupy a\ncentral place in modern data science ( Lattimore and Szepesvari, 2020). Bandit\nalgorithms can be used in any situation where treatment decisions need to be\nmade to optimize some health outcome. Since precision medicine focuses on the\nuse of patient characteristics to guide treatment, contextual bandit algorithms\nare especially useful since they are designed to take such information into\naccount. The role of bandit algorithms in areas of precision medicine such as\nmobile health and digital phenotyping has been reviewed before (Tewari and\nMurphy, 2017; Rabbi et al., 2019). Since these reviews were published, bandit\nalgorithms have continued to find uses in mobile health and several new topics\nhave emerged in the research on bandit algorithms. This chapter is written for\nquantitative researchers in fields such as statistics, machine learning, and\noperations research who might be interested in knowing more about the\nalgorithmic and mathematical details of bandit algorithms that have been used\nin mobile health.",
          "link": "http://arxiv.org/abs/2108.04782",
          "publishedOn": "2021-08-11T01:55:21.435Z",
          "wordCount": 741,
          "title": "Bandit Algorithms for Precision Medicine. (arXiv:2108.04782v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Almeida_G/0/1/0/all/0/1\">Gon&#xe7;alo Sim&#xf5;es de Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abreu_G/0/1/0/all/0/1\">Gon&#xe7;alo Faria Abreu</a>",
          "description": "In this study we propose a new concept of databases (crowdsourced databases),\nadding a new conceptual approach to the debate on legal protection of databases\nin Europe. We also summarise the current legal framework and current indexing\nand web scraping practices - it would not be prudent to suggest a new theory\nwithout contextualising it in the legal and practical context in which it is\ndeveloped.",
          "link": "http://arxiv.org/abs/2108.04727",
          "publishedOn": "2021-08-11T01:55:21.429Z",
          "wordCount": 494,
          "title": "Crowdsourced Databases and Sui Generis Rights. (arXiv:2108.04727v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04725",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scheliga_D/0/1/0/all/0/1\">Daniel Scheliga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mader_P/0/1/0/all/0/1\">Patrick M&#xe4;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeland_M/0/1/0/all/0/1\">Marco Seeland</a>",
          "description": "Collaborative training of neural networks leverages distributed data by\nexchanging gradient information between different clients. Although training\ndata entirely resides with the clients, recent work shows that training data\ncan be reconstructed from such exchanged gradient information. To enhance\nprivacy, gradient perturbation techniques have been proposed. However, they\ncome at the cost of reduced model performance, increased convergence time, or\nincreased data demand. In this paper, we introduce PRECODE, a PRivacy EnhanCing\nmODulE that can be used as generic extension for arbitrary model architectures.\nWe propose a simple yet effective realization of PRECODE using variational\nmodeling. The stochastic sampling induced by variational modeling effectively\nprevents privacy leakage from gradients and in turn preserves privacy of data\nowners. We evaluate PRECODE using state of the art gradient inversion attacks\non two different model architectures trained on three datasets. In contrast to\ncommonly used defense mechanisms, we find that our proposed modification\nconsistently reduces the attack success rate to 0% while having almost no\nnegative impact on model training and final performance. As a result, PRECODE\nreveals a promising path towards privacy enhancing model extensions.",
          "link": "http://arxiv.org/abs/2108.04725",
          "publishedOn": "2021-08-11T01:55:21.423Z",
          "wordCount": 631,
          "title": "PRECODE - A Generic Model Extension to Prevent Deep Gradient Leakage. (arXiv:2108.04725v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.06731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tennenholtz_G/0/1/0/all/0/1\">Guy Tennenholtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalit_U/0/1/0/all/0/1\">Uri Shalit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1\">Shie Mannor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efroni_Y/0/1/0/all/0/1\">Yonathan Efroni</a>",
          "description": "We study linear contextual bandits with access to a large, confounded,\noffline dataset that was sampled from some fixed policy. We show that this\nproblem is closely related to a variant of the bandit problem with side\ninformation. We construct a linear bandit algorithm that takes advantage of the\nprojected information, and prove regret bounds. Our results demonstrate the\nability to take advantage of confounded offline data. Particularly, we prove\nregret bounds that improve current bounds by a factor related to the visible\ndimensionality of the contexts in the data. Our results indicate that\nconfounded offline data can significantly improve online learning algorithms.\nFinally, we demonstrate various characteristics of our approach through\nsynthetic simulations.",
          "link": "http://arxiv.org/abs/2006.06731",
          "publishedOn": "2021-08-11T01:55:21.407Z",
          "wordCount": 587,
          "title": "Bandits with Partially Observable Confounded Data. (arXiv:2006.06731v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04552",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Difan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jingfeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1\">Vladimir Braverman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1\">Dean P. Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1\">Sham M. Kakade</a>",
          "description": "Stochastic gradient descent (SGD) exhibits strong algorithmic regularization\neffects in practice, which has been hypothesized to play an important role in\nthe generalization of modern machine learning approaches. In this work, we seek\nto understand these issues in the simpler setting of linear regression\n(including both underparameterized and overparameterized regimes), where our\ngoal is to make sharp instance-based comparisons of the implicit regularization\nafforded by (unregularized) average SGD with the explicit regularization of\nridge regression. For a broad class of least squares problem instances (that\nare natural in high-dimensional settings), we show: (1) for every problem\ninstance and for every ridge parameter, (unregularized) SGD, when provided with\nlogarithmically more samples than that provided to the ridge algorithm,\ngeneralizes no worse than the ridge solution (provided SGD uses a tuned\nconstant stepsize); (2) conversely, there exist instances (in this wide problem\nclass) where optimally-tuned ridge regression requires quadratically more\nsamples than SGD in order to have the same generalization performance. Taken\ntogether, our results show that, up to the logarithmic factors, the\ngeneralization performance of SGD is always no worse than that of ridge\nregression in a wide range of overparameterized problems, and, in fact, could\nbe much better for some problem instances. More generally, our results show how\nalgorithmic regularization has important consequences even in simpler\n(overparameterized) convex settings.",
          "link": "http://arxiv.org/abs/2108.04552",
          "publishedOn": "2021-08-11T01:55:21.400Z",
          "wordCount": 677,
          "title": "The Benefits of Implicit Regularization from SGD in Least Squares Problems. (arXiv:2108.04552v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04763",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ciosek_K/0/1/0/all/0/1\">Kamil Ciosek</a>",
          "description": "Imitation Learning algorithms learn a policy from demonstrations of expert\nbehavior. Somewhat counterintuitively, we show that, for deterministic experts,\nimitation learning can be done by reduction to reinforcement learning, which is\ncommonly considered more difficult. We conduct experiments which confirm that\nour reduction works well in practice for a continuous control task.",
          "link": "http://arxiv.org/abs/2108.04763",
          "publishedOn": "2021-08-11T01:55:21.394Z",
          "wordCount": 474,
          "title": "Imitation Learning by Reinforcement Learning. (arXiv:2108.04763v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04551",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1\">Hyejun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Joonyong Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1\">Tai Myung Chung</a>",
          "description": "Federated Learning is a distributed machine learning framework designed for\ndata privacy preservation i.e., local data remain private throughout the entire\ntraining and testing procedure. Federated Learning is gaining popularity\nbecause it allows one to use machine learning techniques while preserving\nprivacy. However, it inherits the vulnerabilities and susceptibilities raised\nin deep learning techniques. For instance, Federated Learning is particularly\nvulnerable to data poisoning attacks that may deteriorate its performance and\nintegrity due to its distributed nature and inaccessibility to the raw data. In\naddition, it is extremely difficult to correctly identify malicious clients due\nto the non-Independently and/or Identically Distributed (non-IID) data. The\nreal-world data can be complex and diverse, making them hardly distinguishable\nfrom the malicious data without direct access to the raw data. Prior research\nhas focused on detecting malicious clients while treating only the clients\nhaving IID data as benign. In this study, we propose a method that detects and\nclassifies anomalous clients from benign clients when benign ones have non-IID\ndata. Our proposed method leverages feature dimension reduction, dynamic\nclustering, and cosine similarity-based clipping. The experimental results\nvalidates that our proposed method not only classifies the malicious clients\nbut also alleviates their negative influences from the entire procedure. Our\nfindings may be used in future studies to effectively eliminate anomalous\nclients when building a model with diverse data.",
          "link": "http://arxiv.org/abs/2108.04551",
          "publishedOn": "2021-08-11T01:55:21.389Z",
          "wordCount": 669,
          "title": "ABC-FL: Anomalous and Benign client Classification in Federated Learning. (arXiv:2108.04551v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaorui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kun Yuan</a>",
          "description": "Decentralized optimization and communication compression have exhibited their\ngreat potential in accelerating distributed machine learning by mitigating the\ncommunication bottleneck in practice. While existing decentralized algorithms\nwith communication compression mostly focus on the problems with only smooth\ncomponents, we study the decentralized stochastic composite optimization\nproblem with a potentially non-smooth component. A \\underline{Prox}imal\ngradient \\underline{L}in\\underline{EA}r convergent \\underline{D}ecentralized\nalgorithm with compression, Prox-LEAD, is proposed with rigorous theoretical\nanalyses in the general stochastic setting and the finite-sum setting. Our\ntheorems indicate that Prox-LEAD works with arbitrary compression precision,\nand it tremendously reduces the communication cost almost for free. The\nsuperiorities of the proposed algorithms are demonstrated through the\ncomparison with state-of-the-art algorithms in terms of convergence\ncomplexities and numerical experiments. Our algorithmic framework also\ngenerally enlightens the compressed communication on other primal-dual\nalgorithms by reducing the impact of inexact iterations, which might be of\nindependent interest.",
          "link": "http://arxiv.org/abs/2108.04448",
          "publishedOn": "2021-08-11T01:55:21.383Z",
          "wordCount": 583,
          "title": "Decentralized Composite Optimization with Compression. (arXiv:2108.04448v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostler_H/0/1/0/all/0/1\">Harald K&#xf6;stler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heisig_M/0/1/0/all/0/1\">Marco Heisig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauss_P/0/1/0/all/0/1\">Patrick Krauss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seung Hee Yang</a>",
          "description": "In this article, we perform a review of the state-of-the-art of hybrid\nmachine learning in medical imaging. We start with a short summary of the\ngeneral developments of the past in machine learning and how general and\nspecialized approaches have been in competition in the past decades. A\nparticular focus will be the theoretical and experimental evidence pro and\ncontra hybrid modelling. Next, we inspect several new developments regarding\nhybrid machine learning with a particular focus on so-called known operator\nlearning and how hybrid approaches gain more and more momentum across\nessentially all applications in medical imaging and medical image analysis. As\nwe will point out by numerous examples, hybrid models are taking over in image\nreconstruction and analysis. Even domains such as physical simulation and\nscanner and acquisition design are being addressed using machine learning grey\nbox modelling approaches. Towards the end of the article, we will investigate a\nfew future directions and point out relevant areas in which hybrid modelling,\nmeta learning, and other domains will likely be able to drive the\nstate-of-the-art ahead.",
          "link": "http://arxiv.org/abs/2108.04543",
          "publishedOn": "2021-08-11T01:55:21.367Z",
          "wordCount": 657,
          "title": "Known Operator Learning and Hybrid Machine Learning in Medical Imaging --- A Review of the Past, the Present, and the Future. (arXiv:2108.04543v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yousfi_S/0/1/0/all/0/1\">Siham Yousfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhanoui_M/0/1/0/all/0/1\">Maryem Rhanoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiadmi_D/0/1/0/all/0/1\">Dalila Chiadmi</a>",
          "description": "Big Data are rapidly produced from various heterogeneous data sources. They\nare of different types (text, image, video or audio) and have different levels\nof reliability and completeness. One of the most interesting architectures that\ndeal with the large amount of emerging data at high velocity is called the\nlambda architecture. In fact, it combines two different processing layers\nnamely batch and speed layers, each providing specific views of data while\nensuring robustness, fast and scalable data processing. However, most papers\ndealing with the lambda architecture are focusing one single type of data\ngenerally produced by a single data source. Besides, the layers of the\narchitecture are implemented independently, or, at best, are combined to\nperform basic processing without assessing either the data reliability or\ncompleteness. Therefore, inspired by the lambda architecture, we propose in\nthis paper a generic multimodal architecture that combines both batch and\nstreaming processing in order to build a complete, global and accurate insight\nin near-real-time based on the knowledge extracted from multiple heterogeneous\nBig Data sources. Our architecture uses batch processing to analyze the data\nstructures and contents, build the learning models and calculate the\nreliability index of the involved sources, while the streaming processing uses\nthe built-in models of the batch layer to immediately process incoming data and\nrapidly provide results. We validate our architecture in the context of urban\ntraffic management systems in order to detect congestions.",
          "link": "http://arxiv.org/abs/2108.04343",
          "publishedOn": "2021-08-11T01:55:21.352Z",
          "wordCount": 686,
          "title": "Towards a Generic Multimodal Architecture for Batch and Streaming Big Data Integration. (arXiv:2108.04343v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04462",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1\">Zefang Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1\">Tao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Depeng/0/1/0/all/0/1\">Depeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>",
          "description": "Recent technology development brings the booming of numerous new\nDemand-Driven Services (DDS) into urban lives, including ridesharing, on-demand\ndelivery, express systems and warehousing. In DDS, a service loop is an\nelemental structure, including its service worker, the service providers and\ncorresponding service targets. The service workers should transport either\nhumans or parcels from the providers to the target locations. Various planning\ntasks within DDS can thus be classified into two individual stages: 1)\nDispatching, which is to form service loops from demand/supply distributions,\nand 2)Routing, which is to decide specific serving orders within the\nconstructed loops. Generating high-quality strategies in both stages is\nimportant to develop DDS but faces several challenging. Meanwhile, deep\nreinforcement learning (DRL) has been developed rapidly in recent years. It is\na powerful tool to solve these problems since DRL can learn a parametric model\nwithout relying on too many problem-based assumptions and optimize long-term\neffect by learning sequential decisions. In this survey, we first define DDS,\nthen highlight common applications and important decision/control problems\nwithin. For each problem, we comprehensively introduce the existing DRL\nsolutions, and further summarize them in\n\\textit{https://github.com/tsinghua-fib-lab/DDS\\_Survey}. We also introduce\nopen simulation environments for development and evaluation of DDS\napplications. Finally, we analyze remaining challenges and discuss further\nresearch opportunities in DRL solutions for DDS.",
          "link": "http://arxiv.org/abs/2108.04462",
          "publishedOn": "2021-08-11T01:55:21.345Z",
          "wordCount": 667,
          "title": "Deep Reinforcement Learning for Demand Driven Services in Logistics and Transportation Systems: A Survey. (arXiv:2108.04462v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Minhao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaocheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>",
          "description": "Differentiable Neural Architecture Search is one of the most popular Neural\nArchitecture Search (NAS) methods for its search efficiency and simplicity,\naccomplished by jointly optimizing the model weight and architecture parameters\nin a weight-sharing supernet via gradient-based algorithms. At the end of the\nsearch phase, the operations with the largest architecture parameters will be\nselected to form the final architecture, with the implicit assumption that the\nvalues of architecture parameters reflect the operation strength. While much\nhas been discussed about the supernet's optimization, the architecture\nselection process has received little attention. We provide empirical and\ntheoretical analysis to show that the magnitude of architecture parameters does\nnot necessarily indicate how much the operation contributes to the supernet's\nperformance. We propose an alternative perturbation-based architecture\nselection that directly measures each operation's influence on the supernet. We\nre-evaluate several differentiable NAS methods with the proposed architecture\nselection and find that it is able to extract significantly improved\narchitectures from the underlying supernets consistently. Furthermore, we find\nthat several failure modes of DARTS can be greatly alleviated with the proposed\nselection method, indicating that much of the poor generalization observed in\nDARTS can be attributed to the failure of magnitude-based architecture\nselection rather than entirely the optimization of its supernet.",
          "link": "http://arxiv.org/abs/2108.04392",
          "publishedOn": "2021-08-11T01:55:21.338Z",
          "wordCount": 656,
          "title": "Rethinking Architecture Selection in Differentiable NAS. (arXiv:2108.04392v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patrascu_A/0/1/0/all/0/1\">Andrei Patrascu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irofti_P/0/1/0/all/0/1\">Paul Irofti</a>",
          "description": "Several decades ago the Proximal Point Algorithm (PPA) started to gain much\nattraction for both abstract operator theory and the numerical optimization\ncommunities. Even in modern applications, researchers still use proximal\nminimization theory to design scalable algorithms that overcome nonsmoothness\nin high dimensional models. Several remarkable references as\n\\cite{Fer:91,Ber:82constrained,Ber:89parallel,Tom:11} analyzed the tight local\nrelations between the convergence rate of PPA and the regularity of the\nobjective function. However, without taking into account the concrete\ncomputational effort paid for computing each PPA iteration, any iteration\ncomplexity remains abstract and purely informative. In this manuscript we aim\nto evaluate the computational complexity of practical PPA in terms of\n(proximal) gradient/subgradient iterations, which might allow a fair\npositioning of the famous PPA numerical performance in the class of first order\nmethods. First, we derive nonasymptotic iteration complexity estimates of exact\nand inexact PPA to minimize convex functions under $\\gamma-$Holderian growth:\n$\\BigO{\\log(1/\\epsilon)}$ (for $\\gamma \\in [1,2]$) and\n$\\BigO{1/\\epsilon^{\\gamma - 2}}$ (for $\\gamma > 2$). In particular, we recover\nwell-known results on exact PPA: finite convergence for sharp minima and linear\nconvergence for quadratic growth, even under presence of inexactness. Second,\nassuming that an usual (proximal) gradient/subgradient method subroutine is\nemployed to compute inexact PPA iteration, we show novel computational\ncomplexity bounds on a restarted variant of the inexact PPA, available when no\ninformation on the growth of the objective function is known. In the numerical\nexperiments we confirm the practical performance and implementability of our\nschemes.",
          "link": "http://arxiv.org/abs/2108.04482",
          "publishedOn": "2021-08-11T01:55:21.322Z",
          "wordCount": 678,
          "title": "Computational complexity of Inexact Proximal Point Algorithm for Convex Optimization under Holderian Growth. (arXiv:2108.04482v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rasaee_H/0/1/0/all/0/1\">Hamza Rasaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivaz_H/0/1/0/all/0/1\">Hassan Rivaz</a>",
          "description": "Ultrasound is a non-invasive imaging modality that can be conveniently used\nto classify suspicious breast nodules and potentially detect the onset of\nbreast cancer. Recently, Convolutional Neural Networks (CNN) techniques have\nshown promising results in classifying ultrasound images of the breast into\nbenign or malignant. However, CNN inference acts as a black-box model, and as\nsuch, its decision-making is not interpretable. Therefore, increasing effort\nhas been dedicated to explaining this process, most notably through GRAD-CAM\nand other techniques that provide visual explanations into inner workings of\nCNNs. In addition to interpretation, these methods provide clinically important\ninformation, such as identifying the location for biopsy or treatment. In this\nwork, we analyze how adversarial assaults that are practically undetectable may\nbe devised to alter these importance maps dramatically. Furthermore, we will\nshow that this change in the importance maps can come with or without altering\nthe classification result, rendering them even harder to detect. As such, care\nmust be taken when using these importance maps to shed light on the inner\nworkings of deep learning. Finally, we utilize Multi-Task Learning (MTL) and\npropose a new network based on ResNet-50 to improve the classification\naccuracies. Our sensitivity and specificity is comparable to the state of the\nart results.",
          "link": "http://arxiv.org/abs/2108.04345",
          "publishedOn": "2021-08-11T01:55:21.313Z",
          "wordCount": 667,
          "title": "Explainable AI and susceptibility to adversarial attacks: a case study in classification of breast ultrasound images. (arXiv:2108.04345v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaopeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhijie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_M/0/1/0/all/0/1\">Mingyu Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_M/0/1/0/all/0/1\">Maojing Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>",
          "description": "We study the problem of knowledge tracing (KT) where the goal is to trace the\nstudents' knowledge mastery over time so as to make predictions on their future\nperformance. Owing to the good representation capacity of deep neural networks\n(DNNs), recent advances on KT have increasingly concentrated on exploring DNNs\nto improve the performance of KT. However, we empirically reveal that the DNNs\nbased KT models may run the risk of overfitting, especially on small datasets,\nleading to limited generalization. In this paper, by leveraging the current\nadvances in adversarial training (AT), we propose an efficient AT based KT\nmethod (ATKT) to enhance KT model's generalization and thus push the limit of\nKT. Specifically, we first construct adversarial perturbations and add them on\nthe original interaction embeddings as adversarial examples. The original and\nadversarial examples are further used to jointly train the KT model, forcing it\nis not only to be robust to the adversarial examples, but also to enhance the\ngeneralization over the original ones. To better implement AT, we then present\nan efficient attentive-LSTM model as KT backbone, where the key is a proposed\nknowledge hidden state attention module that adaptively aggregates information\nfrom previous knowledge hidden states while simultaneously highlighting the\nimportance of current knowledge hidden state to make a more accurate\nprediction. Extensive experiments on four public benchmark datasets demonstrate\nthat our ATKT achieves new state-of-the-art performance. Code is available at:\n\\color{blue} {\\url{https://github.com/xiaopengguo/ATKT}}.",
          "link": "http://arxiv.org/abs/2108.04430",
          "publishedOn": "2021-08-11T01:55:21.306Z",
          "wordCount": 681,
          "title": "Enhancing Knowledge Tracing via Adversarial Training. (arXiv:2108.04430v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Renjie Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanzhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiabao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Aiqun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_D/0/1/0/all/0/1\">Derrick Wing Kwan Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swindlehurst_A/0/1/0/all/0/1\">A. Lee Swindlehurst</a>",
          "description": "Radio-frequency fingerprints~(RFFs) are promising solutions for realizing\nlow-cost physical layer authentication. Machine learning-based methods have\nbeen proposed for RFF extraction and discrimination. However, most existing\nmethods are designed for the closed-set scenario where the set of devices is\nremains unchanged. These methods can not be generalized to the RFF\ndiscrimination of unknown devices. To enable the discrimination of RFF from\nboth known and unknown devices, we propose a new end-to-end deep learning\nframework for extracting RFFs from raw received signals. The proposed framework\ncomprises a novel preprocessing module, called neural synchronization~(NS),\nwhich incorporates the data-driven learning with signal processing priors as an\ninductive bias from communication-model based processing. Compared to\ntraditional carrier synchronization techniques, which are static, this module\nestimates offsets by two learnable deep neural networks jointly trained by the\nRFF extractor. Additionally, a hypersphere representation is proposed to\nfurther improve the discrimination of RFF. Theoretical analysis shows that such\na data-and-model framework can better optimize the mutual information between\ndevice identity and the RFF, which naturally leads to better performance.\nExperimental results verify that the proposed RFF significantly outperforms\npurely data-driven DNN-design and existing handcrafted RFF methods in terms of\nboth discrimination and network generalizability.",
          "link": "http://arxiv.org/abs/2108.04436",
          "publishedOn": "2021-08-11T01:55:21.300Z",
          "wordCount": 659,
          "title": "A Generalizable Model-and-Data Driven Approach for Open-Set RFF Authentication. (arXiv:2108.04436v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>",
          "description": "This survey is meant to provide an introduction to the fundamental theorem of\nlinear algebra and the theories behind them. Our goal is to give a rigorous\nintroduction to the readers with prior exposure to linear algebra.\nSpecifically, we provide some details and proofs of some results from (Strang,\n1993). We then describe the fundamental theorem of linear algebra from\ndifferent views and find the properties and relationships behind the views. The\nfundamental theorem of linear algebra is essential in many fields, such as\nelectrical engineering, computer science, machine learning, and deep learning.\nThis survey is primarily a summary of purpose, significance of important\ntheories behind it.\n\nThe sole aim of this survey is to give a self-contained introduction to\nconcepts and mathematical tools in theory behind the fundamental theorem of\nlinear algebra and rigorous analysis in order to seamlessly introduce its\nproperties in four subspaces in subsequent sections. However, we clearly\nrealize our inability to cover all the useful and interesting results and given\nthe paucity of scope to present this discussion, e.g., the separated analysis\nof the (orthogonal) projection matrices. We refer the reader to literature in\nthe field of linear algebra for a more detailed introduction to the related\nfields. Some excellent examples include (Rose, 1982; Strang, 2009; Trefethen\nand Bau III, 1997; Strang, 2019, 2021).",
          "link": "http://arxiv.org/abs/2108.04432",
          "publishedOn": "2021-08-11T01:55:21.293Z",
          "wordCount": 646,
          "title": "Revisit the Fundamental Theorem of Linear Algebra. (arXiv:2108.04432v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04496",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Ming Xiao</a>",
          "description": "The recurrent neural networks (RNN) with richly distributed internal states\nand flexible non-linear transition functions, have overtaken the dynamic\nBayesian networks such as the hidden Markov models (HMMs) in the task of\nmodeling highly structured sequential data. These data, such as from speech and\nhandwriting, often contain complex relationships between the underlaying\nvariational factors and the observed data. The standard RNN model has very\nlimited randomness or variability in its structure, coming from the output\nconditional probability model. This paper will present different ways of using\nhigh level latent random variables in RNN to model the variability in the\nsequential data, and the training method of such RNN model under the VAE\n(Variational Autoencoder) principle. We will explore possible ways of using\nadversarial method to train a variational RNN model. Contrary to competing\napproaches, our approach has theoretical optimum in the model training and\nprovides better model training stability. Our approach also improves the\nposterior approximation in the variational inference network by a separated\nadversarial training step. Numerical results simulated from TIMIT speech data\nshow that reconstruction loss and evidence lower bound converge to the same\nlevel and adversarial training loss converges to 0.",
          "link": "http://arxiv.org/abs/2108.04496",
          "publishedOn": "2021-08-11T01:55:21.273Z",
          "wordCount": 627,
          "title": "Regularized Sequential Latent Variable Models with Adversarial Neural Networks. (arXiv:2108.04496v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1\">Ayaan Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutradhar_I/0/1/0/all/0/1\">Ipsita Sutradhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mahziba Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mehedi Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1\">Malabika Sarker</a>",
          "description": "Diabetes is one of the most prevalent chronic diseases in Bangladesh, and as\na result, Diabetic Retinopathy (DR) is widespread in the population. DR, an eye\nillness caused by diabetes, can lead to blindness if it is not identified and\ntreated in its early stages. Unfortunately, diagnosis of DR requires medically\ntrained professionals, but Bangladesh has limited specialists in comparison to\nits population. Moreover, the screening process is often expensive, prohibiting\nmany from receiving timely and proper diagnosis. To address the problem, we\nintroduce a deep learning algorithm which screens for different stages of DR.\nWe use a state-of-the-art CNN architecture to diagnose patients based on\nretinal fundus imagery. This paper is an experimental evaluation of the\nalgorithm we developed for DR diagnosis and screening specifically for\nBangladeshi patients. We perform this validation study using separate pools of\nretinal image data of real patients from a hospital and field studies in\nBangladesh. Our results show that the algorithm is effective at screening\nBangladeshi eyes even when trained on a public dataset which is out of domain,\nand can accurately determine the stage of DR as well, achieving an overall\naccuracy of 92.27\\% and 93.02\\% on two validation sets of Bangladeshi eyes. The\nresults confirm the ability of the algorithm to be used in real clinical\nsettings and applications due to its high accuracy and classwise metrics. Our\nalgorithm is implemented in the application Drishti, which is used to screen\nfor DR in patients living in rural areas in Bangladesh, where access to\nprofessional screening is limited.",
          "link": "http://arxiv.org/abs/2108.04358",
          "publishedOn": "2021-08-11T01:55:21.268Z",
          "wordCount": 706,
          "title": "Convolutional Nets for Diabetic Retinopathy Screening in Bangladeshi Patients. (arXiv:2108.04358v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Amey Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1\">Mega Satish</a>",
          "description": "This paper aims to demonstrate the efficiency of the Adversarial Open Domain\nAdaption framework for sketch-to-photo synthesis. The unsupervised open domain\nadaption for generating realistic photos from a hand-drawn sketch is\nchallenging as there is no such sketch of that class for training data. The\nabsence of learning supervision and the huge domain gap between both the\nfreehand drawing and picture domains make it hard. We present an approach that\nlearns both sketch-to-photo and photo-to-sketch generation to synthesise the\nmissing freehand drawings from pictures. Due to the domain gap between\nsynthetic sketches and genuine ones, the generator trained on false drawings\nmay produce unsatisfactory results when dealing with drawings of lacking\nclasses. To address this problem, we offer a simple but effective open-domain\nsampling and optimization method that tricks the generator into considering\nfalse drawings as genuine. Our approach generalises the learnt sketch-to-photo\nand photo-to-sketch mappings from in-domain input to open-domain categories. On\nthe Scribble and SketchyCOCO datasets, we compared our technique to the most\ncurrent competing methods. For many types of open-domain drawings, our model\noutperforms impressive results in synthesising accurate colour, substance, and\nretaining the structural layout.",
          "link": "http://arxiv.org/abs/2108.04351",
          "publishedOn": "2021-08-11T01:55:21.262Z",
          "wordCount": 633,
          "title": "Adversarial Open Domain Adaption Framework (AODA): Sketch-to-Photo Synthesis. (arXiv:2108.04351v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Haocheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Caleb Chen Cao</a>",
          "description": "Explanation of AI, as well as fairness of algorithms' decisions and the\ntransparency of the decision model, are becoming more and more important. And\nit is crucial to design effective and human-friendly techniques when opening\nthe black-box model. Counterfactual conforms to the human way of thinking and\nprovides a human-friendly explanation, and its corresponding explanation\nalgorithm refers to a strategic alternation of a given data point so that its\nmodel output is \"counter-facted\", i.e. the prediction is reverted. In this\npaper, we adapt counterfactual explanation over fine-grained image\nclassification problem. We demonstrated an adaptive method that could give a\ncounterfactual explanation by showing the composed counterfactual feature map\nusing top-down layer searching algorithm (TDLS). We have proved that our TDLS\nalgorithm could provide more flexible counterfactual visual explanation in an\nefficient way using VGG-16 model on Caltech-UCSD Birds 200 dataset. At the end,\nwe discussed several applicable scenarios of counterfactual visual\nexplanations.",
          "link": "http://arxiv.org/abs/2108.04238",
          "publishedOn": "2021-08-11T01:55:21.256Z",
          "wordCount": 608,
          "title": "TDLS: A Top-Down Layer Searching Algorithm for Generating Counterfactual Visual Explanation. (arXiv:2108.04238v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Curtis_C/0/1/0/all/0/1\">Christopher W. Curtis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alford_Lago_D/0/1/0/all/0/1\">Daniel Jay Alford-Lago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Issan_O/0/1/0/all/0/1\">Opal Issan</a>",
          "description": "Koopman operator theory shows how nonlinear dynamical systems can be\nrepresented as an infinite-dimensional, linear operator acting on a Hilbert\nspace of observables of the system. However, determining the relevant modes and\neigenvalues of this infinite-dimensional operator can be difficult. The\nextended dynamic mode decomposition (EDMD) is one such method for generating\napproximations to Koopman spectra and modes, but the EDMD method faces its own\nset of challenges due to the need of user defined observables. To address this\nissue, we explore the use of convolutional autoencoder networks to\nsimultaneously find optimal families of observables which also generate both\naccurate embeddings of the flow into a space of observables and immersions of\nthe observables back into flow coordinates. This network results in a global\ntransformation of the flow and affords future state prediction via EDMD and the\ndecoder network. We call this method deep learning dynamic mode decomposition\n(DLDMD). The method is tested on canonical nonlinear data sets and is shown to\nproduce results that outperform a standard DMD approach.",
          "link": "http://arxiv.org/abs/2108.04433",
          "publishedOn": "2021-08-11T01:55:21.250Z",
          "wordCount": 606,
          "title": "Deep Learning Enhanced Dynamic Mode Decomposition. (arXiv:2108.04433v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiaoyang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Huilin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wancheng Ge</a>",
          "description": "Deep Neural Networks (DNNs) are vulnerable to adversarial examples which\nwould inveigle neural networks to make prediction errors with small per-\nturbations on the input images. Researchers have been devoted to promoting the\nresearch on the universal adversarial perturbations (UAPs) which are\ngradient-free and have little prior knowledge on data distributions. Procedural\nadversarial noise at- tack is a data-free universal perturbation generation\nmethod. In this paper, we propose two universal adversarial perturbation (UAP)\ngeneration methods based on procedural noise functions: Simplex noise and\nWorley noise. In our framework, the shading which disturbs visual\nclassification is generated with rendering technology. Without changing the\nsemantic representations, the adversarial examples generated via our methods\nshow superior performance on the attack.",
          "link": "http://arxiv.org/abs/2108.04409",
          "publishedOn": "2021-08-11T01:55:21.244Z",
          "wordCount": 560,
          "title": "On Procedural Adversarial Noise Attack And Defense. (arXiv:2108.04409v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuntao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wenjie Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Sinno Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongjun Wang</a>",
          "description": "Time series has wide applications in the real world and is known to be\ndifficult to forecast. Since its statistical properties change over time, its\ndistribution also changes temporally, which will cause severe distribution\nshift problem to existing methods. However, it remains unexplored to model the\ntime series in the distribution perspective. In this paper, we term this as\nTemporal Covariate Shift (TCS). This paper proposes Adaptive RNNs (AdaRNN) to\ntackle the TCS problem by building an adaptive model that generalizes well on\nthe unseen test data. AdaRNN is sequentially composed of two novel algorithms.\nFirst, we propose Temporal Distribution Characterization to better characterize\nthe distribution information in the TS. Second, we propose Temporal\nDistribution Matching to reduce the distribution mismatch in TS to learn the\nadaptive TS model. AdaRNN is a general framework with flexible distribution\ndistances integrated. Experiments on human activity recognition, air quality\nprediction, and financial analysis show that AdaRNN outperforms the latest\nmethods by a classification accuracy of 2.6% and significantly reduces the RMSE\nby 9.0%. We also show that the temporal distribution matching algorithm can be\nextended in Transformer structure to boost its performance.",
          "link": "http://arxiv.org/abs/2108.04443",
          "publishedOn": "2021-08-11T01:55:21.221Z",
          "wordCount": 642,
          "title": "AdaRNN: Adaptive Learning and Forecasting of Time Series. (arXiv:2108.04443v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tailin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Desen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yu Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>",
          "description": "The task of skeleton-based action recognition remains a core challenge in\nhuman-centred scene understanding due to the multiple granularities and large\nvariation in human motion. Existing approaches typically employ a single neural\nrepresentation for different motion patterns, which has difficulty in capturing\nfine-grained action classes given limited training data. To address the\naforementioned problems, we propose a novel multi-granular spatio-temporal\ngraph network for skeleton-based action classification that jointly models the\ncoarse- and fine-grained skeleton motion patterns. To this end, we develop a\ndual-head graph network consisting of two interleaved branches, which enables\nus to extract features at two spatio-temporal resolutions in an effective and\nefficient manner. Moreover, our network utilises a cross-head communication\nstrategy to mutually enhance the representations of both heads. We conducted\nextensive experiments on three large-scale datasets, namely NTU RGB+D 60, NTU\nRGB+D 120, and Kinetics-Skeleton, and achieves the state-of-the-art performance\non all the benchmarks, which validates the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2108.04536",
          "publishedOn": "2021-08-11T01:55:21.215Z",
          "wordCount": 607,
          "title": "Learning Multi-Granular Spatio-Temporal Graph Network for Skeleton-based Action Recognition. (arXiv:2108.04536v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04428",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Han_Y/0/1/0/all/0/1\">Yuefeng Han</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_C/0/1/0/all/0/1\">Cun-Hui Zhang</a>",
          "description": "The CP decomposition for high dimensional non-orthogonal spike tensors is an\nimportant problem with broad applications across many disciplines. However,\nprevious works with theoretical guarantee typically assume restrictive\nincoherence conditions on the basis vectors for the CP components. In this\npaper, we propose new computationally efficient composite PCA and concurrent\northogonalization algorithms for tensor CP decomposition with theoretical\nguarantees under mild incoherence conditions. The composite PCA applies the\nprincipal component or singular value decompositions twice, first to a matrix\nunfolding of the tensor data to obtain singular vectors and then to the matrix\nfolding of the singular vectors obtained in the first step. It can be used as\nan initialization for any iterative optimization schemes for the tensor CP\ndecomposition. The concurrent orthogonalization algorithm iteratively estimates\nthe basis vector in each mode of the tensor by simultaneously applying\nprojections to the orthogonal complements of the spaces generated by others CP\ncomponents in other modes. It is designed to improve the alternating least\nsquares estimator and other forms of the high order orthogonal iteration for\ntensors with low or moderately high CP ranks. Our theoretical investigation\nprovides estimation accuracy and statistical convergence rates for the two\nproposed algorithms. Our implementations on synthetic data demonstrate\nsignificant practical superiority of our approach over existing methods.",
          "link": "http://arxiv.org/abs/2108.04428",
          "publishedOn": "2021-08-11T01:55:21.207Z",
          "wordCount": 649,
          "title": "Tensor Principal Component Analysis in High Dimensional CP Models. (arXiv:2108.04428v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baracaldo_N/0/1/0/all/0/1\">Nathalie Baracaldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_J/0/1/0/all/0/1\">James Joshi</a>",
          "description": "Machine learning (ML) is increasingly being adopted in a wide variety of\napplication domains. Usually, a well-performing ML model, especially, emerging\ndeep neural network model, relies on a large volume of training data and\nhigh-powered computational resources. The need for a vast volume of available\ndata raises serious privacy concerns because of the risk of leakage of highly\nprivacy-sensitive information and the evolving regulatory environments that\nincreasingly restrict access to and use of privacy-sensitive data. Furthermore,\na trained ML model may also be vulnerable to adversarial attacks such as\nmembership/property inference attacks and model inversion attacks. Hence,\nwell-designed privacy-preserving ML (PPML) solutions are crucial and have\nattracted increasing research interest from academia and industry. More and\nmore efforts of PPML are proposed via integrating privacy-preserving techniques\ninto ML algorithms, fusing privacy-preserving approaches into ML pipeline, or\ndesigning various privacy-preserving architectures for existing ML systems. In\nparticular, existing PPML arts cross-cut ML, system, security, and privacy;\nhence, there is a critical need to understand state-of-art studies, related\nchallenges, and a roadmap for future research. This paper systematically\nreviews and summarizes existing privacy-preserving approaches and proposes a\nPGU model to guide evaluation for various PPML solutions through elaborately\ndecomposing their privacy-preserving functionalities. The PGU model is designed\nas the triad of Phase, Guarantee, and technical Utility. Furthermore, we also\ndiscuss the unique characteristics and challenges of PPML and outline possible\ndirections of future work that benefit a wide range of research communities\namong ML, distributed systems, security, and privacy areas.",
          "link": "http://arxiv.org/abs/2108.04417",
          "publishedOn": "2021-08-11T01:55:21.191Z",
          "wordCount": 682,
          "title": "Privacy-Preserving Machine Learning: Methods, Challenges and Directions. (arXiv:2108.04417v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saeed_S/0/1/0/all/0/1\">Shaheer U. Saeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yunguan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stavrinides_V/0/1/0/all/0/1\">Vasilis Stavrinides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baum_Z/0/1/0/all/0/1\">Zachary M. C. Baum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qianye Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusu_M/0/1/0/all/0/1\">Mirabela Rusu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Richard E. Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonn_G/0/1/0/all/0/1\">Geoffrey A. Sonn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1\">J. Alison Noble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barratt_D/0/1/0/all/0/1\">Dean C. Barratt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>",
          "description": "The performance of many medical image analysis tasks are strongly associated\nwith image data quality. When developing modern deep learning algorithms,\nrather than relying on subjective (human-based) image quality assessment (IQA),\ntask amenability potentially provides an objective measure of task-specific\nimage quality. To predict task amenability, an IQA agent is trained using\nreinforcement learning (RL) with a simultaneously optimised task predictor,\nsuch as a classification or segmentation neural network. In this work, we\ndevelop transfer learning or adaptation strategies to increase the adaptability\nof both the IQA agent and the task predictor so that they are less dependent on\nhigh-quality, expert-labelled training data. The proposed transfer learning\nstrategy re-formulates the original RL problem for task amenability in a\nmeta-reinforcement learning (meta-RL) framework. The resulting algorithm\nfacilitates efficient adaptation of the agent to different definitions of image\nquality, each with its own Markov decision process environment including\ndifferent images, labels and an adaptable task predictor. Our work demonstrates\nthat the IQA agents pre-trained on non-expert task labels can be adapted to\npredict task amenability as defined by expert task labels, using only a small\nset of expert labels. Using 6644 clinical ultrasound images from 249 prostate\ncancer patients, our results for image classification and segmentation tasks\nshow that the proposed IQA method can be adapted using data with as few as\nrespective 19.7% and 29.6% expert-reviewed consensus labels and still achieve\ncomparable IQA and task performance, which would otherwise require a training\ndataset with 100% expert labels.",
          "link": "http://arxiv.org/abs/2108.04359",
          "publishedOn": "2021-08-11T01:55:21.186Z",
          "wordCount": 727,
          "title": "Adaptable image quality assessment using meta-reinforcement learning of task amenability. (arXiv:2108.04359v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Unnikrishnan_B/0/1/0/all/0/1\">Balagopal Unnikrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaram_S/0/1/0/all/0/1\">Shafa Balaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1\">Chuan Sheng Foo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_P/0/1/0/all/0/1\">Pavitra Krishnaswamy</a>",
          "description": "Deep learning models achieve strong performance for radiology image\nclassification, but their practical application is bottlenecked by the need for\nlarge labeled training datasets. Semi-supervised learning (SSL) approaches\nleverage small labeled datasets alongside larger unlabeled datasets and offer\npotential for reducing labeling cost. In this work, we introduce NoTeacher, a\nnovel consistency-based SSL framework which incorporates probabilistic\ngraphical models. Unlike Mean Teacher which maintains a teacher network updated\nvia a temporal ensemble, NoTeacher employs two independent networks, thereby\neliminating the need for a teacher network. We demonstrate how NoTeacher can be\ncustomized to handle a range of challenges in radiology image classification.\nSpecifically, we describe adaptations for scenarios with 2D and 3D inputs, uni\nand multi-label classification, and class distribution mismatch between labeled\nand unlabeled portions of the training data. In realistic empirical evaluations\non three public benchmark datasets spanning the workhorse modalities of\nradiology (X-Ray, CT, MRI), we show that NoTeacher achieves over 90-95% of the\nfully supervised AUROC with less than 5-15% labeling budget. Further, NoTeacher\noutperforms established SSL methods with minimal hyperparameter tuning, and has\nimplications as a principled and practical option for semisupervised learning\nin radiology applications.",
          "link": "http://arxiv.org/abs/2108.04423",
          "publishedOn": "2021-08-11T01:55:21.164Z",
          "wordCount": 669,
          "title": "Semi-supervised classification of radiology images with NoTeacher: A Teacher that is not Mean. (arXiv:2108.04423v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tatsunami_Y/0/1/0/all/0/1\">Yuki Tatsunami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taki_M/0/1/0/all/0/1\">Masato Taki</a>",
          "description": "For the past ten years, CNN has reigned supreme in the world of computer\nvision, but recently, Transformer is on the rise. However, the quadratic\ncomputational cost of self-attention has become a severe problem of practice.\nThere has been much research on architectures without CNN and self-attention in\nthis context. In particular, MLP-Mixer is a simple idea designed using MLPs and\nhit an accuracy comparable to the Vision Transformer. However, the only\ninductive bias in this architecture is the embedding of tokens. Thus, there is\nstill a possibility to build a non-convolutional inductive bias into the\narchitecture itself, and we built in an inductive bias using two simple ideas.\nA way is to divide the token-mixing block vertically and horizontally. Another\nway is to make spatial correlations denser among some channels of token-mixing.\nWith this approach, we were able to improve the accuracy of the MLP-Mixer while\nreducing its parameters and computational complexity. Compared to other\nMLP-based models, the proposed model, named RaftMLP has a good balance of\ncomputational complexity, the number of parameters, and actual memory usage. In\naddition, our work indicates that MLP-based models have the potential to\nreplace CNNs by adopting inductive bias. The source code in PyTorch version is\navailable at \\url{https://github.com/okojoalg/raft-mlp}.",
          "link": "http://arxiv.org/abs/2108.04384",
          "publishedOn": "2021-08-11T01:55:21.158Z",
          "wordCount": 651,
          "title": "RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?. (arXiv:2108.04384v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this paper, we present a new network named Attention Aware Network (AASeg)\nfor real time semantic image segmentation. Our network incorporates spatial and\nchannel information using Spatial Attention (SA) and Channel Attention (CA)\nmodules respectively. It also uses dense local multi-scale context information\nusing Multi Scale Context (MSC) module. The feature maps are concatenated\nindividually to produce the final segmentation map. We demonstrate the\neffectiveness of our method using a comprehensive analysis, quantitative\nexperimental results and ablation study using Cityscapes, ADE20K and Camvid\ndatasets. Our network performs better than most previous architectures with a\n74.4\\% Mean IOU on Cityscapes test dataset while running at 202.7 FPS.",
          "link": "http://arxiv.org/abs/2108.04349",
          "publishedOn": "2021-08-11T01:55:21.151Z",
          "wordCount": 556,
          "title": "AASeg: Attention Aware Network for Real Time Semantic Segmentation. (arXiv:2108.04349v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gonga_W/0/1/0/all/0/1\">Wenwen Gonga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuyun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yucong Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yawei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chena_Y/0/1/0/all/0/1\">Yifei Chena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lianyong Qi</a>",
          "description": "With the ever-increasing prevalence of web APIs (Application Programming\nInterfaces) in enabling smart software developments, finding and composing a\nlist of existing web APIs that can corporately fulfil the software developers'\nfunctional needs have become a promising way to develop a successful mobile\napp, economically and conveniently. However, the big volume and diversity of\ncandidate web APIs put additional burden on the app developers' web APIs\nselection decision-makings, since it is often a challenging task to\nsimultaneously guarantee the diversity and compatibility of the finally\nselected a set of web APIs. Considering this challenge, a Diversity-aware and\nCompatibility-driven web APIs Recommendation approach, namely DivCAR, is put\nforward in this paper. First, to achieve diversity, DivCAR employs random walk\nsampling technique on a pre-built correlation graph to generate diverse\ncorrelation subgraphs. Afterwards, with the diverse correlation subgraphs, we\nmodel the compatible web APIs recommendation problem to be a minimum group\nSteiner tree search problem. Through solving the minimum group Steiner tree\nsearch problem, manifold sets of compatible and diverse web APIs ranked are\nreturned to the app developers. At last, we design and enact a set of\nexperiments on a real-world dataset crawled from www.programmableWeb.com.\nExperimental results validate the effectiveness and efficiency of our proposed\nDivCAR approach in balancing the web APIs recommendation diversity and\ncompatibility.",
          "link": "http://arxiv.org/abs/2108.04389",
          "publishedOn": "2021-08-11T01:55:21.139Z",
          "wordCount": 655,
          "title": "Diversity-aware Web APIs Recommendation with Compatibility Guarantee. (arXiv:2108.04389v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04327",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Mikula_K/0/1/0/all/0/1\">Karol Mikula</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kollar_M/0/1/0/all/0/1\">Michal Kollar</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ozvat_A/0/1/0/all/0/1\">Aneta A. Ozvat</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ambroz_M/0/1/0/all/0/1\">Martin Ambroz</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cahojova_L/0/1/0/all/0/1\">Lucia Cahojova</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jarolimek_I/0/1/0/all/0/1\">Ivan Jarolimek</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sibik_J/0/1/0/all/0/1\">Jozef Sibik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sibikova_M/0/1/0/all/0/1\">Maria Sibikova</a>",
          "description": "Natural numerical networks are introduced as a new classification algorithm\nbased on the numerical solution of nonlinear partial differential equations of\nforward-backward diffusion type on complete graphs. The proposed natural\nnumerical network is applied to open important environmental and nature\nconservation task, the automated identification of protected habitats by using\nsatellite images. In the natural numerical network, the forward diffusion\ncauses the movement of points in a feature space toward each other. The\nopposite effect, keeping the points away from each other, is caused by backward\ndiffusion. This yields the desired classification. The natural numerical\nnetwork contains a few parameters that are optimized in the learning phase of\nthe method. After learning parameters and optimizing the topology of the\nnetwork graph, classification necessary for habitat identification is\nperformed. A relevancy map for each habitat is introduced as a tool for\nvalidating the classification and finding new Natura 2000 habitat appearances.",
          "link": "http://arxiv.org/abs/2108.04327",
          "publishedOn": "2021-08-11T01:55:21.132Z",
          "wordCount": 615,
          "title": "Natural Numerical Networks for Natura 2000 habitats classification by satellite images. (arXiv:2108.04327v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kummen_A/0/1/0/all/0/1\">Ashild Kummen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_A/0/1/0/all/0/1\">Ali Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganeva_T/0/1/0/all/0/1\">Teodora Ganeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qianying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_R/0/1/0/all/0/1\">Robert Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratwatte_C/0/1/0/all/0/1\">Chenuka Ratwatte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almazov_E/0/1/0/all/0/1\">Emil Almazov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visram_S/0/1/0/all/0/1\">Sheena Visram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_A/0/1/0/all/0/1\">Andrew Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebire_N/0/1/0/all/0/1\">Neil J Sebire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stott_L/0/1/0/all/0/1\">Lee Stott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_Y/0/1/0/all/0/1\">Yvonne Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_G/0/1/0/all/0/1\">Graham Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamedally_D/0/1/0/all/0/1\">Dean Mohamedally</a>",
          "description": "Touchless computer interaction has become an important consideration during\nthe COVID-19 pandemic period. Despite progress in machine learning and computer\nvision that allows for advanced gesture recognition, an integrated collection\nof such open-source methods and a user-customisable approach to utilising them\nin a low-cost solution for touchless interaction in existing software is still\nmissing. In this paper, we introduce the MotionInput v2.0 application. This\napplication utilises published open-source libraries and additional gesture\ndefinitions developed to take the video stream from a standard RGB webcam as\ninput. It then maps human motion gestures to input operations for existing\napplications and games. The user can choose their own preferred way of\ninteracting from a series of motion types, including single and bi-modal hand\ngesturing, full-body repetitive or extremities-based exercises, head and facial\nmovements, eye tracking, and combinations of the above. We also introduce a\nseries of bespoke gesture recognition classifications as DirectInput triggers,\nincluding gestures for idle states, auto calibration, depth capture from a 2D\nRGB webcam stream and tracking of facial motions such as mouth motions,\nwinking, and head direction with rotation. Three use case areas assisted the\ndevelopment of the modules: creativity software, office and clinical software,\nand gaming software. A collection of open-source libraries has been integrated\nand provide a layer of modular gesture mapping on top of existing mouse and\nkeyboard controls in Windows via DirectX. With ease of access to webcams\nintegrated into most laptops and desktop computers, touchless computing becomes\nmore available with MotionInput v2.0, in a federated and locally processed\nmethod.",
          "link": "http://arxiv.org/abs/2108.04357",
          "publishedOn": "2021-08-11T01:55:21.119Z",
          "wordCount": 818,
          "title": "MotionInput v2.0 supporting DirectX: A modular library of open-source gesture-based machine learning and computer vision methods for interacting and controlling existing software with a webcam. (arXiv:2108.04357v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Panday_A/0/1/0/all/0/1\">Aishwarza Panday</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Muhammad Ashad Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1\">Nihad Karim Chowdhury</a>",
          "description": "Due to the limited availability and high cost of the reverse\ntranscription-polymerase chain reaction (RT-PCR) test, many studies have\nproposed machine learning techniques for detecting COVID-19 from medical\nimaging. The purpose of this study is to systematically review, assess, and\nsynthesize research articles that have used different machine learning\ntechniques to detect and diagnose COVID-19 from chest X-ray and CT scan images.\nA structured literature search was conducted in the relevant bibliographic\ndatabases to ensure that the survey solely centered on reproducible and\nhigh-quality research. We selected papers based on our inclusion criteria. In\nthis survey, we reviewed $98$ articles that fulfilled our inclusion criteria.\nWe have surveyed a complete pipeline of chest imaging analysis techniques\nrelated to COVID-19, including data collection, pre-processing, feature\nextraction, classification, and visualization. We have considered CT scans and\nX-rays as both are widely used to describe the latest developments in medical\nimaging to detect COVID-19. This survey provides researchers with valuable\ninsights into different machine learning techniques and their performance in\nthe detection and diagnosis of COVID-19 from chest imaging. At the end, the\nchallenges and limitations in detecting COVID-19 using machine learning\ntechniques and the future direction of research are discussed.",
          "link": "http://arxiv.org/abs/2108.04344",
          "publishedOn": "2021-08-11T01:55:21.078Z",
          "wordCount": 714,
          "title": "A Survey of Machine Learning Techniques for Detecting and Diagnosing COVID-19 from Imaging. (arXiv:2108.04344v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04267",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Estrada_S/0/1/0/all/0/1\">Santiago Estrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Ran Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diers_K/0/1/0/all/0/1\">Kersten Diers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Weiyi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehses_P/0/1/0/all/0/1\">Philipp Ehses</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stocker_T/0/1/0/all/0/1\">Tony St&#xf6;cker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breteler_M/0/1/0/all/0/1\">Monique M.B Breteler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuter_M/0/1/0/all/0/1\">Martin Reuter</a>",
          "description": "The neuroimage analysis community has neglected the automated segmentation of\nthe olfactory bulb (OB) despite its crucial role in olfactory function. The\nlack of an automatic processing method for the OB can be explained by its\nchallenging properties. Nonetheless, recent advances in MRI acquisition\ntechniques and resolution have allowed raters to generate more reliable manual\nannotations. Furthermore, the high accuracy of deep learning methods for\nsolving semantic segmentation problems provides us with an option to reliably\nassess even small structures. In this work, we introduce a novel, fast, and\nfully automated deep learning pipeline to accurately segment OB tissue on\nsub-millimeter T2-weighted (T2w) whole-brain MR images. To this end, we\ndesigned a three-stage pipeline: (1) Localization of a region containing both\nOBs using FastSurferCNN, (2) Segmentation of OB tissue within the localized\nregion through four independent AttFastSurferCNN - a novel deep learning\narchitecture with a self-attention mechanism to improve modeling of contextual\ninformation, and (3) Ensemble of the predicted label maps. The OB pipeline\nexhibits high performance in terms of boundary delineation, OB localization,\nand volume estimation across a wide range of ages in 203 participants of the\nRhineland Study. Moreover, it also generalizes to scans of an independent\ndataset never encountered during training, the Human Connectome Project (HCP),\nwith different acquisition parameters and demographics, evaluated in 30 cases\nat the native 0.7mm HCP resolution, and the default 0.8mm pipeline resolution.\nWe extensively validated our pipeline not only with respect to segmentation\naccuracy but also to known OB volume effects, where it can sensitively\nreplicate age effects.",
          "link": "http://arxiv.org/abs/2108.04267",
          "publishedOn": "2021-08-11T01:55:21.005Z",
          "wordCount": 707,
          "title": "Automated Olfactory Bulb Segmentation on High Resolutional T2-Weighted MRI. (arXiv:2108.04267v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04289",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Krempl/0/1/0/all/0/1\">Krempl</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Georg/0/1/0/all/0/1\">Georg</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kottke/0/1/0/all/0/1\">Kottke</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Daniel/0/1/0/all/0/1\">Daniel</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Minh_P/0/1/0/all/0/1\">Pham Minh</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Tuan/0/1/0/all/0/1\">Tuan</a>",
          "description": "Analysing correlations between streams of events is an important problem. It\narises for example in Neurosciences, when the connectivity of neurons should be\ninferred from spike trains that record neurons' individual spiking activity.\nWhile recently some approaches for inferring delayed synaptic connections have\nbeen proposed, they are limited in the types of connectivities and delays they\nare able to handle, or require computation-intensive procedures. This paper\nproposes a faster and more flexible approach for analysing such delayed\ncorrelated activity: a statistical approach for the Analysis of Connectivity in\nspiking Events (ACE), based on the idea of hypothesis testing. It first\ncomputes for any pair of a source and a target neuron the inter-spike delays\nbetween subsequent source- and target-spikes. Then, it derives a null model for\nthe distribution of inter-spike delays for \\emph{uncorrelated}~neurons.\nFinally, it compares the observed distribution of inter-spike delays to this\nnull model and infers pairwise connectivity based on the Pearson's Chi-squared\ntest statistic. Thus, ACE is capable to detect connections with a priori\nunknown, non-discrete (and potentially large) inter-spike delays, which might\nvary between pairs of neurons. Since ACE works incrementally, it has potential\nfor being used in online processing. In our experiments, we visualise the\nadvantages of ACE in varying experimental scenarios (except for one special\ncase) and in a state-of-the-art dataset which has been generated for\nneuro-scientific research under most realistic conditions.",
          "link": "http://arxiv.org/abs/2108.04289",
          "publishedOn": "2021-08-11T01:55:20.948Z",
          "wordCount": 696,
          "title": "ACE: A Novel Approach for the Statistical Analysis of Pairwise Connectivity. (arXiv:2108.04289v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04240",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Chrysostomou_C/0/1/0/all/0/1\">Charalambos Chrysostomou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Alexandrou_F/0/1/0/all/0/1\">Floris Alexandrou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nicolaou_M/0/1/0/all/0/1\">Mihalis A. Nicolaou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Seker_H/0/1/0/all/0/1\">Huseyin Seker</a>",
          "description": "The Influenza virus can be considered as one of the most severe viruses that\ncan infect multiple species with often fatal consequences to the hosts. The\nHemagglutinin (HA) gene of the virus can be a target for antiviral drug\ndevelopment realised through accurate identification of its sub-types and\npossible the targeted hosts. This paper focuses on accurately predicting if an\nInfluenza type A virus can infect specific hosts, and more specifically, Human,\nAvian and Swine hosts, using only the protein sequence of the HA gene. In more\ndetail, we propose encoding the protein sequences into numerical signals using\nthe Hydrophobicity Index and subsequently utilising a Convolutional Neural\nNetwork-based predictive model. The Influenza HA protein sequences used in the\nproposed work are obtained from the Influenza Research Database (IRD).\nSpecifically, complete and unique HA protein sequences were used for avian,\nhuman and swine hosts. The data obtained for this work was 17999 human-host\nproteins, 17667 avian-host proteins and 9278 swine-host proteins. Given this\nset of collected proteins, the proposed method yields as much as 10% higher\naccuracy for an individual class (namely, Avian) and 5% higher overall accuracy\nthan in an earlier study. It is also observed that the accuracy for each class\nin this work is more balanced than what was presented in this earlier study. As\nthe results show, the proposed model can distinguish HA protein sequences with\nhigh accuracy whenever the virus under investigation can infect Human, Avian or\nSwine hosts.",
          "link": "http://arxiv.org/abs/2108.04240",
          "publishedOn": "2021-08-11T01:55:20.890Z",
          "wordCount": 682,
          "title": "Classification of Influenza Hemagglutinin Protein Sequences using Convolutional Neural Networks. (arXiv:2108.04240v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1\">Gang Mei</a>",
          "description": "Geohazards such as landslides have caused great losses to the safety of\npeople's lives and property, which is often accompanied with surface cracks. If\nsuch surface cracks could be identified in time, it is of great significance\nfor the monitoring and early warning of geohazards. Currently, the most common\nmethod for crack identification is manual detection, which is with low\nefficiency and accuracy. In this paper, a deep transfer learning framework is\nproposed to effectively and efficiently identify slope surface cracks for the\nsake of fast monitoring and early warning of geohazards such as landslides. The\nessential idea is to employ transfer learning by training (a) the large sample\ndataset of concrete cracks and (b) the small sample dataset of soil and rock\nmasses cracks. In the proposed framework, (1) pretrained cracks identification\nmodels are constructed based on the large sample dataset of concrete cracks;\n(2) refined cracks identification models are further constructed based on the\nsmall sample dataset of soil and rock masses cracks. The proposed framework\ncould be applied to conduct UAV surveys on high-steep slopes to realize the\nmonitoring and early warning of landslides to ensure the safety of people's\nlives and property.",
          "link": "http://arxiv.org/abs/2108.04235",
          "publishedOn": "2021-08-11T01:55:20.882Z",
          "wordCount": 632,
          "title": "Deep Transfer Learning for Identifications of Slope Surface Cracks. (arXiv:2108.04235v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tanhim Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chisty_T/0/1/0/all/0/1\">Tanjir Alam Chisty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_A/0/1/0/all/0/1\">Amitabha Chakrabarty</a>",
          "description": "Agriculture is the essential ingredients to mankind which is a major source\nof livelihood. Agriculture work in Bangladesh is mostly done in old ways which\ndirectly affects our economy. In addition, institutions of agriculture are\nworking with manual data which cannot provide a proper solution for crop\nselection and yield prediction. This paper shows the best way of crop selection\nand yield prediction in minimum cost and effort. Artificial Neural Network is\nconsidered robust tools for modeling and prediction. This algorithm aims to get\nbetter output and prediction, as well as, support vector machine, Logistic\nRegression, and random forest algorithm is also considered in this study for\ncomparing the accuracy and error rate. Moreover, all of these algorithms used\nhere are just to see how well they performed for a dataset which is over 0.3\nmillion. We have collected 46 parameters such as maximum and minimum\ntemperature, average rainfall, humidity, climate, weather, and types of land,\ntypes of chemical fertilizer, types of soil, soil structure, soil composition,\nsoil moisture, soil consistency, soil reaction and soil texture for applying\ninto this prediction process. In this paper, we have suggested using the deep\nneural network for agricultural crop selection and yield prediction.",
          "link": "http://arxiv.org/abs/2108.03320",
          "publishedOn": "2021-08-10T02:00:12.259Z",
          "wordCount": 639,
          "title": "A Deep Neural Network Approach for Crop Selection and Yield Prediction in Bangladesh. (arXiv:2108.03320v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongjun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seungjae Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kyungwoo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1\">Wanmo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_I/0/1/0/all/0/1\">Il-Chul Moon</a>",
          "description": "Recent advance in diffusion models incorporates the Stochastic Differential\nEquation (SDE), which brings the state-of-the art performance on image\ngeneration tasks. This paper improves such diffusion models by analyzing the\nmodel at the zero diffusion time. In real datasets, the score function diverges\nas the diffusion time ($t$) decreases to zero, and this observation leads an\nargument that the score estimation fails at $t=0$ with any neural network\nstructure. Subsequently, we introduce Unbounded Diffusion Model (UDM) that\nresolves the score diverging problem with an easily applicable modification to\nany diffusion models. Additionally, we introduce a new SDE that overcomes the\ntheoretic and practical limitations of Variance Exploding SDE. On top of that,\nthe introduced Soft Truncation method improves the sample quality by mitigating\nthe loss scale issue that happens at $t=0$. We further provide a theoretic\nresult of the proposed method to uncover the behind mechanism of the diffusion\nmodels.",
          "link": "http://arxiv.org/abs/2106.05527",
          "publishedOn": "2021-08-10T02:00:12.224Z",
          "wordCount": 622,
          "title": "Score Matching Model for Unbounded Data Score. (arXiv:2106.05527v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Giovannangeli_L/0/1/0/all/0/1\">Loann Giovannangeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalanne_F/0/1/0/all/0/1\">Frederic Lalanne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auber_D/0/1/0/all/0/1\">David Auber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giot_R/0/1/0/all/0/1\">Romain Giot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourqui_R/0/1/0/all/0/1\">Romain Bourqui</a>",
          "description": "By leveraging recent progress of stochastic gradient descent methods, several\nworks have shown that graphs could be efficiently laid out through the\noptimization of a tailored objective function. In the meantime, Deep Learning\n(DL) techniques achieved great performances in many applications. We\ndemonstrate that it is possible to use DL techniques to learn a graph-to-layout\nsequence of operations thanks to a graph-related objective function. In this\npaper, we present a novel graph drawing framework called (DNN)^2: Deep Neural\nNetwork for DrawiNg Networks. Our method uses Graph Convolution Networks to\nlearn a model. Learning is achieved by optimizing a graph topology related loss\nfunction that evaluates (DNN)^2 generated layouts during training. Once\ntrained, the (DNN)^ model is able to quickly lay any input graph out. We\nexperiment (DNN)^2 and statistically compare it to optimization-based and\nregular graph layout algorithms. The results show that (DNN)^2 performs well\nand are encouraging as the Deep Learning approach to Graph Drawing is novel and\nmany leads for future works are identified.",
          "link": "http://arxiv.org/abs/2108.03632",
          "publishedOn": "2021-08-10T02:00:12.211Z",
          "wordCount": 614,
          "title": "Deep Neural Network for DrawiNg Networks, (DNN)^2. (arXiv:2108.03632v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gottwald_G/0/1/0/all/0/1\">Georg A. Gottwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reich_S/0/1/0/all/0/1\">Sebastian Reich</a>",
          "description": "We present a supervised learning method to learn the propagator map of a\ndynamical system from partial and noisy observations. In our computationally\ncheap and easy-to-implement framework a neural network consisting of random\nfeature maps is trained sequentially by incoming observations within a data\nassimilation procedure. By employing Takens' embedding theorem, the network is\ntrained on delay coordinates. We show that the combination of random feature\nmaps and data assimilation, called RAFDA, outperforms standard random feature\nmaps for which the dynamics is learned using batch data.",
          "link": "http://arxiv.org/abs/2108.03561",
          "publishedOn": "2021-08-10T02:00:12.192Z",
          "wordCount": 540,
          "title": "Combining machine learning and data assimilation to forecast dynamical systems from noisy partial observations. (arXiv:2108.03561v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.01662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_L/0/1/0/all/0/1\">Lokesh Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_U/0/1/0/all/0/1\">Utkarsh A. Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castillo_G/0/1/0/all/0/1\">Guillermo A. Castillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hereid_A/0/1/0/all/0/1\">Ayonga Hereid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolathaya_S/0/1/0/all/0/1\">Shishir Kolathaya</a>",
          "description": "In this paper, with a view toward deployment of light-weight control\nframeworks for bipedal walking robots, we realize end-foot trajectories that\nare shaped by a single linear feedback policy. We learn this policy via a\nmodel-free and a gradient-free learning algorithm, Augmented Random Search\n(ARS), in the two robot platforms Rabbit and Digit. Our contributions are\ntwo-fold: a) By using torso and support plane orientation as inputs, we achieve\nrobust walking on slopes of up to 20 degrees in simulation. b) We demonstrate\nadditional behaviors like walking backwards, stepping-in-place, and recovery\nfrom external pushes of up to 120 N. The end result is a robust and a fast\nfeedback control law for bipedal walking on terrains with varying slopes.\nTowards the end, we also provide preliminary results of hardware transfer to\nDigit.",
          "link": "http://arxiv.org/abs/2104.01662",
          "publishedOn": "2021-08-10T02:00:12.186Z",
          "wordCount": 637,
          "title": "Learning Linear Policies for Robust Bipedal Locomotion on Terrains with Varying Slopes. (arXiv:2104.01662v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zang_T/0/1/0/all/0/1\">Tianzi Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanmin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haobing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiadi Yu</a>",
          "description": "Traditional recommendation systems are faced with two long-standing\nobstacles, namely, data sparsity and cold-start problems, which promote the\nemergence and development of Cross-Domain Recommendation (CDR). The core idea\nof CDR is to leverage information collected from other domains to alleviate the\ntwo problems in one domain. Over the last decade, many efforts have been\nengaged for cross-domain recommendation. Recently, with the development of deep\nlearning and neural networks, a large number of methods have emerged. However,\nthere is a limited number of systematic surveys on CDR, especially regarding\nthe latest proposed methods as well as the recommendation scenarios and\nrecommendation tasks they address. In this survey paper, we first proposed a\ntwo-level taxonomy of cross-domain recommendation which classifies different\nrecommendation scenarios and recommendation tasks. We then introduce and\nsummarize existing cross-domain recommendation approaches under different\nrecommendation scenarios in a structured manner. We also organize datasets\ncommonly used. We conclude this survey by providing several potential research\ndirections about this field.",
          "link": "http://arxiv.org/abs/2108.03357",
          "publishedOn": "2021-08-10T02:00:12.178Z",
          "wordCount": 600,
          "title": "A Survey on Cross-domain Recommendation: Taxonomies, Methods, and Future Directions. (arXiv:2108.03357v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Saikat Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samavatian_M/0/1/0/all/0/1\">Mohammad Hossein Samavatian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barber_K/0/1/0/all/0/1\">Kristin Barber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_R/0/1/0/all/0/1\">Radu Teodorescu</a>",
          "description": "Deep neural network (DNN) classifiers are powerful tools that drive a broad\nspectrum of important applications, from image recognition to autonomous\nvehicles. Unfortunately, DNNs are known to be vulnerable to adversarial attacks\nthat affect virtually all state-of-the-art models. These attacks make small\nimperceptible modifications to inputs that are sufficient to induce the DNNs to\nproduce the wrong classification.\n\nIn this paper we propose a novel, lightweight adversarial correction and/or\ndetection mechanism for image classifiers that relies on undervolting (running\na chip at a voltage that is slightly below its safe margin). We propose using\ncontrolled undervolting of the chip running the inference process in order to\nintroduce a limited number of compute errors. We show that these errors disrupt\nthe adversarial input in a way that can be used either to correct the\nclassification or detect the input as adversarial. We evaluate the proposed\nsolution in an FPGA design and through software simulation. We evaluate 10\nattacks and show average detection rates of 77% and 90% on two popular DNNs.",
          "link": "http://arxiv.org/abs/2107.09804",
          "publishedOn": "2021-08-10T02:00:12.171Z",
          "wordCount": 644,
          "title": "Using Undervolting as an On-Device Defense Against Adversarial Machine Learning Attacks. (arXiv:2107.09804v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03576",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Heydari_M/0/1/0/all/0/1\">Mojtaba Heydari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cwitkowitz_F/0/1/0/all/0/1\">Frank Cwitkowitz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_Z/0/1/0/all/0/1\">Zhiyao Duan</a>",
          "description": "The online estimation of rhythmic information, such as beat positions,\ndownbeat positions, and meter, is critical for many real-time music\napplications. Musical rhythm comprises complex hierarchical relationships\nacross time, rendering its analysis intrinsically challenging and at times\nsubjective. Furthermore, systems which attempt to estimate rhythmic information\nin real-time must be causal and must produce estimates quickly and efficiently.\nIn this work, we introduce an online system for joint beat, downbeat, and meter\ntracking, which utilizes causal convolutional and recurrent layers, followed by\na pair of sequential Monte Carlo particle filters applied during inference. The\nproposed system does not need to be primed with a time signature in order to\nperform downbeat tracking, and is instead able to estimate meter and adjust the\npredictions over time. Additionally, we propose an information gate strategy to\nsignificantly decrease the computational cost of particle filtering during the\ninference step, making the system much faster than previous sampling-based\nmethods. Experiments on the GTZAN dataset, which is unseen during training,\nshow that the system outperforms various online beat and downbeat tracking\nsystems and achieves comparable performance to a baseline offline joint method.",
          "link": "http://arxiv.org/abs/2108.03576",
          "publishedOn": "2021-08-10T02:00:12.164Z",
          "wordCount": 672,
          "title": "BeatNet: CRNN and Particle Filtering for Online Joint Beat Downbeat and Meter Tracking. (arXiv:2108.03576v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03273",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zenisek_J/0/1/0/all/0/1\">Jan Zenisek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1\">Gabriel Kronberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfartsberger_J/0/1/0/all/0/1\">Josef Wolfartsberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wild_N/0/1/0/all/0/1\">Norbert Wild</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Affenzeller_M/0/1/0/all/0/1\">Michael Affenzeller</a>",
          "description": "The current development of today's production industry towards seamless\nsensor-based monitoring is paving the way for concepts such as Predictive\nMaintenance. By this means, the condition of plants and products in future\nproduction lines will be continuously analyzed with the objective to predict\nany kind of breakdown and trigger preventing actions proactively. Such\nambitious predictions are commonly performed with support of machine learning\nalgorithms. In this work, we utilize these algorithms to model complex systems,\nsuch as production plants, by focusing on their variable interactions. The core\nof this contribution is a sliding window based algorithm, designed to detect\nchanges of the identified interactions, which might indicate beginning\nmalfunctions in the context of a monitored production plant. Besides a detailed\ndescription of the algorithm, we present results from experiments with a\nsynthetic dynamical system, simulating stable and drifting system behavior.",
          "link": "http://arxiv.org/abs/2108.03273",
          "publishedOn": "2021-08-10T02:00:12.130Z",
          "wordCount": 629,
          "title": "Concept Drift Detection with Variable Interaction Networks. (arXiv:2108.03273v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kayo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1\">Danish Pruthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Aditi Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "Context-aware machine translation models are designed to leverage contextual\ninformation, but often fail to do so. As a result, they inaccurately\ndisambiguate pronouns and polysemous words that require context for resolution.\nIn this paper, we ask several questions: What contexts do human translators use\nto resolve ambiguous words? Are models paying large amounts of attention to the\nsame context? What if we explicitly train them to do so? To answer these\nquestions, we introduce SCAT (Supporting Context for Ambiguous Translations), a\nnew English-French dataset comprising supporting context words for 14K\ntranslations that professional translators found useful for pronoun\ndisambiguation. Using SCAT, we perform an in-depth analysis of the context used\nto disambiguate, examining positional and lexical characteristics of the\nsupporting words. Furthermore, we measure the degree of alignment between the\nmodel's attention scores and the supporting context from SCAT, and apply a\nguided attention strategy to encourage agreement between the two.",
          "link": "http://arxiv.org/abs/2105.06977",
          "publishedOn": "2021-08-10T02:00:12.123Z",
          "wordCount": 643,
          "title": "Do Context-Aware Translation Models Pay the Right Attention?. (arXiv:2105.06977v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03465",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Ali_S/0/1/0/all/0/1\">Sarwan Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sahoo_B/0/1/0/all/0/1\">Bikram Sahoo</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ullah_N/0/1/0/all/0/1\">Naimat Ullah</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zelikovskiy_A/0/1/0/all/0/1\">Alexander Zelikovskiy</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Patterson_M/0/1/0/all/0/1\">Murray Patterson</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khan_I/0/1/0/all/0/1\">Imdadullah Khan</a>",
          "description": "With the rapid spread of the novel coronavirus (COVID-19) across the globe\nand its continuous mutation, it is of pivotal importance to design a system to\nidentify different known (and unknown) variants of SARS-CoV-2. Identifying\nparticular variants helps to understand and model their spread patterns, design\neffective mitigation strategies, and prevent future outbreaks. It also plays a\ncrucial role in studying the efficacy of known vaccines against each variant\nand modeling the likelihood of breakthrough infections. It is well known that\nthe spike protein contains most of the information/variation pertaining to\ncoronavirus variants.\n\nIn this paper, we use spike sequences to classify different variants of the\ncoronavirus in humans. We show that preserving the order of the amino acids\nhelps the underlying classifiers to achieve better performance. We also show\nthat we can train our model to outperform the baseline algorithms using only a\nsmall number of training samples ($1\\%$ of the data). Finally, we show the\nimportance of the different amino acids which play a key role in identifying\nvariants and how they coincide with those reported by the USA's Centers for\nDisease Control and Prevention (CDC).",
          "link": "http://arxiv.org/abs/2108.03465",
          "publishedOn": "2021-08-10T02:00:12.097Z",
          "wordCount": 671,
          "title": "A k-mer Based Approach for SARS-CoV-2 Variant Identification. (arXiv:2108.03465v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03336",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_F/0/1/0/all/0/1\">Fan Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Roch_S/0/1/0/all/0/1\">Sebastien Roch</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rohe_K/0/1/0/all/0/1\">Karl Rohe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yu_S/0/1/0/all/0/1\">Shuqi Yu</a>",
          "description": "In applied multivariate statistics, estimating the number of latent\ndimensions or the number of clusters is a fundamental and recurring problem.\nOne common diagnostic is the scree plot, which shows the largest eigenvalues of\nthe data matrix; the user searches for a \"gap\" or \"elbow\" in the decreasing\neigenvalues; unfortunately, these patterns can hide beneath the bias of the\nsample eigenvalues. This methodological problem is conceptually difficult\nbecause, in many situations, there is only enough signal to detect a subset of\nthe $k$ population dimensions/eigenvectors. In this situation, one could argue\nthat the correct choice of $k$ is the number of detectable dimensions. We\nalleviate these problems with cross-validated eigenvalues. Under a large class\nof random graph models, without any parametric assumptions, we provide a\np-value for each sample eigenvector. It tests the null hypothesis that this\nsample eigenvector is orthogonal to (i.e., uncorrelated with) the true latent\ndimensions. This approach naturally adapts to problems where some dimensions\nare not statistically detectable. In scenarios where all $k$ dimensions can be\nestimated, we prove that our procedure consistently estimates $k$. In\nsimulations and a data example, the proposed estimator compares favorably to\nalternative approaches in both computational and statistical performance.",
          "link": "http://arxiv.org/abs/2108.03336",
          "publishedOn": "2021-08-10T02:00:12.075Z",
          "wordCount": 643,
          "title": "Estimating Graph Dimension with Cross-validated Eigenvalues. (arXiv:2108.03336v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Rongrong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_N/0/1/0/all/0/1\">Na Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "We present a novel approach to joint depth and normal estimation for\ntime-of-flight (ToF) sensors. Our model learns to predict the high-quality\ndepth and normal maps jointly from ToF raw sensor data. To achieve this, we\nmeticulously constructed the first large-scale dataset (named ToF-100) with\npaired raw ToF data and ground-truth high-resolution depth maps provided by an\nindustrial depth camera. In addition, we also design a simple but effective\nframework for joint depth and normal estimation, applying a robust Chamfer loss\nvia jittering to improve the performance of our model. Our experiments\ndemonstrate that our proposed method can efficiently reconstruct\nhigh-resolution depth and normal maps and significantly outperforms\nstate-of-the-art approaches. Our code and data will be available at\n\\url{https://github.com/hkustVisionRr/JointlyDepthNormalEstimation}",
          "link": "http://arxiv.org/abs/2108.03649",
          "publishedOn": "2021-08-10T02:00:12.067Z",
          "wordCount": 573,
          "title": "Joint Depth and Normal Estimation from Real-world Time-of-flight Raw Data. (arXiv:2108.03649v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bian_Z/0/1/0/all/0/1\">Zhengda Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shenggui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>",
          "description": "Efficient GPU resource scheduling is essential to maximize resource\nutilization and save training costs for the increasing amount of deep learning\nworkloads in shared GPU clusters. Existing GPU schedulers largely rely on\nstatic policies to leverage the performance characteristics of deep learning\njobs. However, they can hardly reach optimal efficiency due to the lack of\nelasticity. To address the problem, we propose ONES, an ONline Evolutionary\nScheduler for elastic batch size orchestration. ONES automatically manages the\nelasticity of each job based on the training batch size, so as to maximize GPU\nutilization and improve scheduling efficiency. It determines the batch size for\neach job through an online evolutionary search that can continuously optimize\nthe scheduling decisions. We evaluate the effectiveness of ONES with 64 GPUs on\nTACC's Longhorn supercomputers. The results show that ONES can outperform the\nprior deep learning schedulers with a significantly shorter average job\ncompletion time.",
          "link": "http://arxiv.org/abs/2108.03645",
          "publishedOn": "2021-08-10T02:00:12.058Z",
          "wordCount": 628,
          "title": "Online Evolutionary Batch Size Orchestration for Scheduling Deep Learning Workloads in GPU Clusters. (arXiv:2108.03645v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2012.02346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kimura_T/0/1/0/all/0/1\">Takumi Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_T/0/1/0/all/0/1\">Takashi Matsubara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uehara_K/0/1/0/all/0/1\">Kuniaki Uehara</a>",
          "description": "A point cloud serves as a representation of the surface of a\nthree-dimensional (3D) shape. Deep generative models have been adapted to model\ntheir variations typically using a map from a ball-like set of latent\nvariables. However, previous approaches did not pay much attention to the\ntopological structure of a point cloud, despite that a continuous map cannot\nexpress the varying numbers of holes and intersections. Moreover, a point cloud\nis often composed of multiple subparts, and it is also difficult to express. In\nthis study, we propose ChartPointFlow, a flow-based generative model with\nmultiple latent labels for 3D point clouds. Each label is assigned to points in\nan unsupervised manner. Then, a map conditioned on a label is assigned to a\ncontinuous subset of a point cloud, similar to a chart of a manifold. This\nenables our proposed model to preserve the topological structure with clear\nboundaries, whereas previous approaches tend to generate blurry point clouds\nand fail to generate holes. The experimental results demonstrate that\nChartPointFlow achieves state-of-the-art performance in terms of generation and\nreconstruction compared with other point cloud generators. Moreover,\nChartPointFlow divides an object into semantic subparts using charts, and it\ndemonstrates superior performance in case of unsupervised segmentation.",
          "link": "http://arxiv.org/abs/2012.02346",
          "publishedOn": "2021-08-10T02:00:12.030Z",
          "wordCount": 692,
          "title": "ChartPointFlow for Topology-Aware 3D Point Cloud Generation. (arXiv:2012.02346v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lingelbach_M/0/1/0/all/0/1\">Michael Lingelbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1\">Kent Vainio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokmen_C/0/1/0/all/0/1\">Cem Gokmen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharan_G/0/1/0/all/0/1\">Gokul Dharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_T/0/1/0/all/0/1\">Tanish Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurenkov_A/0/1/0/all/0/1\">Andrey Kurenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gweon_H/0/1/0/all/0/1\">Hyowon Gweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>",
          "description": "Recent research in embodied AI has been boosted by the use of simulation\nenvironments to develop and train robot learning approaches. However, the use\nof simulation has skewed the attention to tasks that only require what robotics\nsimulators can simulate: motion and physical contact. We present iGibson 2.0,\nan open-source simulation environment that supports the simulation of a more\ndiverse set of household tasks through three key innovations. First, iGibson\n2.0 supports object states, including temperature, wetness level, cleanliness\nlevel, and toggled and sliced states, necessary to cover a wider range of\ntasks. Second, iGibson 2.0 implements a set of predicate logic functions that\nmap the simulator states to logic states like Cooked or Soaked. Additionally,\ngiven a logic state, iGibson 2.0 can sample valid physical states that satisfy\nit. This functionality can generate potentially infinite instances of tasks\nwith minimal effort from the users. The sampling mechanism allows our scenes to\nbe more densely populated with small objects in semantically meaningful\nlocations. Third, iGibson 2.0 includes a virtual reality (VR) interface to\nimmerse humans in its scenes to collect demonstrations. As a result, we can\ncollect demonstrations from humans on these new types of tasks, and use them\nfor imitation learning. We evaluate the new capabilities of iGibson 2.0 to\nenable robot learning of novel tasks, in the hope of demonstrating the\npotential of this new simulator to support new research in embodied AI. iGibson\n2.0 and its new dataset will be publicly available at\nthis http URL",
          "link": "http://arxiv.org/abs/2108.03272",
          "publishedOn": "2021-08-10T02:00:12.020Z",
          "wordCount": 731,
          "title": "IGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks. (arXiv:2108.03272v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2002.07873",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Winn_E/0/1/0/all/0/1\">Emily T. Winn</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Vazquez_M/0/1/0/all/0/1\">Marilyn Vazquez</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Loliencar_P/0/1/0/all/0/1\">Prachi Loliencar</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Taipale_K/0/1/0/all/0/1\">Kaisa Taipale</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Heo_G/0/1/0/all/0/1\">Giseon Heo</a>",
          "description": "Pediatric obstructive sleep apnea affects an estimated 1-5% of\nelementary-school aged children and can lead to other detrimental health\nproblems. Swift diagnosis and treatment are critical to a child's growth and\ndevelopment, but the variability of symptoms and the complexity of the\navailable data make this a challenge. We take a first step in streamlining the\nprocess by focusing on inexpensive data from questionnaires and craniofacial\nmeasurements. We apply correlation networks, the Mapper algorithm from\ntopological data analysis, and singular value decomposition in a process of\nexploratory data analysis. We then apply a variety of supervised and\nunsupervised learning techniques from statistics, machine learning, and\ntopology, ranging from support vector machines to Bayesian classifiers and\nmanifold learning. Finally, we analyze the results of each of these methods and\ndiscuss the implications for a multi-data-sourced algorithm moving forward.",
          "link": "http://arxiv.org/abs/2002.07873",
          "publishedOn": "2021-08-10T02:00:12.012Z",
          "wordCount": 637,
          "title": "A survey of statistical learning techniques as applied to inexpensive pediatric Obstructive Sleep Apnea data. (arXiv:2002.07873v3 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03498",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhan_X/0/1/0/all/0/1\">Xianghao Zhan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1\">Yiheng Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1\">Yuzhe Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cecchi_N/0/1/0/all/0/1\">Nicholas J. Cecchi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gevaert_O/0/1/0/all/0/1\">Olivier Gevaert</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zeineh_M/0/1/0/all/0/1\">Michael M. Zeineh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Grant_G/0/1/0/all/0/1\">Gerald A. Grant</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Camarillo_D/0/1/0/all/0/1\">David B. Camarillo</a>",
          "description": "Traumatic brain injury can be caused by various types of head impacts.\nHowever, due to different kinematic characteristics, many brain injury risk\nestimation models are not generalizable across the variety of impacts that\nhumans may sustain. The current definitions of head impact subtypes are based\non impact sources (e.g., football, traffic accident), which may not reflect the\nintrinsic kinematic similarities of impacts across the impact sources. To\ninvestigate the potential new definitions of impact subtypes based on\nkinematics, 3,161 head impacts from various sources including simulation,\ncollege football, mixed martial arts, and car racing were collected. We applied\nthe K-means clustering to cluster the impacts on 16 standardized temporal\nfeatures from head rotation kinematics. Then, we developed subtype-specific\nridge regression models for cumulative strain damage (using the threshold of\n15%), which significantly improved the estimation accuracy compared with the\nbaseline method which mixed impacts from different sources and developed one\nmodel (R^2 from 0.7 to 0.9). To investigate the effect of kinematic features,\nwe presented the top three critical features (maximum resultant angular\nacceleration, maximum angular acceleration along the z-axis, maximum linear\nacceleration along the y-axis) based on regression accuracy and used logistic\nregression to find the critical points for each feature that partitioned the\nsubtypes. This study enables researchers to define head impact subtypes in a\ndata-driven manner, which leads to more generalizable brain injury risk\nestimation.",
          "link": "http://arxiv.org/abs/2108.03498",
          "publishedOn": "2021-08-10T02:00:12.004Z",
          "wordCount": 691,
          "title": "Kinematics clustering enables head impact subtyping for better traumatic brain injury prediction. (arXiv:2108.03498v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03489",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_C/0/1/0/all/0/1\">Cristina Vasconcelos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1\">Hugo Larochelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1\">Vincent Dumoulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romijnders_R/0/1/0/all/0/1\">Rob Romijnders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1\">Nicolas Le Roux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goroshin_R/0/1/0/all/0/1\">Ross Goroshin</a>",
          "description": "We investigate the impact of aliasing on generalization in Deep Convolutional\nNetworks and show that data augmentation schemes alone are unable to prevent it\ndue to structural limitations in widely used architectures. Drawing insights\nfrom frequency analysis theory, we take a closer look at ResNet and\nEfficientNet architectures and review the trade-off between aliasing and\ninformation loss in each of their major components. We show how to mitigate\naliasing by inserting non-trainable low-pass filters at key locations,\nparticularly where networks lack the capacity to learn them. These simple\narchitectural changes lead to substantial improvements in generalization on\ni.i.d. and even more on out-of-distribution conditions, such as image\nclassification under natural corruptions on ImageNet-C [11] and few-shot\nlearning on Meta-Dataset [26]. State-of-the art results are achieved on both\ndatasets without introducing additional trainable parameters and using the\ndefault hyper-parameters of open source codebases.",
          "link": "http://arxiv.org/abs/2108.03489",
          "publishedOn": "2021-08-10T02:00:11.986Z",
          "wordCount": 594,
          "title": "Impact of Aliasing on Generalization in Deep Convolutional Networks. (arXiv:2108.03489v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stripelis_D/0/1/0/all/0/1\">Dimitris Stripelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleem_H/0/1/0/all/0/1\">Hamza Saleem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghai_T/0/1/0/all/0/1\">Tanmay Ghai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhinagar_N/0/1/0/all/0/1\">Nikhil Dhinagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_U/0/1/0/all/0/1\">Umang Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasiou_C/0/1/0/all/0/1\">Chrysovalantis Anastasiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1\">Greg Ver Steeg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Srivatsan Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naveed_M/0/1/0/all/0/1\">Muhammad Naveed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_P/0/1/0/all/0/1\">Paul M. Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambite_J/0/1/0/all/0/1\">Jose Luis Ambite</a>",
          "description": "Federated learning (FL) enables distributed computation of machine learning\nmodels over various disparate, remote data sources, without requiring to\ntransfer any individual data to a centralized location. This results in an\nimproved generalizability of models and efficient scaling of computation as\nmore sources and larger datasets are added to the federation. Nevertheless,\nrecent membership attacks show that private or sensitive personal data can\nsometimes be leaked or inferred when model parameters or summary statistics are\nshared with a central site, requiring improved security solutions. In this\nwork, we propose a framework for secure FL using fully-homomorphic encryption\n(FHE). Specifically, we use the CKKS construction, an approximate, floating\npoint compatible scheme that benefits from ciphertext packing and rescaling. In\nour evaluation on large-scale brain MRI datasets, we use our proposed secure FL\nframework to train a deep learning model to predict a person's age from\ndistributed MRI scans, a common benchmarking task, and demonstrate that there\nis no degradation in the learning performance between the encrypted and\nnon-encrypted federated models.",
          "link": "http://arxiv.org/abs/2108.03437",
          "publishedOn": "2021-08-10T02:00:11.980Z",
          "wordCount": 623,
          "title": "Secure Neuroimaging Analysis using Federated Learning with Homomorphic Encryption. (arXiv:2108.03437v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2011.14694",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Ali_O/0/1/0/all/0/1\">Omair Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Saif_ur_Rehman_M/0/1/0/all/0/1\">Muhammad Saif-ur-Rehman</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Dyck_S/0/1/0/all/0/1\">Susanne Dyck</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Glasmachers_T/0/1/0/all/0/1\">Tobias Glasmachers</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Iossifidis_I/0/1/0/all/0/1\">Ioannis Iossifidis</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Klaes_C/0/1/0/all/0/1\">Christian Klaes</a>",
          "description": "Brain-computer interfaces (BCIs) enable communication between humans and\nmachines by translating brain activity into control commands.\nElectroencephalography (EEG) signals are one of the most used brain signals in\nnon-invasive BCI applications but are often contaminated with noise. Therefore,\nit is possible that meaningful patterns for classifying EEG signals are deeply\nhidden. State-of-the-art deep-learning algorithms are successful in learning\nhidden, meaningful patterns. However, the quality and the quantity of the\npresented inputs is pivotal. Here, we propose a novel feature extraction method\ncalled anchored Short Time Fourier Transform (anchored-STFT), which is an\nadvanced version of STFT, as it minimizes the trade-off between temporal and\nspectral resolution presented by STFT. In addition, we propose a novel\naugmentation method, called gradient norm adversarial augmentation (GNAA). GNAA\nis not only an augmentation method but is also used to harness adversarial\ninputs in EEG data, which not only improves the classification accuracy but\nalso enhances the robustness of the classifier. In addition, we also propose a\nnew CNN architecture, namely Skip-Net, for the classification of EEG signals.\nThe proposed pipeline outperforms all state-of-the-art methods and yields an\naverage classification accuracy of 90.7 % and 89.54 % on BCI competition II\ndataset III and BCI competition IV dataset 2b, respectively.",
          "link": "http://arxiv.org/abs/2011.14694",
          "publishedOn": "2021-08-10T02:00:11.972Z",
          "wordCount": 709,
          "title": "Anchored-STFT and GNAA: An extension of STFT in conjunction with an adversarial data augmentation technique for the decoding of neural signals. (arXiv:2011.14694v4 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mandlekar_A/0/1/0/all/0/1\">Ajay Mandlekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Danfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Josiah Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasiriany_S/0/1/0/all/0/1\">Soroush Nasiriany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_R/0/1/0/all/0/1\">Rohun Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>",
          "description": "Imitating human demonstrations is a promising approach to endow robots with\nvarious manipulation capabilities. While recent advances have been made in\nimitation learning and batch (offline) reinforcement learning, a lack of\nopen-source human datasets and reproducible learning methods make assessing the\nstate of the field difficult. In this paper, we conduct an extensive study of\nsix offline learning algorithms for robot manipulation on five simulated and\nthree real-world multi-stage manipulation tasks of varying complexity, and with\ndatasets of varying quality. Our study analyzes the most critical challenges\nwhen learning from offline human data for manipulation. Based on the study, we\nderive a series of lessons including the sensitivity to different algorithmic\ndesign choices, the dependence on the quality of the demonstrations, and the\nvariability based on the stopping criteria due to the different objectives in\ntraining and evaluation. We also highlight opportunities for learning from\nhuman datasets, such as the ability to learn proficient policies on\nchallenging, multi-stage tasks beyond the scope of current reinforcement\nlearning methods, and the ability to easily scale to natural, real-world\nmanipulation scenarios where only raw sensory signals are available. We have\nopen-sourced our datasets and all algorithm implementations to facilitate\nfuture research and fair comparisons in learning from human demonstration data.\nCodebase, datasets, trained models, and more available at\nhttps://arise-initiative.github.io/robomimic-web/",
          "link": "http://arxiv.org/abs/2108.03298",
          "publishedOn": "2021-08-10T02:00:11.964Z",
          "wordCount": 672,
          "title": "What Matters in Learning from Offline Human Demonstrations for Robot Manipulation. (arXiv:2108.03298v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Matej%5Cr%7Bu%7D_L/0/1/0/all/0/1\">Luk&#xe1;&#x161; Mat&#x11b;j&#x16f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griol_D/0/1/0/all/0/1\">David Griol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callejas_Z/0/1/0/all/0/1\">Zoraida Callejas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molina_J/0/1/0/all/0/1\">Jos&#xe9; Manuel Molina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchis_A/0/1/0/all/0/1\">Araceli Sanchis</a>",
          "description": "Deep learning is providing very positive results in areas related to\nconversational interfaces, such as speech recognition, but its potential\nbenefit for dialog management has still not been fully studied. In this paper,\nwe perform an assessment of different configurations for deep-learned dialog\nmanagement with three dialog corpora from different application domains and\nvarying in size, dimensionality and possible system responses. Our results have\nallowed us to identify several aspects that can have an impact on accuracy,\nincluding the approaches used for feature extraction, input representation,\ncontext consideration and the hyper-parameters of the deep neural networks\nemployed.",
          "link": "http://arxiv.org/abs/2108.03478",
          "publishedOn": "2021-08-10T02:00:11.956Z",
          "wordCount": 574,
          "title": "An empirical assessment of deep learning approaches to task-oriented dialog management. (arXiv:2108.03478v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.10684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taille_B/0/1/0/all/0/1\">Bruno Taill&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guigue_V/0/1/0/all/0/1\">Vincent Guigue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scoutheeten_G/0/1/0/all/0/1\">Geoffrey Scoutheeten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>",
          "description": "Despite efforts to distinguish three different evaluation setups (Bekoulis et\nal., 2018), numerous end-to-end Relation Extraction (RE) articles present\nunreliable performance comparison to previous work. In this paper, we first\nidentify several patterns of invalid comparisons in published papers and\ndescribe them to avoid their propagation. We then propose a small empirical\nstudy to quantify the impact of the most common mistake and evaluate it leads\nto overestimating the final RE performance by around 5% on ACE05. We also seize\nthis opportunity to study the unexplored ablations of two recent developments:\nthe use of language model pretraining (specifically BERT) and span-level NER.\nThis meta-analysis emphasizes the need for rigor in the report of both the\nevaluation setting and the datasets statistics and we call for unifying the\nevaluation setting in end-to-end RE.",
          "link": "http://arxiv.org/abs/2009.10684",
          "publishedOn": "2021-08-10T02:00:11.949Z",
          "wordCount": 613,
          "title": "Let's Stop Incorrect Comparisons in End-to-end Relation Extraction!. (arXiv:2009.10684v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anuraganand Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Prabhat Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohitash Chandra</a>",
          "description": "Class imbalance in a dataset is a major problem for classifiers that results\nin poor prediction with a high true positive rate (TPR) but a low true negative\nrate (TNR) for a majority positive training dataset. Generally, the\npre-processing technique of oversampling of minority class(es) are used to\novercome this deficiency. Our focus is on using the hybridization of Generative\nAdversarial Network (GAN) and Synthetic Minority Over-Sampling Technique\n(SMOTE) to address class imbalanced problems. We propose a novel two-phase\noversampling approach that has the synergy of SMOTE and GAN. The initial data\nof minority class(es) generated by SMOTE is further enhanced by GAN that\nproduces better quality samples. We named it SMOTified-GAN as GAN works on\npre-sampled minority data produced by SMOTE rather than randomly generating the\nsamples itself. The experimental results prove the sample quality of minority\nclass(es) has been improved in a variety of tested benchmark datasets. Its\nperformance is improved by up to 9\\% from the next best algorithm tested on\nF1-score measurements. Its time complexity is also reasonable which is around\n$O(N^2d^2T)$ for a sequential algorithm.",
          "link": "http://arxiv.org/abs/2108.03235",
          "publishedOn": "2021-08-10T02:00:11.931Z",
          "wordCount": 610,
          "title": "SMOTified-GAN for class imbalanced pattern classification problems. (arXiv:2108.03235v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03712",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Haseli_M/0/1/0/all/0/1\">Masih Haseli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cortes_J/0/1/0/all/0/1\">Jorge Cort&#xe9;s</a>",
          "description": "This paper tackles the data-driven approximation of unknown dynamical systems\nusing Koopman-operator methods. Given a dictionary of functions, these methods\napproximate the projection of the action of the operator on the\nfinite-dimensional subspace spanned by the dictionary. We propose the Tunable\nSymmetric Subspace Decomposition algorithm to refine the dictionary, balancing\nits expressiveness and accuracy. Expressiveness corresponds to the ability of\nthe dictionary to describe the evolution of as many observables as possible and\naccuracy corresponds to the ability to correctly predict their evolution. Based\non the observation that Koopman-invariant subspaces give rise to exact\npredictions, we reason that prediction accuracy is a function of the degree of\ninvariance of the subspace generated by the dictionary and provide a\ndata-driven measure to measure invariance proximity. The proposed algorithm\niteratively prunes the initial functional space to identify a refined\ndictionary of functions that satisfies the desired level of accuracy while\nretaining as much of the original expressiveness as possible. We provide a full\ncharacterization of the algorithm properties and show that it generalizes both\nExtended Dynamic Mode Decomposition and Symmetric Subspace Decomposition.\nSimulations on planar systems show the effectiveness of the proposed methods in\nproducing Koopman approximations of tunable accuracy that capture relevant\ninformation about the dynamical system.",
          "link": "http://arxiv.org/abs/2108.03712",
          "publishedOn": "2021-08-10T02:00:11.923Z",
          "wordCount": 659,
          "title": "Generalizing Dynamic Mode Decomposition: Balancing Accuracy and Expressiveness in Koopman Approximations. (arXiv:2108.03712v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dube_S/0/1/0/all/0/1\">Simant Dube</a>",
          "description": "The goals of this paper are two-fold. The first goal is to serve as an\nexpository tutorial on the working of deep learning models which emphasizes\ngeometrical intuition about the reasons for success of deep learning. The\nsecond goal is to complement the current results on the expressive power of\ndeep learning models and their loss surfaces with novel insights and results.\nIn particular, we describe how deep neural networks carve out manifolds\nespecially when the multiplication neurons are introduced. Multiplication is\nused in dot products and the attention mechanism and it is employed in capsule\nnetworks and self-attention based transformers. We also describe how random\npolynomial, random matrix, spin glass and computational complexity perspectives\non the loss surfaces are interconnected.",
          "link": "http://arxiv.org/abs/2108.03579",
          "publishedOn": "2021-08-10T02:00:11.916Z",
          "wordCount": 567,
          "title": "Expressive Power and Loss Surfaces of Deep Learning Models. (arXiv:2108.03579v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03442",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hofmeyr_D/0/1/0/all/0/1\">David P. Hofmeyr</a>",
          "description": "An efficient method for obtaining low-density hyperplane separators in the\nunsupervised context is proposed. Low density separators can be used to obtain\na partition of a set of data based on their allocations to the different sides\nof the separators. The proposed method is based on applying stochastic gradient\ndescent to the integrated density on the hyperplane with respect to a\nconvolution of the underlying distribution and a smoothing kernel. In the case\nwhere the bandwidth of the smoothing kernel is decreased towards zero, the bias\nof these updates with respect to the true underlying density tends to zero, and\nconvergence to a minimiser of the density on the hyperplane can be obtained. A\npost-processing of the partition induced by a collection of low-density\nhyperplanes yields an efficient and accurate clustering method which is capable\nof automatically selecting an appropriate number of clusters. Experiments with\nthe proposed approach show that it is highly competitive in terms of both speed\nand accuracy when compared with relevant benchmarks. Code to implement the\nproposed approach is available in the form of an R package from\nhttps://github.com/DavidHofmeyr/iMDH.",
          "link": "http://arxiv.org/abs/2108.03442",
          "publishedOn": "2021-08-10T02:00:11.910Z",
          "wordCount": 620,
          "title": "Clustering Large Data Sets with Incremental Estimation of Low-density Separating Hyperplanes. (arXiv:2108.03442v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tongzhou Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1\">Fanbo Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Derek Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Stone Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhiwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>",
          "description": "Learning generalizable manipulation skills is central for robots to achieve\ntask automation in environments with endless scene and object variations.\nHowever, existing robot learning environments are limited in both scale and\ndiversity of 3D assets (especially of articulated objects), making it difficult\nto train and evaluate the generalization ability of agents over novel objects.\nIn this work, we focus on object-level generalization and propose SAPIEN\nManipulation Skill Benchmark (abbreviated as ManiSkill), a large-scale\nlearning-from-demonstrations benchmark for articulated object manipulation with\n3D visual input (point cloud and RGB-D image). ManiSkill supports object-level\nvariations by utilizing a rich and diverse set of articulated objects, and each\ntask is carefully designed for learning manipulations on a single category of\nobjects. We equip ManiSkill with a large number of high-quality demonstrations\nto facilitate learning-from-demonstrations approaches and perform evaluations\non baseline algorithms. We believe that ManiSkill can encourage the robot\nlearning community to explore more on learning generalizable object\nmanipulation skills.",
          "link": "http://arxiv.org/abs/2107.14483",
          "publishedOn": "2021-08-10T02:00:11.904Z",
          "wordCount": null,
          "title": "ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills. (arXiv:2107.14483v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shibu_A/0/1/0/all/0/1\">Aebel Joe Shibu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_S/0/1/0/all/0/1\">Sadhana S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+N_S/0/1/0/all/0/1\">Shilpa N</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>",
          "description": "Digital hardware is verified by comparing its behavior against a reference\nmodel on a range of randomly generated input signals. The random generation of\nthe inputs hopes to achieve sufficient coverage of the different parts of the\ndesign. However, such coverage is often difficult to achieve, amounting to\nlarge verification efforts and delays. An alternative is to use Reinforcement\nLearning (RL) to generate the inputs by learning to prioritize those inputs\nwhich can more efficiently explore the design under test. In this work, we\npresent VeRLPy an open-source library to allow RL-driven verification with\nlimited additional engineering overhead. This contributes to two broad\nmovements within the EDA community of (a) moving to open-source toolchains and\n(b) reducing barriers for development with Python support. We also demonstrate\nthe use of VeRLPy for a few designs and establish its value over randomly\ngenerated input signals.",
          "link": "http://arxiv.org/abs/2108.03978",
          "publishedOn": "2021-08-10T02:00:11.903Z",
          "wordCount": null,
          "title": "VeRLPy: Python Library for Verification of Digital Designs with Reinforcement Learning. (arXiv:2108.03978v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.00591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Assine_J/0/1/0/all/0/1\">Juliano S. Assine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filho_J/0/1/0/all/0/1\">J. C. S. Santos Filho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valle_E/0/1/0/all/0/1\">Eduardo Valle</a>",
          "description": "In the past few years, mobile deep-learning deployment progressed by leaps\nand bounds, but solutions still struggle to accommodate its severe and\nfluctuating operational restrictions, which include bandwidth, latency,\ncomputation, and energy. In this work, we help to bridge that gap, introducing\nthe first configurable solution for object detection that manages the triple\ncommunication-computation-accuracy trade-off with a single set of weights. Our\nsolution shows state-of-the-art results on COCO-2017, adding only a minor\npenalty on the base EfficientDet-D2 architecture. Our design is robust to the\nchoice of base architecture and compressor and should adapt well for future\narchitectures.",
          "link": "http://arxiv.org/abs/2105.00591",
          "publishedOn": "2021-08-10T02:00:11.903Z",
          "wordCount": null,
          "title": "Single-Training Collaborative Object Detectors Adaptive to Bandwidth and Computation. (arXiv:2105.00591v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pontil_M/0/1/0/all/0/1\">Massimiliano Pontil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciliberto_C/0/1/0/all/0/1\">Carlo Ciliberto</a>",
          "description": "Few-shot learning (FSL) is a central problem in meta-learning, where learners\nmust quickly adapt to new tasks given limited training data. Surprisingly,\nrecent works have outperformed meta-learning methods tailored to FSL by casting\nit as standard supervised learning to jointly classify all classes shared\nacross tasks. However, this approach violates the standard FSL setting by\nrequiring global labels shared across tasks, which are often unavailable in\npractice. In this paper, we show why solving FSL via standard classification is\ntheoretically advantageous. This motivates us to propose Meta Label Learning\n(MeLa), a novel algorithm that infers global labels and obtains robust few-shot\nmodels via standard classification. Empirically, we demonstrate that MeLa\noutperforms meta-learning competitors and is comparable to the oracle setting\nwhere ground truth labels are given. We provide extensive ablation studies to\nhighlight the key properties of the proposed strategy.",
          "link": "http://arxiv.org/abs/2108.04055",
          "publishedOn": "2021-08-10T02:00:11.902Z",
          "wordCount": null,
          "title": "The Role of Global Labels in Few-Shot Classification and How to Infer Them. (arXiv:2108.04055v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1903.08543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beeler_C/0/1/0/all/0/1\">Chris Beeler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yahorau_U/0/1/0/all/0/1\">Uladzimir Yahorau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coles_R/0/1/0/all/0/1\">Rory Coles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1\">Kyle Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitelam_S/0/1/0/all/0/1\">Stephen Whitelam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamblyn_I/0/1/0/all/0/1\">Isaac Tamblyn</a>",
          "description": "Using a model heat engine, we show that neural network-based reinforcement\nlearning can identify thermodynamic trajectories of maximal efficiency. We\nconsider both gradient and gradient-free reinforcement learning. We use an\nevolutionary learning algorithm to evolve a population of neural networks,\nsubject to a directive to maximize the efficiency of a trajectory composed of a\nset of elementary thermodynamic processes; the resulting networks learn to\ncarry out the maximally-efficient Carnot, Stirling, or Otto cycles. When given\nan additional irreversible process, this evolutionary scheme learns a\npreviously unknown thermodynamic cycle. Gradient-based reinforcement learning\nis able to learn the Stirling cycle, whereas an evolutionary approach achieves\nthe optimal Carnot cycle. Our results show how the reinforcement learning\nstrategies developed for game playing can be applied to solve physical problems\nconditioned upon path-extensive order parameters.",
          "link": "http://arxiv.org/abs/1903.08543",
          "publishedOn": "2021-08-10T02:00:11.902Z",
          "wordCount": null,
          "title": "Optimizing thermodynamic trajectories using evolutionary and gradient-based reinforcement learning. (arXiv:1903.08543v4 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.06487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Q/0/1/0/all/0/1\">Qingfeng Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yangchen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fyshe_A/0/1/0/all/0/1\">Alona Fyshe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1\">Martha White</a>",
          "description": "Q-learning suffers from overestimation bias, because it approximates the\nmaximum action value using the maximum estimated action value. Algorithms have\nbeen proposed to reduce overestimation bias, but we lack an understanding of\nhow bias interacts with performance, and the extent to which existing\nalgorithms mitigate bias. In this paper, we 1) highlight that the effect of\noverestimation bias on learning efficiency is environment-dependent; 2) propose\na generalization of Q-learning, called \\emph{Maxmin Q-learning}, which provides\na parameter to flexibly control bias; 3) show theoretically that there exists a\nparameter choice for Maxmin Q-learning that leads to unbiased estimation with a\nlower approximation variance than Q-learning; and 4) prove the convergence of\nour algorithm in the tabular case, as well as convergence of several previous\nQ-learning variants, using a novel Generalized Q-learning framework. We\nempirically verify that our algorithm better controls estimation bias in toy\nenvironments, and that it achieves superior performance on several benchmark\nproblems.",
          "link": "http://arxiv.org/abs/2002.06487",
          "publishedOn": "2021-08-10T02:00:11.901Z",
          "wordCount": null,
          "title": "Maxmin Q-learning: Controlling the Estimation Bias of Q-learning. (arXiv:2002.06487v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Menzen_C/0/1/0/all/0/1\">Clara Menzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kok_M/0/1/0/all/0/1\">Manon Kok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batselier_K/0/1/0/all/0/1\">Kim Batselier</a>",
          "description": "Multiway data often naturally occurs in a tensorial format which can be\napproximately represented by a low-rank tensor decomposition. This is useful\nbecause complexity can be significantly reduced and the treatment of\nlarge-scale data sets can be facilitated. In this paper, we find a low-rank\nrepresentation for a given tensor by solving a Bayesian inference problem. This\nis achieved by dividing the overall inference problem into sub-problems where\nwe sequentially infer the posterior distribution of one tensor decomposition\ncomponent at a time. This leads to a probabilistic interpretation of the\nwell-known iterative algorithm alternating linear scheme (ALS). In this way,\nthe consideration of measurement noise is enabled, as well as the incorporation\nof application-specific prior knowledge and the uncertainty quantification of\nthe low-rank tensor estimate. To compute the low-rank tensor estimate from the\nposterior distributions of the tensor decomposition components, we present an\nalgorithm that performs the unscented transform in tensor train format.",
          "link": "http://arxiv.org/abs/2012.11228",
          "publishedOn": "2021-08-10T02:00:11.901Z",
          "wordCount": null,
          "title": "Alternating linear scheme in a Bayesian framework for low-rank tensor approximation. (arXiv:2012.11228v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Andre T. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1\">Edward Raff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicholas_C/0/1/0/all/0/1\">Charles Nicholas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holt_J/0/1/0/all/0/1\">James Holt</a>",
          "description": "The detection of malware is a critical task for the protection of computing\nenvironments. This task often requires extremely low false positive rates (FPR)\nof 0.01% or even lower, for which modern machine learning has no readily\navailable tools. We introduce the first broad investigation of the use of\nuncertainty for malware detection across multiple datasets, models, and feature\ntypes. We show how ensembling and Bayesian treatments of machine learning\nmethods for static malware detection allow for improved identification of model\nerrors, uncovering of new malware families, and predictive performance under\nextreme false positive constraints. In particular, we improve the true positive\nrate (TPR) at an actual realized FPR of 1e-5 from an expected 0.69 for previous\nmethods to 0.80 on the best performing model class on the Sophos industry scale\ndataset. We additionally demonstrate how previous works have used an evaluation\nprotocol that can lead to misleading results.",
          "link": "http://arxiv.org/abs/2108.04081",
          "publishedOn": "2021-08-10T02:00:11.900Z",
          "wordCount": null,
          "title": "Leveraging Uncertainty for Improved Static Malware Detection Under Extreme False Positive Constraints. (arXiv:2108.04081v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.09128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eliasof_M/0/1/0/all/0/1\">Moshe Eliasof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ephrath_J/0/1/0/all/0/1\">Jonathan Ephrath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruthotto_L/0/1/0/all/0/1\">Lars Ruthotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treister_E/0/1/0/all/0/1\">Eran Treister</a>",
          "description": "We present a multigrid-in-channels (MGIC) approach that tackles the quadratic\ngrowth of the number of parameters with respect to the number of channels in\nstandard convolutional neural networks (CNNs). Thereby our approach addresses\nthe redundancy in CNNs that is also exposed by the recent success of\nlightweight CNNs. Lightweight CNNs can achieve comparable accuracy to standard\nCNNs with fewer parameters; however, the number of weights still scales\nquadratically with the CNN's width. Our MGIC architectures replace each CNN\nblock with an MGIC counterpart that utilizes a hierarchy of nested grouped\nconvolutions of small group size to address this.\n\nHence, our proposed architectures scale linearly with respect to the\nnetwork's width while retaining full coupling of the channels as in standard\nCNNs.\n\nOur extensive experiments on image classification, segmentation, and point\ncloud classification show that applying this strategy to different\narchitectures like ResNet and MobileNetV3 reduces the number of parameters\nwhile obtaining similar or better accuracy.",
          "link": "http://arxiv.org/abs/2011.09128",
          "publishedOn": "2021-08-10T02:00:11.899Z",
          "wordCount": 638,
          "title": "MGIC: Multigrid-in-Channels Neural Network Architectures. (arXiv:2011.09128v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14072",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gruber_A/0/1/0/all/0/1\">Anthony Gruber</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gunzburger_M/0/1/0/all/0/1\">Max Gunzburger</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ju_L/0/1/0/all/0/1\">Lili Ju</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Teng_Y/0/1/0/all/0/1\">Yuankai Teng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1\">Zhu Wang</a>",
          "description": "A dimension reduction method based on the \"Nonlinear Level set Learning\"\n(NLL) approach is presented for the pointwise prediction of functions which\nhave been sparsely sampled. Leveraging geometric information provided by the\nImplicit Function Theorem, the proposed algorithm effectively reduces the input\ndimension to the theoretical lower bound with minor accuracy loss, providing a\none-dimensional representation of the function which can be used for regression\nand sensitivity analysis. Experiments and applications are presented which\ncompare this modified NLL with the original NLL and the Active Subspaces (AS)\nmethod. While accommodating sparse input data, the proposed algorithm is shown\nto train quickly and provide a much more accurate and informative reduction\nthan either AS or the original NLL on two example functions with\nhigh-dimensional domains, as well as two state-dependent quantities depending\non the solutions to parametric differential equations.",
          "link": "http://arxiv.org/abs/2104.14072",
          "publishedOn": "2021-08-10T02:00:11.899Z",
          "wordCount": null,
          "title": "Nonlinear Level Set Learning for Function Approximation on Sparse Data with Applications to Parametric Differential Equations. (arXiv:2104.14072v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gammulle_H/0/1/0/all/0/1\">Harshala Gammulle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_T/0/1/0/all/0/1\">Tharindu Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>",
          "description": "This paper presents a novel lightweight COVID-19 diagnosis framework using CT\nscans. Our system utilises a novel two-stage approach to generate robust and\nefficient diagnoses across heterogeneous patient level inputs. We use a\npowerful backbone network as a feature extractor to capture discriminative\nslice-level features. These features are aggregated by a lightweight network to\nobtain a patient level diagnosis. The aggregation network is carefully designed\nto have a small number of trainable parameters while also possessing sufficient\ncapacity to generalise to diverse variations within different CT volumes and to\nadapt to noise introduced during the data acquisition. We achieve a significant\nperformance increase over the baselines when benchmarked on the SPGC COVID-19\nRadiomics Dataset, despite having only 2.5 million trainable parameters and\nrequiring only 0.623 seconds on average to process a single patient's CT volume\nusing an Nvidia-GeForce RTX 2080 GPU.",
          "link": "http://arxiv.org/abs/2108.03786",
          "publishedOn": "2021-08-10T02:00:11.880Z",
          "wordCount": 640,
          "title": "Multi-Slice Net: A novel light weight framework for COVID-19 Diagnosis. (arXiv:2108.03786v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuofan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_K/0/1/0/all/0/1\">Kaicheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_C/0/1/0/all/0/1\">Chaouki Abdallah</a>",
          "description": "Federated Learning is an algorithm suited for training models on\ndecentralized data, but the requirement of a central \"server\" node is a\nbottleneck. In this document, we first introduce the notion of Decentralized\nFederated Learning (DFL). We then perform various experiments on different\nsetups, such as changing model aggregation frequency, switching from\nindependent and identically distributed (IID) dataset partitioning to non-IID\npartitioning with partial global sharing, using different optimization methods\nacross clients, and breaking models into segments with partial sharing. All\nexperiments are run on the MNIST handwritten digits dataset. We observe that\nthose altered training procedures are generally robust, albeit non-optimal. We\nalso observe failures in training when the variance between model weights is\ntoo large. The open-source experiment code is accessible through\nGitHub\\footnote{Code was uploaded at\n\\url{https://github.com/zhzhang2018/DecentralizedFL}}.",
          "link": "http://arxiv.org/abs/2108.03508",
          "publishedOn": "2021-08-10T02:00:11.873Z",
          "wordCount": 575,
          "title": "The Effect of Training Parameters and Mechanisms on Decentralized Federated Learning based on MNIST Dataset. (arXiv:2108.03508v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.03041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yuting Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1\">Yuejie Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuantao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxin Chen</a>",
          "description": "Asynchronous Q-learning aims to learn the optimal action-value function (or\nQ-function) of a Markov decision process (MDP), based on a single trajectory of\nMarkovian samples induced by a behavior policy. Focusing on a\n$\\gamma$-discounted MDP with state space $\\mathcal{S}$ and action space\n$\\mathcal{A}$, we demonstrate that the $\\ell_{\\infty}$-based sample complexity\nof classical asynchronous Q-learning --- namely, the number of samples needed\nto yield an entrywise $\\varepsilon$-accurate estimate of the Q-function --- is\nat most on the order of $\\frac{1}{\\mu_{\\min}(1-\\gamma)^5\\varepsilon^2}+\n\\frac{t_{mix}}{\\mu_{\\min}(1-\\gamma)}$ up to some logarithmic factor, provided\nthat a proper constant learning rate is adopted. Here, $t_{mix}$ and\n$\\mu_{\\min}$ denote respectively the mixing time and the minimum state-action\noccupancy probability of the sample trajectory. The first term of this bound\nmatches the sample complexity in the synchronous case with independent samples\ndrawn from the stationary distribution of the trajectory. The second term\nreflects the cost taken for the empirical distribution of the Markovian\ntrajectory to reach a steady state, which is incurred at the very beginning and\nbecomes amortized as the algorithm runs. Encouragingly, the above bound\nimproves upon the state-of-the-art result \\cite{qu2020finite} by a factor of at\nleast $|\\mathcal{S}||\\mathcal{A}|$ for all scenarios, and by a factor of at\nleast $t_{mix}|\\mathcal{S}||\\mathcal{A}|$ for any sufficiently small accuracy\nlevel $\\varepsilon$. Further, we demonstrate that the scaling on the effective\nhorizon $\\frac{1}{1-\\gamma}$ can be improved by means of variance reduction.",
          "link": "http://arxiv.org/abs/2006.03041",
          "publishedOn": "2021-08-10T02:00:11.865Z",
          "wordCount": null,
          "title": "Sample Complexity of Asynchronous Q-Learning: Sharper Analysis and Variance Reduction. (arXiv:2006.03041v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdellatif_A/0/1/0/all/0/1\">Alaa Awad Abdellatif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mhaisen_N/0/1/0/all/0/1\">Naram Mhaisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chkirbene_Z/0/1/0/all/0/1\">Zina Chkirbene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Amr Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1\">Aiman Erbad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1\">Mohsen Guizani</a>",
          "description": "The rapid increase in the percentage of chronic disease patients along with\nthe recent pandemic pose immediate threats on healthcare expenditure and\nelevate causes of death. This calls for transforming healthcare systems away\nfrom one-on-one patient treatment into intelligent health systems, to improve\nservices, access and scalability, while reducing costs. Reinforcement Learning\n(RL) has witnessed an intrinsic breakthrough in solving a variety of complex\nproblems for diverse applications and services. Thus, we conduct in this paper\na comprehensive survey of the recent models and techniques of RL that have been\ndeveloped/used for supporting Intelligent-healthcare (I-health) systems. This\npaper can guide the readers to deeply understand the state-of-the-art regarding\nthe use of RL in the context of I-health. Specifically, we first present an\noverview for the I-health systems challenges, architecture, and how RL can\nbenefit these systems. We then review the background and mathematical modeling\nof different RL, Deep RL (DRL), and multi-agent RL models. After that, we\nprovide a deep literature review for the applications of RL in I-health\nsystems. In particular, three main areas have been tackled, i.e., edge\nintelligence, smart core network, and dynamic treatment regimes. Finally, we\nhighlight emerging challenges and outline future research directions in driving\nthe future success of RL in I-health systems, which opens the door for\nexploring some interesting and unsolved problems.",
          "link": "http://arxiv.org/abs/2108.04087",
          "publishedOn": "2021-08-10T02:00:11.864Z",
          "wordCount": null,
          "title": "Reinforcement Learning for Intelligent Healthcare Systems: A Comprehensive Survey. (arXiv:2108.04087v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.02193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1\">Abulikemu Abuduweili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingjian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng-Zhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>",
          "description": "While recent studies on semi-supervised learning have shown remarkable\nprogress in leveraging both labeled and unlabeled data, most of them presume a\nbasic setting of the model is randomly initialized. In this work, we consider\nsemi-supervised learning and transfer learning jointly, leading to a more\npractical and competitive paradigm that can utilize both powerful pre-trained\nmodels from source domain as well as labeled/unlabeled data in the target\ndomain. To better exploit the value of both pre-trained weights and unlabeled\ntarget examples, we introduce adaptive consistency regularization that consists\nof two complementary components: Adaptive Knowledge Consistency (AKC) on the\nexamples between the source and target model, and Adaptive Representation\nConsistency (ARC) on the target model between labeled and unlabeled examples.\nExamples involved in the consistency regularization are adaptively selected\naccording to their potential contributions to the target task. We conduct\nextensive experiments on popular benchmarks including CIFAR-10, CUB-200, and\nMURA, by fine-tuning the ImageNet pre-trained ResNet-50 model. Results show\nthat our proposed adaptive consistency regularization outperforms\nstate-of-the-art semi-supervised learning techniques such as Pseudo Label, Mean\nTeacher, and FixMatch. Moreover, our algorithm is orthogonal to existing\nmethods and thus able to gain additional improvements on top of MixMatch and\nFixMatch. Our code is available at\nhttps://github.com/SHI-Labs/Semi-Supervised-Transfer-Learning.",
          "link": "http://arxiv.org/abs/2103.02193",
          "publishedOn": "2021-08-10T02:00:11.864Z",
          "wordCount": 678,
          "title": "Adaptive Consistency Regularization for Semi-Supervised Transfer Learning. (arXiv:2103.02193v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06307",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_B/0/1/0/all/0/1\">Bowen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_F/0/1/0/all/0/1\">Fanghong Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_C/0/1/0/all/0/1\">Changyun Wen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_R/0/1/0/all/0/1\">Ruilong Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wen-An Zhang</a>",
          "description": "Most traditional false data injection attack (FDIA) detection approaches rely\non a key assumption, i.e., the power system can be accurately modeled. However,\nthe transmission line parameters are dynamic and cannot be accurately known\nduring operation and thus the involved modeling errors should not be neglected.\nIn this paper, an illustrative case has revealed that modeling errors in\ntransmission lines significantly weaken the detection effectiveness of\nconventional FDIA approaches. To tackle this issue, we propose an FDIA\ndetection mechanism from the perspective of transfer learning. Specifically,\nthe simulated power system is treated as a source domain, which provides\nabundant simulated normal and attack data. The real world's running system\nwhose transmission line parameters are unknown is taken as a target domain\nwhere sufficient real normal data are collected for tracking the latest system\nstates online. The designed transfer strategy that aims at making full use of\ndata in hand is divided into two optimization stages. In the first stage, a\ndeep neural network (DNN) is built by simultaneously optimizing several\nwell-designed objective terms with both simulated data and real data, and then\nit is fine-tuned via real data in the second stage. Several case studies on the\nIEEE 14-bus and 118-bus systems verify the effectiveness of the proposed\nmechanism.",
          "link": "http://arxiv.org/abs/2104.06307",
          "publishedOn": "2021-08-10T02:00:11.857Z",
          "wordCount": null,
          "title": "Detecting False Data Injection Attacks in Smart Grids with Modeling Errors: A Deep Transfer Learning Based Approach. (arXiv:2104.06307v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sontakke_S/0/1/0/all/0/1\">Sumedh A. Sontakke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehrjou_A/0/1/0/all/0/1\">Arash Mehrjou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itti_L/0/1/0/all/0/1\">Laurent Itti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "Animals exhibit an innate ability to learn regularities of the world through\ninteraction. By performing experiments in their environment, they are able to\ndiscern the causal factors of variation and infer how they affect the world's\ndynamics. Inspired by this, we attempt to equip reinforcement learning agents\nwith the ability to perform experiments that facilitate a categorization of the\nrolled-out trajectories, and to subsequently infer the causal factors of the\nenvironment in a hierarchical manner. We introduce {\\em causal curiosity}, a\nnovel intrinsic reward, and show that it allows our agents to learn optimal\nsequences of actions and discover causal factors in the dynamics of the\nenvironment. The learned behavior allows the agents to infer a binary quantized\nrepresentation for the ground-truth causal factors in every environment.\nAdditionally, we find that these experimental behaviors are semantically\nmeaningful (e.g., our agents learn to lift blocks to categorize them by\nweight), and are learnt in a self-supervised manner with approximately 2.5\ntimes less data than conventional supervised planners. We show that these\nbehaviors can be re-purposed and fine-tuned (e.g., from lifting to pushing or\nother downstream tasks). Finally, we show that the knowledge of causal factor\nrepresentations aids zero-shot learning for more complex tasks. Visit\nhttps://sites.google.com/usc.edu/causal-curiosity/home for website.",
          "link": "http://arxiv.org/abs/2010.03110",
          "publishedOn": "2021-08-10T02:00:11.856Z",
          "wordCount": null,
          "title": "Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning. (arXiv:2010.03110v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00489",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Simini_F/0/1/0/all/0/1\">Filippo Simini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barlacchi_G/0/1/0/all/0/1\">Gianni Barlacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luca_M/0/1/0/all/0/1\">Massimiliano Luca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappalardo_L/0/1/0/all/0/1\">Luca Pappalardo</a>",
          "description": "The movements of individuals within and among cities influence critical\naspects of our society, such as well-being, the spreading of epidemics, and the\nquality of the environment. When information about mobility flows is not\navailable for a particular region of interest, we must rely on mathematical\nmodels to generate them. In this work, we propose the Deep Gravity model, an\neffective method to generate flow probabilities that exploits many variables\n(e.g., land use, road network, transport, food, health facilities) extracted\nfrom voluntary geographic data, and uses deep neural networks to discover\nnon-linear relationships between those variables and mobility flows. Our\nexperiments, conducted on mobility flows in England, Italy, and New York State,\nshow that Deep Gravity has good geographic generalization capability, achieving\na significant increase in performance (especially in densely populated regions\nof interest) with respect to the classic gravity model and models that do not\nuse deep neural networks or geographic data. We also show how flows generated\nby Deep Gravity may be explained in terms of the geographic features using\nexplainable AI techniques.",
          "link": "http://arxiv.org/abs/2012.00489",
          "publishedOn": "2021-08-10T02:00:11.856Z",
          "wordCount": null,
          "title": "Deep Gravity: enhancing mobility flows generation with deep neural networks and geographic information. (arXiv:2012.00489v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Donghua Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maoyin Chen</a>",
          "description": "This paper proposes a novel sparse principal component analysis algorithm\nwith self-learning ability for successive modes, where synaptic intelligence is\nemployed to measure the importance of variables and a regularization term is\nadded to preserve the learned knowledge of previous modes. Different from\ntraditional multimode monitoring methods, the monitoring model is updated based\non the current model and new data when a new mode arrives, thus delivering\nprominent performance for sequential modes. Besides, the computation and\nstorage resources are saved in the long run, because it is not necessary to\nretrain the model from scratch frequently and store data from previous modes.\nMore importantly, the model furnishes excellent interpretability owing to the\nsparsity of parameters. Finally, a numerical case and a practical pulverizing\nsystem are adopted to illustrate the effectiveness of the proposed algorithm.",
          "link": "http://arxiv.org/abs/2108.03449",
          "publishedOn": "2021-08-10T02:00:11.855Z",
          "wordCount": 571,
          "title": "Self-learning sparse PCA for multimode process monitoring. (arXiv:2108.03449v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_M/0/1/0/all/0/1\">Md. Tareq Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohammed Eunus Ali</a>",
          "description": "Reconstructing a layout of indoor spaces has been a crucial part of growing\nindoor location based services. One of the key challenges in the proliferation\nof indoor location based services is the unavailability of indoor spatial maps\ndue to the complex nature of capturing an indoor space model (e.g., floor plan)\nof an existing building. In this paper, we propose a system to automatically\ngenerate floor plans that can recognize rooms from the point-clouds obtained\nthrough smartphones like Google's Tango. In particular, we propose two\napproaches - a Recurrent Neural Network based approach using Pointer Network\nand a Convolutional Neural Network based approach using Mask-RCNN to identify\nrooms (and thereby floor plans) from point-clouds. Experimental results on\ndifferent datasets demonstrate approximately 0.80-0.90 Intersection-over-Union\nscores, which show that our models can effectively identify the rooms and\nregenerate the shapes of the rooms in heterogeneous environment.",
          "link": "http://arxiv.org/abs/2108.03378",
          "publishedOn": "2021-08-10T02:00:11.848Z",
          "wordCount": 578,
          "title": "Learning Indoor Layouts from Simple Point-Clouds. (arXiv:2108.03378v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartel_J/0/1/0/all/0/1\">Jacob Bartel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palowitch_J/0/1/0/all/0/1\">John Palowitch</a>",
          "description": "The widespread adoption of online social networks in daily life has created a\npressing need for effectively classifying user-generated content. This work\npresents techniques for classifying linked content spread on forum websites --\nspecifically, links to news articles or blogs -- using user interaction signals\nalone. Importantly, online forums such as Reddit do not have a user-generated\nsocial graph, which is assumed in social network behavioral-based\nclassification settings. Using Reddit as a case-study, we show how to obtain a\nderived social graph, and use this graph, Reddit post sequences, and comment\ntrees as inputs to a Recurrent Graph Neural Network (R-GNN) encoder. We train\nthe R-GNN on news link categorization and rumor detection, showing superior\nresults to recent baselines. Our code is made publicly available at\nhttps://github.com/google-research/social_cascades.",
          "link": "http://arxiv.org/abs/2108.03548",
          "publishedOn": "2021-08-10T02:00:11.827Z",
          "wordCount": 568,
          "title": "Recurrent Graph Neural Networks for Rumor Detection in Online Forums. (arXiv:2108.03548v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2104.15007",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ayoobi_N/0/1/0/all/0/1\">Nooshin Ayoobi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifrazi_D/0/1/0/all/0/1\">Danial Sharifrazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeibi_A/0/1/0/all/0/1\">Afshin Shoeibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorriz_J/0/1/0/all/0/1\">Juan M. Gorriz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosaei_H/0/1/0/all/0/1\">Hossein Moosaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chofreh_A/0/1/0/all/0/1\">Abdoulmohammad Gholamzadeh Chofreh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goni_F/0/1/0/all/0/1\">Feybi Ariani Goni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klemes_J/0/1/0/all/0/1\">Jiri Jaromir Klemes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosavi_A/0/1/0/all/0/1\">Amir Mosavi</a>",
          "description": "The first known case of Coronavirus disease 2019 (COVID-19) was identified in\nDecember 2019. It has spread worldwide, leading to an ongoing pandemic, imposed\nrestrictions and costs to many countries. Predicting the number of new cases\nand deaths during this period can be a useful step in predicting the costs and\nfacilities required in the future. The purpose of this study is to predict new\ncases and deaths rate one, three and seven-day ahead during the next 100 days.\nThe motivation for predicting every n days (instead of just every day) is the\ninvestigation of the possibility of computational cost reduction and still\nachieving reasonable performance. Such a scenario may be encountered real-time\nforecasting of time series. Six different deep learning methods are examined on\nthe data adopted from the WHO website. Three methods are LSTM, Convolutional\nLSTM, and GRU. The bidirectional extension is then considered for each method\nto forecast the rate of new cases and new deaths in Australia and Iran\ncountries.",
          "link": "http://arxiv.org/abs/2104.15007",
          "publishedOn": "2021-08-10T02:00:11.813Z",
          "wordCount": null,
          "title": "Time Series Forecasting of New Cases and New Deaths Rate for COVID-19 using Deep Learning Methods. (arXiv:2104.15007v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Najafi_B/0/1/0/all/0/1\">Bahareh Najafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parsaeefard_S/0/1/0/all/0/1\">Saeedeh Parsaeefard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leon_Garcia_A/0/1/0/all/0/1\">Alberto Leon-Garcia</a>",
          "description": "GNNs have been proven to perform highly effective in various node-level,\nedge-level, and graph-level prediction tasks in several domains. Existing\napproaches mainly focus on static graphs. However, many graphs change over time\nwith their edge may disappear, or node or edge attribute may alter from one\ntime to the other. It is essential to consider such evolution in representation\nlearning of nodes in time varying graphs. In this paper, we propose a Temporal\nMultilayered Position-aware Graph Neural Network (TMP-GNN), a node embedding\napproach for dynamic graph that incorporates the interdependence of temporal\nrelations into embedding computation. We evaluate the performance of TMP-GNN on\ntwo different representations of temporal multilayered graphs. The performance\nis assessed against the most popular GNNs on node-level prediction tasks. Then,\nwe incorporate TMP-GNN into a deep learning framework to estimate missing data\nand compare the performance with their corresponding competent GNNs from our\nformer experiment, and a baseline method. Experimental results on four\nreal-world datasets yield up to 58% of lower ROC AUC for pairwise node\nclassification task, and 96% of lower MAE in missing feature estimation,\nparticularly for graphs with a relatively high number of nodes and lower mean\ndegree of connectivity.",
          "link": "http://arxiv.org/abs/2108.03400",
          "publishedOn": "2021-08-10T02:00:11.811Z",
          "wordCount": 635,
          "title": "Missing Data Estimation in Temporal Multilayer Position-aware Graph Neural Network (TMP-GNN). (arXiv:2108.03400v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Al_Saffar_A/0/1/0/all/0/1\">A. Al-Saffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stancombe_A/0/1/0/all/0/1\">A. Stancombe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_A/0/1/0/all/0/1\">A. Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbosh_A/0/1/0/all/0/1\">A. Abbosh</a>",
          "description": "Incorporating boundaries of the imaging object as a priori information to\nimaging algorithms can significantly improve the performance of electromagnetic\nmedical imaging systems. To avoid overly complicating the system by using\ndifferent sensors and the adverse effect of the subject's movement, a\nlearning-based method is proposed to estimate the boundary (external contour)\nof the imaged object using the same electromagnetic imaging data. While imaging\ntechniques may discard the reflection coefficients for being dominant and\nuninformative for imaging, these parameters are made use of for boundary\ndetection. The learned model is verified through independent clinical human\ntrials by using a head imaging system with a 16-element antenna array that\nworks across the band 0.7-1.6 GHz. The evaluation demonstrated that the model\nachieves average dissimilarity of 0.012 in Hu-moment while detecting head\nboundary. The model enables fast scan and image creation while eliminating the\nneed for additional devices for accurate boundary estimation.",
          "link": "http://arxiv.org/abs/2108.03233",
          "publishedOn": "2021-08-10T02:00:11.805Z",
          "wordCount": 597,
          "title": "Operational Learning-based Boundary Estimation in Electromagnetic Medical Imaging. (arXiv:2108.03233v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alb_C/0/1/0/all/0/1\">Cristian Alb</a>",
          "description": "A family of concurrent data predictors is derived from the decision tree\nclassifier by removing the limitation of sequentially evaluating attributes. By\nevaluating attributes concurrently, the decision tree collapses into a flat\nstructure. Experiments indicate improvements of the prediction accuracy.",
          "link": "http://arxiv.org/abs/2108.03887",
          "publishedOn": "2021-08-10T02:00:11.799Z",
          "wordCount": null,
          "title": "Collapsing the Decision Tree: the Concurrent Data Predictor. (arXiv:2108.03887v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pitzer_E/0/1/0/all/0/1\">Erik Pitzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1\">Gabriel Kronberger</a>",
          "description": "The typical methods for symbolic regression produce rather abrupt changes in\nsolution candidates. In this work, we have tried to transform symbolic\nregression from an optimization problem, with a landscape that is so rugged\nthat typical analysis methods do not produce meaningful results, to one that\ncan be compared to typical and very smooth real-valued problems. While the\nruggedness might not interfere with the performance of optimization, it\nrestricts the possibilities of analysis. Here, we have explored different\naspects of a transformation and propose a simple procedure to create\nreal-valued optimization problems from symbolic regression problems.",
          "link": "http://arxiv.org/abs/2108.03274",
          "publishedOn": "2021-08-10T02:00:11.797Z",
          "wordCount": 573,
          "title": "Smooth Symbolic Regression: Transformation of Symbolic Regression into a Real-valued Optimization Problem. (arXiv:2108.03274v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wenqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Wei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaorui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Han Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xianfeng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Suhang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1\">Charu Aggarwal</a>",
          "description": "Graph Neural Networks (GNNs) have boosted the performance for many\ngraph-related tasks. Despite the great success, recent studies have shown that\nGNNs are highly vulnerable to adversarial attacks, where adversaries can\nmislead the GNNs' prediction by modifying graphs. On the other hand, the\nexplanation of GNNs (GNNExplainer) provides a better understanding of a trained\nGNN model by generating a small subgraph and features that are most influential\nfor its prediction. In this paper, we first perform empirical studies to\nvalidate that GNNExplainer can act as an inspection tool and have the potential\nto detect the adversarial perturbations for graphs. This finding motivates us\nto further initiate a new problem investigation: Whether a graph neural network\nand its explanations can be jointly attacked by modifying graphs with malicious\ndesires? It is challenging to answer this question since the goals of\nadversarial attacks and bypassing the GNNExplainer essentially contradict each\nother. In this work, we give a confirmative answer to this question by\nproposing a novel attack framework (GEAttack), which can attack both a GNN\nmodel and its explanations by simultaneously exploiting their vulnerabilities.\nExtensive experiments on two explainers (GNNExplainer and PGExplainer) under\nvarious real-world datasets demonstrate the effectiveness of the proposed\nmethod.",
          "link": "http://arxiv.org/abs/2108.03388",
          "publishedOn": "2021-08-10T02:00:11.782Z",
          "wordCount": 658,
          "title": "Jointly Attacking Graph Neural Network and its Explanations. (arXiv:2108.03388v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.04395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1\">Xiang Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Saizhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengfei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fangli Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alex X. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chunming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>",
          "description": "While the celebrated graph neural networks yield effective representations\nfor individual nodes of a graph, there has been relatively less success in\nextending to the task of graph similarity learning. Recent work on graph\nsimilarity learning has considered either global-level graph-graph interactions\nor low-level node-node interactions, however ignoring the rich cross-level\ninteractions (e.g., between each node of one graph and the other whole graph).\nIn this paper, we propose a multi-level graph matching network (MGMN) framework\nfor computing the graph similarity between any pair of graph-structured objects\nin an end-to-end fashion. In particular, the proposed MGMN consists of a\nnode-graph matching network for effectively learning cross-level interactions\nbetween each node of one graph and the other whole graph, and a siamese graph\nneural network to learn global-level interactions between two input graphs.\nFurthermore, to compensate for the lack of standard benchmark datasets, we have\ncreated and collected a set of datasets for both the graph-graph classification\nand graph-graph regression tasks with different sizes in order to evaluate the\neffectiveness and robustness of our models. Comprehensive experiments\ndemonstrate that MGMN consistently outperforms state-of-the-art baseline models\non both the graph-graph classification and graph-graph regression tasks.\nCompared with previous work, MGMN also exhibits stronger robustness as the\nsizes of the two input graphs increase.",
          "link": "http://arxiv.org/abs/2007.04395",
          "publishedOn": "2021-08-10T02:00:11.775Z",
          "wordCount": null,
          "title": "Multilevel Graph Matching Networks for Deep Graph Similarity Learning. (arXiv:2007.04395v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bryan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhourong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grossman_T/0/1/0/all/0/1\">Tovi Grossman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>",
          "description": "Mobile User Interface Summarization generates succinct language descriptions\nof mobile screens for conveying important contents and functionalities of the\nscreen, which can be useful for many language-based application scenarios. We\npresent Screen2Words, a novel screen summarization approach that automatically\nencapsulates essential information of a UI screen into a coherent language\nphrase. Summarizing mobile screens requires a holistic understanding of the\nmulti-modal data of mobile UIs, including text, image, structures as well as UI\nsemantics, motivating our multi-modal learning approach. We collected and\nanalyzed a large-scale screen summarization dataset annotated by human workers.\nOur dataset contains more than 112k language summarization across $\\sim$22k\nunique UI screens. We then experimented with a set of deep models with\ndifferent configurations. Our evaluation of these models with both automatic\naccuracy metrics and human rating shows that our approach can generate\nhigh-quality summaries for mobile screens. We demonstrate potential use cases\nof Screen2Words and open-source our dataset and model to lay the foundations\nfor further bridging language and user interfaces.",
          "link": "http://arxiv.org/abs/2108.03353",
          "publishedOn": "2021-08-10T02:00:11.763Z",
          "wordCount": 608,
          "title": "Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning. (arXiv:2108.03353v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Ziyu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Youfang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhiyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiangheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Caijie Chen</a>",
          "description": "The research on human emotion under multimedia stimulation based on\nphysiological signals is an emerging field, and important progress has been\nachieved for emotion recognition based on multi-modal signals. However, it is\nchallenging to make full use of the complementarity among\nspatial-spectral-temporal domain features for emotion recognition, as well as\nmodel the heterogeneity and correlation among multi-modal signals. In this\npaper, we propose a novel two-stream heterogeneous graph recurrent neural\nnetwork, named HetEmotionNet, fusing multi-modal physiological signals for\nemotion recognition. Specifically, HetEmotionNet consists of the\nspatial-temporal stream and the spatial-spectral stream, which can fuse\nspatial-spectral-temporal domain features in a unified framework. Each stream\nis composed of the graph transformer network for modeling the heterogeneity,\nthe graph convolutional network for modeling the correlation, and the gated\nrecurrent unit for capturing the temporal domain or spectral domain dependency.\nExtensive experiments on two real-world datasets demonstrate that our proposed\nmodel achieves better performance than state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2108.03354",
          "publishedOn": "2021-08-10T02:00:11.757Z",
          "wordCount": 620,
          "title": "HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent Neural Network for Multi-modal Emotion Recognition. (arXiv:2108.03354v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03288",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Faysal_A/0/1/0/all/0/1\">Atik Faysal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keng_N/0/1/0/all/0/1\">Ngui Wai Keng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_M/0/1/0/all/0/1\">M. H. Lim</a>",
          "description": "Time-series data are one of the fundamental types of raw data representation\nused in data-driven techniques. In machine condition monitoring, time-series\nvibration data are overly used in data mining for deep neural networks.\nTypically, vibration data is converted into images for classification using\nDeep Neural Networks (DNNs), and scalograms are the most effective form of\nimage representation. However, the DNN classifiers require huge labeled\ntraining samples to reach their optimum performance. So, many forms of data\naugmentation techniques are applied to the classifiers to compensate for the\nlack of training samples. However, the scalograms are graphical representations\nwhere the existing augmentation techniques suffer because they either change\nthe graphical meaning or have too much noise in the samples that change the\nphysical meaning. In this study, a data augmentation technique named ensemble\naugmentation is proposed to overcome this limitation. This augmentation method\nuses the power of white noise added in ensembles to the original samples to\ngenerate real-like samples. After averaging the signal with ensembles, a new\nsignal is obtained that contains the characteristics of the original signal.\nThe parameters for the ensemble augmentation are validated using a simulated\nsignal. The proposed method is evaluated using 10 class bearing vibration data\nusing three state-of-the-art Transfer Learning (TL) models, namely,\nInception-V3, MobileNet-V2, and ResNet50. Augmented samples are generated in\ntwo increments: the first increment generates the same number of fake samples\nas the training samples, and in the second increment, the number of samples is\nincreased gradually. The outputs from the proposed method are compared with no\naugmentation, augmentations using deep convolution generative adversarial\nnetwork (DCGAN), and several geometric transformation-based augmentations...",
          "link": "http://arxiv.org/abs/2108.03288",
          "publishedOn": "2021-08-10T02:00:11.749Z",
          "wordCount": 711,
          "title": "Ensemble Augmentation for Deep Neural Networks Using 1-D Time Series Vibration Data. (arXiv:2108.03288v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.02856",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Chakrabarti_K/0/1/0/all/0/1\">Kushal Chakrabarti</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gupta_N/0/1/0/all/0/1\">Nirupam Gupta</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chopra_N/0/1/0/all/0/1\">Nikhil Chopra</a>",
          "description": "This paper considers the multi-agent linear least-squares problem in a\nserver-agent network. In this problem, the system comprises multiple agents,\neach having a set of local data points, that are connected to a server. The\ngoal for the agents is to compute a linear mathematical model that optimally\nfits the collective data points held by all the agents, without sharing their\nindividual local data points. This goal can be achieved, in principle, using\nthe server-agent variant of the traditional iterative gradient-descent method.\nThe gradient-descent method converges linearly to a solution, and its rate of\nconvergence is lower bounded by the conditioning of the agents' collective data\npoints. If the data points are ill-conditioned, the gradient-descent method may\nrequire a large number of iterations to converge.\n\nWe propose an iterative pre-conditioning technique that mitigates the\ndeleterious effect of the conditioning of data points on the rate of\nconvergence of the gradient-descent method. We rigorously show that the\nresulting pre-conditioned gradient-descent method, with the proposed iterative\npre-conditioning, achieves superlinear convergence when the least-squares\nproblem has a unique solution. In general, the convergence is linear with\nimproved rate of convergence in comparison to the traditional gradient-descent\nmethod and the state-of-the-art accelerated gradient-descent methods. We\nfurther illustrate the improved rate of convergence of our proposed algorithm\nthrough experiments on different real-world least-squares problems in both\nnoise-free and noisy computation environment.",
          "link": "http://arxiv.org/abs/2008.02856",
          "publishedOn": "2021-08-10T02:00:11.712Z",
          "wordCount": null,
          "title": "Iterative Pre-Conditioning for Expediting the Gradient-Descent Method: The Distributed Linear Least-Squares Problem. (arXiv:2008.02856v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Keqin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Ting Wu</a>",
          "description": "We consider a class of restless bandit problems that finds a broad\napplication area in stochastic optimization, reinforcement learning and\noperations research. In our model, there are $N$ independent $2$-state Markov\nprocesses that may be observed and accessed for accruing rewards. The\nobservation is error-prone, i.e., both false alarm and miss detection may\nhappen. Furthermore, the user can only choose a subset of $M~(M<N)$ processes\nto observe at each discrete time. If a process in state~$1$ is correctly\nobserved, then it will offer some reward. Due to the partial and imperfect\nobservation model, the system is formulated as a restless multi-armed bandit\nproblem with an information state space of uncountable cardinality. Restless\nbandit problems with finite state spaces are PSPACE-HARD in general. In this\npaper, we establish a low-complexity algorithm that achieves a strong\nperformance for this class of restless bandits. Under certain conditions, we\ntheoretically prove the existence (indexability) of Whittle index and its\nequivalence to our algorithm. When those conditions do not hold, we show by\nnumerical experiments the near-optimal performance of our algorithm in general.",
          "link": "http://arxiv.org/abs/2108.03812",
          "publishedOn": "2021-08-10T02:00:11.708Z",
          "wordCount": null,
          "title": "Whittle Index for A Class of Restless Bandits with Imperfect Observations. (arXiv:2108.03812v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04035",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seo_B/0/1/0/all/0/1\">Beomseok Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>",
          "description": "Deep neural network (DNN) models have achieved phenomenal success for\napplications in many domains, ranging from academic research in science and\nengineering to industry and business. The modeling power of DNN is believed to\nhave come from the complexity and over-parameterization of the model, which on\nthe other hand has been criticized for the lack of interpretation. Although\ncertainly not true for every application, in some applications, especially in\neconomics, social science, healthcare industry, and administrative decision\nmaking, scientists or practitioners are resistant to use predictions made by a\nblack-box system for multiple reasons. One reason is that a major purpose of a\nstudy can be to make discoveries based upon the prediction function, e.g., to\nreveal the relationships between measurements. Another reason can be that the\ntraining dataset is not large enough to make researchers feel completely sure\nabout a purely data-driven result. Being able to examine and interpret the\nprediction function will enable researchers to connect the result with existing\nknowledge or gain insights about new directions to explore. Although classic\nstatistical models are much more explainable, their accuracy often falls\nconsiderably below DNN. In this paper, we propose an approach to fill the gap\nbetween relatively simple explainable models and DNN such that we can more\nflexibly tune the trade-off between interpretability and accuracy. Our main\nidea is a mixture of discriminative models that is trained with the guidance\nfrom a DNN. Although mixtures of discriminative models have been studied\nbefore, our way of generating the mixture is quite different.",
          "link": "http://arxiv.org/abs/2108.04035",
          "publishedOn": "2021-08-10T02:00:11.707Z",
          "wordCount": null,
          "title": "Mixture of Linear Models Co-supervised by Deep Neural Networks. (arXiv:2108.04035v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.07986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Sanghamitra Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_P/0/1/0/all/0/1\">Praveen Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mardziel_P/0/1/0/all/0/1\">Piotr Mardziel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1\">Anupam Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grover_P/0/1/0/all/0/1\">Pulkit Grover</a>",
          "description": "With the growing use of ML in highly consequential domains, quantifying\ndisparity with respect to protected attributes, e.g., gender, race, etc., is\nimportant. While quantifying disparity is essential, sometimes the needs of an\noccupation may require the use of certain features that are critical in a way\nthat any disparity that can be explained by them might need to be exempted.\nE.g., in hiring a software engineer for a safety-critical application,\ncoding-skills may be weighed strongly, whereas name, zip code, or reference\nletters may be used only to the extent that they do not add disparity. In this\nwork, we propose an information-theoretic decomposition of the total disparity\n(a quantification inspired from counterfactual fairness) into two components: a\nnon-exempt component which quantifies the part that cannot be accounted for by\nthe critical features, and an exempt component that quantifies the remaining\ndisparity. This decomposition allows one to check if the disparity arose purely\ndue to the critical features (inspired from the business necessity defense of\ndisparate impact law) and also enables selective removal of the non-exempt\ncomponent if desired. We arrive at this decomposition through canonical\nexamples that lead to a set of desirable properties (axioms) that a measure of\nnon-exempt disparity should satisfy. Our proposed measure satisfies all of\nthem. Our quantification bridges ideas of causality, Simpson's paradox, and a\nbody of work from information theory called Partial Information Decomposition.\nWe also obtain an impossibility result showing that no observational measure\ncan satisfy all the desirable properties, leading us to relax our goals and\nexamine observational measures that satisfy only some of them. We perform case\nstudies to show how one can audit/train models while reducing non-exempt\ndisparity.",
          "link": "http://arxiv.org/abs/2006.07986",
          "publishedOn": "2021-08-10T02:00:11.707Z",
          "wordCount": null,
          "title": "Fairness Under Feature Exemptions: Counterfactual and Observational Measures. (arXiv:2006.07986v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noarov_G/0/1/0/all/0/1\">Georgy Noarov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pai_M/0/1/0/all/0/1\">Mallesh Pai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1\">Aaron Roth</a>",
          "description": "We introduce a simple but general online learning framework, in which at\nevery round, an adaptive adversary introduces a new game, consisting of an\naction space for the learner, an action space for the adversary, and a vector\nvalued objective function that is convex-concave in every coordinate. The\nlearner and the adversary then play in this game. The learner's goal is to play\nso as to minimize the maximum coordinate of the cumulative vector-valued loss.\nThe resulting one-shot game is not convex-concave, and so the minimax theorem\ndoes not apply. Nevertheless, we give a simple algorithm that can compete with\nthe setting in which the adversary must announce their action first, with\noptimally diminishing regret.\n\nWe demonstrate the power of our simple framework by using it to derive\noptimal bounds and algorithms across a variety of domains. This includes no\nregret learning: we can recover optimal algorithms and bounds for minimizing\nexternal regret, internal regret, adaptive regret, multigroup regret,\nsubsequence regret, and a notion of regret in the sleeping experts setting.\nNext, we use it to derive a variant of Blackwell's Approachability Theorem,\nwhich we term \"Fast Polytope Approachability\". Finally, we are able to recover\nrecently derived algorithms and bounds for online adversarial multicalibration\nand related notions (mean-conditioned moment multicalibration, and prediction\ninterval multivalidity).",
          "link": "http://arxiv.org/abs/2108.03837",
          "publishedOn": "2021-08-10T02:00:11.706Z",
          "wordCount": null,
          "title": "Online Multiobjective Minimax Optimization and Applications. (arXiv:2108.03837v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Md. Rezaul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Sumon Kanti Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tanhim Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarker_S/0/1/0/all/0/1\">Sagor Sarker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_M/0/1/0/all/0/1\">Mehadi Hasan Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_K/0/1/0/all/0/1\">Kabir Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md. Azam Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decker_S/0/1/0/all/0/1\">Stefan Decker</a>",
          "description": "The exponential growths of social media and micro-blogging sites not only\nprovide platforms for empowering freedom of expressions and individual voices,\nbut also enables people to express anti-social behaviour like online\nharassment, cyberbullying, and hate speech. Numerous works have been proposed\nto utilize textual data for social and anti-social behaviour analysis, by\npredicting the contexts mostly for highly-resourced languages like English.\nHowever, some languages are under-resourced, e.g., South Asian languages like\nBengali, that lack computational resources for accurate natural language\nprocessing (NLP). In this paper, we propose an explainable approach for hate\nspeech detection from the under-resourced Bengali language, which we called\nDeepHateExplainer. Bengali texts are first comprehensively preprocessed, before\nclassifying them into political, personal, geopolitical, and religious hates\nusing a neural ensemble method of transformer-based neural architectures (i.e.,\nmonolingual Bangla BERT-base, multilingual BERT-cased/uncased, and\nXLM-RoBERTa). Important(most and least) terms are then identified using\nsensitivity analysis and layer-wise relevance propagation(LRP), before\nproviding human-interpretable explanations. Finally, we compute\ncomprehensiveness and sufficiency scores to measure the quality of explanations\nw.r.t faithfulness. Evaluations against machine learning~(linear and tree-based\nmodels) and neural networks (i.e., CNN, Bi-LSTM, and Conv-LSTM with word\nembeddings) baselines yield F1-scores of 78%, 91%, 89%, and 84%, for political,\npersonal, geopolitical, and religious hates, respectively, outperforming both\nML and DNN baselines.",
          "link": "http://arxiv.org/abs/2012.14353",
          "publishedOn": "2021-08-10T02:00:11.706Z",
          "wordCount": null,
          "title": "DeepHateExplainer: Explainable Hate Speech Detection in Under-resourced Bengali Language. (arXiv:2012.14353v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03457",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zifan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_N/0/1/0/all/0/1\">Na Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1\">Dit-Yan Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "Existing vision systems for autonomous driving or robots are sensitive to\nwaterdrops adhered to windows or camera lenses. Most recent waterdrop removal\napproaches take a single image as input and often fail to recover the missing\ncontent behind waterdrops faithfully. Thus, we propose a learning-based model\nfor waterdrop removal with stereo images. To better detect and remove\nwaterdrops from stereo images, we propose a novel row-wise dilated attention\nmodule to enlarge attention's receptive field for effective information\npropagation between the two stereo images. In addition, we propose an attention\nconsistency loss between the ground-truth disparity map and attention scores to\nenhance the left-right consistency in stereo images. Because of related\ndatasets' unavailability, we collect a real-world dataset that contains stereo\nimages with and without waterdrops. Extensive experiments on our dataset\nsuggest that our model outperforms state-of-the-art methods both quantitatively\nand qualitatively. Our source code and the stereo waterdrop dataset are\navailable at\n\\href{https://github.com/VivianSZF/Stereo-Waterdrop-Removal}{https://github.com/VivianSZF/Stereo-Waterdrop-Removal}",
          "link": "http://arxiv.org/abs/2108.03457",
          "publishedOn": "2021-08-10T02:00:11.705Z",
          "wordCount": null,
          "title": "Stereo Waterdrop Removal with Row-wise Dilated Attention. (arXiv:2108.03457v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.00909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Lingfei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dongrui Wu</a>",
          "description": "Transfer learning (TL) utilizes data or knowledge from one or more source\ndomains to facilitate the learning in a target domain. It is particularly\nuseful when the target domain has very few or no labeled data, due to\nannotation expense, privacy concerns, etc. Unfortunately, the effectiveness of\nTL is not always guaranteed. Negative transfer (NT), i.e., leveraging source\ndomain data/knowledge undesirably reduces the learning performance in the\ntarget domain, has been a long-standing and challenging problem in TL. Various\napproaches have been proposed in the literature to handle it. However, there\ndoes not exist a systematic survey on the formulation of NT, the factors\nleading to NT, and the algorithms that mitigate NT. This paper fills this gap,\nby first introducing the definition of NT and its factors, then reviewing about\nfifty representative approaches for overcoming NT, according to four\ncategories: secure transfer, domain similarity estimation, distant transfer,\nand NT mitigation. NT in related fields, e.g., multi-task learning, lifelong\nlearning, and adversarial attacks, are also discussed.",
          "link": "http://arxiv.org/abs/2009.00909",
          "publishedOn": "2021-08-10T02:00:11.705Z",
          "wordCount": null,
          "title": "A Survey on Negative Transfer. (arXiv:2009.00909v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saha_T/0/1/0/all/0/1\">Tanujay Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aaraj_N/0/1/0/all/0/1\">Najwa Aaraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_N/0/1/0/all/0/1\">Niraj K. Jha</a>",
          "description": "The core network architecture of telecommunication systems has undergone a\nparadigm shift in the fifth-generation (5G)networks. 5G networks have\ntransitioned to software-defined infrastructures, thereby reducing their\ndependence on hardware-based network functions. New technologies, like network\nfunction virtualization and software-defined networking, have been incorporated\nin the 5G core network (5GCN) architecture to enable this transition. This has\nresulted in significant improvements in efficiency, performance, and robustness\nof the networks. However, this has also made the core network more vulnerable,\nas software systems are generally easier to compromise than hardware systems.\nIn this article, we present a comprehensive security analysis framework for the\n5GCN. The novelty of this approach lies in the creation and analysis of attack\ngraphs of the software-defined and virtualized 5GCN through machine learning.\nThis analysis points to 119 novel possible exploits in the 5GCN. We demonstrate\nthat these possible exploits of 5GCN vulnerabilities generate five novel\nattacks on the 5G Authentication and Key Agreement protocol. We combine the\nattacks at the network, protocol, and the application layers to generate\ncomplex attack vectors. In a case study, we use these attack vectors to find\nfour novel security loopholes in WhatsApp running on a 5G network.",
          "link": "http://arxiv.org/abs/2108.03514",
          "publishedOn": "2021-08-10T02:00:11.704Z",
          "wordCount": null,
          "title": "Machine Learning Assisted Security Analysis of 5G-Network-Connected Systems. (arXiv:2108.03514v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2010.05690",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+B_L/0/1/0/all/0/1\">Lalith Bharadwaj B</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boddeda_R/0/1/0/all/0/1\">Rohit Boddeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1\">Sai Vardhan K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_M/0/1/0/all/0/1\">Madhu G</a>",
          "description": "The issue of COVID-19, increasing with a massive mortality rate. This led to\nthe WHO declaring it as a pandemic. In this situation, it is crucial to perform\nefficient and fast diagnosis. The reverse transcript polymerase chain reaction\n(RTPCR) test is conducted to detect the presence of SARS-CoV-2. This test is\ntime-consuming and instead chest CT (or Chest X-ray) can be used for a fast and\naccurate diagnosis. Automated diagnosis is considered to be important as it\nreduces human effort and provides accurate and low-cost tests. The\ncontributions of our research are three-fold. First, it is aimed to analyse the\nbehaviour and performance of variant vision models ranging from Inception to\nNAS networks with the appropriate fine-tuning procedure. Second, the behaviour\nof these models is visually analysed by plotting CAMs for individual networks\nand determining classification performance with AUCROC curves. Thirdly, stacked\nensembles techniques are imparted to provide higher generalisation on combining\nthe fine-tuned models, in which six ensemble neural networks are designed by\ncombining the existing fine-tuned networks. Implying these stacked ensembles\nprovides a great generalization to the models. The ensemble model designed by\ncombining all the fine-tuned networks obtained a state-of-the-art accuracy\nscore of 99.17%. The precision and recall for the COVID-19 class are 99.99% and\n89.79% respectively, which resembles the robustness of the stacked ensembles.",
          "link": "http://arxiv.org/abs/2010.05690",
          "publishedOn": "2021-08-10T02:00:11.704Z",
          "wordCount": null,
          "title": "COVID-19 Classification Using Staked Ensembles: A Comprehensive Analysis. (arXiv:2010.05690v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">An Nguyen</a>",
          "description": "Although current state-of-the-art language models have achieved impressive\nresults in numerous natural language processing tasks, still they could not\nsolve the problem of producing repetitive, dull and sometimes inconsistent text\nin open-ended text generation. Studies often attribute this problem to the\nmaximum likelihood training objective, and propose alternative approaches by\nusing stochastic decoding methods or altering the training objective. However,\nthere is still a lack of consistent evaluation metrics to directly compare the\nefficacy of these solutions. In this work, we study different evaluation\nmetrics that have been proposed to evaluate quality, diversity and consistency\nof machine-generated text. From there, we propose a practical pipeline to\nevaluate language models in open-ended generation task, and research on how to\nimprove the model's performance in all dimensions by leveraging different\nauxiliary training objectives.",
          "link": "http://arxiv.org/abs/2108.03578",
          "publishedOn": "2021-08-10T02:00:11.703Z",
          "wordCount": null,
          "title": "Language Model Evaluation in Open-ended Text Generation. (arXiv:2108.03578v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03706",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ramprasad_P/0/1/0/all/0/1\">Pratik Ramprasad</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1\">Yuantong Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_W/0/1/0/all/0/1\">Will Wei Sun</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cheng_G/0/1/0/all/0/1\">Guang Cheng</a>",
          "description": "The recent emergence of reinforcement learning has created a demand for\nrobust statistical inference methods for the parameter estimates computed using\nthese algorithms. Existing methods for statistical inference in online learning\nare restricted to settings involving independently sampled observations, while\nexisting statistical inference methods in reinforcement learning (RL) are\nlimited to the batch setting. The online bootstrap is a flexible and efficient\napproach for statistical inference in linear stochastic approximation\nalgorithms, but its efficacy in settings involving Markov noise, such as RL,\nhas yet to be explored. In this paper, we study the use of the online bootstrap\nmethod for statistical inference in RL. In particular, we focus on the temporal\ndifference (TD) learning and Gradient TD (GTD) learning algorithms, which are\nthemselves special instances of linear stochastic approximation under Markov\nnoise. The method is shown to be distributionally consistent for statistical\ninference in policy evaluation, and numerical experiments are included to\ndemonstrate the effectiveness of this algorithm at statistical inference tasks\nacross a range of real RL environments.",
          "link": "http://arxiv.org/abs/2108.03706",
          "publishedOn": "2021-08-10T02:00:11.702Z",
          "wordCount": null,
          "title": "Online Bootstrap Inference For Policy Evaluation in Reinforcement Learning. (arXiv:2108.03706v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03322",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Clement_C/0/1/0/all/0/1\">Colin B. Clement</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>",
          "description": "Pre-trained transformers have recently clinched top spots in the gamut of\nnatural language tasks and pioneered solutions to software engineering tasks.\nEven information retrieval has not been immune to the charm of the transformer,\nthough their large size and cost is generally a barrier to deployment. While\nthere has been much work in streamlining, caching, and modifying transformer\narchitectures for production, here we explore a new direction: distilling a\nlarge pre-trained translation model into a lightweight bi-encoder which can be\nefficiently cached and queried. We argue from a probabilistic perspective that\nsequence-to-sequence models are a conceptually ideal---albeit highly\nimpractical---retriever. We derive a new distillation objective, implementing\nit as a data augmentation scheme. Using natural language source code search as\na case study for cross-domain search, we demonstrate the validity of this idea\nby significantly improving upon the current leader of the CodeSearchNet\nchallenge, a recent natural language code search benchmark.",
          "link": "http://arxiv.org/abs/2108.03322",
          "publishedOn": "2021-08-10T02:00:11.701Z",
          "wordCount": null,
          "title": "Distilling Transformers for Neural Cross-Domain Search. (arXiv:2108.03322v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tolpin_D/0/1/0/all/0/1\">David Tolpin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobkin_T/0/1/0/all/0/1\">Tomer Dobkin</a>",
          "description": "Agent preferences should be specified stochastically rather than\ndeterministically. Planning as inference with stochastic preferences naturally\ndescribes agent behaviors, does not require introducing rewards and exponential\nweighing of behaviors, and allows to reason about agents using the solid\nfoundation of Bayesian statistics. Stochastic conditioning is the formalism\nbehind agents with stochastic preferences.",
          "link": "http://arxiv.org/abs/2108.03834",
          "publishedOn": "2021-08-10T02:00:11.700Z",
          "wordCount": 491,
          "title": "Bob and Alice Go to a Bar: Reasoning About Future With Probabilistic Programs. (arXiv:2108.03834v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03411",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Munoz_Gil_G/0/1/0/all/0/1\">Gorka Mu&#xf1;oz-Gil</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Corominas_G/0/1/0/all/0/1\">Guillem Guig&#xf3; i Corominas</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Lewenstein_M/0/1/0/all/0/1\">Maciej Lewenstein</a>",
          "description": "The characterization of diffusion processes is a keystone in our\nunderstanding of a variety of physical phenomena. Many of these deviate from\nBrownian motion, giving rise to anomalous diffusion. Various theoretical models\nexists nowadays to describe such processes, but their application to\nexperimental setups is often challenging, due to the stochastic nature of the\nphenomena and the difficulty to harness reliable data. The latter often\nconsists on short and noisy trajectories, which are hard to characterize with\nusual statistical approaches. In recent years, we have witnessed an impressive\neffort to bridge theory and experiments by means of supervised machine learning\ntechniques, with astonishing results. In this work, we explore the use of\nunsupervised methods in anomalous diffusion data. We show that the main\ndiffusion characteristics can be learnt without the need of any labelling of\nthe data. We use such method to discriminate between anomalous diffusion models\nand extract their physical parameters. Moreover, we explore the feasibility of\nfinding novel types of diffusion, in this case represented by compositions of\nexisting diffusion models. At last, we showcase the use of the method in\nexperimental data and demonstrate its advantages for cases where supervised\nlearning is not applicable.",
          "link": "http://arxiv.org/abs/2108.03411",
          "publishedOn": "2021-08-10T02:00:11.679Z",
          "wordCount": 651,
          "title": "Unsupervised learning of anomalous diffusion data. (arXiv:2108.03411v1 [cond-mat.stat-mech])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_E/0/1/0/all/0/1\">Elbert Du</a>",
          "description": "In this work, we showed that the Implicit Update and Predictive Methods\ndynamics introduced in prior work satisfy last iterate convergence to a\nneighborhood around the optimum in overparameterized GANs, where the size of\nthe neighborhood shrinks with the width of the neural network. This is in\ncontrast to prior results, which only guaranteed average iterate convergence.",
          "link": "http://arxiv.org/abs/2108.03491",
          "publishedOn": "2021-08-10T02:00:11.672Z",
          "wordCount": 479,
          "title": "Approximate Last Iterate Convergence in Overparameterized GANs. (arXiv:2108.03491v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03348",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_M/0/1/0/all/0/1\">Md Shamim Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1\">Mohammed J. Zaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_D/0/1/0/all/0/1\">Dharmashankar Subramanian</a>",
          "description": "Transformer neural networks have achieved state-of-the-art results for\nunstructured data such as text and images but their adoption for\ngraph-structured data has been limited. This is partly due to the difficulty in\nincorporating complex structural information in the basic transformer\nframework. We propose a simple yet powerful extension to the transformer -\nresidual edge channels. The resultant framework, which we call Edge-augmented\nGraph Transformer (EGT), can directly accept, process and output structural\ninformation as well as node information. This simple addition allows us to use\nglobal self-attention, the key element of transformers, directly for graphs and\ncomes with the benefit of long-range interaction among nodes. Moreover, the\nedge channels allow the structural information to evolve from layer to layer,\nand prediction tasks on edges can be derived directly from these channels. In\naddition to that, we introduce positional encodings based on Singular Value\nDecomposition which can improve the performance of EGT. Our framework, which\nrelies on global node feature aggregation, achieves better performance compared\nto Graph Convolutional Networks (GCN), which rely on local feature aggregation\nwithin a neighborhood. We verify the performance of EGT in a supervised\nlearning setting on a wide range of experiments on benchmark datasets. Our\nfindings indicate that convolutional aggregation is not an essential inductive\nbias for graphs and global self-attention can serve as a flexible and adaptive\nalternative to graph convolution.",
          "link": "http://arxiv.org/abs/2108.03348",
          "publishedOn": "2021-08-10T02:00:11.665Z",
          "wordCount": 665,
          "title": "Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs. (arXiv:2108.03348v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_K/0/1/0/all/0/1\">Kyung-bin Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>",
          "description": "Effectively operating electrical vehicle charging station (EVCS) is crucial\nfor enabling the rapid transition of electrified transportation. To solve this\nproblem using reinforcement learning (RL), the dimension of state/action spaces\nscales with the number of EVs and is thus very large and time-varying. This\ndimensionality issue affects the efficiency and convergence properties of\ngeneric RL algorithms. We develop aggregation schemes that are based on the\nemergency of EV charging, namely the laxity value. A least-laxity first (LLF)\nrule is adopted to consider only the total charging power of the EVCS which\nensures the feasibility of individual EV schedules. In addition, we propose an\nequivalent state aggregation that can guarantee to attain the same optimal\npolicy. Based on the proposed representation, policy gradient method is used to\nfind the best parameters for the linear Gaussian policy . Numerical results\nhave validated the performance improvement of the proposed representation\napproaches in attaining higher rewards and more effective policies as compared\nto existing approximation based approach.",
          "link": "http://arxiv.org/abs/2108.03236",
          "publishedOn": "2021-08-10T02:00:11.596Z",
          "wordCount": 601,
          "title": "Efficient Representation for Electric Vehicle Charging Station Operations using Reinforcement Learning. (arXiv:2108.03236v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Souri_Y/0/1/0/all/0/1\">Yaser Souri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farha_Y/0/1/0/all/0/1\">Yazan Abu Farha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Despinoy_F/0/1/0/all/0/1\">Fabien Despinoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francesca_G/0/1/0/all/0/1\">Gianpiero Francesca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>",
          "description": "We introduce FIFA, a fast approximate inference method for action\nsegmentation and alignment. Unlike previous approaches, FIFA does not rely on\nexpensive dynamic programming for inference. Instead, it uses an approximate\ndifferentiable energy function that can be minimized using gradient-descent.\nFIFA is a general approach that can replace exact inference improving its speed\nby more than 5 times while maintaining its performance. FIFA is an anytime\ninference algorithm that provides a better speed vs. accuracy trade-off\ncompared to exact inference. We apply FIFA on top of state-of-the-art\napproaches for weakly supervised action segmentation and alignment as well as\nfully supervised action segmentation. FIFA achieves state-of-the-art results on\nmost metrics on two action segmentation datasets.",
          "link": "http://arxiv.org/abs/2108.03894",
          "publishedOn": "2021-08-10T02:00:11.460Z",
          "wordCount": null,
          "title": "FIFA: Fast Inference Approximation for Action Segmentation. (arXiv:2108.03894v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Garcia_M/0/1/0/all/0/1\">Miguel Ruiz-Garcia</a>",
          "description": "The work of McCloskey and Cohen popularized the concept of catastrophic\ninterference. They used a neural network that tried to learn addition using two\ngroups of examples as two different tasks. In their case, learning the second\ntask rapidly deteriorated the acquired knowledge about the previous one. This\ncould be a symptom of a fundamental problem: addition is an algorithmic task\nthat should not be learned through pattern recognition. We propose to use a\nneural network with a different architecture that can be trained to recover the\ncorrect algorithm for the addition of binary numbers. We test it in the setting\nproposed by McCloskey and Cohen and training on random examples one by one. The\nneural network not only does not suffer from catastrophic forgetting but it\nimproves its predictive power on unseen pairs of numbers as training\nprogresses. This work emphasizes the importance that neural network\narchitecture has for the emergence of catastrophic forgetting and introduces a\nneural network that is able to learn an algorithm.",
          "link": "http://arxiv.org/abs/2108.03940",
          "publishedOn": "2021-08-10T02:00:11.460Z",
          "wordCount": null,
          "title": "Some thoughts on catastrophic forgetting and how to learn an algorithm. (arXiv:2108.03940v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ashwin Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Adriel Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Steven QH Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1\">Du Nguyen Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>",
          "description": "Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.",
          "link": "http://arxiv.org/abs/2106.14463",
          "publishedOn": "2021-08-10T02:00:11.452Z",
          "wordCount": null,
          "title": "RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenyi Zhang</a>",
          "description": "Decentralized federated learning (DFL) is a powerful framework of distributed\nmachine learning and decentralized stochastic gradient descent (SGD) is a\ndriving engine for DFL. The performance of decentralized SGD is jointly\ninfluenced by communication-efficiency and convergence rate. In this paper, we\npropose a general decentralized federated learning framework to strike a\nbalance between communication-efficiency and convergence performance. The\nproposed framework performs both multiple local updates and multiple inter-node\ncommunications periodically, unifying traditional decentralized SGD methods. We\nestablish strong convergence guarantees for the proposed DFL algorithm without\nthe assumption of convex objective function. The balance of communication and\ncomputation rounds is essential to optimize decentralized federated learning\nunder constrained communication and computation resources. For further\nimproving communication-efficiency of DFL, compressed communication is applied\nto DFL, named DFL with compressed communication (C-DFL). The proposed C-DFL\nexhibits linear convergence for strongly convex objectives. Experiment results\nbased on MNIST and CIFAR-10 datasets illustrate the superiority of DFL over\ntraditional decentralized SGD methods and show that C-DFL further enhances\ncommunication-efficiency.",
          "link": "http://arxiv.org/abs/2107.12048",
          "publishedOn": "2021-08-10T02:00:11.405Z",
          "wordCount": null,
          "title": "Decentralized Federated Learning: Balancing Communication and Computing Costs. (arXiv:2107.12048v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>",
          "description": "Recent studies unveil the vulnerabilities of deep ranking models, where an\nimperceptible perturbation can trigger dramatic changes in the ranking result.\nWhile previous attempts focus on manipulating absolute ranks of certain\ncandidates, the possibility of adjusting their relative order remains\nunder-explored. In this paper, we formulate a new adversarial attack against\ndeep ranking systems, i.e., the Order Attack, which covertly alters the\nrelative order among a selected set of candidates according to an\nattacker-specified permutation, with limited interference to other unrelated\ncandidates. Specifically, it is formulated as a triplet-style loss imposing an\ninequality chain reflecting the specified permutation. However, direct\noptimization of such white-box objective is infeasible in a real-world attack\nscenario due to various black-box limitations. To cope with them, we propose a\nShort-range Ranking Correlation metric as a surrogate objective for black-box\nOrder Attack to approximate the white-box method. The Order Attack is evaluated\non the Fashion-MNIST and Stanford-Online-Products datasets under both white-box\nand black-box threat models. The black-box attack is also successfully\nimplemented on a major e-commerce platform. Comprehensive experimental\nevaluations demonstrate the effectiveness of the proposed methods, revealing a\nnew type of ranking model vulnerability.",
          "link": "http://arxiv.org/abs/2103.05248",
          "publishedOn": "2021-08-10T02:00:11.403Z",
          "wordCount": null,
          "title": "Practical Relative Order Attack in Deep Ranking. (arXiv:2103.05248v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zekai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1\">Fangtian Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pless_R/0/1/0/all/0/1\">Robert Pless</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiuzhen Cheng</a>",
          "description": "User response prediction, which aims to predict the probability that a user\nwill provide a predefined positive response in a given context such as clicking\non an ad or purchasing an item, is crucial to many industrial applications such\nas online advertising, recommender systems, and search ranking. However, due to\nthe high dimensionality and super sparsity of the data collected in these\ntasks, handcrafting cross features is inevitably time expensive. Prior studies\nin predicting user response leveraged the feature interactions by enhancing\nfeature vectors with products of features to model second-order or high-order\ncross features, either explicitly or implicitly. Nevertheless, these existing\nmethods can be hindered by not learning sufficient cross features due to model\narchitecture limitations or modeling all high-order feature interactions with\nequal weights. This work aims to fill this gap by proposing a novel\narchitecture Deep Cross Attentional Product Network (DCAP), which keeps cross\nnetwork's benefits in modeling high-order feature interactions explicitly at\nthe vector-wise level. Beyond that, it can differentiate the importance of\ndifferent cross features in each network layer inspired by the multi-head\nattention mechanism and Product Neural Network (PNN), allowing practitioners to\nperform a more in-depth analysis of user behaviors. Additionally, our proposed\nmodel can be easily implemented and train in parallel. We conduct comprehensive\nexperiments on three real-world datasets. The results have robustly\ndemonstrated that our proposed model DCAP achieves superior prediction\nperformance compared with the state-of-the-art models.",
          "link": "http://arxiv.org/abs/2105.08649",
          "publishedOn": "2021-08-10T02:00:11.403Z",
          "wordCount": null,
          "title": "DCAP: Deep Cross Attentional Product Network for User Response Prediction. (arXiv:2105.08649v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fujiwara_T/0/1/0/all/0/1\">Takanori Fujiwara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xinhai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kwan-Liu Ma</a>",
          "description": "Finding the similarities and differences between groups of datasets is a\nfundamental analysis task. For high-dimensional data, dimensionality reduction\n(DR) methods are often used to find the characteristics of each group. However,\nexisting DR methods provide limited capability and flexibility for such\ncomparative analysis as each method is designed only for a narrow analysis\ntarget, such as identifying factors that most differentiate groups. This paper\npresents an interactive DR framework where we integrate our new DR method,\ncalled ULCA (unified linear comparative analysis), with an interactive visual\ninterface. ULCA unifies two DR schemes, discriminant analysis and contrastive\nlearning, to support various comparative analysis tasks. To provide flexibility\nfor comparative analysis, we develop an optimization algorithm that enables\nanalysts to interactively refine ULCA results. Additionally, the interactive\nvisualization interface facilitates interpretation and refinement of the ULCA\nresults. We evaluate ULCA and the optimization algorithm to show their\nefficiency as well as present multiple case studies using real-world datasets\nto demonstrate the usefulness of this framework.",
          "link": "http://arxiv.org/abs/2106.15481",
          "publishedOn": "2021-08-10T02:00:11.346Z",
          "wordCount": null,
          "title": "Interactive Dimensionality Reduction for Comparative Analysis. (arXiv:2106.15481v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_E/0/1/0/all/0/1\">Eric Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tu Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swaminathan_V/0/1/0/all/0/1\">Vishy Swaminathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>",
          "description": "Images tell powerful stories but cannot always be trusted. Matching images\nback to trusted sources (attribution) enables users to make a more informed\njudgment of the images they encounter online. We propose a robust image hashing\nalgorithm to perform such matching. Our hash is sensitive to manipulation of\nsubtle, salient visual details that can substantially change the story told by\nan image. Yet the hash is invariant to benign transformations (changes in\nquality, codecs, sizes, shapes, etc.) experienced by images during online\nredistribution. Our key contribution is OSCAR-Net (Object-centric Scene Graph\nAttention for Image Attribution Network); a robust image hashing model inspired\nby recent successes of Transformers in the visual domain. OSCAR-Net constructs\na scene graph representation that attends to fine-grained changes of every\nobject's visual appearance and their spatial relationships. The network is\ntrained via contrastive learning on a dataset of original and manipulated\nimages yielding a state of the art image hash for content fingerprinting that\nscales to millions of images.",
          "link": "http://arxiv.org/abs/2108.03541",
          "publishedOn": "2021-08-10T02:00:11.344Z",
          "wordCount": null,
          "title": "OSCAR-Net: Object-centric Scene Graph Attention for Image Attribution. (arXiv:2108.03541v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sow_B/0/1/0/all/0/1\">Boubacar Sow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suguri_H/0/1/0/all/0/1\">Hiroki Suguri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhtar_H/0/1/0/all/0/1\">Hamid Mukhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_H/0/1/0/all/0/1\">Hafiz Farooq Ahmad</a>",
          "description": "Integrating machine learning techniques in healthcare becomes very common\nnowadays, and it contributes positively to improving clinical care and health\ndecisions planning. Anemia and malaria are two life-threatening diseases in\nAfrica that affect the red blood cells and reduce hemoglobin production. This\npaper focuses on analyzing child health data in Senegal using four machine\nlearning algorithms in Python: KNN, Random Forests, SVM, and Na\\\"ive Bayes. Our\ntask aims to investigate large-scale data from The Demographic and Health\nSurvey (DHS) and to find out hidden information for anemia and malaria. We\npresent two classification models for the two blood disorders using biological\nvariables and social determinants. The findings of this research will\ncontribute to improving child healthcare in Senegal by eradicating anemia and\nmalaria, and decreasing the child mortality rate.",
          "link": "http://arxiv.org/abs/2108.03601",
          "publishedOn": "2021-08-10T02:00:11.344Z",
          "wordCount": null,
          "title": "Using Biological Variables and Social Determinants to Predict Malaria and Anemia among Children in Senegal. (arXiv:2108.03601v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.06400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Awad_N/0/1/0/all/0/1\">Noor Awad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallik_N/0/1/0/all/0/1\">Neeratyoy Mallik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>",
          "description": "Neural architecture search (NAS) methods rely on a search strategy for\ndeciding which architectures to evaluate next and a performance estimation\nstrategy for assessing their performance (e.g., using full evaluations,\nmulti-fidelity evaluations, or the one-shot model). In this paper, we focus on\nthe search strategy. We introduce the simple yet powerful evolutionary\nalgorithm of differential evolution to the NAS community. Using the simplest\nperformance evaluation strategy of full evaluations, we comprehensively compare\nthis search strategy to regularized evolution and Bayesian optimization and\ndemonstrate that it yields improved and more robust results for 13 tabular NAS\nbenchmarks based on NAS-Bench-101, NAS-Bench-1Shot1, NAS-Bench-201 and NAS-HPO\nbench.",
          "link": "http://arxiv.org/abs/2012.06400",
          "publishedOn": "2021-08-10T02:00:11.344Z",
          "wordCount": null,
          "title": "Differential Evolution for Neural Architecture Search. (arXiv:2012.06400v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mahnaz Rafia Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenny_I/0/1/0/all/0/1\">Israt Jahan Jenny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayon_M/0/1/0/all/0/1\">Moniruzzaman Nayon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md. Rajibul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiruzzaman_M/0/1/0/all/0/1\">Md Amiruzzaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdullah_Al_Wadud_M/0/1/0/all/0/1\">M. Abdullah-Al-Wadud</a>",
          "description": "Selecting an appropriate clustering method as well as an optimal number of\nclusters in road accident data is at times confusing and difficult. This paper\nanalyzes shortcomings of different existing techniques applied to cluster\naccident-prone areas and recommends using Density-Based Spatial Clustering of\nApplications with Noise (DBSCAN) and Ordering Points To Identify the Clustering\nStructure (OPTICS) to overcome them. Comparative performance analysis based on\nreal-life data on the recorded cases of road accidents in North Carolina also\nshow more effectiveness and efficiency achieved by these algorithms.",
          "link": "http://arxiv.org/abs/2108.03490",
          "publishedOn": "2021-08-10T02:00:11.316Z",
          "wordCount": null,
          "title": "Clustering Algorithms to Analyze the Road Traffic Crashes. (arXiv:2108.03490v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03506",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagmar_A/0/1/0/all/0/1\">Aadesh Bagmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiya_S/0/1/0/all/0/1\">Shishira R Maiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bidwalka_S/0/1/0/all/0/1\">Shruti Bidwalka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Amol Deshpande</a>",
          "description": "The vulnerability of the Lottery Ticket Hypothesis has not been studied from\nthe purview of Membership Inference Attacks. Through this work, we are the\nfirst to empirically show that the lottery ticket networks are equally\nvulnerable to membership inference attacks. A Membership Inference Attack (MIA)\nis the process of determining whether a data sample belongs to a training set\nof a trained model or not. Membership Inference Attacks could leak critical\ninformation about the training data that can be used for targeted attacks.\nRecent deep learning models often have very large memory footprints and a high\ncomputational cost associated with training and drawing inferences. Lottery\nTicket Hypothesis is used to prune the networks to find smaller sub-networks\nthat at least match the performance of the original model in terms of test\naccuracy in a similar number of iterations. We used CIFAR-10, CIFAR-100, and\nImageNet datasets to perform image classification tasks and observe that the\nattack accuracies are similar. We also see that the attack accuracy varies\ndirectly according to the number of classes in the dataset and the sparsity of\nthe network. We demonstrate that these attacks are transferable across models\nwith high accuracy.",
          "link": "http://arxiv.org/abs/2108.03506",
          "publishedOn": "2021-08-10T02:00:11.315Z",
          "wordCount": null,
          "title": "Membership Inference Attacks on Lottery Ticket Networks. (arXiv:2108.03506v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karn_A/0/1/0/all/0/1\">Aryan Karn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1\">Ashutosh Acharya</a>",
          "description": "We present a study of the manners by which Domain information has been\nincorporated when building models with Neural Networks. Integrating space data\nis uniquely important to the development of Knowledge understanding model, as\nwell as other fields that aid in understanding information by utilizing the\nhuman-machine interface and Reinforcement Learning. On numerous such occasions,\nmachine-based model development may profit essentially from the human\ninformation on the world encoded in an adequately exact structure. This paper\ninspects expansive ways to affect encode such information as sensible and\nmathematical limitations and portrays methods and results that came to a couple\nof subcategories under all of those methodologies.",
          "link": "http://arxiv.org/abs/2107.14613",
          "publishedOn": "2021-08-10T02:00:11.308Z",
          "wordCount": null,
          "title": "Incorporation of Deep Neural Network & Reinforcement Learning with Domain Knowledge. (arXiv:2107.14613v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04235",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Schulze_S/0/1/0/all/0/1\">S&#xf6;ren Schulze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leuschner_J/0/1/0/all/0/1\">Johannes Leuschner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+King_E/0/1/0/all/0/1\">Emily J. King</a>",
          "description": "We propose a method for the blind separation of sounds of musical instruments\nin audio signals. We describe the individual tones via a parametric model,\ntraining a dictionary to capture the relative amplitudes of the harmonics. The\nmodel parameters are predicted via a U-Net, which is a type of deep neural\nnetwork. The network is trained without ground truth information, based on the\ndifference between the model prediction and the individual time frames of the\nshort-time Fourier transform. Since some of the model parameters do not yield a\nuseful backpropagation gradient, we model them stochastically and employ the\npolicy gradient instead. To provide phase information and account for\ninaccuracies in the dictionary-based representation, we also let the network\noutput a direct prediction, which we then use to resynthesize the audio signals\nfor the individual instruments. Due to the flexibility of the neural network,\ninharmonicity can be incorporated seamlessly and no preprocessing of the input\nspectra is required. Our algorithm yields high-quality separation results with\nparticularly low interference on a variety of different audio samples, both\nacoustic and synthetic, provided that the sample contains enough data for the\ntraining and that the spectral characteristics of the musical instruments are\nsufficiently stable to be approximated by the dictionary.",
          "link": "http://arxiv.org/abs/2107.04235",
          "publishedOn": "2021-08-10T02:00:11.290Z",
          "wordCount": null,
          "title": "Blind Source Separation in Polyphonic Music Recordings Using Deep Neural Networks Trained via Policy Gradients. (arXiv:2107.04235v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ashesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1\">Luca Del Pero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1\">Hugo Grimmett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1\">Peter Ondruska</a>",
          "description": "Despite the numerous successes of machine learning over the past decade\n(image recognition, decision-making, NLP, image synthesis), self-driving\ntechnology has not yet followed the same trend. In this paper, we study the\nhistory, composition, and development bottlenecks of the modern self-driving\nstack. We argue that the slow progress is caused by approaches that require too\nmuch hand-engineering, an over-reliance on road testing, and high fleet\ndeployment costs. We observe that the classical stack has several bottlenecks\nthat preclude the necessary scale needed to capture the long tail of rare\nevents. To resolve these problems, we outline the principles of Autonomy 2.0,\nan ML-first approach to self-driving, as a viable alternative to the currently\nadopted state-of-the-art. This approach is based on (i) a fully differentiable\nAV stack trainable from human demonstrations, (ii) closed-loop data-driven\nreactive simulation, and (iii) large-scale, low-cost data collections as\ncritical solutions towards scalability issues. We outline the general\narchitecture, survey promising works in this direction and propose key\nchallenges to be addressed by the community in the future.",
          "link": "http://arxiv.org/abs/2107.08142",
          "publishedOn": "2021-08-10T02:00:11.290Z",
          "wordCount": null,
          "title": "Autonomy 2.0: Why is self-driving always 5 years away?. (arXiv:2107.08142v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.11769",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Ohn_I/0/1/0/all/0/1\">Ilsang Ohn</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kim_Y/0/1/0/all/0/1\">Yongdai Kim</a>",
          "description": "Recent theoretical studies proved that deep neural network (DNN) estimators\nobtained by minimizing empirical risk with a certain sparsity constraint can\nattain optimal convergence rates for regression and classification problems.\nHowever, the sparsity constraint requires to know certain properties of the\ntrue model, which are not available in practice. Moreover, computation is\ndifficult due to the discrete nature of the sparsity constraint. In this paper,\nwe propose a novel penalized estimation method for sparse DNNs, which resolves\nthe aforementioned problems existing in the sparsity constraint. We establish\nan oracle inequality for the excess risk of the proposed sparse-penalized DNN\nestimator and derive convergence rates for several learning tasks. In\nparticular, we prove that the sparse-penalized estimator can adaptively attain\nminimax convergence rates for various nonparametric regression problems. For\ncomputation, we develop an efficient gradient-based optimization algorithm that\nguarantees the monotonic reduction of the objective function.",
          "link": "http://arxiv.org/abs/2003.11769",
          "publishedOn": "2021-08-10T02:00:11.288Z",
          "wordCount": null,
          "title": "Nonconvex sparse regularization for deep neural networks and its optimality. (arXiv:2003.11769v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.15082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1\">Rui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Le Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xianyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiamang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Di Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>",
          "description": "Mixture-of-Experts (MoE) models can achieve promising results with outrageous\nlarge amount of parameters but constant computation cost, and thus it has\nbecome a trend in model scaling. Still it is a mystery how MoE layers bring\nquality gains by leveraging the parameters with sparse activation. In this\nwork, we investigate several key factors in sparse expert models. We observe\nthat load imbalance may not be a significant problem affecting model quality,\ncontrary to the perspectives of recent studies, while the number of sparsely\nactivated experts $k$ and expert capacity $C$ in top-$k$ routing can\nsignificantly make a difference in this context. Furthermore, we take a step\nforward to propose a simple method called expert prototyping that splits\nexperts into different prototypes and applies $k$ top-$1$ routing. This\nstrategy improves the model quality but maintains constant computational costs,\nand our further exploration on extremely large-scale models reflects that it is\nmore effective in training larger models. We push the model scale to over $1$\ntrillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in\ncomparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model\nachieves substantial speedup in convergence over the same-size baseline.",
          "link": "http://arxiv.org/abs/2105.15082",
          "publishedOn": "2021-08-10T02:00:11.276Z",
          "wordCount": null,
          "title": "M6-T: Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Ding Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_P/0/1/0/all/0/1\">Parth H. Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zizhan Zheng</a>",
          "description": "We consider a set of APs with unknown data rates that cooperatively serve a\nmobile client. The data rate of each link is i.i.d. sampled from a distribution\nthat is unknown a priori. In contrast to traditional link scheduling problems\nunder uncertainty, we assume that in each time step, the device can probe a\nsubset of links before deciding which one to use. We model this problem as a\ncontextual bandit problem with probing (CBwP) and present an efficient\nalgorithm. We further establish the regret of our algorithm for links with\nBernoulli data rates. Our CBwP model is a novel extension of the classic\ncontextual bandit model and can potentially be applied to a large class of\nsequential decision-making problems that involve joint probing and play under\nuncertainty.",
          "link": "http://arxiv.org/abs/2108.03297",
          "publishedOn": "2021-08-10T02:00:11.215Z",
          "wordCount": null,
          "title": "Joint AP Probing and Scheduling: A Contextual Bandit Approach. (arXiv:2108.03297v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheebaelhamd_Z/0/1/0/all/0/1\">Ziyad Sheebaelhamd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisis_K/0/1/0/all/0/1\">Konstantinos Zisis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nisioti_A/0/1/0/all/0/1\">Athina Nisioti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkouletsos_D/0/1/0/all/0/1\">Dimitris Gkouletsos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavllo_D/0/1/0/all/0/1\">Dario Pavllo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1\">Jonas Kohler</a>",
          "description": "Multi-agent control problems constitute an interesting area of application\nfor deep reinforcement learning models with continuous action spaces. Such\nreal-world applications, however, typically come with critical safety\nconstraints that must not be violated. In order to ensure safety, we enhance\nthe well-known multi-agent deep deterministic policy gradient (MADDPG)\nframework by adding a safety layer to the deep policy network. %which\nautomatically corrects invalid actions. In particular, we extend the idea of\nlinearizing the single-step transition dynamics, as was done for single-agent\nsystems in Safe DDPG (Dalal et al., 2018), to multi-agent settings. We\nadditionally propose to circumvent infeasibility problems in the action\ncorrection step using soft constraints (Kerrigan & Maciejowski, 2000). Results\nfrom the theory of exact penalty functions can be used to guarantee constraint\nsatisfaction of the soft constraints under mild assumptions. We empirically\nfind that the soft formulation achieves a dramatic decrease in constraint\nviolations, making safety available even during the learning procedure.",
          "link": "http://arxiv.org/abs/2108.03952",
          "publishedOn": "2021-08-10T02:00:11.119Z",
          "wordCount": null,
          "title": "Safe Deep Reinforcement Learning for Multi-Agent Systems with Continuous Action Spaces. (arXiv:2108.03952v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingming Liu</a>",
          "description": "Recently, there has been an increasing interest in the roll-out of electric\nvehicles (EVs) in the global automotive market. Compared to conventional\ninternal combustion engine vehicles (ICEVs), EVs can not only help users reduce\nmonetary costs in their daily commuting, but also can effectively help mitigate\nthe increasing level of traffic emissions produced in cities. Among many\nothers, battery electric vehicles (BEVs) exclusively use chemical energy stored\nin their battery packs for propulsion. Hence, it becomes important to\nunderstand how much energy can be consumed by such vehicles in various traffic\nscenarios towards effective energy management. To address this challenge, we\npropose a novel framework in this paper by leveraging the federated learning\napproaches for modelling energy consumption for BEVs (Fed-BEV). More\nspecifically, a group of BEVs involved in the Fed-BEV framework can learn from\neach other to jointly enhance their energy consumption model. We present the\ndesign of the proposed system architecture and implementation details in a\nco-simulation environment. Finally, comparative studies and simulation results\nare discussed to illustrate the efficacy of our proposed framework for accurate\nenergy modelling of BEVs.",
          "link": "http://arxiv.org/abs/2108.04036",
          "publishedOn": "2021-08-10T02:00:11.119Z",
          "wordCount": null,
          "title": "Fed-BEV: A Federated Learning Framework for Modelling Energy Consumption of Battery Electric Vehicles. (arXiv:2108.04036v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qingyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boshen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiayi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>",
          "description": "Recently, the problem of inaccurate learning targets in crowd counting draws\nincreasing attention. Inspired by a few pioneering work, we solve this problem\nby trying to predict the indices of pre-defined interval bins of counts instead\nof the count values themselves. However, an inappropriate interval setting\nmight make the count error contributions from different intervals extremely\nimbalanced, leading to inferior counting performance. Therefore, we propose a\nnovel count interval partition criterion called Uniform Error Partition (UEP),\nwhich always keeps the expected counting error contributions equal for all\nintervals to minimize the prediction risk. Then to mitigate the inevitably\nintroduced discretization errors in the count quantization process, we propose\nanother criterion called Mean Count Proxies (MCP). The MCP criterion selects\nthe best count proxy for each interval to represent its count value during\ninference, making the overall expected discretization error of an image nearly\nnegligible. As far as we are aware, this work is the first to delve into such a\nclassification task and ends up with a promising solution for count interval\npartition. Following the above two theoretically demonstrated criterions, we\npropose a simple yet effective model termed Uniform Error Partition Network\n(UEPNet), which achieves state-of-the-art performance on several challenging\ndatasets. The codes will be available at:\nhttps://github.com/TencentYoutuResearch/CrowdCounting-UEPNet.",
          "link": "http://arxiv.org/abs/2107.12619",
          "publishedOn": "2021-08-10T02:00:11.114Z",
          "wordCount": null,
          "title": "Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting. (arXiv:2107.12619v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bin Shi</a>",
          "description": "Following the same routine as [SSJ20], we continue to present the theoretical\nanalysis for stochastic gradient descent with momentum (SGD with momentum) in\nthis paper. Differently, for SGD with momentum, we demonstrate it is the two\nhyperparameters together, the learning rate and the momentum coefficient, that\nplay the significant role for the linear rate of convergence in non-convex\noptimization. Our analysis is based on the use of a hyperparameters-dependent\nstochastic differential equation (hp-dependent SDE) that serves as a continuous\nsurrogate for SGD with momentum. Similarly, we establish the linear convergence\nfor the continuous-time formulation of SGD with momentum and obtain an explicit\nexpression for the optimal linear rate by analyzing the spectrum of the\nKramers-Fokker-Planck operator. By comparison, we demonstrate how the optimal\nlinear rate of convergence and the final gap for SGD only about the learning\nrate varies with the momentum coefficient increasing from zero to one when the\nmomentum is introduced. Then, we propose a mathematical interpretation why the\nSGD with momentum converges faster and more robust about the learning rate than\nthe standard SGD in practice. Finally, we show the Nesterov momentum under the\nexistence of noise has no essential difference with the standard momentum.",
          "link": "http://arxiv.org/abs/2108.03947",
          "publishedOn": "2021-08-10T02:00:11.113Z",
          "wordCount": null,
          "title": "On the Hyperparameters in Stochastic Gradient Descent with Momentum. (arXiv:2108.03947v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1\">Oleh Rybkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chuning Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagabandi_A/0/1/0/all/0/1\">Anusha Nagabandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "The ability to plan into the future while utilizing only raw high-dimensional\nobservations, such as images, can provide autonomous agents with broad\ncapabilities. Visual model-based reinforcement learning (RL) methods that plan\nfuture actions directly have shown impressive results on tasks that require\nonly short-horizon reasoning, however, these methods struggle on temporally\nextended tasks. We argue that it is easier to solve long-horizon tasks by\nplanning sequences of states rather than just actions, as the effects of\nactions greatly compound over time and are harder to optimize. To achieve this,\nwe draw on the idea of collocation, which has shown good results on\nlong-horizon tasks in optimal control literature, and adapt it to the\nimage-based setting by utilizing learned latent state space models. The\nresulting latent collocation method (LatCo) optimizes trajectories of latent\nstates, which improves over previously proposed shooting methods for visual\nmodel-based RL on tasks with sparse rewards and long-term goals. Videos and\ncode at https://orybkin.github.io/latco/.",
          "link": "http://arxiv.org/abs/2106.13229",
          "publishedOn": "2021-08-10T02:00:11.109Z",
          "wordCount": null,
          "title": "Model-Based Reinforcement Learning via Latent-Space Collocation. (arXiv:2106.13229v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_K/0/1/0/all/0/1\">Karla Saldana Ochoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comes_T/0/1/0/all/0/1\">Tina Comes</a>",
          "description": "Along with climate change, more frequent extreme events, such as flooding and\ntropical cyclones, threaten the livelihoods and wellbeing of poor and\nvulnerable populations. One of the most immediate needs of people affected by a\ndisaster is finding shelter. While the proliferation of data on disasters is\nalready helping to save lives, identifying damages in buildings, assessing\nshelter needs, and finding appropriate places to establish emergency shelters\nor settlements require a wide range of data to be combined rapidly. To address\nthis gap and make a headway in comprehensive assessments, this paper proposes a\nmachine learning workflow that aims to fuse and rapidly analyse multimodal\ndata. This workflow is built around open and online data to ensure scalability\nand broad accessibility. Based on a database of 19 characteristics for more\nthan 200 disasters worldwide, a fusion approach at the decision level was used.\nThis technique allows the collected multimodal data to share a common semantic\nspace that facilitates the prediction of individual variables. Each fused\nnumerical vector was fed into an unsupervised clustering algorithm called\nSelf-Organizing-Maps (SOM). The trained SOM serves as a predictor for future\ncases, allowing predicting consequences such as total deaths, total people\naffected, and total damage, and provides specific recommendations for\nassessments in the shelter and housing sector. To achieve such prediction, a\nsatellite image from before the disaster and the geographic and demographic\nconditions are shown to the trained model, which achieved a prediction accuracy\nof 62 %",
          "link": "http://arxiv.org/abs/2108.00887",
          "publishedOn": "2021-08-10T02:00:10.898Z",
          "wordCount": 712,
          "title": "A Machine learning approach for rapid disaster response based on multi-modal data. The case of housing & shelter needs. (arXiv:2108.00887v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wright_L/0/1/0/all/0/1\">Less Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeure_N/0/1/0/all/0/1\">Nestor Demeure</a>",
          "description": "As optimizers are critical to the performances of neural networks, every year\na large number of papers innovating on the subject are published. However,\nwhile most of these publications provide incremental improvements to existing\nalgorithms, they tend to be presented as new optimizers rather than composable\nalgorithms. Thus, many worthwhile improvements are rarely seen out of their\ninitial publication. Taking advantage of this untapped potential, we introduce\nRanger21, a new optimizer which combines AdamW with eight components, carefully\nselected after reviewing and testing ideas from the literature. We found that\nthe resulting optimizer provides significantly improved validation accuracy and\ntraining speed, smoother training curves, and is even able to train a ResNet50\non ImageNet2012 without Batch Normalization layers. A problem on which AdamW\nstays systematically stuck in a bad initial state.",
          "link": "http://arxiv.org/abs/2106.13731",
          "publishedOn": "2021-08-10T02:00:10.680Z",
          "wordCount": null,
          "title": "Ranger21: a synergistic deep learning optimizer. (arXiv:2106.13731v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03126",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Hong_R/0/1/0/all/0/1\">Rui Hong</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhou_P/0/1/0/all/0/1\">Peng-Fei Zhou</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Xi_B/0/1/0/all/0/1\">Bin Xi</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ji_A/0/1/0/all/0/1\">An-Chun Ji</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ran_S/0/1/0/all/0/1\">Shi-Ju Ran</a>",
          "description": "The hybridizations of machine learning and quantum physics have caused\nessential impacts to the methodology in both fields. Inspired by quantum\npotential neural network, we here propose to solve the potential in the\nSchrodinger equation provided the eigenstate, by combining Metropolis sampling\nwith deep neural network, which we dub as Metropolis potential neural network\n(MPNN). A loss function is proposed to explicitly involve the energy in the\noptimization for its accurate evaluation. Benchmarking on the harmonic\noscillator and hydrogen atom, MPNN shows excellent accuracy and stability on\npredicting not just the potential to satisfy the Schrodinger equation, but also\nthe eigen-energy. Our proposal could be potentially applied to the ab-initio\nsimulations, and to inversely solving other partial differential equations in\nphysics and beyond.",
          "link": "http://arxiv.org/abs/2106.03126",
          "publishedOn": "2021-08-10T02:00:10.585Z",
          "wordCount": null,
          "title": "Predicting Quantum Potentials by Deep Neural Network and Metropolis Sampling. (arXiv:2106.03126v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.12138",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shmueli_G/0/1/0/all/0/1\">Galit Shmueli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tafti_A/0/1/0/all/0/1\">Ali Tafti</a>",
          "description": "Many internet platforms that collect behavioral big data use it to predict\nuser behavior for internal purposes and for their business customers (e.g.,\nadvertisers, insurers, security forces, governments, political consulting\nfirms) who utilize the predictions for personalization, targeting, and other\ndecision-making. Improving predictive accuracy is therefore extremely valuable.\nData science researchers design algorithms, models, and approaches to improve\nprediction. Prediction is also improved with larger and richer data. Beyond\nimproving algorithms and data, platforms can stealthily achieve better\nprediction accuracy by \"pushing\" users' behaviors towards their predicted\nvalues, using behavior modification techniques, thereby demonstrating more\ncertain predictions. Such apparent \"improved\" prediction can unintentionally\nresult from employing reinforcement learning algorithms that combine prediction\nand behavior modification. This strategy is absent from the machine learning\nand statistics literature. Investigating its properties requires integrating\ncausal with predictive notation. To this end, we incorporate Pearl's causal\ndo(.) operator into the predictive vocabulary. We then decompose the expected\nprediction error given behavior modification, and identify the components\nimpacting predictive power. Our derivation elucidates implications of such\nbehavior modification to data scientists, platforms, their customers, and the\nhumans whose behavior is manipulated. Behavior modification can make users'\nbehavior more predictable and even more homogeneous; yet this apparent\npredictability might not generalize when customers use predictions in practice.\nOutcomes pushed towards their predictions can be at odds with customers'\nintentions, and harmful to manipulated users.",
          "link": "http://arxiv.org/abs/2008.12138",
          "publishedOn": "2021-08-10T02:00:10.491Z",
          "wordCount": 692,
          "title": "How to \"Improve\" Prediction Using Behavior Modification. (arXiv:2008.12138v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06534",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Immidisetti_R/0/1/0/all/0/1\">Rakhil Immidisetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shuowen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Existing thermal-to-visible face verification approaches expect the thermal\nand visible face images to be of similar resolution. This is unlikely in\nreal-world long-range surveillance systems, since humans are distant from the\ncameras. To address this issue, we introduce the task of thermal-to-visible\nface verification from low-resolution thermal images. Furthermore, we propose\nAxial-Generative Adversarial Network (Axial-GAN) to synthesize high-resolution\nvisible images for matching. In the proposed approach we augment the GAN\nframework with axial-attention layers which leverage the recent advances in\ntransformers for modelling long-range dependencies. We demonstrate the\neffectiveness of the proposed method by evaluating on two different\nthermal-visible face datasets. When compared to related state-of-the-art works,\nour results show significant improvements in both image quality and face\nverification performance, and are also much more efficient.",
          "link": "http://arxiv.org/abs/2104.06534",
          "publishedOn": "2021-08-10T02:00:10.485Z",
          "wordCount": 621,
          "title": "Simultaneous Face Hallucination and Translation for Thermal to Visible Face Verification using Axial-GAN. (arXiv:2104.06534v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.10202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Martin Q. Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yao-Hung Hubert Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>",
          "description": "While deep learning has received a surge of interest in a variety of fields\nin recent years, major deep learning models barely use complex numbers.\nHowever, speech, signal and audio data are naturally complex-valued after\nFourier Transform, and studies have shown a potentially richer representation\nof complex nets. In this paper, we propose a Complex Transformer, which\nincorporates the transformer model as a backbone for sequence modeling; we also\ndevelop attention and encoder-decoder network operating for complex input. The\nmodel achieves state-of-the-art performance on the MusicNet dataset and an\nIn-phase Quadrature (IQ) signal dataset.",
          "link": "http://arxiv.org/abs/1910.10202",
          "publishedOn": "2021-08-10T02:00:10.477Z",
          "wordCount": 572,
          "title": "Complex Transformer: A Framework for Modeling Complex-Valued Sequence. (arXiv:1910.10202v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.00959",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Tirer_T/0/1/0/all/0/1\">Tom Tirer</a>, <a href=\"http://arxiv.org/find/math/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>",
          "description": "Ill-posed linear inverse problems appear in many scientific setups, and are\ntypically addressed by solving optimization problems, which are composed of\ndata fidelity and prior terms. Recently, several works have considered a\nback-projection (BP) based fidelity term as an alternative to the common least\nsquares (LS), and demonstrated excellent results for popular inverse problems.\nThese works have also empirically shown that using the BP term, rather than the\nLS term, requires fewer iterations of optimization algorithms. In this paper,\nwe examine the convergence rate of the projected gradient descent (PGD)\nalgorithm for the BP objective. Our analysis allows to identify an inherent\nsource for its faster convergence compared to using the LS objective, while\nmaking only mild assumptions. We also analyze the more general proximal\ngradient method under a relaxed contraction condition on the proximal mapping\nof the prior. This analysis further highlights the advantage of BP when the\nlinear measurement operator is badly conditioned. Numerical experiments with\nboth $\\ell_1$-norm and GAN-based priors corroborate our theoretical results.",
          "link": "http://arxiv.org/abs/2005.00959",
          "publishedOn": "2021-08-10T02:00:10.460Z",
          "wordCount": 661,
          "title": "On the Convergence Rate of Projected Gradient Descent for a Back-Projection based Objective. (arXiv:2005.00959v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wanqi Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Wei Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1\">Bo An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabinovich_Z/0/1/0/all/0/1\">Zinovi Rabinovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obraztsova_S/0/1/0/all/0/1\">Svetlana Obraztsova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_C/0/1/0/all/0/1\">Chai Kiat Yeo</a>",
          "description": "Recent studies in multi-agent communicative reinforcement learning (MACRL)\ndemonstrate that multi-agent coordination can be significantly improved when\ncommunication between agents is allowed. Meanwhile, advances in adversarial\nmachine learning (ML) have shown that ML and reinforcement learning (RL) models\nare vulnerable to a variety of attacks that significantly degrade the\nperformance of learned behaviours. However, despite the obvious and growing\nimportance, the combination of adversarial ML and MACRL remains largely\nuninvestigated. In this paper, we make the first step towards conducting\nmessage attacks on MACRL methods. In our formulation, one agent in the\ncooperating group is taken over by an adversary and can send malicious messages\nto disrupt a deployed MACRL-based coordinated strategy during the deployment\nphase. We further our study by developing a defence method via message\nreconstruction. Finally, we address the resulting arms race, i.e., we consider\nthe ability of the malicious agent to adapt to the changing and improving\ndefensive communicative policies of the benign agents. Specifically, we model\nthe adversarial MACRL problem as a two-player zero-sum game and then utilize\nPolicy-Space Response Oracle to achieve communication robustness. Empirically,\nwe demonstrate that MACRL methods are vulnerable to message attacks while our\ndefence method the game-theoretic framework can effectively improve the\nrobustness of MACRL.",
          "link": "http://arxiv.org/abs/2108.03803",
          "publishedOn": "2021-08-10T02:00:10.454Z",
          "wordCount": 648,
          "title": "Mis-spoke or mis-lead: Achieving Robustness in Multi-Agent Communicative Reinforcement Learning. (arXiv:2108.03803v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03019",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Ma_X/0/1/0/all/0/1\">Xinran Ma</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Tu_Z/0/1/0/all/0/1\">Z. C. Tu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ran_S/0/1/0/all/0/1\">Shi-Ju Ran</a>",
          "description": "Human experts cannot efficiently access the physical information of quantum\nmany-body states by simply \"reading\" the coefficients, but have to reply on the\nprevious knowledge such as order parameters and quantum measurements. In this\nwork, we demonstrate that convolutional neural network (CNN) can learn from the\ncoefficients of local reduced density matrices to estimate the physical\nparameters of the many-body Hamiltonians, such as coupling strengths and\nmagnetic fields, provided the states as the ground states. We propose QubismNet\nthat consists of two main parts: the Qubism map that visualizes the ground\nstates (or the purified reduced density matrices) as images, and a CNN that\nmaps the images to the target physical parameters. By assuming certain\nconstraints on the training set for the sake of balance, QubismNet exhibits\nimpressive powers of learning and generalization on several quantum spin\nmodels. While the training samples are restricted to the states from certain\nranges of the parameters, QubismNet can accurately estimate the parameters of\nthe states beyond such training regions. For instance, our results show that\nQubismNet can estimate the magnetic fields near the critical point by learning\nfrom the states away from the critical vicinity. Our work illuminates a\ndata-driven way to infer the Hamiltonians that give the designed ground states,\nand therefore would benefit the existing and future generalizations of quantum\ntechnologies such as Hamiltonian-based quantum simulations and state\ntomography.",
          "link": "http://arxiv.org/abs/2012.03019",
          "publishedOn": "2021-08-10T02:00:10.447Z",
          "wordCount": 692,
          "title": "Deep learning Local Reduced Density Matrices for Many-body Hamiltonian Estimation. (arXiv:2012.03019v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03995",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_W/0/1/0/all/0/1\">Wei-Chang Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yi-Ping Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yun-Chia Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Chyh-Ming Lai</a>",
          "description": "Convolutional neural networks (CNNs) are widely used in image recognition.\nNumerous CNN models, such as LeNet, AlexNet, VGG, ResNet, and GoogLeNet, have\nbeen proposed by increasing the number of layers, to improve the performance of\nCNNs. However, performance deteriorates beyond a certain number of layers.\nHence, hyperparameter optimisation is a more efficient way to improve CNNs. To\nvalidate this concept, a new algorithm based on simplified swarm optimisation\nis proposed to optimise the hyperparameters of the simplest CNN model, which is\nLeNet. The results of experiments conducted on the MNIST, Fashion MNIST, and\nCifar10 datasets showed that the accuracy of the proposed algorithm is higher\nthan the original LeNet model and PSO-LeNet and that it has a high potential to\nbe extended to more complicated models, such as AlexNet.",
          "link": "http://arxiv.org/abs/2103.03995",
          "publishedOn": "2021-08-10T02:00:10.441Z",
          "wordCount": 601,
          "title": "Convolution Neural Network Hyperparameter Optimization Using Simplified Swarm Optimization. (arXiv:2103.03995v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04058",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mitrentsis_G/0/1/0/all/0/1\">Georgios Mitrentsis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lens_H/0/1/0/all/0/1\">Hendrik Lens</a>",
          "description": "The stochastic nature of photovoltaic (PV) power has led both academia and\nindustry to a large amount of research work aiming at the development of\naccurate PV power forecasting models. However, most of those models are based\non machine learning algorithms and are considered as black boxes which do not\nprovide any insight or explanation about their predictions. Therefore, their\ndirect implementation in environments, where transparency is required, and the\ntrust associated with their predictions may be questioned. To this end, we\npropose a two stage probabilistic forecasting framework able to generate highly\naccurate, reliable, and sharp forecasts yet offering full transparency on both\nthe point forecasts and the prediction intervals (PIs). In the first stage, we\nexploit natural gradient boosting (NGBoost) for yielding probabilistic\nforecasts while in the second stage, we calculate the Shapley additive\nexplanation (SHAP) values in order to fully understand why a prediction was\nmade. To highlight the performance and the applicability of the proposed\nframework, real data from two PV parks located in Southern Germany are\nemployed. Initially, the natural gradient boosting is thoroughly compared with\ntwo state-of-the-art algorithms, namely Gaussian process and lower upper bound\nestimation, in a wide range of forecasting metrics. Secondly, a detailed\nanalysis of the model's complex nonlinear relationships and interaction effects\nbetween the various features is presented. The latter allows us to interpret\nthe model, identify some learned physical properties, explain individual\npredictions, reduce the computational requirements for the training without\njeopardizing the model accuracy, detect possible bugs, and gain trust in the\nmodel. Finally, we conclude that the model was able to develop nonlinear\nrelationships following human logic and intuition based on learned physical\nproperties.",
          "link": "http://arxiv.org/abs/2108.04058",
          "publishedOn": "2021-08-10T02:00:10.434Z",
          "wordCount": 714,
          "title": "An Interpretable Probabilistic Model for Short-Term Solar Power Forecasting Using Natural Gradient Boosting. (arXiv:2108.04058v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04063",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Cheng Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>",
          "description": "Noisy labels, resulting from mistakes in manual labeling or webly data\ncollecting for supervised learning, can cause neural networks to overfit the\nmisleading information and degrade the generalization performance.\nSelf-supervised learning works in the absence of labels and thus eliminates the\nnegative impact of noisy labels. Motivated by co-training with both supervised\nlearning view and self-supervised learning view, we propose a simple yet\neffective method called Co-learning for learning with noisy labels. Co-learning\nperforms supervised learning and self-supervised learning in a cooperative way.\nThe constraints of intrinsic similarity with the self-supervised module and the\nstructural similarity with the noisily-supervised module are imposed on a\nshared common feature encoder to regularize the network to maximize the\nagreement between the two constraints. Co-learning is compared with peer\nmethods on corrupted data from benchmark datasets fairly, and extensive results\nare provided which demonstrate that Co-learning is superior to many\nstate-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.04063",
          "publishedOn": "2021-08-10T02:00:10.403Z",
          "wordCount": 583,
          "title": "Co-learning: Learning from Noisy Labels with Self-supervision. (arXiv:2108.04063v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03632",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Fukushima_K/0/1/0/all/0/1\">Kimihiko Fukushima</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Sakai_K/0/1/0/all/0/1\">Kazumitsu Sakai</a>",
          "description": "Employing a deep convolutional neural network (deep CNN) trained on spin\nconfigurations of the 2D Ising model and the temperatures, we examine whether\nthe deep CNN can detect the phase transition of the 2D $q$-state Potts model.\nTo this end, we generate binarized images of spin configurations of the\n$q$-state Potts model ($q\\ge 3$) by replacing the spin variables\n$\\{0,1,\\dots,\\lfloor q/2\\rfloor-1\\}$ and $\\{\\lfloor q/2\\rfloor,\\dots,q-1\\}$\nwith $\\{0\\}$ and $\\{1\\}$, respectively. Then, we input these images to the\ntrained CNN to output the predicted temperatures. The binarized images of the\n$q$-state Potts model are entirely different from Ising spin configurations,\nparticularly at the transition temperature. Moreover, our CNN model is not\ntrained on the information about whether phases are ordered/disordered but is\nnaively trained by Ising spin configurations labeled with temperatures at which\nthey are generated. Nevertheless, the deep CNN can detect the transition point\nwith high accuracy, regardless of the type of transition. We also find that, in\nthe high-temperature region, the CNN outputs the temperature based on the\ninternal energy, whereas, in the low-temperature region, the output depends on\nthe magnetization and possibly the internal energy as well. However, in the\nvicinity of the transition point, the CNN may use more general factors to\ndetect the transition point.",
          "link": "http://arxiv.org/abs/2104.03632",
          "publishedOn": "2021-08-10T02:00:10.395Z",
          "wordCount": 716,
          "title": "Can a CNN trained on the Ising model detect the phase transition of the $q$-state Potts model?. (arXiv:2104.03632v3 [cond-mat.dis-nn] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1\">Andres Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tretter_A/0/1/0/all/0/1\">Andreas Tretter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hager_P/0/1/0/all/0/1\">Pascal Alexander Hager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanmugarajah_P/0/1/0/all/0/1\">Praveenth Sanmugarajah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1\">Luca Benini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiele_L/0/1/0/all/0/1\">Lothar Thiele</a>",
          "description": "Sensing systems powered by energy harvesting have traditionally been designed\nto tolerate long periods without energy. As the Internet of Things (IoT)\nevolves towards a more transient and opportunistic execution paradigm, reducing\nenergy storage costs will be key for its economic and ecologic viability.\nHowever, decreasing energy storage in harvesting systems introduces reliability\nissues. Transducers only produce intermittent energy at low voltage and current\nlevels, making guaranteed task completion a challenge. Existing ad hoc methods\novercome this by buffering enough energy either for single tasks, incurring\nlarge data-retention overheads, or for one full application cycle, requiring a\nlarge energy buffer. We present Julienning: an automated method for optimizing\nthe total energy cost of batteryless applications. Using a custom specification\nmodel, developers can describe transient applications as a set of atomically\nexecuted kernels with explicit data dependencies. Our optimization flow can\npartition data- and energy-intensive applications into multiple execution\ncycles with bounded energy consumption. By leveraging interkernel data\ndependencies, these energy-bounded execution cycles minimize the number of\nsystem activations and nonvolatile data transfers, and thus the total energy\noverhead. We validate our methodology with two batteryless cameras running\nenergy-intensive machine learning applications. Results demonstrate that\ncompared to ad hoc solutions, our method can reduce the required energy storage\nby over 94% while only incurring a 0.12% energy overhead.",
          "link": "http://arxiv.org/abs/2108.04059",
          "publishedOn": "2021-08-10T02:00:10.388Z",
          "wordCount": 678,
          "title": "Memory-Aware Partitioning of Machine Learning Applications for Optimal Energy Use in Batteryless Systems. (arXiv:2108.04059v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miller_C/0/1/0/all/0/1\">Clayton Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picchetti_B/0/1/0/all/0/1\">Bianca Picchetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantelic_J/0/1/0/all/0/1\">Jovan Pantelic</a>",
          "description": "Machine learning for building energy prediction has exploded in popularity in\nrecent years, yet understanding its limitations and potential for improvement\nare lacking. The ASHRAE Great Energy Predictor III (GEPIII) Kaggle competition\nwas the largest building energy meter machine learning competition ever held\nwith 4,370 participants who submitted 39,403 predictions. The test data set\nincluded two years of hourly electricity, hot water, chilled water, and steam\nreadings from 2,380 meters in 1,448 buildings at 16 locations. This paper\nanalyzes the various sources and types of residual model error from an\naggregation of the competition's top 50 solutions. This analysis reveals the\nlimitations for machine learning using the standard model inputs of historical\nmeter, weather, and basic building metadata. The types of error are classified\naccording to the amount of time errors occur in each instance, abrupt versus\ngradual behavior, the magnitude of error, and whether the error existed on\nsingle buildings or several buildings at once from a single location. The\nresults show machine learning models have errors within a range of\nacceptability on 79.1% of the test data. Lower magnitude model errors occur in\n16.1% of the test data. These discrepancies can likely be addressed through\nadditional training data sources or innovations in machine learning. Higher\nmagnitude errors occur in 4.8% of the test data and are unlikely to be\naccurately predicted regardless of innovation. There is a diversity of error\nbehavior depending on the energy meter type (electricity prediction models have\nunacceptable error in under 10% of test data, while hot water is over 60%) and\nbuilding use type (public service less than 14%, while technology/science is\njust over 46%).",
          "link": "http://arxiv.org/abs/2106.13475",
          "publishedOn": "2021-08-10T02:00:10.382Z",
          "wordCount": 730,
          "title": "Limitations of machine learning for building energy prediction. (arXiv:2106.13475v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baoxiang Wang</a>",
          "description": "Strategic behavior against sequential learning methods, such as \"click\nframing\" in real recommendation systems, has been widely observed. Motivated by\nsuch behavior we study the problem of combinatorial multi-armed bandits (CMAB)\nunder strategic manipulations of rewards, where each arm can modify the emitted\nreward signals for its own interest. This characterization of the adversarial\nbehavior is a relaxation of previously well-studied settings such as\nadversarial attacks and adversarial corruption. We propose a strategic variant\nof the combinatorial UCB algorithm, which has a regret of at most $O(m\\log T +\nm B_{max})$ under strategic manipulations, where $T$ is the time horizon, $m$\nis the number of arms, and $B_{max}$ is the maximum budget of an arm. We\nprovide lower bounds on the budget for arms to incur certain regret of the\nbandit algorithm. Extensive experiments on online worker selection for\ncrowdsourcing systems, online influence maximization and online recommendations\nwith both synthetic and real datasets corroborate our theoretical findings on\nrobustness and regret bounds, in a variety of regimes of manipulation budgets.",
          "link": "http://arxiv.org/abs/2102.12722",
          "publishedOn": "2021-08-10T02:00:10.375Z",
          "wordCount": 626,
          "title": "Combinatorial Bandits under Strategic Manipulations. (arXiv:2102.12722v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1\">Sahil Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1\">Surbhi Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>",
          "description": "Training convolutional neural networks (CNNs) with a strict Lipschitz\nconstraint under the l_{2} norm is useful for provable adversarial robustness,\ninterpretable gradients and stable training. While 1-Lipschitz CNNs can be\ndesigned by enforcing a 1-Lipschitz constraint on each layer, training such\nnetworks requires each layer to have an orthogonal Jacobian matrix (for all\ninputs) to prevent gradients from vanishing during backpropagation. A layer\nwith this property is said to be Gradient Norm Preserving (GNP). To construct\nexpressive GNP activation functions, we first prove that the Jacobian of any\nGNP piecewise linear function is only allowed to change via Householder\ntransformations for the function to be continuous. Building on this result, we\nintroduce a class of nonlinear GNP activations with learnable Householder\ntransformations called Householder activations. A householder activation\nparameterized by the vector $\\mathbf{v}$ outputs $(\\mathbf{I} -\n2\\mathbf{v}\\mathbf{v}^{T})\\mathbf{z}$ for its input $\\mathbf{z}$ if\n$\\mathbf{v}^{T}\\mathbf{z} \\leq 0$; otherwise it outputs $\\mathbf{z}$. Existing\nGNP activations such as $\\mathrm{MaxMin}$ can be viewed as special cases of\n$\\mathrm{HH}$ activations for certain settings of these transformations. Thus,\nnetworks with $\\mathrm{HH}$ activations have higher expressive power than those\nwith $\\mathrm{MaxMin}$ activations. Although networks with $\\mathrm{HH}$\nactivations have nontrivial provable robustness against adversarial attacks, we\nfurther boost their robustness by (i) introducing a certificate regularization\nand (ii) relaxing orthogonalization of the last layer of the network. Our\nexperiments on CIFAR-10 and CIFAR-100 show that our regularized networks with\n$\\mathrm{HH}$ activations lead to significant improvements in both the standard\nand provable robust accuracy over the prior works (gain of 3.65\\% and 4.46\\% on\nCIFAR-100 respectively).",
          "link": "http://arxiv.org/abs/2108.04062",
          "publishedOn": "2021-08-10T02:00:10.368Z",
          "wordCount": 683,
          "title": "Householder Activations for Provable Robustness against Adversarial Attacks. (arXiv:2108.04062v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04012",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Daniel_T/0/1/0/all/0/1\">Thomas Daniel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Casenave_F/0/1/0/all/0/1\">Fabien Casenave</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Akkari_N/0/1/0/all/0/1\">Nissrine Akkari</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ryckelynck_D/0/1/0/all/0/1\">David Ryckelynck</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rey_C/0/1/0/all/0/1\">Christian Rey</a>",
          "description": "We consider the dictionary-based ROM-net (Reduced Order Model) framework [T.\nDaniel, F. Casenave, N. Akkari, D. Ryckelynck, Model order reduction assisted\nby deep neural networks (ROM-net), Advanced modeling and Simulation in\nEngineering Sciences 7 (16), 2020] and summarize the underlying methodologies\nand their recent improvements. The main contribution of this work is the\napplication of the complete workflow to a real-life industrial model of an\nelastoviscoplastic high-pressure turbine blade subjected to thermal,\ncentrifugal and pressure loadings, for the quantification of the uncertainty on\ndual quantities (such as the accumulated plastic strain and the stress tensor),\ngenerated by the uncertainty on the temperature loading field. The\ndictionary-based ROM-net computes predictions of dual quantities of interest\nfor 1008 Monte Carlo draws of the temperature loading field in 2 hours and 48\nminutes, which corresponds to a speedup greater than 600 with respect to a\nreference parallel solver using domain decomposition, with a relative error in\nthe order of 2%. Another contribution of this work consists in the derivation\nof a meta-model to reconstruct the dual quantities of interest over the\ncomplete mesh from their values on the reduced integration points.",
          "link": "http://arxiv.org/abs/2108.04012",
          "publishedOn": "2021-08-10T02:00:10.350Z",
          "wordCount": 630,
          "title": "Uncertainty quantification for industrial design using dictionaries of reduced order models. (arXiv:2108.04012v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1908.01602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1\">Sebastian Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheridito_P/0/1/0/all/0/1\">Patrick Cheridito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jentzen_A/0/1/0/all/0/1\">Arnulf Jentzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welti_T/0/1/0/all/0/1\">Timo Welti</a>",
          "description": "Nowadays many financial derivatives, such as American or Bermudan options,\nare of early exercise type. Often the pricing of early exercise options gives\nrise to high-dimensional optimal stopping problems, since the dimension\ncorresponds to the number of underlying assets. High-dimensional optimal\nstopping problems are, however, notoriously difficult to solve due to the\nwell-known curse of dimensionality. In this work, we propose an algorithm for\nsolving such problems, which is based on deep learning and computes, in the\ncontext of early exercise option pricing, both approximations of an optimal\nexercise strategy and the price of the considered option. The proposed\nalgorithm can also be applied to optimal stopping problems that arise in other\nareas where the underlying stochastic process can be efficiently simulated. We\npresent numerical results for a large number of example problems, which include\nthe pricing of many high-dimensional American and Bermudan options, such as\nBermudan max-call options in up to 5000 dimensions. Most of the obtained\nresults are compared to reference values computed by exploiting the specific\nproblem design or, where available, to reference values from the literature.\nThese numerical results suggest that the proposed algorithm is highly effective\nin the case of many underlyings, in terms of both accuracy and speed.",
          "link": "http://arxiv.org/abs/1908.01602",
          "publishedOn": "2021-08-10T02:00:10.343Z",
          "wordCount": 712,
          "title": "Solving high-dimensional optimal stopping problems using deep learning. (arXiv:1908.01602v3 [cs.CE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03981",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xinhang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_T/0/1/0/all/0/1\">Tianhao Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_C/0/1/0/all/0/1\">Chen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>",
          "description": "Federated Deep Learning (FDL) is helping to realize distributed machine\nlearning in the Internet of Vehicles (IoV). However, FDL's global model needs\nmultiple clients to upload learning model parameters, thus still existing\nunavoidable communication overhead and data privacy risks. The recently\nproposed Swarm Learning (SL) provides a decentralized machine-learning approach\nuniting edge computing and blockchain-based coordination without the need for a\ncentral coordinator. This paper proposes a Swarm-Federated Deep Learning\nframework in the IoV system (IoV-SFDL) that integrates SL into the FDL\nframework. The IoV-SFDL organizes vehicles to generate local SL models with\nadjacent vehicles based on the blockchain empowered SL, then aggregates the\nglobal FDL model among different SL groups with a proposed credibility weights\nprediction algorithm. Extensive experimental results demonstrate that compared\nwith the baseline frameworks, the proposed IoV-SFDL framework achieves a 16.72%\nreduction in edge-to-global communication overhead while improving about 5.02%\nin model performance with the same training iterations.",
          "link": "http://arxiv.org/abs/2108.03981",
          "publishedOn": "2021-08-10T02:00:10.336Z",
          "wordCount": 598,
          "title": "A Credibility-aware Swarm-Federated Deep Learning Framework in Internet of Vehicles. (arXiv:2108.03981v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Ji Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karbasi_A/0/1/0/all/0/1\">Amin Karbasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmoody_M/0/1/0/all/0/1\">Mohammad Mahmoody</a>",
          "description": "In this paper, we study PAC learnability and certification of predictions\nunder instance-targeted poisoning attacks, where the adversary who knows the\ntest instance may change a fraction of the training set with the goal of\nfooling the learner at the test instance. Our first contribution is to\nformalize the problem in various settings and to explicitly model subtle\naspects such as the proper or improper nature of the learning, learner's\nrandomness, and whether (or not) adversary's attack can depend on it. Our main\nresult shows that when the budget of the adversary scales sublinearly with the\nsample complexity, (improper) PAC learnability and certification are\nachievable; in contrast, when the adversary's budget grows linearly with the\nsample complexity, the adversary can potentially drive up the expected 0-1 loss\nto one. We also study distribution-specific PAC learning in the same attack\nmodel and show that proper learning with certification is possible for learning\nhalf spaces under natural distributions. Finally, we empirically study the\nrobustness of K nearest neighbour, logistic regression, multi-layer perceptron,\nand convolutional neural network on real data sets against targeted-poisoning\nattacks. Our experimental results show that many models, especially\nstate-of-the-art neural networks, are indeed vulnerable to these strong\nattacks. Interestingly, we observe that methods with high standard accuracy\nmight be more vulnerable to instance-targeted poisoning attacks.",
          "link": "http://arxiv.org/abs/2105.08709",
          "publishedOn": "2021-08-10T02:00:10.328Z",
          "wordCount": 693,
          "title": "Learning and Certification under Instance-targeted Poisoning. (arXiv:2105.08709v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03995",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadzadeh_S/0/1/0/all/0/1\">S. Mohammadzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lejeune_E/0/1/0/all/0/1\">E. Lejeune</a>",
          "description": "Using simulation to predict the mechanical behavior of heterogeneous\nmaterials has applications ranging from topology optimization to multi-scale\nstructural analysis. However, full-fidelity simulation techniques such as\nFinite Element Analysis can be prohibitively computationally expensive when\nthey are used to explore the massive input parameter space of heterogeneous\nmaterials. Therefore, there has been significant recent interest in machine\nlearning-based models that, once trained, can predict mechanical behavior at a\nfraction of the computational cost. Over the past several years, research in\nthis area has been focused mainly on predicting single Quantities of Interest\n(QoIs). However, there has recently been an increased interest in a more\nchallenging problem: predicting full-field QoI (e.g., displacement/strain\nfields, damage fields) for mechanical problems. Due to the added complexity of\nfull-field information, network architectures that perform well on single QoI\nproblems may perform poorly in the full-field QoI problem setting. The work\npresented in this paper is twofold. First, we made a significant extension to\nthe Mechanical MNIST dataset designed to enable the investigation of full field\nQoI prediction. Specifically, we added Finite Element simulation results of\nquasi-static brittle fracture in a heterogeneous material captured with the\nphase-field method. Second, we established strong baseline performance for\npredicting full-field QoI with MultiRes-WNet architecture. In addition to\npresenting the results in this paper, we have released our model implementation\nand the Mechanical MNIST Crack Path dataset under open-source licenses. We\nanticipate that future researchers will directly use our model architecture on\nrelated datasets and potentially design models that exceed the baseline\nperformance for predicting full-field QoI established in this paper.",
          "link": "http://arxiv.org/abs/2108.03995",
          "publishedOn": "2021-08-10T02:00:10.319Z",
          "wordCount": 718,
          "title": "Predicting Mechanically Driven Full-Field Quantities of Interest with Deep Learning-Based Metamodels. (arXiv:2108.03995v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04085",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Bonneville_C/0/1/0/all/0/1\">Christophe Bonneville</a>, <a href=\"http://arxiv.org/find/math/1/au:+Earls_C/0/1/0/all/0/1\">Christopher J. Earls</a>",
          "description": "Scientific machine learning has been successfully applied to inverse problems\nand PDE discoveries in computational physics. One caveat of current methods\nhowever is the need for large amounts of (clean) data in order to recover full\nsystem responses or underlying physical models. Bayesian methods may be\nparticularly promising to overcome these challenges as they are naturally less\nsensitive to sparse and noisy data. In this paper, we propose to use Bayesian\nneural networks (BNN) in order to: 1) Recover the full system states from\nmeasurement data (e.g. temperature, velocity field, etc.). We use Hamiltonian\nMonte-Carlo to sample the posterior distribution of a deep and dense BNN, and\nshow that it is possible to accurately capture physics of varying complexity\nwithout overfitting. 2) Recover the parameters in the underlying partial\ndifferential equation (PDE) governing the physical system. Using the trained\nBNN as a surrogate of the system response, we generate datasets of derivatives\npotentially comprising the latent PDE of the observed system and perform a\nBayesian linear regression (BLR) between the successive derivatives in space\nand time to recover the original PDE parameters. We take advantage of the\nconfidence intervals on the BNN outputs and introduce the spatial derivative\nvariance into the BLR likelihood to discard the influence of highly uncertain\nsurrogate data points, which allows for more accurate parameter discovery. We\ndemonstrate our approach on a handful of example applied to physics and\nnon-linear dynamics.",
          "link": "http://arxiv.org/abs/2108.04085",
          "publishedOn": "2021-08-10T02:00:10.301Z",
          "wordCount": 679,
          "title": "Bayesian Deep Learning for Partial Differential Equation Parameter Discovery with Sparse and Noisy Data. (arXiv:2108.04085v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jakob_P/0/1/0/all/0/1\">Peter Jakob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_M/0/1/0/all/0/1\">Manav Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_Schirling_T/0/1/0/all/0/1\">Tobias Schmid-Schirling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>",
          "description": "Anomaly detection is a critical problem in the manufacturing industry. In\nmany applications, images of objects to be analyzed are captured from multiple\nperspectives which can be exploited to improve the robustness of anomaly\ndetection. In this work, we build upon the deep support vector data description\nalgorithm and address multi-perspective anomaly detection using three different\nfusion techniques, i.e., early fusion, late fusion, and late fusion with\nmultiple decoders. We employ different augmentation techniques with a denoising\nprocess to deal with scarce one-class data, which further improves the\nperformance (ROC AUC $= 80\\%$). Furthermore, we introduce the dices dataset,\nwhich consists of over 2000 grayscale images of falling dices from multiple\nperspectives, with 5\\% of the images containing rare anomalies (e.g., drill\nholes, sawing, or scratches). We evaluate our approach on the new dices dataset\nusing images from two different perspectives and also benchmark on the standard\nMNIST dataset. Extensive experiments demonstrate that our proposed\n{multi-perspective} approach exceeds the state-of-the-art {single-perspective\nanomaly detection on both the MNIST and dices datasets}. To the best of our\nknowledge, this is the first work that focuses on addressing multi-perspective\nanomaly detection in images by jointly using different perspectives together\nwith one single objective function for anomaly detection.",
          "link": "http://arxiv.org/abs/2105.09903",
          "publishedOn": "2021-08-10T02:00:10.295Z",
          "wordCount": 692,
          "title": "Multi-Perspective Anomaly Detection. (arXiv:2105.09903v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rohm_A/0/1/0/all/0/1\">Andr&#xe9; R&#xf6;hm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gauthier_D/0/1/0/all/0/1\">Daniel J. Gauthier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_I/0/1/0/all/0/1\">Ingo Fischer</a>",
          "description": "Reservoir computers are powerful tools for chaotic time series prediction.\nThey can be trained to approximate phase space flows and can thus both predict\nfuture values to a high accuracy, as well as reconstruct the general properties\nof a chaotic attractor without requiring a model. In this work, we show that\nthe ability to learn the dynamics of a complex system can be extended to\nsystems with co-existing attractors, here a 4-dimensional extension of the\nwell-known Lorenz chaotic system. We demonstrate that a reservoir computer can\ninfer entirely unexplored parts of the phase space: a properly trained\nreservoir computer can predict the existence of attractors that were never\napproached during training and therefore are labelled as unseen. We provide\nexamples where attractor inference is achieved after training solely on a\nsingle noisy trajectory.",
          "link": "http://arxiv.org/abs/2108.04074",
          "publishedOn": "2021-08-10T02:00:10.287Z",
          "wordCount": 586,
          "title": "Model-free inference of unseen attractors: Reconstructing phase space features from a single noisy trajectory using reservoir computing. (arXiv:2108.04074v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hur_K/0/1/0/all/0/1\">Kyunghoon Hur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jungwoo Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_W/0/1/0/all/0/1\">Wesley Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Hak Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>",
          "description": "Substantial increase in the use of Electronic Health Records (EHRs) has\nopened new frontiers for predictive healthcare. However, while EHR systems are\nnearly ubiquitous, they lack a unified code system for representing medical\nconcepts. Heterogeneous formats of EHR present a substantial barrier for the\ntraining and deployment of state-of-the-art deep learning models at scale. To\novercome this problem, we introduce Description-based Embedding, DescEmb, a\ncode-agnostic description-based representation learning framework for\npredictive modeling on EHR. DescEmb takes advantage of the flexibility of\nneural language understanding models while maintaining a neutral approach that\ncan be combined with prior frameworks for task-specific representation learning\nor predictive modeling. We tested our model's capacity on various experiments\nincluding prediction tasks, transfer learning and pooled learning. DescEmb\nshows higher performance in overall experiments compared to code-based\napproach, opening the door to a text-based approach in predictive healthcare\nresearch that is not constrained by EHR structure nor special domain knowledge.",
          "link": "http://arxiv.org/abs/2108.03625",
          "publishedOn": "2021-08-10T02:00:10.279Z",
          "wordCount": 610,
          "title": "Unifying Heterogenous Electronic Health Records Systems via Text-Based Code Embedding. (arXiv:2108.03625v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinyan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>",
          "description": "In this paper, we revisit the problem of Differentially Private Stochastic\nConvex Optimization (DP-SCO) and provide excess population risks for some\nspecial classes of functions that are faster than the previous results of\ngeneral convex and strongly convex functions. In the first part of the paper,\nwe study the case where the population risk function satisfies the Tysbakov\nNoise Condition (TNC) with some parameter $\\theta>1$. Specifically, we first\nshow that under some mild assumptions on the loss functions, there is an\nalgorithm whose output could achieve an upper bound of\n$\\tilde{O}((\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d\\log\n\\frac{1}{\\delta}}}{n\\epsilon})^\\frac{\\theta}{\\theta-1})$ for $(\\epsilon,\n\\delta)$-DP when $\\theta\\geq 2$, here $n$ is the sample size and $d$ is the\ndimension of the space. Then we address the inefficiency issue, improve the\nupper bounds by $\\text{Poly}(\\log n)$ factors and extend to the case where\n$\\theta\\geq \\bar{\\theta}>1$ for some known $\\bar{\\theta}$. Next we show that\nthe excess population risk of population functions satisfying TNC with\nparameter $\\theta>1$ is always lower bounded by\n$\\Omega((\\frac{d}{n\\epsilon})^\\frac{\\theta}{\\theta-1}) $ and\n$\\Omega((\\frac{\\sqrt{d\\log\n\\frac{1}{\\delta}}}{n\\epsilon})^\\frac{\\theta}{\\theta-1})$ for $\\epsilon$-DP and\n$(\\epsilon, \\delta)$-DP, respectively. In the second part, we focus on a\nspecial case where the population risk function is strongly convex. Unlike the\nprevious studies, here we assume the loss function is {\\em non-negative} and\n{\\em the optimal value of population risk is sufficiently small}. With these\nadditional assumptions, we propose a new method whose output could achieve an\nupper bound of\n$O(\\frac{d\\log\\frac{1}{\\delta}}{n^2\\epsilon^2}+\\frac{1}{n^{\\tau}})$ for any\n$\\tau\\geq 1$ in $(\\epsilon,\\delta)$-DP model if the sample size $n$ is\nsufficiently large.",
          "link": "http://arxiv.org/abs/2108.00331",
          "publishedOn": "2021-08-10T02:00:10.273Z",
          "wordCount": 703,
          "title": "Faster Rates of Differentially Private Stochastic Convex Optimization. (arXiv:2108.00331v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochiai_H/0/1/0/all/0/1\">Hideya Ochiai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esaki_H/0/1/0/all/0/1\">Hiroshi Esaki</a>",
          "description": "A wider coverage and a better solution to latency reduction in 5G\nnecessitates its combination with mobile edge computing (MEC) technology.\nDecentralized deep learning (DDL) as a promising solution to privacy-preserving\ndata processing for millions of edge smart devices, it leverages federated\nlearning within the networking of local models, without disclosing a client's\nraw data. Especially, in industries such as finance and healthcare where\nsensitive data of transactions and personal medical records is cautiously\nmaintained, DDL facilitates the collaboration among these institutes to improve\nthe performance of local models, while protecting data privacy of participating\nclients. In this survey paper, we demonstrate technical fundamentals of DDL for\nbenefiting many walks of society through decentralized learning. Furthermore,\nwe offer a comprehensive overview of recent challenges of DDL and the most\nrelevant solutions from novel perspectives of communication efficiency and\ntrustworthiness.",
          "link": "http://arxiv.org/abs/2108.03980",
          "publishedOn": "2021-08-10T02:00:10.265Z",
          "wordCount": 601,
          "title": "Decentralized Deep Learning for Mobile Edge Computing: A Survey on Communication Efficiency and Trustworthiness. (arXiv:2108.03980v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shipeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiale Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiangwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>",
          "description": "Incremental learning of semantic segmentation has emerged as a promising\nstrategy for visual scene interpretation in the open- world setting. However,\nit remains challenging to acquire novel classes in an online fashion for the\nsegmentation task, mainly due to its continuously-evolving semantic label\nspace, partial pixelwise ground-truth annotations, and constrained data\navailability. To ad- dress this, we propose an incremental learning strategy\nthat can fast adapt deep segmentation models without catastrophic forgetting,\nusing a streaming input data with pixel annotations on the novel classes only.\nTo this end, we develop a uni ed learning strategy based on the\nExpectation-Maximization (EM) framework, which integrates an iterative\nrelabeling strategy that lls in the missing labels and a rehearsal-based\nincremental learning step that balances the stability-plasticity of the model.\nMoreover, our EM algorithm adopts an adaptive sampling method to select\ninformative train- ing data and a class-balancing training strategy in the\nincremental model updates, both improving the e cacy of model learning. We\nvalidate our approach on the PASCAL VOC 2012 and ADE20K datasets, and the\nresults demonstrate its superior performance over the existing incremental\nmethods.",
          "link": "http://arxiv.org/abs/2108.03613",
          "publishedOn": "2021-08-10T02:00:10.246Z",
          "wordCount": 634,
          "title": "An EM Framework for Online Incremental Learning of Semantic Segmentation. (arXiv:2108.03613v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04051",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mustafa_A/0/1/0/all/0/1\">Ahmed Mustafa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buthe_J/0/1/0/all/0/1\">Jan B&#xfc;the</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korse_S/0/1/0/all/0/1\">Srikanth Korse</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_K/0/1/0/all/0/1\">Kishan Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fuchs_G/0/1/0/all/0/1\">Guillaume Fuchs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pia_N/0/1/0/all/0/1\">Nicola Pia</a>",
          "description": "Recently, GAN vocoders have seen rapid progress in speech synthesis, starting\nto outperform autoregressive models in perceptual quality with much higher\ngeneration speed. However, autoregressive vocoders are still the common choice\nfor neural generation of speech signals coded at very low bit rates. In this\npaper, we present a GAN vocoder which is able to generate wideband speech\nwaveforms from parameters coded at 1.6 kbit/s. The proposed model is a modified\nversion of the StyleMelGAN vocoder that can run in frame-by-frame manner,\nmaking it suitable for streaming applications. The experimental results show\nthat the proposed model significantly outperforms prior autoregressive vocoders\nlike LPCNet for very low bit rate speech coding, with computational complexity\nof about 5 GMACs, providing a new state of the art in this domain. Moreover,\nthis streamwise adversarial vocoder delivers quality competitive to advanced\nspeech codecs such as EVS at 5.9 kbit/s on clean speech, which motivates\nfurther usage of feed-forward fully-convolutional models for low bit rate\nspeech coding.",
          "link": "http://arxiv.org/abs/2108.04051",
          "publishedOn": "2021-08-10T02:00:10.240Z",
          "wordCount": 643,
          "title": "A Streamwise GAN Vocoder for Wideband Speech Coding at Very Low Bit Rate. (arXiv:2108.04051v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03979",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Baumgartner_S/0/1/0/all/0/1\">Stefan Baumgartner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huemer_M/0/1/0/all/0/1\">Mario Huemer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lunglmayr_M/0/1/0/all/0/1\">Michael Lunglmayr</a>",
          "description": "In recent years, machine learning methods became increasingly important for a\nmanifold number of applications. However, they often suffer from high\ncomputational requirements impairing their efficient use in real-time systems,\neven when employing dedicated hardware accelerators. Ensemble learning methods\nare especially suitable for hardware acceleration since they can be constructed\nfrom individual learners of low complexity and thus offer large parallelization\npotential. For classification, the outputs of these learners are typically\ncombined by majority voting, which often represents the bottleneck of a\nhardware accelerator for ensemble inference. In this work, we present a novel\narchitecture that allows obtaining a majority decision in a number of clock\ncycles that is logarithmic in the number of inputs. We show, that for the\nexample application of handwritten digit recognition a random forest processing\nengine employing this majority decision architecture implemented on an FPGA\nallows the classification of more than seven million images per second.",
          "link": "http://arxiv.org/abs/2108.03979",
          "publishedOn": "2021-08-10T02:00:10.233Z",
          "wordCount": 583,
          "title": "Efficient Majority Voting in Digital Hardware. (arXiv:2108.03979v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huimin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Rongwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhuyun Qi</a>",
          "description": "In the current deep learning based recommendation system, the embedding\nmethod is generally employed to complete the conversion from the\nhigh-dimensional sparse feature vector to the low-dimensional dense feature\nvector. However, as the dimension of the input vector of the embedding layer is\ntoo large, the addition of the embedding layer significantly slows down the\nconvergence speed of the entire neural network, which is not acceptable in\nreal-world scenarios. In addition, as the interaction between users and items\nincreases and the relationship between items becomes more complicated, the\nembedding method proposed for sequence data is no longer suitable for graphic\ndata in the current real environment. Therefore, in this paper, we propose the\nDual-modal Graph Embedding Method (DGEM) to solve these problems. DGEM includes\ntwo modes, static and dynamic. We first construct the item graph to extract the\ngraph structure and use random walk of unequal probability to capture the\nhigh-order proximity between the items. Then we generate the graph embedding\nvector through the Skip-Gram model, and finally feed the downstream deep neural\nnetwork for the recommendation task. The experimental results show that DGEM\ncan mine the high-order proximity between items and enhance the expression\nability of the recommendation model. Meanwhile it also improves the\nrecommendation performance by utilizing the time dependent relationship between\nitems.",
          "link": "http://arxiv.org/abs/2108.04031",
          "publishedOn": "2021-08-10T02:00:10.227Z",
          "wordCount": 659,
          "title": "DGEM: A New Dual-modal Graph Embedding Method in Recommendation System. (arXiv:2108.04031v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1\">Rayyan Ahmad Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinsteuber_M/0/1/0/all/0/1\">Martin Kleinsteuber</a>",
          "description": "Heterogeneous Information Network (HIN) embedding refers to the\nlow-dimensional projections of the HIN nodes that preserve the HIN structure\nand semantics. HIN embedding has emerged as a promising research field for\nnetwork analysis as it enables downstream tasks such as clustering and node\nclassification. In this work, we propose \\ours for joint learning of cluster\nembeddings as well as cluster-aware HIN embedding. We assume that the connected\nnodes are highly likely to fall in the same cluster, and adopt a variational\napproach to preserve the information in the pairwise relations in a\ncluster-aware manner. In addition, we deploy contrastive modules to\nsimultaneously utilize the information in multiple meta-paths, thereby\nalleviating the meta-path selection problem - a challenge faced by many of the\nfamous HIN embedding approaches. The HIN embedding, thus learned, not only\nimproves the clustering performance but also preserves pairwise proximity as\nwell as the high-order HIN structure. We show the effectiveness of our approach\nby comparing it with many competitive baselines on three real-world datasets on\nclustering and downstream node classification.",
          "link": "http://arxiv.org/abs/2108.03953",
          "publishedOn": "2021-08-10T02:00:10.210Z",
          "wordCount": 609,
          "title": "A Framework for Joint Unsupervised Learning of Cluster-Aware Embedding for Heterogeneous Networks. (arXiv:2108.03953v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kottke_D/0/1/0/all/0/1\">Daniel Kottke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krempl_G/0/1/0/all/0/1\">Georg Krempl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stecklina_M/0/1/0/all/0/1\">Marianne Stecklina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekowski_C/0/1/0/all/0/1\">Cornelius Styp von Rekowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabsch_T/0/1/0/all/0/1\">Tim Sabsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minh_T/0/1/0/all/0/1\">Tuan Pham Minh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deliano_M/0/1/0/all/0/1\">Matthias Deliano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiliopoulou_M/0/1/0/all/0/1\">Myra Spiliopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1\">Bernhard Sick</a>",
          "description": "In machine learning, active class selection (ACS) algorithms aim to actively\nselect a class and ask the oracle to provide an instance for that class to\noptimize a classifier's performance while minimizing the number of requests. In\nthis paper, we propose a new algorithm (PAL-ACS) that transforms the ACS\nproblem into an active learning task by introducing pseudo instances. These are\nused to estimate the usefulness of an upcoming instance for each class using\nthe performance gain model from probabilistic active learning. Our experimental\nevaluation (on synthetic and real data) shows the advantages of our algorithm\ncompared to state-of-the-art algorithms. It effectively prefers the sampling of\ndifficult classes and thereby improves the classification performance.",
          "link": "http://arxiv.org/abs/2108.03891",
          "publishedOn": "2021-08-10T02:00:10.204Z",
          "wordCount": 573,
          "title": "Probabilistic Active Learning for Active Class Selection. (arXiv:2108.03891v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosu_R/0/1/0/all/0/1\">Radu Alexandru Rosu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutt_P/0/1/0/all/0/1\">Peer Sch&#xfc;tt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quenzel_J/0/1/0/all/0/1\">Jan Quenzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>",
          "description": "Deep convolutional neural networks (CNNs) have shown outstanding performance\nin the task of semantically segmenting images. Applying the same methods on 3D\ndata still poses challenges due to the heavy memory requirements and the lack\nof structured data. Here, we propose LatticeNet, a novel approach for 3D\nsemantic segmentation, which takes raw point clouds as input. A PointNet\ndescribes the local geometry which we embed into a sparse permutohedral\nlattice. The lattice allows for fast convolutions while keeping a low memory\nfootprint. Further, we introduce DeformSlice, a novel learned data-dependent\ninterpolation for projecting lattice features back onto the point cloud. We\npresent results of 3D segmentation on multiple datasets where our method\nachieves state-of-the-art performance. We also extend and evaluate our network\nfor instance and dynamic object segmentation.",
          "link": "http://arxiv.org/abs/2108.03917",
          "publishedOn": "2021-08-10T02:00:10.197Z",
          "wordCount": 572,
          "title": "LatticeNet: Fast Spatio-Temporal Point Cloud Segmentation Using Permutohedral Lattices. (arXiv:2108.03917v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosendo_D/0/1/0/all/0/1\">Daniel Rosendo</a> (KerData), <a href=\"http://arxiv.org/find/cs/1/au:+Costan_A/0/1/0/all/0/1\">Alexandru Costan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniu_G/0/1/0/all/0/1\">Gabriel Antoniu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonin_M/0/1/0/all/0/1\">Matthieu Simonin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lombardo_J/0/1/0/all/0/1\">Jean-Christophe Lombardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joly_A/0/1/0/all/0/1\">Alexis Joly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valduriez_P/0/1/0/all/0/1\">Patrick Valduriez</a>",
          "description": "In more and more application areas, we are witnessing the emergence of\ncomplex workflows that combine computing, analytics and learning. They often\nrequire a hybrid execution infrastructure with IoT devices interconnected to\ncloud/HPC systems (aka Computing Continuum). Such workflows are subject to\ncomplex constraints and requirements in terms of performance, resource usage,\nenergy consumption and financial costs. This makes it challenging to optimize\ntheir configuration and deployment. We propose a methodology to support the\noptimization of real-life applications on the Edge-to-Cloud Continuum. We\nimplement it as an extension of E2Clab, a previously proposed framework\nsupporting the complete experimental cycle across the Edge-to-Cloud Continuum.\nOur approach relies on a rigorous analysis of possible configurations in a\ncontrolled testbed environment to understand their behaviour and related\nperformance trade-offs. We illustrate our methodology by optimizing Pl@ntNet, a\nworld-wide plant identification application. Our methodology can be generalized\nto other applications in the Edge-to-Cloud Continuum.",
          "link": "http://arxiv.org/abs/2108.04033",
          "publishedOn": "2021-08-10T02:00:10.190Z",
          "wordCount": 628,
          "title": "Reproducible Performance Optimization of Complex Applications on the Edge-to-Cloud Continuum. (arXiv:2108.04033v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaoning Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yuanqi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuehu Liu</a>",
          "description": "Fundamental machine learning theory shows that different samples contribute\nunequally both in learning and testing processes. Contemporary studies on DNN\nimply that such sample di?erence is rooted on the distribution of intrinsic\npattern information, namely sample regularity. Motivated by the recent\ndiscovery on network memorization and generalization, we proposed a pair of\nsample regularity measures for both processes with a formulation-consistent\nrepresentation. Specifically, cumulative binary training/generalizing loss\n(CBTL/CBGL), the cumulative number of correct classi?cations of the\ntraining/testing sample within training stage, is proposed to quantize the\nstability in memorization-generalization process; while\nforgetting/mal-generalizing events, i.e., the mis-classification of previously\nlearned or generalized sample, are utilized to represent the uncertainty of\nsample regularity with respect to optimization dynamics. Experiments validated\nthe effectiveness and robustness of the proposed approaches for mini-batch SGD\noptimization. Further applications on training/testing sample selection show\nthe proposed measures sharing the uni?ed computing procedure could benefit for\nboth tasks.",
          "link": "http://arxiv.org/abs/2108.03913",
          "publishedOn": "2021-08-10T02:00:10.183Z",
          "wordCount": 598,
          "title": "Unified Regularity Measures for Sample-wise Learning and Generalization. (arXiv:2108.03913v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Shuang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fanlin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xizhong Chen</a>",
          "description": "Non-intrusive load monitoring (NILM), which usually utilizes machine learning\nmethods and is effective in disaggregating smart meter readings from the\nhousehold-level into appliance-level consumptions, can help to analyze\nelectricity consumption behaviours of users and enable practical smart energy\nand smart grid applications. However, smart meters are privately owned and\ndistributed, which make real-world applications of NILM challenging. To this\nend, this paper develops a distributed and privacy-preserving federated deep\nlearning framework for NILM (FederatedNILM), which combines federated learning\nwith a state-of-the-art deep learning architecture to conduct NILM for the\nclassification of typical states of household appliances. Through extensive\ncomparative experiments, the effectiveness of the proposed FederatedNILM\nframework is demonstrated.",
          "link": "http://arxiv.org/abs/2108.03591",
          "publishedOn": "2021-08-10T02:00:10.177Z",
          "wordCount": 563,
          "title": "FederatedNILM: A Distributed and Privacy-preserving Framework for Non-intrusive Load Monitoring based on Federated Deep Learning. (arXiv:2108.03591v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pashazadeh_M/0/1/0/all/0/1\">Mostafa Pashazadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kui Wu</a>",
          "description": "Combinatorial optimization problems (COPs) on the graph with real-life\napplications are canonical challenges in Computer Science. The difficulty of\nfinding quality labels for problem instances holds back leveraging supervised\nlearning across combinatorial problems. Reinforcement learning (RL) algorithms\nhave recently been adopted to solve this challenge automatically. The\nunderlying principle of this approach is to deploy a graph neural network (GNN)\nfor encoding both the local information of the nodes and the graph-structured\ndata in order to capture the current state of the environment. Then, it is\nfollowed by the actor to learn the problem-specific heuristics on its own and\nmake an informed decision at each state for finally reaching a good solution.\nRecent studies on this subject mainly focus on a family of combinatorial\nproblems on the graph, such as the travel salesman problem, where the proposed\nmodel aims to find an ordering of vertices that optimizes a given objective\nfunction. We use the security-aware phone clone allocation in the cloud as a\nclassical quadratic assignment problem (QAP) to investigate whether or not deep\nRL-based model is generally applicable to solve other classes of such hard\nproblems. Extensive empirical evaluation shows that existing RL-based model may\nnot generalize to QAP.",
          "link": "http://arxiv.org/abs/2108.03713",
          "publishedOn": "2021-08-10T02:00:10.170Z",
          "wordCount": 631,
          "title": "On the Difficulty of Generalizing Reinforcement Learning Framework for Combinatorial Optimization. (arXiv:2108.03713v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1\">Sakib Shahriar</a>",
          "description": "\"Art is the lie that enables us to realize the truth.\" - Pablo Picasso. For\ncenturies, humans have dedicated themselves to producing arts to convey their\nimagination. The advancement in technology and deep learning in particular, has\ncaught the attention of many researchers trying to investigate whether art\ngeneration is possible by computers and algorithms. Using generative\nadversarial networks (GANs), applications such as synthesizing photorealistic\nhuman faces and creating captions automatically from images were realized. This\nsurvey takes a comprehensive look at the recent works using GANs for generating\nvisual arts, music, and literary text. A performance comparison and description\nof the various GAN architecture are also presented. Finally, some of the key\nchallenges in art generation using GANs are highlighted along with\nrecommendations for future work.",
          "link": "http://arxiv.org/abs/2108.03857",
          "publishedOn": "2021-08-10T02:00:10.162Z",
          "wordCount": 597,
          "title": "GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network. (arXiv:2108.03857v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_M/0/1/0/all/0/1\">Mao V. Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quek_T/0/1/0/all/0/1\">Tony Q.S. Quek</a>",
          "description": "The advances in deep neural networks (DNN) have significantly enhanced\nreal-time detection of anomalous data in IoT applications. However, the\ncomplexity-accuracy-delay dilemma persists: complex DNN models offer higher\naccuracy, but typical IoT devices can barely afford the computation load, and\nthe remedy of offloading the load to the cloud incurs long delay. In this\npaper, we address this challenge by proposing an adaptive anomaly detection\nscheme with hierarchical edge computing (HEC). Specifically, we first construct\nmultiple anomaly detection DNN models with increasing complexity, and associate\neach of them to a corresponding HEC layer. Then, we design an adaptive model\nselection scheme that is formulated as a contextual-bandit problem and solved\nby using a reinforcement learning policy network. We also incorporate a\nparallelism policy training method to accelerate the training process by taking\nadvantage of distributed models. We build an HEC testbed using real IoT\ndevices, implement and evaluate our contextual-bandit approach with both\nunivariate and multivariate IoT datasets. In comparison with both baseline and\nstate-of-the-art schemes, our adaptive approach strikes the best accuracy-delay\ntradeoff on the univariate dataset, and achieves the best accuracy and F1-score\non the multivariate dataset with only negligibly longer delay than the best\n(but inflexible) scheme.",
          "link": "http://arxiv.org/abs/2108.03872",
          "publishedOn": "2021-08-10T02:00:10.155Z",
          "wordCount": 673,
          "title": "Adaptive Anomaly Detection for Internet of Things in Hierarchical Edge Computing: A Contextual-Bandit Approach. (arXiv:2108.03872v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_D/0/1/0/all/0/1\">Darshan Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abichandani_H/0/1/0/all/0/1\">Harshavardhan Abichandani</a>",
          "description": "With active research in audio compression techniques yielding substantial\nbreakthroughs, spectral reconstruction of low-quality audio waves remains a\nless indulged topic. In this paper, we propose a novel approach for\nreconstructing higher frequencies from considerably longer sequences of\nlow-quality MP3 audio waves. Our technique involves inpainting audio\nspectrograms with residually stacked autoencoder blocks by manipulating\nindividual amplitude and phase values in relation to perceptual differences.\nOur architecture presents several bottlenecks while preserving the spectral\nstructure of the audio wave via skip-connections. We also compare several task\nmetrics and demonstrate our visual guide to loss selection. Moreover, we show\nhow to leverage differential quantization techniques to reduce the initial\nmodel size by more than half while simultaneously reducing inference time,\nwhich is crucial in real-world applications.",
          "link": "http://arxiv.org/abs/2108.03703",
          "publishedOn": "2021-08-10T02:00:10.107Z",
          "wordCount": 564,
          "title": "Audio Spectral Enhancement: Leveraging Autoencoders for Low Latency Reconstruction of Long, Lossy Audio Sequences. (arXiv:2108.03703v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Priyanshu_A/0/1/0/all/0/1\">Aman Priyanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1\">Rakshit Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malekzadeh_M/0/1/0/all/0/1\">Mohammad Malekzadeh</a>",
          "description": "Tuning the hyperparameters in the differentially private stochastic gradient\ndescent (DPSGD) is a fundamental challenge. Unlike the typical SGD, private\ndatasets cannot be used many times for hyperparameter search in DPSGD; e.g.,\nvia a grid search. Therefore, there is an essential need for algorithms that,\nwithin a given search space, can find near-optimal hyperparameters for the best\nachievable privacy-utility tradeoffs efficiently. We formulate this problem\ninto a general optimization framework for establishing a desirable\nprivacy-utility tradeoff, and systematically study three cost-effective\nalgorithms for being used in the proposed framework: evolutionary, Bayesian,\nand reinforcement learning. Our experiments, for hyperparameter tuning in DPSGD\nconducted on MNIST and CIFAR-10 datasets, show that these three algorithms\nsignificantly outperform the widely used grid search baseline. As this paper\noffers a first-of-a-kind framework for hyperparameter tuning in DPSGD, we\ndiscuss existing challenges and open directions for future studies. As we\nbelieve our work has implications to be utilized in the pipeline of private\ndeep learning, we open-source our code at\nhttps://github.com/AmanPriyanshu/DP-HyperparamTuning.",
          "link": "http://arxiv.org/abs/2108.03888",
          "publishedOn": "2021-08-10T02:00:10.089Z",
          "wordCount": 608,
          "title": "Efficient Hyperparameter Optimization for Differentially Private Deep Learning. (arXiv:2108.03888v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alsaidi_S/0/1/0/all/0/1\">Safa Alsaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decker_A/0/1/0/all/0/1\">Amandine Decker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lay_P/0/1/0/all/0/1\">Puthineath Lay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marquer_E/0/1/0/all/0/1\">Esteban Marquer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murena_P/0/1/0/all/0/1\">Pierre-Alexandre Murena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couceiro_M/0/1/0/all/0/1\">Miguel Couceiro</a>",
          "description": "Analogical proportions are statements of the form \"A is to B as C is to D\"\nthat are used for several reasoning and classification tasks in artificial\nintelligence and natural language processing (NLP). For instance, there are\nanalogy based approaches to semantics as well as to morphology. In fact,\nsymbolic approaches were developed to solve or to detect analogies between\ncharacter strings, e.g., the axiomatic approach as well as that based on\nKolmogorov complexity. In this paper, we propose a deep learning approach to\ndetect morphological analogies, for instance, with reinflexion or conjugation.\nWe present empirical results that show that our framework is competitive with\nthe above-mentioned state of the art symbolic approaches. We also explore\nempirically its transferability capacity across languages, which highlights\ninteresting similarities between them.",
          "link": "http://arxiv.org/abs/2108.03945",
          "publishedOn": "2021-08-10T02:00:10.082Z",
          "wordCount": 586,
          "title": "A Neural Approach for Detecting Morphological Analogies. (arXiv:2108.03945v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yaobin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weitang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhongyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zixuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_T/0/1/0/all/0/1\">Tingyun Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lili Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingwei Zhou</a>",
          "description": "Traffic forecasting is a core element of intelligent traffic monitoring\nsystem. Approaches based on graph neural networks have been widely used in this\ntask to effectively capture spatial and temporal dependencies of road networks.\nHowever, these approaches can not effectively define the complicated network\ntopology. Besides, their cascade network structures have limitations in\ntransmitting distinct features in the time and space dimensions. In this paper,\nwe propose a Multi-adaptive Spatiotemporal-flow Graph Neural Network (MAF-GNN)\nfor traffic speed forecasting. MAF-GNN introduces an effective Multi-adaptive\nAdjacency Matrices Mechanism to capture multiple latent spatial dependencies\nbetween traffic nodes. Additionally, we propose Spatiotemporal-flow Modules\naiming to further enhance feature propagation in both time and space\ndimensions. MAF-GNN achieves better performance than other models on two\nreal-world datasets of public traffic network, METR-LA and PeMS-Bay,\ndemonstrating the effectiveness of the proposed approach.",
          "link": "http://arxiv.org/abs/2108.03594",
          "publishedOn": "2021-08-10T02:00:10.074Z",
          "wordCount": 576,
          "title": "MAF-GNN: Multi-adaptive Spatiotemporal-flow Graph Neural Network for Traffic Speed Forecasting. (arXiv:2108.03594v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Subhroshekhar Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarlett_J/0/1/0/all/0/1\">Jonathan Scarlett</a>",
          "description": "In 1-bit compressive sensing, each measurement is quantized to a single bit,\nnamely the sign of a linear function of an unknown vector, and the goal is to\naccurately recover the vector. While it is most popular to assume a standard\nGaussian sensing matrix for 1-bit compressive sensing, using structured sensing\nmatrices such as partial Gaussian circulant matrices is of significant\npractical importance due to their faster matrix operations. In this paper, we\nprovide recovery guarantees for a correlation-based optimization algorithm for\nrobust 1-bit compressive sensing with randomly signed partial Gaussian\ncirculant matrices and generative models. Under suitable assumptions, we match\nguarantees that were previously only known to hold for i.i.d.~Gaussian matrices\nthat require significantly more computation. We make use of a practical\niterative algorithm, and perform numerical experiments on image datasets to\ncorroborate our theoretical results.",
          "link": "http://arxiv.org/abs/2108.03570",
          "publishedOn": "2021-08-10T02:00:10.067Z",
          "wordCount": 585,
          "title": "Robust 1-bit Compressive Sensing with Partial Gaussian Circulant Matrices and Generative Priors. (arXiv:2108.03570v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runze Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haiyong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xuechun Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhiqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yida Zhu</a>",
          "description": "Human Activity Recognition (HAR) based on IMU sensors is a crucial area in\nubiquitous computing. Because of the trend of deploying AI on IoT devices or\nsmartphones, more researchers are designing different HAR models for embedded\ndevices. Deployment of models in embedded devices can help enhance the\nefficiency of HAR. We propose a multi-level HAR modeling pipeline called\nStage-Logits-Memory Distillation (SMLDist) for constructing deep convolutional\nHAR models with embedded hardware support. SMLDist includes stage distillation,\nmemory distillation, and logits distillation. Stage distillation constrains the\nlearning direction of the intermediate features. The teacher model teaches the\nstudent models how to explain and store the inner relationship among\nhigh-dimensional features based on Hopfield networks in memory distillation.\nLogits distillation builds logits distilled by a smoothed conditional rule to\npreserve the probability distribution and enhance the softer target accuracy.\nWe compare the accuracy, F1 macro score, and energy cost on embedded platforms\nof a MobileNet V3 model built by our SMLDist with those of various\nstate-of-the-art HAR frameworks. The product model has a good balance with\nrobustness and efficiency. SMLDist can also compress models with a minor\nperformance loss at an equal compression ratio to other advanced knowledge\ndistillation methods on seven public datasets.",
          "link": "http://arxiv.org/abs/2107.07331",
          "publishedOn": "2021-08-10T02:00:10.045Z",
          "wordCount": 704,
          "title": "Modeling Accurate Human Activity Recognition for Embedded Devices Using Multi-level Distillation. (arXiv:2107.07331v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mondal_S/0/1/0/all/0/1\">Sudipta Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manasi_S/0/1/0/all/0/1\">Susmita Dey Manasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunal_K/0/1/0/all/0/1\">Kishor Kunal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramprasath_S/0/1/0/all/0/1\">S. Ramprasath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapatnekar_S/0/1/0/all/0/1\">Sachin S. Sapatnekar</a>",
          "description": "Graph neural networks (GNN) analysis engines are vital for real-world\nproblems that use large graph models. Challenges for a GNN hardware platform\ninclude the ability to (a) host a variety of GNNs, (b) handle high sparsity in\ninput vertex feature vectors and the graph adjacency matrix and the\naccompanying random memory access patterns, and (c) maintain load-balanced\ncomputation in the face of uneven workloads, induced by high sparsity and\npower-law vertex degree distributions. This paper proposes GNNIE, an\naccelerator designed to run a broad range of GNNs. It tackles workload\nimbalance by (i)~splitting vertex feature operands into blocks, (ii)~reordering\nand redistributing computations, (iii)~using a novel flexible MAC architecture.\nIt adopts a graph-specific, degree-aware caching policy that is well suited to\nreal-world graph characteristics. The policy enhances on-chip data reuse and\navoids random memory access to DRAM.\n\nGNNIE achieves average speedups of 21233x over a CPU and 699x over a GPU over\nmultiple datasets on graph attention networks (GATs), graph convolutional\nnetworks (GCNs), GraphSAGE, GINConv, and DiffPool. Compared to prior\napproaches, GNNIE achieves an average speedup of 35x over HyGCN (which cannot\nimplement GATs) for GCN, GraphSAGE, and GINConv, and, using 3.4x fewer\nprocessing units, an average speedup of 2.1x over AWB-GCN (which runs only\nGCNs).",
          "link": "http://arxiv.org/abs/2105.10554",
          "publishedOn": "2021-08-10T02:00:10.037Z",
          "wordCount": 674,
          "title": "GNNIE: GNN Inference Engine with Load-balancing and Graph-Specific Caching. (arXiv:2105.10554v2 [cs.AR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03782",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carpenter_B/0/1/0/all/0/1\">Bob Carpenter</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gelman_A/0/1/0/all/0/1\">Andrew Gelman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vehtari_A/0/1/0/all/0/1\">Aki Vehtari</a>",
          "description": "We introduce Pathfinder, a variational method for approximately sampling from\ndifferentiable log densities. Starting from a random initialization, Pathfinder\nlocates normal approximations to the target density along a quasi-Newton\noptimization path, with local covariance estimated using the inverse Hessian\nestimates produced by the optimizer. Pathfinder returns draws from the\napproximation with the lowest estimated Kullback-Leibler (KL) divergence to the\ntrue posterior. We evaluate Pathfinder on a wide range of posterior\ndistributions, demonstrating that its approximate draws are better than those\nfrom automatic differentiation variational inference (ADVI) and comparable to\nthose produced by short chains of dynamic Hamiltonian Monte Carlo (HMC), as\nmeasured by 1-Wasserstein distance. Compared to ADVI and short dynamic HMC\nruns, Pathfinder requires one to two orders of magnitude fewer log density and\ngradient evaluations, with greater reductions for more challenging posteriors.\nImportance resampling over multiple runs of Pathfinder improves the diversity\nof approximate draws, reducing 1-Wasserstein distance further and providing a\nmeasure of robustness to optimization failures on plateaus, saddle points, or\nin minor modes. The Monte Carlo KL-divergence estimates are embarrassingly\nparallelizable in the core Pathfinder algorithm, as are multiple runs in the\nresampling version, further increasing Pathfinder's speed advantage with\nmultiple cores.",
          "link": "http://arxiv.org/abs/2108.03782",
          "publishedOn": "2021-08-10T02:00:10.028Z",
          "wordCount": 630,
          "title": "Pathfinder: Parallel quasi-Newton variational inference. (arXiv:2108.03782v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Buechler_R/0/1/0/all/0/1\">Robert Buechler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balogun_E/0/1/0/all/0/1\">Emmanuel Balogun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1\">Arun Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_R/0/1/0/all/0/1\">Ram Rajagopal</a>",
          "description": "The nexus between transportation, the power grid, and consumer behavior is\nmore pronounced than ever before as the race to decarbonize the transportation\nsector intensifies. Electrification in the transportation sector has led to\ntechnology shifts and rapid deployment of electric vehicles (EVs). The\npotential increase in stochastic and spatially heterogeneous charging load\npresents a unique challenge that is not well studied, and will have significant\nimpacts on grid operations, emissions, and system reliability if not managed\neffectively. Realistic scenario generators can help operators prepare, and\nmachine learning can be leveraged to this end. In this work, we develop\ngenerative adversarial networks (GANs) to learn distributions of electric\nvehicle (EV) charging sessions and disentangled representations. We show that\nthis model structure successfully parameterizes unlabeled temporal and power\npatterns without supervision and is able to generate synthetic data conditioned\non these parameters. We benchmark the generation capability of this model with\nGaussian Mixture Models (GMMs), and empirically show that our proposed model\nframework is better at capturing charging distributions and temporal dynamics.",
          "link": "http://arxiv.org/abs/2108.03762",
          "publishedOn": "2021-08-10T02:00:10.019Z",
          "wordCount": 624,
          "title": "EVGen: Adversarial Networks for Learning Electric Vehicle Charging Loads and Hidden Representations. (arXiv:2108.03762v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cong T. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_N/0/1/0/all/0/1\">Nguyen Van Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_N/0/1/0/all/0/1\">Nam H. Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saputra_Y/0/1/0/all/0/1\">Yuris Mulya Saputra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_D/0/1/0/all/0/1\">Dinh Thai Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Diep N. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1\">Quoc-Viet Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1\">Dusit Niyato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutkiewicz_E/0/1/0/all/0/1\">Eryk Dutkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Won-Joo Hwang</a>",
          "description": "With outstanding features, Machine Learning (ML) has been the backbone of\nnumerous applications in wireless networks. However, the conventional ML\napproaches have been facing many challenges in practical implementation, such\nas the lack of labeled data, the constantly changing wireless environments, the\nlong training process, and the limited capacity of wireless devices. These\nchallenges, if not addressed, will impede the effectiveness and applicability\nof ML in future wireless networks. To address these problems, Transfer Learning\n(TL) has recently emerged to be a very promising solution. The core idea of TL\nis to leverage and synthesize distilled knowledge from similar tasks as well as\nfrom valuable experiences accumulated from the past to facilitate the learning\nof new problems. Doing so, TL techniques can reduce the dependence on labeled\ndata, improve the learning speed, and enhance the ML methods' robustness to\ndifferent wireless environments. This article aims to provide a comprehensive\nsurvey on applications of TL in wireless networks. Particularly, we first\nprovide an overview of TL including formal definitions, classification, and\nvarious types of TL techniques. We then discuss diverse TL approaches proposed\nto address emerging issues in wireless networks. The issues include spectrum\nmanagement, localization, signal recognition, security, human activity\nrecognition and caching, which are all important to next-generation networks\nsuch as 5G and beyond. Finally, we highlight important challenges, open issues,\nand future research directions of TL in future wireless networks.",
          "link": "http://arxiv.org/abs/2102.07572",
          "publishedOn": "2021-08-10T02:00:09.936Z",
          "wordCount": 722,
          "title": "Transfer Learning for Future Wireless Networks: A Comprehensive Survey. (arXiv:2102.07572v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Planta_C/0/1/0/all/0/1\">Cyrill von Planta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopanicakova_A/0/1/0/all/0/1\">Alena Kopanicakova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_R/0/1/0/all/0/1\">Rolf Krause</a>",
          "description": "We train deep residual networks with a stochastic variant of the nonlinear\nmultigrid method MG/OPT. To build the multilevel hierarchy, we use the\ndynamical systems viewpoint specific to residual networks. We report\nsignificant speed-ups and additional robustness for training MNIST on deep\nresidual networks. Our numerical experiments also indicate that multilevel\ntraining can be used as a pruning technique, as many of the auxiliary networks\nhave accuracies comparable to the original network.",
          "link": "http://arxiv.org/abs/2108.04052",
          "publishedOn": "2021-08-10T02:00:09.906Z",
          "wordCount": null,
          "title": "Training of deep residual networks with stochastic MG/OPT. (arXiv:2108.04052v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lijie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_S/0/1/0/all/0/1\">Shuo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Hanshen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>",
          "description": "As one of the most fundamental problems in machine learning, statistics and\ndifferential privacy, Differentially Private Stochastic Convex Optimization\n(DP-SCO) has been extensively studied in recent years. However, most of the\nprevious work can only handle either regular data distribution or irregular\ndata in the low dimensional space case. To better understand the challenges\narising from irregular data distribution, in this paper we provide the first\nstudy on the problem of DP-SCO with heavy-tailed data in the high dimensional\nspace. In the first part we focus on the problem over some polytope constraint\n(such as the $\\ell_1$-norm ball). We show that if the loss function is smooth\nand its gradient has bounded second order moment, it is possible to get a (high\nprobability) error bound (excess population risk) of $\\tilde{O}(\\frac{\\log\nd}{(n\\epsilon)^\\frac{1}{3}})$ in the $\\epsilon$-DP model, where $n$ is the\nsample size and $d$ is the dimensionality of the underlying space. Next, for\nLASSO, if the data distribution that has bounded fourth-order moments, we\nimprove the bound to $\\tilde{O}(\\frac{\\log d}{(n\\epsilon)^\\frac{2}{5}})$ in the\n$(\\epsilon, \\delta)$-DP model. In the second part of the paper, we study sparse\nlearning with heavy-tailed data. We first revisit the sparse linear model and\npropose a truncated DP-IHT method whose output could achieve an error of\n$\\tilde{O}(\\frac{s^{*2}\\log d}{n\\epsilon})$, where $s^*$ is the sparsity of the\nunderlying parameter. Then we study a more general problem over the sparsity\n({\\em i.e.,} $\\ell_0$-norm) constraint, and show that it is possible to achieve\nan error of $\\tilde{O}(\\frac{s^{*\\frac{3}{2}}\\log d}{n\\epsilon})$, which is\nalso near optimal up to a factor of $\\tilde{O}{(\\sqrt{s^*})}$, if the loss\nfunction is smooth and strongly convex.",
          "link": "http://arxiv.org/abs/2107.11136",
          "publishedOn": "2021-08-10T02:00:09.838Z",
          "wordCount": 734,
          "title": "High Dimensional Differentially Private Stochastic Optimization with Heavy-tailed Data. (arXiv:2107.11136v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Inturrisi_J/0/1/0/all/0/1\">Jordan Inturrisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoo_S/0/1/0/all/0/1\">Sui Yang Khoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouzani_A/0/1/0/all/0/1\">Abbas Kouzani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagliarella_R/0/1/0/all/0/1\">Riccardo Pagliarella</a>",
          "description": "The activation function is at the heart of a deep neural networks\nnonlinearity; the choice of the function has great impact on the success of\ntraining. Currently, many practitioners prefer the Rectified Linear Unit (ReLU)\ndue to its simplicity and reliability, despite its few drawbacks. While most\nprevious functions proposed to supplant ReLU have been hand-designed, recent\nwork on learning the function during training has shown promising results. In\nthis paper we propose an adaptive piecewise linear activation function, the\nPiecewise Linear Unit (PiLU), which can be learned independently for each\ndimension of the neural network. We demonstrate how PiLU is a generalised\nrectifier unit and note its similarities with the Adaptive Piecewise Linear\nUnits, namely adaptive and piecewise linear. Across a distribution of 30\nexperiments, we show that for the same model architecture, hyperparameters, and\npre-processing, PiLU significantly outperforms ReLU: reducing classification\nerror by 18.53% on CIFAR-10 and 13.13% on CIFAR-100, for a minor increase in\nthe number of neurons. Further work should be dedicated to exploring\ngeneralised piecewise linear units, as well as verifying these results across\nother challenging domains and larger problems.",
          "link": "http://arxiv.org/abs/2108.00700",
          "publishedOn": "2021-08-10T02:00:09.826Z",
          "wordCount": 646,
          "title": "Piecewise Linear Units Improve Deep Neural Networks. (arXiv:2108.00700v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.10581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1\">Aman Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OKelly_M/0/1/0/all/0/1\">Matthew O&#x27;Kelly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tedrake_R/0/1/0/all/0/1\">Russ Tedrake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duchi_J/0/1/0/all/0/1\">John Duchi</a>",
          "description": "Learning-based methodologies increasingly find applications in\nsafety-critical domains like autonomous driving and medical robotics. Due to\nthe rare nature of dangerous events, real-world testing is prohibitively\nexpensive and unscalable. In this work, we employ a probabilistic approach to\nsafety evaluation in simulation, where we are concerned with computing the\nprobability of dangerous events. We develop a novel rare-event simulation\nmethod that combines exploration, exploitation, and optimization techniques to\nfind failure modes and estimate their rate of occurrence. We provide rigorous\nguarantees for the performance of our method in terms of both statistical and\ncomputational efficiency. Finally, we demonstrate the efficacy of our approach\non a variety of scenarios, illustrating its usefulness as a tool for rapid\nsensitivity analysis and model comparison that are essential to developing and\ntesting safety-critical autonomous systems.",
          "link": "http://arxiv.org/abs/2008.10581",
          "publishedOn": "2021-08-10T02:00:09.268Z",
          "wordCount": 608,
          "title": "Neural Bridge Sampling for Evaluating Safety-Critical Autonomous Systems. (arXiv:2008.10581v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuesong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Guinan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>",
          "description": "Generative Adversarial Networks (GANs) are formulated as minimax game\nproblems, whereby generators attempt to approach real data distributions by\nvirtue of adversarial learning against discriminators. The intrinsic problem\ncomplexity poses the challenge to enhance the performance of generative\nnetworks. In this work, we aim to boost model learning from the perspective of\nnetwork architectures, by incorporating recent progress on automated\narchitecture search into GANs. To this end, we propose a fully differentiable\nsearch framework for generative adversarial networks, dubbed alphaGAN. The\nsearching process is formalized as solving a bi-level minimax optimization\nproblem, in which the outer-level objective aims for seeking a suitable network\narchitecture towards pure Nash Equilibrium conditioned on the generator and the\ndiscriminator network parameters optimized with a traditional GAN loss in the\ninner level. The entire optimization performs a first-order method by\nalternately minimizing the two-level objective in a fully differentiable\nmanner, enabling architecture search to be completed in an enormous search\nspace. Extensive experiments on CIFAR-10 and STL-10 datasets show that our\nalgorithm can obtain high-performing architectures only with 3-GPU hours on a\nsingle GPU in the search space comprised of approximate 2 ? 1011 possible\nconfigurations. We also provide a comprehensive analysis on the behavior of the\nsearching process and the properties of searched architectures, which would\nbenefit further research on architectures for generative models. Pretrained\nmodels and codes are available at https://github.com/yuesongtian/AlphaGAN.",
          "link": "http://arxiv.org/abs/2006.09134",
          "publishedOn": "2021-08-10T02:00:09.237Z",
          "wordCount": 730,
          "title": "AlphaGAN: Fully Differentiable Architecture Search for Generative Adversarial Networks. (arXiv:2006.09134v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.04138",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Szeskin_A/0/1/0/all/0/1\">Adi Szeskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faivishevsky_L/0/1/0/all/0/1\">Lev Faivishevsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muppalla_A/0/1/0/all/0/1\">Ashwin K Muppalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armon_A/0/1/0/all/0/1\">Amitai Armon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>",
          "description": "We present a deep learning system for testing graphics units by detecting\nnovel visual corruptions in videos. Unlike previous work in which manual\ntagging was required to collect labeled training data, our weak supervision\nmethod is fully automatic and needs no human labelling. This is achieved by\nreproducing driver bugs that increase the probability of generating\ncorruptions, and by making use of ideas and methods from the Multiple Instance\nLearning (MIL) setting. In our experiments, we significantly outperform\nunsupervised methods such as GAN-based models and discover novel corruptions\nundetected by baselines, while adhering to strict requirements on accuracy and\nefficiency of our real-time system.",
          "link": "http://arxiv.org/abs/1912.04138",
          "publishedOn": "2021-08-10T02:00:09.231Z",
          "wordCount": 591,
          "title": "A Weak Supervision Approach to Detecting Visual Anomalies for Automated Testing of Graphics Units. (arXiv:1912.04138v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tsung-Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueh-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Ping-Chia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>",
          "description": "Despite the success of deep learning on supervised point cloud semantic\nsegmentation, obtaining large-scale point-by-point manual annotations is still\na significant challenge. To reduce the huge annotation burden, we propose a\nRegion-based and Diversity-aware Active Learning (ReDAL), a general framework\nfor many deep learning approaches, aiming to automatically select only\ninformative and diverse sub-scene regions for label acquisition. Observing that\nonly a small portion of annotated regions are sufficient for 3D scene\nunderstanding with deep learning, we use softmax entropy, color discontinuity,\nand structural complexity to measure the information of sub-scene regions. A\ndiversity-aware selection algorithm is also developed to avoid redundant\nannotations resulting from selecting informative but similar regions in a\nquerying batch. Extensive experiments show that our method highly outperforms\nprevious active learning strategies, and we achieve the performance of 90%\nfully supervised learning, while less than 15% and 5% annotations are required\non S3DIS and SemanticKITTI datasets, respectively.",
          "link": "http://arxiv.org/abs/2107.11769",
          "publishedOn": "2021-08-10T02:00:09.223Z",
          "wordCount": 638,
          "title": "ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation. (arXiv:2107.11769v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03531",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1\">Ammar Shaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shujian Yu</a>",
          "description": "The similarity of feature representations plays a pivotal role in the success\nof domain adaptation and generalization. Feature similarity includes both the\ninvariance of marginal distributions and the closeness of conditional\ndistributions given the desired response $y$ (e.g., class labels).\nUnfortunately, traditional methods always learn such features without fully\ntaking into consideration the information in $y$, which in turn may lead to a\nmismatch of the conditional distributions or the mix-up of discriminative\nstructures underlying data distributions. In this work, we introduce the\nrecently proposed von Neumann conditional divergence to improve the\ntransferability across multiple domains. We show that this new divergence is\ndifferentiable and eligible to easily quantify the functional dependence\nbetween features and $y$. Given multiple source tasks, we integrate this\ndivergence to capture discriminative information in $y$ and design novel\nlearning objectives assuming those source tasks are observed either\nsimultaneously or sequentially. In both scenarios, we obtain favorable\nperformance against state-of-the-art methods in terms of smaller generalization\nerror on new tasks and less catastrophic forgetting on source tasks (in the\nsequential setup).",
          "link": "http://arxiv.org/abs/2108.03531",
          "publishedOn": "2021-08-10T02:00:09.165Z",
          "wordCount": 606,
          "title": "Learning to Transfer with von Neumann Conditional Divergence. (arXiv:2108.03531v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1811.00414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_E/0/1/0/all/0/1\">Ewin Tang</a>",
          "description": "A central roadblock to analyzing quantum algorithms on quantum states is the\nlack of a comparable input model for classical algorithms. Inspired by recent\nwork of the author [E. Tang, STOC'19], we introduce such a model, where we\nassume we can efficiently perform $\\ell^2$-norm samples of input data, a\nnatural analogue to quantum algorithms that assume efficient state preparation\nof classical data. Though this model produces less practical algorithms than\nthe (stronger) standard model of classical computation, it captures versions of\nmany of the features and nuances of quantum linear algebra algorithms. With\nthis model, we describe classical analogues to Lloyd, Mohseni, and Rebentrost's\nquantum algorithms for principal component analysis [Nat. Phys. 10, 631 (2014)]\nand nearest-centroid clustering [arXiv:1307.0411]. Since they are only\npolynomially slower, these algorithms suggest that the exponential speedups of\ntheir quantum counterparts are simply an artifact of state preparation\nassumptions.",
          "link": "http://arxiv.org/abs/1811.00414",
          "publishedOn": "2021-08-10T02:00:09.125Z",
          "wordCount": 673,
          "title": "Quantum principal component analysis only achieves an exponential speedup because of its state preparation assumptions. (arXiv:1811.00414v3 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jamisola_R/0/1/0/all/0/1\">Rodrigo S. Jamisola Jr</a>",
          "description": "This paper investigates the possibility of creating a machine learning tool\nthat automatically determines the state of mind and emotion of an individual\nthrough a questionnaire, without the aid of a human expert. The state of mind\nand emotion is defined in this work as pertaining to preference, feelings, or\nopinion that is not based on logic or reason. It is the case when a person\ngives out an answer to start by saying, \"I feel...\". The tool is designed to\nmimic the expertise of a psychologist and is built without any formal knowledge\nof psychology. The idea is to build the expertise by purely computational\nmethods through thousands of questions collected from users. It is aimed\ntowards possibly diagnosing substance addiction, alcoholism, sexual attraction,\nHIV status, degree of commitment, activity inclination, etc. First, the paper\npresents the related literature and classifies them according to data gathering\nmethods. Another classification is created according to preference, emotion,\ngrouping, and rules to achieve a deeper interpretation and better understanding\nof the state of mind and emotion. Second, the proposed tool is developed using\nan online addiction questionnaire with 10 questions and 292 respondents. In\naddition, an initial investigation on the dimension of addiction is presented\nthrough the built machine learning model. Machine learning methods, namely,\nartificial neural network (ANN) and support vector machine (SVM), are used to\ndetermine a true or false or degree of state of a respondent.",
          "link": "http://arxiv.org/abs/2108.03444",
          "publishedOn": "2021-08-10T02:00:09.084Z",
          "wordCount": 677,
          "title": "A Machine Learning Tool to Determine State of Mind and Emotion. (arXiv:2108.03444v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1909.11799",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perry_R/0/1/0/all/0/1\">Ronan Perry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Adam Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_C/0/1/0/all/0/1\">Chester Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomita_T/0/1/0/all/0/1\">Tyler M. Tomita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1\">Ronak Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arroyo_J/0/1/0/all/0/1\">Jesus Arroyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patsolic_J/0/1/0/all/0/1\">Jesse Patsolic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falk_B/0/1/0/all/0/1\">Benjamin Falk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1\">Joshua T. Vogelstein</a>",
          "description": "Decision forests (Forests), in particular random forests and gradient\nboosting trees, have demonstrated state-of-the-art accuracy compared to other\nmethods in many supervised learning scenarios. In particular, Forests dominate\nother methods in tabular data, that is, when the feature space is unstructured,\nso that the signal is invariant to a permutation of the feature indices.\nHowever, in structured data lying on a manifold (such as images, text, and\nspeech) deep networks (Networks), specifically convolutional deep networks\n(ConvNets), tend to outperform Forests. We conjecture that at least part of the\nreason for this is that the input to Networks is not simply the feature\nmagnitudes, but also their indices. In contrast, naive Forest implementations\nfail to explicitly consider feature indices. A recently proposed Forest\napproach demonstrates that Forests, for each node, implicitly sample a random\nmatrix from some specific distribution. These Forests, like some classes of\nNetworks, learn by partitioning the feature space into convex polytopes\ncorresponding to linear functions. We build on that approach and show that one\ncan choose distributions in a manifold-aware fashion to incorporate feature\nlocality. We demonstrate the empirical performance on data whose features live\non three different manifolds: a torus, images, and time-series. Moreover, we\ndemonstrate its strength in multivariate simulated settings and also show\nsuperiority in predicting surgical outcome in epilepsy patients and predicting\nmovement direction from raw stereotactic EEG data from non-motor brain regions.\nIn all simulations and real data, Manifold Oblique Random Forest (MORF)\nalgorithm outperforms approaches that ignore feature space structure and\nchallenges the performance of ConvNets. Moreover, MORF runs fast and maintains\ninterpretability and theoretical justification.",
          "link": "http://arxiv.org/abs/1909.11799",
          "publishedOn": "2021-08-10T02:00:09.078Z",
          "wordCount": 769,
          "title": "Manifold Oblique Random Forests: Towards Closing the Gap on Convolutional Deep Networks. (arXiv:1909.11799v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.15634",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Foorthuis_R/0/1/0/all/0/1\">Ralph Foorthuis</a>",
          "description": "Anomalies are occurrences in a dataset that are in some way unusual and do\nnot fit the general patterns. The concept of the anomaly is typically\nill-defined and perceived as vague and domain-dependent. Moreover, despite some\n250 years of publications on the topic, no comprehensive and concrete overviews\nof the different types of anomalies have hitherto been published. By means of\nan extensive literature review this study therefore offers the first\ntheoretically principled and domain-independent typology of data anomalies and\npresents a full overview of anomaly types and subtypes. To concretely define\nthe concept of the anomaly and its different manifestations, the typology\nemploys five dimensions: data type, cardinality of relationship, anomaly level,\ndata structure, and data distribution. These fundamental and data-centric\ndimensions naturally yield 3 broad groups, 9 basic types, and 63 subtypes of\nanomalies. The typology facilitates the evaluation of the functional\ncapabilities of anomaly detection algorithms, contributes to explainable data\nscience, and provides insights into relevant topics such as local versus global\nanomalies.",
          "link": "http://arxiv.org/abs/2007.15634",
          "publishedOn": "2021-08-10T02:00:09.033Z",
          "wordCount": 717,
          "title": "On the Nature and Types of Anomalies: A Review of Deviations in Data. (arXiv:2007.15634v4 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Cheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Abhishek Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzey_J/0/1/0/all/0/1\">Joseph Linzey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Rushikesh Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sung Jik Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Sudharsan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alber_D/0/1/0/all/0/1\">Daniel Alber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondepudi_A/0/1/0/all/0/1\">Akhil Kondepudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urias_E/0/1/0/all/0/1\">Esteban Urias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandian_B/0/1/0/all/0/1\">Balaji Pandian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Holou_W/0/1/0/all/0/1\">Wajd Al-Holou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_S/0/1/0/all/0/1\">Steve Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_B/0/1/0/all/0/1\">B. Gregory Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heth_J/0/1/0/all/0/1\">Jason Heth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freudiger_C/0/1/0/all/0/1\">Chris Freudiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalsa_S/0/1/0/all/0/1\">Siri Khalsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacione_D/0/1/0/all/0/1\">Donato Pacione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golfinos_J/0/1/0/all/0/1\">John G. Golfinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camelo_Piragua_S/0/1/0/all/0/1\">Sandra Camelo-Piragua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orringer_D/0/1/0/all/0/1\">Daniel A. Orringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollon_T/0/1/0/all/0/1\">Todd Hollon</a>",
          "description": "Background: Accurate diagnosis of skull base tumors is essential for\nproviding personalized surgical treatment strategies. Intraoperative diagnosis\ncan be challenging due to tumor diversity and lack of intraoperative pathology\nresources.\n\nObjective: To develop an independent and parallel intraoperative pathology\nworkflow that can provide rapid and accurate skull base tumor diagnoses using\nlabel-free optical imaging and artificial intelligence (AI).\n\nMethod: We used a fiber laser-based, label-free, non-consumptive,\nhigh-resolution microscopy method ($<$ 60 sec per 1 $\\times$ 1 mm$^\\text{2}$),\ncalled stimulated Raman histology (SRH), to image a consecutive, multicenter\ncohort of skull base tumor patients. SRH images were then used to train a\nconvolutional neural network (CNN) model using three representation learning\nstrategies: cross-entropy, self-supervised contrastive learning, and supervised\ncontrastive learning. Our trained CNN models were tested on a held-out,\nmulticenter SRH dataset.\n\nResults: SRH was able to image the diagnostic features of both benign and\nmalignant skull base tumors. Of the three representation learning strategies,\nsupervised contrastive learning most effectively learned the distinctive and\ndiagnostic SRH image features for each of the skull base tumor types. In our\nmulticenter testing set, cross-entropy achieved an overall diagnostic accuracy\nof 91.5%, self-supervised contrastive learning 83.9%, and supervised\ncontrastive learning 96.6%. Our trained model was able to identify tumor-normal\nmargins and detect regions of microscopic tumor infiltration in whole-slide SRH\nimages.\n\nConclusion: SRH with AI models trained using contrastive representation\nlearning can provide rapid and accurate intraoperative diagnosis of skull base\ntumors.",
          "link": "http://arxiv.org/abs/2108.03555",
          "publishedOn": "2021-08-10T02:00:09.008Z",
          "wordCount": 738,
          "title": "Contrastive Representation Learning for Rapid Intraoperative Diagnosis of Skull Base Tumors Imaged Using Stimulated Raman Histology. (arXiv:2108.03555v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12018",
          "author": "<a href=\"http://arxiv.org/find/hep-ph/1/au:+Arganda_E/0/1/0/all/0/1\">Ernesto Arganda</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Medina_A/0/1/0/all/0/1\">Anibal D. Medina</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Perez_A/0/1/0/all/0/1\">Andres D. Perez</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Szynkman_A/0/1/0/all/0/1\">Alejandro Szynkman</a>",
          "description": "We study several simplified dark matter (DM) models and their signatures at\nthe LHC using neural networks. We focus on the usual monojet plus missing\ntransverse energy channel, but to train the algorithms we organize the data in\n2D histograms instead of event-by-event arrays. This results in a large\nperformance boost to distinguish between standard model (SM) only and SM plus\nnew physics signals. We use the kinematic monojet features as input data which\nallow us to describe families of models with a single data sample. We found\nthat the neural network performance does not depend on the simulated number of\nbackground events if they are presented as a function of $S/\\sqrt{B}$, where\n$S$ and $B$ are the number of signal and background events per histogram,\nrespectively. This provides flexibility to the method, since testing a\nparticular model in that case only requires knowing the new physics monojet\ncross section. Furthermore, we also discuss the network performance under\nincorrect assumptions about the true DM nature. Finally, we propose multimodel\nclassifiers to search and identify new signals in a more general way, for the\nnext LHC run.",
          "link": "http://arxiv.org/abs/2105.12018",
          "publishedOn": "2021-08-10T02:00:08.984Z",
          "wordCount": 686,
          "title": "Towards a method to anticipate dark matter signals with deep learning at the LHC. (arXiv:2105.12018v2 [hep-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hewamalage_H/0/1/0/all/0/1\">Hansika Hewamalage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montero_Manso_P/0/1/0/all/0/1\">Pablo Montero-Manso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergmeir_C/0/1/0/all/0/1\">Christoph Bergmeir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyndman_R/0/1/0/all/0/1\">Rob J Hyndman</a>",
          "description": "Forecast evaluation plays a key role in how empirical evidence shapes the\ndevelopment of the discipline. Domain experts are interested in error measures\nrelevant for their decision making needs. Such measures may produce unreliable\nresults. Although reliability properties of several metrics have already been\ndiscussed, it has hardly been quantified in an objective way. We propose a\nmeasure named Rank Stability, which evaluates how much the rankings of an\nexperiment differ in between similar datasets, when the models and errors are\nconstant. We use this to study the evaluation setup of the M5. We find that the\nevaluation setup of the M5 is less reliable than other measures. The main\ndrivers of instability are hierarchical aggregation and scaling.\nPrice-weighting reduces the stability of all tested error measures. Scale\nnormalization of the M5 error measure results in less stability than other\nscale-free errors. Hierarchical levels taken separately are less stable with\nmore aggregation, and their combination is even less stable than individual\nlevels. We also show positive tradeoffs of retaining aggregation importance\nwithout affecting stability. Aggregation and stability can be linked to the\ninfluence of much debated magic numbers. Many of our findings can be applied to\ngeneral hierarchical forecast benchmarking.",
          "link": "http://arxiv.org/abs/2108.03588",
          "publishedOn": "2021-08-10T02:00:08.970Z",
          "wordCount": 641,
          "title": "A Look at the Evaluation Setup of the M5 Forecasting Competition. (arXiv:2108.03588v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kummer_L/0/1/0/all/0/1\">Lorenz Kummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidak_K/0/1/0/all/0/1\">Kevin Sidak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichmann_T/0/1/0/all/0/1\">Tabea Reichmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gansterer_W/0/1/0/all/0/1\">Wilfried Gansterer</a>",
          "description": "Quantization is a technique for reducing deep neural networks (DNNs) training\nand inference times, which is crucial for training in resource constrained\nenvironments or time critical inference applications. State-of-the-art (SOTA)\nquantization approaches focus on post-training quantization, i.e. quantization\nof pre-trained DNNs for speeding up inference. Very little work on quantized\ntraining exists, which neither al-lows dynamic intra-epoch precision switches\nnor em-ploys an information theory based switching heuristic. Usually, existing\napproaches require full precision refinement afterwards and enforce a global\nword length across the whole DNN. This leads to suboptimal quantization\nmappings and resource usage. Recognizing these limits, we introduce MARViN, a\nnew quantized training strategy using information theory-based intra-epoch\nprecision switching, which decides on a per-layer basis which precision should\nbe used in order to minimize quantization-induced information loss. Note that\nany quantization must leave enough precision such that future learning steps do\nnot suffer from vanishing gradients. We achieve an average speedup of 1.86\ncompared to a float32 basis while limiting mean accuracy degradation on\nAlexNet/ResNet to only -0.075%.",
          "link": "http://arxiv.org/abs/2107.13490",
          "publishedOn": "2021-08-10T02:00:08.937Z",
          "wordCount": 636,
          "title": "MARViN -- Multiple Arithmetic Resolutions Vacillating in Neural Networks. (arXiv:2107.13490v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03718",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bing_Z/0/1/0/all/0/1\">Zhenshan Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knak_L/0/1/0/all/0/1\">Lukas Knak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robin_F/0/1/0/all/0/1\">Fabrice Oliver Robin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a>",
          "description": "Recent state-of-the-art artificial agents lack the ability to adapt rapidly\nto new tasks, as they are trained exclusively for specific objectives and\nrequire massive amounts of interaction to learn new skills. Meta-reinforcement\nlearning (meta-RL) addresses this challenge by leveraging knowledge learned\nfrom training tasks to perform well in previously unseen tasks. However,\ncurrent meta-RL approaches limit themselves to narrow parametric task\ndistributions, ignoring qualitative differences between tasks that occur in the\nreal world. In this paper, we introduce TIGR, a Task-Inference-based meta-RL\nalgorithm using Gaussian mixture models (GMM) and gated Recurrent units,\ndesigned for tasks in non-parametric environments. We employ a generative model\ninvolving a GMM to capture the multi-modality of the tasks. We decouple the\npolicy training from the task-inference learning and efficiently train the\ninference mechanism on the basis of an unsupervised reconstruction objective.\nWe provide a benchmark with qualitatively distinct tasks based on the\nhalf-cheetah environment and demonstrate the superior performance of TIGR\ncompared to state-of-the-art meta-RL approaches in terms of sample efficiency\n(3-10 times faster), asymptotic performance, and applicability in\nnon-parametric environments with zero-shot adaptation.",
          "link": "http://arxiv.org/abs/2108.03718",
          "publishedOn": "2021-08-10T02:00:08.928Z",
          "wordCount": 613,
          "title": "Meta-Reinforcement Learning in Broad and Non-Parametric Environments. (arXiv:2108.03718v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Faber_K/0/1/0/all/0/1\">Kamil Faber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zurek_D/0/1/0/all/0/1\">Dominik &#x17b;urek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietron_M/0/1/0/all/0/1\">Marcin Pietro&#x144;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietak_K/0/1/0/all/0/1\">Kamil Pi&#x119;tak</a>",
          "description": "Multivariate time series anomaly detection is a very common problem in the\nfield of failure prevention. Fast prevention means lower repair costs and\nlosses. The amount of sensors in novel industry systems makes the anomaly\ndetection process quite difficult for humans. Algorithms which automates the\nprocess of detecting anomalies are crucial in modern failure-prevention\nsystems. Therefore, many machine and deep learning models have been designed to\naddress this problem. Mostly, they are autoencoder-based architectures with\nsome generative adversarial elements. In this work, a framework is shown which\nincorporates neuroevolution methods to boost the anomaly-detection scores of\nnew and already known models. The presented approach adapts evolution\nstrategies for evolving ensemble model, in which every single model works on a\nsubgroup of data sensors. The next goal of neuroevolution is to optimise\narchitecture and hyperparameters like window size, the number of layers, layer\ndepths, etc. The proposed framework shows that it is possible to boost most of\nthe anomaly detection deep learning models in a reasonable time and a fully\nautomated mode. The tests were run on SWAT and WADI datasets. To our knowledge,\nthis is the first approach in which an ensemble deep learning anomaly detection\nmodel is built in a fully automatic way using a neuroevolution strategy.",
          "link": "http://arxiv.org/abs/2108.03585",
          "publishedOn": "2021-08-10T02:00:08.861Z",
          "wordCount": 645,
          "title": "Ensemble neuroevolution based approach for multivariate time series anomaly detection. (arXiv:2108.03585v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.00524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Binhang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jankov_D/0/1/0/all/0/1\">Dimitrije Jankov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Jia Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yuxin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourgeois_D/0/1/0/all/0/1\">Daniel Bourgeois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jermaine_C/0/1/0/all/0/1\">Chris Jermaine</a>",
          "description": "We consider the question: what is the abstraction that should be implemented\nby the computational engine of a machine learning system? Current machine\nlearning systems typically push whole tensors through a series of compute\nkernels such as matrix multiplications or activation functions, where each\nkernel runs on an AI accelerator (ASIC) such as a GPU. This implementation\nabstraction provides little built-in support for ML systems to scale past a\nsingle machine, or for handling large models with matrices or tensors that do\nnot easily fit into the RAM of an ASIC. In this paper, we present an\nalternative implementation abstraction called the tensor relational algebra\n(TRA). The TRA is a set-based algebra based on the relational algebra.\nExpressions in the TRA operate over binary tensor relations, where keys are\nmulti-dimensional arrays and values are tensors. The TRA is easily executed\nwith high efficiency in a parallel or distributed environment, and amenable to\nautomatic optimization. Our empirical study shows that the optimized TRA-based\nback-end can significantly outperform alternatives for running ML workflows in\ndistributed clusters.",
          "link": "http://arxiv.org/abs/2009.00524",
          "publishedOn": "2021-08-10T02:00:08.853Z",
          "wordCount": 648,
          "title": "Tensor Relational Algebra for Machine Learning System Design. (arXiv:2009.00524v3 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08334",
          "author": "<a href=\"http://arxiv.org/find/hep-ph/1/au:+Araz_J/0/1/0/all/0/1\">Jack Y. Araz</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Spannowsky_M/0/1/0/all/0/1\">Michael Spannowsky</a>",
          "description": "Tensor Networks are non-trivial representations of high-dimensional tensors,\noriginally designed to describe quantum many-body systems. We show that Tensor\nNetworks are ideal vehicles to connect quantum mechanical concepts to machine\nlearning techniques, thereby facilitating an improved interpretability of\nneural networks. This study presents the discrimination of top quark signal\nover QCD background processes using a Matrix Product State classifier. We show\nthat entanglement entropy can be used to interpret what a network learns, which\ncan be used to reduce the complexity of the network and feature space without\nloss of generality or performance. For the optimisation of the network, we\ncompare the Density Matrix Renormalization Group (DMRG) algorithm to stochastic\ngradient descent (SGD) and propose a joined training algorithm to harness the\nexplainability of DMRG with the efficiency of SGD.",
          "link": "http://arxiv.org/abs/2106.08334",
          "publishedOn": "2021-08-09T00:49:28.895Z",
          "wordCount": 615,
          "title": "Quantum-inspired event reconstruction with Tensor Networks: Matrix Product States. (arXiv:2106.08334v2 [hep-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samavatian_M/0/1/0/all/0/1\">Mohammad Hossein Samavatian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Saikat Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barber_K/0/1/0/all/0/1\">Kristin Barber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_R/0/1/0/all/0/1\">Radu Teodorescu</a>",
          "description": "Deep Neural Networks (DNNs) are employed in an increasing number of\napplications, some of which are safety critical. Unfortunately, DNNs are known\nto be vulnerable to so-called adversarial attacks that manipulate inputs to\ncause incorrect results that can be beneficial to an attacker or damaging to\nthe victim. Multiple defenses have been proposed to increase the robustness of\nDNNs. In general, these defenses have high overhead, some require\nattack-specific re-training of the model or careful tuning to adapt to\ndifferent attacks.\n\nThis paper presents HASI, a hardware-accelerated defense that uses a process\nwe call stochastic inference to detect adversarial inputs. We show that by\ncarefully injecting noise into the model at inference time, we can\ndifferentiate adversarial inputs from benign ones. HASI uses the output\ndistribution characteristics of noisy inference compared to a non-noisy\nreference to detect adversarial inputs. We show an adversarial detection rate\nof 86% when applied to VGG16 and 93% when applied to ResNet50, which exceeds\nthe detection rate of the state of the art approaches, with a much lower\noverhead. We demonstrate two software/hardware-accelerated co-designs, which\nreduces the performance impact of stochastic inference to 1.58X-2X relative to\nthe unprotected baseline, compared to 15X-20X overhead for a software-only GPU\nimplementation.",
          "link": "http://arxiv.org/abs/2106.05825",
          "publishedOn": "2021-08-09T00:49:28.877Z",
          "wordCount": 686,
          "title": "HASI: Hardware-Accelerated Stochastic Inference, A Defense Against Adversarial Machine Learning Attacks. (arXiv:2106.05825v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14144",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1\">Shichao Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Yangyang Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yixuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ONeill_Z/0/1/0/all/0/1\">Zheng O&#x27;Neill</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>",
          "description": "As people spend up to 87% of their time indoors, intelligent Heating,\nVentilation, and Air Conditioning (HVAC) systems in buildings are essential for\nmaintaining occupant comfort and reducing energy consumption. These HVAC\nsystems in smart buildings rely on real-time sensor readings, which in practice\noften suffer from various faults and could also be vulnerable to malicious\nattacks. Such faulty sensor inputs may lead to the violation of indoor\nenvironment requirements (e.g., temperature, humidity, etc.) and the increase\nof energy consumption. While many model-based approaches have been proposed in\nthe literature for building HVAC control, it is costly to develop accurate\nphysical models for ensuring their performance and even more challenging to\naddress the impact of sensor faults. In this work, we present a novel\nlearning-based framework for sensor fault-tolerant HVAC control, which includes\nthree deep learning based components for 1) generating temperature proposals\nwith the consideration of possible sensor faults, 2) selecting one of the\nproposals based on the assessment of their accuracy, and 3) applying\nreinforcement learning with the selected temperature proposal. Moreover, to\naddress the challenge of training data insufficiency in building-related tasks,\nwe propose a model-assisted learning method leveraging an abstract model of\nbuilding physical dynamics. Through extensive experiments, we demonstrate that\nthe proposed fault-tolerant HVAC control framework can significantly reduce\nbuilding temperature violations under a variety of sensor fault patterns while\nmaintaining energy efficiency.",
          "link": "http://arxiv.org/abs/2106.14144",
          "publishedOn": "2021-08-09T00:49:28.870Z",
          "wordCount": 694,
          "title": "Learning-based Framework for Sensor Fault-Tolerant Building HVAC Control with Model-assisted Learning. (arXiv:2106.14144v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06073",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Royset_J/0/1/0/all/0/1\">Johannes O. Royset</a>",
          "description": "A basic requirement for a mathematical model is often that its solution\n(output) shouldn't change much if the model's parameters (input) are perturbed.\nThis is important because the exact values of parameters may not be known and\none would like to avoid being mislead by an output obtained using incorrect\nvalues. Thus, it's rarely enough to address an application by formulating a\nmodel, solving the resulting optimization problem and presenting the solution\nas the answer. One would need to confirm that the model is suitable, i.e.,\n\"good,\" and this can, at least in part, be achieved by considering a family of\noptimization problems constructed by perturbing parameters of concern. The\nresulting sensitivity analysis uncovers troubling situations with unstable\nsolutions, which we referred to as \"bad\" models, and indicates better model\nformulations. Embedding an actual problem of interest within a family of\nproblems is also a primary path to optimality conditions as well as\ncomputationally attractive, alternative problems, which under ideal\ncircumstances, and when properly tuned, may even furnish the minimum value of\nthe actual problem. The tuning of these alternative problems turns out to be\nintimately tied to finding multipliers in optimality conditions and thus\nemerges as a main component of several optimization algorithms. In fact, the\ntuning amounts to solving certain dual optimization problems. In this tutorial,\nwe'll discuss the opportunities and insights afforded by this broad\nperspective.",
          "link": "http://arxiv.org/abs/2105.06073",
          "publishedOn": "2021-08-09T00:49:28.863Z",
          "wordCount": 687,
          "title": "Good and Bad Optimization Models: Insights from Rockafellians. (arXiv:2105.06073v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03167",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yamac_M/0/1/0/all/0/1\">Mehmet Yamac</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akpinar_U/0/1/0/all/0/1\">Ugur Akpinar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sahin_E/0/1/0/all/0/1\">Erdem Sahin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan Kiranyaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gabbouj_M/0/1/0/all/0/1\">Moncef Gabbouj</a>",
          "description": "In CS literature, the efforts can be divided into two groups: finding a\nmeasurement matrix that preserves the compressed information at the maximum\nlevel, and finding a reconstruction algorithm for the compressed information.\nIn the traditional CS setup, the measurement matrices are selected as random\nmatrices, and optimization-based iterative solutions are used to recover the\nsignals. However, when we handle large signals, using random matrices become\ncumbersome especially when it comes to iterative optimization-based solutions.\nEven though recent deep learning-based solutions boost the reconstruction\naccuracy performance while speeding up the recovery, still jointly learning the\nwhole measurement matrix is a difficult process. In this work, we introduce a\nseparable multi-linear learning of the CS matrix by representing it as the\nsummation of arbitrary number of tensors. For a special case where the CS\noperation is set as a single tensor multiplication, the model is reduced to the\nlearning-based separable CS; while a dense CS matrix can be approximated and\nlearned as the summation of multiple tensors. Both cases can be used in CS of\ntwo or multi-dimensional signals e.g., images, multi-spectral images, videos,\netc. Structural CS matrices can also be easily approximated and learned in our\nmulti-linear separable learning setup with structural tensor sum\nrepresentation. Hence, our learnable generalized tensor summation CS operation\nencapsulates most CS setups including separable CS, non-separable CS\n(traditional vector-matrix multiplication), structural CS, and CS of the\nmulti-dimensional signals. For both gray-scale and RGB images, the proposed\nscheme surpasses most state-of-the-art solutions, especially in lower\nmeasurement rates. Although the performance gain remains limited from tensor to\nthe sum of tensor representation for gray-scale images, it becomes significant\nin the RGB case.",
          "link": "http://arxiv.org/abs/2108.03167",
          "publishedOn": "2021-08-09T00:49:28.856Z",
          "wordCount": 747,
          "title": "Generalized Tensor Summation Compressive Sensing Network (GTSNET): An Easy to Learn Compressive Sensing Operation. (arXiv:2108.03167v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>",
          "description": "Active learning (AL) aims at reducing labeling effort by identifying the most\nvaluable unlabeled data points from a large pool. Traditional AL frameworks\nhave two limitations: First, they perform data selection in a multi-round\nmanner, which is time-consuming and impractical. Second, they usually assume\nthat there are a small amount of labeled data points available in the same\ndomain as the data in the unlabeled pool. Recent work proposes a solution for\none-round active learning based on data utility learning and optimization,\nwhich fixes the first issue but still requires the initially labeled data\npoints in the same domain. In this paper, we propose $\\mathrm{D^2ULO}$ as a\nsolution that solves both issues. Specifically, $\\mathrm{D^2ULO}$ leverages the\nidea of domain adaptation (DA) to train a data utility model which can\neffectively predict the utility for any given unlabeled data in the target\ndomain once labeled. The trained data utility model can then be used to select\nhigh-utility data and at the same time, provide an estimate for the utility of\nthe selected data. Our algorithm does not rely on any feedback from annotators\nin the target domain and hence, can be used to perform zero-round active\nlearning or warm-start existing multi-round active learning strategies. Our\nexperiments show that $\\mathrm{D^2ULO}$ outperforms the existing\nstate-of-the-art AL strategies equipped with domain adaptation over various\ndomain shift settings (e.g., real-to-real data and synthetic-to-real data).\nParticularly, $\\mathrm{D^2ULO}$ is applicable to the scenario where source and\ntarget labels have mismatches, which is not supported by the existing works.",
          "link": "http://arxiv.org/abs/2107.06703",
          "publishedOn": "2021-08-09T00:49:28.849Z",
          "wordCount": 705,
          "title": "Zero-Round Active Learning. (arXiv:2107.06703v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11713",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Nanni_L/0/1/0/all/0/1\">Loris Nanni</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lumini_A/0/1/0/all/0/1\">Alessandra Lumini</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Brahnam_S/0/1/0/all/0/1\">Sheryl Brahnam</a>",
          "description": "Motivation: Automatic Anatomical Therapeutic Chemical (ATC) classification is\na critical and highly competitive area of research in bioinformatics because of\nits potential for expediting drug develop-ment and research. Predicting an\nunknown compound's therapeutic and chemical characteristics ac-cording to how\nthese characteristics affect multiple organs/systems makes automatic ATC\nclassifica-tion a challenging multi-label problem. Results: In this work, we\npropose combining multiple multi-label classifiers trained on distinct sets of\nfeatures, including sets extracted from a Bidirectional Long Short-Term Memory\nNetwork (BiLSTM). Experiments demonstrate the power of this approach, which is\nshown to outperform the best methods reported in the literature, including the\nstate-of-the-art developed by the fast.ai research group. Availability: All\nsource code developed for this study is available at\nhttps://github.com/LorisNanni. Contact: loris.nanni@unipd.it",
          "link": "http://arxiv.org/abs/2101.11713",
          "publishedOn": "2021-08-09T00:49:28.831Z",
          "wordCount": 582,
          "title": "Neural networks for Anatomical Therapeutic Chemical (ATC) classification. (arXiv:2101.11713v3 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06782",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Rana_M/0/1/0/all/0/1\">Mimansa Rana</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Mao_N/0/1/0/all/0/1\">Nanxiang Mao</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Ao_M/0/1/0/all/0/1\">Ming Ao</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Wu_X/0/1/0/all/0/1\">Xiaohui Wu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Liang_P/0/1/0/all/0/1\">Poning Liang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Khushi_M/0/1/0/all/0/1\">Matloob Khushi</a>",
          "description": "The foreign exchange market has taken an important role in the global\nfinancial market. While foreign exchange trading brings high-yield\nopportunities to investors, it also brings certain risks. Since the\nestablishment of the foreign exchange market in the 20th century, foreign\nexchange rate forecasting has become a hot issue studied by scholars from all\nover the world. Due to the complexity and number of factors affecting the\nforeign exchange market, technical analysis cannot respond to administrative\nintervention or unexpected events. Our team chose several pairs of foreign\ncurrency historical data and derived technical indicators from 2005 to 2021 as\nthe dataset and established different machine learning models for event-driven\nprice prediction for oversold scenario.",
          "link": "http://arxiv.org/abs/2107.06782",
          "publishedOn": "2021-08-09T00:49:28.824Z",
          "wordCount": 569,
          "title": "Clustering and attention model based for intelligent trading. (arXiv:2107.06782v2 [q-fin.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garibaldi_J/0/1/0/all/0/1\">Jonathan M Garibaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>",
          "description": "The goal of representation learning is different from the ultimate objective\nof machine learning such as decision making, it is therefore very difficult to\nestablish clear and direct objectives for training representation learning\nmodels. It has been argued that a good representation should disentangle the\nunderlying variation factors, yet how to translate this into training\nobjectives remains unknown. This paper presents an attempt to establish direct\ntraining criterions and design principles for developing good representation\nlearning models. We propose that a good representation learning model should be\nmaximally expressive, i.e., capable of distinguishing the maximum number of\ninput configurations. We formally define expressiveness and introduce the\nmaximum expressiveness (MEXS) theorem of a general learning model. We propose\nto train a model by maximizing its expressiveness while at the same time\nincorporating general priors such as model smoothness. We present a conscience\ncompetitive learning algorithm which encourages the model to reach its MEXS\nwhilst at the same time adheres to model smoothness prior. We also introduce a\nlabel consistent training (LCT) technique to boost model smoothness by\nencouraging it to assign consistent labels to similar samples. We present\nextensive experimental results to show that our method can indeed design\nrepresentation learning models capable of developing representations that are\nas good as or better than state of the art. We also show that our technique is\ncomputationally efficient, robust against different parameter settings and can\nwork effectively on a variety of datasets. Code available at\nhttps://github.com/qlilx/odgrlm.git",
          "link": "http://arxiv.org/abs/2107.05948",
          "publishedOn": "2021-08-09T00:49:28.812Z",
          "wordCount": 716,
          "title": "On Designing Good Representation Learning Models. (arXiv:2107.05948v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1806.00984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fahad_M/0/1/0/all/0/1\">Md. Shah Fahad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_J/0/1/0/all/0/1\">Jainath Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradhan_G/0/1/0/all/0/1\">Gyadhar Pradhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deepak_A/0/1/0/all/0/1\">Akshay Deepak</a>",
          "description": "Speech is produced when time varying vocal tract system is excited with time\nvarying excitation source. Therefore, the information present in a speech such\nas message, emotion, language, speaker is due to the combined effect of both\nexcitation source and vocal tract system. However, there is very less\nutilization of excitation source features to recognize emotion. In our earlier\nwork, we have proposed a novel method to extract glottal closure instants\n(GCIs) known as epochs. In this paper, we have explored epoch features namely\ninstantaneous pitch, phase and strength of epochs for discriminating emotions.\nWe have combined the excitation source features and the well known\nMale-frequency cepstral coefficient (MFCC) features to develop an emotion\nrecognition system with improved performance. DNN-HMM speaker adaptive models\nhave been developed using MFCC, epoch and combined features. IEMOCAP emotional\ndatabase has been used to evaluate the models. The average accuracy for emotion\nrecognition system when using MFCC and epoch features separately is 59.25% and\n54.52% respectively. The recognition performance improves to 64.2% when MFCC\nand epoch features are combined.",
          "link": "http://arxiv.org/abs/1806.00984",
          "publishedOn": "2021-08-09T00:49:28.805Z",
          "wordCount": 650,
          "title": "DNN-HMM based Speaker Adaptive Emotion Recognition using Proposed Epoch and MFCC Features. (arXiv:1806.00984v1 [cs.SD] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandagale_S/0/1/0/all/0/1\">Sujay Khandagale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1\">Colin White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1\">Willie Neiswanger</a>",
          "description": "As machine learning models grow more complex and their applications become\nmore high-stakes, tools for explaining model predictions have become\nincreasingly important. This has spurred a flurry of research in model\nexplainability and has given rise to feature attribution methods such as LIME\nand SHAP. Despite their widespread use, evaluating and comparing different\nfeature attribution methods remains challenging: evaluations ideally require\nhuman studies, and empirical evaluation metrics are often data-intensive or\ncomputationally prohibitive on real-world datasets. In this work, we address\nthis issue by releasing XAI-Bench: a suite of synthetic datasets along with a\nlibrary for benchmarking feature attribution algorithms. Unlike real-world\ndatasets, synthetic datasets allow the efficient computation of conditional\nexpected values that are needed to evaluate ground-truth Shapley values and\nother metrics. The synthetic datasets we release offer a wide variety of\nparameters that can be configured to simulate real-world data. We demonstrate\nthe power of our library by benchmarking popular explainability techniques\nacross several evaluation metrics and across a variety of settings. The\nversatility and efficiency of our library will help researchers bring their\nexplainability methods from development to deployment. Our code is available at\nhttps://github.com/abacusai/xai-bench.",
          "link": "http://arxiv.org/abs/2106.12543",
          "publishedOn": "2021-08-09T00:49:28.797Z",
          "wordCount": 661,
          "title": "Synthetic Benchmarks for Scientific Research in Explainable Machine Learning. (arXiv:2106.12543v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03169",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Abouheaf_M/0/1/0/all/0/1\">Mohammed Abouheaf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qu_S/0/1/0/all/0/1\">Shuzheng Qu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gueaieb_W/0/1/0/all/0/1\">Wail Gueaieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abielmona_R/0/1/0/all/0/1\">Rami Abielmona</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harb_M/0/1/0/all/0/1\">Moufid Harb</a>",
          "description": "This article elaborates on how machine learning (ML) can leverage the\nsolution of a contemporary problem related to the security of maritime domains.\nThe worldwide ``Illegal, Unreported, and Unregulated'' (IUU) fishing incidents\nhave led to serious environmental and economic consequences which involve\ndrastic changes in our ecosystems in addition to financial losses caused by the\ndepletion of natural resources. The Fisheries and Aquatic Department (FAD) of\nthe United Nation's Food and Agriculture Organization (FAO) issued a report\nwhich indicated that the annual losses due to IUU fishing reached $25 Billion.\nThis imposes negative impacts on the future-biodiversity of the marine\necosystem and domestic Gross National Product (GNP). Hence, robust interception\nmechanisms are increasingly needed for detecting and pursuing the unrelenting\nillegal fishing incidents in maritime territories. This article addresses the\nproblem of coordinating the motion of a fleet of marine vessels (pursuers) to\ncatch an IUU vessel while still in local waters. The problem is formulated as a\npursuer-evader problem that is tackled within an ML framework. One or more\npursuers, such as law enforcement vessels, intercept an evader (i.e., the\nillegal fishing ship) using an online reinforcement learning mechanism that is\nbased on a value iteration process. It employs real-time navigation\nmeasurements of the evader ship as well as those of the pursuing vessels and\nreturns back model-free interception strategies.",
          "link": "http://arxiv.org/abs/2108.03169",
          "publishedOn": "2021-08-09T00:49:28.775Z",
          "wordCount": 696,
          "title": "Responding to Illegal Activities Along the Canadian Coastlines Using Reinforcement Learning. (arXiv:2108.03169v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03877",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Konstantinova_T/0/1/0/all/0/1\">Tatiana Konstantinova</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Wiegart_L/0/1/0/all/0/1\">Lutz Wiegart</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Rakitin_M/0/1/0/all/0/1\">Maksim Rakitin</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+DeGennaro_A/0/1/0/all/0/1\">Anthony M. DeGennaro</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Barbour_A/0/1/0/all/0/1\">Andi M. Barbour</a>",
          "description": "Like other experimental techniques, X-ray Photon Correlation Spectroscopy is\nsubject to various kinds of noise. Random and correlated fluctuations and\nheterogeneities can be present in a two-time correlation function and obscure\nthe information about the intrinsic dynamics of a sample. Simultaneously\naddressing the disparate origins of noise in the experimental data is\nchallenging. We propose a computational approach for improving the\nsignal-to-noise ratio in two-time correlation functions that is based on\nConvolutional Neural Network Encoder-Decoder (CNN-ED) models. Such models\nextract features from an image via convolutional layers, project them to a low\ndimensional space and then reconstruct a clean image from this reduced\nrepresentation via transposed convolutional layers. Not only are ED models a\ngeneral tool for random noise removal, but their application to low\nsignal-to-noise data can enhance the data quantitative usage since they are\nable to learn the functional form of the signal. We demonstrate that the CNN-ED\nmodels trained on real-world experimental data help to effectively extract\nequilibrium dynamics parameters from two-time correlation functions, containing\nstatistical noise and dynamic heterogeneities. Strategies for optimizing the\nmodels performance and their applicability limits are discussed.",
          "link": "http://arxiv.org/abs/2102.03877",
          "publishedOn": "2021-08-09T00:49:28.767Z",
          "wordCount": 657,
          "title": "Noise Reduction in X-ray Photon Correlation Spectroscopy with Convolutional Neural Networks Encoder-Decoder Models. (arXiv:2102.03877v2 [cond-mat.mtrl-sci] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1\">Ching Pui Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "Federated learning is vulnerable to various attacks, such as model poisoning\nand backdoor attacks, even if some existing defense strategies are used. To\naddress this challenge, we propose an attack-adaptive aggregation strategy to\ndefend against various attacks for robust federated learning. The proposed\napproach is based on training a neural network with an attention mechanism that\nlearns the vulnerability of federated learning models from a set of plausible\nattacks. To the best of our knowledge, our aggregation strategy is the first\none that can be adapted to defend against various attacks in a data-driven\nfashion. Our approach has achieved competitive performance in defending model\npoisoning and backdoor attacks in federated learning tasks on image and text\ndatasets.",
          "link": "http://arxiv.org/abs/2102.05257",
          "publishedOn": "2021-08-09T00:49:28.759Z",
          "wordCount": 577,
          "title": "Robust Federated Learning with Attack-Adaptive Aggregation. (arXiv:2102.05257v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Joey Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1\">David Dohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rishabh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1\">Charles Sutton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>",
          "description": "In many sequence learning tasks, such as program synthesis and document\nsummarization, a key problem is searching over a large space of possible output\nsequences. We propose to learn representations of the outputs that are\nspecifically meant for search: rich enough to specify the desired output but\ncompact enough to make search more efficient. Discrete latent codes are\nappealing for this purpose, as they naturally allow sophisticated combinatorial\nsearch strategies. The latent codes are learned using a self-supervised\nlearning principle, in which first a discrete autoencoder is trained on the\noutput sequences, and then the resulting latent codes are used as intermediate\ntargets for the end-to-end sequence prediction task. Based on these insights,\nwe introduce the \\emph{Latent Programmer}, a program synthesis method that\nfirst predicts a discrete latent code from input/output examples, and then\ngenerates the program in the target language. We evaluate the Latent Programmer\non two domains: synthesis of string transformation programs, and generation of\nprograms from natural language descriptions. We demonstrate that the discrete\nlatent representation significantly improves synthesis accuracy.",
          "link": "http://arxiv.org/abs/2012.00377",
          "publishedOn": "2021-08-09T00:49:28.752Z",
          "wordCount": 646,
          "title": "Latent Programmer: Discrete Latent Codes for Program Synthesis. (arXiv:2012.00377v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09638",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Glasgow_M/0/1/0/all/0/1\">Margalit Glasgow</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wootters_M/0/1/0/all/0/1\">Mary Wootters</a>",
          "description": "In distributed optimization problems, a technique called gradient coding,\nwhich involves replicating data points, has been used to mitigate the effect of\nstraggling machines. Recent work has studied approximate gradient coding, which\nconcerns coding schemes where the replication factor of the data is too low to\nrecover the full gradient exactly. Our work is motivated by the challenge of\ncreating approximate gradient coding schemes that simultaneously work well in\nboth the adversarial and stochastic models. To that end, we introduce novel\napproximate gradient codes based on expander graphs, in which each machine\nreceives exactly two blocks of data points. We analyze the decoding error both\nin the random and adversarial straggler setting, when optimal decoding\ncoefficients are used. We show that in the random setting, our schemes achieve\nan error to the gradient that decays exponentially in the replication factor.\nIn the adversarial setting, the error is nearly a factor of two smaller than\nany existing code with similar performance in the random setting. We show\nconvergence bounds both in the random and adversarial setting for gradient\ndescent under standard assumptions using our codes. In the random setting, our\nconvergence rate improves upon block-box bounds. In the adversarial setting, we\nshow that gradient descent can converge down to a noise floor that scales\nlinearly with the adversarial error to the gradient. We demonstrate empirically\nthat our schemes achieve near-optimal error in the random setting and converge\nfaster than algorithms which do not use the optimal decoding coefficients.",
          "link": "http://arxiv.org/abs/2006.09638",
          "publishedOn": "2021-08-09T00:49:28.745Z",
          "wordCount": 717,
          "title": "Approximate Gradient Coding with Optimal Decoding. (arXiv:2006.09638v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shangtong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Hengshuai Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1\">Shimon Whiteson</a>",
          "description": "The deadly triad refers to the instability of a reinforcement learning\nalgorithm when it employs off-policy learning, function approximation, and\nbootstrapping simultaneously. In this paper, we investigate the target network\nas a tool for breaking the deadly triad, providing theoretical support for the\nconventional wisdom that a target network stabilizes training. We first propose\nand analyze a novel target network update rule which augments the commonly used\nPolyak-averaging style update with two projections. We then apply the target\nnetwork and ridge regularization in several divergent algorithms and show their\nconvergence to regularized TD fixed points. Those algorithms are off-policy\nwith linear function approximation and bootstrapping, spanning both policy\nevaluation and control, as well as both discounted and average-reward settings.\nIn particular, we provide the first convergent linear $Q$-learning algorithms\nunder nonrestrictive and changing behavior policies without bi-level\noptimization.",
          "link": "http://arxiv.org/abs/2101.08862",
          "publishedOn": "2021-08-09T00:49:28.725Z",
          "wordCount": 624,
          "title": "Breaking the Deadly Triad with a Target Network. (arXiv:2101.08862v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.06175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Juhyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kihyun Kim</a>",
          "description": "We propose a novel approach to optimize fleet management by combining\nmulti-agent reinforcement learning with graph neural network. To provide\nride-hailing service, one needs to optimize dynamic resources and demands over\nspatial domain. While the spatial structure was previously approximated with a\nregular grid, our approach represents the road network with a graph, which\nbetter reflects the underlying geometric structure. Dynamic resource allocation\nis formulated as multi-agent reinforcement learning, whose action-value\nfunction (Q function) is approximated with graph neural networks. We use\nstochastic policy update rule over the graph with deep Q-networks (DQN), and\nachieve superior results over the greedy policy update. We design a realistic\nsimulator that emulates the empirical taxi call data, and confirm the\neffectiveness of the proposed model under various conditions.",
          "link": "http://arxiv.org/abs/2011.06175",
          "publishedOn": "2021-08-09T00:49:28.717Z",
          "wordCount": 604,
          "title": "Optimizing Large-Scale Fleet Management on a Road Network using Multi-Agent Deep Reinforcement Learning with Graph Neural Network. (arXiv:2011.06175v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Torrent_N/0/1/0/all/0/1\">Neus Llop Torrent</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Visani_G/0/1/0/all/0/1\">Giorgio Visani</a> (2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Bagli_E/0/1/0/all/0/1\">Enrico Bagli</a> (2) ((1) Politecnico di Milano Graduate School of Business, (2) CRIF S.p.A, (3) University of Bologna School of Informatics and Engineering)",
          "description": "The aim of this project is to develop and test advanced analytical methods to\nimprove the prediction accuracy of Credit Risk Models, preserving at the same\ntime the model interpretability. In particular, the project focuses on applying\nan explainable machine learning model to bank-related databases. The input data\nwere obtained from open data. Over the total proven models, CatBoost has shown\nthe highest performance. The algorithm implementation produces a GINI of 0.68\nafter tuning the hyper-parameters. SHAP package is used to provide a global and\nlocal interpretation of the model predictions to formulate a\nhuman-comprehensive approach to understanding the decision-maker algorithm. The\n20 most important features are selected using the Shapley values to present a\nfull human-understandable model that reveals how the attributes of an\nindividual are related to its model prediction.",
          "link": "http://arxiv.org/abs/2011.10367",
          "publishedOn": "2021-08-09T00:49:28.710Z",
          "wordCount": 623,
          "title": "PSD2 Explainable AI Model for Credit Scoring. (arXiv:2011.10367v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03068",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Huth_B/0/1/0/all/0/1\">Benjamin Huth</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Salzburger_A/0/1/0/all/0/1\">Andreas Salzburger</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wettig_T/0/1/0/all/0/1\">Tilo Wettig</a>",
          "description": "We present an ongoing R&D activity for machine-learning-assisted navigation\nthrough detectors to be used for track reconstruction. We investigate different\napproaches of training neural networks for surface prediction and compare their\nresults. This work is carried out in the context of the ACTS tracking toolkit.",
          "link": "http://arxiv.org/abs/2108.03068",
          "publishedOn": "2021-08-09T00:49:28.703Z",
          "wordCount": 495,
          "title": "Machine learning for surface prediction in ACTS. (arXiv:2108.03068v1 [physics.ins-det])"
        },
        {
          "id": "http://arxiv.org/abs/2009.13370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Truong_L/0/1/0/all/0/1\">Lan V. Truong</a>",
          "description": "This paper estimates free energy, average mutual information, and minimum\nmean square error (MMSE) of a linear model under two assumptions: (1) the\nsource is generated by a Markov chain, (2) the source is generated via a hidden\nMarkov model. Our estimates are based on the replica method in statistical\nphysics. We show that under the posterior mean estimator, the linear model with\nMarkov sources or hidden Markov sources is decoupled into single-input AWGN\nchannels with state information available at both encoder and decoder where the\nstate distribution follows the left Perron-Frobenius eigenvector with unit\nManhattan norm of the stochastic matrix of Markov chains. Numerical results\nshow that the free energies and MSEs obtained via the replica method are\nclosely approximate to their counterparts achieved by the Metropolis-Hastings\nalgorithm or some well-known approximate message passing algorithms in the\nresearch literature.",
          "link": "http://arxiv.org/abs/2009.13370",
          "publishedOn": "2021-08-09T00:49:28.696Z",
          "wordCount": 638,
          "title": "Replica Analysis of the Linear Model with Markov or Hidden Markov Signal Priors. (arXiv:2009.13370v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03132",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Q/0/1/0/all/0/1\">Qiang Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">Dongxiao Zhang</a>",
          "description": "Random reconstruction of three-dimensional (3D) digital rocks from\ntwo-dimensional (2D) slices is crucial for elucidating the microstructure of\nrocks and its effects on pore-scale flow in terms of numerical modeling, since\nmassive samples are usually required to handle intrinsic uncertainties. Despite\nremarkable advances achieved by traditional process-based methods, statistical\napproaches and recently famous deep learning-based models, few works have\nfocused on producing several kinds of rocks with one trained model and allowing\nthe reconstructed samples to satisfy certain given properties, such as\nporosity. To fill this gap, we propose a new framework, named RockGPT, which is\ncomposed of VQ-VAE and conditional GPT, to synthesize 3D samples based on a\nsingle 2D slice from the perspective of video generation. The VQ-VAE is\nutilized to compress high-dimensional input video, i.e., the sequence of\ncontinuous rock slices, to discrete latent codes and reconstruct them. In order\nto obtain diverse reconstructions, the discrete latent codes are modeled using\nconditional GPT in an autoregressive manner, while incorporating conditional\ninformation from a given slice, rock type, and porosity. We conduct two\nexperiments on five kinds of rocks, and the results demonstrate that RockGPT\ncan produce different kinds of rocks with the same model, and the reconstructed\nsamples can successfully meet certain specified porosities. In a broader sense,\nthrough leveraging the proposed conditioning scheme, RockGPT constitutes an\neffective way to build a general model to produce multiple kinds of rocks\nsimultaneously that also satisfy user-defined properties.",
          "link": "http://arxiv.org/abs/2108.03132",
          "publishedOn": "2021-08-09T00:49:28.677Z",
          "wordCount": 702,
          "title": "RockGPT: Reconstructing three-dimensional digital rocks from single two-dimensional slice from the perspective of video generation. (arXiv:2108.03132v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.11777",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Elbir_A/0/1/0/all/0/1\">Ahmet M. Elbir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Papazafeiropoulos_A/0/1/0/all/0/1\">Anastasios K. Papazafeiropoulos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chatzinotas_S/0/1/0/all/0/1\">Symeon Chatzinotas</a>",
          "description": "Model-free techniques, such as machine learning (ML), have recently attracted\nmuch interest towards the physical layer design, e.g., symbol detection,\nchannel estimation, and beamforming. Most of these ML techniques employ\ncentralized learning (CL) schemes and assume the availability of datasets at a\nparameter server (PS), demanding the transmission of data from edge devices,\nsuch as mobile phones, to the PS. Exploiting the data generated at the edge,\nfederated learning (FL) has been proposed recently as a distributed learning\nscheme, in which each device computes the model parameters and sends them to\nthe PS for model aggregation while the datasets are kept intact at the edge.\nThus, FL is more communication-efficient and privacy-preserving than CL and\napplicable to the wireless communication scenarios, wherein the data are\ngenerated at the edge devices. This article presents the recent advances in\nFL-based training for physical layer design problems. Compared to CL, the\neffectiveness of FL is presented in terms of communication overhead with a\nslight performance loss in the learning accuracy. The design challenges, such\nas model, data, and hardware complexity, are also discussed in detail along\nwith possible solutions.",
          "link": "http://arxiv.org/abs/2102.11777",
          "publishedOn": "2021-08-09T00:49:28.669Z",
          "wordCount": 651,
          "title": "Federated Learning for Physical Layer Design. (arXiv:2102.11777v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03120",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Joshi_G/0/1/0/all/0/1\">Girish Joshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdhary_G/0/1/0/all/0/1\">Girish Chowdhary</a>",
          "description": "In this paper, we present a Stochastic Deep Neural Network-based Model\nReference Adaptive Control. Building on our work \"Deep Model Reference Adaptive\nControl\", we extend the controller capability by using Bayesian deep neural\nnetworks (DNN) to represent uncertainties and model non-linearities. Stochastic\nDeep Model Reference Adaptive Control uses a Lyapunov-based method to adapt the\noutput-layer weights of the DNN model in real-time, while a data-driven\nsupervised learning algorithm is used to update the inner-layers parameters.\nThis asynchronous network update ensures boundedness and guaranteed tracking\nperformance with a learning-based real-time feedback controller. A Bayesian\napproach to DNN learning helped avoid over-fitting the data and provide\nconfidence intervals over the predictions. The controller's stochastic nature\nalso ensured \"Induced Persistency of excitation,\" leading to convergence of the\noverall system signal.",
          "link": "http://arxiv.org/abs/2108.03120",
          "publishedOn": "2021-08-09T00:49:28.663Z",
          "wordCount": 572,
          "title": "Stochastic Deep Model Reference Adaptive Control. (arXiv:2108.03120v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2005.00478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Putatunda_S/0/1/0/all/0/1\">Sayan Putatunda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ubrangala_D/0/1/0/all/0/1\">Dayananda Ubrangala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rama_K/0/1/0/all/0/1\">Kiran Rama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondapalli_R/0/1/0/all/0/1\">Ravi Kondapalli</a>",
          "description": "In recent years, the concept of automated machine learning has become very\npopular. Automated Machine Learning (AutoML) mainly refers to the automated\nmethods for model selection and hyper-parameter optimization of various\nalgorithms such as random forests, gradient boosting, neural networks, etc. In\nthis paper, we introduce a new package i.e. DriveML for automated machine\nlearning. DriveML helps in implementing some of the pillars of an automated\nmachine learning pipeline such as automated data preparation, feature\nengineering, model building and model explanation by running the function\ninstead of writing lengthy R codes. The DriveML package is available in CRAN.\nWe compare the DriveML package with other relevant packages in CRAN/Github and\nfind that DriveML performs the best across different parameters. We also\nprovide an illustration by applying the DriveML package with default\nconfiguration on a real world dataset. Overall, the main benefits of DriveML\nare in development time savings, reduce developer's errors, optimal tuning of\nmachine learning models and reproducibility.",
          "link": "http://arxiv.org/abs/2005.00478",
          "publishedOn": "2021-08-09T00:49:28.656Z",
          "wordCount": 663,
          "title": "DriveML: An R Package for Driverless Machine Learning. (arXiv:2005.00478v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1\">Chaitanya Devaguptapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Devansh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_G/0/1/0/all/0/1\">Gaurav Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>",
          "description": "Adversarial robustness of deep learning models has gained much traction in\nthe last few years. Various attacks and defenses are proposed to improve the\nadversarial robustness of modern-day deep learning architectures. While all\nthese approaches help improve the robustness, one promising direction for\nimproving adversarial robustness is un-explored, i.e., the complex topology of\nthe neural network architecture. In this work, we answer the following\nquestion: \"Can the complex topology of a neural network give adversarial\nrobustness without any form of adversarial training?\" empirically by\nexperimenting with different hand-crafted and NAS based architectures. Our\nfindings show that, for small-scale attacks, NAS-based architectures are more\nrobust for small-scale datasets and simple tasks than hand-crafted\narchitectures. However, as the dataset's size or the task's complexity\nincrease, hand-crafted architectures are more robust than NAS-based\narchitectures. We perform the first large scale study to understand adversarial\nrobustness purely from an architectural perspective. Our results show that\nrandom sampling in the search space of DARTS (a popular NAS method) with simple\nensembling can improve the robustness to PGD attack by nearly ~12\\%. We show\nthat NAS, which is popular for SoTA accuracy, can provide adversarial accuracy\nas a free add-on without any form of adversarial training. Our results show\nthat leveraging the power of neural network topology with methods like\nensembles can be an excellent way to achieve adversarial robustness without any\nform of adversarial training. We also introduce a metric that can be used to\ncalculate the trade-off between clean accuracy and adversarial robustness.",
          "link": "http://arxiv.org/abs/2007.08428",
          "publishedOn": "2021-08-09T00:49:28.648Z",
          "wordCount": 753,
          "title": "On Adversarial Robustness: A Neural Architecture Search perspective. (arXiv:2007.08428v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khetarpal_K/0/1/0/all/0/1\">Khimya Khetarpal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1\">Zafarali Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comanici_G/0/1/0/all/0/1\">Gheorghe Comanici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>",
          "description": "Humans and animals have the ability to reason and make predictions about\ndifferent courses of action at many time scales. In reinforcement learning,\noption models (Sutton, Precup \\& Singh, 1999; Precup, 2000) provide the\nframework for this kind of temporally abstract prediction and reasoning.\nNatural intelligent agents are also able to focus their attention on courses of\naction that are relevant or feasible in a given situation, sometimes termed\naffordable actions. In this paper, we define a notion of affordances for\noptions, and develop temporally abstract partial option models, that take into\naccount the fact that an option might be affordable only in certain situations.\nWe analyze the trade-offs between estimation and approximation error in\nplanning and learning when using such models, and identify some interesting\nspecial cases. Additionally, we demonstrate empirically the potential impact of\npartial option models on the efficiency of planning.",
          "link": "http://arxiv.org/abs/2108.03213",
          "publishedOn": "2021-08-09T00:49:28.628Z",
          "wordCount": 580,
          "title": "Temporally Abstract Partial Models. (arXiv:2108.03213v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.03809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumais_S/0/1/0/all/0/1\">Susan Dumais</a>",
          "description": "Leveraging weak or noisy supervision for building effective machine learning\nmodels has long been an important research problem. Its importance has further\nincreased recently due to the growing need for large-scale datasets to train\ndeep learning models. Weak or noisy supervision could originate from multiple\nsources including non-expert annotators or automatic labeling based on\nheuristics or user interaction signals. There is an extensive amount of\nprevious work focusing on leveraging noisy labels. Most notably, recent work\nhas shown impressive gains by using a meta-learned instance re-weighting\napproach where a meta-learning framework is used to assign instance weights to\nnoisy labels. In this paper, we extend this approach via posing the problem as\nlabel correction problem within a meta-learning framework. We view the label\ncorrection procedure as a meta-process and propose a new meta-learning based\nframework termed MLC (Meta Label Correction) for learning with noisy labels.\nSpecifically, a label correction network is adopted as a meta-model to produce\ncorrected labels for noisy labels while the main model is trained to leverage\nthe corrected labeled. Both models are jointly trained by solving a bi-level\noptimization problem. We run extensive experiments with different label noise\nlevels and types on both image recognition and text classification tasks. We\ncompare the reweighing and correction approaches showing that the correction\nframing addresses some of the limitation of reweighting. We also show that the\nproposed MLC approach achieves large improvements over previous methods in many\nsettings.",
          "link": "http://arxiv.org/abs/1911.03809",
          "publishedOn": "2021-08-09T00:49:28.622Z",
          "wordCount": 704,
          "title": "Meta Label Correction for Noisy Label Learning. (arXiv:1911.03809v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cayci_S/0/1/0/all/0/1\">Semih Cayci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satpathi_S/0/1/0/all/0/1\">Siddhartha Satpathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_N/0/1/0/all/0/1\">Niao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1\">R. Srikant</a>",
          "description": "In this paper, we study the dynamics of temporal difference learning with\nneural network-based value function approximation over a general state space,\nnamely, \\emph{Neural TD learning}. We consider two practically used algorithms,\nprojection-free and max-norm regularized Neural TD learning, and establish the\nfirst convergence bounds for these algorithms. An interesting observation from\nour results is that max-norm regularization can dramatically improve the\nperformance of TD learning algorithms, both in terms of sample complexity and\noverparameterization. In particular, we prove that max-norm regularization\nimproves state-of-the-art sample complexity and overparameterization bounds.\nThe results in this work rely on a novel Lyapunov drift analysis of the network\nparameters as a stopped and controlled random process.",
          "link": "http://arxiv.org/abs/2103.01391",
          "publishedOn": "2021-08-09T00:49:28.615Z",
          "wordCount": 606,
          "title": "Sample Complexity and Overparameterization Bounds for Temporal Difference Learning with Neural Network Approximation. (arXiv:2103.01391v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>",
          "description": "This survey is meant to provide an introduction to linear models and the\ntheories behind them. Our goal is to give a rigorous introduction to the\nreaders with prior exposure to ordinary least squares. In machine learning, the\noutput is usually a nonlinear function of the input. Deep learning even aims to\nfind a nonlinear dependence with many layers which require a large amount of\ncomputation. However, most of these algorithms build upon simple linear models.\nWe then describe linear models from different views and find the properties and\ntheories behind the models. The linear model is the main technique in\nregression problems and the primary tool for it is the least squares\napproximation which minimizes a sum of squared errors. This is a natural choice\nwhen we're interested in finding the regression function which minimizes the\ncorresponding expected squared error. This survey is primarily a summary of\npurpose, significance of important theories behind linear models, e.g.,\ndistribution theory, minimum variance estimator. We first describe ordinary\nleast squares from three different points of view upon which we disturb the\nmodel with random noise and Gaussian noise. By Gaussian noise, the model gives\nrise to the likelihood so that we introduce a maximum likelihood estimator. It\nalso develops some distribution theories via this Gaussian disturbance. The\ndistribution theory of least squares will help us answer various questions and\nintroduce related applications. We then prove least squares is the best\nunbiased linear model in the sense of mean squared error and most importantly,\nit actually approaches the theoretical limit. We end up with linear models with\nthe Bayesian approach and beyond.",
          "link": "http://arxiv.org/abs/2105.04240",
          "publishedOn": "2021-08-09T00:49:28.608Z",
          "wordCount": 728,
          "title": "A rigorous introduction for linear models. (arXiv:2105.04240v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1\">Shagufta Henna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reji_A/0/1/0/all/0/1\">Aparna Reji</a>",
          "description": "Covid-19 detection at an early stage can aid in an effective treatment and\nisolation plan to prevent its spread. Recently, transfer learning has been used\nfor Covid-19 detection using X-ray, ultrasound, and CT scans. One of the major\nlimitations inherent to these proposed methods is limited labeled dataset size\nthat affects the reliability of Covid-19 diagnosis and disease progression. In\nthis work, we demonstrate that how we can augment limited X-ray images data by\nusing Contrast limited adaptive histogram equalization (CLAHE) to train the\nlast layer of the pre-trained deep learning models to mitigate the bias of\ntransfer learning for Covid-19 detection. We transfer learned various\npre-trained deep learning models including AlexNet, ZFNet, VGG-16, ResNet-18,\nand GoogLeNet, and fine-tune the last layer by using CLAHE-augmented dataset.\nThe experiment results reveal that the CLAHE-based augmentation to various\npre-trained deep learning models significantly improves the model efficiency.\nThe pre-trained VCG-16 model with CLAHEbased augmented images achieves a\nsensitivity of 95% using 15 epochs. AlexNet works show good sensitivity when\ntrained on non-augmented data. Other models demonstrate a value of less than\n60% when trained on non-augmented data. Our results reveal that the sample bias\ncan negatively impact the performance of transfer learning which is\nsignificantly improved by using CLAHE-based augmentation.",
          "link": "http://arxiv.org/abs/2108.02870",
          "publishedOn": "2021-08-09T00:49:28.600Z",
          "wordCount": 692,
          "title": "A Data Augmented Approach to Transfer Learning for Covid-19 Detection. (arXiv:2108.02870v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+HaoChen_J/0/1/0/all/0/1\">Jeff Z. HaoChen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Colin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>",
          "description": "Recent works in self-supervised learning have advanced the state-of-the-art\nby relying on the contrastive learning paradigm, which learns representations\nby pushing positive pairs, or similar examples from the same class, closer\ntogether while keeping negative pairs far apart. Despite the empirical\nsuccesses, theoretical foundations are limited -- prior analyses assume\nconditional independence of the positive pairs given the same class label, but\nrecent empirical applications use heavily correlated positive pairs (i.e., data\naugmentations of the same image). Our work analyzes contrastive learning\nwithout assuming conditional independence of positive pairs using a novel\nconcept of the augmentation graph on data. Edges in this graph connect\naugmentations of the same data, and ground-truth classes naturally form\nconnected sub-graphs. We propose a loss that performs spectral decomposition on\nthe population augmentation graph and can be succinctly written as a\ncontrastive learning objective on neural net representations. Minimizing this\nobjective leads to features with provable accuracy guarantees under linear\nprobe evaluation. By standard generalization bounds, these accuracy guarantees\nalso hold when minimizing the training contrastive loss. Empirically, the\nfeatures learned by our objective can match or outperform several strong\nbaselines on benchmark vision datasets. In all, this work provides the first\nprovable analysis for contrastive learning where guarantees for linear probe\nevaluation can apply to realistic empirical settings.",
          "link": "http://arxiv.org/abs/2106.04156",
          "publishedOn": "2021-08-09T00:49:28.592Z",
          "wordCount": 710,
          "title": "Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss. (arXiv:2106.04156v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05476",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1\">Kun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zengfeng Huang</a>",
          "description": "Node embedding learns a low-dimensional representation for each node in the\ngraph. Recent progress on node embedding shows that proximity matrix\nfactorization methods gain superb performance and scale to large graphs with\nmillions of nodes. Existing approaches first define a proximity matrix and then\nlearn the embeddings that fit the proximity by matrix factorization. Most\nexisting matrix factorization methods adopt the same proximity for different\ntasks, while it is observed that different tasks and datasets may require\ndifferent proximity, limiting their representation power.\n\nMotivated by this, we propose {\\em Lemane}, a framework with trainable\nproximity measures, which can be learned to best suit the datasets and tasks at\nhand automatically. Our method is end-to-end, which incorporates differentiable\nSVD in the pipeline so that the parameters can be trained via backpropagation.\nHowever, this learning process is still expensive on large graphs. To improve\nthe scalability, we train proximity measures only on carefully subsampled\ngraphs, and then apply standard proximity matrix factorization on the original\ngraph using the learned proximity. Note that, computing the learned proximities\nfor each pair is still expensive for large graphs, and existing techniques for\ncomputing proximities are not applicable to the learned proximities. Thus, we\npresent generalized push techniques to make our solution scalable to large\ngraphs with millions of nodes. Extensive experiments show that our proposed\nsolution outperforms existing solutions on both link prediction and node\nclassification tasks on almost all datasets.",
          "link": "http://arxiv.org/abs/2106.05476",
          "publishedOn": "2021-08-09T00:49:28.574Z",
          "wordCount": 757,
          "title": "Learning Based Proximity Matrix Factorization for Node Embedding. (arXiv:2106.05476v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02504",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Meunier_D/0/1/0/all/0/1\">Dimitri Meunier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Alquier_P/0/1/0/all/0/1\">Pierre Alquier</a>",
          "description": "Online learning methods, like the online gradient algorithm (OGA) and\nexponentially weighted aggregation (EWA), often depend on tuning parameters\nthat are difficult to set in practice. We consider an online meta-learning\nscenario, and we propose a meta-strategy to learn these parameters from past\ntasks. Our strategy is based on the minimization of a regret bound. It allows\nto learn the initialization and the step size in OGA with guarantees. It also\nallows to learn the prior or the learning rate in EWA. We provide a regret\nanalysis of the strategy. It allows to identify settings where meta-learning\nindeed improves on learning each task in isolation.",
          "link": "http://arxiv.org/abs/2102.02504",
          "publishedOn": "2021-08-09T00:49:28.566Z",
          "wordCount": 568,
          "title": "Meta-strategy for Learning Tuning Parameters with Guarantees. (arXiv:2102.02504v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohtasib_A/0/1/0/all/0/1\">Abdalkarim Mohtasib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1\">Gerhard Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuayahuitl_H/0/1/0/all/0/1\">Heriberto Cuayahuitl</a>",
          "description": "Deep Reinforcement Learning (DRL) is a promising approach for teaching robots\nnew behaviour. However, one of its main limitations is the need for carefully\nhand-coded reward signals by an expert. We argue that it is crucial to automate\nthe reward learning process so that new skills can be taught to robots by their\nusers. To address such automation, we consider task success classifiers using\nvisual observations to estimate the rewards in terms of task success. In this\nwork, we study the performance of multiple state-of-the-art deep reinforcement\nlearning algorithms under different types of reward: Dense, Sparse, Visual\nDense, and Visual Sparse rewards. Our experiments in various simulation tasks\n(Pendulum, Reacher, Pusher, and Fetch Reach) show that while DRL agents can\nlearn successful behaviours using visual rewards when the goal targets are\ndistinguishable, their performance may decrease if the task goal is not clearly\nvisible. Our results also show that visual dense rewards are more successful\nthan visual sparse rewards and that there is no single best algorithm for all\ntasks.",
          "link": "http://arxiv.org/abs/2108.03222",
          "publishedOn": "2021-08-09T00:49:28.560Z",
          "wordCount": 607,
          "title": "A Study on Dense and Sparse (Visual) Rewards in Robot Policy Learning. (arXiv:2108.03222v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demir_U/0/1/0/all/0/1\">Ufuk Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozer_A/0/1/0/all/0/1\">Atahan Ozer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahin_Y/0/1/0/all/0/1\">Yusuf H. Sahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_G/0/1/0/all/0/1\">Gozde Unal</a>",
          "description": "In recent years, deep learning based methods have shown success in essential\nmedical image analysis tasks such as segmentation. Post-processing and refining\nthe results of segmentation is a common practice to decrease the\nmisclassifications originating from the segmentation network. In addition to\nwidely used methods like Conditional Random Fields (CRFs) which focus on the\nstructure of the segmented volume/area, a graph-based recent approach makes use\nof certain and uncertain points in a graph and refines the segmentation\naccording to a small graph convolutional network (GCN). However, there are two\ndrawbacks of the approach: most of the edges in the graph are assigned randomly\nand the GCN is trained independently from the segmentation network. To address\nthese issues, we define a new neighbor-selection mechanism according to feature\ndistances and combine the two networks in the training procedure. According to\nthe experimental results on pancreas segmentation from Computed Tomography (CT)\nimages, we demonstrate improvement in the quantitative measures. Also,\nexamining the dynamic neighbors created by our method, edges between\nsemantically similar image parts are observed. The proposed method also shows\nqualitative enhancements in the segmentation maps, as demonstrated in the\nvisual results.",
          "link": "http://arxiv.org/abs/2108.03117",
          "publishedOn": "2021-08-09T00:49:28.553Z",
          "wordCount": 651,
          "title": "Uncertainty-Based Dynamic Graph Neighborhoods For Medical Segmentation. (arXiv:2108.03117v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pepe_A/0/1/0/all/0/1\">Antonio Pepe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1\">Christina Gsaxner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fen-hua Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1\">Jens Kleesiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F. Frangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1\">Jan Egger</a>",
          "description": "The aortic vessel tree is composed of the aorta and its branching arteries,\nand plays a key role in supplying the whole body with blood. Aortic diseases,\nlike aneurysms or dissections, can lead to an aortic rupture, whose treatment\nwith open surgery is highly risky. Therefore, patients commonly undergo drug\ntreatment under constant monitoring, which requires regular inspections of the\nvessels through imaging. The standard imaging modality for diagnosis and\nmonitoring is computed tomography (CT), which can provide a detailed picture of\nthe aorta and its branching vessels if combined with a contrast agent,\nresulting in a CT angiography (CTA). Optimally, the whole aortic vessel tree\ngeometry from consecutive CTAs, are overlaid and compared. This allows to not\nonly detect changes in the aorta, but also more peripheral vessel tree changes,\ncaused by the primary pathology or newly developed. When performed manually,\nthis reconstruction requires slice by slice contouring, which could easily take\na whole day for a single aortic vessel tree and, hence, is not feasible in\nclinical practice. Automatic or semi-automatic vessel tree segmentation\nalgorithms, on the other hand, can complete this task in a fraction of the\nmanual execution time and run in parallel to the clinical routine of the\nclinicians. In this paper, we systematically review computing techniques for\nthe automatic and semi-automatic segmentation of the aortic vessel tree. The\nreview concludes with an in-depth discussion on how close these\nstate-of-the-art approaches are to an application in clinical practice and how\nactive this research field is, taking into account the number of publications,\ndatasets and challenges.",
          "link": "http://arxiv.org/abs/2108.02998",
          "publishedOn": "2021-08-09T00:49:28.536Z",
          "wordCount": 744,
          "title": "AI-based Aortic Vessel Tree Segmentation for Cardiovascular Diseases Treatment: Status Quo. (arXiv:2108.02998v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08991",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Banchi_L/0/1/0/all/0/1\">Leonardo Banchi</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Pereira_J/0/1/0/all/0/1\">Jason Pereira</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Pirandola_S/0/1/0/all/0/1\">Stefano Pirandola</a>",
          "description": "Quantum classification and hypothesis testing are two tightly related\nsubjects, the main difference being that the former is data driven: how to\nassign to quantum states $\\rho(x)$ the corresponding class $c$ (or hypothesis)\nis learnt from examples during training, where $x$ can be either tunable\nexperimental parameters or classical data \"embedded\" into quantum states. Does\nthe model generalize? This is the main question in any data-driven strategy,\nnamely the ability to predict the correct class even of previously unseen\nstates. Here we establish a link between quantum machine learning\nclassification and quantum hypothesis testing (state and channel\ndiscrimination) and then show that the accuracy and generalization capability\nof quantum classifiers depend on the (R\\'enyi) mutual informations $I(C{:}Q)$\nand $I_2(X{:}Q)$ between the quantum state space $Q$ and the classical\nparameter space $X$ or class space $C$. Based on the above characterization, we\nthen show how different properties of $Q$ affect classification accuracy and\ngeneralization, such as the dimension of the Hilbert space, the amount of\nnoise, and the amount of neglected information from $X$ via, e.g., pooling\nlayers. Moreover, we introduce a quantum version of the Information Bottleneck\nprinciple that allows us to explore the various tradeoffs between accuracy and\ngeneralization. Finally, in order to check our theoretical predictions, we\nstudy the classification of the quantum phases of an Ising spin chain, and we\npropose the Variational Quantum Information Bottleneck (VQIB) method to\noptimize quantum embeddings of classical data to favor generalization.",
          "link": "http://arxiv.org/abs/2102.08991",
          "publishedOn": "2021-08-09T00:49:28.528Z",
          "wordCount": 722,
          "title": "Generalization in Quantum Machine Learning: a Quantum Information Perspective. (arXiv:2102.08991v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08637",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Rajpal_S/0/1/0/all/0/1\">Sheetal Rajpal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_M/0/1/0/all/0/1\">Manoj Agarwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpal_A/0/1/0/all/0/1\">Ankit Rajpal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lakhyani_N/0/1/0/all/0/1\">Navin Lakhyani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saggar_A/0/1/0/all/0/1\">Arpita Saggar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_N/0/1/0/all/0/1\">Naveen Kumar</a>",
          "description": "Coronaviruses constitute a family of viruses that gives rise to respiratory\ndiseases. As COVID-19 is highly contagious, early diagnosis of COVID-19 is\ncrucial for an effective treatment strategy. However, the RT-PCR test which is\nconsidered to be a gold standard in the diagnosis of COVID-19 suffers from a\nhigh false-negative rate. Chest X-ray (CXR) image analysis has emerged as a\nfeasible and effective diagnostic technique towards this objective. In this\nwork, we propose the COVID-19 classification problem as a three-class\nclassification problem to distinguish between COVID-19, normal, and pneumonia\nclasses. We propose a three-stage framework, named COV-ELM. Stage one deals\nwith preprocessing and transformation while stage two deals with feature\nextraction. These extracted features are passed as an input to the ELM at the\nthird stage, resulting in the identification of COVID-19. The choice of ELM in\nthis work has been motivated by its faster convergence, better generalization\ncapability, and shorter training time in comparison to the conventional\ngradient-based learning algorithms. As bigger and diverse datasets become\navailable, ELM can be quickly retrained as compared to its gradient-based\ncompetitor models. The proposed model achieved a macro average F1-score of 0.95\nand the overall sensitivity of ${0.94 \\pm 0.02} at a 95% confidence interval.\nWhen compared to state-of-the-art machine learning algorithms, the COV-ELM is\nfound to outperform its competitors in this three-class classification\nscenario. Further, LIME has been integrated with the proposed COV-ELM model to\ngenerate annotated CXR images. The annotations are based on the superpixels\nthat have contributed to distinguish between the different classes. It was\nobserved that the superpixels correspond to the regions of the human lungs that\nare clinically observed in COVID-19 and Pneumonia cases.",
          "link": "http://arxiv.org/abs/2007.08637",
          "publishedOn": "2021-08-09T00:49:28.521Z",
          "wordCount": 831,
          "title": "COV-ELM classifier: An Extreme Learning Machine based identification of COVID-19 using Chest X-Ray Images. (arXiv:2007.08637v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ranjbar_A/0/1/0/all/0/1\">Alireza Ranjbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1\">Ngo Anh Vien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1\">Hanna Ziesche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boedecker_J/0/1/0/all/0/1\">Joschka Boedecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1\">Gerhard Neumann</a>",
          "description": "While classic control theory offers state of the art solutions in many\nproblem scenarios, it is often desired to improve beyond the structure of such\nsolutions and surpass their limitations. To this end, residual policy learning\n(RPL) offers a formulation to improve existing controllers with reinforcement\nlearning (RL) by learning an additive \"residual\" to the output of a given\ncontroller. However, the applicability of such an approach highly depends on\nthe structure of the controller. Often, internal feedback signals of the\ncontroller limit an RL algorithm to adequately change the policy and, hence,\nlearn the task. We propose a new formulation that addresses these limitations\nby also modifying the feedback signals to the controller with an RL policy and\nshow superior performance of our approach on a contact-rich peg-insertion task\nunder position and orientation uncertainty. In addition, we use a recent\nCartesian impedance control architecture as the control framework which can be\navailable to us as a black-box while assuming no knowledge about its\ninput/output structure, and show the difficulties of standard RPL. Furthermore,\nwe introduce an adaptive curriculum for the given task to gradually increase\nthe task difficulty in terms of position and orientation uncertainty. A video\nshowing the results can be found at https://youtu.be/SAZm_Krze7U .",
          "link": "http://arxiv.org/abs/2106.04306",
          "publishedOn": "2021-08-09T00:49:28.514Z",
          "wordCount": 685,
          "title": "Residual Feedback Learning for Contact-Rich Manipulation Tasks with Uncertainty. (arXiv:2106.04306v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trott_A/0/1/0/all/0/1\">Alexander Trott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1\">Sunil Srinivasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wal_D/0/1/0/all/0/1\">Douwe van der Wal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haneuse_S/0/1/0/all/0/1\">Sebastien Haneuse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Stephan Zheng</a>",
          "description": "Optimizing economic and public policy is critical to address socioeconomic\nissues and trade-offs, e.g., improving equality, productivity, or wellness, and\nposes a complex mechanism design problem. A policy designer needs to consider\nmultiple objectives, policy levers, and behavioral responses from strategic\nactors who optimize for their individual objectives. Moreover, real-world\npolicies should be explainable and robust to simulation-to-reality gaps, e.g.,\ndue to calibration issues. Existing approaches are often limited to a narrow\nset of policy levers or objectives that are hard to measure, do not yield\nexplicit optimal policies, or do not consider strategic behavior, for example.\nHence, it remains challenging to optimize policy in real-world scenarios. Here\nwe show that the AI Economist framework enables effective, flexible, and\ninterpretable policy design using two-level reinforcement learning (RL) and\ndata-driven simulations. We validate our framework on optimizing the stringency\nof US state policies and Federal subsidies during a pandemic, e.g., COVID-19,\nusing a simulation fitted to real data. We find that log-linear policies\ntrained using RL significantly improve social welfare, based on both public\nhealth and economic outcomes, compared to past outcomes. Their behavior can be\nexplained, e.g., well-performing policies respond strongly to changes in\nrecovery and vaccination rates. They are also robust to calibration errors,\ne.g., infection rates that are over or underestimated. As of yet, real-world\npolicymaking has not seen adoption of machine learning methods at large,\nincluding RL and AI-driven simulations. Our results show the potential of AI to\nguide policy design and improve social welfare amidst the complexity of the\nreal world.",
          "link": "http://arxiv.org/abs/2108.02904",
          "publishedOn": "2021-08-09T00:49:28.507Z",
          "wordCount": 775,
          "title": "Building a Foundation for Data-Driven, Interpretable, and Robust Policy Design using the AI Economist. (arXiv:2108.02904v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Habib_M/0/1/0/all/0/1\">Mohammad Kasra Habib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_S/0/1/0/all/0/1\">Stefan Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graziotin_D/0/1/0/all/0/1\">Daniel Graziotin</a>",
          "description": "Requirements Engineering (RE) is the initial step towards building a software\nsystem. The success or failure of a software project is firmly tied to this\nphase, based on communication among stakeholders using natural language. The\nproblem with natural language is that it can easily lead to different\nunderstandings if it is not expressed precisely by the stakeholders involved,\nwhich results in building a product different from the expected one. Previous\nwork proposed to enhance the quality of the software requirements detecting\nlanguage errors based on ISO 29148 requirements language criteria. The existing\nsolutions apply classical Natural Language Processing (NLP) to detect them. NLP\nhas some limitations, such as domain dependability which results in poor\ngeneralization capability. Therefore, this work aims to improve the previous\nwork by creating a manually labeled dataset and using ensemble learning, Deep\nLearning (DL), and techniques such as word embeddings and transfer learning to\novercome the generalization problem that is tied with classical NLP and improve\nprecision and recall metrics using a manually labeled dataset. The current\nfindings show that the dataset is unbalanced and which class examples should be\nadded more. It is tempting to train algorithms even if the dataset is not\nconsiderably representative. Whence, the results show that models are\noverfitting; in Machine Learning this issue is solved by adding more instances\nto the dataset, improving label quality, removing noise, and reducing the\nlearning algorithms complexity, which is planned for this research.",
          "link": "http://arxiv.org/abs/2108.03087",
          "publishedOn": "2021-08-09T00:49:28.489Z",
          "wordCount": 695,
          "title": "Detecting Requirements Smells With Deep Learning: Experiences, Challenges and Future Work. (arXiv:2108.03087v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2012.06341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1\">Ant&#xf4;nio H. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendriks_J/0/1/0/all/0/1\">Johannes N. Hendriks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wills_A/0/1/0/all/0/1\">Adrian G. Wills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1\">Thomas B. Sch&#xf6;n</a>",
          "description": "System identification aims to build models of dynamical systems from data.\nTraditionally, choosing the model requires the designer to balance between two\ngoals of conflicting nature; the model must be rich enough to capture the\nsystem dynamics, but not so flexible that it learns spurious random effects\nfrom the dataset. It is typically observed that the model validation\nperformance follows a U-shaped curve as the model complexity increases. Recent\ndevelopments in machine learning and statistics, however, have observed\nsituations where a \"double-descent\" curve subsumes this U-shaped\nmodel-performance curve. With a second decrease in performance occurring beyond\nthe point where the model has reached the capacity of interpolating - i.e.,\n(near) perfectly fitting - the training data. To the best of our knowledge,\nsuch phenomena have not been studied within the context of dynamic systems. The\npresent paper aims to answer the question: \"Can such a phenomenon also be\nobserved when estimating parameters of dynamic systems?\" We show that the\nanswer is yes, verifying such behavior experimentally both for artificially\ngenerated and real-world datasets.",
          "link": "http://arxiv.org/abs/2012.06341",
          "publishedOn": "2021-08-09T00:49:28.462Z",
          "wordCount": 671,
          "title": "Beyond Occam's Razor in System Identification: Double-Descent when Modeling Dynamics. (arXiv:2012.06341v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kudisthalert_W/0/1/0/all/0/1\">Wasu Kudisthalert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasupa_K/0/1/0/all/0/1\">Kitsuchart Pasupa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>",
          "description": "Extreme Learning Machine is a powerful classification method very competitive\nexisting classification methods. It is extremely fast at training.\nNevertheless, it cannot perform face verification tasks properly because face\nverification tasks require comparison of facial images of two individuals at\nthe same time and decide whether the two faces identify the same person. The\nstructure of Extreme Leaning Machine was not designed to feed two input data\nstreams simultaneously, thus, in 2-input scenarios Extreme Learning Machine\nmethods are normally applied using concatenated inputs. However, this setup\nconsumes two times more computational resources and it is not optimized for\nrecognition tasks where learning a separable distance metric is critical. For\nthese reasons, we propose and develop a Siamese Extreme Learning Machine\n(SELM). SELM was designed to be fed with two data streams in parallel\nsimultaneously. It utilizes a dual-stream Siamese condition in the extra\nSiamese layer to transform the data before passing it along to the hidden\nlayer. Moreover, we propose a Gender-Ethnicity-Dependent triplet feature\nexclusively trained on a variety of specific demographic groups. This feature\nenables learning and extracting of useful facial features of each group.\nExperiments were conducted to evaluate and compare the performances of SELM,\nExtreme Learning Machine, and DCNN. The experimental results showed that the\nproposed feature was able to perform correct classification at 97.87% accuracy\nand 99.45% AUC. They also showed that using SELM in conjunction with the\nproposed feature provided 98.31% accuracy and 99.72% AUC. They outperformed the\nwell-known DCNN and Extreme Leaning Machine methods by a wide margin.",
          "link": "http://arxiv.org/abs/2108.03140",
          "publishedOn": "2021-08-09T00:49:28.455Z",
          "wordCount": 703,
          "title": "SELM: Siamese Extreme Learning Machine with Application to Face Biometrics. (arXiv:2108.03140v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02837",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Mianroodi_J/0/1/0/all/0/1\">Jaber Rezaei Mianroodi</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Rezaei_S/0/1/0/all/0/1\">Shahed Rezaei</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Siboni_N/0/1/0/all/0/1\">Nima H. Siboni</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Xu_B/0/1/0/all/0/1\">Bai-Xiang Xu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Raabe_D/0/1/0/all/0/1\">Dierk Raabe</a>",
          "description": "The elastic properties of materials derive from their electronic and atomic\nnature. However, simulating bulk materials fully at these scales is not\nfeasible, so that typically homogenized continuum descriptions are used\ninstead. A seamless and lossless transition of the constitutive description of\nthe elastic response of materials between these two scales has been so far\nelusive. Here we show how this problem can be overcome by using Artificial\nIntelligence (AI). A Convolutional Neural Network (CNN) model is trained, by\ntaking the structure image of a nanoporous material as input and the\ncorresponding elasticity tensor, calculated from Molecular Statics (MS), as\noutput. Trained with the atomistic data, the CNN model captures the size- and\npore-dependency of the material's elastic properties which, on the physics\nside, can stem from surfaces and non-local effects. Such effects are often\nignored in upscaling from atomistic to classical continuum theory. To\ndemonstrate the accuracy and the efficiency of the trained CNN model, a Finite\nElement Method (FEM) based result of an elastically deformed nanoporous beam\nequipped with the CNN as constitutive law is compared with that by a full\natomistic simulation. The good agreement between the atomistic simulations and\nthe FEM-AI combination for a system with size and surface effects establishes a\nnew lossless scale bridging approach to such problems. The trained CNN model\ndeviates from the atomistic result by 9.6\\% for porosity scenarios of up to\n90\\% but it is about 230 times faster than the MS calculation and does not\nrequire to change simulation methods between different scales. The efficiency\nof the CNN evaluation together with the preservation of important atomistic\neffects makes the trained model an effective atomistically-informed\nconstitutive model for macroscopic simulations of nanoporous materials and\nsolving of inverse problems.",
          "link": "http://arxiv.org/abs/2108.02837",
          "publishedOn": "2021-08-09T00:49:28.448Z",
          "wordCount": 734,
          "title": "Lossless Multi-Scale Constitutive Elastic Relations with Artificial Intelligence. (arXiv:2108.02837v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/1912.07942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zanella_Beguelin_S/0/1/0/all/0/1\">Santiago Zanella-B&#xe9;guelin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1\">Lukas Wutschitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1\">Shruti Tople</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruhle_V/0/1/0/all/0/1\">Victor R&#xfc;hle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paverd_A/0/1/0/all/0/1\">Andrew Paverd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohrimenko_O/0/1/0/all/0/1\">Olga Ohrimenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopf_B/0/1/0/all/0/1\">Boris K&#xf6;pf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockschmidt_M/0/1/0/all/0/1\">Marc Brockschmidt</a>",
          "description": "To continuously improve quality and reflect changes in data, machine learning\napplications have to regularly retrain and update their core models. We show\nthat a differential analysis of language model snapshots before and after an\nupdate can reveal a surprising amount of detailed information about changes in\nthe training data. We propose two new metrics---\\emph{differential score} and\n\\emph{differential rank}---for analyzing the leakage due to updates of natural\nlanguage models. We perform leakage analysis using these metrics across models\ntrained on several different datasets using different methods and\nconfigurations. We discuss the privacy implications of our findings, propose\nmitigation strategies and evaluate their effect.",
          "link": "http://arxiv.org/abs/1912.07942",
          "publishedOn": "2021-08-09T00:49:28.441Z",
          "wordCount": 609,
          "title": "Analyzing Information Leakage of Updates to Natural Language Models. (arXiv:1912.07942v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrevoets_J/0/1/0/all/0/1\">Jeroen Berrevoets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1\">Mihaela van der Schaar</a>",
          "description": "Conditional average treatment effects (CATEs) allow us to understand the\neffect heterogeneity across a large population of individuals. However, typical\nCATE learners assume all confounding variables are measured in order for the\nCATE to be identifiable. Often, this requirement is satisfied by simply\ncollecting many variables, at the expense of increased sample complexity for\nestimating CATEs. To combat this, we propose an energy-based model (EBM) that\nlearns a low-dimensional representation of the variables by employing a noise\ncontrastive loss function. With our EBM we introduce a preprocessing step that\nalleviates the dimensionality curse for any existing model and learner\ndeveloped for estimating CATE. We prove that our EBM keeps the representations\npartially identifiable up to some universal constant, as well as having\nuniversal approximation capability to avoid excessive information loss from\nmodel misspecification; these properties combined with our loss function,\nenable the representations to converge and keep the CATE estimation consistent.\nExperiments demonstrate the convergence of the representations, as well as show\nthat estimating CATEs on our representations performs better than on the\nvariables or the representations obtained via various benchmark dimensionality\nreduction methods.",
          "link": "http://arxiv.org/abs/2108.03039",
          "publishedOn": "2021-08-09T00:49:28.423Z",
          "wordCount": 630,
          "title": "Identifiable Energy-based Representations: An Application to Estimating Heterogeneous Causal Effects. (arXiv:2108.03039v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03002",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhang_H/0/1/0/all/0/1\">HongBing Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1\">XinYi Liu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fan_H/0/1/0/all/0/1\">HongTao Fan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1\">YaJing Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ye_Y/0/1/0/all/0/1\">Yinlin Ye</a>",
          "description": "More recently, an Approximate SVD Based on Qatar Riyal (QR) Decomposition\n(CSVD-QR) method for matrix complete problem is presented, whose computational\ncomplexity is $O(r^2(m+n))$, which is mainly due to that $r$ is far less than\n$\\min\\{m,n\\}$, where $r$ represents the largest number of singular values of\nmatrix $X$. What is particularly interesting is that after replacing the\nnuclear norm with the $L_{2,1}$ norm proposed based on this decomposition, as\nthe upper bound of the nuclear norm, when the intermediate matrix $D$ in its\ndecomposition is close to the diagonal matrix, it will converge to the nuclear\nnorm, and is exactly equal, when the $D$ matrix is equal to the diagonal\nmatrix, to the nuclear norm, which ingeniously avoids the calculation of the\nsingular value of the matrix. To the best of our knowledge, there is no\nliterature to generalize and apply it to solve tensor complete problems.\nInspired by this, in this paper we propose a class of tensor minimization model\nbased on $L_{2,1}$ norm and CSVD-QR method for the tensor complete problem,\nwhich is convex and therefore has a global minimum solution.",
          "link": "http://arxiv.org/abs/2108.03002",
          "publishedOn": "2021-08-09T00:49:28.416Z",
          "wordCount": 640,
          "title": "Fast and Accurate Low-Rank Tensor Completion Methods Based on QR Decomposition and $L_{2,1}$ Norm Minimization. (arXiv:2108.03002v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03131",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+MacLean_A/0/1/0/all/0/1\">Alexander MacLean</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abbasi_S/0/1/0/all/0/1\">Saad Abbasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebadi_A/0/1/0/all/0/1\">Ashkan Ebadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_A/0/1/0/all/0/1\">Andy Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pavlova_M/0/1/0/all/0/1\">Maya Pavlova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gunraj_H/0/1/0/all/0/1\">Hayden Gunraj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xi_P/0/1/0/all/0/1\">Pengcheng Xi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kohli_S/0/1/0/all/0/1\">Sonny Kohli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>",
          "description": "The Coronavirus Disease 2019 (COVID-19) pandemic has impacted many aspects of\nlife globally, and a critical factor in mitigating its effects is screening\nindividuals for infections, thereby allowing for both proper treatment for\nthose individuals as well as action to be taken to prevent further spread of\nthe virus. Point-of-care ultrasound (POCUS) imaging has been proposed as a\nscreening tool as it is a much cheaper and easier to apply imaging modality\nthan others that are traditionally used for pulmonary examinations, namely\nchest x-ray and computed tomography. Given the scarcity of expert radiologists\nfor interpreting POCUS examinations in many highly affected regions around the\nworld, low-cost deep learning-driven clinical decision support solutions can\nhave a large impact during the on-going pandemic. Motivated by this, we\nintroduce COVID-Net US, a highly efficient, self-attention deep convolutional\nneural network design tailored for COVID-19 screening from lung POCUS images.\nExperimental results show that the proposed COVID-Net US can achieve an AUC of\nover 0.98 while achieving 353X lower architectural complexity, 62X lower\ncomputational complexity, and 14.3X faster inference times on a Raspberry Pi.\nClinical validation was also conducted, where select cases were reviewed and\nreported on by a practicing clinician (20 years of clinical practice)\nspecializing in intensive care (ICU) and 15 years of expertise in POCUS\ninterpretation. To advocate affordable healthcare and artificial intelligence\nfor resource-constrained environments, we have made COVID-Net US open source\nand publicly available as part of the COVID-Net open source initiative.",
          "link": "http://arxiv.org/abs/2108.03131",
          "publishedOn": "2021-08-09T00:49:28.409Z",
          "wordCount": 781,
          "title": "COVID-Net US: A Tailored, Highly Efficient, Self-Attention Deep Convolutional Neural Network Design for Detection of COVID-19 Patient Cases from Point-of-care Ultrasound Imaging. (arXiv:2108.03131v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02838",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Karatas_T/0/1/0/all/0/1\">Tugce Karatas</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Hirsa_A/0/1/0/all/0/1\">Ali Hirsa</a>",
          "description": "Market indicators such as CPI and GDP have been widely used over decades to\nidentify the stage of business cycles and also investment attractiveness of\nsectors given market conditions. In this paper, we propose a two-stage\nmethodology that consists of predicting ETF prices for each sector using market\nindicators and ranking sectors based on their predicted rate of returns. We\ninitially start with choosing sector specific macroeconomic indicators and\nimplement Recursive Feature Elimination algorithm to select the most important\nfeatures for each sector. Using our prediction tool, we implement different\nRecurrent Neural Networks models to predict the future ETF prices for each\nsector. We then rank the sectors based on their predicted rate of returns. We\nselect the best performing model by evaluating the annualized return,\nannualized Sharpe ratio, and Calmar ratio of the portfolios that includes the\ntop four ranked sectors chosen by the model. We also test the robustness of the\nmodel performance with respect to lookback windows and look ahead windows. Our\nempirical results show that our methodology beats the equally weighted\nportfolio performance even in the long run. We also find that Echo State\nNetworks exhibits an outstanding performance compared to other models yet it is\nfaster to implement compared to other RNN models.",
          "link": "http://arxiv.org/abs/2108.02838",
          "publishedOn": "2021-08-09T00:49:28.402Z",
          "wordCount": 647,
          "title": "Two-Stage Sector Rotation Methodology Using Machine Learning and Deep Learning Techniques. (arXiv:2108.02838v1 [q-fin.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jarl_S/0/1/0/all/0/1\">Sanna Jarl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahrovani_S/0/1/0/all/0/1\">Sadegh Rahrovani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chehreghani_M/0/1/0/all/0/1\">Morteza Haghir Chehreghani</a>",
          "description": "Annotating the driving scenario trajectories based only on explicit rules\n(i.e., knowledge-based methods) can be subject to errors, such as false\npositive/negative classification of scenarios that lie on the border of two\nscenario classes, missing unknown scenario classes, and also anomalies. On the\nother side, verifying the labels by the annotators is not cost-efficient. For\nthis purpose, active learning (AL) could potentially improve the annotation\nprocedure by inclusion of an annotator/expert in an efficient way. In this\nstudy, we develop an active learning framework to annotate driving trajectory\ntime-series data. At the first step, we compute an embedding of the time-series\ntrajectories into a latent space in order to extract the temporal nature. For\nthis purpose, we study three different latent space representations:\nmultivariate Time Series t-Distributed Stochastic Neighbor Embedding (mTSNE),\nRecurrent Auto-Encoder (RAE) and Variational Recurrent Auto-Encoder (VRAE). We\nthen apply different active learning paradigms with different classification\nmodels to the embedded data. In particular, we study the two classifiers Neural\nNetwork (NN) and Support Vector Machines (SVM), with three active learning\nquery strategies (i.e., entropy, margin and random). In the following, we\nexplore the possibilities of the framework to discover unknown classes and\ndemonstrate how it can be used to identify the out-of-class trajectories.",
          "link": "http://arxiv.org/abs/2108.03217",
          "publishedOn": "2021-08-09T00:49:28.385Z",
          "wordCount": 640,
          "title": "Analysis of Driving Scenario Trajectories with Active Learning. (arXiv:2108.03217v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendoza_Cardenas_C/0/1/0/all/0/1\">Carlos H. Mendoza-Cardenas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockmeier_A/0/1/0/all/0/1\">Austin J. Brockmeier</a>",
          "description": "Seizure detection algorithms must discriminate abnormal neuronal activity\nassociated with a seizure from normal neural activity in a variety of\nconditions. Our approach is to seek spatiotemporal waveforms with distinct\nmorphology in electrocorticographic (ECoG) recordings of epileptic patients\nthat are indicative of a subsequent seizure (preictal) versus non-seizure\nsegments (interictal). To find these waveforms we apply a shift-invariant\nk-means algorithm to segments of spatially filtered signals to learn codebooks\nof prototypical waveforms. The frequency of the cluster labels from the\ncodebooks is then used to train a binary classifier that predicts the class\n(preictal or interictal) of a test ECoG segment. We use the Matthews\ncorrelation coefficient to evaluate the performance of the classifier and the\nquality of the codebooks. We found that our method finds recurrent\nnon-sinusoidal waveforms that could be used to build interpretable features for\nseizure prediction and that are also physiologically meaningful.",
          "link": "http://arxiv.org/abs/2108.03177",
          "publishedOn": "2021-08-09T00:49:28.378Z",
          "wordCount": 595,
          "title": "Shift-invariant waveform learning on epileptic ECoG. (arXiv:2108.03177v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yunxia Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+chen_S/0/1/0/all/0/1\">Songcan chen</a>",
          "description": "Euler k-means (EulerK) first maps data onto the unit hyper-sphere surface of\nequi-dimensional space via a complex mapping which induces the robust Euler\nkernel and next employs the popular $k$-means. Consequently, besides enjoying\nthe virtues of k-means such as simplicity and scalability to large data sets,\nEulerK is also robust to noises and outliers. Although so, the centroids\ncaptured by EulerK deviate from the unit hyper-sphere surface and thus in\nstrict distributional sense, actually are outliers. This weird phenomenon also\noccurs in some generic kernel clustering methods. Intuitively, using such\noutlier-like centroids should not be quite reasonable but it is still seldom\nattended. To eliminate the deviation, we propose two Rectified Euler k-means\nmethods, i.e., REK1 and REK2, which retain the merits of EulerK while acquire\nreal centroids residing on the mapped space to better characterize the data\nstructures. Specifically, REK1 rectifies EulerK by imposing the constraint on\nthe centroids while REK2 views each centroid as the mapped image from a\npre-image in the original space and optimizes these pre-images in Euler kernel\ninduced space. Undoubtedly, our proposed REKs can methodologically be extended\nto solve problems of such a category. Finally, the experiments validate the\neffectiveness of REK1 and REK2.",
          "link": "http://arxiv.org/abs/2108.03081",
          "publishedOn": "2021-08-09T00:49:28.371Z",
          "wordCount": 619,
          "title": "Rectified Euler k-means and Beyond. (arXiv:2108.03081v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiangqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Quan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhihua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Tangzhi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojuan Ma</a>",
          "description": "Bank credit rating classifies banks into different levels based on publicly\ndisclosed and internal information, serving as an important input in financial\nrisk management. However, domain experts have a vague idea of exploring and\ncomparing different bank credit rating schemes. A loose connection between\nsubjective and quantitative analysis and difficulties in determining\nappropriate indicator weights obscure understanding of bank credit ratings.\nFurthermore, existing models fail to consider bank types by just applying a\nunified indicator weight set to all banks. We propose RatingVis to assist\nexperts in exploring and comparing different bank credit rating schemes. It\nsupports interactively inferring indicator weights for banks by involving\ndomain knowledge and considers bank types in the analysis loop. We conduct a\ncase study with real-world bank data to verify the efficacy of RatingVis.\nExpert feedback suggests that our approach helps them better understand\ndifferent rating schemes.",
          "link": "http://arxiv.org/abs/2108.03011",
          "publishedOn": "2021-08-09T00:49:28.361Z",
          "wordCount": 588,
          "title": "Inspecting the Process of Bank Credit Rating via Visual Analytics. (arXiv:2108.03011v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02850",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ojha_R/0/1/0/all/0/1\">Rupam Ojha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sekhar_C/0/1/0/all/0/1\">C Chandra Sekhar</a>",
          "description": "Automatic speech recognition is a difficult problem in pattern recognition\nbecause several sources of variability exist in the speech input like the\nchannel variations, the input might be clean or noisy, the speakers may have\ndifferent accent and variations in the gender, etc. As a result, domain\nadaptation is important in speech recognition where we train the model for a\nparticular source domain and test it on a different target domain. In this\npaper, we propose a technique to perform unsupervised gender-based domain\nadaptation in speech recognition using phonetic features. The experiments are\nperformed on the TIMIT dataset and there is a considerable decrease in the\nphoneme error rate using the proposed approach.",
          "link": "http://arxiv.org/abs/2108.02850",
          "publishedOn": "2021-08-09T00:49:28.354Z",
          "wordCount": 561,
          "title": "Unsupervised Domain Adaptation in Speech Recognition using Phonetic Features. (arXiv:2108.02850v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02889",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoi Khac Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masaracchia_A/0/1/0/all/0/1\">Antonino Masaracchia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Do_Duy_T/0/1/0/all/0/1\">Tan Do-Duy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Poor_H/0/1/0/all/0/1\">H. Vincent Poor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duong_T/0/1/0/all/0/1\">Trung Q. Duong</a>",
          "description": "Many of the devices used in Internet-of-Things (IoT) applications are\nenergy-limited, and thus supplying energy while maintaining seamless\nconnectivity for IoT devices is of considerable importance. In this context, we\npropose a simultaneous wireless power transfer and information transmission\nscheme for IoT devices with support from reconfigurable intelligent surface\n(RIS)-aided unmanned aerial vehicle (UAV) communications. In particular, in a\nfirst phase, IoT devices harvest energy from the UAV through wireless power\ntransfer; and then in a second phase, the UAV collects data from the IoT\ndevices through information transmission. To characterise the agility of the\nUAV, we consider two scenarios: a hovering UAV and a mobile UAV. Aiming at\nmaximizing the total network sum-rate, we jointly optimize the trajectory of\nthe UAV, the energy harvesting scheduling of IoT devices, and the phaseshift\nmatrix of the RIS. We formulate a Markov decision process and propose two deep\nreinforcement learning algorithms to solve the optimization problem of\nmaximizing the total network sum-rate. Numerical results illustrate the\neffectiveness of the UAV's flying path optimization and the network's\nthroughput of our proposed techniques compared with other benchmark schemes.\nGiven the strict requirements of the RIS and UAV, the significant improvement\nin processing time and throughput performance demonstrates that our proposed\nscheme is well applicable for practical IoT applications.",
          "link": "http://arxiv.org/abs/2108.02889",
          "publishedOn": "2021-08-09T00:49:28.345Z",
          "wordCount": 687,
          "title": "RIS-assisted UAV Communications for IoT with Wireless Power Transfer Using Deep Reinforcement Learning. (arXiv:2108.02889v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kunho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopi_S/0/1/0/all/0/1\">Sivakanth Gopi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_J/0/1/0/all/0/1\">Janardhan Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yekhanin_S/0/1/0/all/0/1\">Sergey Yekhanin</a>",
          "description": "We revisit the problem of $n$-gram extraction in the differential privacy\nsetting. In this problem, given a corpus of private text data, the goal is to\nrelease as many $n$-grams as possible while preserving user level privacy.\nExtracting $n$-grams is a fundamental subroutine in many NLP applications such\nas sentence completion, response generation for emails etc. The problem also\narises in other applications such as sequence mining, and is a generalization\nof recently studied differentially private set union (DPSU). In this paper, we\ndevelop a new differentially private algorithm for this problem which, in our\nexperiments, significantly outperforms the state-of-the-art. Our improvements\nstem from combining recent advances in DPSU, privacy accounting, and new\nheuristics for pruning in the tree-based approach initiated by Chen et al.\n(2012).",
          "link": "http://arxiv.org/abs/2108.02831",
          "publishedOn": "2021-08-09T00:49:28.337Z",
          "wordCount": 558,
          "title": "Differentially Private n-gram Extraction. (arXiv:2108.02831v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jiaqian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yongxin Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>",
          "description": "Recently published graph neural networks (GNNs) show promising performance at\nsocial event detection tasks. However, most studies are oriented toward\nmonolingual data in languages with abundant training samples. This has left the\nmore common multilingual settings and lesser-spoken languages relatively\nunexplored. Thus, we present a GNN that incorporates cross-lingual word\nembeddings for detecting events in multilingual data streams. The first exploit\nis to make the GNN work with multilingual data. For this, we outline a\nconstruction strategy that aligns messages in different languages at both the\nnode and semantic levels. Relationships between messages are established by\nmerging entities that are the same but are referred to in different languages.\nNon-English message representations are converted into English semantic space\nvia the cross-lingual word embeddings. The resulting message graph is then\nuniformly encoded by a GNN model. In special cases where a lesser-spoken\nlanguage needs to be detected, a novel cross-lingual knowledge distillation\nframework, called CLKD, exploits prior knowledge learned from similar threads\nin English to make up for the paucity of annotated data. Experiments on both\nsynthetic and real-world datasets show the framework to be highly effective at\ndetection in both multilingual data and in languages where training samples are\nscarce.",
          "link": "http://arxiv.org/abs/2108.03084",
          "publishedOn": "2021-08-09T00:49:28.330Z",
          "wordCount": 639,
          "title": "Transferring Knowledge Distillation for Multilingual Social Event Detection. (arXiv:2108.03084v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Regehr_M/0/1/0/all/0/1\">Matthew T. Regehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayoub_A/0/1/0/all/0/1\">Alex Ayoub</a>",
          "description": "Watkins' and Dayan's Q-learning is a model-free reinforcement learning\nalgorithm that iteratively refines an estimate for the optimal action-value\nfunction of an MDP by stochastically \"visiting\" many state-ation pairs [Watkins\nand Dayan, 1992]. Variants of the algorithm lie at the heart of numerous recent\nstate-of-the-art achievements in reinforcement learning, including the\nsuperhuman Atari-playing deep Q-network [Mnih et al., 2015]. The goal of this\npaper is to reproduce a precise and (nearly) self-contained proof that\nQ-learning converges. Much of the available literature leverages powerful\ntheory to obtain highly generalizable results in this vein. However, this\napproach requires the reader to be familiar with and make many deep connections\nto different research areas. A student seeking to deepen their understand of\nQ-learning risks becoming caught in a vicious cycle of \"RL-learning Hell\". For\nthis reason, we give a complete proof from start to finish using only one\nexternal result from the field of stochastic approximation, despite the fact\nthat this minimal dependence on other results comes at the expense of some\n\"shininess\".",
          "link": "http://arxiv.org/abs/2108.02827",
          "publishedOn": "2021-08-09T00:49:28.310Z",
          "wordCount": 597,
          "title": "An Elementary Proof that Q-learning Converges Almost Surely. (arXiv:2108.02827v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Orts_O/0/1/0/all/0/1\">&#xd2;scar Garibo i Orts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_March_M/0/1/0/all/0/1\">Miguel A. Garcia-March</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conejero_J/0/1/0/all/0/1\">J. Alberto Conejero</a>",
          "description": "Anomalous diffusion occurs at very different scales in nature, from atomic\nsystems to motions in cell organelles, biological tissues or ecology, and also\nin artificial materials, such as cement. Being able to accurately measure the\nanomalous exponent associated with a given particle trajectory, thus\ndetermining whether the particle subdiffuses, superdiffuses or performs normal\ndiffusion is of key importance to understand the diffusion process. Also, it is\noften important to trustingly identify the model behind the trajectory, as this\ngives a large amount of information on the system dynamics. Both aspects are\nparticularly difficult when the input data are short and noisy trajectories. It\nis even more difficult if one cannot guarantee that the trajectories output in\nexperiments is homogeneous, hindering the statistical methods based on\nensembles of trajectories. We present a data-driven method able to infer the\nanomalous exponent and to identify the type of anomalous diffusion process\nbehind single, noisy and short trajectories, with good accuracy. This model was\nused in our participation in the Anomalous Diffusion (AnDi) Challenge. A\ncombination of convolutional and recurrent neural networks were used to achieve\nstate-of-the-art results when compared to methods participating in the AnDi\nChallenge, ranking top 4 in both classification and diffusion exponent\nregression.",
          "link": "http://arxiv.org/abs/2108.02834",
          "publishedOn": "2021-08-09T00:49:28.302Z",
          "wordCount": 654,
          "title": "Efficient recurrent neural network methods for anomalously diffusing single particle short and noisy trajectories. (arXiv:2108.02834v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03090",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bartolomaeus_W/0/1/0/all/0/1\">Wiebke Bartolomaeus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Boutaib_Y/0/1/0/all/0/1\">Youness Boutaib</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nestler_S/0/1/0/all/0/1\">Sandra Nestler</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rauhut_H/0/1/0/all/0/1\">Holger Rauhut</a>",
          "description": "We investigate the functioning of a classifying biological neural network\nfrom the perspective of statistical learning theory, modelled, in a simplified\nsetting, as a continuous-time stochastic recurrent neural network (RNN) with\nidentity activation function. In the purely stochastic (robust) regime, we give\na generalisation error bound that holds with high probability, thus showing\nthat the empirical risk minimiser is the best-in-class hypothesis. We show that\nRNNs retain a partial signature of the paths they are fed as the unique\ninformation exploited for training and classification tasks. We argue that\nthese RNNs are easy to train and robust and back these observations with\nnumerical experiments on both synthetic and real data. We also exhibit a\ntrade-off phenomenon between accuracy and robustness.",
          "link": "http://arxiv.org/abs/2108.03090",
          "publishedOn": "2021-08-09T00:49:28.288Z",
          "wordCount": 570,
          "title": "Path classification by stochastic linear recurrent neural networks. (arXiv:2108.03090v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cheng-Ju Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>",
          "description": "Human-robot collaboration is an essential research topic in artificial\nintelligence (AI), enabling researchers to devise cognitive AI systems and\naffords an intuitive means for users to interact with the robot. Of note,\ncommunication plays a central role. To date, prior studies in embodied agent\nnavigation have only demonstrated that human languages facilitate communication\nby instructions in natural languages. Nevertheless, a plethora of other forms\nof communication is left unexplored. In fact, human communication originated in\ngestures and oftentimes is delivered through multimodal cues, e.g. \"go there\"\nwith a pointing gesture. To bridge the gap and fill in the missing dimension of\ncommunication in embodied agent navigation, we propose investigating the\neffects of using gestures as the communicative interface instead of verbal\ncues. Specifically, we develop a VR-based 3D simulation environment, named\nGes-THOR, based on AI2-THOR platform. In this virtual environment, a human\nplayer is placed in the same virtual scene and shepherds the artificial agent\nusing only gestures. The agent is tasked to solve the navigation problem guided\nby natural gestures with unknown semantics; we do not use any predefined\ngestures due to the diversity and versatile nature of human gestures. We argue\nthat learning the semantics of natural gestures is mutually beneficial to\nlearning the navigation task--learn to communicate and communicate to learn. In\na series of experiments, we demonstrate that human gesture cues, even without\npredefined semantics, improve the object-goal navigation for an embodied agent,\noutperforming various state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.02846",
          "publishedOn": "2021-08-09T00:49:28.272Z",
          "wordCount": 701,
          "title": "Communicative Learning with Natural Gestures for Embodied Navigation Agents with Human-in-the-Scene. (arXiv:2108.02846v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1\">Sim&#xf3;n C. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthy_S/0/1/0/all/0/1\">Subramanian Ramamoorthy</a>",
          "description": "Understanding a controller's performance in different scenarios is crucial\nfor robots that are going to be deployed in safety-critical tasks. If we do not\nhave a model of the dynamics of the world, which is often the case in complex\ndomains, we may need to approximate a performance function of the robot based\non its interaction with the environment. Such a performance function gives us\ninsights into the behaviour of the robot, allowing us to fine-tune the\ncontroller with manual interventions. In high-dimensionality systems, where the\nactionstate space is large, fine-tuning a controller is non-trivial. To\novercome this problem, we propose a performance function whose domain is\ndefined by external features and parameters of the controller. Attainment\nregions are defined over such a domain defined by feature-parameter pairs, and\nserve the purpose of enabling prediction of successful execution of the task.\nThe use of the feature-parameter space -in contrast to the action-state space-\nallows us to adapt, explain and finetune the controller over a simpler (i.e.,\nlower dimensional space). When the robot successfully executes the task, we use\nthe attainment regions to gain insights into the limits of the controller, and\nits robustness. When the robot fails to execute the task, we use the regions to\ndebug the controller and find adaptive and counterfactual changes to the\nsolutions. Another advantage of this approach is that we can generalise through\nthe use of Gaussian processes regression of the performance function in the\nhigh-dimensional space. To test our approach, we demonstrate learning an\napproximation to the performance function in simulation, with a mobile robot\ntraversing different terrain conditions. Then, with a sample-efficient method,\nwe propagate the attainment regions to a physical robot in a similar\nenvironment.",
          "link": "http://arxiv.org/abs/2108.03150",
          "publishedOn": "2021-08-09T00:49:28.258Z",
          "wordCount": 743,
          "title": "Attainment Regions in Feature-Parameter Space for High-Level Debugging in Autonomous Robots. (arXiv:2108.03150v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fiedler_J/0/1/0/all/0/1\">James Fiedler</a>",
          "description": "There is growing interest in neural network architectures for tabular data.\nMany general-purpose tabular deep learning models have been introduced\nrecently, with performance sometimes rivaling gradient boosted decision trees\n(GBDTs). These recent models draw inspiration from various sources, including\nGBDTs, factorization machines, and neural networks from other application\ndomains. Previous tabular neural networks are also drawn upon, but are possibly\nunder-considered, especially models associated with specific tabular problems.\nThis paper focuses on several such models, and proposes modifications for\nimproving their performance. When modified, these models are shown to be\ncompetitive with leading general-purpose tabular models, including GBDTs.",
          "link": "http://arxiv.org/abs/2108.03214",
          "publishedOn": "2021-08-09T00:49:28.237Z",
          "wordCount": 520,
          "title": "Simple Modifications to Improve Tabular Neural Networks. (arXiv:2108.03214v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03166",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Rashid_N/0/1/0/all/0/1\">Nafiul Rashid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Luke Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dautta_M/0/1/0/all/0/1\">Manik Dautta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jimenez_A/0/1/0/all/0/1\">Abel Jimenez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tseng_P/0/1/0/all/0/1\">Peter Tseng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Faruque_M/0/1/0/all/0/1\">Mohammad Abdullah Al Faruque</a>",
          "description": "Stress is a physiological state that hampers mental health and has serious\nconsequences to physical health. Moreover, the COVID-19 pandemic has increased\nstress levels among people across the globe. Therefore, continuous monitoring\nand detection of stress are necessary. The recent advances in wearable devices\nhave allowed the monitoring of several physiological signals related to stress.\nAmong them, wrist-worn wearable devices like smartwatches are most popular due\nto their convenient usage. And the photoplethysmography (PPG) sensor is the\nmost prevalent sensor in almost all consumer-grade wrist-worn smartwatches.\nTherefore, this paper focuses on using a wrist-based PPG sensor that collects\nBlood Volume Pulse (BVP) signals to detect stress which may be applicable for\nconsumer-grade wristwatches. Moreover, state-of-the-art works have used either\nclassical machine learning algorithms to detect stress using hand-crafted\nfeatures or have used deep learning algorithms like Convolutional Neural\nNetwork (CNN) which automatically extracts features. This paper proposes a\nnovel hybrid CNN (H-CNN) classifier that uses both the hand-crafted features\nand the automatically extracted features by CNN to detect stress using the BVP\nsignal. Evaluation on the benchmark WESAD dataset shows that, for 3-class\nclassification (Baseline vs. Stress vs. Amusement), our proposed H-CNN\noutperforms traditional classifiers and normal CNN by 5% and 7% accuracy, and\n10% and 7% macro F1 score, respectively. Also for 2-class classification\n(Stress vs. Non-stress), our proposed H-CNN outperforms traditional classifiers\nand normal CNN by 3% and ~5% accuracy, and ~3% and ~7% macro F1 score,\nrespectively.",
          "link": "http://arxiv.org/abs/2108.03166",
          "publishedOn": "2021-08-09T00:49:28.230Z",
          "wordCount": 759,
          "title": "Feature Augmented Hybrid CNN for Stress Recognition Using Wrist-based Photoplethysmography Sensor. (arXiv:2108.03166v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02883",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Donhauser_K/0/1/0/all/0/1\">Konstantin Donhauser</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tifrea_A/0/1/0/all/0/1\">Alexandru &#x162;ifrea</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Aerni_M/0/1/0/all/0/1\">Michael Aerni</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Heckel_R/0/1/0/all/0/1\">Reinhard Heckel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_F/0/1/0/all/0/1\">Fanny Yang</a>",
          "description": "Numerous recent works show that overparameterization implicitly reduces\nvariance for min-norm interpolators and max-margin classifiers. These findings\nsuggest that ridge regularization has vanishing benefits in high dimensions. We\nchallenge this narrative by showing that, even in the absence of noise,\navoiding interpolation through ridge regularization can significantly improve\ngeneralization. We prove this phenomenon for the robust risk of both linear\nregression and classification and hence provide the first theoretical result on\nrobust overfitting.",
          "link": "http://arxiv.org/abs/2108.02883",
          "publishedOn": "2021-08-09T00:49:28.209Z",
          "wordCount": 515,
          "title": "Interpolation can hurt robust generalization even when there is no noise. (arXiv:2108.02883v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tuxworth_D/0/1/0/all/0/1\">David Tuxworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antypas_D/0/1/0/all/0/1\">Dimosthenis Antypas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1\">Luis Espinosa-Anke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preece_A/0/1/0/all/0/1\">Alun Preece</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_D/0/1/0/all/0/1\">David Rogers</a>",
          "description": "This paper demonstrates a two-stage method for deriving insights from social\nmedia data relating to disinformation by applying a combination of geospatial\nclassification and embedding-based language modelling across multiple\nlanguages. In particular, the analysis in centered on Twitter and\ndisinformation for three European languages: English, French and Spanish.\nFirstly, Twitter data is classified into European and non-European sets using\nBERT. Secondly, Word2vec is applied to the classified texts resulting in\nEurocentric, non-Eurocentric and global representations of the data for the\nthree target languages. This comparative analysis demonstrates not only the\nefficacy of the classification method but also highlights geographic, temporal\nand linguistic differences in the disinformation-related media. Thus, the\ncontributions of the work are threefold: (i) a novel language-independent\ntransformer-based geolocation method; (ii) an analytical approach that exploits\nlexical specificity and word embeddings to interrogate user-generated content;\nand (iii) a dataset of 36 million disinformation related tweets in English,\nFrench and Spanish.",
          "link": "http://arxiv.org/abs/2108.03067",
          "publishedOn": "2021-08-09T00:49:28.201Z",
          "wordCount": 604,
          "title": "Deriving Disinformation Insights from Geolocalized Twitter Callouts. (arXiv:2108.03067v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haijian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Rose Qingyang Hu</a>",
          "description": "A new machine learning (ML) technique termed as federated learning (FL) aims\nto preserve data at the edge devices and to only exchange ML model parameters\nin the learning process. FL not only reduces the communication needs but also\nhelps to protect the local privacy. Although FL has these advantages, it can\nstill experience large communication latency when there are massive edge\ndevices connected to the central parameter server (PS) and/or millions of model\nparameters involved in the learning process. Over-the-air computation (AirComp)\nis capable of computing while transmitting data by allowing multiple devices to\nsend data simultaneously by using analog modulation. To achieve good\nperformance in FL through AirComp, user scheduling plays a critical role. In\nthis paper, we investigate and compare different user scheduling policies,\nwhich are based on various criteria such as wireless channel conditions and the\nsignificance of model updates. Receiver beamforming is applied to minimize the\nmean-square-error (MSE) of the distortion of function aggregation result via\nAirComp. Simulation results show that scheduling based on the significance of\nmodel updates has smaller fluctuations in the training process while scheduling\nbased on channel condition has the advantage on energy efficiency.",
          "link": "http://arxiv.org/abs/2108.02891",
          "publishedOn": "2021-08-09T00:49:28.181Z",
          "wordCount": 633,
          "title": "User Scheduling for Federated Learning Through Over-the-Air Computation. (arXiv:2108.02891v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sihwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1\">Dae Yon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1\">Taejang Park</a>",
          "description": "The merit of ensemble learning lies in having different outputs from many\nindividual models on a single input, i.e., the diversity of the base models.\nThe high quality of diversity can be achieved when each model is specialized to\ndifferent subsets of the whole dataset. Moreover, when each model explicitly\nknows to which subsets it is specialized, more opportunities arise to improve\ndiversity. In this paper, we propose an advanced ensemble method, called\nAuxiliary class based Multiple Choice Learning (AMCL), to ultimately specialize\neach model under the framework of multiple choice learning (MCL). The\nadvancement of AMCL is originated from three novel techniques which control the\nframework from different directions: 1) the concept of auxiliary class to\nprovide more distinct information through the labels, 2) the strategy, named\nmemory-based assignment, to determine the association between the inputs and\nthe models, and 3) the feature fusion module to achieve generalized features.\nTo demonstrate the performance of our method compared to all variants of MCL\nmethods, we conduct extensive experiments on the image classification and\nsegmentation tasks. Overall, the performance of AMCL exceeds all others in most\nof the public datasets trained with various networks as members of the\nensembles.",
          "link": "http://arxiv.org/abs/2108.02949",
          "publishedOn": "2021-08-09T00:49:28.174Z",
          "wordCount": 623,
          "title": "Auxiliary Class Based Multiple Choice Learning. (arXiv:2108.02949v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02867",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1\">Shagufta Henna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalliadan_S/0/1/0/all/0/1\">Shyam Krishnan Kalliadan</a>",
          "description": "In a business-to-business (B2B) customer relationship management (CRM) use\ncase, each client is a potential business organization/company with a solid\nbusiness strategy and focused and rational decisions. This paper introduces a\ngraph-based analytics approach to improve CRM within a B2B environment. In our\napproach, in the first instance, we have designed a graph database using the\nNeo4j platform. Secondly, the graph database has been investigated by using\ndata mining and exploratory analysis coupled with cypher graph query language.\nSpecifically, we have applied the graph convolution network (GCN) to enable CRM\nanalytics to forecast sales. This is the first step towards a GCN-based binary\nclassification based on graph databases in the domain of B2B CRM. We evaluate\nthe performance of the proposed GCN model on graph databases and compare it\nwith Random Forest (RF), Convolutional Neural Network (CNN), and Artificial\nNeural Network (ANN). The proposed GCN approach is further augmented with the\nshortest path and eigenvector centrality attribute to significantly improve the\naccuracy of sales prediction. Experimental results reveal that the proposed\ngraph-based deep learning approach outperforms the Random Forests (RsF) and two\ndeep learning models, i.e., CNN and ANN under different combinations of graph\nfeatures.",
          "link": "http://arxiv.org/abs/2108.02867",
          "publishedOn": "2021-08-09T00:49:28.168Z",
          "wordCount": 623,
          "title": "Enterprise Analytics using Graph Database and Graph-based Deep Learning. (arXiv:2108.02867v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Shuvendu Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>",
          "description": "We propose a self-supervised contrastive learning approach for facial\nexpression recognition (FER) in videos. We propose a novel temporal\nsampling-based augmentation scheme to be utilized in addition to standard\nspatial augmentations used for contrastive learning. Our proposed temporal\naugmentation scheme randomly picks from one of three temporal sampling\ntechniques: (1) pure random sampling, (2) uniform sampling, and (3) sequential\nsampling. This is followed by a combination of up to three standard spatial\naugmentations. We then use a deep R(2+1)D network for FER, which we train in a\nself-supervised fashion based on the augmentations and subsequently fine-tune.\nExperiments are performed on the Oulu-CASIA dataset and the performance is\ncompared to other works in FER. The results indicate that our method achieves\nan accuracy of 89.4%, setting a new state-of-the-art by outperforming other\nworks. Additional experiments and analysis confirm the considerable\ncontribution of the proposed temporal augmentation versus the existing spatial\nones.",
          "link": "http://arxiv.org/abs/2108.03064",
          "publishedOn": "2021-08-09T00:49:28.161Z",
          "wordCount": 599,
          "title": "Spatiotemporal Contrastive Learning of Facial Expressions in Videos. (arXiv:2108.03064v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Seonguk Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joon-Young Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bohyung Han</a>",
          "description": "Dataset bias is a critical challenge in machine learning, and its negative\nimpact is aggravated when models capture unintended decision rules with\nspurious correlations. Although existing works often handle this issue using\nhuman supervision, the availability of the proper annotations is impractical\nand even unrealistic. To better tackle this challenge, we propose a simple but\neffective debiasing technique in an unsupervised manner. Specifically, we\nperform clustering on the feature embedding space and identify pseudoattributes\nby taking advantage of the clustering results even without an explicit\nattribute supervision. Then, we employ a novel cluster-based reweighting scheme\nfor learning debiased representation; this prevents minority groups from being\ndiscounted for minimizing the overall loss, which is desirable for worst-case\ngeneralization. The extensive experiments demonstrate the outstanding\nperformance of our approach on multiple standard benchmarks, which is even as\ncompetitive as the supervised counterpart.",
          "link": "http://arxiv.org/abs/2108.02943",
          "publishedOn": "2021-08-09T00:49:28.153Z",
          "wordCount": 565,
          "title": "Unsupervised Learning of Debiased Representations with Pseudo-Attributes. (arXiv:2108.02943v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02922",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kenny Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1\">Arunesh Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1\">Arvind Narayanan</a>",
          "description": "Concerns about privacy, bias, and harmful applications have shone a light on\nthe ethics of machine learning datasets, even leading to the retraction of\nprominent datasets including DukeMTMC, MS-Celeb-1M, TinyImages, and VGGFace2.\nIn response, the machine learning community has called for higher ethical\nstandards, transparency efforts, and technical fixes in the dataset creation\nprocess. The premise of our work is that these efforts can be more effective if\ninformed by an understanding of how datasets are used in practice in the\nresearch community. We study three influential face and person recognition\ndatasets - DukeMTMC, MS-Celeb-1M, and Labeled Faces in the Wild (LFW) - by\nanalyzing nearly 1000 papers that cite them. We found that the creation of\nderivative datasets and models, broader technological and social change, the\nlack of clarity of licenses, and dataset management practices can introduce a\nwide range of ethical concerns. We conclude by suggesting a distributed\napproach that can mitigate these harms, making recommendations to dataset\ncreators, conference program committees, dataset users, and the broader\nresearch community.",
          "link": "http://arxiv.org/abs/2108.02922",
          "publishedOn": "2021-08-09T00:49:28.130Z",
          "wordCount": 605,
          "title": "Mitigating dataset harms requires stewardship: Lessons from 1000 papers. (arXiv:2108.02922v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1\">Shagufta Henna</a>",
          "description": "Since the rapid outbreak of Covid-19 and with no approved vaccines to date,\nprofound research interest has emerged to understand the innate immune response\nto viruses. This understanding can help to inhibit virus replication, prolong\nadaptive immune response, accelerated virus clearance, and tissue recovery, a\nkey milestone to propose a vaccine to combat coronaviruses (CoVs), e.g.,\nCovid-19. Although an innate immune system triggers inflammatory responses\nagainst CoVs upon recognition of viruses, however, a vaccine is the ultimate\nprotection against CoV spread. The development of this vaccine is\ntime-consuming and requires a deep understanding of the innate immune response\nsystem. In this work, we propose a graph neural network-based model that\nexploits the interactions between pattern recognition receptors (PRRs), i.e.,\nthe human immune response system. These interactions can help to recognize\npathogen-associated molecular patterns (PAMPs) to predict the activation\nrequirements of each PRR. The immune response information of each PRR is\nderived from combining its historical PAMPs activation coupled with the modeled\neffect on the same from PRRs in its neighborhood. On one hand, this work can\nhelp to understand how long Covid-19 can confer immunity where a strong immune\nresponse means people already been infected can safely return to work. On the\nother hand, this GNN-based understanding can also abode well for vaccine\ndevelopment efforts. Our proposal has been evaluated using CoVs immune response\ndataset, with results showing an average IFNs activation prediction accuracy of\n90%, compared to 85% using feed-forward neural networks.",
          "link": "http://arxiv.org/abs/2108.02872",
          "publishedOn": "2021-08-09T00:49:28.108Z",
          "wordCount": 715,
          "title": "Understanding Human Innate Immune System Dependencies using Graph Neural Networks. (arXiv:2108.02872v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wet_H/0/1/0/all/0/1\">Harm de Wet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>",
          "description": "Disinformation through fake news is an ongoing problem in our society and has\nbecome easily spread through social media. The most cost and time effective way\nto filter these large amounts of data is to use a combination of human and\ntechnical interventions to identify it. From a technical perspective, Natural\nLanguage Processing (NLP) is widely used in detecting fake news. Social media\ncompanies use NLP techniques to identify the fake news and warn their users,\nbut fake news may still slip through undetected. It is especially a problem in\nmore localised contexts (outside the United States of America). How do we\nadjust fake news detection systems to work better for local contexts such as in\nSouth Africa. In this work we investigate fake news detection on South African\nwebsites. We curate a dataset of South African fake news and then train\ndetection models. We contrast this with using widely available fake news\ndatasets (from mostly USA website). We also explore making the datasets more\ndiverse by combining them and observe the differences in behaviour in writing\nbetween nations' fake news using interpretable machine learning.",
          "link": "http://arxiv.org/abs/2108.02941",
          "publishedOn": "2021-08-09T00:49:28.092Z",
          "wordCount": 640,
          "title": "Is it Fake? News Disinformation Detection on South African News Websites. (arXiv:2108.02941v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arango_S/0/1/0/all/0/1\">Sebastian Pineda Arango</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinrich_F/0/1/0/all/0/1\">Felix Heinrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhusudhanan_K/0/1/0/all/0/1\">Kiran Madhusudhanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1\">Lars Schmidt-Thieme</a>",
          "description": "Recent work has shown the efficiency of deep learning models such as Fully\nConvolutional Networks (FCN) or Recurrent Neural Networks (RNN) to deal with\nTime Series Regression (TSR) problems. These models sometimes need a lot of\ndata to be able to generalize, yet the time series are sometimes not long\nenough to be able to learn patterns. Therefore, it is important to make use of\ninformation across time series to improve learning. In this paper, we will\nexplore the idea of using meta-learning for quickly adapting model parameters\nto new short-history time series by modifying the original idea of Model\nAgnostic Meta-Learning (MAML) \\cite{finn2017model}. Moreover, based on prior\nwork on multimodal MAML \\cite{vuorio2019multimodal}, we propose a method for\nconditioning parameters of the model through an auxiliary network that encodes\nglobal information of the time series to extract meta-features. Finally, we\napply the data to time series of different domains, such as pollution\nmeasurements, heart-rate sensors, and electrical battery data. We show\nempirically that our proposed meta-learning method learns TSR with few data\nfast and outperforms the baselines in 9 of 12 experiments.",
          "link": "http://arxiv.org/abs/2108.02842",
          "publishedOn": "2021-08-09T00:49:28.085Z",
          "wordCount": 612,
          "title": "Multimodal Meta-Learning for Time Series Regression. (arXiv:2108.02842v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Remil_Y/0/1/0/all/0/1\">Youcef Remil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendimerad_A/0/1/0/all/0/1\">Anes Bendimerad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plantevit_M/0/1/0/all/0/1\">Marc Plantevit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robardet_C/0/1/0/all/0/1\">C&#xe9;line Robardet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaytoue_M/0/1/0/all/0/1\">Mehdi Kaytoue</a>",
          "description": "The need of predictive maintenance comes with an increasing number of\nincidents reported by monitoring systems and equipment/software users. In the\nfront line, on-call engineers (OCEs) have to quickly assess the degree of\nseverity of an incident and decide which service to contact for corrective\nactions. To automate these decisions, several predictive models have been\nproposed, but the most efficient models are opaque (say, black box), strongly\nlimiting their adoption. In this paper, we propose an efficient black box model\nbased on 170K incidents reported to our company over the last 7 years and\nemphasize on the need of automating triage when incidents are massively\nreported on thousands of servers running our product, an ERP. Recent\ndevelopments in eXplainable Artificial Intelligence (XAI) help in providing\nglobal explanations to the model, but also, and most importantly, with local\nexplanations for each model prediction/outcome. Sadly, providing a human with\nan explanation for each outcome is not conceivable when dealing with an\nimportant number of daily predictions. To address this problem, we propose an\noriginal data-mining method rooted in Subgroup Discovery, a pattern mining\ntechnique with the natural ability to group objects that share similar\nexplanations of their black box predictions and provide a description for each\ngroup. We evaluate this approach and present our preliminary results which give\nus good hope towards an effective OCE's adoption. We believe that this approach\nprovides a new way to address the problem of model agnostic outcome\nexplanation.",
          "link": "http://arxiv.org/abs/2108.03013",
          "publishedOn": "2021-08-09T00:49:28.078Z",
          "wordCount": 686,
          "title": "Interpretable Summaries of Black Box Incident Triaging with Subgroup Discovery. (arXiv:2108.03013v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02892",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoi Khac Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masaracchia_A/0/1/0/all/0/1\">Antonino Masaracchia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yin_C/0/1/0/all/0/1\">Cheng Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_L/0/1/0/all/0/1\">Long D. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dobre_O/0/1/0/all/0/1\">Octavia A. Dobre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duong_T/0/1/0/all/0/1\">Trung Q. Duong</a>",
          "description": "In this paper, we propose a deep reinforcement learning (DRL) approach for\nsolving the optimisation problem of the network's sum-rate in device-to-device\n(D2D) communications supported by an intelligent reflecting surface (IRS). The\nIRS is deployed to mitigate the interference and enhance the signal between the\nD2D transmitter and the associated D2D receiver. Our objective is to jointly\noptimise the transmit power at the D2D transmitter and the phase shift matrix\nat the IRS to maximise the network sum-rate. We formulate a Markov decision\nprocess and then propose the proximal policy optimisation for solving the\nmaximisation game. Simulation results show impressive performance in terms of\nthe achievable rate and processing time.",
          "link": "http://arxiv.org/abs/2108.02892",
          "publishedOn": "2021-08-09T00:49:28.057Z",
          "wordCount": 581,
          "title": "Deep Reinforcement Learning for Intelligent Reflecting Surface-assisted D2D Communications. (arXiv:2108.02892v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadreddin_A/0/1/0/all/0/1\">Armin Sadreddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadaoui_S/0/1/0/all/0/1\">Samira Sadaoui</a>",
          "description": "This study addresses the actual behavior of the credit-card fraud detection\nenvironment where financial transactions containing sensitive data must not be\namassed in an enormous amount to conduct learning. We introduce a new adaptive\nlearning approach that adjusts frequently and efficiently to new transaction\nchunks; each chunk is discarded after each incremental training step. Our\napproach combines transfer learning and incremental feature learning. The\nformer improves the feature relevancy for subsequent chunks, and the latter, a\nnew paradigm, increases accuracy during training by determining the optimal\nnetwork architecture dynamically for each new chunk. The architectures of past\nincremental approaches are fixed; thus, the accuracy may not improve with new\nchunks. We show the effectiveness and superiority of our approach\nexperimentally on an actual fraud dataset.",
          "link": "http://arxiv.org/abs/2108.02932",
          "publishedOn": "2021-08-09T00:49:28.037Z",
          "wordCount": 547,
          "title": "Incremental Feature Learning For Infinite Data. (arXiv:2108.02932v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02853",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Karatas_T/0/1/0/all/0/1\">Tugce Karatas</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Klinkert_F/0/1/0/all/0/1\">Federico Klinkert</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Hirsa_A/0/1/0/all/0/1\">Ali Hirsa</a>",
          "description": "Institutional investors have been increasing the allocation of the illiquid\nalternative assets such as private equity funds in their portfolios, yet there\nexists a very limited literature on cash flow forecasting of illiquid\nalternative assets. The net cash flow of private equity funds typically follow\na J-curve pattern, however the timing and the size of the contributions and\ndistributions depend on the investment opportunities. In this paper, we develop\na benchmark model and present two novel approaches (direct vs. indirect) to\npredict the cash flows of private equity funds. We introduce a sliding window\napproach to apply on our cash flow data because different vintage year funds\ncontain different lengths of cash flow information. We then pass the data to an\nLSTM/ GRU model to predict the future cash flows either directly or indirectly\n(based on the benchmark model). We further integrate macroeconomic indicators\ninto our data, which allows us to consider the impact of market environment on\ncash flows and to apply stress testing. Our results indicate that the direct\nmodel is easier to implement compared to the benchmark model and the indirect\nmodel, but still the predicted cash flows align better with the actual cash\nflows. We also show that macroeconomic variables improve the performance of the\ndirect model whereas the impact is not obvious on the indirect model.",
          "link": "http://arxiv.org/abs/2108.02853",
          "publishedOn": "2021-08-09T00:49:28.021Z",
          "wordCount": 664,
          "title": "Supervised Neural Networks for Illiquid Alternative Asset Cash Flow Forecasting. (arXiv:2108.02853v1 [q-fin.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupte_A/0/1/0/all/0/1\">Amit Gupte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romanov_A/0/1/0/all/0/1\">Alexey Romanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mantravadi_S/0/1/0/all/0/1\">Sahitya Mantravadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banda_D/0/1/0/all/0/1\">Dalitso Banda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1\">Raza Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meenal_L/0/1/0/all/0/1\">Lakshmanan Ramu Meenal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Benjamin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Soundar Srinivasan</a>",
          "description": "Document digitization is essential for the digital transformation of our\nsocieties, yet a crucial step in the process, Optical Character Recognition\n(OCR), is still not perfect. Even commercial OCR systems can produce\nquestionable output depending on the fidelity of the scanned documents. In this\npaper, we demonstrate an effective framework for mitigating OCR errors for any\ndownstream NLP task, using Named Entity Recognition (NER) as an example. We\nfirst address the data scarcity problem for model training by constructing a\ndocument synthesis pipeline, generating realistic but degraded data with NER\nlabels. We measure the NER accuracy drop at various degradation levels and show\nthat a text restoration model, trained on the degraded data, significantly\ncloses the NER accuracy gaps caused by OCR errors, including on an\nout-of-domain dataset. For the benefit of the community, we have made the\ndocument synthesis pipeline available as an open-source project.",
          "link": "http://arxiv.org/abs/2108.02899",
          "publishedOn": "2021-08-09T00:49:28.013Z",
          "wordCount": 621,
          "title": "Lights, Camera, Action! A Framework to Improve NLP Accuracy over OCR documents. (arXiv:2108.02899v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02817",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Floricel_C/0/1/0/all/0/1\">Carla Floricel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nipu_N/0/1/0/all/0/1\">Nafiul Nipu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biggs_M/0/1/0/all/0/1\">Mikayla Biggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wentzel_A/0/1/0/all/0/1\">Andrew Wentzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canahuate_G/0/1/0/all/0/1\">Guadalupe Canahuate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijk_L/0/1/0/all/0/1\">Lisanne Van Dijk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdallah Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuller_C/0/1/0/all/0/1\">C. David Fuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marai_G/0/1/0/all/0/1\">G. Elisabeta Marai</a>",
          "description": "Although cancer patients survive years after oncologic therapy, they are\nplagued with long-lasting or permanent residual symptoms, whose severity, rate\nof development, and resolution after treatment vary largely between survivors.\nThe analysis and interpretation of symptoms is complicated by their partial\nco-occurrence, variability across populations and across time, and, in the case\nof cancers that use radiotherapy, by further symptom dependency on the tumor\nlocation and prescribed treatment. We describe THALIS, an environment for\nvisual analysis and knowledge discovery from cancer therapy symptom data,\ndeveloped in close collaboration with oncology experts. Our approach leverages\nunsupervised machine learning methodology over cohorts of patients, and, in\nconjunction with custom visual encodings and interactions, provides context for\nnew patients based on patients with similar diagnostic features and symptom\nevolution. We evaluate this approach on data collected from a cohort of head\nand neck cancer patients. Feedback from our clinician collaborators indicates\nthat THALIS supports knowledge discovery beyond the limits of machines or\nhumans alone, and that it serves as a valuable tool in both the clinic and\nsymptom research.",
          "link": "http://arxiv.org/abs/2108.02817",
          "publishedOn": "2021-08-09T00:49:27.993Z",
          "wordCount": 630,
          "title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy. (arXiv:2108.02817v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Moin Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahzad_K/0/1/0/all/0/1\">Khurram Shahzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1\">Kamran Malik</a>",
          "description": "Hate speech is a specific type of controversial content that is widely\nlegislated as a crime that must be identified and blocked. However, due to the\nsheer volume and velocity of the Twitter data stream, hate speech detection\ncannot be performed manually. To address this issue, several studies have been\nconducted for hate speech detection in European languages, whereas little\nattention has been paid to low-resource South Asian languages, making the\nsocial media vulnerable for millions of users. In particular, to the best of\nour knowledge, no study has been conducted for hate speech detection in Roman\nUrdu text, which is widely used in the sub-continent. In this study, we have\nscrapped more than 90,000 tweets and manually parsed them to identify 5,000\nRoman Urdu tweets. Subsequently, we have employed an iterative approach to\ndevelop guidelines and used them for generating the Hate Speech Roman Urdu 2020\ncorpus. The tweets in the this corpus are classified at three levels:\nNeutral-Hostile, Simple-Complex, and Offensive-Hate speech. As another\ncontribution, we have used five supervised learning techniques, including a\ndeep learning technique, to evaluate and compare their effectiveness for hate\nspeech detection. The results show that Logistic Regression outperformed all\nother techniques, including deep learning techniques for the two levels of\nclassification, by achieved an F1 score of 0.906 for distinguishing between\nNeutral-Hostile tweets, and 0.756 for distinguishing between Offensive-Hate\nspeech tweets.",
          "link": "http://arxiv.org/abs/2108.02830",
          "publishedOn": "2021-08-09T00:49:27.972Z",
          "wordCount": 719,
          "title": "Hate Speech Detection in Roman Urdu. (arXiv:2108.02830v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02799",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Tiffany D. Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Seong Ioi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dylan S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMillian_M/0/1/0/all/0/1\">Matthew G. McMillian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMahan_R/0/1/0/all/0/1\">Ryan P. McMahan</a>",
          "description": "League of Legends (LoL) is the most widely played multiplayer online battle\narena (MOBA) game in the world. An important aspect of LoL is competitive\nranked play, which utilizes a skill-based matchmaking system to form fair\nteams. However, players' skill levels vary widely depending on which champion,\nor hero, that they choose to play as. In this paper, we propose a method for\npredicting game outcomes in ranked LoL games based on players' experience with\ntheir selected champion. Using a deep neural network, we found that game\noutcomes can be predicted with 75.1% accuracy after all players have selected\nchampions, which occurs before gameplay begins. Our results have important\nimplications for playing LoL and matchmaking. Firstly, individual champion\nskill plays a significant role in the outcome of a match, regardless of team\ncomposition. Secondly, even after the skill-based matchmaking, there is still a\nwide variance in team skill before gameplay begins. Finally, players should\nonly play champions that they have mastered, if they want to win games.",
          "link": "http://arxiv.org/abs/2108.02799",
          "publishedOn": "2021-08-09T00:49:27.867Z",
          "wordCount": 640,
          "title": "Using Machine Learning to Predict Game Outcomes Based on Player-Champion Experience in League of Legends. (arXiv:2108.02799v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenjie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhide Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Dong-Ling Deng</a>",
          "description": "Catastrophic forgetting describes the fact that machine learning models will\nlikely forget the knowledge of previously learned tasks after the learning\nprocess of a new one. It is a vital problem in the continual learning scenario\nand recently has attracted tremendous concern across different communities. In\nthis paper, we explore the catastrophic forgetting phenomena in the context of\nquantum machine learning. We find that, similar to those classical learning\nmodels based on neural networks, quantum learning systems likewise suffer from\nsuch forgetting problem in classification tasks emerging from various\napplication scenes. We show that based on the local geometrical information in\nthe loss function landscape of the trained model, a uniform strategy can be\nadapted to overcome the forgetting problem in the incremental learning setting.\nOur results uncover the catastrophic forgetting phenomena in quantum machine\nlearning and offer a practical method to overcome this problem, which opens a\nnew avenue for exploring potential quantum advantages towards continual\nlearning.",
          "link": "http://arxiv.org/abs/2108.02786",
          "publishedOn": "2021-08-09T00:49:27.841Z",
          "wordCount": 595,
          "title": "Quantum Continual Learning Overcoming Catastrophic Forgetting. (arXiv:2108.02786v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02811",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Ubaru_S/0/1/0/all/0/1\">Shashanka Ubaru</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Akhalwaya_I/0/1/0/all/0/1\">Ismail Yunus Akhalwaya</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Squillante_M/0/1/0/all/0/1\">Mark S. Squillante</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Clarkson_K/0/1/0/all/0/1\">Kenneth L. Clarkson</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Horesh_L/0/1/0/all/0/1\">Lior Horesh</a>",
          "description": "Quantum computing offers the potential of exponential speedups for certain\nclassical computations. Over the last decade, many quantum machine learning\n(QML) algorithms have been proposed as candidates for such exponential\nimprovements. However, two issues unravel the hope of exponential speedup for\nsome of these QML algorithms: the data-loading problem and, more recently, the\nstunning dequantization results of Tang et al. A third issue, namely the\nfault-tolerance requirements of most QML algorithms, has further hindered their\npractical realization. The quantum topological data analysis (QTDA) algorithm\nof Lloyd, Garnerone and Zanardi was one of the first QML algorithms that\nconvincingly offered an expected exponential speedup. From the outset, it did\nnot suffer from the data-loading problem. A recent result has also shown that\nthe generalized problem solved by this algorithm is likely classically\nintractable, and would therefore be immune to any dequantization efforts.\nHowever, the QTDA algorithm of Lloyd et~al. has a time complexity of\n$O(n^4/(\\epsilon^2 \\delta))$ (where $n$ is the number of data points,\n$\\epsilon$ is the error tolerance, and $\\delta$ is the smallest nonzero\neigenvalue of the restricted Laplacian) and requires fault-tolerant quantum\ncomputing, which has not yet been achieved. In this paper, we completely\noverhaul the QTDA algorithm to achieve an improved exponential speedup and\ndepth complexity of $O(n\\log(1/(\\delta\\epsilon)))$. Our approach includes three\nkey innovations: (a) an efficient realization of the combinatorial Laplacian as\na sum of Pauli operators; (b) a quantum rejection sampling approach to restrict\nthe superposition to the simplices in the complex; and (c) a stochastic rank\nestimation method to estimate the Betti numbers. We present a theoretical error\nanalysis, and the circuit and computational time and depth complexities for\nBetti number estimation.",
          "link": "http://arxiv.org/abs/2108.02811",
          "publishedOn": "2021-08-09T00:49:27.826Z",
          "wordCount": 729,
          "title": "Quantum Topological Data Analysis with Linear Depth and Exponential Speedup. (arXiv:2108.02811v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Webb_E/0/1/0/all/0/1\">E. William Webb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scott_P/0/1/0/all/0/1\">Peter J.H. Scott</a>",
          "description": "Artificial intelligence and machine learning are poised to disrupt PET\nimaging from bench to clinic. In this perspective we offer insights into how\nthe technology could be applied to improve the design and synthesis of new\nradiopharmaceuticals for PET imaging, including identification of an optimal\nlabeling approach as well as strategies for radiolabeling reaction\noptimization.",
          "link": "http://arxiv.org/abs/2108.02814",
          "publishedOn": "2021-08-09T00:49:27.788Z",
          "wordCount": 496,
          "title": "Potential Applications of Artificial Intelligence and Machine Learning in Radiochemistry and Radiochemical Engineering. (arXiv:2108.02814v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kukacka_J/0/1/0/all/0/1\">Jan Kuka&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zenz_A/0/1/0/all/0/1\">Anja Zenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kollovieh_M/0/1/0/all/0/1\">Marcel Kollovieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Justel_D/0/1/0/all/0/1\">Dominik J&#xfc;stel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntziachristos_V/0/1/0/all/0/1\">Vasilis Ntziachristos</a>",
          "description": "Fundus photography is the primary method for retinal imaging and essential\nfor diabetic retinopathy prevention. Automated segmentation of fundus\nphotographs would improve the quality, capacity, and cost-effectiveness of eye\ncare screening programs. However, current segmentation methods are not robust\ntowards the diversity in imaging conditions and pathologies typical for\nreal-world clinical applications. To overcome these limitations, we utilized\ncontrastive self-supervised learning to exploit the large variety of unlabeled\nfundus images in the publicly available EyePACS dataset. We pre-trained an\nencoder of a U-Net, which we later fine-tuned on several retinal vessel and\nlesion segmentation datasets. We demonstrate for the first time that by using\ncontrastive self-supervised learning, the pre-trained network can recognize\nblood vessels, optic disc, fovea, and various lesions without being provided\nany labels. Furthermore, when fine-tuned on a downstream blood vessel\nsegmentation task, such pre-trained networks achieve state-of-the-art\nperformance on images from different datasets. Additionally, the pre-training\nalso leads to shorter training times and an improved few-shot performance on\nboth blood vessel and lesion segmentation tasks. Altogether, our results\nshowcase the benefits of contrastive self-supervised pre-training which can\nplay a crucial role in real-world clinical applications requiring robust models\nable to adapt to new devices with only a few annotated samples.",
          "link": "http://arxiv.org/abs/2108.02798",
          "publishedOn": "2021-08-09T00:49:27.760Z",
          "wordCount": 650,
          "title": "Self-Supervised Learning from Unlabeled Fundus Photographs Improves Segmentation of the Retina. (arXiv:2108.02798v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xu Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hjelm_D/0/1/0/all/0/1\">Devon Hjelm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>",
          "description": "Piecewise linear neural networks can be split into subfunctions, each with\nits own activation pattern, domain, and empirical error. Empirical error for\nthe full network can be written as an expectation over empirical error of\nsubfunctions. Constructing a generalization bound on subfunction empirical\nerror indicates that the more densely a subfunction is surrounded by training\nsamples in representation space, the more reliable its predictions are.\nFurther, it suggests that models with fewer activation regions generalize\nbetter, and models that abstract knowledge to a greater degree generalize\nbetter, all else equal. We propose not only a theoretical framework to reason\nabout subfunction error bounds but also a pragmatic way of approximately\nevaluating it, which we apply to predicting which samples the network will not\nsuccessfully generalize to. We test our method on detection of\nmisclassification and out-of-distribution samples, finding that it performs\ncompetitively in both cases. In short, some network activation patterns are\nassociated with higher reliability than others, and these can be identified\nusing subfunction error bounds.",
          "link": "http://arxiv.org/abs/2106.08365",
          "publishedOn": "2021-08-06T00:51:48.178Z",
          "wordCount": 642,
          "title": "Predicting Unreliable Predictions by Shattering a Neural Network. (arXiv:2106.08365v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuelu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Ziniu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>",
          "description": "Answering complex First-Order Logical (FOL) queries on large-scale incomplete\nknowledge graphs (KGs) is an important yet challenging task. Recent advances\nembed logical queries and KG entities in the vector space and conduct query\nanswering via dense similarity search. However, most of the designed logical\noperators in existing works do not satisfy the axiomatic system of classical\nlogic. Moreover, these logical operators are parameterized so that they require\na large number of complex FOL queries as training data, which are often arduous\nor even inaccessible to collect in most real-world KGs. In this paper, we\npresent FuzzQE, a fuzzy logic based query embedding framework for answering FOL\nqueries over KGs. FuzzQE follows fuzzy logic to define logical operators in a\nprincipled and learning free manner. Extensive experiments on two benchmark\ndatasets demonstrate that FuzzQE achieves significantly better performance in\nanswering FOL queries compared to the state-of-the-art methods. In addition,\nFuzzQE trained with only KG link prediction without any complex queries can\nachieve comparable performance with the systems trained with all FOL queries.",
          "link": "http://arxiv.org/abs/2108.02390",
          "publishedOn": "2021-08-06T00:51:48.138Z",
          "wordCount": 600,
          "title": "Fuzzy Logic based Logical Query Answering on Knowledge Graph. (arXiv:2108.02390v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_N/0/1/0/all/0/1\">Nitish Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prados_D/0/1/0/all/0/1\">David Ramon Prados</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodzic_N/0/1/0/all/0/1\">Nedim Hodzic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanassios_C/0/1/0/all/0/1\">Christos Karanassios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1\">H.R. Tizhoosh</a>",
          "description": "Lung nodules are commonly missed in chest radiographs. We propose and\nevaluate P-AnoGAN, an unsupervised anomaly detection approach for lung nodules\nin radiographs. P-AnoGAN modifies the fast anomaly detection generative\nadversarial network (f-AnoGAN) by utilizing a progressive GAN and a\nconvolutional encoder-decoder-encoder pipeline. Model training uses only\nunlabelled healthy lung patches extracted from the Indiana University Chest\nX-Ray Collection. External validation and testing are performed using healthy\nand unhealthy patches extracted from the ChestX-ray14 and Japanese Society for\nRadiological Technology datasets, respectively. Our model robustly identifies\npatches containing lung nodules in external validation and test data with\nROC-AUC of 91.17% and 87.89%, respectively. These results show unsupervised\nmethods may be useful in challenging tasks such as lung nodule detection in\nradiographs.",
          "link": "http://arxiv.org/abs/2108.02233",
          "publishedOn": "2021-08-06T00:51:47.947Z",
          "wordCount": 594,
          "title": "Unsupervised Detection of Lung Nodules in Chest Radiography Using Generative Adversarial Networks. (arXiv:2108.02233v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Canyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eschweiler_D/0/1/0/all/0/1\">Dennis Eschweiler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stegmaier_J/0/1/0/all/0/1\">Johannes Stegmaier</a>",
          "description": "Recent developments in fluorescence microscopy allow capturing\nhigh-resolution 3D images over time for living model organisms. To be able to\nimage even large specimens, techniques like multi-view light-sheet imaging\nrecord different orientations at each time point that can then be fused into a\nsingle high-quality volume. Based on measured point spread functions (PSF),\ndeconvolution and content fusion are able to largely revert the inevitable\ndegradation occurring during the imaging process. Classical multi-view\ndeconvolution and fusion methods mainly use iterative procedures and\ncontent-based averaging. Lately, Convolutional Neural Networks (CNNs) have been\ndeployed to approach 3D single-view deconvolution microscopy, but the\nmulti-view case waits to be studied. We investigated the efficacy of CNN-based\nmulti-view deconvolution and fusion with two synthetic data sets that mimic\ndeveloping embryos and involve either two or four complementary 3D views.\nCompared with classical state-of-the-art methods, the proposed semi- and\nself-supervised models achieve competitive and superior deconvolution and\nfusion quality in the two-view and quad-view cases, respectively.",
          "link": "http://arxiv.org/abs/2108.02743",
          "publishedOn": "2021-08-06T00:51:47.898Z",
          "wordCount": 620,
          "title": "Semi- and Self-Supervised Multi-View Fusion of 3D Microscopy Images using Generative Adversarial Networks. (arXiv:2108.02743v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Ming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Feng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "Deep learning on graphs has attracted significant interests recently.\nHowever, most of the works have focused on (semi-) supervised learning,\nresulting in shortcomings including heavy label reliance, poor generalization,\nand weak robustness. To address these issues, self-supervised learning (SSL),\nwhich extracts informative knowledge through well-designed pretext tasks\nwithout relying on manual labels, has become a promising and trending learning\nparadigm for graph data. Different from SSL on other domains like computer\nvision and natural language processing, SSL on graphs has an exclusive\nbackground, design ideas, and taxonomies. Under the umbrella of graph\nself-supervised learning, we present a timely and comprehensive review of the\nexisting approaches which employ SSL techniques for graph data. We construct a\nunified framework that mathematically formalizes the paradigm of graph SSL.\nAccording to the objectives of pretext tasks, we divide these approaches into\nfour categories: generation-based, auxiliary property-based, contrast-based,\nand hybrid approaches. We further conclude the applications of graph SSL across\nvarious research fields and summarize the commonly used datasets, evaluation\nbenchmark, performance comparison and open-source codes of graph SSL. Finally,\nwe discuss the remaining challenges and potential future directions in this\nresearch field.",
          "link": "http://arxiv.org/abs/2103.00111",
          "publishedOn": "2021-08-06T00:51:47.887Z",
          "wordCount": 654,
          "title": "Graph Self-Supervised Learning: A Survey. (arXiv:2103.00111v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng-Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun-Yan Zhu</a>",
          "description": "Can a user create a deep generative model by sketching a single example?\nTraditionally, creating a GAN model has required the collection of a\nlarge-scale dataset of exemplars and specialized knowledge in deep learning. In\ncontrast, sketching is possibly the most universally accessible way to convey a\nvisual concept. In this work, we present a method, GAN Sketching, for rewriting\nGANs with one or more sketches, to make GANs training easier for novice users.\nIn particular, we change the weights of an original GAN model according to user\nsketches. We encourage the model's output to match the user sketches through a\ncross-domain adversarial loss. Furthermore, we explore different regularization\nmethods to preserve the original model's diversity and image quality.\nExperiments have shown that our method can mold GANs to match shapes and poses\nspecified by sketches while maintaining realism and diversity. Finally, we\ndemonstrate a few applications of the resulting GAN, including latent space\ninterpolation and image editing.",
          "link": "http://arxiv.org/abs/2108.02774",
          "publishedOn": "2021-08-06T00:51:47.880Z",
          "wordCount": 599,
          "title": "Sketch Your Own GAN. (arXiv:2108.02774v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rafee_S/0/1/0/all/0/1\">Syed Rifat Mahmud Rafee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gyorgy Fazekas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiggins_G/0/1/0/all/0/1\">Geraint A.~Wiggins</a>",
          "description": "Music Performers have their own idiosyncratic way of interpreting a musical\npiece. A group of skilled performers playing the same piece of music would\nlikely to inject their unique artistic styles in their performances. The\nvariations of the tempo, timing, dynamics, articulation etc. from the actual\nnotated music are what make the performers unique in their performances. This\nstudy presents a dataset consisting of four movements of Schubert's ``Sonata in\nB-flat major, D.960\" performed by nine virtuoso pianists individually. We\nproposed and extracted a set of expressive features that are able to capture\nthe characteristics of an individual performer's style. We then present a\nperformer identification method based on the similarity of feature\ndistribution, given a set of piano performances. The identification is done\nconsidering each feature individually as well as a fusion of the features.\nResults show that the proposed method achieved a precision of 0.903 using\nfusion features. Moreover, the onset time deviation feature shows promising\nresult when considered individually.",
          "link": "http://arxiv.org/abs/2108.02576",
          "publishedOn": "2021-08-06T00:51:47.874Z",
          "wordCount": 610,
          "title": "Performer Identification From Symbolic Representation of Music Using Statistical Models. (arXiv:2108.02576v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/1909.08074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Assefa_B/0/1/0/all/0/1\">Beakal Gizachew Assefa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozkasap_O/0/1/0/all/0/1\">Oznur Ozkasap</a>",
          "description": "Software Defined Networking (SDN) achieves programmability of a network\nthrough separation of the control and data planes. It enables flexibility in\nnetwork management and control. Energy efficiency is one of the challenging\nglobal problems which has both economic and environmental impact. A massive\namount of information is generated in the controller of an SDN based network.\nMachine learning gives the ability to computers to progressively learn from\ndata without having to write specific instructions. In this work, we propose\nMER-SDN: a machine learning framework for traffic-aware energy efficient\nrouting in SDN. Feature extraction, training, and testing are the three main\nstages of the learning machine. Experiments are conducted on Mininet and POX\ncontroller using real-world network topology and dynamic traffic traces from\nSNDlib. Results show that our approach achieves more than 65\\% feature size\nreduction, more than 70% accuracy in parameter prediction of an energy\nefficient heuristics algorithm, also our prediction refine heuristics converges\nthe predicted value to the optimal parameters values with up to 25X speedup as\ncompared to the brute force method.",
          "link": "http://arxiv.org/abs/1909.08074",
          "publishedOn": "2021-08-06T00:51:47.852Z",
          "wordCount": 672,
          "title": "MER-SDN: Machine Learning Framework for Traffic Aware Energy Efficient Routing in SDN. (arXiv:1909.08074v3 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08176",
          "author": "<a href=\"http://arxiv.org/find/hep-lat/1/au:+Albergo_M/0/1/0/all/0/1\">Michael S. Albergo</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Boyda_D/0/1/0/all/0/1\">Denis Boyda</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Hackett_D/0/1/0/all/0/1\">Daniel C. Hackett</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Kanwar_G/0/1/0/all/0/1\">Gurtej Kanwar</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Cranmer_K/0/1/0/all/0/1\">Kyle Cranmer</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Racaniere_S/0/1/0/all/0/1\">S&#xe9;bastien Racani&#xe8;re</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Rezende_D/0/1/0/all/0/1\">Danilo Jimenez Rezende</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Shanahan_P/0/1/0/all/0/1\">Phiala E. Shanahan</a>",
          "description": "This notebook tutorial demonstrates a method for sampling Boltzmann\ndistributions of lattice field theories using a class of machine learning\nmodels known as normalizing flows. The ideas and approaches proposed in\narXiv:1904.12072, arXiv:2002.02428, and arXiv:2003.06413 are reviewed and a\nconcrete implementation of the framework is presented. We apply this framework\nto a lattice scalar field theory and to U(1) gauge theory, explicitly encoding\ngauge symmetries in the flow-based approach to the latter. This presentation is\nintended to be interactive and working with the attached Jupyter notebook is\nrecommended.",
          "link": "http://arxiv.org/abs/2101.08176",
          "publishedOn": "2021-08-06T00:51:47.845Z",
          "wordCount": 575,
          "title": "Introduction to Normalizing Flows for Lattice Field Theory. (arXiv:2101.08176v2 [hep-lat] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Stephan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trott_A/0/1/0/all/0/1\">Alexander Trott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1\">Sunil Srinivasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parkes_D/0/1/0/all/0/1\">David C. Parkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1\">Richard Socher</a>",
          "description": "AI and reinforcement learning (RL) have improved many areas, but are not yet\nwidely adopted in economic policy design, mechanism design, or economics at\nlarge. At the same time, current economic methodology is limited by a lack of\ncounterfactual data, simplistic behavioral models, and limited opportunities to\nexperiment with policies and evaluate behavioral responses. Here we show that\nmachine-learning-based economic simulation is a powerful policy and mechanism\ndesign framework to overcome these limitations. The AI Economist is a\ntwo-level, deep RL framework that trains both agents and a social planner who\nco-adapt, providing a tractable solution to the highly unstable and novel\ntwo-level RL challenge. From a simple specification of an economy, we learn\nrational agent behaviors that adapt to learned planner policies and vice versa.\nWe demonstrate the efficacy of the AI Economist on the problem of optimal\ntaxation. In simple one-step economies, the AI Economist recovers the optimal\ntax policy of economic theory. In complex, dynamic economies, the AI Economist\nsubstantially improves both utilitarian social welfare and the trade-off\nbetween equality and productivity over baselines. It does so despite emergent\ntax-gaming strategies, while accounting for agent interactions and behavioral\nchange more accurately than economic theory. These results demonstrate for the\nfirst time that two-level, deep RL can be used for understanding and as a\ncomplement to theory for economic design, unlocking a new computational\nlearning-based approach to understanding economic policy.",
          "link": "http://arxiv.org/abs/2108.02755",
          "publishedOn": "2021-08-06T00:51:47.833Z",
          "wordCount": 690,
          "title": "The AI Economist: Optimal Economic Policy Design via Two-level Deep Reinforcement Learning. (arXiv:2108.02755v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00116",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Memarian_F/0/1/0/all/0/1\">Farzan Memarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemi_A/0/1/0/all/0/1\">Abolfazl Hashemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1\">Scott Niekum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>",
          "description": "We explore methodologies to improve the robustness of generative adversarial\nimitation learning (GAIL) algorithms to observation noise. Towards this\nobjective, we study the effect of local Lipschitzness of the discriminator and\nthe generator on the robustness of policies learned by GAIL. In many robotics\napplications, the learned policies by GAIL typically suffer from a degraded\nperformance at test time since the observations from the environment might be\ncorrupted by noise. Hence, robustifying the learned policies against the\nobservation noise is of critical importance. To this end, we propose a\nregularization method to induce local Lipschitzness in the generator and the\ndiscriminator of adversarial imitation learning methods. We show that the\nmodified objective leads to learning significantly more robust policies.\nMoreover, we demonstrate --- both theoretically and experimentally --- that\ntraining a locally Lipschitz discriminator leads to a locally Lipschitz\ngenerator, thereby improving the robustness of the resultant policy. We perform\nextensive experiments on simulated robot locomotion environments from the\nMuJoCo suite that demonstrate the proposed method learns policies that\nsignificantly outperform the state-of-the-art generative adversarial imitation\nlearning algorithm when applied to test scenarios with noise-corrupted\nobservations.",
          "link": "http://arxiv.org/abs/2107.00116",
          "publishedOn": "2021-08-06T00:51:47.826Z",
          "wordCount": 644,
          "title": "Robust Generative Adversarial Imitation Learning via Local Lipschitzness. (arXiv:2107.00116v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tungyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Youting Wang</a>",
          "description": "For the highly imbalanced credit card fraud detection problem, most existing\nmethods either use data augmentation methods or conventional machine learning\nmodels, while neural network-based anomaly detection approaches are lacking.\nFurthermore, few studies have employed AI interpretability tools to investigate\nthe feature importance of transaction data, which is crucial for the black-box\nfraud detection module. Considering these two points together, we propose a\nnovel anomaly detection framework for credit card fraud detection as well as a\nmodel-explaining module responsible for prediction explanations. The fraud\ndetection model is composed of two deep neural networks, which are trained in\nan unsupervised and adversarial manner. Precisely, the generator is an\nAutoEncoder aiming to reconstruct genuine transaction data, while the\ndiscriminator is a fully-connected network for fraud detection. The explanation\nmodule has three white-box explainers in charge of interpretations of the\nAutoEncoder, discriminator, and the whole detection model, respectively.\nExperimental results show the state-of-the-art performances of our fraud\ndetection model on the benchmark dataset compared with baselines. In addition,\nprediction analyses by three explainers are presented, offering a clear\nperspective on how each feature of an instance of interest contributes to the\nfinal model output.",
          "link": "http://arxiv.org/abs/2108.02501",
          "publishedOn": "2021-08-06T00:51:47.820Z",
          "wordCount": 632,
          "title": "Locally Interpretable One-Class Anomaly Detection for Credit Card Fraud Detection. (arXiv:2108.02501v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Velichko_A/0/1/0/all/0/1\">Andrei Velichko</a>",
          "description": "The study presents a new method for analyzing medical data based on the\nLogNNet neural network, which uses chaotic mappings to transform input\ninformation. The technique calculates risk factors for the presence of a\ndisease in a patient according to a set of medical health indicators. The\nLogNNet architecture allows the implementation of artificial intelligence on\nmedical pe-ripherals of the Internet of Things with low RAM resources, and the\ndevelopment of edge computing in healthcare. The efficiency of LogNNet in\nassessing perinatal risk is illustrated on cardiotocogram data of 2126 pregnant\nwomen, obtained from the UC Irvine machine learning repository. The\nclassification accuracy reaches ~ 91%, with the ~ 3-10 kB of RAM used on the\nArduino microcontroller. In addition, examples for diagnosing COVID-19 are\nprovided, using LogNNet trained on a publicly available database from the\nIsraeli Ministry of Health. The service concept has been developed, which uses\nthe data of the express test for COVID-19 and reaches the classification\naccuracy of ~ 95% with the ~ 0.6 kB of RAM used on Arduino microcontrollers. In\nall examples, the model is tested using standard classification quality\nmetrics: Precision, Recall, and F1-measure. The study results can be used in\nclinical decision support systems.",
          "link": "http://arxiv.org/abs/2108.02428",
          "publishedOn": "2021-08-06T00:51:47.801Z",
          "wordCount": 709,
          "title": "A Method for Medical Data Analysis Using the LogNNet for Clinical Decision Support Systems and Edge Computing in Healthcare. (arXiv:2108.02428v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farinhas_A/0/1/0/all/0/1\">Ant&#xf3;nio Farinhas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1\">Wilker Aziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1\">Vlad Niculae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>",
          "description": "Neural networks and other machine learning models compute continuous\nrepresentations, while humans communicate mostly through discrete symbols.\nReconciling these two forms of communication is desirable for generating\nhuman-readable interpretations or learning discrete latent variable models,\nwhile maintaining end-to-end differentiability. Some existing approaches (such\nas the Gumbel-Softmax transformation) build continuous relaxations that are\ndiscrete approximations in the zero-temperature limit, while others (such as\nsparsemax transformations and the Hard Concrete distribution) produce\ndiscrete/continuous hybrids. In this paper, we build rigorous theoretical\nfoundations for these hybrids, which we call \"mixed random variables.\" Our\nstarting point is a new \"direct sum\" base measure defined on the face lattice\nof the probability simplex. From this measure, we introduce new entropy and\nKullback-Leibler divergence functions that subsume the discrete and\ndifferential cases and have interpretations in terms of code optimality. Our\nframework suggests two strategies for representing and sampling mixed random\nvariables, an extrinsic (\"sample-and-project\") and an intrinsic one (based on\nface stratification). We experiment with both approaches on an emergent\ncommunication benchmark and on modeling MNIST and Fashion-MNIST data with\nvariational auto-encoders with mixed latent variables.",
          "link": "http://arxiv.org/abs/2108.02658",
          "publishedOn": "2021-08-06T00:51:47.788Z",
          "wordCount": 607,
          "title": "Sparse Communication via Mixed Distributions. (arXiv:2108.02658v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ochi_V/0/1/0/all/0/1\">Virginia Ochi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Estrada_R/0/1/0/all/0/1\">Ricardo Estrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaji_T/0/1/0/all/0/1\">Teezal Gaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadea_W/0/1/0/all/0/1\">Wendy Gadea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_E/0/1/0/all/0/1\">Emily Duong</a>",
          "description": "Our analysis reviews and visualizes the audio features and popularity of\nsongs streamed on Spotify*. Our dataset, downloaded from Kaggle and originally\nsourced from Spotify API, consists of multiple Excel files containing\ninformation relevant to our visualization and regression analysis. The exercise\nseeks to determine the connection between the popularity of the songs and the\ndanceability. Insights to be included and factored as part of our analysis\ninclude song energy, valence, BPM, release date, and year.",
          "link": "http://arxiv.org/abs/2108.02370",
          "publishedOn": "2021-08-06T00:51:47.749Z",
          "wordCount": 524,
          "title": "Spotify Danceability and Popularity Analysis using SAP. (arXiv:2108.02370v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bennequin_E/0/1/0/all/0/1\">Etienne Bennequin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouvier_V/0/1/0/all/0/1\">Victor Bouvier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tami_M/0/1/0/all/0/1\">Myriam Tami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toubhans_A/0/1/0/all/0/1\">Antoine Toubhans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1\">C&#xe9;line Hudelot</a>",
          "description": "Few-Shot Learning (FSL) algorithms have made substantial progress in learning\nnovel concepts with just a handful of labelled data. To classify query\ninstances from novel classes encountered at test-time, they only require a\nsupport set composed of a few labelled samples. FSL benchmarks commonly assume\nthat those queries come from the same distribution as instances in the support\nset. However, in a realistic set-ting, data distribution is plausibly subject\nto change, a situation referred to as Distribution Shift (DS). The present work\naddresses the new and challenging problem of Few-Shot Learning under\nSupport/Query Shift (FSQS) i.e., when support and query instances are sampled\nfrom related but different distributions. Our contributions are the following.\nFirst, we release a testbed for FSQS, including datasets, relevant baselines\nand a protocol for a rigorous and reproducible evaluation. Second, we observe\nthat well-established FSL algorithms unsurprisingly suffer from a considerable\ndrop in accuracy when facing FSQS, stressing the significance of our study.\nFinally, we show that transductive algorithms can limit the inopportune effect\nof DS. In particular, we study both the role of Batch-Normalization and Optimal\nTransport (OT) in aligning distributions, bridging Unsupervised Domain\nAdaptation with FSL. This results in a new method that efficiently combines OT\nwith the celebrated Prototypical Networks. We bring compelling experiments\ndemonstrating the advantage of our method. Our work opens an exciting line of\nresearch by providing a testbed and strong baselines. Our code is available at\nhttps://github.com/ebennequin/meta-domain-shift.",
          "link": "http://arxiv.org/abs/2105.11804",
          "publishedOn": "2021-08-06T00:51:47.743Z",
          "wordCount": 713,
          "title": "Bridging Few-Shot Learning and Adaptation: New Challenges of Support-Query Shift. (arXiv:2105.11804v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01583",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Woodworth_B/0/1/0/all/0/1\">Blake Woodworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bullins_B/0/1/0/all/0/1\">Brian Bullins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1\">Ohad Shamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srebro_N/0/1/0/all/0/1\">Nathan Srebro</a>",
          "description": "We resolve the min-max complexity of distributed stochastic convex\noptimization (up to a log factor) in the intermittent communication setting,\nwhere $M$ machines work in parallel over the course of $R$ rounds of\ncommunication to optimize the objective, and during each round of\ncommunication, each machine may sequentially compute $K$ stochastic gradient\nestimates. We present a novel lower bound with a matching upper bound that\nestablishes an optimal algorithm.",
          "link": "http://arxiv.org/abs/2102.01583",
          "publishedOn": "2021-08-06T00:51:47.735Z",
          "wordCount": 544,
          "title": "The Min-Max Complexity of Distributed Stochastic Convex Optimization with Intermittent Communication. (arXiv:2102.01583v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.06610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Havasi_M/0/1/0/all/0/1\">Marton Havasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1\">Rodolphe Jenatton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jeremiah Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_J/0/1/0/all/0/1\">Jasper Snoek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Dustin Tran</a>",
          "description": "Recent approaches to efficiently ensemble neural networks have shown that\nstrong robustness and uncertainty performance can be achieved with a negligible\ngain in parameters over the original network. However, these methods still\nrequire multiple forward passes for prediction, leading to a significant\ncomputational cost. In this work, we show a surprising result: the benefits of\nusing multiple predictions can be achieved `for free' under a single model's\nforward pass. In particular, we show that, using a multi-input multi-output\n(MIMO) configuration, one can utilize a single model's capacity to train\nmultiple subnetworks that independently learn the task at hand. By ensembling\nthe predictions made by the subnetworks, we improve model robustness without\nincreasing compute. We observe a significant improvement in negative\nlog-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet,\nand their out-of-distribution variants compared to previous methods.",
          "link": "http://arxiv.org/abs/2010.06610",
          "publishedOn": "2021-08-06T00:51:47.717Z",
          "wordCount": 632,
          "title": "Training independent subnetworks for robust prediction. (arXiv:2010.06610v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ganzfried_S/0/1/0/all/0/1\">Sam Ganzfried</a>",
          "description": "Many real-world games contain parameters which can affect payoffs, action\nspaces, and information states. For fixed values of the parameters, the game\ncan be solved using standard algorithms. However, in many settings agents must\nact without knowing the values of the parameters that will be encountered in\nadvance. Often the decisions must be made by a human under time and resource\nconstraints, and it is unrealistic to assume that a human can solve the game in\nreal time. We present a new framework that enables human decision makers to\nmake fast decisions without the aid of real-time solvers. We demonstrate\napplicability to a variety of situations including settings with multiple\nplayers and imperfect information.",
          "link": "http://arxiv.org/abs/2104.14744",
          "publishedOn": "2021-08-06T00:51:47.711Z",
          "wordCount": 581,
          "title": "Human strategic decision making in parametrized games. (arXiv:2104.14744v2 [cs.GT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02574",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_F/0/1/0/all/0/1\">Fei Wen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1\">Zeyu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ying_R/0/1/0/all/0/1\">Rendong Ying</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peilin Liu</a>",
          "description": "Recently, much progress has been made in unsupervised restoration learning.\nHowever, existing methods more or less rely on some assumptions on the signal\nand/or degradation model, which limits their practical performance. How to\nconstruct an optimal criterion for unsupervised restoration learning without\nany prior knowledge on the degradation model is still an open question. Toward\nanswering this question, this work proposes a criterion for unsupervised\nrestoration learning based on the optimal transport theory. This criterion has\nfavorable properties, e.g., approximately maximal preservation of the\ninformation of the signal, whilst achieving perceptual reconstruction.\nFurthermore, though a relaxed unconstrained formulation is used in practical\nimplementation, we show that the relaxed formulation in theory has the same\nsolution as the original constrained formulation. Experiments on synthetic and\nreal-world data, including realistic photographic, microscopy, depth, and raw\ndepth images, demonstrate that the proposed method even compares favorably with\nsupervised methods, e.g., approaching the PSNR of supervised methods while\nhaving better perceptual quality. Particularly, for spatially correlated noise\nand realistic microscopy images, the proposed method not only achieves better\nperceptual quality but also has higher PSNR than supervised methods. Besides,\nit shows remarkable superiority in harsh practical conditions with complex\nnoise, e.g., raw depth images.",
          "link": "http://arxiv.org/abs/2108.02574",
          "publishedOn": "2021-08-06T00:51:47.704Z",
          "wordCount": 652,
          "title": "Optimal Transport for Unsupervised Restoration Learning. (arXiv:2108.02574v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weijie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang%2A_C/0/1/0/all/0/1\">Chong Wang*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haohe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shenghao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Song Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xulun Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiafei Wu</a>",
          "description": "Expensive bounding-box annotations have limited the development of object\ndetection task. Thus, it is necessary to focus on more challenging task of\nfew-shot object detection. It requires the detector to recognize objects of\nnovel classes with only a few training samples. Nowadays, many existing popular\nmethods based on meta-learning have achieved promising performance, such as\nMeta R-CNN series. However, only a single category of support data is used as\nthe attention to guide the detecting of query images each time. Their relevance\nto each other remains unexploited. Moreover, a lot of recent works treat the\nsupport data and query images as independent branch without considering the\nrelationship between them. To address this issue, we propose a dynamic\nrelevance learning model, which utilizes the relationship between all support\nimages and Region of Interest (RoI) on the query images to construct a dynamic\ngraph convolutional network (GCN). By adjusting the prediction distribution of\nthe base detector using the output of this GCN, the proposed model can guide\nthe detector to improve the class representation implicitly. Comprehensive\nexperiments have been conducted on Pascal VOC and MS-COCO dataset. The proposed\nmodel achieves the best overall performance, which shows its effectiveness of\nlearning more generalized features. Our code is available at\nhttps://github.com/liuweijie19980216/DRL-for-FSOD.",
          "link": "http://arxiv.org/abs/2108.02235",
          "publishedOn": "2021-08-06T00:51:47.697Z",
          "wordCount": 664,
          "title": "Dynamic Relevance Learning for Few-Shot Object Detection. (arXiv:2108.02235v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1\">Juan P. Vigueras-Guill&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_A/0/1/0/all/0/1\">Arijit Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engkvist_O/0/1/0/all/0/1\">Ola Engkvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1\">Frank Seeliger</a>",
          "description": "Capsule Networks (CapsNets) is a machine learning architecture proposed to\novercome some of the shortcomings of convolutional neural networks (CNNs).\nHowever, CapsNets have mainly outperformed CNNs in datasets where images are\nsmall and/or the objects to identify have minimal background noise. In this\nwork, we present a new architecture, parallel CapsNets, which exploits the\nconcept of branching the network to isolate certain capsules, allowing each\nbranch to identify different entities. We applied our concept to the two\ncurrent types of CapsNet architectures, studying the performance for networks\nwith different layers of capsules. We tested our design in a public, highly\nunbalanced dataset of acute myeloid leukaemia images (15 classes). Our\nexperiments showed that conventional CapsNets show similar performance than our\nbaseline CNN (ResNeXt-50) but depict instability problems. In contrast,\nparallel CapsNets can outperform ResNeXt-50, is more stable, and shows better\nrotational invariance than both, conventional CapsNets and ResNeXt-50.",
          "link": "http://arxiv.org/abs/2108.02644",
          "publishedOn": "2021-08-06T00:51:47.691Z",
          "wordCount": 607,
          "title": "Parallel Capsule Networks for Classification of White Blood Cells. (arXiv:2108.02644v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2001.02309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kjos_Hanssen_B/0/1/0/all/0/1\">Bj&#xf8;rn Kjos-Hanssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felix_C/0/1/0/all/0/1\">Clyde James Felix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sun Young Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamb_E/0/1/0/all/0/1\">Ethan Lamb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_D/0/1/0/all/0/1\">Davin Takahashi</a>",
          "description": "Let $NFA_b(q)$ denote the set of languages accepted by nondeterministic\nfinite automata with $q$ states over an alphabet with $b$ letters. Let $B_n$\ndenote the set of words of length $n$. We give a quadratic lower bound on the\nVC dimension of \\[\n\nNFA_2(q)\\cap B_n = \\{L\\cap B_n \\mid L \\in NFA_2(q)\\} \\] as a function of $q$.\n\nNext, the work of Gruber and Holzer (2007) gives an upper bound for the\nnondeterministic state complexity of finite languages contained in $B_n$, which\nwe strengthen using our methods.\n\nFinally, we give some theoretical and experimental results on the dependence\non $n$ of the VC dimension and testing dimension of $NFA_2(q)\\cap B_n$.",
          "link": "http://arxiv.org/abs/2001.02309",
          "publishedOn": "2021-08-06T00:51:47.679Z",
          "wordCount": 610,
          "title": "VC-dimensions of nondeterministic finite automata for words of equal length. (arXiv:2001.02309v2 [cs.FL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.00964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheikhalishahi_S/0/1/0/all/0/1\">Seyedmostafa Sheikhalishahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaraman_V/0/1/0/all/0/1\">Vevake Balaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osmani_V/0/1/0/all/0/1\">Venet Osmani</a>",
          "description": "Progress of machine learning in critical care has been difficult to track, in\npart due to absence of public benchmarks. Other fields of research (such as\ncomputer vision and natural language processing) have established various\ncompetitions and public benchmarks. Recent availability of large clinical\ndatasets has enabled the possibility of establishing public benchmarks. Taking\nadvantage of this opportunity, we propose a public benchmark suite to address\nfour areas of critical care, namely mortality prediction, estimation of length\nof stay, patient phenotyping and risk of decompensation. We define each task\nand compare the performance of both clinical models as well as baseline and\ndeep learning models using eICU critical care dataset of around 73,000\npatients. This is the first public benchmark on a multi-centre critical care\ndataset, comparing the performance of clinical gold standard with our\npredictive model. We also investigate the impact of numerical variables as well\nas handling of categorical variables on each of the defined tasks. The source\ncode, detailing our methods and experiments is publicly available such that\nanyone can replicate our results and build upon our work.",
          "link": "http://arxiv.org/abs/1910.00964",
          "publishedOn": "2021-08-06T00:51:47.616Z",
          "wordCount": 664,
          "title": "Benchmarking machine learning models on multi-centre eICU critical care dataset. (arXiv:1910.00964v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alves_G/0/1/0/all/0/1\">Guilherme Alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amblard_M/0/1/0/all/0/1\">Maxime Amblard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernier_F/0/1/0/all/0/1\">Fabien Bernier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couceiro_M/0/1/0/all/0/1\">Miguel Couceiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Napoli_A/0/1/0/all/0/1\">Amedeo Napoli</a>",
          "description": "Unintended biases in machine learning (ML) models are among the major\nconcerns that must be addressed to maintain public trust in ML. In this paper,\nwe address process fairness of ML models that consists in reducing the\ndependence of models on sensitive features, without compromising their\nperformance. We revisit the framework FixOut that is inspired in the approach\n\"fairness through unawareness\" to build fairer models. We introduce several\nimprovements such as automating the choice of FixOut's parameters. Also, FixOut\nwas originally proposed to improve fairness of ML models on tabular data. We\nalso demonstrate the feasibility of FixOut's workflow for models on textual\ndata. We present several experimental results that illustrate the fact that\nFixOut improves process fairness on different classification settings.",
          "link": "http://arxiv.org/abs/2108.02662",
          "publishedOn": "2021-08-06T00:51:47.605Z",
          "wordCount": 586,
          "title": "Reducing Unintended Bias of ML Models on Tabular and Textual Data. (arXiv:2108.02662v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatti_A/0/1/0/all/0/1\">Anubhav Bhatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behinaein_B/0/1/0/all/0/1\">Behnam Behinaein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodenburg_D/0/1/0/all/0/1\">Dirk Rodenburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hungler_P/0/1/0/all/0/1\">Paul Hungler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>",
          "description": "Classification of human emotions can play an essential role in the design and\nimprovement of human-machine systems. While individual biological signals such\nas Electrocardiogram (ECG) and Electrodermal Activity (EDA) have been widely\nused for emotion recognition with machine learning methods, multimodal\napproaches generally fuse extracted features or final classification/regression\nresults to boost performance. To enhance multimodal learning, we present a\nnovel attentive cross-modal connection to share information between\nconvolutional neural networks responsible for learning individual modalities.\nSpecifically, these connections improve emotion classification by sharing\nintermediate representations among EDA and ECG and apply attention weights to\nthe shared information, thus learning more effective multimodal embeddings. We\nperform experiments on the WESAD dataset to identify the best configuration of\nthe proposed method for emotion classification. Our experiments show that the\nproposed approach is capable of learning strong multimodal representations and\noutperforms a number of baselines methods.",
          "link": "http://arxiv.org/abs/2108.02241",
          "publishedOn": "2021-08-06T00:51:47.598Z",
          "wordCount": 603,
          "title": "Attentive Cross-modal Connections for Deep Multimodal Wearable-based Emotion Recognition. (arXiv:2108.02241v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Donevski_I/0/1/0/all/0/1\">Igor Donevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_J/0/1/0/all/0/1\">Jimmy Jessen Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popovski_P/0/1/0/all/0/1\">Petar Popovski</a>,",
          "description": "In this paper we envision a federated learning (FL) scenario in service of\namending the performance of autonomous road vehicles, through a drone traffic\nmonitor (DTM), that also acts as an orchestrator. Expecting non-IID data\ndistribution, we focus on the issue of accelerating the learning of a\nparticular class of critical object (CO), that may harm the nominal operation\nof an autonomous vehicle. This can be done through proper allocation of the\nwireless resources for addressing learner and data heterogeneity. Thus, we\npropose a reactive method for the allocation of wireless resources, that\nhappens dynamically each FL round, and is based on each learner's contribution\nto the general model. In addition to this, we explore the use of static methods\nthat remain constant across all rounds. Since we expect partial work from each\nlearner, we use the FedProx FL algorithm, in the task of computer vision. For\ntesting, we construct a non-IID data distribution of the MNIST and FMNIST\ndatasets among four types of learners, in scenarios that represent the quickly\nchanging environment. The results show that proactive measures are effective\nand versatile at improving system accuracy, and quickly learning the CO class\nwhen underrepresented in the network. Furthermore, the experiments show a\ntradeoff between FedProx intensity and resource allocation efforts.\nNonetheless, a well adjusted FedProx local optimizer allows for an even better\noverall accuracy, particularly when using deeper neural network (NN)\nimplementations.",
          "link": "http://arxiv.org/abs/2108.02712",
          "publishedOn": "2021-08-06T00:51:47.589Z",
          "wordCount": 691,
          "title": "On Addressing Heterogeneity in Federated Learning for Autonomous Vehicles Connected to a Drone Orchestrator. (arXiv:2108.02712v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wagenmaker_A/0/1/0/all/0/1\">Andrew Wagenmaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1\">Max Simchowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1\">Kevin Jamieson</a>",
          "description": "The theory of reinforcement learning has focused on two fundamental problems:\nachieving low regret, and identifying $\\epsilon$-optimal policies. While a\nsimple reduction allows one to apply a low-regret algorithm to obtain an\n$\\epsilon$-optimal policy and achieve the worst-case optimal rate, it is\nunknown whether low-regret algorithms can obtain the instance-optimal rate for\npolicy identification. We show that this is not possible -- there exists a\nfundamental tradeoff between achieving low regret and identifying an\n$\\epsilon$-optimal policy at the instance-optimal rate.\n\nMotivated by our negative finding, we propose a new measure of\ninstance-dependent sample complexity for PAC tabular reinforcement learning\nwhich explicitly accounts for the attainable state visitation distributions in\nthe underlying MDP. We then propose and analyze a novel, planning-based\nalgorithm which attains this sample complexity -- yielding a complexity which\nscales with the suboptimality gaps and the ``reachability'' of a state. We show\nthat our algorithm is nearly minimax optimal, and on several examples that our\ninstance-dependent sample complexity offers significant improvements over\nworst-case bounds.",
          "link": "http://arxiv.org/abs/2108.02717",
          "publishedOn": "2021-08-06T00:51:47.581Z",
          "wordCount": 597,
          "title": "Beyond No Regret: Instance-Dependent PAC Reinforcement Learning. (arXiv:2108.02717v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02570",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Kim_M/0/1/0/all/0/1\">Minhong Kim</a>",
          "description": "In this paper, machine learning models are used to predict outcomes for\npatients with persistent post-concussion syndrome (PCS). Patients had sustained\na concussion at an average of two to three months before the study. By\nutilizing assessed data, the machine learning models aimed to predict whether\nor not a patient would continue to have PCS after four to five months. The\nrandom forest classifier achieved the highest performance with an 85% accuracy\nand an area under the receiver operating characteristic curve (AUC) of 0.94.\nFactors found to be predictive of PCS outcome were Post-Traumatic Stress\nDisorder (PTSD), perceived injustice, self-rated prognosis, and symptom\nseverity post-injury. The results of this study demonstrate that machine\nlearning models can predict PCS outcomes with high accuracy. With further\nresearch, machine learning models may be implemented in healthcare settings to\nhelp patients with persistent PCS.",
          "link": "http://arxiv.org/abs/2108.02570",
          "publishedOn": "2021-08-06T00:51:47.564Z",
          "wordCount": 567,
          "title": "Predicting Post-Concussion Syndrome Outcomes with Machine Learning. (arXiv:2108.02570v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02431",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Watanabe_C/0/1/0/all/0/1\">Chihiro Watanabe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1\">Taiji Suzuki</a>",
          "description": "Linear layouts are a graph visualization method that can be used to capture\nan entry pattern in an adjacency matrix of a given graph. By reordering the\nnode indices of the original adjacency matrix, linear layouts provide knowledge\nof latent graph structures. Conventional linear layout methods commonly aim to\nfind an optimal reordering solution based on predefined features of a given\nmatrix and loss function. However, prior knowledge of the appropriate features\nto use or structural patterns in a given adjacency matrix is not always\navailable. In such a case, performing the reordering based on data-driven\nfeature extraction without assuming a specific structure in an adjacency matrix\nis preferable. Recently, a neural-network-based matrix reordering method called\nDeepTMR has been proposed to perform this function. However, it is limited to a\ntwo-mode reordering (i.e., the rows and columns are reordered separately) and\nit cannot be applied in the one-mode setting (i.e., the same node order is used\nfor reordering both rows and columns), owing to the characteristics of its\nmodel architecture. In this study, we extend DeepTMR and propose a new one-mode\nlinear layout method referred to as AutoLL. We developed two types of neural\nnetwork models, AutoLL-D and AutoLL-U, for reordering directed and undirected\nnetworks, respectively. To perform one-mode reordering, these AutoLL models\nhave specific encoder architectures, which extract node features from an\nobserved adjacency matrix. We conducted both qualitative and quantitative\nevaluations of the proposed approach, and the experimental results demonstrate\nits effectiveness.",
          "link": "http://arxiv.org/abs/2108.02431",
          "publishedOn": "2021-08-06T00:51:47.558Z",
          "wordCount": 680,
          "title": "AutoLL: Automatic Linear Layout of Graphs based on Deep Neural Network. (arXiv:2108.02431v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hengyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerer_A/0/1/0/all/0/1\">Adam Lerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Brandon Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">David Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1\">Luis Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1\">Noam Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1\">Jakob Foerster</a>",
          "description": "The standard problem setting in Dec-POMDPs is self-play, where the goal is to\nfind a set of policies that play optimally together. Policies learned through\nself-play may adopt arbitrary conventions and implicitly rely on multi-step\nreasoning based on fragile assumptions about other agents' actions and thus\nfail when paired with humans or independently trained agents at test time. To\naddress this, we present off-belief learning (OBL). At each timestep OBL agents\nfollow a policy $\\pi_1$ that is optimized assuming past actions were taken by a\ngiven, fixed policy ($\\pi_0$), but assuming that future actions will be taken\nby $\\pi_1$. When $\\pi_0$ is uniform random, OBL converges to an optimal policy\nthat does not rely on inferences based on other agents' behavior (an optimal\ngrounded policy). OBL can be iterated in a hierarchy, where the optimal policy\nfrom one level becomes the input to the next, thereby introducing multi-level\ncognitive reasoning in a controlled manner. Unlike existing approaches, which\nmay converge to any equilibrium policy, OBL converges to a unique policy,\nmaking it suitable for zero-shot coordination (ZSC). OBL can be scaled to\nhigh-dimensional settings with a fictitious transition mechanism and shows\nstrong performance in both a toy-setting and the benchmark human-AI & ZSC\nproblem Hanabi.",
          "link": "http://arxiv.org/abs/2103.04000",
          "publishedOn": "2021-08-06T00:51:47.552Z",
          "wordCount": 681,
          "title": "Off-Belief Learning. (arXiv:2103.04000v4 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02507",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ge_S/0/1/0/all/0/1\">Shufei Ge</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1\">Shijia Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Elliott_L/0/1/0/all/0/1\">Lloyd Elliott</a>",
          "description": "Shape modelling (with methods that output shapes) is a new and important task\nin Bayesian nonparametrics and bioinformatics. In this work, we focus on\nBayesian nonparametric methods for capturing shapes by partitioning a space\nusing curves. In related work, the classical Mondrian process is used to\npartition spaces recursively with axis-aligned cuts, and is widely applied in\nmulti-dimensional and relational data. The Mondrian process outputs\nhyper-rectangles. Recently, the random tessellation process was introduced as a\ngeneralization of the Mondrian process, partitioning a domain with non-axis\naligned cuts in an arbitrary dimensional space, and outputting polytopes.\nMotivated by these processes, in this work, we propose a novel parallelized\nBayesian nonparametric approach to partition a domain with curves, enabling\ncomplex data-shapes to be acquired. We apply our method to HIV-1-infected human\nmacrophage image dataset, and also simulated datasets sets to illustrate our\napproach. We compare to support vector machines, random forests and\nstate-of-the-art computer vision methods such as simple linear iterative\nclustering super pixel image segmentation. We develop an R package that is\navailable at\n\\url{https://github.com/ShufeiGe/Shape-Modeling-with-Spline-Partitions}.",
          "link": "http://arxiv.org/abs/2108.02507",
          "publishedOn": "2021-08-06T00:51:47.545Z",
          "wordCount": 601,
          "title": "Shape Modeling with Spline Partitions. (arXiv:2108.02507v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_N/0/1/0/all/0/1\">Naili Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1\">Sai Ho Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chenghao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_T/0/1/0/all/0/1\">Teck Khim Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meihui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Gang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ooi_B/0/1/0/all/0/1\">Beng Chin Ooi</a>",
          "description": "Deep learning has achieved great success in a wide spectrum of multimedia\napplications such as image classification, natural language processing and\nmultimodal data analysis. Recent years have seen the development of many deep\nlearning frameworks that provide a high-level programming interface for users\nto design models, conduct training and deploy inference. However, it remains\nchallenging to build an efficient end-to-end multimedia application with most\nexisting frameworks. Specifically, in terms of usability, it is demanding for\nnon-experts to implement deep learning models, obtain the right settings for\nthe entire machine learning pipeline, manage models and datasets, and exploit\nexternal data sources all together. Further, in terms of adaptability, elastic\ncomputation solutions are much needed as the actual serving workload fluctuates\nconstantly, and scaling the hardware resources to handle the fluctuating\nworkload is typically infeasible. To address these challenges, we introduce\nSINGA-Easy, a new deep learning framework that provides distributed\nhyper-parameter tuning at the training stage, dynamic computational cost\ncontrol at the inference stage, and intuitive user interactions with multimedia\ncontents facilitated by model explanation. Our experiments on the training and\ndeployment of multi-modality data analysis applications show that the framework\nis both usable and adaptable to dynamic inference loads. We implement\nSINGA-Easy on top of Apache SINGA and demonstrate our system with the entire\nmachine learning life cycle.",
          "link": "http://arxiv.org/abs/2108.02572",
          "publishedOn": "2021-08-06T00:51:47.491Z",
          "wordCount": 671,
          "title": "SINGA-Easy: An Easy-to-Use Framework for MultiModal Analysis. (arXiv:2108.02572v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02606",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rixner_M/0/1/0/all/0/1\">Maximilian Rixner</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Koutsourelakis_P/0/1/0/all/0/1\">Phaedon-Stelios Koutsourelakis</a>",
          "description": "While the forward and backward modeling of the process-structure-property\nchain has received a lot of attention from the materials community, fewer\nefforts have taken into consideration uncertainties. Those arise from a\nmultitude of sources and their quantification and integration in the inversion\nprocess are essential in meeting the materials design objectives. The first\ncontribution of this paper is a flexible, fully probabilistic formulation of\nsuch optimization problems that accounts for the uncertainty in the\nprocess-structure and structure-property linkages and enables the\nidentification of optimal, high-dimensional, process parameters. We employ a\nprobabilistic, data-driven surrogate for the structure-property link which\nexpedites computations and enables handling of non-differential objectives. We\ncouple this with a novel active learning strategy, i.e. a self-supervised\ncollection of data, which significantly improves accuracy while requiring small\namounts of training data. We demonstrate its efficacy in optimizing the\nmechanical and thermal properties of two-phase, random media but envision its\napplicability encompasses a wide variety of microstructure-sensitive design\nproblems.",
          "link": "http://arxiv.org/abs/2108.02606",
          "publishedOn": "2021-08-06T00:51:47.485Z",
          "wordCount": 593,
          "title": "Self-supervised optimization of random material microstructures in the small-data regime. (arXiv:2108.02606v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lones_M/0/1/0/all/0/1\">Michael A. Lones</a>",
          "description": "This document gives a concise outline of some of the common mistakes that\noccur when using machine learning techniques, and what can be done to avoid\nthem. It is intended primarily as a guide for research students, and focuses on\nissues that are of particular concern within academic research, such as the\nneed to do rigorous comparisons and reach valid conclusions. It covers five\nstages of the machine learning process: what to do before model building, how\nto reliably build models, how to robustly evaluate models, how to compare\nmodels fairly, and how to report results.",
          "link": "http://arxiv.org/abs/2108.02497",
          "publishedOn": "2021-08-06T00:51:47.458Z",
          "wordCount": 530,
          "title": "How to avoid machine learning pitfalls: a guide for academic researchers. (arXiv:2108.02497v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02221",
          "author": "<a href=\"http://arxiv.org/find/hep-th/1/au:+Erbin_H/0/1/0/all/0/1\">Harold Erbin</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Finotello_R/0/1/0/all/0/1\">Riccardo Finotello</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Schneider_R/0/1/0/all/0/1\">Robin Schneider</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Tamaazousti_M/0/1/0/all/0/1\">Mohamed Tamaazousti</a>",
          "description": "We continue earlier efforts in computing the dimensions of tangent space\ncohomologies of Calabi-Yau manifolds using deep learning. In this paper, we\nconsider the dataset of all Calabi-Yau four-folds constructed as complete\nintersections in products of projective spaces. Employing neural networks\ninspired by state-of-the-art computer vision architectures, we improve earlier\nbenchmarks and demonstrate that all four non-trivial Hodge numbers can be\nlearned at the same time using a multi-task architecture. With 30% (80%)\ntraining ratio, we reach an accuracy of 100% for $h^{(1,1)}$ and 97% for\n$h^{(2,1)}$ (100% for both), 81% (96%) for $h^{(3,1)}$, and 49% (83%) for\n$h^{(2,2)}$. Assuming that the Euler number is known, as it is easy to compute,\nand taking into account the linear constraint arising from index computations,\nwe get 100% total accuracy.",
          "link": "http://arxiv.org/abs/2108.02221",
          "publishedOn": "2021-08-06T00:51:47.441Z",
          "wordCount": 569,
          "title": "Deep multi-task mining Calabi-Yau four-folds. (arXiv:2108.02221v1 [hep-th])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendes_P/0/1/0/all/0/1\">Pedro Mendes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casimiro_M/0/1/0/all/0/1\">Maria Casimiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romano_P/0/1/0/all/0/1\">Paolo Romano</a>",
          "description": "In the literature on hyper-parameter tuning, a number of recent solutions\nrely on low-fidelity observations (e.g., training with sub-sampled datasets or\nfor short periods of time) to extrapolate good configurations to use when\nperforming full training. Among these, HyperBand is arguably one of the most\npopular solutions, due to its efficiency and theoretically provable robustness.\nIn this work, we introduce HyperJump, a new approach that builds on HyperBand's\nrobust search strategy and complements it with novel model-based risk analysis\ntechniques that accelerate the search by jumping the evaluation of low risk\nconfigurations, i.e., configurations that are likely to be discarded by\nHyperBand. We evaluate HyperJump on a suite of hyper-parameter optimization\nproblems and show that it provides over one-order of magnitude speed-ups on a\nvariety of deep-learning and kernel-based learning problems when compared to\nHyperBand as well as to a number of state of the art optimizers.",
          "link": "http://arxiv.org/abs/2108.02479",
          "publishedOn": "2021-08-06T00:51:47.433Z",
          "wordCount": 570,
          "title": "HyperJump: Accelerating HyperBand via Risk Modelling. (arXiv:2108.02479v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.00968",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Berrisch_J/0/1/0/all/0/1\">Jonathan Berrisch</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ziel_F/0/1/0/all/0/1\">Florian Ziel</a>",
          "description": "Combination and aggregation techniques can significantly improve forecast\naccuracy. This also holds for probabilistic forecasting methods where\npredictive distributions are combined. There are several time-varying and\nadaptive weighting schemes such as Bayesian model averaging (BMA). However, the\nquality of different forecasts may vary not only over time but also within the\ndistribution. For example, some distribution forecasts may be more accurate in\nthe center of the distributions, while others are better at predicting the\ntails. Therefore, we introduce a new weighting method that considers the\ndifferences in performance over time and within the distribution. We discuss\npointwise combination algorithms based on aggregation across quantiles that\noptimize with respect to the continuous ranked probability score (CRPS). After\nanalyzing the theoretical properties of pointwise CRPS learning, we discuss B-\nand P-Spline-based estimation techniques for batch and online learning, based\non quantile regression and prediction with expert advice. We prove that the\nproposed fully adaptive Bernstein online aggregation (BOA) method for pointwise\nCRPS online learning has optimal convergence properties. They are confirmed in\nsimulations and a probabilistic forecasting study for European emission\nallowance (EUA) prices.",
          "link": "http://arxiv.org/abs/2102.00968",
          "publishedOn": "2021-08-06T00:51:47.426Z",
          "wordCount": 661,
          "title": "CRPS Learning. (arXiv:2102.00968v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khashin_S/0/1/0/all/0/1\">Sergey Khashin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shemyakova_E/0/1/0/all/0/1\">Ekaterina Shemyakova</a>",
          "description": "We propose a new kind of automatic architecture search algorithm. The\nalgorithm alternates pruning connections and adding neurons, and it is not\nrestricted to layered architectures only. Here architecture is an arbitrary\noriented graph with some weights (along with some biases and an activation\nfunction), so there may be no layered structure in such a network. The\nalgorithm minimizes the complexity of staying within a given error. We\ndemonstrate our algorithm on the brightness prediction problem of the next\npoint through the previous points on an image. Our second test problem is the\napproximation of the bivariate function defining the brightness of a black and\nwhite image. Our optimized networks significantly outperform the standard\nsolution for neural network architectures in both cases.",
          "link": "http://arxiv.org/abs/2108.02231",
          "publishedOn": "2021-08-06T00:51:47.399Z",
          "wordCount": 555,
          "title": "Growing an architecture for a neural network. (arXiv:2108.02231v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1\">Juan P. Vigueras-Guill&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1\">Frank Seeliger</a>",
          "description": "The automated segmentation of cancer tissue in histopathology images can help\nclinicians to detect, diagnose, and analyze such disease. Different from other\nnatural images used in many convolutional networks for benchmark,\nhistopathology images can be extremely large, and the cancerous patterns can\nreach beyond 1000 pixels. Therefore, the well-known networks in the literature\nwere never conceived to handle these peculiarities. In this work, we propose a\nFully Convolutional DenseUNet that is particularly designed to solve\nhistopathology problems. We evaluated our network in two public pathology\ndatasets published as challenges in the recent MICCAI 2019: binary segmentation\nin colon cancer images (DigestPath2019), and multi-class segmentation in\nprostate cancer images (Gleason2019), achieving similar and better results than\nthe winners of the challenges, respectively. Furthermore, we discussed some\ngood practices in the training setup to yield the best performance and the main\nchallenges in these histopathology datasets.",
          "link": "http://arxiv.org/abs/2108.02676",
          "publishedOn": "2021-08-06T00:51:47.393Z",
          "wordCount": 609,
          "title": "Redesigning Fully Convolutional DenseUNets for Large Histopathology Images. (arXiv:2108.02676v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asi_H/0/1/0/all/0/1\">Hilal Asi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_D/0/1/0/all/0/1\">Daniel Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duchi_J/0/1/0/all/0/1\">John Duchi</a>",
          "description": "We develop algorithms for private stochastic convex optimization that adapt\nto the hardness of the specific function we wish to optimize. While previous\nwork provide worst-case bounds for arbitrary convex functions, it is often the\ncase that the function at hand belongs to a smaller class that enjoys faster\nrates. Concretely, we show that for functions exhibiting $\\kappa$-growth around\nthe optimum, i.e., $f(x) \\ge f(x^*) + \\lambda \\kappa^{-1} \\|x-x^*\\|_2^\\kappa$\nfor $\\kappa > 1$, our algorithms improve upon the standard\n${\\sqrt{d}}/{n\\varepsilon}$ privacy rate to the faster\n$({\\sqrt{d}}/{n\\varepsilon})^{\\tfrac{\\kappa}{\\kappa - 1}}$. Crucially, they\nachieve these rates without knowledge of the growth constant $\\kappa$ of the\nfunction. Our algorithms build upon the inverse sensitivity mechanism, which\nadapts to instance difficulty (Asi & Duchi, 2020), and recent localization\ntechniques in private optimization (Feldman et al., 2020). We complement our\nalgorithms with matching lower bounds for these function classes and\ndemonstrate that our adaptive algorithm is \\emph{simultaneously} (minimax)\noptimal over all $\\kappa \\ge 1+c$ whenever $c = \\Theta(1)$.",
          "link": "http://arxiv.org/abs/2108.02391",
          "publishedOn": "2021-08-06T00:51:47.369Z",
          "wordCount": 613,
          "title": "Adapting to Function Difficulty and Growth Conditions in Private Optimization. (arXiv:2108.02391v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.14621",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Williamson_S/0/1/0/all/0/1\">Sinead A. Williamson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Henderson_J/0/1/0/all/0/1\">Jette Henderson</a>",
          "description": "Understanding how two datasets differ can help us determine whether one\ndataset under-represents certain sub-populations, and provides insights into\nhow well models will generalize across datasets. Representative points selected\nby a maximum mean discrepency (MMD) coreset can provide interpretable summaries\nof a single dataset, but are not easily compared across datasets. In this paper\nwe introduce dependent MMD coresets, a data summarization method for\ncollections of datasets that facilitates comparison of distributions. We show\nthat dependent MMD coresets are useful for understanding multiple related\ndatasets and understanding model generalization between such datasets.",
          "link": "http://arxiv.org/abs/2006.14621",
          "publishedOn": "2021-08-06T00:51:47.344Z",
          "wordCount": 544,
          "title": "Understanding collections of related datasets using dependent MMD coresets. (arXiv:2006.14621v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaming Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shaohui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1\">Qi Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zidong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunji Chen</a>",
          "description": "Policy gradient methods are appealing in deep reinforcement learning but\nsuffer from high variance of gradient estimate. To reduce the variance, the\nstate value function is applied commonly. However, the effect of the state\nvalue function becomes limited in stochastic dynamic environments, where the\nunexpected state dynamics and rewards will increase the variance. In this\npaper, we propose to replace the state value function with a novel hindsight\nvalue function, which leverages the information from the future to reduce the\nvariance of the gradient estimate for stochastic dynamic environments.\n\nParticularly, to obtain an ideally unbiased gradient estimate, we propose an\ninformation-theoretic approach, which optimizes the embeddings of the future to\nbe independent of previous actions. In our experiments, we apply the proposed\nhindsight value function in stochastic dynamic environments, including\ndiscrete-action environments and continuous-action environments. Compared with\nthe standard state value function, the proposed hindsight value function\nconsistently reduces the variance, stabilizes the training, and improves the\neventual policy.",
          "link": "http://arxiv.org/abs/2107.12216",
          "publishedOn": "2021-08-06T00:51:47.329Z",
          "wordCount": 642,
          "title": "Hindsight Value Function for Variance Reduction in Stochastic Dynamic Environment. (arXiv:2107.12216v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>",
          "description": "Despite tremendous progress in missing data imputation task, designing new\nimputation models has become more and more cumbersome but the corresponding\ngains are relatively small. Is there any simple but general approach that can\nexploit the existing models to further improve the quality of the imputation?\nIn this article, we aim to respond to this concern and propose a novel general\ndata augmentation method called Missingness Augmentation (MA), which can be\napplied in many existing generative imputation frameworks to further improve\nthe performance of these models. For MA, before each training epoch, we use the\noutputs of the generator to expand the incomplete samples on the fly, and then\ndetermine a special reconstruction loss for these augmented samples. This\nreconstruction loss plus the original loss constitutes the final optimization\nobjective of the model. It is noteworthy that MA is very efficient and does not\nneed to change the structure of the original model. Experimental results\ndemonstrate that MA can significantly improve the performance of many recently\ndeveloped generative imputation models on a variety of datasets. Our code is\navailable at https://github.com/WYu-Feng/Missingness-Augmentation.",
          "link": "http://arxiv.org/abs/2108.02566",
          "publishedOn": "2021-08-06T00:51:47.242Z",
          "wordCount": 620,
          "title": "Missingness Augmentation: A General Approach for Improving Generative Imputation Models. (arXiv:2108.02566v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02289",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_K/0/1/0/all/0/1\">Kaiming Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chih-Hang J. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Arieh_D/0/1/0/all/0/1\">David Ben-Arieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1\">Ashesh Sinha</a>",
          "description": "At present, high-dimensional global optimization problems with time-series\nmodels have received much attention from engineering fields. Since it was\nproposed, Bayesian optimization has quickly become a popular and promising\napproach for solving global optimization problems. However, the standard\nBayesian optimization algorithm is insufficient to solving the global optimal\nsolution when the model is high-dimensional. Hence, this paper presents a novel\nhigh dimensional Bayesian optimization algorithm by considering dimension\nreduction and different dimension fill-in strategies. Most existing literature\nabout Bayesian optimization algorithms did not discuss the sampling strategies\nto optimize the acquisition function. This study proposed a new sampling method\nbased on both the multi-armed bandit and random search methods while optimizing\nthe acquisition function. Besides, based on the time-dependent or\ndimension-dependent characteristics of the model, the proposed algorithm can\nreduce the dimension evenly. Then, five different dimension fill-in strategies\nwere discussed and compared in this study. Finally, to increase the final\naccuracy of the optimal solution, the proposed algorithm adds a local search\nbased on a series of Adam-based steps at the final stage. Our computational\nexperiments demonstrated that the proposed Bayesian optimization algorithm\ncould achieve reasonable solutions with excellent performances for high\ndimensional global optimization problems with a time-series optimal control\nmodel.",
          "link": "http://arxiv.org/abs/2108.02289",
          "publishedOn": "2021-08-06T00:51:47.223Z",
          "wordCount": 659,
          "title": "High dimensional Bayesian Optimization Algorithm for Complex System in Time Series. (arXiv:2108.02289v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.02703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1\">Stephen Bates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1\">Anastasios Angelopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1\">Lihua Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "While improving prediction accuracy has been the focus of machine learning in\nrecent years, this alone does not suffice for reliable decision-making.\nDeploying learning systems in consequential settings also requires calibrating\nand communicating the uncertainty of predictions. To convey instance-wise\nuncertainty for prediction tasks, we show how to generate set-valued\npredictions from a black-box predictor that control the expected loss on future\ntest points at a user-specified level. Our approach provides explicit\nfinite-sample guarantees for any dataset by using a holdout set to calibrate\nthe size of the prediction sets. This framework enables simple,\ndistribution-free, rigorous error control for many tasks, and we demonstrate it\nin five large-scale machine learning problems: (1) classification problems\nwhere some mistakes are more costly than others; (2) multi-label\nclassification, where each observation has multiple associated labels; (3)\nclassification problems where the labels have a hierarchical structure; (4)\nimage segmentation, where we wish to predict a set of pixels containing an\nobject of interest; and (5) protein structure prediction. Lastly, we discuss\nextensions to uncertainty quantification for ranking, metric learning and\ndistributionally robust learning.",
          "link": "http://arxiv.org/abs/2101.02703",
          "publishedOn": "2021-08-06T00:51:47.208Z",
          "wordCount": 677,
          "title": "Distribution-Free, Risk-Controlling Prediction Sets. (arXiv:2101.02703v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02283",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Bai_Y/0/1/0/all/0/1\">Yang Bai</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Pukthuanthong_K/0/1/0/all/0/1\">Kuntara Pukthuanthong</a>",
          "description": "We design a novel framework to examine market efficiency through\nout-of-sample (OOS) predictability. We frame the asset pricing problem as a\nmachine learning classification problem and construct classification models to\npredict return states. The prediction-based portfolios beat the market with\nsignificant OOS economic gains. We measure prediction accuracies directly. For\neach model, we introduce a novel application of binomial test to test the\naccuracy of 3.34 million return state predictions. The tests show that our\nmodels can extract useful contents from historical information to predict\nfuture return states. We provide unique economic insights about OOS\npredictability and machine learning models.",
          "link": "http://arxiv.org/abs/2108.02283",
          "publishedOn": "2021-08-06T00:51:47.182Z",
          "wordCount": 556,
          "title": "Machine Learning Classification Methods and Portfolio Allocation: An Examination of Market Efficiency. (arXiv:2108.02283v1 [q-fin.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baranchuk_D/0/1/0/all/0/1\">Dmitry Baranchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aliev_V/0/1/0/all/0/1\">Vladimir Aliev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1\">Artem Babenko</a>",
          "description": "Normalizing flows are a powerful class of generative models demonstrating\nstrong performance in several speech and vision problems. In contrast to other\ngenerative models, normalizing flows are latent variable models with tractable\nlikelihoods and allow for stable training. However, they have to be carefully\ndesigned to represent invertible functions with efficient Jacobian determinant\ncalculation. In practice, these requirements lead to overparameterized and\nsophisticated architectures that are inferior to alternative feed-forward\nmodels in terms of inference time and memory consumption. In this work, we\ninvestigate whether one can distill flow-based models into more efficient\nalternatives. We provide a positive answer to this question by proposing a\nsimple distillation approach and demonstrating its effectiveness on\nstate-of-the-art conditional flow-based models for image super-resolution and\nspeech synthesis.",
          "link": "http://arxiv.org/abs/2106.12699",
          "publishedOn": "2021-08-06T00:51:47.176Z",
          "wordCount": 589,
          "title": "Distilling the Knowledge from Conditional Normalizing Flows. (arXiv:2106.12699v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.02156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mu_C/0/1/0/all/0/1\">Cong Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mele_A/0/1/0/all/0/1\">Angelo Mele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_L/0/1/0/all/0/1\">Lingxin Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cape_J/0/1/0/all/0/1\">Joshua Cape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athreya_A/0/1/0/all/0/1\">Avanti Athreya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1\">Carey E. Priebe</a>",
          "description": "In network inference applications, it is often desirable to detect community\nstructure, namely to cluster vertices into groups, or blocks, according to some\nmeasure of similarity. Beyond mere adjacency matrices, many real networks also\ninvolve vertex covariates that carry key information about underlying block\nstructure in graphs. To assess the effects of such covariates on block\nrecovery, we present a comparative analysis of two model-based spectral\nalgorithms for clustering vertices in stochastic blockmodel graphs with vertex\ncovariates. The first algorithm uses only the adjacency matrix, and directly\nestimates the block assignments. The second algorithm incorporates both the\nadjacency matrix and the vertex covariates into the estimation of block\nassignments, and moreover quantifies the explicit impact of the vertex\ncovariates on the resulting estimate of the block assignments. We employ\nChernoff information to analytically compare the algorithms' performance and\nderive the information-theoretic Chernoff ratio for certain models of interest.\nAnalytic results and simulations suggest that the second algorithm is often\npreferred: we can often better estimate the induced block assignments by first\nestimating the effect of vertex covariates. In addition, real data examples\nalso indicate that the second algorithm has the advantages of revealing\nunderlying block structure and taking observed vertex heterogeneity into\naccount in real applications. Our findings emphasize the importance of\ndistinguishing between observed and unobserved factors that can affect block\nstructure in graphs.",
          "link": "http://arxiv.org/abs/2007.02156",
          "publishedOn": "2021-08-06T00:51:47.161Z",
          "wordCount": 735,
          "title": "On spectral algorithms for community detection in stochastic blockmodel graphs with vertex covariates. (arXiv:2007.02156v3 [cs.SI] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khorrami_K/0/1/0/all/0/1\">Khazar Khorrami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>",
          "description": "Systems that can find correspondences between multiple modalities, such as\nbetween speech and images, have great potential to solve different recognition\nand data analysis tasks in an unsupervised manner. This work studies multimodal\nlearning in the context of visually grounded speech (VGS) models, and focuses\non their recently demonstrated capability to extract spatiotemporal alignments\nbetween spoken words and the corresponding visual objects without ever been\nexplicitly trained for object localization or word recognition. As the main\ncontributions, we formalize the alignment problem in terms of an audiovisual\nalignment tensor that is based on earlier VGS work, introduce systematic\nmetrics for evaluating model performance in aligning visual objects and spoken\nwords, and propose a new VGS model variant for the alignment task utilizing\ncross-modal attention layer. We test our model and a previously proposed model\nin the alignment task using SPEECH-COCO captions coupled with MSCOCO images. We\ncompare the alignment performance using our proposed evaluation metrics to the\nsemantic retrieval task commonly used to evaluate VGS models. We show that\ncross-modal attention layer not only helps the model to achieve higher semantic\ncross-modal retrieval performance, but also leads to substantial improvements\nin the alignment performance between image object and spoken words.",
          "link": "http://arxiv.org/abs/2108.02562",
          "publishedOn": "2021-08-06T00:51:47.141Z",
          "wordCount": 652,
          "title": "Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models. (arXiv:2108.02562v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tan M. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suliafu_V/0/1/0/all/0/1\">Vai Suliafu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osher_S/0/1/0/all/0/1\">Stanley J. Osher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bao Wang</a>",
          "description": "We propose FMMformers, a class of efficient and flexible transformers\ninspired by the celebrated fast multipole method (FMM) for accelerating\ninteracting particle simulation. FMM decomposes particle-particle interaction\ninto near-field and far-field components and then performs direct and\ncoarse-grained computation, respectively. Similarly, FMMformers decompose the\nattention into near-field and far-field attention, modeling the near-field\nattention by a banded matrix and the far-field attention by a low-rank matrix.\nComputing the attention matrix for FMMformers requires linear complexity in\ncomputational time and memory footprint with respect to the sequence length. In\ncontrast, standard transformers suffer from quadratic complexity. We analyze\nand validate the advantage of FMMformers over the standard transformer on the\nLong Range Arena and language modeling benchmarks. FMMformers can even\noutperform the standard transformer in terms of accuracy by a significant\nmargin. For instance, FMMformers achieve an average classification accuracy of\n$60.74\\%$ over the five Long Range Arena tasks, which is significantly better\nthan the standard transformer's average accuracy of $58.70\\%$.",
          "link": "http://arxiv.org/abs/2108.02347",
          "publishedOn": "2021-08-06T00:51:47.134Z",
          "wordCount": 620,
          "title": "FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention. (arXiv:2108.02347v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alkhouri_I/0/1/0/all/0/1\">Ismail Alkhouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velasquez_A/0/1/0/all/0/1\">Alvaro Velasquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atia_G/0/1/0/all/0/1\">George Atia</a>",
          "description": "The design of additive imperceptible perturbations to the inputs of deep\nclassifiers to maximize their misclassification rates is a central focus of\nadversarial machine learning. An alternative approach is to synthesize\nadversarial examples from scratch using GAN-like structures, albeit with the\nuse of large amounts of training data. By contrast, this paper considers\none-shot synthesis of adversarial examples; the inputs are synthesized from\nscratch to induce arbitrary soft predictions at the output of pre-trained\nmodels, while simultaneously maintaining high similarity to specified inputs.\nTo this end, we present a problem that encodes objectives on the distance\nbetween the desired and output distributions of the trained model and the\nsimilarity between such inputs and the synthesized examples. We prove that the\nformulated problem is NP-complete. Then, we advance a generative approach to\nthe solution in which the adversarial examples are obtained as the output of a\ngenerative network whose parameters are iteratively updated by optimizing\nsurrogate loss functions for the dual-objective. We demonstrate the generality\nand versatility of the framework and approach proposed through applications to\nthe design of targeted adversarial attacks, generation of decision boundary\nsamples, and synthesis of low confidence classification inputs. The approach is\nfurther extended to an ensemble of models with different soft output\nspecifications. The experimental results verify that the targeted and\nconfidence reduction attack methods developed perform on par with\nstate-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2108.02756",
          "publishedOn": "2021-08-06T00:51:47.128Z",
          "wordCount": 652,
          "title": "BOSS: Bidirectional One-Shot Synthesis of Adversarial Examples. (arXiv:2108.02756v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.10861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shunqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurkoski_B/0/1/0/all/0/1\">Brian M. Kurkoski</a>",
          "description": "Approximate message passing (AMP) is a low-cost iterative\nparameter-estimation technique for certain high-dimensional linear systems with\nnon-Gaussian distributions. However, AMP only applies to independent\nidentically distributed (IID) transform matrices, but may become unreliable\n(e.g. perform poorly or even diverge) for other matrix ensembles, especially\nfor ill-conditioned ones. Orthogonal/vector AMP (OAMP/VAMP) was proposed for\ngeneral right-unitarily-invariant matrices to handle this difficulty. However,\nthe Bayes-optimal OAMP/VAMP requires a high-complexity linear minimum mean\nsquare error (MMSE) estimator. This limits the application of OAMP/VAMP to\nlarge-scale systems.\n\nTo solve the disadvantages of AMP and OAMP/VAMP, this paper proposes a memory\nAMP (MAMP) framework under an orthogonality principle, which guarantees the\nasymptotic IID Gaussianity of estimation errors in MAMP. We present an\northogonalization procedure for the local memory estimators to realize the\nrequired orthogonality for MAMP. Furthermore, we propose a Bayes-optimal MAMP\n(BO-MAMP), in which a long-memory matched filter is proposed for interference\nsuppression. The complexity of BO-MAMP is comparable to AMP. A state evolution\nis derived to asymptotically characterize the performance of BO-MAMP. Based on\nstate evolution, the relaxation parameters and damping vector in BO-MAMP are\noptimized. For all right-unitarily-invariant matrices, the optimized BO-MAMP\nconverges to the high-complexity OAMP/VAMP, and thus is Bayes-optimal if it has\na unique fixed point. Finally, simulations are provided to verify the validity\nand accuracy of the theoretical results.",
          "link": "http://arxiv.org/abs/2012.10861",
          "publishedOn": "2021-08-06T00:51:47.077Z",
          "wordCount": 755,
          "title": "Memory AMP. (arXiv:2012.10861v4 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Powell_B/0/1/0/all/0/1\">Brian A. Powell</a>",
          "description": "Adversarial lateral movement via compromised accounts remains difficult to\ndiscover via traditional rule-based defenses because it generally lacks\nexplicit indicators of compromise. We propose a behavior-based, unsupervised\nframework comprising two methods of lateral movement detection on enterprise\nnetworks: one aimed at generic lateral movement via either exploit or\nauthenticated connections, and one targeting the specific techniques of process\ninjection and hijacking. The first method is based on the premise that the role\nof a system---the functions it performs on the network---determines the roles\nof the systems it should make connections with. The adversary meanwhile might\nmove between any systems whatever, possibly seeking out systems with unusual\nroles that facilitate certain accesses. We use unsupervised learning to cluster\nsystems according to role and identify connections to systems with novel roles\nas potentially malicious. The second method is based on the premise that the\ntemporal patterns of inter-system processes that facilitate these connections\ndepend on the roles of the systems involved. If a process is compromised by an\nattacker, these normal patterns might be disrupted in discernible ways. We\napply frequent-itemset mining to process sequences to establish regular\npatterns of communication between systems based on role, and identify rare\nprocess sequences as signalling potentially malicious connections.",
          "link": "http://arxiv.org/abs/2108.02713",
          "publishedOn": "2021-08-06T00:51:47.013Z",
          "wordCount": 642,
          "title": "Role-based lateral movement detection with unsupervised learning. (arXiv:2108.02713v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1\">Tal Ridnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1\">Emanuel Ben-Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1\">Lihi Zelnik-Manor</a>",
          "description": "ImageNet-1K serves as the primary dataset for pretraining deep learning\nmodels for computer vision tasks. ImageNet-21K dataset, which is bigger and\nmore diverse, is used less frequently for pretraining, mainly due to its\ncomplexity, low accessibility, and underestimation of its added value. This\npaper aims to close this gap, and make high-quality efficient pretraining on\nImageNet-21K available for everyone. Via a dedicated preprocessing stage,\nutilization of WordNet hierarchical structure, and a novel training scheme\ncalled semantic softmax, we show that various models significantly benefit from\nImageNet-21K pretraining on numerous datasets and tasks, including small\nmobile-oriented models. We also show that we outperform previous ImageNet-21K\npretraining schemes for prominent new models like ViT and Mixer. Our proposed\npretraining pipeline is efficient, accessible, and leads to SoTA reproducible\nresults, from a publicly available dataset. The training code and pretrained\nmodels are available at: https://github.com/Alibaba-MIIL/ImageNet21K",
          "link": "http://arxiv.org/abs/2104.10972",
          "publishedOn": "2021-08-06T00:51:47.007Z",
          "wordCount": 632,
          "title": "ImageNet-21K Pretraining for the Masses. (arXiv:2104.10972v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10234",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fanglan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1\">Taoran Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Kaiqun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1\">Charu Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chang-Tien Lu</a>",
          "description": "Deep learning's performance has been extensively recognized recently. Graph\nneural networks (GNNs) are designed to deal with graph-structural data that\nclassical deep learning does not easily manage. Since most GNNs were created\nusing distinct theories, direct comparisons are impossible. Prior research has\nprimarily concentrated on categorizing existing models, with little attention\npaid to their intrinsic connections. The purpose of this study is to establish\na unified framework that integrates GNNs based on spectral graph and\napproximation theory. The framework incorporates a strong integration between\nspatial- and spectral-based GNNs while tightly associating approaches that\nexist within each respective domain.",
          "link": "http://arxiv.org/abs/2107.10234",
          "publishedOn": "2021-08-06T00:51:47.000Z",
          "wordCount": 589,
          "title": "Bridging the Gap between Spatial and Spectral Domains: A Unified Framework for Graph Neural Networks. (arXiv:2107.10234v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1\">Juan P. Vigueras-Guill&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1\">Frank Seeliger</a>",
          "description": "Regularization in convolutional neural networks (CNNs) is usually addressed\nwith dropout layers. However, dropout is sometimes detrimental in the\nconvolutional part of a CNN as it simply sets to zero a percentage of pixels in\nthe feature maps, adding unrepresentative examples during training. Here, we\npropose a CNN layer that performs regularization by applying random rotations\nof reflections to a small percentage of feature maps after every convolutional\nlayer. We prove how this concept is beneficial for images with orientational\nsymmetries, such as in medical images, as it provides a certain degree of\nrotational invariance. We tested this method in two datasets, a patch-based set\nof histopathology images (PatchCamelyon) to perform classification using a\ngeneric DenseNet, and a set of specular microscopy images of the corneal\nendothelium to perform segmentation using a tailored U-net, improving the\nperformance in both cases.",
          "link": "http://arxiv.org/abs/2108.02704",
          "publishedOn": "2021-08-06T00:51:46.986Z",
          "wordCount": 615,
          "title": "Rotaflip: A New CNN Layer for Regularization and Rotational Invariance in Medical Images. (arXiv:2108.02704v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00001",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singhal_V/0/1/0/all/0/1\">Vikrant Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinke_T/0/1/0/all/0/1\">Thomas Steinke</a>",
          "description": "Private data analysis suffers a costly curse of dimensionality. However, the\ndata often has an underlying low-dimensional structure. For example, when\noptimizing via gradient descent, the gradients often lie in or near a\nlow-dimensional subspace. If that low-dimensional structure can be identified,\nthen we can avoid paying (in terms of privacy or accuracy) for the high ambient\ndimension.\n\nWe present differentially private algorithms that take input data sampled\nfrom a low-dimensional linear subspace (possibly with a small amount of error)\nand output that subspace (or an approximation to it). These algorithms can\nserve as a pre-processing step for other procedures.",
          "link": "http://arxiv.org/abs/2106.00001",
          "publishedOn": "2021-08-06T00:51:46.961Z",
          "wordCount": 559,
          "title": "Privately Learning Subspaces. (arXiv:2106.00001v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00531",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pourreza_R/0/1/0/all/0/1\">Reza Pourreza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_T/0/1/0/all/0/1\">Taco S Cohen</a>",
          "description": "While most neural video codecs address P-frame coding (predicting each frame\nfrom past ones), in this paper we address B-frame compression (predicting\nframes using both past and future reference frames). Our B-frame solution is\nbased on the existing P-frame methods. As a result, B-frame coding capability\ncan easily be added to an existing neural codec. The basic idea of our B-frame\ncoding method is to interpolate the two reference frames to generate a single\nreference frame and then use it together with an existing P-frame codec to\nencode the input B-frame. Our studies show that the interpolated frame is a\nmuch better reference for the P-frame codec compared to using the previous\nframe as is usually done. Our results show that using the proposed method with\nan existing P-frame codec can lead to 28.5%saving in bit-rate on the UVG\ndataset compared to the P-frame codec while generating the same video quality.",
          "link": "http://arxiv.org/abs/2104.00531",
          "publishedOn": "2021-08-06T00:51:46.953Z",
          "wordCount": 614,
          "title": "Extending Neural P-frame Codecs for B-frame Coding. (arXiv:2104.00531v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xubo Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nouiehed_M/0/1/0/all/0/1\">Maher Nouiehed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kontar_R/0/1/0/all/0/1\">Raed Al Kontar</a>",
          "description": "In this paper we propose \\texttt{GIFAIR-FL}: an approach that imposes group\nand individual fairness to federated learning settings. By adding a\nregularization term, our algorithm penalizes the spread in the loss of client\ngroups to drive the optimizer to fair solutions. Theoretically, we show\nconvergence in non-convex and strongly convex settings. Our convergence\nguarantees hold for both $i.i.d.$ and non-$i.i.d.$ data. To demonstrate the\nempirical performance of our algorithm, we apply our method on image\nclassification and text prediction tasks. Compared to existing algorithms, our\nmethod shows improved fairness results while retaining superior or similar\nprediction accuracy.",
          "link": "http://arxiv.org/abs/2108.02741",
          "publishedOn": "2021-08-06T00:51:46.945Z",
          "wordCount": 539,
          "title": "GIFAIR-FL: An Approach for Group and Individual Fairness in Federated Learning. (arXiv:2108.02741v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Russel_R/0/1/0/all/0/1\">Reazul Hasan Russel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benosman_M/0/1/0/all/0/1\">Mouhacine Benosman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baar_J/0/1/0/all/0/1\">Jeroen Van Baar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corcodel_R/0/1/0/all/0/1\">Radu Corcodel</a>",
          "description": "Safety and robustness are two desired properties for any reinforcement\nlearning algorithm. CMDPs can handle additional safety constraints and RMDPs\ncan perform well under model uncertainties. In this paper, we propose to unite\nthese two frameworks resulting in robust constrained MDPs (RCMDPs). The\nmotivation is to develop a framework that can satisfy safety constraints while\nalso simultaneously offer robustness to model uncertainties. We develop the\nRCMDP objective, derive gradient update formula to optimize this objective and\nthen propose policy gradient based algorithms. We also independently propose\nLyapunov based reward shaping for RCMDPs, yielding better stability and\nconvergence properties.",
          "link": "http://arxiv.org/abs/2108.02701",
          "publishedOn": "2021-08-06T00:51:46.938Z",
          "wordCount": 544,
          "title": "Lyapunov Robust Constrained-MDPs: Soft-Constrained Robustly Stable Policy Optimization under Model Uncertainty. (arXiv:2108.02701v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02081",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Vargas_F/0/1/0/all/0/1\">Francisco Vargas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thodoroff_P/0/1/0/all/0/1\">Pierre Thodoroff</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lawrence_N/0/1/0/all/0/1\">Neil D. Lawrence</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lamacraft_A/0/1/0/all/0/1\">Austen Lamacraft</a>",
          "description": "The Schr\\\"odinger bridge problem (SBP) finds the most likely stochastic\nevolution between two probability distributions given a prior stochastic\nevolution. As well as applications in the natural sciences, problems of this\nkind have important applications in machine learning such as dataset alignment\nand hypothesis testing. Whilst the theory behind this problem is relatively\nmature, scalable numerical recipes to estimate the Schr\\\"odinger bridge remain\nan active area of research. We prove an equivalence between the SBP and maximum\nlikelihood estimation enabling direct application of successful machine\nlearning techniques. We propose a numerical procedure to estimate SBPs using\nGaussian process and demonstrate the practical usage of our approach in\nnumerical simulations and experiments.",
          "link": "http://arxiv.org/abs/2106.02081",
          "publishedOn": "2021-08-06T00:51:46.931Z",
          "wordCount": 585,
          "title": "Solving Schr\\\"odinger Bridges via Maximum Likelihood. (arXiv:2106.02081v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dabeen Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vojnovic_M/0/1/0/all/0/1\">Milan Vojnovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>",
          "description": "Motivated by recent developments in designing algorithms based on individual\nitem scores for solving utility maximization problems, we study the framework\nof using test scores, defined as a statistic of observed individual item\nperformance data, for solving the budgeted stochastic utility maximization\nproblem. We extend an existing scoring mechanism, namely the replication test\nscores, to incorporate heterogeneous item costs as well as item values. We show\nthat a natural greedy algorithm that selects items solely based on their\nreplication test scores outputs solutions within a constant factor of the\noptimum for a broad class of utility functions. Our algorithms and\napproximation guarantees assume that test scores are noisy estimates of certain\nexpected values with respect to marginal distributions of individual item\nvalues, thus making our algorithms practical and extending previous work that\nassumes noiseless estimates. Moreover, we show how our algorithm can be adapted\nto the setting where items arrive in a streaming fashion while maintaining the\nsame approximation guarantee. We present numerical results, using synthetic\ndata and data sets from the Academia.StackExchange Q&A forum, which show that\nour test score algorithm can achieve competitiveness, and in some cases better\nperformance than a benchmark algorithm that requires access to a value oracle\nto evaluate function values.",
          "link": "http://arxiv.org/abs/2012.15194",
          "publishedOn": "2021-08-06T00:51:46.910Z",
          "wordCount": 675,
          "title": "Test Score Algorithms for Budgeted Stochastic Utility Maximization. (arXiv:2012.15194v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhiying Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Raphael Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Ji Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>",
          "description": "Pretrained transformers achieve the state of the art across tasks in natural\nlanguage processing, motivating researchers to investigate their inner\nmechanisms. One common direction is to understand what features are important\nfor prediction. In this paper, we apply information bottlenecks to analyze the\nattribution of each feature for prediction on a black-box model. We use BERT as\nthe example and evaluate our approach both quantitatively and qualitatively. We\nshow the effectiveness of our method in terms of attribution and the ability to\nprovide insight into how information flows through layers. We demonstrate that\nour technique outperforms two competitive methods in degradation tests on four\ndatasets. Code is available at https://github.com/bazingagin/IBA.",
          "link": "http://arxiv.org/abs/2012.13838",
          "publishedOn": "2021-08-06T00:51:46.903Z",
          "wordCount": 594,
          "title": "Inserting Information Bottlenecks for Attribution in Transformers. (arXiv:2012.13838v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_B/0/1/0/all/0/1\">Binayak Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yingtao Jiang</a>",
          "description": "The increasing popularity of deep neural network (DNN) applications demands\nhigh computing power and efficient hardware accelerator architecture. DNN\naccelerators use a large number of processing elements (PEs) and on-chip memory\nfor storing weights and other parameters. As the communication backbone of a\nDNN accelerator, networks-on-chip (NoC) play an important role in supporting\nvarious dataflow patterns and enabling processing with communication\nparallelism in a DNN accelerator. However, the widely used mesh-based NoC\narchitectures inherently cannot support the efficient one-to-many and\nmany-to-one traffic largely existing in DNN workloads. In this paper, we\npropose a modified mesh architecture with a one-way/two-way streaming bus to\nspeedup one-to-many (multicast) traffic, and the use of gather packets to\nsupport many-to-one (gather) traffic. The analysis of the runtime latency of a\nconvolutional layer shows that the two-way streaming architecture achieves\nbetter improvement than the one-way streaming architecture for an Output\nStationary (OS) dataflow architecture. The simulation results demonstrate that\nthe gather packets can help to reduce the runtime latency up to 1.8 times and\nnetwork power consumption up to 1.7 times, compared with the repetitive unicast\nmethod on modified mesh architectures supporting two-way streaming.",
          "link": "http://arxiv.org/abs/2108.02569",
          "publishedOn": "2021-08-06T00:51:46.896Z",
          "wordCount": 644,
          "title": "Data Streaming and Traffic Gathering in Mesh-based NoC for Deep Neural Network Acceleration. (arXiv:2108.02569v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02594",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Perera_S/0/1/0/all/0/1\">Shanaka Perera</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Aglietti_V/0/1/0/all/0/1\">Virginia Aglietti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Damoulas_T/0/1/0/all/0/1\">Theodoros Damoulas</a>",
          "description": "We study the problem of estimating potential revenue or demand at business\nfacilities and understanding its generating mechanism. This problem arises in\ndifferent fields such as operation research or urban science, and more\ngenerally, it is crucial for businesses' planning and decision making. We\ndevelop a Bayesian spatial interaction model, henceforth BSIM, which provides\nprobabilistic predictions about revenues generated by a particular business\nlocation provided their features and the potential customers' characteristics\nin a given region. BSIM explicitly accounts for the competition among the\ncompetitive facilities through a probability value determined by evaluating a\nstore-specific Gaussian distribution at a given customer location. We propose a\nscalable variational inference framework that, while being significantly faster\nthan competing Markov Chain Monte Carlo inference schemes, exhibits comparable\nperformances in terms of parameters identification and uncertainty\nquantification. We demonstrate the benefits of BSIM in various synthetic\nsettings characterised by an increasing number of stores and customers.\nFinally, we construct a real-world, large spatial dataset for pub activities in\nLondon, UK, which includes over 1,500 pubs and 150,000 customer regions. We\ndemonstrate how BSIM outperforms competing approaches on this large dataset in\nterms of prediction performances while providing results that are both\ninterpretable and consistent with related indicators observed for the London\nregion.",
          "link": "http://arxiv.org/abs/2108.02594",
          "publishedOn": "2021-08-06T00:51:46.890Z",
          "wordCount": 654,
          "title": "A variational Bayesian spatial interaction model for estimating revenue and demand at business facilities. (arXiv:2108.02594v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Popov_V/0/1/0/all/0/1\">Vadim Popov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vovk_I/0/1/0/all/0/1\">Ivan Vovk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gogoryan_V/0/1/0/all/0/1\">Vladimir Gogoryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadekova_T/0/1/0/all/0/1\">Tasnima Sadekova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kudinov_M/0/1/0/all/0/1\">Mikhail Kudinov</a>",
          "description": "Recently, denoising diffusion probabilistic models and generative score\nmatching have shown high potential in modelling complex data distributions\nwhile stochastic calculus has provided a unified point of view on these\ntechniques allowing for flexible inference schemes. In this paper we introduce\nGrad-TTS, a novel text-to-speech model with score-based decoder producing\nmel-spectrograms by gradually transforming noise predicted by encoder and\naligned with text input by means of Monotonic Alignment Search. The framework\nof stochastic differential equations helps us to generalize conventional\ndiffusion probabilistic models to the case of reconstructing data from noise\nwith different parameters and allows to make this reconstruction flexible by\nexplicitly controlling trade-off between sound quality and inference speed.\nSubjective human evaluation shows that Grad-TTS is competitive with\nstate-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We\nwill make the code publicly available shortly.",
          "link": "http://arxiv.org/abs/2105.06337",
          "publishedOn": "2021-08-06T00:51:46.883Z",
          "wordCount": 602,
          "title": "Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech. (arXiv:2105.06337v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anil_C/0/1/0/all/0/1\">Cem Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_X/0/1/0/all/0/1\">Xuchan Bao</a>",
          "description": "Voting systems have a wide range of applications including recommender\nsystems, web search, product design and elections. Limited by the lack of\ngeneral-purpose analytical tools, it is difficult to hand-engineer desirable\nvoting rules for each use case. For this reason, it is appealing to\nautomatically discover voting rules geared towards each scenario. In this\npaper, we show that set-input neural network architectures such as Set\nTransformers, fully-connected graph networks and DeepSets are both\ntheoretically and empirically well-suited for learning voting rules. In\nparticular, we show that these network models can not only mimic a number of\nexisting voting rules to compelling accuracy --- both position-based (such as\nPlurality and Borda) and comparison-based (such as Kemeny, Copeland and\nMaximin) --- but also discover near-optimal voting rules that maximize\ndifferent social welfare functions. Furthermore, the learned voting rules\ngeneralize well to different voter utility distributions and election sizes\nunseen during training.",
          "link": "http://arxiv.org/abs/2108.02768",
          "publishedOn": "2021-08-06T00:51:46.864Z",
          "wordCount": 569,
          "title": "Learning to Elect. (arXiv:2108.02768v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01403",
          "author": "<a href=\"http://arxiv.org/find/hep-th/1/au:+Erbin_H/0/1/0/all/0/1\">Harold Erbin</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Lahoche_V/0/1/0/all/0/1\">Vincent Lahoche</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Samary_D/0/1/0/all/0/1\">Dine Ousmane Samary</a>",
          "description": "In a recent work arXiv:2008.08601, Halverson, Maiti and Stoner proposed a\ndescription of neural networks in terms of a Wilsonian effective field theory.\nThe infinite-width limit is mapped to a free field theory, while finite $N$\ncorrections are taken into account by interactions (non-Gaussian terms in the\naction). In this paper, we study two related aspects of this correspondence.\nFirst, we comment on the concepts of locality and power-counting in this\ncontext. Indeed, these usual space-time notions may not hold for neural\nnetworks (since inputs can be arbitrary), however, the renormalization group\nprovides natural notions of locality and scaling. Moreover, we comment on\nseveral subtleties, for example, that data components may not have a\npermutation symmetry: in that case, we argue that random tensor field theories\ncould provide a natural generalization. Second, we improve the perturbative\nWilsonian renormalization from arXiv:2008.08601 by providing an analysis in\nterms of the nonperturbative renormalization group using the Wetterich-Morris\nequation. An important difference with usual nonperturbative RG analysis is\nthat only the effective (IR) 2-point function is known, which requires setting\nthe problem with care. Our aim is to provide a useful formalism to investigate\nneural networks behavior beyond the large-width limit (i.e.~far from Gaussian\nlimit) in a nonperturbative fashion. A major result of our analysis is that\nchanging the standard deviation of the neural network weight distribution can\nbe interpreted as a renormalization flow in the space of networks. We focus on\ntranslations invariant kernels and provide preliminary numerical results.",
          "link": "http://arxiv.org/abs/2108.01403",
          "publishedOn": "2021-08-06T00:51:46.858Z",
          "wordCount": 701,
          "title": "Nonperturbative renormalization for the neural network-QFT correspondence. (arXiv:2108.01403v1 [hep-th] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bohn_B/0/1/0/all/0/1\">Bastian Bohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griebel_M/0/1/0/all/0/1\">Michael Griebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_D/0/1/0/all/0/1\">Dinesh Kannan</a>",
          "description": "In this paper, we propose neural networks that tackle the problems of\nstability and field-of-view of a Convolutional Neural Network (CNN). As an\nalternative to increasing the network's depth or width to improve performance,\nwe propose integral-based spatially nonlocal operators which are related to\nglobal weighted Laplacian, fractional Laplacian and inverse fractional\nLaplacian operators that arise in several problems in the physical sciences.\nThe forward propagation of such networks is inspired by partial\nintegro-differential equations (PIDEs). We test the effectiveness of the\nproposed neural architectures on benchmark image classification datasets and\nsemantic segmentation tasks in autonomous driving. Moreover, we investigate the\nextra computational costs of these dense operators and the stability of forward\npropagation of the proposed neural networks.",
          "link": "http://arxiv.org/abs/2108.02430",
          "publishedOn": "2021-08-06T00:51:46.850Z",
          "wordCount": 553,
          "title": "Deep Neural Networks and PIDE discretizations. (arXiv:2108.02430v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1\">Haotian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaoli Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renyuan Xu</a>",
          "description": "One of the challenges for multi-agent reinforcement learning (MARL) is\ndesigning efficient learning algorithms for a large system in which each agent\nhas only limited or partial information of the entire system. In this system,\nit is desirable to learn policies of a decentralized type. A recent and\npromising paradigm to analyze such decentralized MARL is to take network\nstructures into consideration. While exciting progress has been made to analyze\ndecentralized MARL with the network of agents, often found in social networks\nand team video games, little is known theoretically for decentralized MARL with\nthe network of states, frequently used for modeling self-driving vehicles,\nride-sharing, and data and traffic routing.\n\nThis paper proposes a framework called localized training and decentralized\nexecution to study MARL with network of states, with homogeneous (a.k.a.\nmean-field type) agents. Localized training means that agents only need to\ncollect local information in their neighboring states during the training\nphase; decentralized execution implies that, after the training stage, agents\ncan execute the learned decentralized policies, which only requires knowledge\nof the agents' current states. The key idea is to utilize the homogeneity of\nagents and regroup them according to their states, thus the formulation of a\nnetworked Markov decision process with teams of agents, enabling the update of\nthe Q-function in a localized fashion. In order to design an efficient and\nscalable reinforcement learning algorithm under such a framework, we adopt the\nactor-critic approach with over-parameterized neural networks, and establish\nthe convergence and sample complexity for our algorithm, shown to be scalable\nwith respect to the size of both agents and states.",
          "link": "http://arxiv.org/abs/2108.02731",
          "publishedOn": "2021-08-06T00:51:46.844Z",
          "wordCount": 706,
          "title": "Mean-Field Multi-Agent Reinforcement Learning: A Decentralized Network Approach. (arXiv:2108.02731v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2003.04696",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Perez_Garcia_F/0/1/0/all/0/1\">Fernando P&#xe9;rez-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sparks_R/0/1/0/all/0/1\">Rachel Sparks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">S&#xe9;bastien Ourselin</a>",
          "description": "Processing of medical images such as MRI or CT presents unique challenges\ncompared to RGB images typically used in computer vision. These include a lack\nof labels for large datasets, high computational costs, and metadata to\ndescribe the physical properties of voxels. Data augmentation is used to\nartificially increase the size of the training datasets. Training with image\npatches decreases the need for computational power. Spatial metadata needs to\nbe carefully taken into account in order to ensure a correct alignment of\nvolumes.\n\nWe present TorchIO, an open-source Python library to enable efficient\nloading, preprocessing, augmentation and patch-based sampling of medical images\nfor deep learning. TorchIO follows the style of PyTorch and integrates standard\nmedical image processing libraries to efficiently process images during\ntraining of neural networks. TorchIO transforms can be composed, reproduced,\ntraced and extended. We provide multiple generic preprocessing and augmentation\noperations as well as simulation of MRI-specific artifacts.\n\nSource code, comprehensive tutorials and extensive documentation for TorchIO\ncan be found at https://torchio.rtfd.io/. The package can be installed from the\nPython Package Index running 'pip install torchio'. It includes a command-line\ninterface which allows users to apply transforms to image files without using\nPython. Additionally, we provide a graphical interface within a TorchIO\nextension in 3D Slicer to visualize the effects of transforms.\n\nTorchIO was developed to help researchers standardize medical image\nprocessing pipelines and allow them to focus on the deep learning experiments.\nIt encourages open science, as it supports reproducibility and is version\ncontrolled so that the software can be cited precisely. Due to its modularity,\nthe library is compatible with other frameworks for deep learning with medical\nimages.",
          "link": "http://arxiv.org/abs/2003.04696",
          "publishedOn": "2021-08-06T00:51:46.836Z",
          "wordCount": 819,
          "title": "TorchIO: A Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning. (arXiv:2003.04696v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02551",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1\">Shagufta Henna</a>",
          "description": "Hybrid FSO/RF system requires an efficient FSO and RF link switching\nmechanism to improve the system capacity by realizing the complementary\nbenefits of both the links. The dynamics of network conditions, such as fog,\ndust, and sand storms compound the link switching problem and control\ncomplexity. To address this problem, we initiate the study of deep\nreinforcement learning (DRL) for link switching of hybrid FSO/RF systems.\nSpecifically, in this work, we focus on actor-critic called Actor/Critic-FSO/RF\nand Deep-Q network (DQN) called DQN-FSO/RF for FSO/RF link switching under\natmospheric turbulences. To formulate the problem, we define the state, action,\nand reward function of a hybrid FSO/RF system. DQN-FSO/RF frequently updates\nthe deployed policy that interacts with the environment in a hybrid FSO/RF\nsystem, resulting in high switching costs. To overcome this, we lift this\nproblem to ensemble consensus-based representation learning for deep\nreinforcement called DQNEnsemble-FSO/RF. The proposed novel DQNEnsemble-FSO/RF\nDRL approach uses consensus learned features representations based on an\nensemble of asynchronous threads to update the deployed policy. Experimental\nresults corroborate that the proposed DQNEnsemble-FSO/RF's consensus-learned\nfeatures switching achieves better performance than Actor/Critic-FSO/RF,\nDQN-FSO/RF, and MyOpic for FSO/RF link switching while keeping the switching\ncost significantly low.",
          "link": "http://arxiv.org/abs/2108.02551",
          "publishedOn": "2021-08-06T00:51:46.809Z",
          "wordCount": 641,
          "title": "Ensemble Consensus-based Representation Deep Reinforcement Learning for Hybrid FSO/RF Communication Systems. (arXiv:2108.02551v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.04005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1\">Tsang Ing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1\">Cleber Zanchettin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Adriano L. I. Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "In this paper, we argue that the unsatisfactory out-of-distribution (OOD)\ndetection performance of neural networks is mainly due to the SoftMax loss\nanisotropy and propensity to produce low entropy probability distributions in\ndisagreement with the principle of maximum entropy. Current out-of-distribution\n(OOD) detection approaches usually do not directly fix the SoftMax loss\ndrawbacks, but rather build techniques to circumvent it. Unfortunately, those\nmethods usually produce undesired side effects (e.g., classification accuracy\ndrop, additional hyperparameters, slower inferences, and collecting extra\ndata). In the opposite direction, we propose replacing SoftMax loss with a\nnovel loss function that does not suffer from the mentioned weaknesses. The\nproposed IsoMax loss is isotropic (exclusively distance-based) and provides\nhigh entropy posterior probability distributions. Replacing the SoftMax loss by\nIsoMax loss requires no model or training changes. Additionally, the models\ntrained with IsoMax loss produce as fast and energy-efficient inferences as\nthose trained using SoftMax loss. Moreover, no classification accuracy drop is\nobserved. The proposed method does not rely on outlier/background data,\nhyperparameter tuning, temperature calibration, feature extraction, metric\nlearning, adversarial training, ensemble procedures, or generative models. Our\nexperiments showed that IsoMax loss works as a seamless SoftMax loss drop-in\nreplacement that significantly improves neural networks' OOD detection\nperformance. Hence, it may be used as a baseline OOD detection approach to be\ncombined with current or future OOD detection techniques to achieve even higher\nresults.",
          "link": "http://arxiv.org/abs/2006.04005",
          "publishedOn": "2021-08-06T00:51:46.799Z",
          "wordCount": 734,
          "title": "Entropic Out-of-Distribution Detection: Seamless Detection of Unknown Examples. (arXiv:2006.04005v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samak_T/0/1/0/all/0/1\">Tanmay Vilas Samak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samak_C/0/1/0/all/0/1\">Chinmay Vilas Samak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandhasamy_S/0/1/0/all/0/1\">Sivanathan Kandhasamy</a>",
          "description": "In this work, we present a lightweight pipeline for robust behavioral cloning\nof a human driver using end-to-end imitation learning. The proposed pipeline\nwas employed to train and deploy three distinct driving behavior models onto a\nsimulated vehicle. The training phase comprised of data collection, balancing,\naugmentation, preprocessing and training a neural network, following which, the\ntrained model was deployed onto the ego vehicle to predict steering commands\nbased on the feed from an onboard camera. A novel coupled control law was\nformulated to generate longitudinal control commands on-the-go based on the\npredicted steering angle and other parameters such as actual speed of the ego\nvehicle and the prescribed constraints for speed and steering. We analyzed\ncomputational efficiency of the pipeline and evaluated robustness of the\ntrained models through exhaustive experimentation during the deployment phase.\nWe also compared our approach against state-of-the-art implementation in order\nto comment on its validity.",
          "link": "http://arxiv.org/abs/2010.04767",
          "publishedOn": "2021-08-06T00:51:46.783Z",
          "wordCount": 651,
          "title": "Robust Behavioral Cloning for Autonomous Vehicles using End-to-End Imitation Learning. (arXiv:2010.04767v4 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15758",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rischel_E/0/1/0/all/0/1\">Eigil F. Rischel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Weichwald_S/0/1/0/all/0/1\">Sebastian Weichwald</a>",
          "description": "Interventional causal models describe several joint distributions over some\nvariables used to describe a system, one for each intervention setting. They\nprovide a formal recipe for how to move between the different joint\ndistributions and make predictions about the variables upon intervening on the\nsystem. Yet, it is difficult to formalise how we may change the underlying\nvariables used to describe the system, say moving from fine-grained to\ncoarse-grained variables. Here, we argue that compositionality is a desideratum\nfor such model transformations and the associated errors: When abstracting a\nreference model M iteratively, first obtaining M' and then further simplifying\nthat to obtain M'', we expect the composite transformation from M to M'' to\nexist and its error to be bounded by the errors incurred by each individual\ntransformation step. Category theory, the study of mathematical objects via\ncompositional transformations between them, offers a natural language to\ndevelop our framework for model transformations and abstractions. We introduce\na category of finite interventional causal models and, leveraging theory of\nenriched categories, prove the desired compositionality properties for our\nframework.",
          "link": "http://arxiv.org/abs/2103.15758",
          "publishedOn": "2021-08-06T00:51:46.730Z",
          "wordCount": 652,
          "title": "Compositional Abstraction Error and a Category of Causal Models. (arXiv:2103.15758v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12636",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Fengxiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Detection transformers have recently shown promising object detection results\nand attracted increasing attention. However, how to develop effective domain\nadaptation techniques to improve its cross-domain performance remains\nunexplored and unclear. In this paper, we delve into this topic and empirically\nfind that direct feature distribution alignment on the CNN backbone only brings\nlimited improvements, as it does not guarantee domain-invariant sequence\nfeatures in the transformer for prediction. To address this issue, we propose a\nnovel Sequence Feature Alignment (SFA) method that is specially designed for\nthe adaptation of detection transformers. Technically, SFA consists of a domain\nquery-based feature alignment (DQFA) module and a token-wise feature alignment\n(TDA) module. In DQFA, a novel domain query is used to aggregate and align\nglobal context from the token sequence of both domains. DQFA reduces the domain\ndiscrepancy in global feature representations and object relations when\ndeploying in the transformer encoder and decoder, respectively. Meanwhile, TDA\naligns token features in the sequence from both domains, which reduces the\ndomain gaps in local and instance-level feature representations in the\ntransformer encoder and decoder, respectively. Besides, a novel bipartite\nmatching consistency loss is proposed to enhance the feature discriminability\nfor robust object detection. Experiments on three challenging benchmarks show\nthat SFA outperforms state-of-the-art domain adaptive object detection methods.\nCode has been made available at: https://github.com/encounter1997/SFA.",
          "link": "http://arxiv.org/abs/2107.12636",
          "publishedOn": "2021-08-06T00:51:46.714Z",
          "wordCount": 726,
          "title": "Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1\">Mohammad Javad Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1\">Branislav Kveton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1\">Mohammad Ghavamzadeh</a>",
          "description": "We study the problem of best-arm identification (BAI) in contextual bandits\nin the fixed-budget setting. We propose a general successive elimination\nalgorithm that proceeds in stages and eliminates a fixed fraction of suboptimal\narms in each stage. This design takes advantage of the strengths of static and\nadaptive allocations. We analyze the algorithm in linear models and obtain a\nbetter error bound than prior work. We also apply it to generalized linear\nmodels (GLMs) and bound its error. This is the first BAI algorithm for GLMs in\nthe fixed-budget setting. Our extensive numerical experiments show that our\nalgorithm outperforms the state of art.",
          "link": "http://arxiv.org/abs/2106.04763",
          "publishedOn": "2021-08-06T00:51:46.693Z",
          "wordCount": 602,
          "title": "Fixed-Budget Best-Arm Identification in Contextual Bandits: A Static-Adaptive Algorithm. (arXiv:2106.04763v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02744",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Alamo_M/0/1/0/all/0/1\">Miguel del Alamo</a>",
          "description": "We consider ill-posed inverse problems where the forward operator $T$ is\nunknown, and instead we have access to training data consisting of functions\n$f_i$ and their noisy images $Tf_i$. This is a practically relevant and\nchallenging problem which current methods are able to solve only under strong\nassumptions on the training set. Here we propose a new method that requires\nminimal assumptions on the data, and prove reconstruction rates that depend on\nthe number of training points and the noise level. We show that, in the regime\nof \"many\" training data, the method is minimax optimal. The proposed method\nemploys a type of convolutional neural networks (U-nets) and empirical risk\nminimization in order to \"fit\" the unknown operator. In a nutshell, our\napproach is based on two ideas: the first is to relate U-nets to multiscale\ndecompositions such as wavelets, thereby linking them to the existing theory,\nand the second is to use the hierarchical structure of U-nets and the low\nnumber of parameters of convolutional neural nets to prove entropy bounds that\nare practically useful. A significant difference with the existing works on\nneural networks in nonparametric statistics is that we use them to approximate\noperators and not functions, which we argue is mathematically more natural and\ntechnically more convenient.",
          "link": "http://arxiv.org/abs/2108.02744",
          "publishedOn": "2021-08-06T00:51:46.686Z",
          "wordCount": 656,
          "title": "Deep learning for inverse problems with unknown operator. (arXiv:2108.02744v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1\">Shahroz Tariq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Simon S. Woo</a>",
          "description": "Over the last few decades, artificial intelligence research has made\ntremendous strides, but it still heavily relies on fixed datasets in stationary\nenvironments. Continual learning is a growing field of research that examines\nhow AI systems can learn sequentially from a continuous stream of linked data\nin the same way that biological systems do. Simultaneously, fake media such as\ndeepfakes and synthetic face images have emerged as significant to current\nmultimedia technologies. Recently, numerous method has been proposed which can\ndetect deepfakes with high accuracy. However, they suffer significantly due to\ntheir reliance on fixed datasets in limited evaluation settings. Therefore, in\nthis work, we apply continuous learning to neural networks' learning dynamics,\nemphasizing its potential to increase data efficiency significantly. We propose\nContinual Representation using Distillation (CoReD) method that employs the\nconcept of Continual Learning (CL), Representation Learning (RL), and Knowledge\nDistillation (KD). We design CoReD to perform sequential domain adaptation\ntasks on new deepfake and GAN-generated synthetic face datasets, while\neffectively minimizing the catastrophic forgetting in a teacher-student model\nsetting. Our extensive experimental results demonstrate that our method is\nefficient at domain adaptation to detect low-quality deepfakes videos and\nGAN-generated images from several datasets, outperforming the-state-of-art\nbaseline methods.",
          "link": "http://arxiv.org/abs/2107.02408",
          "publishedOn": "2021-08-06T00:51:46.679Z",
          "wordCount": 710,
          "title": "CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation. (arXiv:2107.02408v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02335",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Malyuta_D/0/1/0/all/0/1\">Danylo Malyuta</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Elango_P/0/1/0/all/0/1\">Purnanand Elango</a>, <a href=\"http://arxiv.org/find/math/1/au:+Acikmese_B/0/1/0/all/0/1\">Behcet Acikmese</a>",
          "description": "Space mission design places a premium on cost and operational efficiency. The\nsearch for new science and life beyond Earth calls for spacecraft that can\ndeliver scientific payloads to geologically rich yet hazardous landing sites.\nAt the same time, the last four decades of optimization research have put a\nsuite of powerful optimization tools at the fingertips of the controls\nengineer. As we enter the new decade, optimization theory, algorithms, and\nsoftware tooling have reached a critical mass to start seeing serious\napplication in space vehicle guidance and control systems. This survey paper\nprovides a detailed overview of recent advances, successes, and promising\ndirections for optimization-based space vehicle control. The considered\napplications include planetary landing, rendezvous and proximity operations,\nsmall body landing, constrained reorientation, endo-atmospheric flight\nincluding ascent and re-entry, and orbit transfer and injection. The primary\nfocus is on the last ten years of progress, which have seen a veritable rise in\nthe number of applications using three core technologies: lossless\nconvexification, sequential convex programming, and model predictive control.\nThe reader will come away with a well-rounded understanding of the\nstate-of-the-art in each space vehicle control application, and will be well\npositioned to tackle important current open problems using convex optimization\nas a core technology.",
          "link": "http://arxiv.org/abs/2108.02335",
          "publishedOn": "2021-08-06T00:51:46.672Z",
          "wordCount": 662,
          "title": "Advances in Trajectory Optimization for Space Vehicle Control. (arXiv:2108.02335v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeilmann_A/0/1/0/all/0/1\">Alexander Zeilmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petra_S/0/1/0/all/0/1\">Stefania Petra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnorr_C/0/1/0/all/0/1\">Christoph Schn&#xf6;rr</a>",
          "description": "We introduce a novel algorithm for estimating optimal parameters of\nlinearized assignment flows for image labeling. An exact formula is derived for\nthe parameter gradient of any loss function that is constrained by the linear\nsystem of ODEs determining the linearized assignment flow. We show how to\nefficiently evaluate this formula using a Krylov subspace and a low-rank\napproximation. This enables us to perform parameter learning by Riemannian\ngradient descent in the parameter space, without the need to backpropagate\nerrors or to solve an adjoint equation, in less than 10 seconds for a\n$512\\times 512$ image using just about $0.5$ GB memory. Experiments demonstrate\nthat our method performs as good as highly-tuned machine learning software\nusing automatic differentiation. Unlike methods employing automatic\ndifferentiation, our approach yields a low-dimensional representation of\ninternal parameters and their dynamics which helps to understand how networks\nwork and perform that realize assignment flows and generalizations thereof.",
          "link": "http://arxiv.org/abs/2108.02571",
          "publishedOn": "2021-08-06T00:51:46.665Z",
          "wordCount": 582,
          "title": "Learning Linearized Assignment Flows for Image Labeling. (arXiv:2108.02571v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.05231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">HanQin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamm_K/0/1/0/all/0/1\">Keaton Hamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Longxiu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Needell_D/0/1/0/all/0/1\">Deanna Needell</a>",
          "description": "This paper considers the use of Robust PCA in a CUR decomposition framework\nand applications thereof. Our main algorithms produce a robust version of\ncolumn-row factorizations of matrices $\\mathbf{D}=\\mathbf{L}+\\mathbf{S}$ where\n$\\mathbf{L}$ is low-rank and $\\mathbf{S}$ contains sparse outliers. These\nmethods yield interpretable factorizations at low computational cost, and\nprovide new CUR decompositions that are robust to sparse outliers, in contrast\nto previous methods. We consider two key imaging applications of Robust PCA:\nvideo foreground-background separation and face modeling. This paper examines\nthe qualitative behavior of our Robust CUR decompositions on the benchmark\nvideos and face datasets, and find that our method works as well as standard\nRobust PCA while being significantly faster. Additionally, we consider hybrid\nrandomized and deterministic sampling methods which produce a compact CUR\ndecomposition of a given matrix, and apply this to video sequences to produce\ncanonical frames thereof.",
          "link": "http://arxiv.org/abs/2101.05231",
          "publishedOn": "2021-08-06T00:51:46.647Z",
          "wordCount": 624,
          "title": "Robust CUR Decomposition: Theory and Imaging Applications. (arXiv:2101.05231v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alman_J/0/1/0/all/0/1\">Josh Alman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1\">Timothy Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_G/0/1/0/all/0/1\">Gary Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shyam Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellke_M/0/1/0/all/0/1\">Mark Sellke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhao Song</a>",
          "description": "In this paper, we develop a new technique which we call representation theory\nof the real hyperrectangle, which describes how to compute the eigenvectors and\neigenvalues of certain matrices arising from hyperrectangles. We show that\nthese matrices arise naturally when analyzing a number of different algorithmic\ntasks such as kernel methods, neural network training, natural language\nprocessing, and the design of algorithms using the polynomial method. We then\nuse our new technique along with these connections to prove several new\nstructural results in these areas, including:\n\n$\\bullet$ A function is a positive definite Manhattan kernel if and only if\nit is a completely monotone function. These kernels are widely used across\nmachine learning; one example is the Laplace kernel which is widely used in\nmachine learning for chemistry.\n\n$\\bullet$ A function transforms Manhattan distances to Manhattan distances if\nand only if it is a Bernstein function. This completes the theory of Manhattan\nto Manhattan metric transforms initiated by Assouad in 1980.\n\n$\\bullet$ A function applied entry-wise to any square matrix of rank $r$\nalways results in a matrix of rank $< 2^{r-1}$ if and only if it is a\npolynomial of sufficiently low degree. This gives a converse to a key lemma\nused by the polynomial method in algorithm design.\n\nOur work includes a sophisticated combination of techniques from different\nfields, including metric embeddings, the polynomial method, and group\nrepresentation theory.",
          "link": "http://arxiv.org/abs/2011.11503",
          "publishedOn": "2021-08-06T00:51:46.641Z",
          "wordCount": 714,
          "title": "Metric Transforms and Low Rank Matrices via Representation Theory of the Real Hyperrectangle. (arXiv:2011.11503v2 [cs.CG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.02443",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pomponi_J/0/1/0/all/0/1\">Jary Pomponi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scardapane_S/0/1/0/all/0/1\">Simone Scardapane</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Uncini_A/0/1/0/all/0/1\">Aurelio Uncini</a>",
          "description": "Catastrophic forgetting (CF) happens whenever a neural network overwrites\npast knowledge while being trained on new tasks. Common techniques to handle CF\ninclude regularization of the weights (using, e.g., their importance on past\ntasks), and rehearsal strategies, where the network is constantly re-trained on\npast data. Generative models have also been applied for the latter, in order to\nhave endless sources of data. In this paper, we propose a novel method that\ncombines the strengths of regularization and generative-based rehearsal\napproaches. Our generative model consists of a normalizing flow (NF), a\nprobabilistic and invertible neural network, trained on the internal embeddings\nof the network. By keeping a single NF conditioned on the task, we show that\nour memory overhead remains constant. In addition, exploiting the invertibility\nof the NF, we propose a simple approach to regularize the network's embeddings\nwith respect to past tasks. We show that our method performs favorably with\nrespect to state-of-the-art approaches in the literature, with bounded\ncomputational power and memory overheads.",
          "link": "http://arxiv.org/abs/2007.02443",
          "publishedOn": "2021-08-06T00:51:46.634Z",
          "wordCount": 655,
          "title": "Pseudo-Rehearsal for Continual Learning with Normalizing Flows. (arXiv:2007.02443v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_B/0/1/0/all/0/1\">Binayak Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yingtao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthukumar_V/0/1/0/all/0/1\">Venkatesan Muthukumar</a>",
          "description": "The increasing application of deep learning technology drives the need for an\nefficient parallel computing architecture for Convolutional Neural Networks\n(CNNs). A significant challenge faced when designing a many-core CNN\naccelerator is to handle the data movement between the processing elements. The\nCNN workload introduces many-to-one traffic in addition to one-to-one and\none-to-many traffic. As the de-facto standard for on-chip communication,\nNetwork-on-Chip (NoC) can support various unicast and multicast traffic. For\nmany-to-one traffic, repetitive unicast is employed which is not an efficient\nway. In this paper, we propose to use the gather packet on mesh-based NoCs\nemploying output stationary systolic array in support of many-to-one traffic.\nThe gather packet will collect the data from the intermediate nodes eventually\nleading to the destination efficiently. This method is evaluated using the\ntraffic traces generated from the convolution layer of AlexNet and VGG-16 with\nimprovement in the latency and power than the repetitive unicast method.",
          "link": "http://arxiv.org/abs/2108.02567",
          "publishedOn": "2021-08-06T00:51:46.627Z",
          "wordCount": 605,
          "title": "Improving the Performance of a NoC-based CNN Accelerator with Gather Support. (arXiv:2108.02567v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mehra_V/0/1/0/all/0/1\">Vipul Mehra</a>",
          "description": "This thesis is divided into two parts:Part I: Analysis of Fruits, Vegetables,\nCheese and Fish based on Image Processing using Computer Vision and Deep\nLearning: A Review. It consists of a comprehensive review of image processing,\ncomputer vision and deep learning techniques applied to carry out analysis of\nfruits, vegetables, cheese and fish.This part also serves as a literature\nreview for Part II.Part II: GuavaNet: A deep neural network architecture for\nautomatic sensory evaluation to predict degree of acceptability for Guava by a\nconsumer. This part introduces to an end-to-end deep neural network\narchitecture that can predict the degree of acceptability by the consumer for a\nguava based on sensory evaluation.",
          "link": "http://arxiv.org/abs/2108.02563",
          "publishedOn": "2021-08-06T00:51:46.621Z",
          "wordCount": 574,
          "title": "GuavaNet: A deep neural network architecture for automatic sensory evaluation to predict degree of acceptability for Guava by a consumer. (arXiv:2108.02563v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Furui Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dongyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_F/0/1/0/all/0/1\">Fan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yanna Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zytek_A/0/1/0/all/0/1\">Alexandra Zytek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haomin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1\">Kalyan Veeramachaneni</a>",
          "description": "Machine learning (ML) is increasingly applied to Electronic Health Records\n(EHRs) to solve clinical prediction tasks. Although many ML models perform\npromisingly, issues with model transparency and interpretability limit their\nadoption in clinical practice. Directly using existing explainable ML\ntechniques in clinical settings can be challenging. Through literature surveys\nand collaborations with six clinicians with an average of 17 years of clinical\nexperience, we identified three key challenges, including clinicians'\nunfamiliarity with ML features, lack of contextual information, and the need\nfor cohort-level evidence. Following an iterative design process, we further\ndesigned and developed VBridge, a visual analytics tool that seamlessly\nincorporates ML explanations into clinicians' decision-making workflow. The\nsystem includes a novel hierarchical display of contribution-based feature\nexplanations and enriched interactions that connect the dots between ML\nfeatures, explanations, and data. We demonstrated the effectiveness of VBridge\nthrough two case studies and expert interviews with four clinicians, showing\nthat visually associating model explanations with patients' situational records\ncan help clinicians better interpret and use model predictions when making\nclinician decisions. We further derived a list of design implications for\ndeveloping future explainable ML tools to support clinical decision-making.",
          "link": "http://arxiv.org/abs/2108.02550",
          "publishedOn": "2021-08-06T00:51:46.597Z",
          "wordCount": 664,
          "title": "VBridge: Connecting the Dots Between Features, Explanations, and Data for Healthcare Models. (arXiv:2108.02550v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Towey_D/0/1/0/all/0/1\">Dave Towey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+French_A/0/1/0/all/0/1\">Andrew French</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benford_S/0/1/0/all/0/1\">Steve Benford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhi Quan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tsong Yueh Chen</a>",
          "description": "Software testing is often hindered where it is impossible or impractical to\ndetermine the correctness of the behaviour or output of the software under test\n(SUT), a situation known as the oracle problem. An example of an area facing\nthe oracle problem is automatic image classification, using machine learning to\nclassify an input image as one of a set of predefined classes. An approach to\nsoftware testing that alleviates the oracle problem is metamorphic testing\n(MT). While traditional software testing examines the correctness of individual\ntest cases, MT instead examines the relations amongst multiple executions of\ntest cases and their outputs. These relations are called metamorphic relations\n(MRs): if an MR is found to be violated, then a fault must exist in the SUT.\nThis paper examines the problem of classifying images containing visually\nhidden markers called Artcodes, and applies MT to verify and enhance the\ntrained classifiers. This paper further examines two MRs, Separation and\nOcclusion, and reports on their capability in verifying the image\nclassification using one-way analysis of variance (ANOVA) in conjunction with\nthree other statistical analysis methods: t-test (for unequal variances),\nKruskal-Wallis test, and Dunnett's test. In addition to our previously-studied\nclassifier, that used Random Forests, we introduce a new classifier that uses a\nsupport vector machine, and present its MR-augmented version. Experimental\nevaluations across a number of performance metrics show that the augmented\nclassifiers can achieve better performance than non-augmented classifiers. This\npaper also analyses how the enhanced performance is obtained.",
          "link": "http://arxiv.org/abs/2108.02694",
          "publishedOn": "2021-08-06T00:51:46.590Z",
          "wordCount": 696,
          "title": "Using Metamorphic Relations to Verify and Enhance Artcode Classification. (arXiv:2108.02694v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuang_H/0/1/0/all/0/1\">Haofei Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwertfeger_S/0/1/0/all/0/1\">S&#xf6;ren Schwertfeger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stachniss_C/0/1/0/all/0/1\">Cyrill Stachniss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>",
          "description": "Contrastive learning has revolutionized self-supervised image representation\nlearning field, and recently been adapted to video domain. One of the greatest\nadvantages of contrastive learning is that it allows us to flexibly define\npowerful loss objectives as long as we can find a reasonable way to formulate\npositive and negative samples to contrast. However, existing approaches rely\nheavily on the short-range spatiotemporal salience to form clip-level\ncontrastive signals, thus limit themselves from using global context. In this\npaper, we propose a new video-level contrastive learning method based on\nsegments to formulate positive pairs. Our formulation is able to capture global\ncontext in a video, thus robust to temporal content change. We also incorporate\na temporal order regularization term to enforce the inherent sequential\nstructure of videos. Extensive experiments show that our video-level\ncontrastive learning framework (VCLR) is able to outperform previous\nstate-of-the-arts on five video datasets for downstream action classification,\naction localization and video retrieval. Code is available at\nhttps://github.com/amazon-research/video-contrastive-learning.",
          "link": "http://arxiv.org/abs/2108.02722",
          "publishedOn": "2021-08-06T00:51:46.583Z",
          "wordCount": 618,
          "title": "Video Contrastive Learning with Global Context. (arXiv:2108.02722v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jingyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1\">Qi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1\">Hongyang Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "Unsupervised learning is just at a tipping point where it could really take\noff. Among these approaches, contrastive learning has seen tremendous progress\nand led to state-of-the-art performance. In this paper, we construct a novel\nprobabilistic graphical model that effectively incorporates the low rank\npromoting prior into the framework of contrastive learning, referred to as\nLORAC. In contrast to the existing conventional self-supervised approaches that\nonly considers independent learning, our hypothesis explicitly requires that\nall the samples belonging to the same instance class lie on the same subspace\nwith small dimension. This heuristic poses particular joint learning\nconstraints to reduce the degree of freedom of the problem during the search of\nthe optimal network parameterization. Most importantly, we argue that the low\nrank prior employed here is not unique, and many different priors can be\ninvoked in a similar probabilistic way, corresponding to different hypotheses\nabout underlying truth behind the contrastive features. Empirical evidences\nshow that the proposed algorithm clearly surpasses the state-of-the-art\napproaches on multiple benchmarks, including image classification, object\ndetection, instance segmentation and keypoint detection.",
          "link": "http://arxiv.org/abs/2108.02696",
          "publishedOn": "2021-08-06T00:51:46.521Z",
          "wordCount": 629,
          "title": "A Low Rank Promoting Prior for Unsupervised Contrastive Learning. (arXiv:2108.02696v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Doran_H/0/1/0/all/0/1\">Hans Dermot Doran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ielpo_G/0/1/0/all/0/1\">Gianluca Ielpo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganz_D/0/1/0/all/0/1\">David Ganz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zapke_M/0/1/0/all/0/1\">Michael Zapke</a>",
          "description": "With edge-AI finding an increasing number of real-world applications,\nespecially in industry, the question of functionally safe applications using AI\nhas begun to be asked. In this body of work, we explore the issue of achieving\ndependable operation of neural networks. We discuss the issue of dependability\nin general implementation terms before examining lockstep solutions. We intuit\nthat it is not necessarily a given that two similar neural networks generate\nresults at precisely the same time and that synchronization between the\nplatforms will be required. We perform some preliminary measurements that may\nsupport this intuition and introduce some work in implementing lockstep neural\nnetwork engines.",
          "link": "http://arxiv.org/abs/2108.02565",
          "publishedOn": "2021-08-06T00:51:46.514Z",
          "wordCount": 573,
          "title": "Dependable Neural Networks Through Redundancy, A Comparison of Redundant Architectures. (arXiv:2108.02565v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02665",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patil_M/0/1/0/all/0/1\">Mihir Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wehbe_B/0/1/0/all/0/1\">Bilal Wehbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1\">Matias Valdenegro-Toro</a>",
          "description": "Docking control of an autonomous underwater vehicle (AUV) is a task that is\nintegral to achieving persistent long term autonomy. This work explores the\napplication of state-of-the-art model-free deep reinforcement learning (DRL)\napproaches to the task of AUV docking in the continuous domain. We provide a\ndetailed formulation of the reward function, utilized to successfully dock the\nAUV onto a fixed docking platform. A major contribution that distinguishes our\nwork from the previous approaches is the usage of a physics simulator to define\nand simulate the underwater environment as well as the DeepLeng AUV. We propose\na new reward function formulation for the docking task, incorporating several\ncomponents, that outperforms previous reward formulations. We evaluate proximal\npolicy optimization (PPO), twin delayed deep deterministic policy gradients\n(TD3) and soft actor-critic (SAC) in combination with our reward function. Our\nevaluation yielded results that conclusively show the TD3 agent to be most\nefficient and consistent in terms of docking the AUV, over multiple evaluation\nruns it achieved a 100% success rate and episode return of 10667.1 +- 688.8. We\nalso show how our reward function formulation improves over the state of the\nart.",
          "link": "http://arxiv.org/abs/2108.02665",
          "publishedOn": "2021-08-06T00:51:46.507Z",
          "wordCount": 641,
          "title": "Deep Reinforcement Learning for Continuous Docking Control of Autonomous Underwater Vehicles: A Benchmarking Study. (arXiv:2108.02665v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02776",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hono_Y/0/1/0/all/0/1\">Yukiya Hono</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kei Hashimoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oura_K/0/1/0/all/0/1\">Keiichiro Oura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nankaku_Y/0/1/0/all/0/1\">Yoshihiko Nankaku</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tokuda_K/0/1/0/all/0/1\">Keiichi Tokuda</a>",
          "description": "This paper presents Sinsy, a deep neural network (DNN)-based singing voice\nsynthesis (SVS) system. In recent years, DNNs have been utilized in statistical\nparametric SVS systems, and DNN-based SVS systems have demonstrated better\nperformance than conventional hidden Markov model-based ones. SVS systems are\nrequired to synthesize a singing voice with pitch and timing that strictly\nfollow a given musical score. Additionally, singing expressions that are not\ndescribed on the musical score, such as vibrato and timing fluctuations, should\nbe reproduced. The proposed system is composed of four modules: a time-lag\nmodel, a duration model, an acoustic model, and a vocoder, and singing voices\ncan be synthesized taking these characteristics of singing voices into account.\nTo better model a singing voice, the proposed system incorporates improved\napproaches to modeling pitch and vibrato and better training criteria into the\nacoustic model. In addition, we incorporated PeriodNet, a non-autoregressive\nneural vocoder with robustness for the pitch, into our systems to generate a\nhigh-fidelity singing voice waveform. Moreover, we propose automatic pitch\ncorrection techniques for DNN-based SVS to synthesize singing voices with\ncorrect pitch even if the training data has out-of-tune phrases. Experimental\nresults show our system can synthesize a singing voice with better timing, more\nnatural vibrato, and correct pitch, and it can achieve better mean opinion\nscores in subjective evaluation tests.",
          "link": "http://arxiv.org/abs/2108.02776",
          "publishedOn": "2021-08-06T00:51:46.498Z",
          "wordCount": 690,
          "title": "Sinsy: A Deep Neural Network-Based Singing Voice Synthesis System. (arXiv:2108.02776v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Novaes_A/0/1/0/all/0/1\">Andre Luiz Farias Novaes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_R/0/1/0/all/0/1\">Rui Alexandre de Matos Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figueiredo_J/0/1/0/all/0/1\">Jose Figueiredo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavanelli_L/0/1/0/all/0/1\">Lucas Aguiar Pavanelli</a>",
          "description": "Meter-level load forecasting is crucial for efficient energy management and\npower system planning for Smart Grids (SGs), in tasks associated with\nregulation, dispatching, scheduling, and unit commitment of power grids.\nAlthough a variety of algorithms have been proposed and applied on the field,\nmore accurate and robust models are still required: the overall utility cost of\noperations in SGs increases 10 million currency units if the load forecasting\nerror increases 1%, and the mean absolute percentage error (MAPE) in\nforecasting is still much higher than 1%. Transformers have become the new\nstate-of-the-art in a variety of tasks, including the ones in computer vision,\nnatural language processing and time series forecasting, surpassing alternative\nneural models such as convolutional and recurrent neural networks. In this\nletter, we present a new state-of-the-art Transformer-based algorithm for the\nmeter-level load forecasting task, which has surpassed the former\nstate-of-the-art, LSTM, and the traditional benchmark, vanilla RNN, in all\nexperiments by a margin of at least 13% in MAPE.",
          "link": "http://arxiv.org/abs/2108.02628",
          "publishedOn": "2021-08-06T00:51:46.475Z",
          "wordCount": 604,
          "title": "A New State-of-the-Art Transformers-Based Load Forecaster on the Smart Grid Domain. (arXiv:2108.02628v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hornauer_J/0/1/0/all/0/1\">Julia Hornauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nalpantidis_L/0/1/0/all/0/1\">Lazaros Nalpantidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>",
          "description": "Real-world perception systems in many cases build on hardware with limited\nresources to adhere to cost and power limitations of their carrying system.\nDeploying deep neural networks on resource-constrained hardware became possible\nwith model compression techniques, as well as efficient and hardware-aware\narchitecture design. However, model adaptation is additionally required due to\nthe diverse operation environments. In this work, we address the problem of\ntraining deep neural networks on resource-constrained hardware in the context\nof visual domain adaptation. We select the task of monocular depth estimation\nwhere our goal is to transform a pre-trained model to the target's domain data.\nWhile the source domain includes labels, we assume an unlabelled target domain,\nas it happens in real-world applications. Then, we present an adversarial\nlearning approach that is adapted for training on the device with limited\nresources. Since visual domain adaptation, i.e. neural network training, has\nnot been previously explored for resource-constrained hardware, we present the\nfirst feasibility study for image-based depth estimation. Our experiments show\nthat visual domain adaptation is relevant only for efficient network\narchitectures and training sets at the order of a few hundred samples. Models\nand code are publicly available.",
          "link": "http://arxiv.org/abs/2108.02671",
          "publishedOn": "2021-08-06T00:51:46.467Z",
          "wordCount": 650,
          "title": "Visual Domain Adaptation for Monocular Depth Estimation on Resource-Constrained Hardware. (arXiv:2108.02671v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ilin_V/0/1/0/all/0/1\">Valery Ilin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinov_I/0/1/0/all/0/1\">Ivan Kalinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpyshev_P/0/1/0/all/0/1\">Pavel Karpyshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsetserukou_D/0/1/0/all/0/1\">Dzmitry Tsetserukou</a>",
          "description": "In the proposed study, we describe the possibility of automated dataset\ncollection using an articulated robot. The proposed technology reduces the\nnumber of pixel errors on a polygonal dataset and the time spent on manual\nlabeling of 2D objects. The paper describes a novel automatic dataset\ncollection and annotation system, and compares the results of automated and\nmanual dataset labeling. Our approach increases the speed of data labeling\n240-fold, and improves the accuracy compared to manual labeling 13-fold. We\nalso present a comparison of metrics for training a neural network on a\nmanually annotated and an automatically collected dataset.",
          "link": "http://arxiv.org/abs/2108.02555",
          "publishedOn": "2021-08-06T00:51:46.445Z",
          "wordCount": 559,
          "title": "DeepScanner: a Robotic System for Automated 2D Object Dataset Collection with Annotations. (arXiv:2108.02555v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esteves_J/0/1/0/all/0/1\">Jose Jurandir Alves Esteves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boubendir_A/0/1/0/all/0/1\">Amina Boubendir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillemin_F/0/1/0/all/0/1\">Fabrice Guillemin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sens_P/0/1/0/all/0/1\">Pierre Sens</a>",
          "description": "The evaluation of the impact of using Machine Learning in the management of\nsoftwarized networks is considered in multiple research works. Beyond that, we\npropose to evaluate the robustness of online learning for optimal network slice\nplacement. A major assumption to this study is to consider that slice request\narrivals are non-stationary. In this context, we simulate unpredictable network\nload variations and compare two Deep Reinforcement Learning (DRL) algorithms: a\npure DRL-based algorithm and a heuristically controlled DRL as a hybrid\nDRL-heuristic algorithm, to assess the impact of these unpredictable changes of\ntraffic load on the algorithms performance. We conduct extensive simulations of\na large-scale operator infrastructure. The evaluation results show that the\nproposed hybrid DRL-heuristic approach is more robust and reliable in case of\nunpredictable network load changes than pure DRL as it reduces the performance\ndegradation. These results are follow-ups for a series of recent research we\nhave performed showing that the proposed hybrid DRL-heuristic approach is\nefficient and more adapted to real network scenarios than pure DRL.",
          "link": "http://arxiv.org/abs/2108.02505",
          "publishedOn": "2021-08-06T00:51:46.419Z",
          "wordCount": 618,
          "title": "On the Robustness of Controlled Deep Reinforcement Learning for Slice Placement. (arXiv:2108.02505v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02393",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Abouheaf_M/0/1/0/all/0/1\">Mohammed Abouheaf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gueaieb_W/0/1/0/all/0/1\">Wail Gueaieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lewis_F/0/1/0/all/0/1\">Frank Lewis</a>",
          "description": "The control problem of the flexible wing aircraft is challenging due to the\nprevailing and high nonlinear deformations in the flexible wing system. This\nurged for new control mechanisms that are robust to the real-time variations in\nthe wing's aerodynamics. An online control mechanism based on a value iteration\nreinforcement learning process is developed for flexible wing aerial\nstructures. It employs a model-free control policy framework and a guaranteed\nconvergent adaptive learning architecture to solve the system's Bellman\noptimality equation. A Riccati equation is derived and shown to be equivalent\nto solving the underlying Bellman equation. The online reinforcement learning\nsolution is implemented using means of an adaptive-critic mechanism. The\ncontroller is proven to be asymptotically stable in the Lyapunov sense. It is\nassessed through computer simulations and its superior performance is\ndemonstrated on two scenarios under different operating conditions.",
          "link": "http://arxiv.org/abs/2108.02393",
          "publishedOn": "2021-08-06T00:51:46.400Z",
          "wordCount": 613,
          "title": "Online Model-Free Reinforcement Learning for the Automatic Control of a Flexible Wing Aircraft. (arXiv:2108.02393v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02495",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esteves_J/0/1/0/all/0/1\">Jose Jurandir Alves Esteves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boubendir_A/0/1/0/all/0/1\">Amina Boubendir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillemin_F/0/1/0/all/0/1\">Fabrice Guillemin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sens_P/0/1/0/all/0/1\">Pierre Sens</a>",
          "description": "We consider online learning for optimal network slice placement under the\nassumption that slice requests arrive according to a non-stationary Poisson\nprocess. We propose a framework based on Deep Reinforcement Learning (DRL)\ncombined with a heuristic to design algorithms. We specifically design two\npure-DRL algorithms and two families of hybrid DRL-heuristic algorithms. To\nvalidate their performance, we perform extensive simulations in the context of\na large-scale operator infrastructure. The evaluation results show that the\nproposed hybrid DRL-heuristic algorithms require three orders of magnitude of\nlearning episodes less than pure-DRL to achieve convergence. This result\nindicates that the proposed hybrid DRL-heuristic approach is more reliable than\npure-DRL in a real non-stationary network scenario.",
          "link": "http://arxiv.org/abs/2108.02495",
          "publishedOn": "2021-08-06T00:51:46.393Z",
          "wordCount": 550,
          "title": "DRL-based Slice Placement Under Non-Stationary Conditions. (arXiv:2108.02495v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Haotian Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhengyang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>",
          "description": "This paper proposes a hypothesis for the aesthetic appreciation that\naesthetic images make a neural network strengthen salient concepts and discard\ninessential concepts. In order to verify this hypothesis, we use multi-variate\ninteractions to represent salient concepts and inessential concepts contained\nin images. Furthermore, we design a set of operations to revise images towards\nmore beautiful ones. In experiments, we find that the revised images are more\naesthetic than the original ones to some extent.",
          "link": "http://arxiv.org/abs/2108.02646",
          "publishedOn": "2021-08-06T00:51:46.386Z",
          "wordCount": 525,
          "title": "A Hypothesis for the Aesthetic Appreciation in Neural Networks. (arXiv:2108.02646v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Siyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Dan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guannan Zhang</a>",
          "description": "We propose a novel prediction interval method to learn prediction mean\nvalues, lower and upper bounds of prediction intervals from three independently\ntrained neural networks only using the standard mean squared error (MSE) loss,\nfor uncertainty quantification in regression tasks. Our method requires no\ndistributional assumption on data, does not introduce unusual hyperparameters\nto either the neural network models or the loss function. Moreover, our method\ncan effectively identify out-of-distribution samples and reasonably quantify\ntheir uncertainty. Numerical experiments on benchmark regression problems show\nthat our method outperforms the state-of-the-art methods with respect to\npredictive uncertainty quality, robustness, and identification of\nout-of-distribution samples.",
          "link": "http://arxiv.org/abs/2108.02327",
          "publishedOn": "2021-08-06T00:51:46.379Z",
          "wordCount": 534,
          "title": "PI3NN: Prediction intervals from three independently trained neural networks. (arXiv:2108.02327v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02424",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_T/0/1/0/all/0/1\">Tiange Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1\">Zijun Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tsui_K/0/1/0/all/0/1\">Kwok-Leung Tsui</a>",
          "description": "Accurate forecasting of traffic conditions is critical for improving safety,\nstability, and efficiency of a city transportation system. In reality, it is\nchallenging to produce accurate traffic forecasts due to the complex and\ndynamic spatiotemporal correlations. Most existing works only consider partial\ncharacteristics and features of traffic data, and result in unsatisfactory\nperformances on modeling and forecasting. In this paper, we propose a periodic\nspatial-temporal deep neural network (PSTN) with three pivotal modules to\nimprove the forecasting performance of traffic conditions through a novel\nintegration of three types of information. First, the historical traffic\ninformation is folded and fed into a module consisting of a graph convolutional\nnetwork and a temporal convolutional network. Second, the recent traffic\ninformation together with the historical output passes through the second\nmodule consisting of a graph convolutional network and a gated recurrent unit\nframework. Finally, a multi-layer perceptron is applied to process the\nauxiliary road attributes and output the final predictions. Experimental\nresults on two publicly accessible real-world urban traffic data sets show that\nthe proposed PSTN outperforms the state-of-the-art benchmarks by significant\nmargins for short-term traffic conditions forecasting",
          "link": "http://arxiv.org/abs/2108.02424",
          "publishedOn": "2021-08-06T00:51:46.373Z",
          "wordCount": 621,
          "title": "PSTN: Periodic Spatial-temporal Deep Neural Network for Traffic Condition Prediction. (arXiv:2108.02424v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02537",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Bharadwaj_P/0/1/0/all/0/1\">Pawan Bharadwaj</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_M/0/1/0/all/0/1\">Matthew Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Demanet_L/0/1/0/all/0/1\">Laurent Demanet</a>",
          "description": "This paper considers physical systems described by hidden states and\nindirectly observed through repeated measurements corrupted by unmodeled\nnuisance parameters. A network-based representation learns to disentangle the\ncoherent information (relative to the state) from the incoherent nuisance\ninformation (relative to the sensing). Instead of physical models, the\nrepresentation uses symmetry and stochastic regularization to inform an\nautoencoder architecture called SymAE. It enables redatuming, i.e., creating\nvirtual data instances where the nuisances are uniformized across measurements.",
          "link": "http://arxiv.org/abs/2108.02537",
          "publishedOn": "2021-08-06T00:51:46.367Z",
          "wordCount": 514,
          "title": "Redatuming physical systems using symmetric autoencoders. (arXiv:2108.02537v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_F/0/1/0/all/0/1\">Flavio Abreu Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riou_M/0/1/0/all/0/1\">Mathieu Riou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torrejon_J/0/1/0/all/0/1\">Jacob Torrejon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravelosona_D/0/1/0/all/0/1\">Dafin&#xe9; Ravelosona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1\">Wang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weisheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grollier_J/0/1/0/all/0/1\">Julie Grollier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Querlioz_D/0/1/0/all/0/1\">Damien Querlioz</a>",
          "description": "Deep learning has an increasing impact to assist research, allowing, for\nexample, the discovery of novel materials. Until now, however, these artificial\nintelligence techniques have fallen short of discovering the full differential\nequation of an experimental physical system. Here we show that a dynamical\nneural network, trained on a minimal amount of data, can predict the behavior\nof spintronic devices with high accuracy and an extremely efficient simulation\ntime, compared to the micromagnetic simulations that are usually employed to\nmodel them. For this purpose, we re-frame the formalism of Neural Ordinary\nDifferential Equations (ODEs) to the constraints of spintronics: few measured\noutputs, multiple inputs and internal parameters. We demonstrate with\nSpin-Neural ODEs an acceleration factor over 200 compared to micromagnetic\nsimulations for a complex problem -- the simulation of a reservoir computer\nmade of magnetic skyrmions (20 minutes compared to three days). In a second\nrealization, we show that we can predict the noisy response of experimental\nspintronic nano-oscillators to varying inputs after training Spin-Neural ODEs\non five milliseconds of their measured response to different excitations.\nSpin-Neural ODE is a disruptive tool for developing spintronic applications in\ncomplement to micromagnetic simulations, which are time-consuming and cannot\nfit experiments when noise or imperfections are present. Spin-Neural ODE can\nalso be generalized to other electronic devices involving dynamics.",
          "link": "http://arxiv.org/abs/2108.02318",
          "publishedOn": "2021-08-06T00:51:46.348Z",
          "wordCount": 695,
          "title": "Forecasting the outcome of spintronic experiments with Neural Ordinary Differential Equations. (arXiv:2108.02318v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahara_S/0/1/0/all/0/1\">Sawan Singh Mahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M%2E_S/0/1/0/all/0/1\">Shruti M.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharath_B/0/1/0/all/0/1\">B. N. Bharath</a>",
          "description": "Federated Learning (FL) has evolved as a promising technique to handle\ndistributed machine learning across edge devices. A single neural network (NN)\nthat optimises a global objective is generally learned in most work in FL,\nwhich could be suboptimal for edge devices. Although works finding a NN\npersonalised for edge device specific tasks exist, they lack generalisation\nand/or convergence guarantees. In this paper, a novel communication efficient\nFL algorithm for personalised learning in a wireless setting with guarantees is\npresented. The algorithm relies on finding a ``better`` empirical estimate of\nlosses at each device, using a weighted average of the losses across different\ndevices. It is devised from a Probably Approximately Correct (PAC) bound on the\ntrue loss in terms of the proposed empirical loss and is bounded by (i) the\nRademacher complexity, (ii) the discrepancy, (iii) and a penalty term. Using a\nsigned gradient feedback to find a personalised NN at each device, it is also\nproven to converge in a Rayleigh flat fading (in the uplink) channel, at a rate\nof the order max{1/SNR,1/sqrt(T)} Experimental results show that the proposed\nalgorithm outperforms locally trained devices as well as the conventionally\nused FedAvg and FedSGD algorithms under practical SNR regimes.",
          "link": "http://arxiv.org/abs/2108.02517",
          "publishedOn": "2021-08-06T00:51:46.337Z",
          "wordCount": 641,
          "title": "Multi-task Federated Edge Learning (MtFEEL) in Wireless Networks. (arXiv:2108.02517v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02307",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Dogan_I/0/1/0/all/0/1\">Ilgin Dogan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Shen_Z/0/1/0/all/0/1\">Zuo-Jun Max Shen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Aswani_A/0/1/0/all/0/1\">Anil Aswani</a>",
          "description": "The exploration/exploitation trade-off is an inherent challenge in\ndata-driven and adaptive control. Though this trade-off has been studied for\nmulti-armed bandits, reinforcement learning (RL) for finite Markov chains, and\nRL for linear control systems; it is less well-studied for learning-based\ncontrol of nonlinear control systems. A significant theoretical challenge in\nthe nonlinear setting is that, unlike the linear case, there is no explicit\ncharacterization of an optimal controller for a given set of cost and system\nparameters. We propose in this paper the use of a finite-horizon oracle\ncontroller with perfect knowledge of all system parameters as a reference for\noptimal control actions. First, this allows us to propose a new regret notion\nwith respect to this oracle finite-horizon controller. Second, this allows us\nto develop learning-based policies that we prove achieve low regret (i.e.,\nsquare-root regret up to a log-squared factor) with respect to this oracle\nfinite-horizon controller. This policy is developed in the context of\nlearning-based model predictive control (LBMPC). We conduct a statistical\nanalysis to prove finite sample concentration bounds for the estimation step of\nour policy, and then we perform a control-theoretic analysis using techniques\nfrom MPC- and optimization-theory to show this policy ensures closed-loop\nstability and achieves low regret. We conclude with numerical experiments on a\nmodel of heating, ventilation, and air-conditioning (HVAC) systems that show\nthe low regret of our policy in a setting where the cost function is\npartially-unknown to the controller.",
          "link": "http://arxiv.org/abs/2108.02307",
          "publishedOn": "2021-08-06T00:51:46.229Z",
          "wordCount": 681,
          "title": "Regret Analysis of Learning-Based MPC with Partially-Unknown Cost Function. (arXiv:2108.02307v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eisermann_A/0/1/0/all/0/1\">Aaron Eisermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jae Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>",
          "description": "Neural networks can be powerful function approximators, which are able to\nmodel high-dimensional feature distributions from a subset of examples drawn\nfrom the target distribution. Naturally, they perform well at generalizing\nwithin the limits of their target function, but they often fail to generalize\noutside of the explicitly learned feature space. It is therefore an open\nresearch topic whether and how neural network-based architectures can be\ndeployed for systematic reasoning. Many studies have shown evidence for poor\ngeneralization, but they often work with abstract data or are limited to\nsingle-channel input. Humans, however, learn and interact through a combination\nof multiple sensory modalities, and rarely rely on just one. To investigate\ncompositional generalization in a multimodal setting, we generate an extensible\ndataset with multimodal input sequences from simulation. We investigate the\ninfluence of the underlying training data distribution on compostional\ngeneralization in a minimal LSTM-based network trained in a supervised, time\ncontinuous setting. We find compositional generalization to fail in simple\nsetups while improving with the number of objects, actions, and particularly\nwith a lot of color overlaps between objects. Furthermore, multimodality\nstrongly improves compositional generalization in settings where a pure vision\nmodel struggles to generalize.",
          "link": "http://arxiv.org/abs/2108.02319",
          "publishedOn": "2021-08-06T00:51:46.200Z",
          "wordCount": 630,
          "title": "Generalization in Multimodal Language Learning from Simulation. (arXiv:2108.02319v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Favaro_S/0/1/0/all/0/1\">Stefano Favaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortini_S/0/1/0/all/0/1\">Sandra Fortini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peluchetti_S/0/1/0/all/0/1\">Stefano Peluchetti</a>",
          "description": "In modern deep learning, there is a recent and growing literature on the\ninterplay between large-width asymptotics for deep Gaussian neural networks\n(NNs), i.e. deep NNs with Gaussian-distributed weights, and classes of Gaussian\nstochastic processes (SPs). Such an interplay has proved to be critical in\nseveral contexts of practical interest, e.g. Bayesian inference under Gaussian\nSP priors, kernel regression for infinite-wide deep NNs trained via gradient\ndescent, and information propagation within infinite-wide NNs. Motivated by\nempirical analysis, showing the potential of replacing Gaussian distributions\nwith Stable distributions for the NN's weights, in this paper we investigate\nlarge-width asymptotics for (fully connected) feed-forward deep Stable NNs,\ni.e. deep NNs with Stable-distributed weights. First, we show that as the width\ngoes to infinity jointly over the NN's layers, a suitable rescaled deep Stable\nNN converges weakly to a Stable SP whose distribution is characterized\nrecursively through the NN's layers. Because of the non-triangular NN's\nstructure, this is a non-standard asymptotic problem, to which we propose a\nnovel and self-contained inductive approach, which may be of independent\ninterest. Then, we establish sup-norm convergence rates of a deep Stable NN to\na Stable SP, quantifying the critical difference between the settings of\n``joint growth\" and ``sequential growth\" of the width over the NN's layers. Our\nwork extends recent results on infinite-wide limits for deep Gaussian NNs to\nthe more general deep Stable NNs, providing the first result on convergence\nrates for infinite-wide deep NNs.",
          "link": "http://arxiv.org/abs/2108.02316",
          "publishedOn": "2021-08-06T00:51:46.193Z",
          "wordCount": 698,
          "title": "Deep Stable neural networks: large-width asymptotics and convergence rates. (arXiv:2108.02316v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02416",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Konstantinidis_K/0/1/0/all/0/1\">Konstantinos Konstantinidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthy_A/0/1/0/all/0/1\">Aditya Ramamoorthy</a>",
          "description": "State of the art machine learning models are routinely trained on large scale\ndistributed clusters. Crucially, such systems can be compromised when some of\nthe computing devices exhibit abnormal (Byzantine) behavior and return\narbitrary results to the parameter server (PS). This behavior may be attributed\nto a plethora of reasons including system failures and orchestrated attacks.\nExisting work suggests robust aggregation and/or computational redundancy to\nalleviate the effect of distorted gradients. However, most of these schemes are\nineffective when an adversary knows the task assignment and can judiciously\nchoose the attacked workers to induce maximal damage. Our proposed method Aspis\nassigns gradient computations to worker nodes using a subset-based assignment\nwhich allows for multiple consistency checks on the behavior of a worker node.\nExamination of the calculated gradients and post-processing (clique-finding in\nan appropriately constructed graph) by the central node allows for efficient\ndetection and subsequent exclusion of adversaries from the training process. We\nprove the Byzantine resilience and detection guarantees of Aspis under weak and\nstrong attacks and extensively evaluate the system on various large-scale\ntraining scenarios. The main metric for our experiments is the test accuracy\nfor which we demonstrate significant improvement of about 30% compared to many\nstate-of-the-art approaches on the CIFAR-10 dataset. The corresponding\nreduction of the fraction of corrupted gradients ranges from 16% to 98%.",
          "link": "http://arxiv.org/abs/2108.02416",
          "publishedOn": "2021-08-06T00:51:46.184Z",
          "wordCount": 668,
          "title": "Aspis: A Robust Detection System for Distributed Learning. (arXiv:2108.02416v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Peng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>",
          "description": "The past decade has seen the rapid development of Reinforcement Learning,\nwhich acquires impressive performance with numerous training resources.\nHowever, one of the greatest challenges in RL is generalization efficiency\n(i.e., generalization performance in a unit time). This paper proposes a\nframework of Active Reinforcement Learning (ARL) over MDPs to improve\ngeneralization efficiency in a limited resource by instance selection. Given a\nnumber of instances, the algorithm chooses out valuable instances as training\nsets while training the policy, thereby costing fewer resources. Unlike\nexisting approaches, we attempt to actively select and use training data rather\nthan train on all the given data, thereby costing fewer resources. Furthermore,\nwe introduce a general instance evaluation metrics and selection mechanism into\nthe framework. Experiments results reveal that the proposed framework with\nProximal Policy Optimization as policy optimizer can effectively improve\ngeneralization efficiency than unselect-ed and unbiased selected methods.",
          "link": "http://arxiv.org/abs/2108.02323",
          "publishedOn": "2021-08-06T00:51:46.175Z",
          "wordCount": 568,
          "title": "Active Reinforcement Learning over MDPs. (arXiv:2108.02323v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Terrill_C/0/1/0/all/0/1\">Caleb Terrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_F/0/1/0/all/0/1\">Fred Chu</a>",
          "description": "Modern hardware design trends have shifted towards specialized hardware\nacceleration for computationally intensive tasks like machine learning and\ncomputer vision. While these complex workloads can be accelerated by commercial\nGPUs, domain-specific hardware is far more optimal when needing to meet the\nstringent memory, throughput, and power constraints of mobile and embedded\ndevices. This paper proposes and evaluates a Binary-Enabled Architecture for\nNeural Network Acceleration (BEANNA), a neural network hardware accelerator\ncapable of processing both floating point and binary network layers. Through\nthe use of a novel 16x16 systolic array based matrix multiplier with processing\nelements that compute both floating point and binary multiply-adds, BEANNA\nseamlessly switches between high precision floating point and binary neural\nnetwork layers. Running at a clock speed of 100MHz, BEANNA achieves a peak\nthroughput of 52.8 GigaOps/second when operating in high precision mode, and\n820 GigaOps/second when operating in binary mode. Evaluation of BEANNA was\nperformed by comparing a hybrid network with floating point outer layers and\nbinary hidden layers to a network with only floating point layers. The hybrid\nnetwork accelerated using BEANNA achieved a 194% throughput increase, a 68%\nmemory usage decrease, and a 66% energy consumption decrease per inference, all\nthis at the cost of a mere 0.23% classification accuracy decrease on the MNIST\ndataset.",
          "link": "http://arxiv.org/abs/2108.02313",
          "publishedOn": "2021-08-06T00:51:46.168Z",
          "wordCount": 653,
          "title": "BEANNA: A Binary-Enabled Architecture for Neural Network Acceleration. (arXiv:2108.02313v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Quiros_A/0/1/0/all/0/1\">Adalberto Claudio Quiros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coudray_N/0/1/0/all/0/1\">Nicolas Coudray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeaton_A/0/1/0/all/0/1\">Anna Yeaton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunhem_W/0/1/0/all/0/1\">Wisuwat Sunhem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_Smith_R/0/1/0/all/0/1\">Roderick Murray-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsirigos_A/0/1/0/all/0/1\">Aristotelis Tsirigos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Ke Yuan</a>",
          "description": "Deep learning based analysis of histopathology images shows promise in\nadvancing the understanding of tumor progression, tumor micro-environment, and\ntheir underpinning biological processes. So far, these approaches have focused\non extracting information associated with annotations. In this work, we ask how\nmuch information can be learned from the tissue architecture itself.\n\nWe present an adversarial learning model to extract feature representations\nof cancer tissue, without the need for manual annotations. We show that these\nrepresentations are able to identify a variety of morphological characteristics\nacross three cancer types: Breast, colon, and lung. This is supported by 1) the\nseparation of morphologic characteristics in the latent space; 2) the ability\nto classify tissue type with logistic regression using latent representations,\nwith an AUC of 0.97 and 85% accuracy, comparable to supervised deep models; 3)\nthe ability to predict the presence of tumor in Whole Slide Images (WSIs) using\nmultiple instance learning (MIL), achieving an AUC of 0.98 and 94% accuracy.\n\nOur results show that our model captures distinct phenotypic characteristics\nof real tissue samples, paving the way for further understanding of tumor\nprogression and tumor micro-environment, and ultimately refining\nhistopathological classification for diagnosis and treatment. The code and\npretrained models are available at:\nhttps://github.com/AdalbertoCq/Adversarial-learning-of-cancer-tissue-representations",
          "link": "http://arxiv.org/abs/2108.02223",
          "publishedOn": "2021-08-06T00:51:46.030Z",
          "wordCount": 655,
          "title": "Adversarial learning of cancer tissue representations. (arXiv:2108.02223v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delbruck_T/0/1/0/all/0/1\">Tobi Delbruck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shih-Chii Liu</a>",
          "description": "Long Short-Term Memory (LSTM) recurrent networks are frequently used for\ntasks involving time sequential data such as speech recognition. However, it is\ndifficult to deploy these networks on hardware to achieve high throughput and\nlow latency because the fully-connected structure makes LSTM networks a\nmemory-bounded algorithm. Previous work in LSTM accelerators either exploited\nweight spatial sparsity or temporal sparsity. In this paper, we present a new\naccelerator called \"Spartus\" that exploits spatio-temporal sparsity to achieve\nultra-low latency inference. The spatial sparsity was induced using our\nproposed pruning method called Column-Balanced Targeted Dropout (CBTD) that\nleads to structured sparse weight matrices benefiting workload balance. It\nachieved up to 96% weight sparsity with negligible accuracy difference for an\nLSTM network trained on a TIMIT phone recognition task. To induce temporal\nsparsity in LSTM, we create the DeltaLSTM by extending the previous DeltaGRU\nmethod to the LSTM network. This combined sparsity saves on weight memory\naccess and associated arithmetic operations simultaneously. Spartus was\nimplemented on a Xilinx Zynq-7100 FPGA. The per-sample latency for a single\nDeltaLSTM layer of 1024 neurons running on Spartus is 1 us. Spartus achieved\n9.4 TOp/s effective batch-1 throughput and 1.1 TOp/J energy efficiency, which\nare respectively 4X and 7X higher than the previous state-of-the-art.",
          "link": "http://arxiv.org/abs/2108.02297",
          "publishedOn": "2021-08-06T00:51:46.007Z",
          "wordCount": 653,
          "title": "Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-temporal Sparsity. (arXiv:2108.02297v1 [cs.AR])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}