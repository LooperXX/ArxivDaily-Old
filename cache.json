{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.07935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1\">Joseph Marvin Imperial</a>",
          "description": "Automatic readability assessment (ARA) is the task of evaluating the level of\nease or difficulty of text documents for a target audience. For researchers,\none of the many open problems in the field is to make such models trained for\nthe task show efficacy even for low-resource languages. In this study, we\npropose an alternative way of utilizing the information-rich embeddings of BERT\nmodels with handcrafted linguistic features through a combined method for\nreadability assessment. Results show that the proposed method outperforms\nclassical approaches in readability assessment using English and Filipino\ndatasets, obtaining as high as 12.4% increase in F1 performance. We also show\nthat the general information encoded in BERT embeddings can be used as a\nsubstitute feature set for low-resource languages like Filipino with limited\nsemantic and syntactic NLP tools to explicitly extract feature values for the\ntask.",
          "link": "http://arxiv.org/abs/2106.07935",
          "publishedOn": "2021-08-02T01:58:23.538Z",
          "wordCount": 592,
          "title": "BERT Embeddings for Automatic Readability Assessment. (arXiv:2106.07935v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohitash Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_A/0/1/0/all/0/1\">Aswin Krishna</a>",
          "description": "Social scientists and psychologists take interest in understanding how people\nexpress emotions and sentiments when dealing with catastrophic events such as\nnatural disasters, political unrest, and terrorism. The COVID-19 pandemic is a\ncatastrophic event that has raised a number of psychological issues such as\ndepression given abrupt social changes and lack of employment. Advancements of\ndeep learning-based language models have been promising for sentiment analysis\nwith data from social networks such as Twitter. Given the situation with\nCOVID-19 pandemic, different countries had different peaks where the rise and\nfall of new cases affected lock-downs which directly affected the economy and\nemployment. During the rise of COVID-19 cases with stricter lock-downs, people\nhave been expressing their sentiments in social media. This can provide a deep\nunderstanding of human psychology during catastrophic events. In this paper, we\npresent a framework that employs deep learning-based language models via long\nshort-term memory (LSTM) recurrent neural networks for sentiment analysis\nduring the rise of novel COVID-19 cases in India. The framework features LSTM\nlanguage model with a global vector embedding and state-of-art BERT language\nmodel. We review the sentiments expressed for selective months in 2020 which\ncovers the first major peak of novel cases in India. Our framework utilises\nmulti-label sentiment classification where more than one sentiment can be\nexpressed at once. Our results indicate that the majority of the tweets have\nbeen positive with high levels of optimism during the rise of the novel\nCOVID-19 cases and the number of tweets significantly lowered towards the peak.\nThe predictions generally indicate that although the majority have been\noptimistic, a significant group of population has been annoyed towards the way\nthe pandemic was handled by the authorities.",
          "link": "http://arxiv.org/abs/2104.10662",
          "publishedOn": "2021-08-02T01:58:23.508Z",
          "wordCount": 798,
          "title": "COVID-19 sentiment analysis via deep learning during the rise of novel cases. (arXiv:2104.10662v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ambroszkiewicz_S/0/1/0/all/0/1\">Stanislaw Ambroszkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartyna_W/0/1/0/all/0/1\">Waldemar Bartyna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bylka_S/0/1/0/all/0/1\">Stanislaw Bylka</a>",
          "description": "Cloud Native Application CNApp (as a distributed system) is a collection of\nindependent components (micro-services) interacting via communication\nprotocols. This gives rise to present an abstract architecture of CNApp as\ndynamically re-configurable acyclic directed multi graph where vertices are\nmicroservices, and edges are the protocols. Generic mechanisms for such\nreconfigurations evidently correspond to higher-level functions (functionals).\nThis implies also internal abstract architecture of microservice as a\ncollection of event-triggered serverless functions (including functions\nimplementing the protocols) that are dynamically composed into event-dependent\ndata-flow graphs. Again, generic mechanisms for such compositions correspond to\ncalculus of functionals and relations.",
          "link": "http://arxiv.org/abs/2105.10362",
          "publishedOn": "2021-08-02T01:58:23.482Z",
          "wordCount": 589,
          "title": "Functionals in the Clouds: An abstract architecture of serverless Cloud-Native Apps. (arXiv:2105.10362v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1\">Matteo Stefanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1\">Silvia Cascianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Connecting Vision and Language plays an essential role in Generative\nIntelligence. For this reason, large research efforts have been devoted to\nimage captioning, i.e. describing images with syntactically and semantically\nmeaningful sentences. Starting from 2015 the task has generally been addressed\nwith pipelines composed of a visual encoder and a language model for text\ngeneration. During these years, both components have evolved considerably\nthrough the exploitation of object regions, attributes, the introduction of\nmulti-modal connections, fully-attentive approaches, and BERT-like early-fusion\nstrategies. However, regardless of the impressive results, research in image\ncaptioning has not reached a conclusive answer yet. This work aims at providing\na comprehensive overview of image captioning approaches, from visual encoding\nand text generation to training strategies, datasets, and evaluation metrics.\nIn this respect, we quantitatively compare many relevant state-of-the-art\napproaches to identify the most impactful technical innovations in\narchitectures and training strategies. Moreover, many variants of the problem\nand its open challenges are discussed. The final goal of this work is to serve\nas a tool for understanding the existing literature and highlighting the future\ndirections for a research area where Computer Vision and Natural Language\nProcessing can find an optimal synergy.",
          "link": "http://arxiv.org/abs/2107.06912",
          "publishedOn": "2021-08-02T01:58:23.400Z",
          "wordCount": 659,
          "title": "From Show to Tell: A Survey on Image Captioning. (arXiv:2107.06912v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boenninghoff_B/0/1/0/all/0/1\">Benedikt Boenninghoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickel_R/0/1/0/all/0/1\">Robert M. Nickel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolossa_D/0/1/0/all/0/1\">Dorothea Kolossa</a>",
          "description": "The PAN 2021 authorship verification (AV) challenge is part of a three-year\nstrategy, moving from a cross-topic/closed-set AV task to a\ncross-topic/open-set AV task over a collection of fanfiction texts. In this\nwork, we present a novel hybrid neural-probabilistic framework that is designed\nto tackle the challenges of the 2021 task. Our system is based on our 2020\nwinning submission, with updates to significantly reduce sensitivities to\ntopical variations and to further improve the system's calibration by means of\nan uncertainty-adaptation layer. Our framework additionally includes an\nout-of-distribution detector (O2D2) for defining non-responses. Our proposed\nsystem outperformed all other systems that participated in the PAN 2021 AV\ntask.",
          "link": "http://arxiv.org/abs/2106.15825",
          "publishedOn": "2021-08-02T01:58:23.394Z",
          "wordCount": 571,
          "title": "O2D2: Out-Of-Distribution Detector to Capture Undecidable Trials in Authorship Verification. (arXiv:2106.15825v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>",
          "description": "We present our works on SemEval-2021 Task 5 about Toxic Spans Detection. This\ntask aims to build a model for identifying toxic words in whole posts. We use\nthe BiLSTM-CRF model combining with ToxicBERT Classification to train the\ndetection model for identifying toxic words in posts. Our model achieves 62.23%\nby F1-score on the Toxic Spans Detection task.",
          "link": "http://arxiv.org/abs/2104.10100",
          "publishedOn": "2021-08-02T01:58:23.371Z",
          "wordCount": 555,
          "title": "UIT-ISE-NLP at SemEval-2021 Task 5: Toxic Spans Detection with BiLSTM-CRF and ToxicBERT Comment Classification. (arXiv:2104.10100v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cetoli_A/0/1/0/all/0/1\">Alberto Cetoli</a>",
          "description": "This paper proposes a message-passing mechanism to address language\nmodelling. A new layer type is introduced that aims to substitute\nself-attention for unidirectional sequence generation tasks. The system is\nshown to be competitive with existing methods: Given N tokens, the\ncomputational complexity is O(N logN) and the memory complexity is O(N) under\nreasonable assumptions. In the end, the Dispatcher layer is seen to achieve\ncomparable perplexity to prior results while being more efficient.",
          "link": "http://arxiv.org/abs/2105.03994",
          "publishedOn": "2021-08-02T01:58:23.363Z",
          "wordCount": 522,
          "title": "Dispatcher: A Message-Passing Approach To Language Modelling. (arXiv:2105.03994v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>",
          "description": "Recently, the development of pre-trained language models has brought natural\nlanguage processing (NLP) tasks to the new state-of-the-art. In this paper we\nexplore the efficiency of various pre-trained language models. We pre-train a\nlist of transformer-based models with the same amount of text and the same\ntraining steps. The experimental results shows that the most improvement upon\nthe origin BERT is adding the RNN-layer to capture more contextual information\nfor the transformer-encoder layers.",
          "link": "http://arxiv.org/abs/2106.11483",
          "publishedOn": "2021-08-02T01:58:23.356Z",
          "wordCount": 527,
          "title": "A Comprehensive Exploration of Pre-training Language Models. (arXiv:2106.11483v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.06402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allein_L/0/1/0/all/0/1\">Liesbeth Allein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>",
          "description": "Truth can vary over time. Fact-checking decisions on claim veracity should\ntherefore take into account temporal information of both the claim and\nsupporting or refuting evidence. In this work, we investigate the hypothesis\nthat the timestamp of a Web page is crucial to how it should be ranked for a\ngiven claim. We delineate four temporal ranking methods that constrain evidence\nranking differently and simulate hypothesis-specific evidence rankings given\nthe evidence timestamps as gold standard. Evidence ranking in three\nfact-checking models is ultimately optimized using a learning-to-rank loss\nfunction. Our study reveals that time-aware evidence ranking not only surpasses\nrelevance assumptions based purely on semantic similarity or position in a\nsearch results list, but also improves veracity predictions of time-sensitive\nclaims in particular.",
          "link": "http://arxiv.org/abs/2009.06402",
          "publishedOn": "2021-08-02T01:58:23.320Z",
          "wordCount": 595,
          "title": "Time-Aware Evidence Ranking for Fact-Checking. (arXiv:2009.06402v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frey_B/0/1/0/all/0/1\">Benjamin Frey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "We introduce ChrEnTranslate, an online machine translation demonstration\nsystem for translation between English and an endangered language Cherokee. It\nsupports both statistical and neural translation models as well as provides\nquality estimation to inform users of reliability, two user feedback interfaces\nfor experts and common users respectively, example inputs to collect human\ntranslations for monolingual data, word alignment visualization, and relevant\nterms from the Cherokee-English dictionary. The quantitative evaluation\ndemonstrates that our backbone translation models achieve state-of-the-art\ntranslation performance and our quality estimation well correlates with both\nBLEU and human judgment. By analyzing 216 pieces of expert feedback, we find\nthat NMT is preferable because it copies less than SMT, and, in general,\ncurrent models can translate fragments of the source sentence but make major\nmistakes. When we add these 216 expert-corrected parallel texts into the\ntraining set and retrain models, equal or slightly better performance is\nobserved, which demonstrates indicates the potential of human-in-the-loop\nlearning. Our online demo is at https://chren.cs.unc.edu/; our code is\nopen-sourced at https://github.com/ZhangShiyue/ChrEnTranslate; and our data is\navailable at https://github.com/ZhangShiyue/ChrEn.",
          "link": "http://arxiv.org/abs/2107.14800",
          "publishedOn": "2021-08-02T01:58:23.313Z",
          "wordCount": 626,
          "title": "ChrEnTranslate: Cherokee-English Machine Translation Demo with Quality Estimation and Corrective Feedback. (arXiv:2107.14800v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1\">Jean-Baptiste Alayrac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doersch_C/0/1/0/all/0/1\">Carl Doersch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_C/0/1/0/all/0/1\">Catalin Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">David Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1\">Skanda Koppula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andrew Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1\">Evan Shelhamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1\">Olivier H&#xe9;naff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1\">Matthew M. Botvinick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Jo&#xe3;o Carreira</a>",
          "description": "The recently-proposed Perceiver model obtains good results on several domains\n(images, audio, multimodal, point clouds) while scaling linearly in compute and\nmemory with the input size. While the Perceiver supports many kinds of inputs,\nit can only produce very simple outputs such as class scores. Perceiver IO\novercomes this limitation without sacrificing the original's appealing\nproperties by learning to flexibly query the model's latent space to produce\noutputs of arbitrary size and semantics. Perceiver IO still decouples model\ndepth from data size and still scales linearly with data size, but now with\nrespect to both input and output sizes. The full Perceiver IO model achieves\nstrong results on tasks with highly structured output spaces, such as natural\nlanguage and visual understanding, StarCraft II, and multi-task and multi-modal\ndomains. As highlights, Perceiver IO matches a Transformer-based BERT baseline\non the GLUE language benchmark without the need for input tokenization and\nachieves state-of-the-art performance on Sintel optical flow estimation.",
          "link": "http://arxiv.org/abs/2107.14795",
          "publishedOn": "2021-08-02T01:58:23.302Z",
          "wordCount": 639,
          "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs. (arXiv:2107.14795v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "Recent years have brought about an interest in the challenging task of\nsummarizing conversation threads (meetings, online discussions, etc.). Such\nsummaries help analysis of the long text to quickly catch up with the decisions\nmade and thus improve our work or communication efficiency. To spur research in\nthread summarization, we have developed an abstractive Email Thread\nSummarization (EmailSum) dataset, which contains human-annotated short (<30\nwords) and long (<100 words) summaries of 2549 email threads (each containing 3\nto 10 emails) over a wide variety of topics. We perform a comprehensive\nempirical study to explore different summarization techniques (including\nextractive and abstractive methods, single-document and hierarchical models, as\nwell as transfer and semisupervised learning) and conduct human evaluations on\nboth short and long summary generation tasks. Our results reveal the key\nchallenges of current abstractive summarization models in this task, such as\nunderstanding the sender's intent and identifying the roles of sender and\nreceiver. Furthermore, we find that widely used automatic evaluation metrics\n(ROUGE, BERTScore) are weakly correlated with human judgments on this email\nthread summarization task. Hence, we emphasize the importance of human\nevaluation and the development of better metrics by the community. Our code and\nsummary data have been made available at:\nhttps://github.com/ZhangShiyue/EmailSum",
          "link": "http://arxiv.org/abs/2107.14691",
          "publishedOn": "2021-08-02T01:58:23.296Z",
          "wordCount": 641,
          "title": "EmailSum: Abstractive Email Thread Summarization. (arXiv:2107.14691v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14638",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paula_A/0/1/0/all/0/1\">Amauri J Paula</a>",
          "description": "It is presented here a machine learning-based (ML) natural language\nprocessing (NLP) approach capable to automatically recognize and extract\ncategorical and numerical parameters from a corpus of articles. The approach\n(named a.RIX) operates with a concomitant/interchangeable use of ML models such\nas neuron networks (NNs), latent semantic analysis (LSA) and naive-Bayes\nclassifiers (NBC), and a pattern recognition model using regular expression\n(REGEX). To demonstrate the efficiency of the a.RIX engine, it was processed a\ncorpus of 7,873 scientific articles dealing with natural products (NPs). The\nengine automatically extracts categorical and numerical parameters such as (i)\nthe plant species from which active molecules are extracted, (ii) the\nmicroorganisms species for which active molecules can act against, and (iii)\nthe values of minimum inhibitory concentration (MIC) against these\nmicroorganisms. The parameters are extracted without part-of-speech tagging\n(POS) and named entity recognition (NER) approaches (i.e. without the need of\ntext annotation), and the models training is performed with unsupervised\napproaches. In this way, a.RIX can be essentially used on articles from any\nscientific field. Finally, it has a potential to make obsolete the currently\nused articles reviewing process in some areas, specially those in which texts\nstructure, text semantics and latent knowledge is captured by machine learning\nmodels.",
          "link": "http://arxiv.org/abs/2107.14638",
          "publishedOn": "2021-08-02T01:58:23.266Z",
          "wordCount": 646,
          "title": "An automated domain-independent text reading, interpreting and extracting approach for reviewing the scientific literature. (arXiv:2107.14638v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1911.12377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Landi_F/0/1/0/all/0/1\">Federico Landi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corsini_M/0/1/0/all/0/1\">Massimiliano Corsini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Vision-and-Language Navigation (VLN) is a challenging task in which an agent\nneeds to follow a language-specified path to reach a target destination. The\ngoal gets even harder as the actions available to the agent get simpler and\nmove towards low-level, atomic interactions with the environment. This setting\ntakes the name of low-level VLN. In this paper, we strive for the creation of\nan agent able to tackle three key issues: multi-modality, long-term\ndependencies, and adaptability towards different locomotive settings. To that\nend, we devise \"Perceive, Transform, and Act\" (PTA): a fully-attentive VLN\narchitecture that leaves the recurrent approach behind and the first\nTransformer-like architecture incorporating three different modalities -\nnatural language, images, and low-level actions for the agent control. In\nparticular, we adopt an early fusion strategy to merge lingual and visual\ninformation efficiently in our encoder. We then propose to refine the decoding\nphase with a late fusion extension between the agent's history of actions and\nthe perceptual modalities. We experimentally validate our model on two\ndatasets: PTA achieves promising results in low-level VLN on R2R and achieves\ngood performance in the recently proposed R4R benchmark. Our code is publicly\navailable at https://github.com/aimagelab/perceive-transform-and-act.",
          "link": "http://arxiv.org/abs/1911.12377",
          "publishedOn": "2021-08-02T01:58:23.225Z",
          "wordCount": 687,
          "title": "Multimodal Attention Networks for Low-Level Vision-and-Language Navigation. (arXiv:1911.12377v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_D/0/1/0/all/0/1\">Deepak Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casper_J/0/1/0/all/0/1\">Jared Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeGresley_P/0/1/0/all/0/1\">Patrick LeGresley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1\">Mostofa Patwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korthikanti_V/0/1/0/all/0/1\">Vijay Anand Korthikanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainbrand_D/0/1/0/all/0/1\">Dmitri Vainbrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashinkunti_P/0/1/0/all/0/1\">Prethvi Kashinkunti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernauer_J/0/1/0/all/0/1\">Julie Bernauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phanishayee_A/0/1/0/all/0/1\">Amar Phanishayee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>",
          "description": "Large language models have led to state-of-the-art accuracies across a range\nof tasks. However, training these models efficiently is challenging for two\nreasons: a) GPU memory capacity is limited, making it impossible to fit large\nmodels on even a multi-GPU server; b) the number of compute operations required\nto train these models can result in unrealistically long training times.\nConsequently, new methods of model parallelism such as tensor and pipeline\nparallelism have been proposed. Unfortunately, naive usage of these methods\nleads to fundamental scaling issues at thousands of GPUs, e.g., due to\nexpensive cross-node communication or devices spending significant time waiting\non other devices to make progress.\n\nIn this paper, we show how different types of parallelism methods (tensor,\npipeline, and data parallelism) can be composed to scale to thousands of GPUs\nand models with trillions of parameters. We survey techniques for pipeline\nparallelism and propose a novel interleaved pipeline parallelism schedule that\ncan improve throughput by 10+% with memory footprint comparable to existing\napproaches. We quantitatively study the trade-offs between tensor, pipeline,\nand data parallelism, and provide intuition as to how to configure distributed\ntraining of a large model. Our approach allows us to perform training\niterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs\nwith achieved per-GPU throughput of 52% of theoretical peak. Our code is open\nsourced at https://github.com/nvidia/megatron-lm.",
          "link": "http://arxiv.org/abs/2104.04473",
          "publishedOn": "2021-08-02T01:58:23.215Z",
          "wordCount": 734,
          "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM. (arXiv:2104.04473v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14596",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongtong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fangxiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>",
          "description": "Multimodal pre-training models, such as LXMERT, have achieved excellent\nresults in downstream tasks. However, current pre-trained models require large\namounts of training data and have huge model sizes, which make them difficult\nto apply in low-resource situations. How to obtain similar or even better\nperformance than a larger model under the premise of less pre-training data and\nsmaller model size has become an important problem. In this paper, we propose a\nnew Multi-stage Pre-training (MSP) method, which uses information at different\ngranularities from word, phrase to sentence in both texts and images to\npre-train the model in stages. We also design several different pre-training\ntasks suitable for the information granularity in different stage in order to\nefficiently capture the diverse knowledge from a limited corpus. We take a\nSimplified LXMERT (LXMERT- S), which has only 45.9% parameters of the original\nLXMERT model and 11.76% of the original pre-training data as the testbed of our\nMSP method. Experimental results show that our method achieves comparable\nperformance to the original LXMERT model in all downstream tasks, and even\noutperforms the original model in Image-Text Retrieval task.",
          "link": "http://arxiv.org/abs/2107.14596",
          "publishedOn": "2021-08-02T01:58:23.204Z",
          "wordCount": 617,
          "title": "Multi-stage Pre-training over Simplified Multimodal Pre-training Models. (arXiv:2107.14596v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hauer_B/0/1/0/all/0/1\">Bradley Hauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondrak_G/0/1/0/all/0/1\">Grzegorz Kondrak</a>",
          "description": "The WiC task has attracted considerable attention in the NLP community, as\ndemonstrated by the popularity of the recent MCL-WiC SemEval task. WSD systems\nand lexical resources have been used for the WiC task, as well as for WiC\ndataset construction. TSV is another task related to both WiC and WSD. We aim\nto establish the exact relationship between WiC, TSV, and WSD. We demonstrate\nthat these semantic classification problems can be pairwise reduced to each\nother, and so they are theoretically equivalent. We analyze the existing WiC\ndatasets to validate this equivalence hypothesis. We conclude that our\nunderstanding of semantic tasks can be increased through the applications of\ntools from theoretical computer science. Our findings also suggests that more\nefficient and simpler methods for one of these tasks could be successfully\napplied in the other two.",
          "link": "http://arxiv.org/abs/2107.14352",
          "publishedOn": "2021-08-02T01:58:23.189Z",
          "wordCount": 572,
          "title": "WiC = TSV = WSD: On the Equivalence of Three Semantic Tasks. (arXiv:2107.14352v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mandy Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>",
          "description": "In this work, we take the first steps towards building a universal rewriter:\na model capable of rewriting text in any language to exhibit a wide variety of\nattributes, including styles and languages, while preserving as much of the\noriginal semantics as possible. In addition to obtaining state-of-the-art\nresults on unsupervised translation, we also demonstrate the ability to do\nzero-shot sentiment transfer in non-English languages using only English\nexemplars for sentiment. We then show that our model is able to modify multiple\nattributes at once, for example adjusting both language and sentiment jointly.\nFinally, we show that our model is capable of performing zero-shot\nformality-sensitive translation.",
          "link": "http://arxiv.org/abs/2107.14749",
          "publishedOn": "2021-08-02T01:58:23.174Z",
          "wordCount": 534,
          "title": "Towards Universality in Multilingual Text Rewriting. (arXiv:2107.14749v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarap_A/0/1/0/all/0/1\">Abien Fred Agarap</a>",
          "description": "We define disentanglement as how far class-different data points from each\nother are, relative to the distances among class-similar data points. When\nmaximizing disentanglement during representation learning, we obtain a\ntransformed feature representation where the class memberships of the data\npoints are preserved. If the class memberships of the data points are\npreserved, we would have a feature representation space in which a nearest\nneighbour classifier or a clustering algorithm would perform well. We take\nadvantage of this method to learn better natural language representation, and\nemploy it on text classification and text clustering tasks. Through\ndisentanglement, we obtain text representations with better-defined clusters\nand improve text classification performance. Our approach had a test\nclassification accuracy of as high as 90.11% and test clustering accuracy of\n88% on the AG News dataset, outperforming our baseline models -- without any\nother training tricks or regularization.",
          "link": "http://arxiv.org/abs/2107.14597",
          "publishedOn": "2021-08-02T01:58:23.167Z",
          "wordCount": 584,
          "title": "Text Classification and Clustering with Annealing Soft Nearest Neighbor Loss. (arXiv:2107.14597v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daphne Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadrzadeh_M/0/1/0/all/0/1\">Mehrnoosh Sadrzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abramsky_S/0/1/0/all/0/1\">Samson Abramsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cervantes_V/0/1/0/all/0/1\">Victor H. Cervantes</a>",
          "description": "Language is contextual as meanings of words are dependent on their contexts.\nContextuality is, concomitantly, a well-defined concept in quantum mechanics\nwhere it is considered a major resource for quantum computations. We\ninvestigate whether natural language exhibits any of the quantum mechanics'\ncontextual features. We show that meaning combinations in ambiguous phrases can\nbe modelled in the sheaf-theoretic framework for quantum contextuality, where\nthey can become possibilistically contextual. Using the framework of\nContextuality-by-Default (CbD), we explore the probabilistic variants of these\nand show that CbD-contextuality is also possible.",
          "link": "http://arxiv.org/abs/2107.14589",
          "publishedOn": "2021-08-02T01:58:23.159Z",
          "wordCount": 528,
          "title": "On the Quantum-like Contextuality of Ambiguous Phrases. (arXiv:2107.14589v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_R/0/1/0/all/0/1\">Runzhe Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>",
          "description": "The high-quality translation results produced by machine translation (MT)\nsystems still pose a huge challenge for automatic evaluation. Current MT\nevaluation pays the same attention to each sentence component, while the\nquestions of real-world examinations (e.g., university examinations) have\ndifferent difficulties and weightings. In this paper, we propose a novel\ndifficulty-aware MT evaluation metric, expanding the evaluation dimension by\ntaking translation difficulty into consideration. A translation that fails to\nbe predicted by most MT systems will be treated as a difficult one and assigned\na large weight in the final score function, and conversely. Experimental\nresults on the WMT19 English-German Metrics shared tasks show that our proposed\nmethod outperforms commonly used MT metrics in terms of human correlation. In\nparticular, our proposed method performs well even when all the MT systems are\nvery competitive, which is when most existing metrics fail to distinguish\nbetween them. The source code is freely available at\nhttps://github.com/NLP2CT/Difficulty-Aware-MT-Evaluation.",
          "link": "http://arxiv.org/abs/2107.14402",
          "publishedOn": "2021-08-02T01:58:23.149Z",
          "wordCount": 589,
          "title": "Difficulty-Aware Machine Translation Evaluation. (arXiv:2107.14402v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mukhtar_S/0/1/0/all/0/1\">Shakeeb A. M. Mukhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joglekar_P/0/1/0/all/0/1\">Pushkar S. Joglekar</a>",
          "description": "One of the major problems writers and poets face is the writer's block. It is\na condition in which an author loses the ability to produce new work or\nexperiences a creative slowdown. The problem is more difficult in the context\nof poetry than prose, as in the latter case authors need not be very concise\nwhile expressing their ideas, also the various aspects such as rhyme, poetic\nmeters are not relevant for prose. One of the most effective ways to overcome\nthis writing block for poets can be, to have a prompt system, which would help\ntheir imagination and open their minds for new ideas. A prompt system can\npossibly generate one liner, two liner or full ghazals. The purpose of this\nwork is to give an ode to the Urdu, Hindi poets, and helping them start their\nnext line of poetry, a couplet or a complete ghazal considering various factors\nlike rhymes, refrain, and meters. The result will help aspiring poets to get\nnew ideas and help them overcome writer's block by auto-generating pieces of\npoetry using Deep Learning techniques. A concern with creative works like this,\nespecially in the literary context, is to ensure that the output is not\nplagiarized. This work also addresses the concern and makes sure that the\nresulting odes are not exact match with input data using parameters like\ntemperature and manual plagiarism check against input corpus. To the best of\nour knowledge, although the automatic text generation problem has been studied\nquite extensively in the literature, the specific problem of Urdu, Hindi poetry\ngeneration has not been explored much. Apart from developing system to\nauto-generate Urdu, Hindi poetry, another key contribution of our work is to\ncreate a cleaned and preprocessed corpus of Urdu, Hindi poetry (derived from\nauthentic resources) and making it freely available for researchers in the\narea.",
          "link": "http://arxiv.org/abs/2107.14587",
          "publishedOn": "2021-08-02T01:58:23.142Z",
          "wordCount": 741,
          "title": "Urdu & Hindi Poetry Generation using Neural Networks. (arXiv:2107.14587v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lamers_W/0/1/0/all/0/1\">Wout S. Lamers</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Boyack_K/0/1/0/all/0/1\">Kevin Boyack</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Lariviere_V/0/1/0/all/0/1\">Vincent Larivi&#xe8;re</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_C/0/1/0/all/0/1\">Cassidy R. Sugimoto</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Eck_N/0/1/0/all/0/1\">Nees Jan van Eck</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Waltman_L/0/1/0/all/0/1\">Ludo Waltman</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Murray_D/0/1/0/all/0/1\">Dakota Murray</a> (4) ((1) Centre for Science and Technology Studies, Leiden University, Leiden, Netherlands, (2) SciTech Strategies, Inc., Albuquerque, NM, USA, (3) &#xc9;cole de biblioth&#xe9;conomie et des sciences de l&#x27;information, Universit&#xe9; de Montr&#xe9;al, Canada, (4) School of Informatics, Computing, and Engineering, Indiana University Bloomington, IN, USA)",
          "description": "Disagreement is essential to scientific progress. However, the extent of\ndisagreement in science, its evolution over time, and the fields in which it\nhappens, remains largely unknown. Leveraging a massive collection of scientific\ntexts, we develop a cue-phrase based approach to identify instances of\ndisagreement citations across more than four million scientific articles. Using\nthis method, we construct an indicator of disagreement across scientific fields\nover the 2000-2015 period. In contrast with black-box text classification\nmethods, our framework is transparent and easily interpretable. We reveal a\ndisciplinary spectrum of disagreement, with higher disagreement in the social\nsciences and lower disagreement in physics and mathematics. However, detailed\ndisciplinary analysis demonstrates heterogeneity across sub-fields, revealing\nthe importance of local disciplinary cultures and epistemic characteristics of\ndisagreement. Paper-level analysis reveals notable episodes of disagreement in\nscience, and illustrates how methodological artefacts can confound analyses of\nscientific texts. These findings contribute to a broader understanding of\ndisagreement and establish a foundation for future research to understanding\nkey processes underlying scientific progress.",
          "link": "http://arxiv.org/abs/2107.14641",
          "publishedOn": "2021-08-02T01:58:23.132Z",
          "wordCount": 658,
          "title": "Measuring Disagreement in Science. (arXiv:2107.14641v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pillai_N/0/1/0/all/0/1\">Nisha Pillai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matuszek_C/0/1/0/all/0/1\">Cynthia Matuszek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1\">Francis Ferraro</a>",
          "description": "We propose a learning system in which language is grounded in visual percepts\nwithout specific pre-defined categories of terms. We present a unified\ngenerative method to acquire a shared semantic/visual embedding that enables\nthe learning of language about a wide range of real-world objects. We evaluate\nthe efficacy of this learning by predicting the semantics of objects and\ncomparing the performance with neural and non-neural inputs. We show that this\ngenerative approach exhibits promising results in language grounding without\npre-specifying visual categories under low resource settings. Our experiments\ndemonstrate that this approach is generalizable to multilingual, highly varied\ndatasets.",
          "link": "http://arxiv.org/abs/2107.14593",
          "publishedOn": "2021-08-02T01:58:23.110Z",
          "wordCount": 552,
          "title": "Neural Variational Learning for Grounded Language Acquisition. (arXiv:2107.14593v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14740",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1\">Shraey Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>",
          "description": "There is unison is the scientific community about human induced climate\nchange. Despite this, we see the web awash with claims around climate change\nscepticism, thus driving the need for fact checking them but at the same time\nproviding an explanation and justification for the fact check. Scientists and\nexperts have been trying to address it by providing manually written feedback\nfor these claims. In this paper, we try to aid them by automating generating\nexplanation for a predicted veracity label for a claim by deploying the\napproach used in open domain question answering of a fusion in decoder\naugmented with retrieved supporting passages from an external knowledge. We\nexperiment with different knowledge sources, retrievers, retriever depths and\ndemonstrate that even a small number of high quality manually written\nexplanations can help us in generating good explanations.",
          "link": "http://arxiv.org/abs/2107.14740",
          "publishedOn": "2021-08-02T01:58:23.097Z",
          "wordCount": 569,
          "title": "Automatic Claim Review for Climate Science via Explanation Generation. (arXiv:2107.14740v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baruah_A/0/1/0/all/0/1\">Arup Baruah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_K/0/1/0/all/0/1\">Kaushik Amar Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbhuiya_F/0/1/0/all/0/1\">Ferdous Ahmed Barbhuiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_K/0/1/0/all/0/1\">Kuntal Dey</a>",
          "description": "This paper presents the results obtained by our SVM and XLM-RoBERTa based\nclassifiers in the shared task Dravidian-CodeMix-HASOC 2020. The SVM classifier\ntrained using TF-IDF features of character and word n-grams performed the best\non the code-mixed Malayalam text. It obtained a weighted F1 score of 0.95 (1st\nRank) and 0.76 (3rd Rank) on the YouTube and Twitter dataset respectively. The\nXLM-RoBERTa based classifier performed the best on the code-mixed Tamil text.\nIt obtained a weighted F1 score of 0.87 (3rd Rank) on the code-mixed Tamil\nTwitter dataset.",
          "link": "http://arxiv.org/abs/2107.14336",
          "publishedOn": "2021-08-02T01:58:23.087Z",
          "wordCount": 525,
          "title": "IIITG-ADBU@HASOC-Dravidian-CodeMix-FIRE2020: Offensive Content Detection in Code-Mixed Dravidian Text. (arXiv:2107.14336v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">GuoLiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyang Li</a>",
          "description": "Although attention-based Neural Machine Translation has achieved remarkable\nprogress in recent layers, it still suffers from issue of making insufficient\nuse of the output of each layer. In transformer, it only uses the top layer of\nencoder and decoder in the subsequent process, which makes it impossible to\ntake advantage of the useful information in other layers. To address this\nissue, we propose a residual tree aggregation of layers for Transformer(RTAL),\nwhich helps to fuse information across layers. Specifically, we try to fuse the\ninformation across layers by constructing a post-order binary tree. In\nadditional to the last node, we add the residual connection to the process of\ngenerating child nodes. Our model is based on the Neural Machine Translation\nmodel Transformer and we conduct our experiments on WMT14 English-to-German and\nWMT17 English-to-France translation tasks. Experimental results across language\npairs show that the proposed approach outperforms the strong baseline model\nsignificantly",
          "link": "http://arxiv.org/abs/2107.14590",
          "publishedOn": "2021-08-02T01:58:23.066Z",
          "wordCount": 584,
          "title": "Residual Tree Aggregation of Layers for Neural Machine Translation. (arXiv:2107.14590v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Apostolova_E/0/1/0/all/0/1\">Emilia Apostolova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karim_F/0/1/0/all/0/1\">Fazle Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muscioni_G/0/1/0/all/0/1\">Guido Muscioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_A/0/1/0/all/0/1\">Anubhav Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clyman_J/0/1/0/all/0/1\">Jeffrey Clyman</a>",
          "description": "In this work, we modify and apply self-supervision techniques to the domain\nof medical health insurance claims. We model patients' healthcare claims\nhistory analogous to free-text narratives, and introduce pre-trained `prior\nknowledge', later utilized for patient outcome predictions on a challenging\ntask: predicting Covid-19 hospitalization, given a patient's pre-Covid-19\ninsurance claims history. Results suggest that pre-training on insurance claims\nnot only produces better prediction performance, but, more importantly,\nimproves the model's `clinical trustworthiness' and model\nstability/reliability.",
          "link": "http://arxiv.org/abs/2107.14591",
          "publishedOn": "2021-08-02T01:58:23.044Z",
          "wordCount": 569,
          "title": "Self-supervision for health insurance claims data: a Covid-19 use case. (arXiv:2107.14591v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lei Lin</a>",
          "description": "It is expensive to evaluate the results of Machine Translation(MT), which\nusually requires manual translation as a reference. Machine Translation Quality\nEstimation (QE) is a task of predicting the quality of machine translations\nwithout relying on any reference. Recently, the emergence of\npredictor-estimator framework which trains the predictor as a feature extractor\nand estimator as a QE predictor, and pre-trained language models(PLM) have\nachieved promising QE performance. However, we argue that there are still gaps\nbetween the predictor and the estimator in both data quality and training\nobjectives, which preclude QE models from benefiting from a large number of\nparallel corpora more directly. Based on previous related work that have\nalleviated gaps to some extent, we propose a novel framework that provides a\nmore accurate direct pretraining for QE tasks. In this framework, a generator\nis trained to produce pseudo data that is closer to the real QE data, and a\nestimator is pretrained on these data with novel objectives that are the same\nas the QE task. Experiments on widely used benchmarks show that our proposed\nframework outperforms existing methods, without using any pretraining models\nsuch as BERT.",
          "link": "http://arxiv.org/abs/2107.14600",
          "publishedOn": "2021-08-02T01:58:23.031Z",
          "wordCount": 632,
          "title": "MDQE: A More Accurate Direct Pretraining for Machine Translation Quality Estimation. (arXiv:2107.14600v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14586",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dupuy_C/0/1/0/all/0/1\">Christophe Dupuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arava_R/0/1/0/all/0/1\">Radhika Arava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>",
          "description": "Recent advances in deep learning have drastically improved performance on\nmany Natural Language Understanding (NLU) tasks. However, the data used to\ntrain NLU models may contain private information such as addresses or phone\nnumbers, particularly when drawn from human subjects. It is desirable that\nunderlying models do not expose private information contained in the training\ndata. Differentially Private Stochastic Gradient Descent (DP-SGD) has been\nproposed as a mechanism to build privacy-preserving models. However, DP-SGD can\nbe prohibitively slow to train. In this work, we propose a more efficient\nDP-SGD for training using a GPU infrastructure and apply it to fine-tuning\nmodels based on LSTM and transformer architectures. We report faster training\ntimes, alongside accuracy, theoretical privacy guarantees and success of\nMembership inference attacks for our models and observe that fine-tuning with\nproposed variant of DP-SGD can yield competitive models without significant\ndegradation in training time and improvement in privacy protection. We also\nmake observations such as looser theoretical $\\epsilon, \\delta$ can translate\ninto significant practical privacy gains.",
          "link": "http://arxiv.org/abs/2107.14586",
          "publishedOn": "2021-08-02T01:58:23.017Z",
          "wordCount": 611,
          "title": "An Efficient DP-SGD Mechanism for Large Scale NLP Models. (arXiv:2107.14586v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14419",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Musaev_M/0/1/0/all/0/1\">Muhammadjon Musaev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mussakhojayeva_S/0/1/0/all/0/1\">Saida Mussakhojayeva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khujayorov_I/0/1/0/all/0/1\">Ilyos Khujayorov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khassanov_Y/0/1/0/all/0/1\">Yerbolat Khassanov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ochilov_M/0/1/0/all/0/1\">Mannon Ochilov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varol_H/0/1/0/all/0/1\">Huseyin Atakan Varol</a>",
          "description": "We present a freely available speech corpus for the Uzbek language and report\npreliminary automatic speech recognition (ASR) results using both the deep\nneural network hidden Markov model (DNN-HMM) and end-to-end (E2E)\narchitectures. The Uzbek speech corpus (USC) comprises 958 different speakers\nwith a total of 105 hours of transcribed audio recordings. To the best of our\nknowledge, this is the first open-source Uzbek speech corpus dedicated to the\nASR task. To ensure high quality, the USC has been manually checked by native\nspeakers. We first describe the design and development procedures of the USC,\nand then explain the conducted ASR experiments in detail. The experimental\nresults demonstrate promising results for the applicability of the USC for ASR.\nSpecifically, 18.1% and 17.4% word error rates were achieved on the validation\nand test sets, respectively. To enable experiment reproducibility, we share the\nUSC dataset, pre-trained models, and training recipes in our GitHub repository.",
          "link": "http://arxiv.org/abs/2107.14419",
          "publishedOn": "2021-08-02T01:58:22.987Z",
          "wordCount": 616,
          "title": "USC: An Open-Source Uzbek Speech Corpus and Initial Speech Recognition Experiments. (arXiv:2107.14419v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Palen_Michel_C/0/1/0/all/0/1\">Chester Palen-Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holley_N/0/1/0/all/0/1\">Nolan Holley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lignos_C/0/1/0/all/0/1\">Constantine Lignos</a>",
          "description": "To address what we believe is a looming crisis of unreproducible evaluation\nfor named entity recognition tasks, we present guidelines for reproducible\nevaluation. The guidelines we propose are extremely simple, focusing on\ntransparency regarding how chunks are encoded and scored, but very few papers\ncurrently being published fully comply with them. We demonstrate that despite\nthe apparent simplicity of NER evaluation, unreported differences in the\nscoring procedure can result in changes to scores that are both of noticeable\nmagnitude and are statistically significant. We provide SeqScore, an open\nsource toolkit that addresses many of the issues that cause replication\nfailures and makes following our guidelines easy.",
          "link": "http://arxiv.org/abs/2107.14154",
          "publishedOn": "2021-07-30T02:13:27.900Z",
          "wordCount": 539,
          "title": "Addressing Barriers to Reproducible Named Entity Recognition Evaluation. (arXiv:2107.14154v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12470",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yadong Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jingzhen Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Duan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xiaoxi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zeng Zhao</a>",
          "description": "Recently, large-scale transformer-based models have been proven to be\neffective over a variety of tasks across many domains. Nevertheless, putting\nthem into production is very expensive, requiring comprehensive optimization\ntechniques to reduce inference costs. This paper introduces a series of\ntransformer inference optimization techniques that are both in algorithm level\nand hardware level. These techniques include a pre-padding decoding mechanism\nthat improves token parallelism for text generation, and highly optimized\nkernels designed for very long input length and large hidden size. On this\nbasis, we propose a transformer inference acceleration library -- Easy and\nEfficient Transformer (EET), which has a significant performance improvement\nover existing libraries. Compared to Faster Transformer v4.0's implementation\nfor GPT-2 layer on A100, EET achieves a 1.5-4.5x state-of-art speedup varying\nwith different context lengths. EET is available at\nhttps://github.com/NetEase-FuXi/EET. A demo video is available at\nhttps://youtu.be/22UPcNGcErg.",
          "link": "http://arxiv.org/abs/2104.12470",
          "publishedOn": "2021-07-30T02:13:27.864Z",
          "wordCount": 627,
          "title": "Easy and Efficient Transformer : Scalable Inference Solution For large NLP model. (arXiv:2104.12470v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfson_T/0/1/0/all/0/1\">Tomer Wolfson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>",
          "description": "Recent efforts to create challenge benchmarks that test the abilities of\nnatural language understanding models have largely depended on human\nannotations. In this work, we introduce the \"Break, Perturb, Build\" (BPB)\nframework for automatic reasoning-oriented perturbation of question-answer\npairs. BPB represents a question by decomposing it into the reasoning steps\nthat are required to answer it, symbolically perturbs the decomposition, and\nthen generates new question-answer pairs. We demonstrate the effectiveness of\nBPB by creating evaluation sets for three reading comprehension (RC)\nbenchmarks, generating thousands of high-quality examples without human\nintervention. We evaluate a range of RC models on our evaluation sets, which\nreveals large performance gaps on generated examples compared to the original\ndata. Moreover, symbolic perturbations enable fine-grained analysis of the\nstrengths and limitations of models. Last, augmenting the training data with\nexamples generated by BPB helps close performance gaps, without any drop on the\noriginal data distribution.",
          "link": "http://arxiv.org/abs/2107.13935",
          "publishedOn": "2021-07-30T02:13:27.852Z",
          "wordCount": 585,
          "title": "Break, Perturb, Build: Automatic Perturbation of Reasoning Paths through Question Decomposition. (arXiv:2107.13935v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oka_Y/0/1/0/all/0/1\">Yui Oka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudoh_K/0/1/0/all/0/1\">Katsuhito Sudoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>",
          "description": "Non-autoregressive neural machine translation (NAT) usually employs\nsequence-level knowledge distillation using autoregressive neural machine\ntranslation (AT) as its teacher model. However, a NAT model often outputs\nshorter sentences than an AT model. In this work, we propose sequence-level\nknowledge distillation (SKD) using perturbed length-aware positional encoding\nand apply it to a student model, the Levenshtein Transformer. Our method\noutperformed a standard Levenshtein Transformer by 2.5 points in bilingual\nevaluation understudy (BLEU) at maximum in a WMT14 German to English\ntranslation. The NAT model output longer sentences than the baseline NAT\nmodels.",
          "link": "http://arxiv.org/abs/2107.13689",
          "publishedOn": "2021-07-30T02:13:27.825Z",
          "wordCount": 536,
          "title": "Using Perturbed Length-aware Positional Encoding for Non-autoregressive Neural Machine Translation. (arXiv:2107.13689v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.11820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1\">Devamanyu Hazarika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1\">Rishabh Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_S/0/1/0/all/0/1\">Samson Yu Bai Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1\">Pengfei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Romila Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Abhinaba Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhaya_N/0/1/0/all/0/1\">Niyati Chhaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>",
          "description": "We address the problem of recognizing emotion cause in conversations, define\ntwo novel sub-tasks of this problem, and provide a corresponding dialogue-level\ndataset, along with strong Transformer-based baselines. The dataset is\navailable at https://github.com/declare-lab/RECCON.\n\nIntroduction: Recognizing the cause behind emotions in text is a fundamental\nyet under-explored area of research in NLP. Advances in this area hold the\npotential to improve interpretability and performance in affect-based models.\nIdentifying emotion causes at the utterance level in conversations is\nparticularly challenging due to the intermingling dynamics among the\ninterlocutors.\n\nMethod: We introduce the task of Recognizing Emotion Cause in CONversations\nwith an accompanying dataset named RECCON, containing over 1,000 dialogues and\n10,000 utterance cause-effect pairs. Furthermore, we define different cause\ntypes based on the source of the causes, and establish strong Transformer-based\nbaselines to address two different sub-tasks on this dataset: causal span\nextraction and causal emotion entailment.\n\nResult: Our Transformer-based baselines, which leverage contextual\npre-trained embeddings, such as RoBERTa, outperform the state-of-the-art\nemotion cause extraction approaches\n\nConclusion: We introduce a new task highly relevant for (explainable)\nemotion-aware artificial intelligence: recognizing emotion cause in\nconversations, provide a new highly challenging publicly available\ndialogue-level dataset for this task, and give strong baseline results on this\ndataset.",
          "link": "http://arxiv.org/abs/2012.11820",
          "publishedOn": "2021-07-30T02:13:27.819Z",
          "wordCount": 705,
          "title": "Recognizing Emotion Cause in Conversations. (arXiv:2012.11820v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grunewald_S/0/1/0/all/0/1\">Stefan Gr&#xfc;newald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_A/0/1/0/all/0/1\">Annemarie Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhn_J/0/1/0/all/0/1\">Jonas Kuhn</a>",
          "description": "The introduction of pre-trained transformer-based contextualized word\nembeddings has led to considerable improvements in the accuracy of graph-based\nparsers for frameworks such as Universal Dependencies (UD). However, previous\nworks differ in various dimensions, including their choice of pre-trained\nlanguage models and whether they use LSTM layers. With the aims of\ndisentangling the effects of these choices and identifying a simple yet widely\napplicable architecture, we introduce STEPS, a new modular graph-based\ndependency parser. Using STEPS, we perform a series of analyses on the UD\ncorpora of a diverse set of languages. We find that the choice of pre-trained\nembeddings has by far the greatest impact on parser performance and identify\nXLM-R as a robust choice across the languages in our study. Adding LSTM layers\nprovides no benefits when using transformer-based embeddings. A multi-task\ntraining setup outputting additional UD features may contort results. Taking\nthese insights together, we propose a simple but widely applicable parser\narchitecture and configuration, achieving new state-of-the-art results (in\nterms of LAS) for 10 out of 12 diverse languages.",
          "link": "http://arxiv.org/abs/2010.12699",
          "publishedOn": "2021-07-30T02:13:27.771Z",
          "wordCount": 665,
          "title": "Applying Occam's Razor to Transformer-Based Dependency Parsing: What Works, What Doesn't, and What is Really Necessary. (arXiv:2010.12699v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01995",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Paniagua_V/0/1/0/all/0/1\">V&#xed;ctor Su&#xe1;rez-Paniagua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huayu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitfield_E/0/1/0/all/0/1\">Emma Whitfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghan Wu</a>",
          "description": "The identification of rare diseases from clinical notes with Natural Language\nProcessing (NLP) is challenging due to the few cases available for machine\nlearning and the need of data annotation from clinical experts. We propose a\nmethod using ontologies and weak supervision. The approach includes two steps:\n(i) Text-to-UMLS, linking text mentions to concepts in Unified Medical Language\nSystem (UMLS), with a named entity linking tool (e.g. SemEHR) and weak\nsupervision based on customised rules and Bidirectional Encoder Representations\nfrom Transformers (BERT) based contextual representations, and (ii)\nUMLS-to-ORDO, matching UMLS concepts to rare diseases in Orphanet Rare Disease\nOntology (ORDO). Using MIMIC-III US intensive care discharge summaries as a\ncase study, we show that the Text-to-UMLS process can be greatly improved with\nweak supervision, without any annotated data from domain experts. Our analysis\nshows that the overall pipeline processing discharge summaries can surface rare\ndisease cases, which are mostly uncaptured in manual ICD codes of the hospital\nadmissions.",
          "link": "http://arxiv.org/abs/2105.01995",
          "publishedOn": "2021-07-30T02:13:27.757Z",
          "wordCount": 656,
          "title": "Rare Disease Identification from Clinical Notes with Ontologies and Weak Supervision. (arXiv:2105.01995v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomerantz_R/0/1/0/all/0/1\">Roi Pomerantz</a>",
          "description": "We present Translatotron 2, a neural direct speech-to-speech translation\nmodel that can be trained end-to-end. Translatotron 2 consists of a speech\nencoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention\nmodule that connects all the previous three components. Experimental results\nsuggest that Translatotron 2 outperforms the original Translatotron by a large\nmargin in terms of translation quality and predicted speech naturalness, and\ndrastically improves the robustness of the predicted speech by mitigating\nover-generation, such as babbling or long pause. We also propose a new method\nfor retaining the source speaker's voice in the translated speech. The trained\nmodel is restricted to retain the source speaker's voice, and unlike the\noriginal Translatotron, it is not able to generate speech in a different\nspeaker's voice, making the model more robust for production deployment, by\nmitigating potential misuse for creating spoofing audio artifacts. When the new\nmethod is used together with a simple concatenation-based data augmentation,\nthe trained Translatotron 2 model is able to retain each speaker's voice for\ninput with speaker turns.",
          "link": "http://arxiv.org/abs/2107.08661",
          "publishedOn": "2021-07-30T02:13:27.734Z",
          "wordCount": 631,
          "title": "Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Clouatre_L/0/1/0/all/0/1\">Louis Clouatre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1\">Prasanna Parthasarathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zouaq_A/0/1/0/all/0/1\">Amal Zouaq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>",
          "description": "Recent research analyzing the sensitivity of natural language understanding\nmodels to word-order perturbations have shown that the state-of-the-art models\nin several language tasks may have a unique way to understand the text that\ncould seldom be explained with conventional syntax and semantics. In this\npaper, we investigate the insensitivity of natural language models to\nword-order by quantifying perturbations and analysing their effect on neural\nmodels' performance on language understanding tasks in GLUE benchmark. Towards\nthat end, we propose two metrics - the Direct Neighbour Displacement (DND) and\nthe Index Displacement Count (IDC) - that score the local and global ordering\nof tokens in the perturbed texts and observe that perturbation functions found\nin prior literature affect only the global ordering while the local ordering\nremains relatively unperturbed. We propose perturbations at the granularity of\nsub-words and characters to study the correlation between DND, IDC and the\nperformance of neural language models on natural language tasks. We find that\nneural language models - pretrained and non-pretrained Transformers, LSTMs, and\nConvolutional architectures - require local ordering more so than the global\nordering of tokens. The proposed metrics and the suite of perturbations allow a\nsystematic way to study the (in)sensitivity of neural language understanding\nmodels to varying degree of perturbations.",
          "link": "http://arxiv.org/abs/2107.13955",
          "publishedOn": "2021-07-30T02:13:27.560Z",
          "wordCount": 648,
          "title": "Demystifying Neural Language Models' Insensitivity to Word-Order. (arXiv:2107.13955v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1\">Ankush Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sohom Ghosh</a>",
          "description": "Hypernym and synonym matching are one of the mainstream Natural Language\nProcessing (NLP) tasks. In this paper, we present systems that attempt to solve\nthis problem. We designed these systems to participate in the FinSim-3, a\nshared task of FinNLP workshop at IJCAI-2021. The shared task is focused on\nsolving this problem for the financial domain. We experimented with various\ntransformer based pre-trained embeddings by fine-tuning these for either\nclassification or phrase similarity tasks. We also augmented the provided\ndataset with abbreviations derived from prospectus provided by the organizers\nand definitions of the financial terms from DBpedia [Auer et al., 2007],\nInvestopedia, and the Financial Industry Business Ontology (FIBO). Our best\nperforming system uses both FinBERT [Araci, 2019] and data augmentation from\nthe afore-mentioned sources. We observed that term expansion using data\naugmentation in conjunction with semantic similarity is beneficial for this\ntask and could be useful for the other tasks that deal with short phrases. Our\nbest performing model (Accuracy: 0.917, Rank: 1.156) was developed by\nfine-tuning SentenceBERT [Reimers et al., 2019] (with FinBERT at the backend)\nover an extended labelled set created using the hierarchy of labels present in\nFIBO.",
          "link": "http://arxiv.org/abs/2107.13764",
          "publishedOn": "2021-07-30T02:13:27.536Z",
          "wordCount": 654,
          "title": "Term Expansion and FinBERT fine-tuning for Hypernym and Synonym Ranking of Financial Terms. (arXiv:2107.13764v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.05680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nortje_L/0/1/0/all/0/1\">Leanne Nortje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>",
          "description": "We propose direct multimodal few-shot models that learn a shared embedding\nspace of spoken words and images from only a few paired examples. Imagine an\nagent is shown an image along with a spoken word describing the object in the\npicture, e.g. pen, book and eraser. After observing a few paired examples of\neach class, the model is asked to identify the \"book\" in a set of unseen\npictures. Previous work used a two-step indirect approach relying on learned\nunimodal representations: speech-speech and image-image comparisons are\nperformed across the support set of given speech-image pairs. We propose two\ndirect models which instead learn a single multimodal space where inputs from\ndifferent modalities are directly comparable: a multimodal triplet network\n(MTriplet) and a multimodal correspondence autoencoder (MCAE). To train these\ndirect models, we mine speech-image pairs: the support set is used to pair up\nunlabelled in-domain speech and images. In a speech-to-image digit matching\ntask, direct models outperform indirect models, with the MTriplet achieving the\nbest multimodal five-shot accuracy. We show that the improvements are due to\nthe combination of unsupervised and transfer learning in the direct models, and\nthe absence of two-step compounding errors.",
          "link": "http://arxiv.org/abs/2012.05680",
          "publishedOn": "2021-07-30T02:13:27.511Z",
          "wordCount": 664,
          "title": "Direct multimodal few-shot learning of speech and images. (arXiv:2012.05680v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.11485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zehong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Son Tran</a>",
          "description": "Word embeddings can reflect the semantic representations, and the embedding\nqualities can be comprehensively evaluated with human natural reading-related\ncognitive data sources. In this paper, we proposed the CogniFNN framework,\nwhich is the first attempt at using fuzzy neural networks to extract non-linear\nand non-stationary characteristics for evaluations of English word embeddings\nagainst the corresponding cognitive datasets. In our experiment, we used 15\nhuman cognitive datasets across three modalities: EEG, fMRI, and eye-tracking,\nand selected the mean square error and multiple hypotheses testing as metrics\nto evaluate our proposed CogniFNN framework. Compared to the recent pioneer\nframework, our proposed CogniFNN showed smaller prediction errors of both\ncontext-independent (GloVe) and context-sensitive (BERT) word embeddings, and\nachieved higher significant ratios with randomly generated word embeddings. Our\nfindings suggested that the CogniFNN framework could provide a more accurate\nand comprehensive evaluation of cognitive word embeddings. It will potentially\nbe beneficial to the further word embeddings evaluation on extrinsic natural\nlanguage processing tasks.",
          "link": "http://arxiv.org/abs/2009.11485",
          "publishedOn": "2021-07-30T02:13:27.470Z",
          "wordCount": 641,
          "title": "CogniFNN: A Fuzzy Neural Network Framework for Cognitive Word Embedding Evaluation. (arXiv:2009.11485v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13586",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weizhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengbao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1\">Hiroaki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "This paper surveys and organizes research works in a new paradigm in natural\nlanguage processing, which we dub \"prompt-based learning\". Unlike traditional\nsupervised learning, which trains a model to take in an input x and predict an\noutput y as P(y|x), prompt-based learning is based on language models that\nmodel the probability of text directly. To use these models to perform\nprediction tasks, the original input x is modified using a template into a\ntextual string prompt x' that has some unfilled slots, and then the language\nmodel is used to probabilistically fill the unfilled information to obtain a\nfinal string x, from which the final output y can be derived. This framework is\npowerful and attractive for a number of reasons: it allows the language model\nto be pre-trained on massive amounts of raw text, and by defining a new\nprompting function the model is able to perform few-shot or even zero-shot\nlearning, adapting to new scenarios with few or no labeled data. In this paper\nwe introduce the basics of this promising paradigm, describe a unified set of\nmathematical notations that can cover a wide variety of existing work, and\norganize existing work along several dimensions, e.g.the choice of pre-trained\nmodels, prompts, and tuning strategies. To make the field more accessible to\ninterested beginners, we not only make a systematic review of existing works\nand a highly structured typology of prompt-based concepts, but also release\nother resources, e.g., a website this http URL including\nconstantly-updated survey, and paperlist.",
          "link": "http://arxiv.org/abs/2107.13586",
          "publishedOn": "2021-07-30T02:13:27.208Z",
          "wordCount": 710,
          "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. (arXiv:2107.13586v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yichun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>",
          "description": "Pre-trained language models (PLMs) have achieved great success in natural\nlanguage processing. Most of PLMs follow the default setting of architecture\nhyper-parameters (e.g., the hidden dimension is a quarter of the intermediate\ndimension in feed-forward sub-networks) in BERT (Devlin et al., 2019). Few\nstudies have been conducted to explore the design of architecture\nhyper-parameters in BERT, especially for the more efficient PLMs with tiny\nsizes, which are essential for practical deployment on resource-constrained\ndevices. In this paper, we adopt the one-shot Neural Architecture Search (NAS)\nto automatically search architecture hyper-parameters. Specifically, we\ncarefully design the techniques of one-shot learning and the search space to\nprovide an adaptive and efficient development way of tiny PLMs for various\nlatency constraints. We name our method AutoTinyBERT and evaluate its\neffectiveness on the GLUE and SQuAD benchmarks. The extensive experiments show\nthat our method outperforms both the SOTA search-based baseline (NAS-BERT) and\nthe SOTA distillation-based methods (such as DistilBERT, TinyBERT, MiniLM and\nMobileBERT). In addition, based on the obtained architectures, we propose a\nmore efficient development method that is even faster than the development of a\nsingle PLM.",
          "link": "http://arxiv.org/abs/2107.13686",
          "publishedOn": "2021-07-30T02:13:27.181Z",
          "wordCount": 639,
          "title": "AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models. (arXiv:2107.13686v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anchit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1\">Aleksandra Piktus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sonal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>",
          "description": "Pre-training on larger datasets with ever increasing model size is now a\nproven recipe for increased performance across almost all NLP tasks. A notable\nexception is information retrieval, where additional pre-training has so far\nfailed to produce convincing results. We show that, with the right pre-training\nsetup, this barrier can be overcome. We demonstrate this by pre-training large\nbi-encoder models on 1) a recently released set of 65 million synthetically\ngenerated questions, and 2) 200 million post-comment pairs from a preexisting\ndataset of Reddit conversations made available by pushshift.io. We evaluate on\na set of information retrieval and dialogue retrieval benchmarks, showing\nsubstantial improvements over supervised baselines.",
          "link": "http://arxiv.org/abs/2107.13602",
          "publishedOn": "2021-07-30T02:13:27.152Z",
          "wordCount": 554,
          "title": "Domain-matched Pre-training Tasks for Dense Retrieval. (arXiv:2107.13602v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vasquez_Rodriguez_L/0/1/0/all/0/1\">Laura V&#xe1;squez-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shardlow_M/0/1/0/all/0/1\">Matthew Shardlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Przybyla_P/0/1/0/all/0/1\">Piotr Przyby&#x142;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>",
          "description": "Modern text simplification (TS) heavily relies on the availability of gold\nstandard data to build machine learning models. However, existing studies show\nthat parallel TS corpora contain inaccurate simplifications and incorrect\nalignments. Additionally, evaluation is usually performed by using metrics such\nas BLEU or SARI to compare system output to the gold standard. A major\nlimitation is that these metrics do not match human judgements and the\nperformance on different datasets and linguistic phenomena vary greatly.\nFurthermore, our research shows that the test and training subsets of parallel\ndatasets differ significantly. In this work, we investigate existing TS\ncorpora, providing new insights that will motivate the improvement of existing\nstate-of-the-art TS evaluation methods. Our contributions include the analysis\nof TS corpora based on existing modifications used for simplification and an\nempirical study on TS models performance by using better-distributed datasets.\nWe demonstrate that by improving the distribution of TS datasets, we can build\nmore robust TS models.",
          "link": "http://arxiv.org/abs/2107.13662",
          "publishedOn": "2021-07-30T02:13:27.101Z",
          "wordCount": 601,
          "title": "Investigating Text Simplification Evaluation. (arXiv:2107.13662v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nurce_E/0/1/0/all/0/1\">Erida Nurce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keci_J/0/1/0/all/0/1\">Jorgel Keci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>",
          "description": "The ever growing usage of social media in the recent years has had a direct\nimpact on the increased presence of hate speech and offensive speech in online\nplatforms. Research on effective detection of such content has mainly focused\non English and a few other widespread languages, while the leftover majority\nfail to have the same work put into them and thus cannot benefit from the\nsteady advancements made in the field. In this paper we present \\textsc{Shaj},\nan annotated Albanian dataset for hate speech and offensive speech that has\nbeen constructed from user-generated content on various social media platforms.\nIts annotation follows the hierarchical schema introduced in OffensEval. The\ndataset is tested using three different classification models, the best of\nwhich achieves an F1 score of 0.77 for the identification of offensive\nlanguage, 0.64 F1 score for the automatic categorization of offensive types and\nlastly, 0.52 F1 score for the offensive language target identification.",
          "link": "http://arxiv.org/abs/2107.13592",
          "publishedOn": "2021-07-30T02:13:27.042Z",
          "wordCount": 573,
          "title": "Detecting Abusive Albanian. (arXiv:2107.13592v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-29T02:00:07.674Z",
          "wordCount": 700,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundgaard_K/0/1/0/all/0/1\">Keld T. Lundgaard</a>",
          "description": "By leveraging large amounts of product data collected across hundreds of live\ne-commerce websites, we construct 1000 unique classification tasks that share\nsimilarly-structured input data, comprised of both text and images. These\nclassification tasks focus on learning the product hierarchy of different\ne-commerce websites, causing many of them to be correlated. Adopting a\nmulti-modal transformer model, we solve these tasks in unison using multi-task\nlearning (MTL). Extensive experiments are presented over an initial 100-task\ndataset to reveal best practices for \"large-scale MTL\" (i.e., MTL with more\nthan 100 tasks). From these experiments, a final, unified methodology is\nderived, which is composed of both best practices and new proposals such as\nDyPa, a simple heuristic for automatically allocating task-specific parameters\nto tasks that could benefit from extra capacity. Using our large-scale MTL\nmethodology, we successfully train a single model across all 1000 tasks in our\ndataset while using minimal task specific parameters, thereby showing that it\nis possible to extend several orders of magnitude beyond current efforts in\nMTL.",
          "link": "http://arxiv.org/abs/2107.13054",
          "publishedOn": "2021-07-29T02:00:07.655Z",
          "wordCount": 623,
          "title": "Exceeding the Limits of Visual-Linguistic Multi-Task Learning. (arXiv:2107.13054v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2104.04039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>",
          "description": "Large pre-trained neural language models (LM) have very powerful text\ngeneration capabilities. However, in practice, they are hard to control for\ncreative purposes. We describe a Plug-and-Play controllable language generation\nframework, Plug-and-Blend, that allows a human user to input multiple control\ncodes (topics). In the context of automated story generation, this allows a\nhuman user loose or fine-grained control of the topics and transitions between\nthem that will appear in the generated story, and can even allow for\noverlapping, blended topics. Automated evaluations show our framework, working\nwith different generative LMs, controls the generation towards given\ncontinuous-weighted control codes while keeping the generated sentences fluent,\ndemonstrating strong blending capability. A human participant evaluation shows\nthat the generated stories are observably transitioning between two topics.",
          "link": "http://arxiv.org/abs/2104.04039",
          "publishedOn": "2021-07-29T02:00:07.636Z",
          "wordCount": 601,
          "title": "Plug-and-Blend: A Framework for Controllable Story Generation with Blended Control Codes. (arXiv:2104.04039v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>",
          "description": "This paper argues that training GANs on local and non-local dependencies in\nspeech data offers insights into how deep neural networks discretize continuous\ndata and how symbolic-like rule-based morphophonological processes emerge in a\ndeep convolutional architecture. Acquisition of speech has recently been\nmodeled as a dependency between latent space and data generated by GANs in\nBegu\\v{s} (2020b; arXiv:2006.03965), who models learning of a simple local\nallophonic distribution. We extend this approach to test learning of local and\nnon-local phonological processes that include approximations of morphological\nprocesses. We further parallel outputs of the model to results of a behavioral\nexperiment where human subjects are trained on the data used for training the\nGAN network. Four main conclusions emerge: (i) the networks provide useful\ninformation for computational models of speech acquisition even if trained on a\ncomparatively small dataset of an artificial grammar learning experiment; (ii)\nlocal processes are easier to learn than non-local processes, which matches\nboth behavioral data in human subjects and typology in the world's languages.\nThis paper also proposes (iii) how we can actively observe the network's\nprogress in learning and explore the effect of training steps on learning\nrepresentations by keeping latent space constant across different training\nsteps. Finally, this paper shows that (iv) the network learns to encode the\npresence of a prefix with a single latent variable; by interpolating this\nvariable, we can actively observe the operation of a non-local phonological\nprocess. The proposed technique for retrieving learning representations has\ngeneral implications for our understanding of how GANs discretize continuous\nspeech data and suggests that rule-like generalizations in the training data\nare represented as an interaction between variables in the network's latent\nspace.",
          "link": "http://arxiv.org/abs/2009.12711",
          "publishedOn": "2021-07-29T02:00:07.628Z",
          "wordCount": 761,
          "title": "Local and non-local dependency learning and emergence of rule-like representations in speech data by Deep Convolutional Generative Adversarial Networks. (arXiv:2009.12711v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1\">Charaf Eddine Benarab</a>",
          "description": "Knowledge is acquired by humans through experience, and no boundary is set\nbetween the kinds of knowledge or skill levels we can achieve on different\ntasks at the same time. When it comes to Neural Networks, that is not the case,\nthe major breakthroughs in the field are extremely task and domain specific.\nVision and language are dealt with in separate manners, using separate methods\nand different datasets. In this work, we propose to use knowledge acquired by\nbenchmark Vision Models which are trained on ImageNet to help a much smaller\narchitecture learn to classify text. After transforming the textual data\ncontained in the IMDB dataset to gray scale images. An analysis of different\ndomains and the Transfer Learning method is carried out. Despite the challenge\nposed by the very different datasets, promising results are achieved. The main\ncontribution of this work is a novel approach which links large pretrained\nmodels on both language and vision to achieve state-of-the-art results in\ndifferent sub-fields from the original task. Without needing high compute\ncapacity resources. Specifically, Sentiment Analysis is achieved after\ntransferring knowledge between vision and language models. BERT embeddings are\ntransformed into grayscale images, these images are then used as training\nexamples for pre-trained vision models such as VGG16 and ResNet\n\nIndex Terms: BERT, Convolutional Neural Networks, Domain Adaptation, image\nclassification, Natural Language Processing, t-SNE, text classification,\nTransfer Learning",
          "link": "http://arxiv.org/abs/2106.12479",
          "publishedOn": "2021-07-29T02:00:07.616Z",
          "wordCount": 706,
          "title": "Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heeringa_W/0/1/0/all/0/1\">Wilbert Heeringa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouma_G/0/1/0/all/0/1\">Gosse Bouma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofman_M/0/1/0/all/0/1\">Martha Hofman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drenth_E/0/1/0/all/0/1\">Eduard Drenth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijffels_J/0/1/0/all/0/1\">Jan Wijffels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velde_H/0/1/0/all/0/1\">Hans Van de Velde</a>",
          "description": "We present a lemmatizer/POS-tagger/dependency parser for West Frisian using a\ncorpus of 44,714 words in 3,126 sentences that were annotated according to the\nguidelines of Universal Dependency version 2. POS tags were assigned to words\nby using a Dutch POS tagger that was applied to a literal word-by-word\ntranslation, or to sentences of a Dutch parallel text. Best results were\nobtained when using literal translations that were created by using the Frisian\ntranslation program Oersetter. Morphologic and syntactic annotations were\ngenerated on the basis of a literal Dutch translation as well. The performance\nof the lemmatizer/tagger/annotator when it was trained using default parameters\nwas compared to the performance that was obtained when using the parameter\nvalues that were used for training the LassySmall UD 2.5 corpus. A significant\nimprovement was found for `lemma'. The Frisian lemmatizer/PoS tagger/dependency\nparser is released as a web app and as a web service.",
          "link": "http://arxiv.org/abs/2107.07974",
          "publishedOn": "2021-07-29T02:00:07.608Z",
          "wordCount": 623,
          "title": "POS tagging, lemmatization and dependency parsing of West Frisian. (arXiv:2107.07974v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1\">Keunwoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>",
          "description": "We propose a multimodal singing language classification model that uses both\naudio content and textual metadata. LRID-Net, the proposed model, takes an\naudio signal and a language probability vector estimated from the metadata and\noutputs the probabilities of the target languages. Optionally, LRID-Net is\nfacilitated with modality dropouts to handle a missing modality. In the\nexperiment, we trained several LRID-Nets with varying modality dropout\nconfiguration and tested them with various combinations of input modalities.\nThe experiment results demonstrate that using multimodal input improves\nperformance. The results also suggest that adopting modality dropout does not\ndegrade the performance of the model when there are full modality inputs while\nenabling the model to handle missing modality cases to some extent.",
          "link": "http://arxiv.org/abs/2103.01893",
          "publishedOn": "2021-07-29T02:00:07.589Z",
          "wordCount": 607,
          "title": "Listen, Read, and Identify: Multimodal Singing Language Identification of Music. (arXiv:2103.01893v4 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdelgwad_M/0/1/0/all/0/1\">Mohammed M.Abdelgwad</a>",
          "description": "Aspect-based sentiment analysis(ABSA) is a textual analysis methodology that\ndefines the polarity of opinions on certain aspects related to specific\ntargets. The majority of research on ABSA is in English, with a small amount of\nwork available in Arabic. Most previous Arabic research has relied on deep\nlearning models that depend primarily on context-independent word embeddings\n(e.g.word2vec), where each word has a fixed representation independent of its\ncontext. This article explores the modeling capabilities of contextual\nembeddings from pre-trained language models, such as BERT, and making use of\nsentence pair input on Arabic ABSA tasks. In particular, we are building a\nsimple but effective BERT-based neural baseline to handle this task. Our BERT\narchitecture with a simple linear classification layer surpassed the\nstate-of-the-art works, according to the experimental results on the\nbenchmarked Arabic hotel reviews dataset.",
          "link": "http://arxiv.org/abs/2107.13290",
          "publishedOn": "2021-07-29T02:00:07.582Z",
          "wordCount": 561,
          "title": "Arabic aspect based sentiment analysis using BERT. (arXiv:2107.13290v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chawla_K/0/1/0/all/0/1\">Kushal Chawla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clever_R/0/1/0/all/0/1\">Rene Clever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_J/0/1/0/all/0/1\">Jaysa Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_G/0/1/0/all/0/1\">Gale Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gratch_J/0/1/0/all/0/1\">Jonathan Gratch</a>",
          "description": "Negotiation is a complex social interaction that encapsulates emotional\nencounters in human decision-making. Virtual agents that can negotiate with\nhumans are useful in pedagogy and conversational AI. To advance the development\nof such agents, we explore the prediction of two important subjective goals in\na negotiation - outcome satisfaction and partner perception. Specifically, we\nanalyze the extent to which emotion attributes extracted from the negotiation\nhelp in the prediction, above and beyond the individual difference variables.\nWe focus on a recent dataset in chat-based negotiations, grounded in a\nrealistic camping scenario. We study three degrees of emotion dimensions -\nemoticons, lexical, and contextual by leveraging affective lexicons and a\nstate-of-the-art deep learning architecture. Our insights will be helpful in\ndesigning adaptive negotiation agents that interact through realistic\ncommunication interfaces.",
          "link": "http://arxiv.org/abs/2107.13165",
          "publishedOn": "2021-07-29T02:00:07.574Z",
          "wordCount": 579,
          "title": "Towards Emotion-Aware Agents For Negotiation Dialogues. (arXiv:2107.13165v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogozhnikov_A/0/1/0/all/0/1\">Alex Rogozhnikov</a>",
          "description": "Without positional information, attention-based transformer neural networks\nare permutation-invariant. Absolute or relative positional embeddings are the\nmost popular ways to feed transformer models positional information. Absolute\npositional embeddings are simple to implement, but suffer from generalization\nissues when evaluating on sequences of different length than those seen at\ntraining time. Relative positions are more robust to length change, but are\nmore complex to implement and yield inferior model throughput. In this paper,\nwe propose an augmentation-based approach (CAPE) for absolute positional\nembeddings, which keeps the advantages of both absolute (simplicity and speed)\nand relative position embeddings (better generalization). In addition, our\nempirical evaluation on state-of-the-art models in machine translation, image\nand speech recognition demonstrates that CAPE leads to better generalization\nperformance as well as increased stability with respect to training\nhyper-parameters.",
          "link": "http://arxiv.org/abs/2106.03143",
          "publishedOn": "2021-07-29T02:00:07.459Z",
          "wordCount": 607,
          "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings. (arXiv:2106.03143v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xinshuai Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1\">Anh Tuan Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>",
          "description": "Robustness against word substitutions has a well-defined and widely\nacceptable form, i.e., using semantically similar words as substitutions, and\nthus it is considered as a fundamental stepping-stone towards broader\nrobustness in natural language processing. Previous defense methods capture\nword substitutions in vector space by using either $l_2$-ball or\nhyper-rectangle, which results in perturbation sets that are not inclusive\nenough or unnecessarily large, and thus impedes mimicry of worst cases for\nrobust training. In this paper, we introduce a novel \\textit{Adversarial Sparse\nConvex Combination} (ASCC) method. We model the word substitution attack space\nas a convex hull and leverages a regularization term to enforce perturbation\ntowards an actual substitution, thus aligning our modeling better with the\ndiscrete textual space. Based on the ASCC method, we further propose\nASCC-defense, which leverages ASCC to generate worst-case perturbations and\nincorporates adversarial training towards robustness. Experiments show that\nASCC-defense outperforms the current state-of-the-arts in terms of robustness\non two prevailing NLP tasks, \\emph{i.e.}, sentiment analysis and natural\nlanguage inference, concerning several attacks across multiple model\narchitectures. Besides, we also envision a new class of defense towards\nrobustness in NLP, where our robustly trained word vectors can be plugged into\na normally trained model and enforce its robustness without applying any other\ndefense techniques.",
          "link": "http://arxiv.org/abs/2107.13541",
          "publishedOn": "2021-07-29T02:00:07.429Z",
          "wordCount": 644,
          "title": "Towards Robustness Against Natural Language Word Substitutions. (arXiv:2107.13541v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1\">Huaixiao Tou</a>",
          "description": "Recent years have witnessed the prosperity of legal artificial intelligence\nwith the development of technologies. In this paper, we propose a novel legal\napplication of legal provision prediction (LPP), which aims to predict the\nrelated legal provisions of affairs. We formulate this task as a challenging\nknowledge graph completion problem, which requires not only text understanding\nbut also graph reasoning. To this end, we propose a novel text-guided graph\nreasoning approach. We collect amounts of real-world legal provision data from\nthe Guangdong government service website and construct a legal dataset called\nLegalLPP. Extensive experimental results on the dataset show that our approach\nachieves better performance compared with baselines. The code and dataset are\navailable in \\url{https://github.com/zjunlp/LegalPP} for reproducibility.",
          "link": "http://arxiv.org/abs/2104.02284",
          "publishedOn": "2021-07-29T02:00:07.417Z",
          "wordCount": 591,
          "title": "Text-guided Legal Knowledge Graph Reasoning. (arXiv:2104.02284v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13530",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kessler_S/0/1/0/all/0/1\">Samuel Kessler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thomas_B/0/1/0/all/0/1\">Bethan Thomas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karout_S/0/1/0/all/0/1\">Salah Karout</a>",
          "description": "We present a method for continual learning of speech representations for\nmultiple languages using self-supervised learning (SSL) and applying these for\nautomatic speech recognition. There is an abundance of unannotated speech, so\ncreating self-supervised representations from raw audio and finetuning on a\nsmall annotated datasets is a promising direction to build speech recognition\nsystems. Wav2vec models perform SSL on raw audio in a pretraining phase and\nthen finetune on a small fraction of annotated data. SSL models have produced\nstate of the art results for ASR. However, these models are very expensive to\npretrain with self-supervision. We tackle the problem of learning new language\nrepresentations continually from audio without forgetting a previous language\nrepresentation. We use ideas from continual learning to transfer knowledge from\na previous task to speed up pretraining a new language task. Our\ncontinual-wav2vec2 model can decrease pretraining times by 32% when learning a\nnew language task, and learn this new audio-language representation without\nforgetting previous language representation.",
          "link": "http://arxiv.org/abs/2107.13530",
          "publishedOn": "2021-07-29T02:00:07.406Z",
          "wordCount": 634,
          "title": "Continual-wav2vec2: an Application of Continual Learning for Self-Supervised Automatic Speech Recognition. (arXiv:2107.13530v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_S/0/1/0/all/0/1\">Stephen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dras_M/0/1/0/all/0/1\">Mark Dras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Mark Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>",
          "description": "Sequence-to-Sequence (S2S) neural text generation models, especially the\npre-trained ones (e.g., BART and T5), have exhibited compelling performance on\nvarious natural language generation tasks. However, the black-box nature of\nthese models limits their application in tasks where specific rules (e.g.,\ncontrollable constraints, prior knowledge) need to be executed. Previous works\neither design specific model structure (e.g., Copy Mechanism corresponding to\nthe rule \"the generated output should include certain words in the source\ninput\") or implement specialized inference algorithm (e.g., Constrained Beam\nSearch) to execute particular rules through the text generation. These methods\nrequire careful design case-by-case and are difficult to support multiple rules\nconcurrently. In this paper, we propose a novel module named Neural\nRule-Execution Tracking Machine that can be equipped into various\ntransformer-based generators to leverage multiple rules simultaneously to guide\nthe neural generation model for superior generation performance in a unified\nand scalable way. Extensive experimental results on several benchmarks verify\nthe effectiveness of our proposed model in both controllable and general text\ngeneration.",
          "link": "http://arxiv.org/abs/2107.13077",
          "publishedOn": "2021-07-29T02:00:07.398Z",
          "wordCount": 605,
          "title": "Neural Rule-Execution Tracking Machine For Transformer-Based Text Generation. (arXiv:2107.13077v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tessler_M/0/1/0/all/0/1\">Michael Henry Tessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsividis_P/0/1/0/all/0/1\">Pedro A. Tsividis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madeano_J/0/1/0/all/0/1\">Jason Madeano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harper_B/0/1/0/all/0/1\">Brin Harper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>",
          "description": "Knowledge built culturally across generations allows humans to learn far more\nthan an individual could glean from their own experience in a lifetime.\nCultural knowledge in turn rests on language: language is the richest record of\nwhat previous generations believed, valued, and practiced. The power and\nmechanisms of language as a means of cultural learning, however, are not well\nunderstood. We take a first step towards reverse-engineering cultural learning\nthrough language. We developed a suite of complex high-stakes tasks in the form\nof minimalist-style video games, which we deployed in an iterated learning\nparadigm. Game participants were limited to only two attempts (two lives) to\nbeat each game and were allowed to write a message to a future participant who\nread the message before playing. Knowledge accumulated gradually across\ngenerations, allowing later generations to advance further in the games and\nperform more efficient actions. Multigenerational learning followed a\nstrikingly similar trajectory to individuals learning alone with an unlimited\nnumber of lives. These results suggest that language provides a sufficient\nmedium to express and accumulate the knowledge people acquire in these diverse\ntasks: the dynamics of the environment, valuable goals, dangerous risks, and\nstrategies for success. The video game paradigm we pioneer here is thus a rich\ntest bed for theories of cultural transmission and learning from language.",
          "link": "http://arxiv.org/abs/2107.13377",
          "publishedOn": "2021-07-29T02:00:07.388Z",
          "wordCount": 675,
          "title": "Growing knowledge culturally across generations to solve novel, complex tasks. (arXiv:2107.13377v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.06467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Andrew Yates</a>",
          "description": "The goal of text ranking is to generate an ordered list of texts retrieved\nfrom a corpus in response to a query. Although the most common formulation of\ntext ranking is search, instances of the task can also be found in many natural\nlanguage processing applications. This survey provides an overview of text\nranking with neural network architectures known as transformers, of which BERT\nis the best-known example. The combination of transformers and self-supervised\npretraining has been responsible for a paradigm shift in natural language\nprocessing (NLP), information retrieval (IR), and beyond. In this survey, we\nprovide a synthesis of existing work as a single point of entry for\npractitioners who wish to gain a better understanding of how to apply\ntransformers to text ranking problems and researchers who wish to pursue work\nin this area. We cover a wide range of modern techniques, grouped into two\nhigh-level categories: transformer models that perform reranking in multi-stage\narchitectures and dense retrieval techniques that perform ranking directly.\nThere are two themes that pervade our survey: techniques for handling long\ndocuments, beyond typical sentence-by-sentence processing in NLP, and\ntechniques for addressing the tradeoff between effectiveness (i.e., result\nquality) and efficiency (e.g., query latency, model and index size). Although\ntransformer architectures and pretraining techniques are recent innovations,\nmany aspects of how they are applied to text ranking are relatively well\nunderstood and represent mature techniques. However, there remain many open\nresearch questions, and thus in addition to laying out the foundations of\npretrained transformers for text ranking, this survey also attempts to\nprognosticate where the field is heading.",
          "link": "http://arxiv.org/abs/2010.06467",
          "publishedOn": "2021-07-29T02:00:07.368Z",
          "wordCount": 725,
          "title": "Pretrained Transformers for Text Ranking: BERT and Beyond. (arXiv:2010.06467v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>",
          "description": "In task-oriented conversation systems, natural language generation systems\nthat generate sentences with specific information related to conversation flow\nare useful. Our study focuses on language generation by considering various\ninformation representing the meaning of utterances as multiple conditions of\ngeneration. NLG from meaning representations, the conditions for sentence\nmeaning, generally goes through two steps: sentence planning and surface\nrealization. However, we propose a simple one-stage framework to generate\nutterances directly from MR (Meaning Representation). Our model is based on\nGPT2 and generates utterances with flat conditions on slot and value pairs,\nwhich does not need to determine the structure of the sentence. We evaluate\nseveral systems in the E2E dataset with 6 automatic metrics. Our system is a\nsimple method, but it demonstrates comparable performance to previous systems\nin automated metrics. In addition, using only 10\\% of the data set without any\nother techniques, our model achieves comparable performance, and shows the\npossibility of performing zero-shot generation and expanding to other datasets.",
          "link": "http://arxiv.org/abs/2101.04257",
          "publishedOn": "2021-07-29T02:00:07.354Z",
          "wordCount": 616,
          "title": "Transforming Multi-Conditioned Generation from Meaning Representation. (arXiv:2101.04257v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>",
          "description": "The knowledge of scripts, common chains of events in stereotypical scenarios,\nis a valuable asset for task-oriented natural language understanding systems.\nWe propose the Goal-Oriented Script Construction task, where a model produces a\nsequence of steps to accomplish a given goal. We pilot our task on the first\nmultilingual script learning dataset supporting 18 languages collected from\nwikiHow, a website containing half a million how-to articles. For baselines, we\nconsider both a generation-based approach using a language model and a\nretrieval-based approach by first retrieving the relevant steps from a large\ncandidate pool and then ordering them. We show that our task is practical,\nfeasible but challenging for state-of-the-art Transformer models, and that our\nmethods can be readily deployed for various other datasets and domains with\ndecent zero-shot performance.",
          "link": "http://arxiv.org/abs/2107.13189",
          "publishedOn": "2021-07-29T02:00:07.330Z",
          "wordCount": 562,
          "title": "Goal-Oriented Script Construction. (arXiv:2107.13189v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13425",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_T/0/1/0/all/0/1\">Tieyun Qian</a>",
          "description": "Existing methods in relation extraction have leveraged the lexical features\nin the word sequence and the syntactic features in the parse tree. Though\neffective, the lexical features extracted from the successive word sequence may\nintroduce some noise that has little or no meaningful content. Meanwhile, the\nsyntactic features are usually encoded via graph convolutional networks which\nhave restricted receptive field. To address the above limitations, we propose a\nmulti-scale feature and metric learning framework for relation extraction.\nSpecifically, we first develop a multi-scale convolutional neural network to\naggregate the non-successive mainstays in the lexical sequence. We also design\na multi-scale graph convolutional network which can increase the receptive\nfield towards specific syntactic roles. Moreover, we present a multi-scale\nmetric learning paradigm to exploit both the feature-level relation between\nlexical and syntactic features and the sample-level relation between instances\nwith the same or different classes. We conduct extensive experiments on three\nreal world datasets for various types of relation extraction tasks. The results\ndemonstrate that our model significantly outperforms the state-of-the-art\napproaches.",
          "link": "http://arxiv.org/abs/2107.13425",
          "publishedOn": "2021-07-29T02:00:07.322Z",
          "wordCount": 600,
          "title": "Multi-Scale Feature and Metric Learning for Relation Extraction. (arXiv:2107.13425v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_V/0/1/0/all/0/1\">Vivek Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witteveen_S/0/1/0/all/0/1\">Sam Witteveen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrews_M/0/1/0/all/0/1\">Martin Andrews</a>",
          "description": "Creating explanations for answers to science questions is a challenging task\nthat requires multi-hop inference over a large set of fact sentences. This\nyear, to refocus the Textgraphs Shared Task on the problem of gathering\nrelevant statements (rather than solely finding a single 'correct path'), the\nWorldTree dataset was augmented with expert ratings of 'relevance' of\nstatements to each overall explanation. Our system, which achieved second place\non the Shared Task leaderboard, combines initial statement retrieval; language\nmodels trained to predict the relevance scores; and ensembling of a number of\nthe resulting rankings. Our code implementation is made available at\nhttps://github.com/mdda/worldtree_corpus/tree/textgraphs_2021",
          "link": "http://arxiv.org/abs/2107.13031",
          "publishedOn": "2021-07-29T02:00:07.305Z",
          "wordCount": 570,
          "title": "Red Dragon AI at TextGraphs 2021 Shared Task: Multi-Hop Inference Explanation Regeneration by Matching Expert Ratings. (arXiv:2107.13031v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.02951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>",
          "description": "How can deep neural networks encode information that corresponds to words in\nhuman speech into raw acoustic data? This paper proposes two neural network\narchitectures for modeling unsupervised lexical learning from raw acoustic\ninputs, ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN),\nthat combine a Deep Convolutional GAN architecture for audio data (WaveGAN;\narXiv:1705.07904) with an information theoretic extension of GAN -- InfoGAN\n(arXiv:1606.03657), and propose a new latent space structure that can model\nfeatural learning simultaneously with a higher level classification and allows\nfor a very low-dimension vector representation of lexical items. Lexical\nlearning is modeled as emergent from an architecture that forces a deep neural\nnetwork to output data such that unique information is retrievable from its\nacoustic outputs. The networks trained on lexical items from TIMIT learn to\nencode unique information corresponding to lexical items in the form of\ncategorical variables in their latent space. By manipulating these variables,\nthe network outputs specific lexical items. The network occasionally outputs\ninnovative lexical items that violate training data, but are linguistically\ninterpretable and highly informative for cognitive modeling and neural network\ninterpretability. Innovative outputs suggest that phonetic and phonological\nrepresentations learned by the network can be productively recombined and\ndirectly paralleled to productivity in human speech: a fiwGAN network trained\non `suit' and `dark' outputs innovative `start', even though it never saw\n`start' or even a [st] sequence in the training data. We also argue that\nsetting latent featural codes to values well beyond training range results in\nalmost categorical generation of prototypical lexical items and reveals\nunderlying values of each latent code.",
          "link": "http://arxiv.org/abs/2006.02951",
          "publishedOn": "2021-07-29T02:00:07.279Z",
          "wordCount": 756,
          "title": "CiwGAN and fiwGAN: Encoding information in acoustic data to model lexical learning with Generative Adversarial Networks. (arXiv:2006.02951v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.12875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arrabales_R/0/1/0/all/0/1\">Ra&#xfa;l Arrabales</a>",
          "description": "Most depression assessment tools are based on self-report questionnaires,\nsuch as the Patient Health Questionnaire (PHQ-9). These psychometric\ninstruments can be easily adapted to an online setting by means of electronic\nforms. However, this approach lacks the interacting and engaging features of\nmodern digital environments. With the aim of making depression screening more\navailable, attractive and effective, we developed Perla, a conversational agent\nable to perform an interview based on the PHQ-9. We also conducted a validation\nstudy in which we compared the results obtained by the traditional self-report\nquestionnaire with Perla's automated interview. Analyzing the results from this\nstudy we draw two significant conclusions: firstly, Perla is much preferred by\nInternet users, achieving more than 2.5 times more reach than a traditional\nform-based questionnaire; secondly, her psychometric properties (Cronbach's\nalpha of 0.81, sensitivity of 96% and specificity of 90%) are excellent and\ncomparable to the traditional well-established depression screening\nquestionnaires.",
          "link": "http://arxiv.org/abs/2008.12875",
          "publishedOn": "2021-07-29T02:00:07.256Z",
          "wordCount": 634,
          "title": "Perla: A Conversational Agent for Depression Screening in Digital Ecosystems. Design, Implementation and Validation. (arXiv:2008.12875v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rocholl_J/0/1/0/all/0/1\">Johann C. Rocholl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zayats_V/0/1/0/all/0/1\">Vicky Zayats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_D/0/1/0/all/0/1\">Daniel D. Walker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murad_N/0/1/0/all/0/1\">Noah B. Murad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_A/0/1/0/all/0/1\">Aaron Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebling_D/0/1/0/all/0/1\">Daniel J. Liebling</a>",
          "description": "Disfluency detection models now approach high accuracy on English text.\nHowever, little exploration has been done in improving the size and inference\ntime of the model. At the same time, automatic speech recognition (ASR) models\nare moving from server-side inference to local, on-device inference. Supporting\nmodels in the transcription pipeline (like disfluency detection) must follow\nsuit. In this work we concentrate on the disfluency detection task, focusing on\nsmall, fast, on-device models based on the BERT architecture. We demonstrate it\nis possible to train disfluency detection models as small as 1.3 MiB, while\nretaining high performance. We build on previous work that showed the benefit\nof data augmentation approaches such as self-training. Then, we evaluate the\neffect of domain mismatch between conversational and written text on model\nperformance. We find that domain adaptation and data augmentation strategies\nhave a more pronounced effect on these smaller models, as compared to\nconventional BERT models.",
          "link": "http://arxiv.org/abs/2104.10769",
          "publishedOn": "2021-07-28T02:02:34.291Z",
          "wordCount": 624,
          "title": "Disfluency Detection with Unlabeled Data and Small BERT Models. (arXiv:2104.10769v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1\">Daniel Deutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dror_R/0/1/0/all/0/1\">Rotem Dror</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>",
          "description": "The quality of a summarization evaluation metric is quantified by calculating\nthe correlation between its scores and human annotations across a large number\nof summaries. Currently, it is unclear how precise these correlation estimates\nare, nor whether differences between two metrics' correlations reflect a true\ndifference or if it is due to mere chance. In this work, we address these two\nproblems by proposing methods for calculating confidence intervals and running\nhypothesis tests for correlations using two resampling methods, bootstrapping\nand permutation. After evaluating which of the proposed methods is most\nappropriate for summarization through two simulation experiments, we analyze\nthe results of applying these methods to several different automatic evaluation\nmetrics across three sets of human annotations. We find that the confidence\nintervals are rather wide, demonstrating high uncertainty in the reliability of\nautomatic metrics. Further, although many metrics fail to show statistical\nimprovements over ROUGE, two recent works, QAEval and BERTScore, do in some\nevaluation settings.",
          "link": "http://arxiv.org/abs/2104.00054",
          "publishedOn": "2021-07-28T02:02:33.975Z",
          "wordCount": 630,
          "title": "A Statistical Analysis of Summarization Evaluation Metrics using Resampling Methods. (arXiv:2104.00054v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>",
          "description": "Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.",
          "link": "http://arxiv.org/abs/2106.13948",
          "publishedOn": "2021-07-28T02:02:30.851Z",
          "wordCount": 654,
          "title": "Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>",
          "description": "NLP is deeply intertwined with the formal study of language, both\nconceptually and historically. Arguably, this connection goes all the way back\nto Chomsky's Syntactic Structures in 1957. It also still holds true today, with\na strand of recent works building formal analysis of modern neural networks\nmethods in terms of formal languages. In this document, I aim to explain\nbackground about formal languages as they relate to this recent work. I will by\nnecessity ignore large parts of the rich history of this field, instead\nfocusing on concepts connecting to modern deep learning-based NLP.",
          "link": "http://arxiv.org/abs/2102.10094",
          "publishedOn": "2021-07-28T02:02:30.830Z",
          "wordCount": 561,
          "title": "Formal Language Theory Meets Modern NLP. (arXiv:2102.10094v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekarchi_Z/0/1/0/all/0/1\">Zahra Shekarchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_S/0/1/0/all/0/1\">Suzanne Stevenson</a>",
          "description": "Children learn word meanings by tapping into the commonalities across\ndifferent situations in which words are used and overcome the high level of\nuncertainty involved in early word learning experiences. We propose a modeling\nframework to investigate the role of mutual exclusivity bias - asserting\none-to-one mappings between words and their meanings - in reducing uncertainty\nin word learning. In a set of computational studies, we show that to\nsuccessfully learn word meanings in the face of uncertainty, a learner needs to\nuse two types of competition: words competing for association to a referent\nwhen learning from an observation and referents competing for a word when the\nword is used. Our work highlights the importance of an algorithmic-level\nanalysis to shed light on the utility of different mechanisms that can\nimplement the same computational-level theory.",
          "link": "http://arxiv.org/abs/2012.03370",
          "publishedOn": "2021-07-28T02:02:30.787Z",
          "wordCount": 619,
          "title": "Competition in Cross-situational Word Learning: A Computational Study. (arXiv:2012.03370v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1\">Daniel Deutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedrax_Weiss_T/0/1/0/all/0/1\">Tania Bedrax-Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>",
          "description": "A desirable property of a reference-based evaluation metric that measures the\ncontent quality of a summary is that it should estimate how much information\nthat summary has in common with a reference. Traditional text overlap based\nmetrics such as ROUGE fail to achieve this because they are limited to matching\ntokens, either lexically or via embeddings. In this work, we propose a metric\nto evaluate the content quality of a summary using question-answering (QA).\nQA-based methods directly measure a summary's information overlap with a\nreference, making them fundamentally different than text overlap metrics. We\ndemonstrate the experimental benefits of QA-based metrics through an analysis\nof our proposed metric, QAEval. QAEval out-performs current state-of-the-art\nmetrics on most evaluations using benchmark datasets, while being competitive\non others due to limitations of state-of-the-art models. Through a careful\nanalysis of each component of QAEval, we identify its performance bottlenecks\nand estimate that its potential upper-bound performance surpasses all other\nautomatic metrics, approaching that of the gold-standard Pyramid Method.",
          "link": "http://arxiv.org/abs/2010.00490",
          "publishedOn": "2021-07-28T02:02:30.780Z",
          "wordCount": 653,
          "title": "Towards Question-Answering as an Automatic Metric for Evaluating the Content Quality of a Summary. (arXiv:2010.00490v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.07032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>",
          "description": "In this paper we apply self-knowledge distillation to text summarization\nwhich we argue can alleviate problems with maximum-likelihood training on\nsingle reference and noisy datasets. Instead of relying on one-hot annotation\nlabels, our student summarization model is trained with guidance from a teacher\nwhich generates smoothed labels to help regularize training. Furthermore, to\nbetter model uncertainty during training, we introduce multiple noise signals\nfor both teacher and student models. We demonstrate experimentally on three\nbenchmarks that our framework boosts the performance of both pretrained and\nnon-pretrained summarizers achieving state-of-the-art results.",
          "link": "http://arxiv.org/abs/2009.07032",
          "publishedOn": "2021-07-28T02:02:30.742Z",
          "wordCount": 548,
          "title": "Noisy Self-Knowledge Distillation for Text Summarization. (arXiv:2009.07032v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dang%7D_%7B/0/1/0/all/0/1\">{Bao Minh} {Doan Dang}</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberlander_L/0/1/0/all/0/1\">Laura Oberl&#xe4;nder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>",
          "description": "Emotion stimulus extraction is a fine-grained subtask of emotion analysis\nthat focuses on identifying the description of the cause behind an emotion\nexpression from a text passage (e.g., in the sentence \"I am happy that I passed\nmy exam\" the phrase \"passed my exam\" corresponds to the stimulus.). Previous\nwork mainly focused on Mandarin and English, with no resources or models for\nGerman. We fill this research gap by developing a corpus of 2006 German news\nheadlines annotated with emotions and 811 instances with annotations of\nstimulus phrases. Given that such corpus creation efforts are time-consuming\nand expensive, we additionally work on an approach for projecting the existing\nEnglish GoodNewsEveryone (GNE) corpus to a machine-translated German version.\nWe compare the performance of a conditional random field (CRF) model (trained\nmonolingually on German and cross-lingually via projection) with a multilingual\nXLM-RoBERTa (XLM-R) model. Our results show that training with the German\ncorpus achieves higher F1 scores than projection. Experiments with XLM-R\noutperform their respective CRF counterparts.",
          "link": "http://arxiv.org/abs/2107.12920",
          "publishedOn": "2021-07-28T02:02:30.731Z",
          "wordCount": 599,
          "title": "Emotion Stimulus Detection in German News Headlines. (arXiv:2107.12920v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riviere_M/0/1/0/all/0/1\">Morgane Rivi&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Anne Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talnikar_C/0/1/0/all/0/1\">Chaitanya Talnikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haziza_D/0/1/0/all/0/1\">Daniel Haziza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_M/0/1/0/all/0/1\">Mary Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>",
          "description": "We introduce VoxPopuli, a large-scale multilingual corpus providing 100K\nhours of unlabelled speech data in 23 languages. It is the largest open data to\ndate for unsupervised representation learning as well as semi-supervised\nlearning. VoxPopuli also contains 1.8K hours of transcribed speeches in 16\nlanguages and their aligned oral interpretations into 5 other languages\ntotaling 5.1K hours. We provide speech recognition baselines and validate the\nversatility of VoxPopuli unlabelled data in semi-supervised learning under\nchallenging out-of-domain settings. We will release the corpus at\nhttps://github.com/facebookresearch/voxpopuli under an open license.",
          "link": "http://arxiv.org/abs/2101.00390",
          "publishedOn": "2021-07-28T02:02:30.716Z",
          "wordCount": 587,
          "title": "VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation. (arXiv:2101.00390v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.02704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1\">Vincent Labatut</a> (LIA), <a href=\"http://arxiv.org/find/cs/1/au:+Bost_X/0/1/0/all/0/1\">Xavier Bost</a> (LIA)",
          "description": "A character network is a graph extracted from a narrative, in which vertices\nrepresent characters and edges correspond to interactions between them. A\nnumber of narrative-related problems can be addressed automatically through the\nanalysis of character networks, such as summarization, classification, or role\ndetection. Character networks are particularly relevant when considering works\nof fictions (e.g. novels, plays, movies, TV series), as their exploitation\nallows developing information retrieval and recommendation systems. However,\nworks of fiction possess specific properties making these tasks harder. This\nsurvey aims at presenting and organizing the scientific literature related to\nthe extraction of character networks from works of fiction, as well as their\nanalysis. We first describe the extraction process in a generic way, and\nexplain how its constituting steps are implemented in practice, depending on\nthe medium of the narrative, the goal of the network analysis, and other\nfactors. We then review the descriptive tools used to characterize character\nnetworks, with a focus on the way they are interpreted in this context. We\nillustrate the relevance of character networks by also providing a review of\napplications derived from their analysis. Finally, we identify the limitations\nof the existing approaches, and the most promising perspectives.",
          "link": "http://arxiv.org/abs/1907.02704",
          "publishedOn": "2021-07-28T02:02:30.707Z",
          "wordCount": 714,
          "title": "Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v4 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.07473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silveira_B/0/1/0/all/0/1\">B&#xe1;rbara Silveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_H/0/1/0/all/0/1\">Henrique S. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murai_F/0/1/0/all/0/1\">Fabricio Murai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1\">Ana Paula Couto da Silva</a>",
          "description": "In recent years, Online Social Networks have become an important medium for\npeople who suffer from mental disorders to share moments of hardship, and\nreceive emotional and informational support. In this work, we analyze how\ndiscussions in Reddit communities related to mental disorders can help improve\nthe health conditions of their users. Using the emotional tone of users'\nwriting as a proxy for emotional state, we uncover relationships between user\ninteractions and state changes. First, we observe that authors of negative\nposts often write rosier comments after engaging in discussions, indicating\nthat users' emotional state can improve due to social support. Second, we build\nmodels based on SOTA text embedding techniques and RNNs to predict shifts in\nemotional tone. This differs from most of related work, which focuses primarily\non detecting mental disorders from user activity. We demonstrate the\nfeasibility of accurately predicting the users' reactions to the interactions\nexperienced in these platforms, and present some examples which illustrate that\nthe models are correctly capturing the effects of comments on the author's\nemotional tone. Our models hold promising implications for interventions to\nprovide support for people struggling with mental illnesses.",
          "link": "http://arxiv.org/abs/2005.07473",
          "publishedOn": "2021-07-28T02:02:30.649Z",
          "wordCount": 698,
          "title": "Predicting User Emotional Tone in Mental Disorder Online Communities. (arXiv:2005.07473v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barry_J/0/1/0/all/0/1\">James Barry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1\">Joachim Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassidy_L/0/1/0/all/0/1\">Lauren Cassidy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowap_A/0/1/0/all/0/1\">Alan Cowap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lynn_T/0/1/0/all/0/1\">Teresa Lynn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walsh_A/0/1/0/all/0/1\">Abigail Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meachair_M/0/1/0/all/0/1\">M&#xed;che&#xe1;l J. &#xd3; Meachair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jennifer Foster</a>",
          "description": "The BERT family of neural language models have become highly popular due to\ntheir ability to provide sequences of text with rich context-sensitive token\nencodings which are able to generalise well to many Natural Language Processing\ntasks. Over 120 monolingual BERT models covering over 50 languages have been\nreleased, as well as a multilingual model trained on 104 languages. We\nintroduce, gaBERT, a monolingual BERT model for the Irish language. We compare\nour gaBERT model to multilingual BERT and show that gaBERT provides better\nrepresentations for a downstream parsing task. We also show how different\nfiltering criteria, vocabulary size and the choice of subword tokenisation\nmodel affect downstream performance. We release gaBERT and related code to the\ncommunity.",
          "link": "http://arxiv.org/abs/2107.12930",
          "publishedOn": "2021-07-28T02:02:30.630Z",
          "wordCount": 555,
          "title": "gaBERT -- an Irish Language Model. (arXiv:2107.12930v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Casel_F/0/1/0/all/0/1\">Felix Casel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heindl_A/0/1/0/all/0/1\">Amelie Heindl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>",
          "description": "Emotion classification in text is typically performed with neural network\nmodels which learn to associate linguistic units with emotions. While this\noften leads to good predictive performance, it does only help to a limited\ndegree to understand how emotions are communicated in various domains. The\nemotion component process model (CPM) by Scherer (2005) is an interesting\napproach to explain emotion communication. It states that emotions are a\ncoordinated process of various subcomponents, in reaction to an event, namely\nthe subjective feeling, the cognitive appraisal, the expression, a\nphysiological bodily reaction, and a motivational action tendency. We\nhypothesize that these components are associated with linguistic realizations:\nan emotion can be expressed by describing a physiological bodily reaction (\"he\nwas trembling\"), or the expression (\"she smiled\"), etc. We annotate existing\nliterature and Twitter emotion corpora with emotion component classes and find\nthat emotions on Twitter are predominantly expressed by event descriptions or\nsubjective reports of the feeling, while in literature, authors prefer to\ndescribe what characters do, and leave the interpretation to the reader. We\nfurther include the CPM in a multitask learning model and find that this\nsupports the emotion categorization. The annotated corpora are available at\nhttps://www.ims.uni-stuttgart.de/data/emotion.",
          "link": "http://arxiv.org/abs/2107.12895",
          "publishedOn": "2021-07-28T02:02:30.591Z",
          "wordCount": 645,
          "title": "Emotion Recognition under Consideration of the Emotion Component Process Model. (arXiv:2107.12895v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05299",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Correia_A/0/1/0/all/0/1\">A. D. Correia</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Moortgat_M/0/1/0/all/0/1\">M. Moortgat</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Stoof_H/0/1/0/all/0/1\">H. T. C. Stoof</a>",
          "description": "Grover's algorithm, a well-know quantum search algorithm, allows one to find\nthe correct item in a database, with quadratic speedup. In this paper we adapt\nGrover's algorithm to the problem of finding a correct answer to a natural\nlanguage question in English, thus contributing to the growing field of Quantum\nNatural Language Processing. Using a grammar that can be interpreted as tensor\ncontractions, each word is represented as a quantum state that serves as input\nto the quantum circuit. We here introduce a quantum measurement to contract the\nrepresentations of words, resulting in the representation of larger text\nfragments. Using this framework, a representation for the question is found\nthat contains all the possible answers in equal quantum superposition, and\nallows for the building of an oracle that can detect a correct answer, being\nagnostic to the specific question. Furthermore, we show that our construction\ncan deal with certain types of ambiguous phrases by keeping the various\ndifferent meanings in quantum superposition.",
          "link": "http://arxiv.org/abs/2106.05299",
          "publishedOn": "2021-07-28T02:02:30.531Z",
          "wordCount": 615,
          "title": "Grover's Algorithm for Question Answering. (arXiv:2106.05299v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eder_T/0/1/0/all/0/1\">Tobias Eder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hangya_V/0/1/0/all/0/1\">Viktor Hangya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>",
          "description": "Good quality monolingual word embeddings (MWEs) can be built for languages\nwhich have large amounts of unlabeled text. MWEs can be aligned to bilingual\nspaces using only a few thousand word translation pairs. For low resource\nlanguages training MWEs monolingually results in MWEs of poor quality, and thus\npoor bilingual word embeddings (BWEs) as well. This paper proposes a new\napproach for building BWEs in which the vector space of the high resource\nsource language is used as a starting point for training an embedding space for\nthe low resource target language. By using the source vectors as anchors the\nvector spaces are automatically aligned during training. We experiment on\nEnglish-German, English-Hiligaynon and English-Macedonian. We show that our\napproach results not only in improved BWEs and bilingual lexicon induction\nperformance, but also in improved target language MWE quality as measured using\nmonolingual word similarity.",
          "link": "http://arxiv.org/abs/2010.12627",
          "publishedOn": "2021-07-28T02:02:30.522Z",
          "wordCount": 624,
          "title": "Anchor-based Bilingual Word Embeddings for Low-Resource Languages. (arXiv:2010.12627v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_S/0/1/0/all/0/1\">Stella Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Longxiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">He Zhang</a>",
          "description": "Federated Learning aims to learn machine learning models from multiple\ndecentralized edge devices (e.g. mobiles) or servers without sacrificing local\ndata privacy. Recent Natural Language Processing techniques rely on deep\nlearning and large pre-trained language models. However, both big deep neural\nand language models are trained with huge amounts of data which often lies on\nthe server side. Since text data is widely originated from end users, in this\nwork, we look into recent NLP models and techniques which use federated\nlearning as the learning framework. Our survey discusses major challenges in\nfederated natural language processing, including the algorithm challenges,\nsystem challenges as well as the privacy issues. We also provide a critical\nreview of the existing Federated NLP evaluation methods and tools. Finally, we\nhighlight the current research gaps and future directions.",
          "link": "http://arxiv.org/abs/2107.12603",
          "publishedOn": "2021-07-28T02:02:30.479Z",
          "wordCount": 584,
          "title": "Federated Learning Meets Natural Language Processing: A Survey. (arXiv:2107.12603v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jinyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuang_K/0/1/0/all/0/1\">Kai Shuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jijie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>",
          "description": "The goal of dialogue state tracking (DST) is to predict the current dialogue\nstate given all previous dialogue contexts. Existing approaches generally\npredict the dialogue state at every turn from scratch. However, the\noverwhelming majority of the slots in each turn should simply inherit the slot\nvalues from the previous turn. Therefore, the mechanism of treating slots\nequally in each turn not only is inefficient but also may lead to additional\nerrors because of the redundant slot value generation. To address this problem,\nwe devise the two-stage DSS-DST which consists of the Dual Slot Selector based\non the current turn dialogue, and the Slot Value Generator based on the\ndialogue history. The Dual Slot Selector determines each slot whether to update\nslot value or to inherit the slot value from the previous turn from two\naspects: (1) if there is a strong relationship between it and the current turn\ndialogue utterances; (2) if a slot value with high reliability can be obtained\nfor it through the current turn dialogue. The slots selected to be updated are\npermitted to enter the Slot Value Generator to update values by a hybrid\nmethod, while the other slots directly inherit the values from the previous\nturn. Empirical results show that our method achieves 56.93%, 60.73%, and\n58.04% joint accuracy on MultiWOZ 2.0, MultiWOZ 2.1, and MultiWOZ 2.2 datasets\nrespectively and achieves a new state-of-the-art performance with significant\nimprovements.",
          "link": "http://arxiv.org/abs/2107.12578",
          "publishedOn": "2021-07-28T02:02:30.433Z",
          "wordCount": 685,
          "title": "Dual Slot Selector via Local Reliability Verification for Dialogue State Tracking. (arXiv:2107.12578v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yuchen Chai</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Palacios_J/0/1/0/all/0/1\">Juan Palacios</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianghao Wang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yichun Fan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siqi Zheng</a> (1) ((1) Massachusetts Institute of Technology, (2) Chinese Academy of Science)",
          "description": "COVID-19, as a global health crisis, has triggered the fear emotion with\nunprecedented intensity. Besides the fear of getting infected, the outbreak of\nCOVID-19 also created significant disruptions in people's daily life and thus\nevoked intensive psychological responses indirect to COVID-19 infections. Here,\nwe construct an expressed fear database using 16 million social media posts\ngenerated by 536 thousand users between January 1st, 2019 and August 31st, 2020\nin China. We employ deep learning techniques to detect the fear emotion within\neach post and apply topic models to extract the central fear topics. Based on\nthis database, we find that sleep disorders (\"nightmare\" and \"insomnia\") take\nup the largest share of fear-labeled posts in the pre-pandemic period (January\n2019-December 2019), and significantly increase during the COVID-19. We\nidentify health and work-related concerns are the two major sources of fear\ninduced by the COVID-19. We also detect gender differences, with females\ngenerating more posts containing the daily-life fear sources during the\nCOVID-19 period. This research adopts a data-driven approach to trace back\npublic emotion, which can be used to complement traditional surveys to achieve\nreal-time emotion monitoring to discern societal concerns and support policy\ndecision-making.",
          "link": "http://arxiv.org/abs/2107.12606",
          "publishedOn": "2021-07-28T02:02:30.394Z",
          "wordCount": 701,
          "title": "Measuring daily-life fear perception change: a computational study in the context of COVID-19. (arXiv:2107.12606v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1\">Mohit Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "Seemingly simple natural language requests to a robot are generally\nunderspecified, for example \"Can you bring me the wireless mouse?\" When viewing\nmice on the shelf, the number of buttons or presence of a wire may not be\nvisible from certain angles or positions. Flat images of candidate mice may not\nprovide the discriminative information needed for \"wireless\". The world, and\nobjects in it, are not flat images but complex 3D shapes. If a human requests\nan object based on any of its basic properties, such as color, shape, or\ntexture, robots should perform the necessary exploration to accomplish the\ntask. In particular, while substantial effort and progress has been made on\nunderstanding explicitly visual attributes like color and category,\ncomparatively little progress has been made on understanding language about\nshapes and contours. In this work, we introduce a novel reasoning task that\ntargets both visual and non-visual language about 3D objects. Our new\nbenchmark, ShapeNet Annotated with Referring Expressions (SNARE), requires a\nmodel to choose which of two objects is being referenced by a natural language\ndescription. We introduce several CLIP-based models for distinguishing objects\nand demonstrate that while recent advances in jointly modeling vision and\nlanguage are useful for robotic language understanding, it is still the case\nthat these models are weaker at understanding the 3D nature of objects --\nproperties which play a key role in manipulation. In particular, we find that\nadding view estimation to language grounding models improves accuracy on both\nSNARE and when identifying objects referred to in language on a robot platform.",
          "link": "http://arxiv.org/abs/2107.12514",
          "publishedOn": "2021-07-28T02:02:30.381Z",
          "wordCount": 709,
          "title": "Language Grounding with 3D Objects. (arXiv:2107.12514v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1\">Anna Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>",
          "description": "Alongside huge volumes of research on deep learning models in NLP in the\nrecent years, there has been also much work on benchmark datasets needed to\ntrack modeling progress. Question answering and reading comprehension have been\nparticularly prolific in this regard, with over 80 new datasets appearing in\nthe past two years. This study is the largest survey of the field to date. We\nprovide an overview of the various formats and domains of the current\nresources, highlighting the current lacunae for future work. We further discuss\nthe current classifications of ``reasoning types\" in question answering and\npropose a new taxonomy. We also discuss the implications of over-focusing on\nEnglish, and survey the current monolingual resources for other languages and\nmultilingual resources. The study is aimed at both practitioners looking for\npointers to the wealth of existing data, and at researchers working on new\nresources.",
          "link": "http://arxiv.org/abs/2107.12708",
          "publishedOn": "2021-07-28T02:02:30.344Z",
          "wordCount": 594,
          "title": "QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension. (arXiv:2107.12708v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1\">Yawen Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiasheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xinyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>",
          "description": "Unknown intent detection aims to identify the out-of-distribution (OOD)\nutterance whose intent has never appeared in the training set. In this paper,\nwe propose using energy scores for this task as the energy score is\ntheoretically aligned with the density of the input and can be derived from any\nclassifier. However, high-quality OOD utterances are required during the\ntraining stage in order to shape the energy gap between OOD and in-distribution\n(IND), and these utterances are difficult to collect in practice. To tackle\nthis problem, we propose a data manipulation framework to Generate high-quality\nOOD utterances with importance weighTs (GOT). Experimental results show that\nthe energy-based detector fine-tuned by GOT can achieve state-of-the-art\nresults on two benchmark datasets.",
          "link": "http://arxiv.org/abs/2107.12542",
          "publishedOn": "2021-07-28T02:02:30.332Z",
          "wordCount": 572,
          "title": "Energy-based Unknown Intent Detection with Data Manipulation. (arXiv:2107.12542v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parnow_K/0/1/0/all/0/1\">Kevin Parnow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utiyama_M/0/1/0/all/0/1\">Masao Utiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumita_E/0/1/0/all/0/1\">Eiichiro Sumita</a>",
          "description": "Though the pre-trained contextualized language model (PrLM) has made a\nsignificant impact on NLP, training PrLMs in languages other than English can\nbe impractical for two reasons: other languages often lack corpora sufficient\nfor training powerful PrLMs, and because of the commonalities among human\nlanguages, computationally expensive PrLM training for different languages is\nsomewhat redundant. In this work, building upon the recent works connecting\ncross-lingual model transferring and neural machine translation, we thus\npropose a novel cross-lingual model transferring framework for PrLMs: TreLM. To\nhandle the symbol order and sequence length differences between languages, we\npropose an intermediate ``TRILayer\" structure that learns from these\ndifferences and creates a better transfer in our primary translation direction,\nas well as a new cross-lingual language modeling objective for transfer\ntraining. Additionally, we showcase an embedding aligning that adversarially\nadapts a PrLM's non-contextualized embedding space and the TRILayer structure\nto learn a text transformation network across languages, which addresses the\nvocabulary difference between languages. Experiments on both language\nunderstanding and structure parsing tasks show the proposed framework\nsignificantly outperforms language models trained from scratch with limited\ndata in both performance and efficiency. Moreover, despite an insignificant\nperformance loss compared to pre-training from scratch in resource-rich\nscenarios, our cross-lingual model transferring framework is significantly more\neconomical.",
          "link": "http://arxiv.org/abs/2107.12627",
          "publishedOn": "2021-07-28T02:02:30.304Z",
          "wordCount": 647,
          "title": "Cross-lingual Transferring of Pre-trained Contextualized Language Models. (arXiv:2107.12627v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.",
          "link": "http://arxiv.org/abs/2107.12651",
          "publishedOn": "2021-07-28T02:02:30.284Z",
          "wordCount": 604,
          "title": "Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1\">Sheikh Muhammad Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murdock_V/0/1/0/all/0/1\">Vanessa Murdock</a>",
          "description": "Online harassment in the form of hate speech has been on the rise in recent\nyears. Addressing the issue requires a combination of content moderation by\npeople, aided by automatic detection methods. As content moderation is itself\nharmful to the people doing it, we desire to reduce the burden by improving the\nautomatic detection of hate speech. Hate speech presents a challenge as it is\ndirected at different target groups using a completely different vocabulary.\nFurther the authors of the hate speech are incentivized to disguise their\nbehavior to avoid being removed from a platform. This makes it difficult to\ndevelop a comprehensive data set for training and evaluating hate speech\ndetection models because the examples that represent one hate speech domain do\nnot typically represent others, even within the same language or culture. We\npropose an unsupervised domain adaptation approach to augment labeled data for\nhate speech detection. We evaluate the approach with three different models\n(character CNNs, BiLSTMs and BERT) on three different collections. We show our\napproach improves Area under the Precision/Recall curve by as much as 42% and\nrecall by as much as 278%, with no loss (and in some cases a significant gain)\nin precision.",
          "link": "http://arxiv.org/abs/2107.12866",
          "publishedOn": "2021-07-28T02:02:30.266Z",
          "wordCount": 639,
          "title": "Unsupervised Domain Adaptation for Hate Speech Detection Using a Data Augmentation Approach. (arXiv:2107.12866v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mehra_S/0/1/0/all/0/1\">Sunakshi Mehra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susan_S/0/1/0/all/0/1\">Seba Susan</a>",
          "description": "We introduce an unsupervised approach for correcting highly imperfect speech\ntranscriptions based on a decision-level fusion of stemming and two-way phoneme\npruning. Transcripts are acquired from videos by extracting audio using Ffmpeg\nframework and further converting audio to text transcript using Google API. In\nthe benchmark LRW dataset, there are 500 word categories, and 50 videos per\nclass in mp4 format. All videos consist of 29 frames (each 1.16 s long) and the\nword appears in the middle of the video. In our approach we tried to improve\nthe baseline accuracy from 9.34% by using stemming, phoneme extraction,\nfiltering and pruning. After applying the stemming algorithm to the text\ntranscript and evaluating the results, we achieved 23.34% accuracy in word\nrecognition. To convert words to phonemes we used the Carnegie Mellon\nUniversity (CMU) pronouncing dictionary that provides a phonetic mapping of\nEnglish words to their pronunciations. A two-way phoneme pruning is proposed\nthat comprises of the two non-sequential steps: 1) filtering and pruning the\nphonemes containing vowels and plosives 2) filtering and pruning the phonemes\ncontaining vowels and fricatives. After obtaining results of stemming and\ntwo-way phoneme pruning, we applied decision-level fusion and that led to an\nimprovement of word recognition rate upto 32.96%.",
          "link": "http://arxiv.org/abs/2107.12428",
          "publishedOn": "2021-07-28T02:02:30.231Z",
          "wordCount": 664,
          "title": "Improving Word Recognition in Speech Transcriptions by Decision-level Fusion of Stemming and Two-way Phoneme Pruning. (arXiv:2107.12428v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.01999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhishan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guohui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Dawei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>",
          "description": "As a critical component for online advertising and marking, click-through\nrate (CTR) prediction has draw lots of attentions from both industry and\nacademia field. Recently, the deep learning has become the mainstream\nmethodological choice for CTR. Despite of sustainable efforts have been made,\nexisting approaches still pose several challenges. On the one hand, high-order\ninteraction between the features is under-explored. On the other hand,\nhigh-order interactions may neglect the semantic information from the low-order\nfields. In this paper, we proposed a novel prediction method, named FINT, that\nemploys the Field-aware INTeraction layer which captures high-order feature\ninteractions while retaining the low-order field information. To empirically\ninvestigate the effectiveness and robustness of the FINT, we perform extensive\nexperiments on the three realistic databases: KDD2012, Criteo and Avazu. The\nobtained results demonstrate that the FINT can significantly improve the\nperformance compared to the existing methods, without increasing the amount of\ncomputation required. Moreover, the proposed method brought about 2.72\\%\nincrease to the advertising revenue of a big online video app through A/B\ntesting. To better promote the research in CTR field, we released our code as\nwell as reference implementation at: https://github.com/zhishan01/FINT.",
          "link": "http://arxiv.org/abs/2107.01999",
          "publishedOn": "2021-08-02T01:58:23.232Z",
          "wordCount": 654,
          "title": "FINT: Field-aware INTeraction Neural Network For CTR Prediction. (arXiv:2107.01999v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.08903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1\">Nan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sichen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_K/0/1/0/all/0/1\">Kyle Kai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabowo_A/0/1/0/all/0/1\">Arian Prabowo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Mohammad Saiedur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1\">Flora D. Salim</a>",
          "description": "Generative Adversarial Networks (GANs) have shown remarkable success in\nproducing realistic-looking images in the computer vision area. Recently,\nGAN-based techniques are shown to be promising for spatio-temporal-based\napplications such as trajectory prediction, events generation and time-series\ndata imputation. While several reviews for GANs in computer vision have been\npresented, no one has considered addressing the practical applications and\nchallenges relevant to spatio-temporal data. In this paper, we have conducted a\ncomprehensive review of the recent developments of GANs for spatio-temporal\ndata. We summarise the application of popular GAN architectures for\nspatio-temporal data and the common practices for evaluating the performance of\nspatio-temporal applications with GANs. Finally, we point out future research\ndirections to benefit researchers in this area.",
          "link": "http://arxiv.org/abs/2008.08903",
          "publishedOn": "2021-08-02T01:58:23.075Z",
          "wordCount": 636,
          "title": "Generative Adversarial Networks for Spatio-temporal Data: A Survey. (arXiv:2008.08903v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Damak_K/0/1/0/all/0/1\">Khalil Damak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khenissi_S/0/1/0/all/0/1\">Sami Khenissi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasraoui_O/0/1/0/all/0/1\">Olfa Nasraoui</a>",
          "description": "Recent work in recommender systems has emphasized the importance of fairness,\nwith a particular interest in bias and transparency, in addition to predictive\naccuracy. In this paper, we focus on the state of the art pairwise ranking\nmodel, Bayesian Personalized Ranking (BPR), which has previously been found to\noutperform pointwise models in predictive accuracy, while also being able to\nhandle implicit feedback. Specifically, we address two limitations of BPR: (1)\nBPR is a black box model that does not explain its outputs, thus limiting the\nuser's trust in the recommendations, and the analyst's ability to scrutinize a\nmodel's outputs; and (2) BPR is vulnerable to exposure bias due to the data\nbeing Missing Not At Random (MNAR). This exposure bias usually translates into\nan unfairness against the least popular items because they risk being\nunder-exposed by the recommender system. In this work, we first propose a novel\nexplainable loss function and a corresponding Matrix Factorization-based model\ncalled Explainable Bayesian Personalized Ranking (EBPR) that generates\nrecommendations along with item-based explanations. Then, we theoretically\nquantify additional exposure bias resulting from the explainability, and use it\nas a basis to propose an unbiased estimator for the ideal EBPR loss. The result\nis a ranking model that aptly captures both debiased and explainable user\npreferences. Finally, we perform an empirical study on three real-world\ndatasets that demonstrate the advantages of our proposed models.",
          "link": "http://arxiv.org/abs/2107.14768",
          "publishedOn": "2021-08-02T01:58:23.001Z",
          "wordCount": 695,
          "title": "Debiased Explainable Pairwise Ranking from Implicit Feedback. (arXiv:2107.14768v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carreon_E/0/1/0/all/0/1\">Elisa Claire Alem&#xe1;n Carre&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espana_H/0/1/0/all/0/1\">Hugo Alberto Mendoza Espa&#xf1;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nonaka_H/0/1/0/all/0/1\">Hirofumi Nonaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hiraoka_T/0/1/0/all/0/1\">Toru Hiraoka</a>",
          "description": "Since culture influences expectations, perceptions, and satisfaction, a\ncross-culture study is necessary to understand the differences between Japan's\nbiggest tourist populations, Chinese and Western tourists. However, with\never-increasing customer populations, this is hard to accomplish without\nextensive customer base studies. There is a need for an automated method for\nidentifying these expectations at a large scale. For this, we used a\ndata-driven approach to our analysis. Our study analyzed their satisfaction\nfactors comparing soft attributes, such as service, with hard attributes, such\nas location and facilities, and studied different price ranges. We collected\nhotel reviews and extracted keywords to classify the sentiment of sentences\nwith an SVC. We then used dependency parsing and part-of-speech tagging to\nextract nouns tied to positive adjectives. We found that Chinese tourists\nconsider room quality more than hospitality, whereas Westerners are delighted\nmore by staff behavior. Furthermore, the lack of a Chinese-friendly environment\nfor Chinese customers and cigarette smell for Western ones can be disappointing\nfactors of their stay. As one of the first studies in the tourism field to use\nthe high-standard Japanese hospitality environment for this analysis, our\ncross-cultural study contributes to both the theoretical understanding of\nsatisfaction and suggests practical applications and strategies for hotel\nmanagers.",
          "link": "http://arxiv.org/abs/2107.14681",
          "publishedOn": "2021-08-02T01:58:22.752Z",
          "wordCount": 662,
          "title": "Differences in Chinese and Western tourists faced with Japanese hospitality: A natural language processing approach. (arXiv:2107.14681v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korzeniowski_F/0/1/0/all/0/1\">Filip Korzeniowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oramas_S/0/1/0/all/0/1\">Sergio Oramas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouyon_F/0/1/0/all/0/1\">Fabien Gouyon</a>",
          "description": "Artist similarity plays an important role in organizing, understanding, and\nsubsequently, facilitating discovery in large collections of music. In this\npaper, we present a hybrid approach to computing similarity between artists\nusing graph neural networks trained with triplet loss. The novelty of using a\ngraph neural network architecture is to combine the topology of a graph of\nartist connections with content features to embed artists into a vector space\nthat encodes similarity. To evaluate the proposed method, we compile the new\nOLGA dataset, which contains artist similarities from AllMusic, together with\ncontent features from AcousticBrainz. With 17,673 artists, this is the largest\nacademic artist similarity dataset that includes content-based features to\ndate. Moreover, we also showcase the scalability of our approach by\nexperimenting with a much larger proprietary dataset. Results show the\nsuperiority of the proposed approach over current state-of-the-art methods for\nmusic similarity. Finally, we hope that the OLGA dataset will facilitate\nresearch on data-driven models for artist similarity.",
          "link": "http://arxiv.org/abs/2107.14541",
          "publishedOn": "2021-08-02T01:58:22.703Z",
          "wordCount": 614,
          "title": "Artist Similarity with Graph Neural Networks. (arXiv:2107.14541v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haokui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenze Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Buzhou Tang</a>",
          "description": "Approximate Nearest neighbor search (ANNS) plays a crucial role in\ninformation retrieval, which has a wide range of application scenarios.\nTherefore, during past several years, a lot of fast ANNS approaches have been\nproposed. Among these approaches, graph-based methods are one of the most\npopular type, as they have shown attractive theoretical guarantees and low\nquery latency. In this paper, we propose a learnable compression network with\ntransformer (LCNT), which projects feature vectors from high dimensional space\nonto low dimensional space, while preserving neighbor relationship. The\nproposed model can be generalized to existing graph-based methods to accelerate\nthe process of building indexing graph and further reduce query latency.\nSpecifically, the proposed LCNT contains two major parts, projection part and\nharmonizing part. In the projection part, input vectors are projected into a\nsequence of subspaces via multi channel sparse projection network. In the\nharmonizing part, a modified Transformer network is employed to harmonize\nfeatures in subspaces and combine them to get a new feature. To evaluate the\neffectiveness of the proposed model, we conduct experiments on two\nmillion-scale databases, GIST1M and Deep1M. Experimental results show that the\nproposed model can improve the speed of building indexing graph to 2-3 times\nits original speed without sacrificing accuracy significantly. The query\nlatency is reduced by a factor of 1.3 to 2.0. In addition, the proposed model\ncan also be combined with other popular quantization methods.",
          "link": "http://arxiv.org/abs/2107.14415",
          "publishedOn": "2021-08-02T01:58:22.680Z",
          "wordCount": 671,
          "title": "Learnable Compression Network with Transformer for Approximate Nearest Neighbor Search. (arXiv:2107.14415v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sciascio_E/0/1/0/all/0/1\">Eugenio Di Sciascio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_A/0/1/0/all/0/1\">Antonio Ferrara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancino_A/0/1/0/all/0/1\">Alberto Carlo Maria Mancino</a>",
          "description": "Deep Learning and factorization-based collaborative filtering recommendation\nmodels have undoubtedly dominated the scene of recommender systems in recent\nyears. However, despite their outstanding performance, these methods require a\ntraining time proportional to the size of the embeddings and it further\nincreases when also side information is considered for the computation of the\nrecommendation list. In fact, in these cases we have that with a large number\nof high-quality features, the resulting models are more complex and difficult\nto train. This paper addresses this problem by presenting KGFlex: a sparse\nfactorization approach that grants an even greater degree of expressiveness. To\nachieve this result, KGFlex analyzes the historical data to understand the\ndimensions the user decisions depend on (e.g., movie direction, musical genre,\nnationality of book writer). KGFlex represents each item feature as an\nembedding and it models user-item interactions as a factorized entropy-driven\ncombination of the item attributes relevant to the user. KGFlex facilitates the\ntraining process by letting users update only those relevant features on which\nthey base their decisions. In other words, the user-item prediction is mediated\nby the user's personal view that considers only relevant features. An extensive\nexperimental evaluation shows the approach's effectiveness, considering the\nrecommendation results' accuracy, diversity, and induced bias. The public\nimplementation of KGFlex is available at https://split.to/kgflex.",
          "link": "http://arxiv.org/abs/2107.14290",
          "publishedOn": "2021-08-02T01:58:22.651Z",
          "wordCount": 657,
          "title": "Sparse Feature Factorization for Recommender Systems with Knowledge Graphs. (arXiv:2107.14290v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lordelo_C/0/1/0/all/0/1\">Carlos Lordelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1\">Emmanouil Benetos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixon_S/0/1/0/all/0/1\">Simon Dixon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahlback_S/0/1/0/all/0/1\">Sven Ahlb&#xe4;ck</a>",
          "description": "This paper proposes a deep convolutional neural network for performing\nnote-level instrument assignment. Given a polyphonic multi-instrumental music\nsignal along with its ground truth or predicted notes, the objective is to\nassign an instrumental source for each note. This problem is addressed as a\npitch-informed classification task where each note is analysed individually. We\nalso propose to utilise several kernel shapes in the convolutional layers in\norder to facilitate learning of efficient timbre-discriminative feature maps.\nExperiments on the MusicNet dataset using 7 instrument classes show that our\napproach is able to achieve an average F-score of 0.904 when the original\nmulti-pitch annotations are used as the pitch information for the system, and\nthat it also excels if the note information is provided using third-party\nmulti-pitch estimation algorithms. We also include ablation studies\ninvestigating the effects of the use of multiple kernel shapes and comparing\ndifferent input representations for the audio and the note-related information.",
          "link": "http://arxiv.org/abs/2107.13617",
          "publishedOn": "2021-07-30T02:13:27.230Z",
          "wordCount": 625,
          "title": "Pitch-Informed Instrument Assignment Using a Deep Convolutional Network with Multiple Kernel Shapes. (arXiv:2107.13617v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elwood_A/0/1/0/all/0/1\">Adam Elwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasparin_A/0/1/0/all/0/1\">Alberto Gasparin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozza_A/0/1/0/all/0/1\">Alessandro Rozza</a>",
          "description": "With the rise in use of social media to promote branded products, the demand\nfor effective influencer marketing has increased. Brands are looking for\nimproved ways to identify valuable influencers among a vast catalogue; this is\neven more challenging with \"micro-influencers\", which are more affordable than\nmainstream ones but difficult to discover. In this paper, we propose a novel\nmulti-task learning framework to improve the state of the art in\nmicro-influencer ranking based on multimedia content. Moreover, since the\nvisual congruence between a brand and influencer has been shown to be good\nmeasure of compatibility, we provide an effective visual method for\ninterpreting our models' decisions, which can also be used to inform brands'\nmedia strategies. We compare with the current state-of-the-art on a recently\nconstructed public dataset and we show significant improvement both in terms of\naccuracy and model complexity. The techniques for ranking and interpretation\npresented in this work can be generalised to arbitrary multimedia ranking tasks\nthat have datasets with a similar structure.",
          "link": "http://arxiv.org/abs/2107.13943",
          "publishedOn": "2021-07-30T02:13:27.216Z",
          "wordCount": 599,
          "title": "Ranking Micro-Influencers: a Novel Multi-Task Learning and Interpretable Framework. (arXiv:2107.13943v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deldjoo_Y/0/1/0/all/0/1\">Yashar Deldjoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merra_F/0/1/0/all/0/1\">Felice Antonio Merra</a>",
          "description": "Recommender systems (RSs) employ user-item feedback, e.g., ratings, to match\ncustomers to personalized lists of products. Approaches to top-k recommendation\nmainly rely on Learning-To-Rank algorithms and, among them, the most widely\nadopted is Bayesian Personalized Ranking (BPR), which bases on a pair-wise\noptimization approach. Recently, BPR has been found vulnerable against\nadversarial perturbations of its model parameters. Adversarial Personalized\nRanking (APR) mitigates this issue by robustifying BPR via an adversarial\ntraining procedure. The empirical improvements of APR's accuracy performance on\nBPR have led to its wide use in several recommender models. However, a key\noverlooked aspect has been the beyond-accuracy performance of APR, i.e.,\nnovelty, coverage, and amplification of popularity bias, considering that\nrecent results suggest that BPR, the building block of APR, is sensitive to the\nintensification of biases and reduction of recommendation novelty. In this\nwork, we model the learning characteristics of the BPR and APR optimization\nframeworks to give mathematical evidence that, when the feedback data have a\ntailed distribution, APR amplifies the popularity bias more than BPR due to an\nunbalanced number of received positive updates from short-head items. Using\nmatrix factorization (MF), we empirically validate the theoretical results by\nperforming preliminary experiments on two public datasets to compare BPR-MF and\nAPR-MF performance on accuracy and beyond-accuracy metrics. The experimental\nresults consistently show the degradation of novelty and coverage measures and\na worrying amplification of bias.",
          "link": "http://arxiv.org/abs/2107.13876",
          "publishedOn": "2021-07-30T02:13:27.172Z",
          "wordCount": 687,
          "title": "Understanding the Effects of Adversarial Personalized Ranking Optimization Method on Recommendation Quality. (arXiv:2107.13876v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Depasquale_E/0/1/0/all/0/1\">Etienne-Victor Depasquale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salam_H/0/1/0/all/0/1\">Humaira Abdul Salam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davoli_F/0/1/0/all/0/1\">Franco Davoli</a>",
          "description": "We suggest an enhancement to structural coding through the use of (a)\ncausally bound codes, (b) basic constructs of graph theory and (c) statistics.\nAs is the norm with structural coding, the codes are collected into categories.\nThe categories are represented by nodes (graph theory). The causality is\nillustrated through links (graph theory) between the nodes and the entire set\nof linked nodes is collected into a single directed acyclic graph. The number\nof occurrences of the nodes and the links provide the input required to analyze\nrelative frequency of occurrence, as well as opening a scope for further\nstatistical analysis. While our raw data was a corpus of literature from a\nspecific discipline, this enhancement is accessible to any qualitative analysis\nthat recognizes causality in its structural codes.",
          "link": "http://arxiv.org/abs/2107.13983",
          "publishedOn": "2021-07-30T02:13:27.143Z",
          "wordCount": 579,
          "title": "PAD: a graphical and numerical enhancement of structural coding to facilitate thematic analysis of a literature corpus. (arXiv:2107.13983v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhizhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>",
          "description": "Despite advances in neural machine translation, cross-lingual retrieval tasks\nin which queries and documents live in different natural language spaces remain\nchallenging. Although neural translation models may provide an intuitive\napproach to tackle the cross-lingual problem, their resource-consuming training\nand advanced model structures may complicate the overall retrieval pipeline and\nreduce users engagement. In this paper, we build our end-to-end Cross-Lingual\nArabic Information REtrieval (CLAIRE) system based on the cross-lingual word\nembedding where searchers are assumed to have a passable passive understanding\nof Arabic and various supporting information in English is provided to aid\nretrieval experience. The proposed system has three major advantages: (1) The\nusage of English-Arabic word embedding simplifies the overall pipeline and\navoids the potential mistakes caused by machine translation. (2) Our CLAIRE\nsystem can incorporate arbitrary word embedding-based neural retrieval models\nwithout structural modification. (3) Early empirical results on an Arabic news\ncollection show promising performance.",
          "link": "http://arxiv.org/abs/2107.13751",
          "publishedOn": "2021-07-30T02:13:27.126Z",
          "wordCount": 572,
          "title": "The Cross-Lingual Arabic Information REtrieval (CLAIRE) System. (arXiv:2107.13751v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/1908.00814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wan-Lei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Peng-Cheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>",
          "description": "k-nearest neighbor graph is a fundamental data structure in many disciplines\nsuch as information retrieval, data-mining, pattern recognition, and machine\nlearning, etc. In the literature, considerable research has been focusing on\nhow to efficiently build an approximate k-nearest neighbor graph (k-NN graph)\nfor a fixed dataset. Unfortunately, a closely related issue of how to merge two\nexisting k-NN graphs has been overlooked. In this paper, we address the issue\nof k-NN graph merging in two different scenarios. In the first scenario, a\nsymmetric merge algorithm is proposed to combine two approximate k-NN graphs.\nThe algorithm facilitates large-scale processing by the efficient merging of\nk-NN graphs that are produced in parallel. In the second scenario, a joint\nmerge algorithm is proposed to expand an existing k-NN graph with a raw\ndataset. The algorithm enables the incremental construction of a hierarchical\napproximate k-NN graph. Superior performance is attained when leveraging the\nhierarchy for NN search of various data types, dimensionality, and distance\nmeasures.",
          "link": "http://arxiv.org/abs/1908.00814",
          "publishedOn": "2021-07-30T02:13:27.117Z",
          "wordCount": 660,
          "title": "On the Merge of k-NN Graph. (arXiv:1908.00814v6 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anchit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1\">Aleksandra Piktus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sonal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>",
          "description": "Pre-training on larger datasets with ever increasing model size is now a\nproven recipe for increased performance across almost all NLP tasks. A notable\nexception is information retrieval, where additional pre-training has so far\nfailed to produce convincing results. We show that, with the right pre-training\nsetup, this barrier can be overcome. We demonstrate this by pre-training large\nbi-encoder models on 1) a recently released set of 65 million synthetically\ngenerated questions, and 2) 200 million post-comment pairs from a preexisting\ndataset of Reddit conversations made available by pushshift.io. We evaluate on\na set of information retrieval and dialogue retrieval benchmarks, showing\nsubstantial improvements over supervised baselines.",
          "link": "http://arxiv.org/abs/2107.13602",
          "publishedOn": "2021-07-30T02:13:27.087Z",
          "wordCount": 554,
          "title": "Domain-matched Pre-training Tasks for Dense Retrieval. (arXiv:2107.13602v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.02590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellogin_A/0/1/0/all/0/1\">Alejandro Bellog&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_A/0/1/0/all/0/1\">Antonio Ferrara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malitesta_D/0/1/0/all/0/1\">Daniele Malitesta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merra_F/0/1/0/all/0/1\">Felice Antonio Merra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomo_C/0/1/0/all/0/1\">Claudio Pomo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donini_F/0/1/0/all/0/1\">Francesco Maria Donini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>",
          "description": "Recommender Systems have shown to be an effective way to alleviate the\nover-choice problem and provide accurate and tailored recommendations. However,\nthe impressive number of proposed recommendation algorithms, splitting\nstrategies, evaluation protocols, metrics, and tasks, has made rigorous\nexperimental evaluation particularly challenging. Puzzled and frustrated by the\ncontinuous recreation of appropriate evaluation benchmarks, experimental\npipelines, hyperparameter optimization, and evaluation procedures, we have\ndeveloped an exhaustive framework to address such needs. Elliot is a\ncomprehensive recommendation framework that aims to run and reproduce an entire\nexperimental pipeline by processing a simple configuration file. The framework\nloads, filters, and splits the data considering a vast set of strategies (13\nsplitting methods and 8 filtering approaches, from temporal training-test\nsplitting to nested K-folds Cross-Validation). Elliot optimizes hyperparameters\n(51 strategies) for several recommendation algorithms (50), selects the best\nmodels, compares them with the baselines providing intra-model statistics,\ncomputes metrics (36) spanning from accuracy to beyond-accuracy, bias, and\nfairness, and conducts statistical analysis (Wilcoxon and Paired t-test). The\naim is to provide the researchers with a tool to ease (and make them\nreproducible) all the experimental evaluation phases, from data reading to\nresults collection. Elliot is available on GitHub\n(https://github.com/sisinflab/elliot).",
          "link": "http://arxiv.org/abs/2103.02590",
          "publishedOn": "2021-07-30T02:13:27.069Z",
          "wordCount": 708,
          "title": "Elliot: a Comprehensive and Rigorous Framework for Reproducible Recommender Systems Evaluation. (arXiv:2103.02590v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daghaghi_S/0/1/0/all/0/1\">Shabnam Daghaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medini_T/0/1/0/all/0/1\">Tharun Medini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meisburger_N/0/1/0/all/0/1\">Nicholas Meisburger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Beidi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengnan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Anshumali Shrivastava</a>",
          "description": "Softmax classifiers with a very large number of classes naturally occur in\nmany applications such as natural language processing and information\nretrieval. The calculation of full softmax is costly from the computational and\nenergy perspective. There have been various sampling approaches to overcome\nthis challenge, popularly known as negative sampling (NS). Ideally, NS should\nsample negative classes from a distribution that is dependent on the input\ndata, the current parameters, and the correct positive class. Unfortunately,\ndue to the dynamically updated parameters and data samples, there is no\nsampling scheme that is provably adaptive and samples the negative classes\nefficiently. Therefore, alternative heuristics like random sampling, static\nfrequency-based sampling, or learning-based biased sampling, which primarily\ntrade either the sampling cost or the adaptivity of samples per iteration are\nadopted. In this paper, we show two classes of distributions where the sampling\nscheme is truly adaptive and provably generates negative samples in\nnear-constant time. Our implementation in C++ on CPU is significantly superior,\nboth in terms of wall-clock time and accuracy, compared to the most optimized\nTensorFlow implementations of other popular negative sampling approaches on\npowerful NVIDIA V100 GPU.",
          "link": "http://arxiv.org/abs/2012.15843",
          "publishedOn": "2021-07-30T02:13:27.055Z",
          "wordCount": 674,
          "title": "A Tale of Two Efficient and Informative Negative Sampling Distributions. (arXiv:2012.15843v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lartigou_F/0/1/0/all/0/1\">Fabrice Lartigou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Govorov_M/0/1/0/all/0/1\">Michael Govorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aisake_T/0/1/0/all/0/1\">Tofiga Aisake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Pankajeshwara N. Sharma</a>",
          "description": "This article deals with the development of an interactive up-to-date Pacific\nIslands Web GIS Atlas. It focuses on the compilation of spatial data from the\ntwelve member countries of the University of the South Pacific (Cook Islands,\nFiji Islands, Kiribati Islands, Marshall Islands, Nauru, Niue, Tonga, Tuvalu,\nTokelau, Solomon Islands, Vanuatu, and Western Samoa). A previous bitmap web\nAtlas was created in 1996, and was a pilot activity investigating the potential\nfor using Geographical Information Systems (GIS) in the South Pacific. The\nobjective of the new atlas is to provide sets of spatial and attributive data\nand maps for use of educators, students, researchers, policy makers and other\nrelevant user groups and the public. GIS is a highly flexible and dynamic\ntechnology that allows the construction and analysis of maps and data sets from\na variety of sources and formats. Nowadays, GIS application has moved from\nlocal and client-server applications to a three-tier architecture: Client (Web\nBrowser) -- Application Web Map Server -- Spatial Data Warehouses. The\nobjective of this project is to produce an Atlas that will include interactive\nmaps and data on an Application Web Map Server. Intergraph products such as\nGeoMedia Professional, Web Map and Web Publisher have been selected for the web\natlas production and design. In an interactive environment, an atlas will be\ncomposed from a series of maps and data profiles, which will be based on legend\nentries, queries, hot spots and cartographic tools. Only the first stage of\ndevelopment of the atlas and related technological solutions are outlined in\nthis article.",
          "link": "http://arxiv.org/abs/2107.14041",
          "publishedOn": "2021-07-30T02:13:27.025Z",
          "wordCount": 706,
          "title": "Interactive GIS Web-Atlas for Twelve Pacific Islands Countries. (arXiv:2107.14041v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhizhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>",
          "description": "The goal of information retrieval is to recommend a list of document\ncandidates that are most relevant to a given query. Listwise learning trains\nneural retrieval models by comparing various candidates simultaneously on a\nlarge scale, offering much more competitive performance than pairwise and\npointwise schemes. Existing listwise ranking losses treat the candidate\ndocument list as a whole unit without further inspection. Some candidates with\nmoderate semantic prominence may be ignored by the noisy similarity signals or\novershadowed by a few especially pronounced candidates. As a result, existing\nranking losses fail to exploit the full potential of neural retrieval models.\nTo address these concerns, we apply the classic pooling technique to conduct\nmulti-level coarse graining and propose ExpertRank, a novel expert-based\nlistwise ranking loss. The proposed scheme has three major advantages: (1)\nExpertRank introduces the profound physics concept of coarse graining to\ninformation retrieval by selecting prominent candidates at various local levels\nbased on model prediction and inter-document comparison. (2) ExpertRank applies\nthe mixture of experts (MoE) technique to combine different experts effectively\nby extending the traditional ListNet. (3) Compared to other existing listwise\nlearning approaches, ExpertRank produces much more reliable and competitive\nperformance for various neural retrieval models with different complexities,\nfrom traditional models, such as KNRM, ConvKNRM, MatchPyramid, to sophisticated\nBERT/ALBERT-based retrieval models.",
          "link": "http://arxiv.org/abs/2107.13752",
          "publishedOn": "2021-07-30T02:13:27.009Z",
          "wordCount": 639,
          "title": "ExpertRank: A Multi-level Coarse-grained Expert-based Listwise Ranking Loss. (arXiv:2107.13752v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadakis_M/0/1/0/all/0/1\">Manolis Fragkiadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putten_P/0/1/0/all/0/1\">Peter van der Putten</a>",
          "description": "Sign language lexica are a useful resource for researchers and people\nlearning sign languages. Current implementations allow a user to search a sign\neither by its gloss or by selecting its primary features such as handshape and\nlocation. This study focuses on exploring a reverse search functionality where\na user can sign a query sign in front of a webcam and retrieve a set of\nmatching signs. By extracting different body joints combinations (upper body,\ndominant hand's arm and wrist) using the pose estimation framework OpenPose, we\ncompare four techniques (PCA, UMAP, DTW and Euclidean distance) as distance\nmetrics between 20 query signs, each performed by eight participants on a 1200\nsign lexicon. The results show that UMAP and DTW can predict a matching sign\nwith an 80\\% and 71\\% accuracy respectively at the top-20 retrieved signs using\nthe movement of the dominant hand arm. Using DTW and adding more sign instances\nfrom other participants in the lexicon, the accuracy can be raised to 90\\% at\nthe top-10 ranking. Our results suggest that our methodology can be used with\nno training in any sign language lexicon regardless of its size.",
          "link": "http://arxiv.org/abs/2107.13637",
          "publishedOn": "2021-07-30T02:13:26.946Z",
          "wordCount": 660,
          "title": "Sign and Search: Sign Search Functionality for Sign Language Lexica. (arXiv:2107.13637v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mayor_O/0/1/0/all/0/1\">Oriol Barbany Mayor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellini_V/0/1/0/all/0/1\">Vito Bellini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buchholz_A/0/1/0/all/0/1\">Alexander Buchholz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benedetto_G/0/1/0/all/0/1\">Giuseppe Di Benedetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granziol_D/0/1/0/all/0/1\">Diego Marco Granziol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruffini_M/0/1/0/all/0/1\">Matteo Ruffini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_Y/0/1/0/all/0/1\">Yannik Stein</a>",
          "description": "Learning-to-rank (LTR) algorithms are ubiquitous and necessary to explore the\nextensive catalogs of media providers. To avoid the user examining all the\nresults, its preferences are used to provide a subset of relatively small size.\nThe user preferences can be inferred from the interactions with the presented\ncontent if explicit ratings are unavailable. However, directly using implicit\nfeedback can lead to learning wrong relevance models and is known as biased\nLTR. The mismatch between implicit feedback and true relevances is due to\nvarious nuisances, with position bias one of the most relevant. Position bias\nmodels consider that the lack of interaction with a presented item is not only\nattributed to the item being irrelevant but because the item was not examined.\nThis paper introduces a method for modeling the probability of an item being\nseen in different contexts, e.g., for different users, with a single estimator.\nOur suggested method, denoted as contextual (EM)-based regression, is\nranker-agnostic and able to correctly learn the latent examination\nprobabilities while only using implicit feedback. Our empirical results\nindicate that the method introduced in this paper outperforms other existing\nposition bias estimators in terms of relative error when the examination\nprobability varies across queries. Moreover, the estimated values provide a\nranking performance boost when used to debias the implicit ranking data even if\nthere is no context dependency on the examination probabilities.",
          "link": "http://arxiv.org/abs/2107.13327",
          "publishedOn": "2021-07-29T02:00:06.739Z",
          "wordCount": 661,
          "title": "Ranker-agnostic Contextual Position Bias Estimation. (arXiv:2107.13327v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Farwa K. Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flanagan_A/0/1/0/all/0/1\">Adrian Flanagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kuan E. Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alamgir_Z/0/1/0/all/0/1\">Zareen Alamgir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammad_Ud_Din_M/0/1/0/all/0/1\">Muhammad Ammad-Ud-Din</a>",
          "description": "We introduce the payload optimization method for federated recommender\nsystems (FRS). In federated learning (FL), the global model payload that is\nmoved between the server and users depends on the number of items to recommend.\nThe model payload grows when there is an increasing number of items. This\nbecomes challenging for an FRS if it is running in production mode. To tackle\nthe payload challenge, we formulated a multi-arm bandit solution that selected\npart of the global model and transmitted it to all users. The selection process\nwas guided by a novel reward function suitable for FL systems. So far as we are\naware, this is the first optimization method that seeks to address item\ndependent payloads. The method was evaluated using three benchmark\nrecommendation datasets. The empirical validation confirmed that the proposed\nmethod outperforms the simpler methods that do not benefit from the bandits for\nthe purpose of item selection. In addition, we have demonstrated the usefulness\nof our proposed method by rigorously evaluating the effects of a payload\nreduction on the recommendation performance degradation. Our method achieved up\nto a 90\\% reduction in model payload, yielding only a $\\sim$4\\% - 8\\% loss in\nthe recommendation performance for highly sparse datasets",
          "link": "http://arxiv.org/abs/2107.13078",
          "publishedOn": "2021-07-29T02:00:06.672Z",
          "wordCount": 674,
          "title": "A Payload Optimization Method for Federated Recommender Systems. (arXiv:2107.13078v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.06467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Andrew Yates</a>",
          "description": "The goal of text ranking is to generate an ordered list of texts retrieved\nfrom a corpus in response to a query. Although the most common formulation of\ntext ranking is search, instances of the task can also be found in many natural\nlanguage processing applications. This survey provides an overview of text\nranking with neural network architectures known as transformers, of which BERT\nis the best-known example. The combination of transformers and self-supervised\npretraining has been responsible for a paradigm shift in natural language\nprocessing (NLP), information retrieval (IR), and beyond. In this survey, we\nprovide a synthesis of existing work as a single point of entry for\npractitioners who wish to gain a better understanding of how to apply\ntransformers to text ranking problems and researchers who wish to pursue work\nin this area. We cover a wide range of modern techniques, grouped into two\nhigh-level categories: transformer models that perform reranking in multi-stage\narchitectures and dense retrieval techniques that perform ranking directly.\nThere are two themes that pervade our survey: techniques for handling long\ndocuments, beyond typical sentence-by-sentence processing in NLP, and\ntechniques for addressing the tradeoff between effectiveness (i.e., result\nquality) and efficiency (e.g., query latency, model and index size). Although\ntransformer architectures and pretraining techniques are recent innovations,\nmany aspects of how they are applied to text ranking are relatively well\nunderstood and represent mature techniques. However, there remain many open\nresearch questions, and thus in addition to laying out the foundations of\npretrained transformers for text ranking, this survey also attempts to\nprognosticate where the field is heading.",
          "link": "http://arxiv.org/abs/2010.06467",
          "publishedOn": "2021-07-29T02:00:06.660Z",
          "wordCount": 725,
          "title": "Pretrained Transformers for Text Ranking: BERT and Beyond. (arXiv:2010.06467v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellogin_A/0/1/0/all/0/1\">Alejandro Bellog&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomo_C/0/1/0/all/0/1\">Claudio Pomo</a>",
          "description": "Collaborative filtering models based on matrix factorization and learned\nsimilarities using Artificial Neural Networks (ANNs) have gained significant\nattention in recent years. This is, in part, because ANNs have demonstrated\ngood results in a wide variety of recommendation tasks. The introduction of\nANNs within the recommendation ecosystem has been recently questioned, raising\nseveral comparisons in terms of efficiency and effectiveness. One aspect most\nof these comparisons have in common is their focus on accuracy, neglecting\nother evaluation dimensions important for the recommendation, such as novelty,\ndiversity, or accounting for biases. We replicate experiments from three papers\nthat compare Neural Collaborative Filtering (NCF) and Matrix Factorization\n(MF), to extend the analysis to other evaluation dimensions. Our contribution\nshows that the experiments are entirely reproducible, and we extend the study\nincluding other accuracy metrics and two statistical hypothesis tests. We\ninvestigated the Diversity and Novelty of the recommendations, showing that MF\nprovides a better accuracy also on the long tail, although NCF provides a\nbetter item coverage and more diversified recommendations. We discuss the bias\neffect generated by the tested methods. They show a relatively small bias, but\nother recommendation baselines, with competitive accuracy performance,\nconsistently show to be less affected by this issue. This is the first work, to\nthe best of our knowledge, where several evaluation dimensions have been\nexplored for an array of SOTA algorithms covering recent adaptations of ANNs\nand MF. Hence, we show the potential these techniques may have on\nbeyond-accuracy evaluation while analyzing the effect on reproducibility these\ncomplementary dimensions may spark. Available at\ngithub.com/sisinflab/Reenvisioning-the-comparison-between-Neural-Collaborative-Filtering-and-Matrix-Factorization",
          "link": "http://arxiv.org/abs/2107.13472",
          "publishedOn": "2021-07-29T02:00:06.641Z",
          "wordCount": 710,
          "title": "Reenvisioning Collaborative Filtering vs Matrix Factorization. (arXiv:2107.13472v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dantong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minjia Zhang</a>",
          "description": "Graph-based algorithms have shown great empirical potential for the\napproximate nearest neighbor (ANN) search problem. Currently, graph-based ANN\nsearch algorithms are designed mainly using heuristics, whereas theoretical\nanalysis of such algorithms is quite lacking. In this paper, we study a\nfundamental model of proximity graphs used in graph-based ANN search, called\nMonotonic Relative Neighborhood Graph (MRNG), from a theoretical perspective.\nWe use mathematical proofs to explain why proximity graphs that are built based\non MRNG tend to have good searching performance. We also run experiments on\nMRNG and graphs generalizing MRNG to obtain a deeper understanding of the\nmodel. Our experiments give guidance on how to approximate and generalize MRNG\nto build proximity graphs on a large scale. In addition, we discover and study\na hidden structure of MRNG called conflicting nodes, and we give theoretical\nevidence how conflicting nodes could be used to improve ANN search methods that\nare based on MRNG.",
          "link": "http://arxiv.org/abs/2107.13052",
          "publishedOn": "2021-07-29T02:00:06.609Z",
          "wordCount": 584,
          "title": "Understanding and Generalizing Monotonic Proximity Graphs for Approximate Nearest Neighbor Search. (arXiv:2107.13052v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_V/0/1/0/all/0/1\">Vivek Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witteveen_S/0/1/0/all/0/1\">Sam Witteveen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrews_M/0/1/0/all/0/1\">Martin Andrews</a>",
          "description": "Creating explanations for answers to science questions is a challenging task\nthat requires multi-hop inference over a large set of fact sentences. This\nyear, to refocus the Textgraphs Shared Task on the problem of gathering\nrelevant statements (rather than solely finding a single 'correct path'), the\nWorldTree dataset was augmented with expert ratings of 'relevance' of\nstatements to each overall explanation. Our system, which achieved second place\non the Shared Task leaderboard, combines initial statement retrieval; language\nmodels trained to predict the relevance scores; and ensembling of a number of\nthe resulting rankings. Our code implementation is made available at\nhttps://github.com/mdda/worldtree_corpus/tree/textgraphs_2021",
          "link": "http://arxiv.org/abs/2107.13031",
          "publishedOn": "2021-07-29T02:00:06.591Z",
          "wordCount": 570,
          "title": "Red Dragon AI at TextGraphs 2021 Shared Task: Multi-Hop Inference Explanation Regeneration by Matching Expert Ratings. (arXiv:2107.13031v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dallmann_A/0/1/0/all/0/1\">Alexander Dallmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoller_D/0/1/0/all/0/1\">Daniel Zoller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hotho_A/0/1/0/all/0/1\">Andreas Hotho</a>",
          "description": "At the present time, sequential item recommendation models are compared by\ncalculating metrics on a small item subset (target set) to speed up\ncomputation. The target set contains the relevant item and a set of negative\nitems that are sampled from the full item set. Two well-known strategies to\nsample negative items are uniform random sampling and sampling by popularity to\nbetter approximate the item frequency distribution in the dataset. Most\nrecently published papers on sequential item recommendation rely on sampling by\npopularity to compare the evaluated models. However, recent work has already\nshown that an evaluation with uniform random sampling may not be consistent\nwith the full ranking, that is, the model ranking obtained by evaluating a\nmetric using the full item set as target set, which raises the question whether\nthe ranking obtained by sampling by popularity is equal to the full ranking. In\nthis work, we re-evaluate current state-of-the-art sequential recommender\nmodels from the point of view, whether these sampling strategies have an impact\non the final ranking of the models. We therefore train four recently proposed\nsequential recommendation models on five widely known datasets. For each\ndataset and model, we employ three evaluation strategies. First, we compute the\nfull model ranking. Then we evaluate all models on a target set sampled by the\ntwo different sampling strategies, uniform random sampling and sampling by\npopularity with the commonly used target set size of 100, compute the model\nranking for each strategy and compare them with each other. Additionally, we\nvary the size of the sampled target set. Overall, we find that both sampling\nstrategies can produce inconsistent rankings compared with the full ranking of\nthe models. Furthermore, both sampling by popularity and uniform random\nsampling do not consistently produce the same ranking ...",
          "link": "http://arxiv.org/abs/2107.13045",
          "publishedOn": "2021-07-29T02:00:06.544Z",
          "wordCount": 740,
          "title": "A Case Study on Sampling Strategies for Evaluating Neural Sequential Item Recommendation Models. (arXiv:2107.13045v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/1907.02704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1\">Vincent Labatut</a> (LIA), <a href=\"http://arxiv.org/find/cs/1/au:+Bost_X/0/1/0/all/0/1\">Xavier Bost</a> (LIA)",
          "description": "A character network is a graph extracted from a narrative, in which vertices\nrepresent characters and edges correspond to interactions between them. A\nnumber of narrative-related problems can be addressed automatically through the\nanalysis of character networks, such as summarization, classification, or role\ndetection. Character networks are particularly relevant when considering works\nof fictions (e.g. novels, plays, movies, TV series), as their exploitation\nallows developing information retrieval and recommendation systems. However,\nworks of fiction possess specific properties making these tasks harder. This\nsurvey aims at presenting and organizing the scientific literature related to\nthe extraction of character networks from works of fiction, as well as their\nanalysis. We first describe the extraction process in a generic way, and\nexplain how its constituting steps are implemented in practice, depending on\nthe medium of the narrative, the goal of the network analysis, and other\nfactors. We then review the descriptive tools used to characterize character\nnetworks, with a focus on the way they are interpreted in this context. We\nillustrate the relevance of character networks by also providing a review of\napplications derived from their analysis. Finally, we identify the limitations\nof the existing approaches, and the most promising perspectives.",
          "link": "http://arxiv.org/abs/1907.02704",
          "publishedOn": "2021-07-28T02:02:30.601Z",
          "wordCount": 714,
          "title": "Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v4 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_L/0/1/0/all/0/1\">Luis Alberto Robles Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callahan_T/0/1/0/all/0/1\">Tiffany J. Callahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banda_J/0/1/0/all/0/1\">Juan M. Banda</a>",
          "description": "The use of social media data, like Twitter, for biomedical research has been\ngradually increasing over the years. With the COVID-19 pandemic, researchers\nhave turned to more nontraditional sources of clinical data to characterize the\ndisease in near real-time, study the societal implications of interventions, as\nwell as the sequelae that recovered COVID-19 cases present (Long-COVID).\nHowever, manually curated social media datasets are difficult to come by due to\nthe expensive costs of manual annotation and the efforts needed to identify the\ncorrect texts. When datasets are available, they are usually very small and\ntheir annotations do not generalize well over time or to larger sets of\ndocuments. As part of the 2021 Biomedical Linked Annotation Hackathon, we\nrelease our dataset of over 120 million automatically annotated tweets for\nbiomedical research purposes. Incorporating best practices, we identify tweets\nwith potentially high clinical relevance. We evaluated our work by comparing\nseveral SpaCy-based annotation frameworks against a manually annotated\ngold-standard dataset. Selecting the best method to use for automatic\nannotation, we then annotated 120 million tweets and released them publicly for\nfuture downstream usage within the biomedical domain.",
          "link": "http://arxiv.org/abs/2107.12565",
          "publishedOn": "2021-07-28T02:02:30.579Z",
          "wordCount": 672,
          "title": "A Biomedically oriented automatically annotated Twitter COVID-19 Dataset. (arXiv:2107.12565v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xueli Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>",
          "description": "Modeling user preference from his historical sequences is one of the core\nproblems of sequential recommendation. Existing methods in this field are\nwidely distributed from conventional methods to deep learning methods. However,\nmost of them only model users' interests within their own sequences and ignore\nthe dynamic collaborative signals among different user sequences, making it\ninsufficient to explore users' preferences. We take inspiration from dynamic\ngraph neural networks to cope with this challenge, modeling the user sequence\nand dynamic collaborative signals into one framework. We propose a new method\nnamed Dynamic Graph Neural Network for Sequential Recommendation (DGSR), which\nconnects different user sequences through a dynamic graph structure, exploring\nthe interactive behavior of users and items with time and order information.\nFurthermore, we design a Dynamic Graph Recommendation Network to extract user's\npreferences from the dynamic graph. Consequently, the next-item prediction task\nin sequential recommendation is converted into a link prediction between the\nuser node and the item node in a dynamic graph. Extensive experiments on three\npublic benchmarks show that DGSR outperforms several state-of-the-art methods.\nFurther studies demonstrate the rationality and effectiveness of modeling user\nsequences through a dynamic graph.",
          "link": "http://arxiv.org/abs/2104.07368",
          "publishedOn": "2021-07-28T02:02:30.556Z",
          "wordCount": 653,
          "title": "Dynamic Graph Neural Networks for Sequential Recommendation. (arXiv:2104.07368v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bobadilla_J/0/1/0/all/0/1\">Jes&#xfa;s Bobadilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_F/0/1/0/all/0/1\">Fernando Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_A/0/1/0/all/0/1\">Abraham Guti&#xe9;rrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Prieto_A/0/1/0/all/0/1\">&#xc1;ngel Gonz&#xe1;lez-Prieto</a>",
          "description": "Deep learning provides accurate collaborative filtering models to improve\nrecommender system results. Deep matrix factorization and their related\ncollaborative neural networks are the state-of-art in the field; nevertheless,\nboth models lack the necessary stochasticity to create the robust, continuous,\nand structured latent spaces that variational autoencoders exhibit. On the\nother hand, data augmentation through variational autoencoder does not provide\naccurate results in the collaborative filtering field due to the high sparsity\nof recommender systems. Our proposed models apply the variational concept to\ninject stochasticity in the latent space of the deep architecture, introducing\nthe variational technique in the neural collaborative filtering field. This\nmethod does not depend on the particular model used to generate the latent\nrepresentation. In this way, this approach can be applied as a plugin to any\ncurrent and future specific models. The proposed models have been tested using\nfour representative open datasets, three different quality measures, and\nstate-of-art baselines. The results show the superiority of the proposed\napproach in scenarios where the variational enrichment exceeds the injected\nnoise effect. Additionally, a framework is provided to enable the\nreproducibility of the conducted experiments.",
          "link": "http://arxiv.org/abs/2107.12677",
          "publishedOn": "2021-07-28T02:02:30.538Z",
          "wordCount": 637,
          "title": "Deep Variational Models for Collaborative Filtering-based Recommender Systems. (arXiv:2107.12677v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2101.08293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nentidis_A/0/1/0/all/0/1\">Anastasios Nentidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krithara_A/0/1/0/all/0/1\">Anastasia Krithara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1\">Georgios Paliouras</a>",
          "description": "The Medical Subject Headings (MeSH) thesaurus is a controlled vocabulary\nwidely used in biomedical knowledge systems, particularly for semantic indexing\nof scientific literature. As the MeSH hierarchy evolves through annual version\nupdates, some new descriptors are introduced that were not previously\navailable. This paper explores the conceptual provenance of these new\ndescriptors. In particular, we investigate whether such new descriptors have\nbeen previously covered by older descriptors and what is their current relation\nto them. To this end, we propose a framework to categorize new descriptors\nbased on their current relation to older descriptors. Based on the proposed\nclassification scheme, we quantify, analyse and present the different types of\nnew descriptors introduced in MeSH during the last fifteen years. The results\nshow that only about 25% of new MeSH descriptors correspond to new emerging\nconcepts, whereas the rest were previously covered by one or more existing\ndescriptors, either implicitly or explicitly. Most of them were covered by a\nsingle existing descriptor and they usually end up as descendants of it in the\ncurrent hierarchy, gradually leading towards a more fine-grained MeSH\nvocabulary. These insights about the dynamics of the thesaurus are useful for\nthe retrospective study of scientific articles annotated with MeSH, but could\nalso be used to inform the policy of updating the thesaurus in the future.",
          "link": "http://arxiv.org/abs/2101.08293",
          "publishedOn": "2021-07-28T02:02:30.319Z",
          "wordCount": 718,
          "title": "What is all this new MeSH about? Exploring the semantic provenance of new descriptors in the MeSH thesaurus. (arXiv:2101.08293v3 [cs.DL] UPDATED)"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2103.03496",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mimnaugh_K/0/1/0/all/0/1\">Katherine J. Mimnaugh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suomalainen_M/0/1/0/all/0/1\">Markku Suomalainen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becerra_I/0/1/0/all/0/1\">Israel Becerra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_E/0/1/0/all/0/1\">Eliezer Lozano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murrieta_Cid_R/0/1/0/all/0/1\">Rafael Murrieta-Cid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LaValle_S/0/1/0/all/0/1\">Steven M. LaValle</a>",
          "description": "This paper considers how the motions of a telepresence robot moving\nautonomously affect a person immersed in the robot through a head-mounted\ndisplay. In particular, we explore the preference, comfort, and naturalness of\nelements of piecewise linear paths compared to the same elements on a smooth\npath. In a user study, thirty-six subjects watched panoramic videos of three\ndifferent paths through a simulated museum in virtual reality and responded to\nquestionnaires regarding each path. Preference for a particular path was\ninfluenced the most by comfort, forward speed, and characteristics of the\nturns. Preference was also strongly associated with the users' perceived\nnaturalness, which was primarily determined by the ability to see salient\nobjects, the distance to the walls and objects, as well as the turns.\nParticipants favored the paths that had a one meter per second forward speed\nand rated the path with the least amount of turns as the most comfortable",
          "link": "http://arxiv.org/abs/2103.03496",
          "publishedOn": "2021-07-30T02:13:27.261Z",
          "wordCount": 628,
          "title": "Analysis of User Preferences for Robot Motions in Immersive Telepresence. (arXiv:2103.03496v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xinyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dongjin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuncong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengzhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingchao Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>",
          "description": "Detecting abnormal activities in real-world surveillance videos is an\nimportant yet challenging task as the prior knowledge about video anomalies is\nusually limited or unavailable. Despite that many approaches have been\ndeveloped to resolve this problem, few of them can capture the normal\nspatio-temporal patterns effectively and efficiently. Moreover, existing works\nseldom explicitly consider the local consistency at frame level and global\ncoherence of temporal dynamics in video sequences. To this end, we propose\nConvolutional Transformer based Dual Discriminator Generative Adversarial\nNetworks (CT-D2GAN) to perform unsupervised video anomaly detection.\nSpecifically, we first present a convolutional transformer to perform future\nframe prediction. It contains three key components, i.e., a convolutional\nencoder to capture the spatial information of the input video clips, a temporal\nself-attention module to encode the temporal dynamics, and a convolutional\ndecoder to integrate spatio-temporal features and predict the future frame.\nNext, a dual discriminator based adversarial training procedure, which jointly\nconsiders an image discriminator that can maintain the local consistency at\nframe-level and a video discriminator that can enforce the global coherence of\ntemporal dynamics, is employed to enhance the future frame prediction. Finally,\nthe prediction error is used to identify abnormal video frames. Thoroughly\nempirical studies on three public video anomaly detection datasets, i.e., UCSD\nPed2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the effectiveness of\nthe proposed adversarial spatio-temporal modeling framework.",
          "link": "http://arxiv.org/abs/2107.13720",
          "publishedOn": "2021-07-30T02:13:27.224Z",
          "wordCount": 708,
          "title": "Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for Video Anomaly Detection. (arXiv:2107.13720v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheuk_K/0/1/0/all/0/1\">Kin Wai Cheuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1\">Dorien Herremans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Li Su</a>",
          "description": "Most of the current supervised automatic music transcription (AMT) models\nlack the ability to generalize. This means that they have trouble transcribing\nreal-world music recordings from diverse musical genres that are not presented\nin the labelled training data. In this paper, we propose a semi-supervised\nframework, ReconVAT, which solves this issue by leveraging the huge amount of\navailable unlabelled music recordings. The proposed ReconVAT uses\nreconstruction loss and virtual adversarial training. When combined with\nexisting U-net models for AMT, ReconVAT achieves competitive results on common\nbenchmark datasets such as MAPS and MusicNet. For example, in the few-shot\nsetting for the string part version of MusicNet, ReconVAT achieves F1-scores of\n61.0% and 41.6% for the note-wise and note-with-offset-wise metrics\nrespectively, which translates into an improvement of 22.2% and 62.5% compared\nto the supervised baseline model. Our proposed framework also demonstrates the\npotential of continual learning on new data, which could be useful in\nreal-world applications whereby new data is constantly available.",
          "link": "http://arxiv.org/abs/2107.04954",
          "publishedOn": "2021-07-30T02:13:27.197Z",
          "wordCount": 632,
          "title": "ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data. (arXiv:2107.04954v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14179",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_A/0/1/0/all/0/1\">Anique Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shan Liu</a>",
          "description": "Photo-realistic point cloud capture and transmission are the fundamental\nenablers for immersive visual communication. The coding process of dynamic\npoint clouds, especially video-based point cloud compression (V-PCC) developed\nby the MPEG standardization group, is now delivering state-of-the-art\nperformance in compression efficiency. V-PCC is based on the projection of the\npoint cloud patches to 2D planes and encoding the sequence as 2D texture and\ngeometry patch sequences. However, the resulting quantization errors from\ncoding can introduce compression artifacts, which can be very unpleasant for\nthe quality of experience (QoE). In this work, we developed a novel\nout-of-the-loop point cloud geometry artifact removal solution that can\nsignificantly improve reconstruction quality without additional bandwidth cost.\nOur novel framework consists of a point cloud sampling scheme, an artifact\nremoval network, and an aggregation scheme. The point cloud sampling scheme\nemploys a cube-based neighborhood patch extraction to divide the point cloud\ninto patches. The geometry artifact removal network then processes these\npatches to obtain artifact-removed patches. The artifact-removed patches are\nthen merged together using an aggregation scheme to obtain the final\nartifact-removed point cloud. We employ 3D deep convolutional feature learning\nfor geometry artifact removal that jointly recovers both the quantization\ndirection and the quantization noise level by exploiting projection and\nquantization prior. The simulation results demonstrate that the proposed method\nis highly effective and can considerably improve the quality of the\nreconstructed point cloud.",
          "link": "http://arxiv.org/abs/2107.14179",
          "publishedOn": "2021-07-30T02:13:27.160Z",
          "wordCount": 673,
          "title": "Video-based Point Cloud Compression Artifact Removal. (arXiv:2107.14179v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadakis_M/0/1/0/all/0/1\">Manolis Fragkiadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putten_P/0/1/0/all/0/1\">Peter van der Putten</a>",
          "description": "Sign language lexica are a useful resource for researchers and people\nlearning sign languages. Current implementations allow a user to search a sign\neither by its gloss or by selecting its primary features such as handshape and\nlocation. This study focuses on exploring a reverse search functionality where\na user can sign a query sign in front of a webcam and retrieve a set of\nmatching signs. By extracting different body joints combinations (upper body,\ndominant hand's arm and wrist) using the pose estimation framework OpenPose, we\ncompare four techniques (PCA, UMAP, DTW and Euclidean distance) as distance\nmetrics between 20 query signs, each performed by eight participants on a 1200\nsign lexicon. The results show that UMAP and DTW can predict a matching sign\nwith an 80\\% and 71\\% accuracy respectively at the top-20 retrieved signs using\nthe movement of the dominant hand arm. Using DTW and adding more sign instances\nfrom other participants in the lexicon, the accuracy can be raised to 90\\% at\nthe top-10 ranking. Our results suggest that our methodology can be used with\nno training in any sign language lexicon regardless of its size.",
          "link": "http://arxiv.org/abs/2107.13637",
          "publishedOn": "2021-07-30T02:13:26.993Z",
          "wordCount": 660,
          "title": "Sign and Search: Sign Search Functionality for Sign Language Lexica. (arXiv:2107.13637v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-29T02:00:07.070Z",
          "wordCount": 700,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1\">Javier Naranjo-Alcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1\">Sergi Perez-Castanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Garcia_A/0/1/0/all/0/1\">Aaron Lopez-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1\">Pedro Zuccarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1\">Maximo Cobos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1\">Francesc J. Ferri</a>",
          "description": "The use of multiple and semantically correlated sources can provide\ncomplementary information to each other that may not be evident when working\nwith individual modalities on their own. In this context, multi-modal models\ncan help producing more accurate and robust predictions in machine learning\ntasks where audio-visual data is available. This paper presents a multi-modal\nmodel for automatic scene classification that exploits simultaneously auditory\nand visual information. The proposed approach makes use of two separate\nnetworks which are respectively trained in isolation on audio and visual data,\nso that each network specializes in a given modality. The visual subnetwork is\na pre-trained VGG16 model followed by a bidiretional recurrent layer, while the\nresidual audio subnetwork is based on stacked squeeze-excitation convolutional\nblocks trained from scratch. After training each subnetwork, the fusion of\ninformation from the audio and visual streams is performed at two different\nstages. The early fusion stage combines features resulting from the last\nconvolutional block of the respective subnetworks at different time steps to\nfeed a bidirectional recurrent structure. The late fusion stage combines the\noutput of the early fusion stage with the independent predictions provided by\nthe two subnetworks, resulting in the final prediction. We evaluate the method\nusing the recently published TAU Audio-Visual Urban Scenes 2021, which contains\nsynchronized audio and video recordings from 12 European cities in 10 different\nscene classes. The proposed model has been shown to provide an excellent\ntrade-off between prediction performance (86.5%) and system complexity (15M\nparameters) in the evaluation results of the DCASE 2021 Challenge.",
          "link": "http://arxiv.org/abs/2107.13180",
          "publishedOn": "2021-07-29T02:00:06.906Z",
          "wordCount": 712,
          "title": "Squeeze-Excitation Convolutional Recurrent Neural Networks for Audio-Visual Scene Classification. (arXiv:2107.13180v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenhua Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "In this work, we propose an adversarial unsupervised domain adaptation (UDA)\napproach with the inherent conditional and label shifts, in which we aim to\nalign the distributions w.r.t. both $p(x|y)$ and $p(y)$. Since the label is\ninaccessible in the target domain, the conventional adversarial UDA assumes\n$p(y)$ is invariant across domains, and relies on aligning $p(x)$ as an\nalternative to the $p(x|y)$ alignment. To address this, we provide a thorough\ntheoretical and empirical analysis of the conventional adversarial UDA methods\nunder both conditional and label shifts, and propose a novel and practical\nalternative optimization scheme for adversarial UDA. Specifically, we infer the\nmarginal $p(y)$ and align $p(x|y)$ iteratively in the training, and precisely\nalign the posterior $p(y|x)$ in testing. Our experimental results demonstrate\nits effectiveness on both classification and segmentation UDA, and partial UDA.",
          "link": "http://arxiv.org/abs/2107.13469",
          "publishedOn": "2021-07-29T02:00:06.786Z",
          "wordCount": 605,
          "title": "Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate. (arXiv:2107.13469v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13385",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wieckowski_A/0/1/0/all/0/1\">Adam Wieckowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lehmann_C/0/1/0/all/0/1\">Christian Lehmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bross_B/0/1/0/all/0/1\">Benjamin Bross</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marpe_D/0/1/0/all/0/1\">Detlev Marpe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biatek_T/0/1/0/all/0/1\">Thibaud Biatek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raulet_M/0/1/0/all/0/1\">Mickael Raulet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feuvre_J/0/1/0/all/0/1\">Jean Le Feuvre</a>",
          "description": "Versatile Video Coding (VVC) is the most recent international video coding\nstandard jointly developed by ITU-T and ISO/IEC, which has been finalized in\nJuly 2020. VVC allows for significant bit-rate reductions around 50% for the\nsame subjective video quality compared to its predecessor, High Efficiency\nVideo Coding (HEVC). One year after finalization, VVC support in devices and\nchipsets is still under development, which is aligned with the typical\ndevelopment cycles of new video coding standards. This paper presents\nopen-source software packages that allow building a complete VVC end-to-end\ntoolchain already one year after its finalization. This includes the Fraunhofer\nHHI VVenC library for fast and efficient VVC encoding as well as HHI's VVdeC\nlibrary for live decoding. An experimental integration of VVC in the GPAC\nsoftware tools and FFmpeg media framework allows packaging VVC bitstreams, e.g.\nencoded with VVenC, in MP4 file format and using DASH for content creation and\nstreaming. The integration of VVdeC allows playback on the receiver. Given\nthese packages, step-by-step tutorials are provided for two possible\napplication scenarios: VVC file encoding plus playback and adaptive streaming\nwith DASH.",
          "link": "http://arxiv.org/abs/2107.13385",
          "publishedOn": "2021-07-29T02:00:06.776Z",
          "wordCount": 652,
          "title": "A Complete End-To-End Open Source Toolchain for the Versatile Video Coding (VVC) Standard. (arXiv:2107.13385v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianhua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">Fei Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xiangui Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yun-Qing Shi</a>",
          "description": "A great challenge to steganography has arisen with the wide application of\nsteganalysis methods based on convolutional neural networks (CNNs). To this\nend, embedding cost learning frameworks based on generative adversarial\nnetworks (GANs) have been proposed and achieved success for spatial\nsteganography. However, the application of GAN to JPEG steganography is still\nin the prototype stage; its anti-detectability and training efficiency should\nbe improved. In conventional steganography, research has shown that the\nside-information calculated from the precover can be used to enhance security.\nHowever, it is hard to calculate the side-information without the spatial\ndomain image. In this work, an embedding cost learning framework for JPEG\nSteganography via a Generative Adversarial Network (JS-GAN) has been proposed,\nthe learned embedding cost can be further adjusted asymmetrically according to\nthe estimated side-information. Experimental results have demonstrated that the\nproposed method can automatically learn a content-adaptive embedding cost\nfunction, and use the estimated side-information properly can effectively\nimprove the security performance. For example, under the attack of a classic\nsteganalyzer GFR with quality factor 75 and 0.4 bpnzAC, the proposed JS-GAN can\nincrease the detection error 2.58% over J-UNIWARD, and the estimated\nside-information aided version JS-GAN(ESI) can further increase the security\nperformance by 11.25% over JS-GAN.",
          "link": "http://arxiv.org/abs/2107.13151",
          "publishedOn": "2021-07-29T02:00:06.723Z",
          "wordCount": 638,
          "title": "JPEG Steganography with Embedding Cost Learning and Side-Information Estimation. (arXiv:2107.13151v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xompero_A/0/1/0/all/0/1\">Alessio Xompero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donaher_S/0/1/0/all/0/1\">Santiago Donaher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iashin_V/0/1/0/all/0/1\">Vladimir Iashin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palermo_F/0/1/0/all/0/1\">Francesca Palermo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solak_G/0/1/0/all/0/1\">G&#xf6;khan Solak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coppola_C/0/1/0/all/0/1\">Claudio Coppola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_R/0/1/0/all/0/1\">Reina Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagao_Y/0/1/0/all/0/1\">Yuichi Nagao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hachiuma_R/0/1/0/all/0/1\">Ryo Hachiuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Chuanlin Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">Rosa H. M. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christmann_G/0/1/0/all/0/1\">Guilherme Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jyun-Ting Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neeharika_G/0/1/0/all/0/1\">Gonuguntla Neeharika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chinnakotla Krishna Teja Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1\">Dinesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehman_B/0/1/0/all/0/1\">Bakhtawar Ur Rehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1\">Andrea Cavallaro</a>",
          "description": "Acoustic and visual sensing can support the contactless estimation of the\nweight of a container and the amount of its content when the container is\nmanipulated by a person. However, transparencies (both of the container and of\nthe content) and the variability of materials, shapes and sizes make this\nproblem challenging. In this paper, we present an open benchmarking framework\nand an in-depth comparative analysis of recent methods that estimate the\ncapacity of a container, as well as the type, mass, and amount of its content.\nThese methods use learned and handcrafted features, such as mel-frequency\ncepstrum coefficients, zero-crossing rate, spectrograms, with different types\nof classifiers to estimate the type and amount of the content with acoustic\ndata, and geometric approaches with visual data to determine the capacity of\nthe container. Results on a newly distributed dataset show that audio alone is\na strong modality and methods achieves a weighted average F1-score up to 81%\nand 97% for content type and level classification, respectively. Estimating the\ncontainer capacity with vision-only approaches and filling mass with\nmulti-modal, multi-stage algorithms reaches up to 65% weighted average capacity\nand mass scores.",
          "link": "http://arxiv.org/abs/2107.12719",
          "publishedOn": "2021-07-28T02:02:30.621Z",
          "wordCount": 692,
          "title": "Multi-modal estimation of the properties of containers and their content: survey and evaluation. (arXiv:2107.12719v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Simonetta_F/0/1/0/all/0/1\">Federico Simonetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntalampiras_S/0/1/0/all/0/1\">Stavros Ntalampiras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avanzini_F/0/1/0/all/0/1\">Federico Avanzini</a>",
          "description": "Audio-to-score alignment (A2SA) is a multimodal task consisting in the\nalignment of audio signals to music scores. Recent literature confirms the\nbenefits of Automatic Music Transcription (AMT) for A2SA at the frame-level. In\nthis work, we aim to elaborate on the exploitation of AMT Deep Learning (DL)\nmodels for achieving alignment at the note-level. We propose a method which\nbenefits from HMM-based score-to-score alignment and AMT, showing a remarkable\nadvancement beyond the state-of-the-art. We design a systematic procedure to\ntake advantage of large datasets which do not offer an aligned score. Finally,\nwe perform a thorough comparison and extensive tests on multiple datasets.",
          "link": "http://arxiv.org/abs/2107.12854",
          "publishedOn": "2021-07-28T02:02:30.546Z",
          "wordCount": 540,
          "title": "Audio-to-Score Alignment Using Deep Automatic Music Transcription. (arXiv:2107.12854v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/1907.02704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1\">Vincent Labatut</a> (LIA), <a href=\"http://arxiv.org/find/cs/1/au:+Bost_X/0/1/0/all/0/1\">Xavier Bost</a> (LIA)",
          "description": "A character network is a graph extracted from a narrative, in which vertices\nrepresent characters and edges correspond to interactions between them. A\nnumber of narrative-related problems can be addressed automatically through the\nanalysis of character networks, such as summarization, classification, or role\ndetection. Character networks are particularly relevant when considering works\nof fictions (e.g. novels, plays, movies, TV series), as their exploitation\nallows developing information retrieval and recommendation systems. However,\nworks of fiction possess specific properties making these tasks harder. This\nsurvey aims at presenting and organizing the scientific literature related to\nthe extraction of character networks from works of fiction, as well as their\nanalysis. We first describe the extraction process in a generic way, and\nexplain how its constituting steps are implemented in practice, depending on\nthe medium of the narrative, the goal of the network analysis, and other\nfactors. We then review the descriptive tools used to characterize character\nnetworks, with a focus on the way they are interpreted in this context. We\nillustrate the relevance of character networks by also providing a review of\napplications derived from their analysis. Finally, we identify the limitations\nof the existing approaches, and the most promising perspectives.",
          "link": "http://arxiv.org/abs/1907.02704",
          "publishedOn": "2021-07-28T02:02:30.498Z",
          "wordCount": 714,
          "title": "Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v4 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Menghan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuzhen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Simon X. Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Ping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>",
          "description": "For people who ardently love painting but unfortunately have visual\nimpairments, holding a paintbrush to create a work is a very difficult task.\nPeople in this special group are eager to pick up the paintbrush, like Leonardo\nda Vinci, to create and make full use of their own talents. Therefore, to\nmaximally bridge this gap, we propose a painting navigation system to assist\nblind people in painting and artistic creation. The proposed system is composed\nof cognitive system and guidance system. The system adopts drawing board\npositioning based on QR code, brush navigation based on target detection and\nbush real-time positioning. Meanwhile, this paper uses human-computer\ninteraction on the basis of voice and a simple but efficient position\ninformation coding rule. In addition, we design a criterion to efficiently\njudge whether the brush reaches the target or not. According to the\nexperimental results, the thermal curves extracted from the faces of testers\nshow that it is relatively well accepted by blindfolded and even blind testers.\nWith the prompt frequency of 1s, the painting navigation system performs best\nwith the completion degree of 89% with SD of 8.37% and overflow degree of 347%\nwith SD of 162.14%. Meanwhile, the excellent and good types of brush tip\ntrajectory account for 74%, and the relative movement distance is 4.21 with SD\nof 2.51. This work demonstrates that it is practicable for the blind people to\nfeel the world through the brush in their hands. In the future, we plan to\ndeploy Angle's Eyes on the phone to make it more portable. The demo video of\nthe proposed painting navigation system is available at:\nhttps://doi.org/10.6084/m9.figshare.9760004.v1.",
          "link": "http://arxiv.org/abs/2107.12921",
          "publishedOn": "2021-07-28T02:02:30.489Z",
          "wordCount": 739,
          "title": "Angel's Girl for Blind Painters: an Efficient Painting Navigation System Validated by Multimodal Evaluation Approach. (arXiv:2107.12921v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14522",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Altinisik_E/0/1/0/all/0/1\">Enes Altinisik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sencar_H/0/1/0/all/0/1\">H&#xfc;srev Taha Sencar</a>",
          "description": "We address the problem of decoding video file fragments when the necessary\nencoding parameters are missing. With this objective, we propose a method that\nautomatically generates H.264 video headers containing these parameters and\nextracts coded pictures in the partially available compressed video data. To\naccomplish this, we examined a very large corpus of videos to learn patterns of\nencoding settings commonly used by encoders and created a parameter dictionary.\nFurther, to facilitate a more efficient search our method identifies\ncharacteristics of a coded bitstream to discriminate the entropy coding mode.\nIt also utilizes the application logs created by the decoder to identify\ncorrect parameter values. Evaluation of the effectiveness of the proposed\nmethod on more than 55K videos with diverse provenance shows that it can\ngenerate valid headers on average in 11.3 decoding trials per video. This\nresult represents an improvement by more than a factor of 10 over the\nconventional approach of video header stitching to recover video file\nfragments.",
          "link": "http://arxiv.org/abs/2104.14522",
          "publishedOn": "2021-07-28T02:02:30.469Z",
          "wordCount": 616,
          "title": "Automatic Generation of H.264 Parameter Sets to Recover Video File Fragments. (arXiv:2104.14522v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1711.05535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrett_M/0/1/0/all/0/1\">Michael Garrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yi-Dong Shen</a>",
          "description": "Matching images and sentences demands a fine understanding of both\nmodalities. In this paper, we propose a new system to discriminatively embed\nthe image and text to a shared visual-textual space. In this field, most\nexisting works apply the ranking loss to pull the positive image / text pairs\nclose and push the negative pairs apart from each other. However, directly\ndeploying the ranking loss is hard for network learning, since it starts from\nthe two heterogeneous features to build inter-modal relationship. To address\nthis problem, we propose the instance loss which explicitly considers the\nintra-modal data distribution. It is based on an unsupervised assumption that\neach image / text group can be viewed as a class. So the network can learn the\nfine granularity from every image/text group. The experiment shows that the\ninstance loss offers better weight initialization for the ranking loss, so that\nmore discriminative embeddings can be learned. Besides, existing works usually\napply the off-the-shelf features, i.e., word2vec and fixed visual feature. So\nin a minor contribution, this paper constructs an end-to-end dual-path\nconvolutional network to learn the image and text representations. End-to-end\nlearning allows the system to directly learn from the data and fully utilize\nthe supervision. On two generic retrieval datasets (Flickr30k and MSCOCO),\nexperiments demonstrate that our method yields competitive accuracy compared to\nstate-of-the-art methods. Moreover, in language based person retrieval, we\nimprove the state of the art by a large margin. The code has been made publicly\navailable.",
          "link": "http://arxiv.org/abs/1711.05535",
          "publishedOn": "2021-07-28T02:02:30.455Z",
          "wordCount": 743,
          "title": "Dual-Path Convolutional Image-Text Embeddings with Instance Loss. (arXiv:1711.05535v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pasandi_H/0/1/0/all/0/1\">Hannaneh Barahouei Pasandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_T/0/1/0/all/0/1\">Tamer Nadeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirpour_H/0/1/0/all/0/1\">Hadi Amirpour</a>",
          "description": "Over the last decade, the bandwidth expansion and MU-MIMO spectral efficiency\nhave promised to increase data throughput by allowing concurrent communication\nbetween one Access Point and multiple users. However, we are still a long way\nfrom enjoying such MU-MIMO MAC protocol improvements for bandwidth hungry\napplications such as video streaming in practical WiFi network settings due to\nheterogeneous channel conditions and devices, unreliable transmissions, and\nlack of useful feedback exchange among the lower and upper layers'\nrequirements. This paper introduces MuViS, a novel dual-phase optimization\nframework that proposes a Quality of Experience (QoE) aware MU-MIMO\noptimization for multi-user video streaming over IEEE 802.11ac. MuViS first\nemploys reinforcement learning to optimize the MU-MIMO user group and mode\nselection for users based on their PHY/MAC layer characteristics. The video\nbitrate is then optimized based on the user's mode (Multi-User (MU) or\nSingle-User (SU)). We present our design and its evaluation on smartphones and\nlaptops using 802.11ac WiFi. Our experimental results in various indoor\nenvironments and configurations show a scalable framework that can support a\nlarge number of users with streaming at high video rates and satisfying QoE\nrequirements.",
          "link": "http://arxiv.org/abs/2106.15262",
          "publishedOn": "2021-07-28T02:02:30.421Z",
          "wordCount": 637,
          "title": "MU-MIMO Grouping For Real-time Applications. (arXiv:2106.15262v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuangjie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaoqing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jin Ye</a>",
          "description": "Recent deep networks have convincingly demonstrated high capability in crowd\ncounting, which is a critical task attracting widespread attention due to its\nvarious industrial applications. Despite such progress, trained data-dependent\nmodels usually can not generalize well to unseen scenarios because of the\ninherent domain shift. To facilitate this issue, this paper proposes a novel\nadversarial scoring network (ASNet) to gradually bridge the gap across domains\nfrom coarse to fine granularity. In specific, at the coarse-grained stage, we\ndesign a dual-discriminator strategy to adapt source domain to be close to the\ntargets from the perspectives of both global and local feature space via\nadversarial learning. The distributions between two domains can thus be aligned\nroughly. At the fine-grained stage, we explore the transferability of source\ncharacteristics by scoring how similar the source samples are to target ones\nfrom multiple levels based on generative probability derived from coarse stage.\nGuided by these hierarchical scores, the transferable source features are\nproperly selected to enhance the knowledge transfer during the adaptation\nprocess. With the coarse-to-fine design, the generalization bottleneck induced\nfrom the domain discrepancy can be effectively alleviated. Three sets of\nmigration experiments show that the proposed methods achieve state-of-the-art\ncounting performance compared with major unsupervised methods.",
          "link": "http://arxiv.org/abs/2107.12858",
          "publishedOn": "2021-07-28T02:02:30.408Z",
          "wordCount": 657,
          "title": "Coarse to Fine: Domain Adaptive Crowd Counting via Adversarial Scoring Network. (arXiv:2107.12858v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.10410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gefei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yuling Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Can Yang</a>",
          "description": "We propose to learn a generative model via entropy interpolation with a\nSchr\\\"{o}dinger Bridge. The generative learning task can be formulated as\ninterpolating between a reference distribution and a target distribution based\non the Kullback-Leibler divergence. At the population level, this entropy\ninterpolation is characterized via an SDE on $[0,1]$ with a time-varying drift\nterm. At the sample level, we derive our Schr\\\"{o}dinger Bridge algorithm by\nplugging the drift term estimated by a deep score estimator and a deep density\nratio estimator into the Euler-Maruyama method. Under some mild smoothness\nassumptions of the target distribution, we prove the consistency of both the\nscore estimator and the density ratio estimator, and then establish the\nconsistency of the proposed Schr\\\"{o}dinger Bridge approach. Our theoretical\nresults guarantee that the distribution learned by our approach converges to\nthe target distribution. Experimental results on multimodal synthetic data and\nbenchmark data support our theoretical findings and indicate that the\ngenerative model via Schr\\\"{o}dinger Bridge is comparable with state-of-the-art\nGANs, suggesting a new formulation of generative learning. We demonstrate its\nusefulness in image interpolation and image inpainting.",
          "link": "http://arxiv.org/abs/2106.10410",
          "publishedOn": "2021-08-02T01:58:24.455Z",
          "wordCount": 648,
          "title": "Deep Generative Learning via Schr\\\"{o}dinger Bridge. (arXiv:2106.10410v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hepburn_A/0/1/0/all/0/1\">Alexander Hepburn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laparra_V/0/1/0/all/0/1\">Valero Laparra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_Rodriguez_R/0/1/0/all/0/1\">Raul Santos-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balle_J/0/1/0/all/0/1\">Johannes Ball&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malo_J/0/1/0/all/0/1\">Jes&#xfa;s Malo</a>",
          "description": "It has been demonstrated many times that the behavior of the human visual\nsystem is connected to the statistics of natural images. Since machine learning\nrelies on the statistics of training data as well, the above connection has\ninteresting implications when using perceptual distances (which mimic the\nbehavior of the human visual system) as a loss function. In this paper, we aim\nto unravel the non-trivial relationship between the probability distribution of\nthe data, perceptual distances, and unsupervised machine learning. To this end,\nwe show that perceptual sensitivity is correlated with the probability of an\nimage in its close neighborhood. We also explore the relation between distances\ninduced by autoencoders and the probability distribution of the data used for\ntraining them, as well as how these induced distances are correlated with human\nperception. Finally, we discuss why perceptual distances might not lead to\nnoticeable gains in performance over standard Euclidean distances in common\nimage processing tasks except when data is scarce and the perceptual distance\nprovides regularization.",
          "link": "http://arxiv.org/abs/2106.04427",
          "publishedOn": "2021-08-02T01:58:24.435Z",
          "wordCount": 649,
          "title": "On the relation between statistical learning and perceptual distances. (arXiv:2106.04427v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sirko_W/0/1/0/all/0/1\">Wojciech Sirko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashubin_S/0/1/0/all/0/1\">Sergii Kashubin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_M/0/1/0/all/0/1\">Marvin Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Annkah_A/0/1/0/all/0/1\">Abigail Annkah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouchareb_Y/0/1/0/all/0/1\">Yasser Salah Eddine Bouchareb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dauphin_Y/0/1/0/all/0/1\">Yann Dauphin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1\">Daniel Keysers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_M/0/1/0/all/0/1\">Maxim Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cisse_M/0/1/0/all/0/1\">Moustapha Cisse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quinn_J/0/1/0/all/0/1\">John Quinn</a>",
          "description": "Identifying the locations and footprints of buildings is vital for many\npractical and scientific purposes. Such information can be particularly useful\nin developing regions where alternative data sources may be scarce. In this\nwork, we describe a model training pipeline for detecting buildings across the\nentire continent of Africa, using 50 cm satellite imagery. Starting with the\nU-Net model, widely used in satellite image analysis, we study variations in\narchitecture, loss functions, regularization, pre-training, self-training and\npost-processing that increase instance segmentation performance. Experiments\nwere carried out using a dataset of 100k satellite images across Africa\ncontaining 1.75M manually labelled building instances, and further datasets for\npre-training and self-training. We report novel methods for improving\nperformance of building detection with this type of model, including the use of\nmixup (mAP +0.12) and self-training with soft KL loss (mAP +0.06). The\nresulting pipeline obtains good results even on a wide variety of challenging\nrural and urban contexts, and was used to create the Open Buildings dataset of\n516M Africa-wide detected footprints.",
          "link": "http://arxiv.org/abs/2107.12283",
          "publishedOn": "2021-08-02T01:58:24.389Z",
          "wordCount": 635,
          "title": "Continental-Scale Building Detection from High Resolution Satellite Imagery. (arXiv:2107.12283v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scholler_C/0/1/0/all/0/1\">Christoph Sch&#xf6;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a>",
          "description": "The future motion of traffic participants is inherently uncertain. To plan\nsafely, therefore, an autonomous agent must take into account multiple possible\ntrajectory outcomes and prioritize them. Recently, this problem has been\naddressed with generative neural networks. However, most generative models\neither do not learn the true underlying trajectory distribution reliably, or do\nnot allow predictions to be associated with likelihoods. In our work, we model\nmotion prediction directly as a density estimation problem with a normalizing\nflow between a noise distribution and the future motion distribution. Our\nmodel, named FloMo, allows likelihoods to be computed in a single network pass\nand can be trained directly with maximum likelihood estimation. Furthermore, we\npropose a method to stabilize training flows on trajectory datasets and a new\ndata augmentation transformation that improves the performance and\ngeneralization of our model. Our method achieves state-of-the-art performance\non three popular prediction datasets, with a significant gap to most competing\nmodels.",
          "link": "http://arxiv.org/abs/2103.03614",
          "publishedOn": "2021-08-02T01:58:24.339Z",
          "wordCount": 634,
          "title": "FloMo: Tractable Motion Prediction with Normalizing Flows. (arXiv:2103.03614v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14803",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Herbreteau_S/0/1/0/all/0/1\">S&#xe9;bastien Herbreteau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kervrann_C/0/1/0/all/0/1\">Charles Kervrann</a>",
          "description": "This work tackles the issue of noise removal from images, focusing on the\nwell-known DCT image denoising algorithm. The latter, stemming from signal\nprocessing, has been well studied over the years. Though very simple, it is\nstill used in crucial parts of state-of-the-art \"traditional\" denoising\nalgorithms such as BM3D. Since a few years however, deep convolutional neural\nnetworks (CNN) have outperformed their traditional counterparts, making signal\nprocessing methods less attractive. In this paper, we demonstrate that a DCT\ndenoiser can be seen as a shallow CNN and thereby its original linear transform\ncan be tuned through gradient descent in a supervised manner, improving\nconsiderably its performance. This gives birth to a fully interpretable CNN\ncalled DCT2net. To deal with remaining artifacts induced by DCT2net, an\noriginal hybrid solution between DCT and DCT2net is proposed combining the best\nthat these two methods can offer; DCT2net is selected to process non-stationary\nimage patches while DCT is optimal for piecewise smooth patches. Experiments on\nartificially noisy images demonstrate that two-layer DCT2net provides\ncomparable results to BM3D and is as fast as DnCNN algorithm composed of more\nthan a dozen of layers.",
          "link": "http://arxiv.org/abs/2107.14803",
          "publishedOn": "2021-08-02T01:58:24.331Z",
          "wordCount": 632,
          "title": "DCT2net: an interpretable shallow CNN for image denoising. (arXiv:2107.14803v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>",
          "description": "Recently, the Siamese-based method has stood out from multitudinous tracking\nmethods owing to its state-of-the-art (SOTA) performance. Nevertheless, due to\nvarious special challenges in UAV tracking, \\textit{e.g.}, severe occlusion and\nfast motion, most existing Siamese-based trackers hardly combine superior\nperformance with high efficiency. To this concern, in this paper, a novel\nattentional Siamese tracker (SiamAPN++) is proposed for real-time UAV tracking.\nBy virtue of the attention mechanism, we conduct a special attentional\naggregation network (AAN) consisting of self-AAN and cross-AAN for raising the\nrepresentation ability of features eventually. The former AAN aggregates and\nmodels the self-semantic interdependencies of the single feature map via\nspatial and channel dimensions. The latter aims to aggregate the\ncross-interdependencies of two different semantic features including the\nlocation information of anchors. In addition, the anchor proposal network based\non dual features is proposed to raise its robustness of tracking objects with\nvarious scales. Experiments on two well-known authoritative benchmarks are\nconducted, where SiamAPN++ outperforms its baseline SiamAPN and other SOTA\ntrackers. Besides, real-world tests onboard a typical embedded platform\ndemonstrate that SiamAPN++ achieves promising tracking results with real-time\nspeed.",
          "link": "http://arxiv.org/abs/2106.08816",
          "publishedOn": "2021-08-02T01:58:24.317Z",
          "wordCount": 662,
          "title": "SiamAPN++: Siamese Attentional Aggregation Network for Real-Time UAV Tracking. (arXiv:2106.08816v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Linxi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_DArpino_C/0/1/0/all/0/1\">Claudia P&#xe9;rez-D&#x27;Arpino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1\">Shyamal Buch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_L/0/1/0/all/0/1\">Lyne P. Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1\">Micael E. Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1\">Kent Vainio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Josiah Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>",
          "description": "We present iGibson 1.0, a novel simulation environment to develop robotic\nsolutions for interactive tasks in large-scale realistic scenes. Our\nenvironment contains 15 fully interactive home-sized scenes with 108 rooms\npopulated with rigid and articulated objects. The scenes are replicas of\nreal-world homes, with distribution and the layout of objects aligned to those\nof the real world. iGibson 1.0 integrates several key features to facilitate\nthe study of interactive tasks: i) generation of high-quality virtual sensor\nsignals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain\nrandomization to change the materials of the objects (both visual and physical)\nand/or their shapes, iii) integrated sampling-based motion planners to generate\ncollision-free trajectories for robot bases and arms, and iv) intuitive\nhuman-iGibson interface that enables efficient collection of human\ndemonstrations. Through experiments, we show that the full interactivity of the\nscenes enables agents to learn useful visual representations that accelerate\nthe training of downstream manipulation tasks. We also show that iGibson 1.0\nfeatures enable the generalization of navigation agents, and that the\nhuman-iGibson interface and integrated motion planners facilitate efficient\nimitation learning of human demonstrated (mobile) manipulation behaviors.\niGibson 1.0 is open-source, equipped with comprehensive examples and\ndocumentation. For more information, visit our project website:\nthis http URL",
          "link": "http://arxiv.org/abs/2012.02924",
          "publishedOn": "2021-08-02T01:58:24.292Z",
          "wordCount": 748,
          "title": "IGibson 1.0: a Simulation Environment for Interactive Tasks in Large Realistic Scenes. (arXiv:2012.02924v5 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.10706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chen Feng</a>",
          "description": "In the domain of visual tracking, most deep learning-based trackers highlight\nthe accuracy but casting aside efficiency. Therefore, their real-world\ndeployment on mobile platforms like the unmanned aerial vehicle (UAV) is\nimpeded. In this work, a novel two-stage Siamese network-based method is\nproposed for aerial tracking, \\textit{i.e.}, stage-1 for high-quality anchor\nproposal generation, stage-2 for refining the anchor proposal. Different from\nanchor-based methods with numerous pre-defined fixed-sized anchors, our\nno-prior method can 1) increase the robustness and generalization to different\nobjects with various sizes, especially to small, occluded, and fast-moving\nobjects, under complex scenarios in light of the adaptive anchor generation, 2)\nmake calculation feasible due to the substantial decrease of anchor numbers. In\naddition, compared to anchor-free methods, our framework has better performance\nowing to refinement at stage-2. Comprehensive experiments on three benchmarks\nhave proven the superior performance of our approach, with a speed of around\n200 frames/s.",
          "link": "http://arxiv.org/abs/2012.10706",
          "publishedOn": "2021-08-02T01:58:24.273Z",
          "wordCount": 643,
          "title": "Siamese Anchor Proposal Network for High-Speed Aerial Tracking. (arXiv:2012.10706v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1\">Congcong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chen Feng</a>",
          "description": "LiDAR point clouds collected from a moving vehicle are functions of its\ntrajectories, because the sensor motion needs to be compensated to avoid\ndistortions. When autonomous vehicles are sending LiDAR point clouds to deep\nnetworks for perception and planning, could the motion compensation\nconsequently become a wide-open backdoor in those networks, due to both the\nadversarial vulnerability of deep learning and GPS-based vehicle trajectory\nestimation that is susceptible to wireless spoofing? We demonstrate such\npossibilities for the first time: instead of directly attacking point cloud\ncoordinates which requires tampering with the raw LiDAR readings, only\nadversarial spoofing of a self-driving car's trajectory with small\nperturbations is enough to make safety-critical objects undetectable or\ndetected with incorrect positions. Moreover, polynomial trajectory perturbation\nis developed to achieve a temporally-smooth and highly-imperceptible attack.\nExtensive experiments on 3D object detection have shown that such attacks not\nonly lower the performance of the state-of-the-art detectors effectively, but\nalso transfer to other detectors, raising a red flag for the community. The\ncode is available on https://ai4ce.github.io/FLAT/.",
          "link": "http://arxiv.org/abs/2103.15326",
          "publishedOn": "2021-08-02T01:58:24.248Z",
          "wordCount": 645,
          "title": "Fooling LiDAR Perception via Adversarial Trajectory Perturbation. (arXiv:2103.15326v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zijian Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>",
          "description": "In this paper, we propose MINE to perform novel view synthesis and depth\nestimation via dense 3D reconstruction from a single image. Our approach is a\ncontinuous depth generalization of the Multiplane Images (MPI) by introducing\nthe NEural radiance fields (NeRF). Given a single image as input, MINE predicts\na 4-channel image (RGB and volume density) at arbitrary depth values to jointly\nreconstruct the camera frustum and fill in occluded contents. The reconstructed\nand inpainted frustum can then be easily rendered into novel RGB or depth views\nusing differentiable rendering. Extensive experiments on RealEstate10K, KITTI\nand Flowers Light Fields show that our MINE outperforms state-of-the-art by a\nlarge margin in novel view synthesis. We also achieve competitive results in\ndepth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our\nsource code is available at https://github.com/vincentfung13/MINE",
          "link": "http://arxiv.org/abs/2103.14910",
          "publishedOn": "2021-08-02T01:58:24.240Z",
          "wordCount": 639,
          "title": "MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis. (arXiv:2103.14910v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15687",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gerg_I/0/1/0/all/0/1\">Isaac Gerg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Monga_V/0/1/0/all/0/1\">Vishal Monga</a>",
          "description": "Synthetic aperture sonar (SAS) requires precise positional and environmental\ninformation to produce well-focused output during the image reconstruction\nstep. However, errors in these measurements are commonly present resulting in\ndefocused imagery. To overcome these issues, an \\emph{autofocus} algorithm is\nemployed as a post-processing step after image reconstruction for the purpose\nof improving image quality using the image content itself. These algorithms are\nusually iterative and metric-based in that they seek to optimize an image\nsharpness metric. In this letter, we demonstrate the potential of machine\nlearning, specifically deep learning, to address the autofocus problem. We\nformulate the problem as a self-supervised, phase error estimation task using a\ndeep network we call Deep Autofocus. Our formulation has the advantages of\nbeing non-iterative (and thus fast) and not requiring ground truth\nfocused-defocused images pairs as often required by other deblurring deep\nlearning methods. We compare our technique against a set of common sharpness\nmetrics optimized using gradient descent over a real-world dataset. Our results\ndemonstrate Deep Autofocus can produce imagery that is perceptually as good as\nbenchmark iterative techniques but at a substantially lower computational cost.\nWe conclude that our proposed Deep Autofocus can provide a more favorable\ncost-quality trade-off than state-of-the-art alternatives with significant\npotential of future research.",
          "link": "http://arxiv.org/abs/2010.15687",
          "publishedOn": "2021-08-02T01:58:24.219Z",
          "wordCount": 670,
          "title": "Deep Autofocus for Synthetic Aperture Sonar. (arXiv:2010.15687v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haizhou Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youcai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenjie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>",
          "description": "It is a consensus that small models perform quite poorly under the paradigm\nof self-supervised contrastive learning. Existing methods usually adopt a large\noff-the-shelf model to transfer knowledge to the small one via knowledge\ndistillation. Despite their effectiveness, distillation-based methods may not\nbe suitable for some resource-restricted scenarios due to the huge\ncomputational expenses of deploying a large model. In this paper, we study the\nissue of training self-supervised small models without distillation signals. We\nfirst evaluate the representation spaces of the small models and make two\nnon-negligible observations: (i) small models can complete the pretext task\nwithout overfitting despite its limited capacity; (ii) small models universally\nsuffer the problem of over-clustering. Then we verify multiple assumptions that\nare considered to alleviate the over-clustering phenomenon. Finally, we combine\nthe validated techniques and improve the baseline of five small architectures\nwith considerable margins, which indicates that training small self-supervised\ncontrastive models is feasible even without distillation signals.",
          "link": "http://arxiv.org/abs/2107.14762",
          "publishedOn": "2021-08-02T01:58:24.206Z",
          "wordCount": 610,
          "title": "On the Efficacy of Small Self-Supervised Contrastive Models without Distillation Signals. (arXiv:2107.14762v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1\">Jean-Baptiste Alayrac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doersch_C/0/1/0/all/0/1\">Carl Doersch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_C/0/1/0/all/0/1\">Catalin Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">David Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1\">Skanda Koppula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andrew Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1\">Evan Shelhamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1\">Olivier H&#xe9;naff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1\">Matthew M. Botvinick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Jo&#xe3;o Carreira</a>",
          "description": "The recently-proposed Perceiver model obtains good results on several domains\n(images, audio, multimodal, point clouds) while scaling linearly in compute and\nmemory with the input size. While the Perceiver supports many kinds of inputs,\nit can only produce very simple outputs such as class scores. Perceiver IO\novercomes this limitation without sacrificing the original's appealing\nproperties by learning to flexibly query the model's latent space to produce\noutputs of arbitrary size and semantics. Perceiver IO still decouples model\ndepth from data size and still scales linearly with data size, but now with\nrespect to both input and output sizes. The full Perceiver IO model achieves\nstrong results on tasks with highly structured output spaces, such as natural\nlanguage and visual understanding, StarCraft II, and multi-task and multi-modal\ndomains. As highlights, Perceiver IO matches a Transformer-based BERT baseline\non the GLUE language benchmark without the need for input tokenization and\nachieves state-of-the-art performance on Sintel optical flow estimation.",
          "link": "http://arxiv.org/abs/2107.14795",
          "publishedOn": "2021-08-02T01:58:24.128Z",
          "wordCount": 639,
          "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs. (arXiv:2107.14795v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kamnitsas_K/0/1/0/all/0/1\">Konstantinos Kamnitsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winzeck_S/0/1/0/all/0/1\">Stefan Winzeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornaropoulos_E/0/1/0/all/0/1\">Evgenios N. Kornaropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitehouse_D/0/1/0/all/0/1\">Daniel Whitehouse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Englman_C/0/1/0/all/0/1\">Cameron Englman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phyu_P/0/1/0/all/0/1\">Poe Phyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pao_N/0/1/0/all/0/1\">Norman Pao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_D/0/1/0/all/0/1\">David K. Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_T/0/1/0/all/0/1\">Tilak Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newcombe_V/0/1/0/all/0/1\">Virginia F.J. Newcombe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>",
          "description": "Semi-supervised learning (SSL) uses unlabeled data during training to learn\nbetter models. Previous studies on SSL for medical image segmentation focused\nmostly on improving model generalization to unseen data. In some applications,\nhowever, our primary interest is not generalization but to obtain optimal\npredictions on a specific unlabeled database that is fully available during\nmodel development. Examples include population studies for extracting imaging\nphenotypes. This work investigates an often overlooked aspect of SSL,\ntransduction. It focuses on the quality of predictions made on the unlabeled\ndata of interest when they are included for optimization during training,\nrather than improving generalization. We focus on the self-training framework\nand explore its potential for transduction. We analyze it through the lens of\nInformation Gain and reveal that learning benefits from the use of calibrated\nor under-confident models. Our extensive experiments on a large MRI database\nfor multi-class segmentation of traumatic brain lesions shows promising results\nwhen comparing transductive with inductive predictions. We believe this study\nwill inspire further research on transductive learning, a well-suited paradigm\nfor medical image analysis.",
          "link": "http://arxiv.org/abs/2107.08964",
          "publishedOn": "2021-08-02T01:58:24.048Z",
          "wordCount": 674,
          "title": "Transductive image segmentation: Self-training and effect of uncertainty estimation. (arXiv:2107.08964v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>",
          "description": "To improve the viewer's Quality of Experience (QoE) and optimize computer\ngraphics applications, 3D model quality assessment (3D-QA) has become an\nimportant task in the multimedia area. Point cloud and mesh are the two most\nwidely used digital representation formats of 3D models, the visual quality of\nwhich is quite sensitive to lossy operations like simplification and\ncompression. Therefore, many related studies such as point cloud quality\nassessment (PCQA) and mesh quality assessment (MQA) have been carried out to\nmeasure the caused visual quality degradations. However, a large part of\nprevious studies utilizes full-reference (FR) metrics, which means they may\nfail to predict the quality level with the absence of the reference 3D model.\nFurthermore, few 3D-QA metrics are carried out to consider color information,\nwhich significantly restricts the effectiveness and scope of application. In\nthis paper, we propose a no-reference (NR) quality assessment metric for\ncolored 3D models represented by both point cloud and mesh. First, we project\nthe 3D models from 3D space into quality-related geometry and color feature\ndomains. Then, the natural scene statistics (NSS) and entropy are utilized to\nextract quality-aware features. Finally, the Support Vector Regressor (SVR) is\nemployed to regress the quality-aware features into quality scores. Our method\nis mainly validated on the colored point cloud quality assessment database\n(SJTU-PCQA) and the colored mesh quality assessment database (CMDM). The\nexperimental results show that the proposed method outperforms all the\nstate-of-art NR 3D-QA metrics and obtains an acceptable gap with the\nstate-of-art FR 3D-QA metrics.",
          "link": "http://arxiv.org/abs/2107.02041",
          "publishedOn": "2021-08-02T01:58:24.001Z",
          "wordCount": 741,
          "title": "No-Reference Quality Assessment for 3D Colored Point Cloud and Mesh Models. (arXiv:2107.02041v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Liangjian Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lili Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>",
          "description": "The goal of few-shot classification is to classify new categories with few\nlabeled examples within each class. Nowadays, the excellent performance in\nhandling few-shot classification problems is shown by metric-based\nmeta-learning methods. However, it is very hard for previous methods to\ndiscriminate the fine-grained sub-categories in the embedding space without\nfine-grained labels. This may lead to unsatisfactory generalization to\nfine-grained subcategories, and thus affects model interpretation. To tackle\nthis problem, we introduce the contrastive loss into few-shot classification\nfor learning latent fine-grained structure in the embedding space. Furthermore,\nto overcome the drawbacks of random image transformation used in current\ncontrastive learning in producing noisy and inaccurate image pairs (i.e.,\nviews), we develop a learning-to-learn algorithm to automatically generate\ndifferent views of the same image. Extensive experiments on standard few-shot\nlearning benchmarks demonstrate the superiority of our method.",
          "link": "http://arxiv.org/abs/2107.09242",
          "publishedOn": "2021-08-02T01:58:23.993Z",
          "wordCount": 595,
          "title": "Boosting Few-Shot Classification with View-Learnable Contrastive Learning. (arXiv:2107.09242v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1\">Matteo Stefanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1\">Silvia Cascianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Connecting Vision and Language plays an essential role in Generative\nIntelligence. For this reason, large research efforts have been devoted to\nimage captioning, i.e. describing images with syntactically and semantically\nmeaningful sentences. Starting from 2015 the task has generally been addressed\nwith pipelines composed of a visual encoder and a language model for text\ngeneration. During these years, both components have evolved considerably\nthrough the exploitation of object regions, attributes, the introduction of\nmulti-modal connections, fully-attentive approaches, and BERT-like early-fusion\nstrategies. However, regardless of the impressive results, research in image\ncaptioning has not reached a conclusive answer yet. This work aims at providing\na comprehensive overview of image captioning approaches, from visual encoding\nand text generation to training strategies, datasets, and evaluation metrics.\nIn this respect, we quantitatively compare many relevant state-of-the-art\napproaches to identify the most impactful technical innovations in\narchitectures and training strategies. Moreover, many variants of the problem\nand its open challenges are discussed. The final goal of this work is to serve\nas a tool for understanding the existing literature and highlighting the future\ndirections for a research area where Computer Vision and Natural Language\nProcessing can find an optimal synergy.",
          "link": "http://arxiv.org/abs/2107.06912",
          "publishedOn": "2021-08-02T01:58:23.964Z",
          "wordCount": 659,
          "title": "From Show to Tell: A Survey on Image Captioning. (arXiv:2107.06912v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guowen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Han Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shangwei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Run Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>",
          "description": "This paper presents a novel fingerprinting scheme for the Intellectual\nProperty (IP) protection of Generative Adversarial Networks (GANs). Prior\nsolutions for classification models adopt adversarial examples as the\nfingerprints, which can raise stealthiness and robustness problems when they\nare applied to the GAN models. Our scheme constructs a composite deep learning\nmodel from the target GAN and a classifier. Then we generate stealthy\nfingerprint samples from this composite model, and register them to the\nclassifier for effective ownership verification. This scheme inspires three\nconcrete methodologies to practically protect the modern GAN models.\nTheoretical analysis proves that these methods can satisfy different security\nrequirements necessary for IP protection. We also conduct extensive experiments\nto show that our solutions outperform existing strategies in terms of\nstealthiness, functionality-preserving and unremovability.",
          "link": "http://arxiv.org/abs/2106.11760",
          "publishedOn": "2021-08-02T01:58:23.958Z",
          "wordCount": 609,
          "title": "A Novel Verifiable Fingerprinting Scheme for Generative Adversarial Networks. (arXiv:2106.11760v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.09193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sekuboyina_A/0/1/0/all/0/1\">Anjany Sekuboyina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Husseini_M/0/1/0/all/0/1\">Malek E. Husseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayat_A/0/1/0/all/0/1\">Amirhossein Bayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loffler_M/0/1/0/all/0/1\">Maximilian L&#xf6;ffler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebl_H/0/1/0/all/0/1\">Hans Liebl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tetteh_G/0/1/0/all/0/1\">Giles Tetteh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukacka_J/0/1/0/all/0/1\">Jan Kuka&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Payer_C/0/1/0/all/0/1\">Christian Payer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stern_D/0/1/0/all/0/1\">Darko &#x160;tern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urschler_M/0/1/0/all/0/1\">Martin Urschler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maodong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">Dalong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lessmann_N/0/1/0/all/0/1\">Nikolas Lessmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yujin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianfu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambellan_F/0/1/0/all/0/1\">Felix Ambellan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiranashvili_T/0/1/0/all/0/1\">Tamaz Amiranashvili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehlke_M/0/1/0/all/0/1\">Moritz Ehlke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamecker_H/0/1/0/all/0/1\">Hans Lamecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehnert_S/0/1/0/all/0/1\">Sebastian Lehnert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lirio_M/0/1/0/all/0/1\">Marilia Lirio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olaguer_N/0/1/0/all/0/1\">Nicol&#xe1;s P&#xe9;rez de Olaguer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramm_H/0/1/0/all/0/1\">Heiko Ramm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_M/0/1/0/all/0/1\">Manish Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tack_A/0/1/0/all/0/1\">Alexander Tack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zachow_S/0/1/0/all/0/1\">Stefan Zachow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinjun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angerman_C/0/1/0/all/0/1\">Christoph Angerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_K/0/1/0/all/0/1\">Kevin Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirszenberg_A/0/1/0/all/0/1\">Alexandre Kirszenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puybareau_E/0/1/0/all/0/1\">&#xc9;lodie Puybareau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Di Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yiwei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rapazzo_B/0/1/0/all/0/1\">Brandon H. Rapazzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeah_T/0/1/0/all/0/1\">Timyoas Yeah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Amber Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shangliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1\">Feng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiqiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiangshang_Z/0/1/0/all/0/1\">Zheng Xiangshang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liming_X/0/1/0/all/0/1\">Xu Liming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netherton_T/0/1/0/all/0/1\">Tucker J. Netherton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mumme_R/0/1/0/all/0/1\">Raymond P. Mumme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Court_L/0/1/0/all/0/1\">Laurence E. Court</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixun Huang</a>, et al. (18 additional authors not shown)",
          "description": "Vertebral labelling and segmentation are two fundamental tasks in an\nautomated spine processing pipeline. Reliable and accurate processing of spine\nimages is expected to benefit clinical decision-support systems for diagnosis,\nsurgery planning, and population-based analysis on spine and bone health.\nHowever, designing automated algorithms for spine processing is challenging\npredominantly due to considerable variations in anatomy and acquisition\nprotocols and due to a severe shortage of publicly available data. Addressing\nthese limitations, the Large Scale Vertebrae Segmentation Challenge (VerSe) was\norganised in conjunction with the International Conference on Medical Image\nComputing and Computer Assisted Intervention (MICCAI) in 2019 and 2020, with a\ncall for algorithms towards labelling and segmentation of vertebrae. Two\ndatasets containing a total of 374 multi-detector CT scans from 355 patients\nwere prepared and 4505 vertebrae have individually been annotated at\nvoxel-level by a human-machine hybrid algorithm (https://osf.io/nqjyw/,\nhttps://osf.io/t98fz/). A total of 25 algorithms were benchmarked on these\ndatasets. In this work, we present the the results of this evaluation and\nfurther investigate the performance-variation at vertebra-level, scan-level,\nand at different fields-of-view. We also evaluate the generalisability of the\napproaches to an implicit domain shift in data by evaluating the top performing\nalgorithms of one challenge iteration on data from the other iteration. The\nprincipal takeaway from VerSe: the performance of an algorithm in labelling and\nsegmenting a spine scan hinges on its ability to correctly identify vertebrae\nin cases of rare anatomical variations. The content and code concerning VerSe\ncan be accessed at: https://github.com/anjany/verse.",
          "link": "http://arxiv.org/abs/2001.09193",
          "publishedOn": "2021-08-02T01:58:23.945Z",
          "wordCount": 933,
          "title": "VerSe: A Vertebrae Labelling and Segmentation Benchmark for Multi-detector CT Images. (arXiv:2001.09193v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhishan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guohui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Dawei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>",
          "description": "As a critical component for online advertising and marking, click-through\nrate (CTR) prediction has draw lots of attentions from both industry and\nacademia field. Recently, the deep learning has become the mainstream\nmethodological choice for CTR. Despite of sustainable efforts have been made,\nexisting approaches still pose several challenges. On the one hand, high-order\ninteraction between the features is under-explored. On the other hand,\nhigh-order interactions may neglect the semantic information from the low-order\nfields. In this paper, we proposed a novel prediction method, named FINT, that\nemploys the Field-aware INTeraction layer which captures high-order feature\ninteractions while retaining the low-order field information. To empirically\ninvestigate the effectiveness and robustness of the FINT, we perform extensive\nexperiments on the three realistic databases: KDD2012, Criteo and Avazu. The\nobtained results demonstrate that the FINT can significantly improve the\nperformance compared to the existing methods, without increasing the amount of\ncomputation required. Moreover, the proposed method brought about 2.72\\%\nincrease to the advertising revenue of a big online video app through A/B\ntesting. To better promote the research in CTR field, we released our code as\nwell as reference implementation at: https://github.com/zhishan01/FINT.",
          "link": "http://arxiv.org/abs/2107.01999",
          "publishedOn": "2021-08-02T01:58:23.925Z",
          "wordCount": 654,
          "title": "FINT: Field-aware INTeraction Neural Network For CTR Prediction. (arXiv:2107.01999v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bengar_J/0/1/0/all/0/1\">Javad Zolfaghari Bengar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raducanu_B/0/1/0/all/0/1\">Bogdan Raducanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>",
          "description": "Active learning aims to select samples to be annotated that yield the largest\nperformance improvement for the learning algorithm. Many methods approach this\nproblem by measuring the informativeness of samples and do this based on the\ncertainty of the network predictions for samples. However, it is well-known\nthat neural networks are overly confident about their prediction and are\ntherefore an untrustworthy source to assess sample informativeness. In this\npaper, we propose a new informativeness-based active learning method. Our\nmeasure is derived from the learning dynamics of a neural network. More\nprecisely we track the label assignment of the unlabeled data pool during the\ntraining of the algorithm. We capture the learning dynamics with a metric\ncalled label-dispersion, which is low when the network consistently assigns the\nsame label to the sample during the training of the network and high when the\nassigned label changes frequently. We show that label-dispersion is a promising\npredictor of the uncertainty of the network, and show on two benchmark datasets\nthat an active learning algorithm based on label-dispersion obtains excellent\nresults.",
          "link": "http://arxiv.org/abs/2107.14707",
          "publishedOn": "2021-08-02T01:58:23.917Z",
          "wordCount": 635,
          "title": "When Deep Learners Change Their Mind: Learning Dynamics for Active Learning. (arXiv:2107.14707v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Duo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>",
          "description": "Domain adaptation is critical for success when confronting with the lack of\nannotations in a new domain. As the huge time consumption of labeling process\non 3D point cloud, domain adaptation for 3D semantic segmentation is of great\nexpectation. With the rise of multi-modal datasets, large amount of 2D images\nare accessible besides 3D point clouds. In light of this, we propose to further\nleverage 2D data for 3D domain adaptation by intra and inter domain cross modal\nlearning. As for intra-domain cross modal learning, most existing works sample\nthe dense 2D pixel-wise features into the same size with sparse 3D point-wise\nfeatures, resulting in the abandon of numerous useful 2D features. To address\nthis problem, we propose Dynamic sparse-to-dense Cross Modal Learning (DsCML)\nto increase the sufficiency of multi-modality information interaction for\ndomain adaptation. For inter-domain cross modal learning, we further advance\nCross Modal Adversarial Learning (CMAL) on 2D and 3D data which contains\ndifferent semantic content aiming to promote high-level modal complementarity.\nWe evaluate our model under various multi-modality domain adaptation settings\nincluding day-to-night, country-to-country and dataset-to-dataset, brings large\nimprovements over both uni-modal and multi-modal domain adaptation methods on\nall settings.",
          "link": "http://arxiv.org/abs/2107.14724",
          "publishedOn": "2021-08-02T01:58:23.910Z",
          "wordCount": 652,
          "title": "Sparse-to-dense Feature Matching: Intra and Inter domain Cross-modal Learning in Domain Adaptation for 3D Semantic Segmentation. (arXiv:2107.14724v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sako_T/0/1/0/all/0/1\">Tomas Sako</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_A/0/1/0/all/0/1\">Arturo Jr M. Martinez</a>",
          "description": "Since the United Nations launched the Sustainable Development Goals (SDG) in\n2015, numerous universities, NGOs and other organizations have attempted to\ndevelop tools for monitoring worldwide progress in achieving them. Led by\nadvancements in the fields of earth observation techniques, data sciences and\nthe emergence of artificial intelligence, a number of research teams have\ndeveloped innovative tools for highlighting areas of vulnerability and tracking\nthe implementation of SDG targets. In this paper we demonstrate that\nindividuals with no organizational affiliation and equipped only with common\nhardware, publicly available datasets and cloud-based computing services can\nparticipate in the improvement of predicting machine-learning-based approaches\nto predicting local poverty levels in a given agro-ecological environment. The\napproach builds upon several pioneering efforts over the last five years\nrelated to mapping poverty by deep learning to process satellite imagery and\n\"ground-truth\" data from the field to link features with incidence of poverty\nin a particular context. The approach employs new methods for object\nidentification in order to optimize the modeled results and achieve\nsignificantly high accuracy. A key goal of the project was to intentionally\nkeep costs as low as possible - by using freely available resources - so that\ncitizen scientists, students and organizations could replicate the method in\nother areas of interest. Moreover, for simplicity, the input data used were\nderived from just a handful of sources (involving only earth observation and\npopulation headcounts). The results of the project could therefore certainly be\nstrengthened further through the integration of proprietary data from social\nnetworks, mobile phone providers, and other sources.",
          "link": "http://arxiv.org/abs/2107.14700",
          "publishedOn": "2021-08-02T01:58:23.903Z",
          "wordCount": 697,
          "title": "Seeing poverty from space, how much can it be tuned?. (arXiv:2107.14700v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_S/0/1/0/all/0/1\">Sakshi Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1\">Vinay Kumar Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1\">Srijith P K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1\">Piyush Rai</a>",
          "description": "We present a continual learning approach for generative adversarial networks\n(GANs), by designing and leveraging parameter-efficient feature map\ntransformations. Our approach is based on learning a set of global and\ntask-specific parameters. The global parameters are fixed across tasks whereas\nthe task-specific parameters act as local adapters for each task, and help in\nefficiently obtaining task-specific feature maps. Moreover, we propose an\nelement-wise addition of residual bias in the transformed feature space, which\nfurther helps stabilize GAN training in such settings. Our approach also\nleverages task similarity information based on the Fisher information matrix.\nLeveraging this knowledge from previous tasks significantly improves the model\nperformance. In addition, the similarity measure also helps reduce the\nparameter growth in continual adaptation and helps to learn a compact model. In\ncontrast to the recent approaches for continually-learned GANs, the proposed\napproach provides a memory-efficient way to perform effective continual data\ngeneration. Through extensive experiments on challenging and diverse datasets,\nwe show that the feature-map-transformation approach outperforms\nstate-of-the-art methods for continually-learned GANs, with substantially fewer\nparameters. The proposed method generates high-quality samples that can also\nimprove the generative-replay-based continual learning for discriminative\ntasks.",
          "link": "http://arxiv.org/abs/2103.04032",
          "publishedOn": "2021-08-02T01:58:23.897Z",
          "wordCount": 667,
          "title": "CAM-GAN: Continual Adaptation Modules for Generative Adversarial Networks. (arXiv:2103.04032v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Budd_S/0/1/0/all/0/1\">Samuel Budd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Day_T/0/1/0/all/0/1\">Thomas Day</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simpson_J/0/1/0/all/0/1\">John Simpson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lloyd_K/0/1/0/all/0/1\">Karen Lloyd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthew_J/0/1/0/all/0/1\">Jacqueline Matthew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skelton_E/0/1/0/all/0/1\">Emily Skelton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_R/0/1/0/all/0/1\">Reza Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>",
          "description": "Probably yes. -- Supervised Deep Learning dominates performance scores for\nmany computer vision tasks and defines the state-of-the-art. However, medical\nimage analysis lags behind natural image applications. One of the many reasons\nis the lack of well annotated medical image data available to researchers. One\nof the first things researchers are told is that we require significant\nexpertise to reliably and accurately interpret and label such data. We see\nsignificant inter- and intra-observer variability between expert annotations of\nmedical images. Still, it is a widely held assumption that novice annotators\nare unable to provide useful annotations for use by clinical Deep Learning\nmodels. In this work we challenge this assumption and examine the implications\nof using a minimally trained novice labelling workforce to acquire annotations\nfor a complex medical image dataset. We study the time and cost implications of\nusing novice annotators, the raw performance of novice annotators compared to\ngold-standard expert annotators, and the downstream effects on a trained Deep\nLearning segmentation model's performance for detecting a specific congenital\nheart disease (hypoplastic left heart syndrome) in fetal ultrasound imaging.",
          "link": "http://arxiv.org/abs/2107.14682",
          "publishedOn": "2021-08-02T01:58:23.880Z",
          "wordCount": 647,
          "title": "Can non-specialists provide high quality gold standard labels in challenging modalities?. (arXiv:2107.14682v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gnecco_L/0/1/0/all/0/1\">Lucas Gnecco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boria_N/0/1/0/all/0/1\">Nicolas Boria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bougleux_S/0/1/0/all/0/1\">S&#xe9;bastien Bougleux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yger_F/0/1/0/all/0/1\">Florian Yger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blumenthal_D/0/1/0/all/0/1\">David B. Blumenthal</a>",
          "description": "The inference of minimum spanning arborescences within a set of objects is a\ngeneral problem which translates into numerous application-specific\nunsupervised learning tasks. We introduce a unified and generic structure\ncalled edit arborescence that relies on edit paths between data in a\ncollection, as well as the Min Edit Arborescence Problem, which asks for an\nedit arborescence that minimizes the sum of costs of its inner edit paths.\nThrough the use of suitable cost functions, this generic framework allows to\nmodel a variety of problems. In particular, we show that by introducing\nencoding size preserving edit costs, it can be used as an efficient method for\ncompressing collections of labeled graphs. Experiments on various graph\ndatasets, with comparisons to standard compression tools, show the potential of\nour method.",
          "link": "http://arxiv.org/abs/2107.14525",
          "publishedOn": "2021-08-02T01:58:23.872Z",
          "wordCount": 585,
          "title": "The Minimum Edit Arborescence Problem and Its Use in Compressing Graph Collections [Extended Version]. (arXiv:2107.14525v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poliarnyi_N/0/1/0/all/0/1\">Nikolai Poliarnyi</a>",
          "description": "We present an out-of-core variational approach for surface reconstruction\nfrom a set of aligned depth maps. Input depth maps are supposed to be\nreconstructed from regular photos or/and can be a representation of terrestrial\nLIDAR point clouds. Our approach is based on surface reconstruction via total\ngeneralized variation minimization ($TGV$) because of its strong\nvisibility-based noise-filtering properties and GPU-friendliness. Our main\ncontribution is an out-of-core OpenCL-accelerated adaptation of this numerical\nalgorithm which can handle arbitrarily large real-world scenes with scale\ndiversity.",
          "link": "http://arxiv.org/abs/2107.14790",
          "publishedOn": "2021-08-02T01:58:23.865Z",
          "wordCount": 535,
          "title": "Out-of-Core Surface Reconstruction via Global $TGV$ Minimization. (arXiv:2107.14790v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soutif__Cormerais_A/0/1/0/all/0/1\">Albin Soutif--Cormerais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masana_M/0/1/0/all/0/1\">Marc Masana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost Van de Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1\">Bart&#x142;omiej Twardowski</a>",
          "description": "In class-incremental learning, an agent with limited resources needs to learn\na sequence of classification tasks, forming an ever growing classification\nproblem, with the constraint of not being able to access data from previous\ntasks. The main difference with task-incremental learning, where a task-ID is\navailable at inference time, is that the learner also needs to perform\ncross-task discrimination, i.e. distinguish between classes that have not been\nseen together. Approaches to tackle this problem are numerous and mostly make\nuse of an external memory (buffer) of non-negligible size. In this paper, we\nablate the learning of cross-task features and study its influence on the\nperformance of basic replay strategies used for class-IL. We also define a new\nforgetting measure for class-incremental learning, and see that forgetting is\nnot the principal cause of low performance. Our experimental results show that\nfuture algorithms for class-incremental learning should not only prevent\nforgetting, but also aim to improve the quality of the cross-task features, and\nthe knowledge transfer between tasks. This is especially important when tasks\ncontain limited amount of data.",
          "link": "http://arxiv.org/abs/2106.11930",
          "publishedOn": "2021-08-02T01:58:23.859Z",
          "wordCount": 651,
          "title": "On the importance of cross-task features for class-incremental learning. (arXiv:2106.11930v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_V/0/1/0/all/0/1\">Vineet Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cech_J/0/1/0/all/0/1\">Jan Cech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horaud_R/0/1/0/all/0/1\">Radu Horaud</a>",
          "description": "The combination of range sensors with color cameras can be very useful for\nrobot navigation, semantic perception, manipulation, and telepresence. Several\nmethods of combining range- and color-data have been investigated and\nsuccessfully used in various robotic applications. Most of these systems suffer\nfrom the problems of noise in the range-data and resolution mismatch between\nthe range sensor and the color cameras, since the resolution of current range\nsensors is much less than the resolution of color cameras. High-resolution\ndepth maps can be obtained using stereo matching, but this often fails to\nconstruct accurate depth maps of weakly/repetitively textured scenes, or if the\nscene exhibits complex self-occlusions. Range sensors provide coarse depth\ninformation regardless of presence/absence of texture. The use of a calibrated\nsystem, composed of a time-of-flight (TOF) camera and of a stereoscopic camera\npair, allows data fusion thus overcoming the weaknesses of both individual\nsensors. We propose a novel TOF-stereo fusion method based on an efficient\nseed-growing algorithm which uses the TOF data projected onto the stereo image\npair as an initial set of correspondences. These initial \"seeds\" are then\npropagated based on a Bayesian model which combines an image similarity score\nwith rough depth priors computed from the low-resolution range data. The\noverall result is a dense and accurate depth map at the resolution of the color\ncameras at hand. We show that the proposed algorithm outperforms 2D image-based\nstereo algorithms and that the results are of higher resolution than\noff-the-shelf color-range sensors, e.g., Kinect. Moreover, the algorithm\npotentially exhibits real-time performance on a single CPU.",
          "link": "http://arxiv.org/abs/2107.14688",
          "publishedOn": "2021-08-02T01:58:23.851Z",
          "wordCount": 701,
          "title": "High-Resolution Depth Maps Based on TOF-Stereo Fusion. (arXiv:2107.14688v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1911.12377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Landi_F/0/1/0/all/0/1\">Federico Landi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corsini_M/0/1/0/all/0/1\">Massimiliano Corsini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Vision-and-Language Navigation (VLN) is a challenging task in which an agent\nneeds to follow a language-specified path to reach a target destination. The\ngoal gets even harder as the actions available to the agent get simpler and\nmove towards low-level, atomic interactions with the environment. This setting\ntakes the name of low-level VLN. In this paper, we strive for the creation of\nan agent able to tackle three key issues: multi-modality, long-term\ndependencies, and adaptability towards different locomotive settings. To that\nend, we devise \"Perceive, Transform, and Act\" (PTA): a fully-attentive VLN\narchitecture that leaves the recurrent approach behind and the first\nTransformer-like architecture incorporating three different modalities -\nnatural language, images, and low-level actions for the agent control. In\nparticular, we adopt an early fusion strategy to merge lingual and visual\ninformation efficiently in our encoder. We then propose to refine the decoding\nphase with a late fusion extension between the agent's history of actions and\nthe perceptual modalities. We experimentally validate our model on two\ndatasets: PTA achieves promising results in low-level VLN on R2R and achieves\ngood performance in the recently proposed R4R benchmark. Our code is publicly\navailable at https://github.com/aimagelab/perceive-transform-and-act.",
          "link": "http://arxiv.org/abs/1911.12377",
          "publishedOn": "2021-08-02T01:58:23.845Z",
          "wordCount": 687,
          "title": "Multimodal Attention Networks for Low-Level Vision-and-Language Navigation. (arXiv:1911.12377v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14599",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hengli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1\">Peide Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "Freespace detection is a fundamental component of autonomous driving\nperception. Recently, deep convolutional neural networks (DCNNs) have achieved\nimpressive performance for this task. In particular, SNE-RoadSeg, our\npreviously proposed method based on a surface normal estimator (SNE) and a\ndata-fusion DCNN (RoadSeg), has achieved impressive performance in freespace\ndetection. However, SNE-RoadSeg is computationally intensive, and it is\ndifficult to execute in real time. To address this problem, we introduce\nSNE-RoadSeg+, an upgraded version of SNE-RoadSeg. SNE-RoadSeg+ consists of 1)\nSNE+, a module for more accurate surface normal estimation, and 2) RoadSeg+, a\ndata-fusion DCNN that can greatly minimize the trade-off between accuracy and\nefficiency with the use of deep supervision. Extensive experimental results\nhave demonstrated the effectiveness of our SNE+ for surface normal estimation\nand the superior performance of our SNE-RoadSeg+ over all other freespace\ndetection approaches. Specifically, our SNE-RoadSeg+ runs in real time, and\nmeanwhile, achieves the state-of-the-art performance on the KITTI road\nbenchmark. Our project page is at\nhttps://www.sne-roadseg.site/sne-roadseg-plus.",
          "link": "http://arxiv.org/abs/2107.14599",
          "publishedOn": "2021-08-02T01:58:23.822Z",
          "wordCount": 619,
          "title": "SNE-RoadSeg+: Rethinking Depth-Normal Translation and Deep Supervision for Freespace Detection. (arXiv:2107.14599v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Youjia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Taotao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minzhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Teng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minye Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>",
          "description": "Photo-realistic facial video portrait reenactment benefits virtual production\nand numerous VR/AR experiences. The task remains challenging as the portrait\nshould maintain high realism and consistency with the target environment. In\nthis paper, we present a relightable neural video portrait, a simultaneous\nrelighting and reenactment scheme that transfers the head pose and facial\nexpressions from a source actor to a portrait video of a target actor with\narbitrary new backgrounds and lighting conditions. Our approach combines 4D\nreflectance field learning, model-based facial performance capture and\ntarget-aware neural rendering. Specifically, we adopt a rendering-to-video\ntranslation network to first synthesize high-quality OLAT imagesets and alpha\nmattes from hybrid facial performance capture results. We then design a\nsemantic-aware facial normalization scheme to enable reliable explicit control\nas well as a multi-frame multi-task learning strategy to encode content,\nsegmentation and temporal information simultaneously for high-quality\nreflectance field inference. After training, our approach further enables\nphoto-realistic and controllable video portrait editing of the target\nperformer. Reliable face poses and expression editing is obtained by applying\nthe same hybrid facial capture and normalization scheme to the source video\ninput, while our explicit alpha and OLAT output enable high-quality relit and\nbackground editing. With the ability to achieve simultaneous relighting and\nreenactment, we are able to improve the realism in a variety of virtual\nproduction and video rewrite applications.",
          "link": "http://arxiv.org/abs/2107.14735",
          "publishedOn": "2021-08-02T01:58:23.815Z",
          "wordCount": 658,
          "title": "Relightable Neural Video Portrait. (arXiv:2107.14735v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiye Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wankou Yang</a>",
          "description": "Video based fall detection accuracy has been largely improved due to the\nrecent progress on deep convolutional neural networks. However, there still\nexists some challenges, such as lighting variation, complex background, which\ndegrade the accuracy and generalization ability of these approaches. Meanwhile,\nlarge computation cost limits the application of existing fall detection\napproaches. To alleviate these problems, a video based fall detection approach\nusing human poses is proposed in this paper. First, a lightweight pose\nestimator extracts 2D poses from video sequences and then 2D poses are lifted\nto 3D poses. Second, we introduce a robust fall detection network to recognize\nfall events using estimated 3D poses, which increases respective filed and\nmaintains low computation cost by dilated convolutions. The experimental\nresults show that the proposed fall detection approach achieves a high accuracy\nof 99.83% on large benchmark action recognition dataset NTU RGB+D and real-time\nperformance of 18 FPS on a non-GPU platform and 63 FPS on a GPU platform.",
          "link": "http://arxiv.org/abs/2107.14633",
          "publishedOn": "2021-08-02T01:58:23.807Z",
          "wordCount": 592,
          "title": "Video Based Fall Detection Using Human Poses. (arXiv:2107.14633v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yung-Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerg_I/0/1/0/all/0/1\">Isaac D. Gerg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monga_V/0/1/0/all/0/1\">Vishal Monga</a>",
          "description": "Deep learning has not been routinely employed for semantic segmentation of\nseabed environment for synthetic aperture sonar (SAS) imagery due to the\nimplicit need of abundant training data such methods necessitate. Abundant\ntraining data, specifically pixel-level labels for all images, is usually not\navailable for SAS imagery due to the complex logistics (e.g., diver survey,\nchase boat, precision position information) needed for obtaining accurate\nground-truth. Many hand-crafted feature based algorithms have been proposed to\nsegment SAS in an unsupervised fashion. However, there is still room for\nimprovement as the feature extraction step of these methods is fixed. In this\nwork, we present a new iterative unsupervised algorithm for learning deep\nfeatures for SAS image segmentation. Our proposed algorithm alternates between\nclustering superpixels and updating the parameters of a convolutional neural\nnetwork (CNN) so that the feature extraction for image segmentation can be\noptimized. We demonstrate the efficacy of our method on a realistic benchmark\ndataset. Our results show that the performance of our proposed method is\nconsiderably better than current state-of-the-art methods in SAS image\nsegmentation.",
          "link": "http://arxiv.org/abs/2107.14563",
          "publishedOn": "2021-08-02T01:58:23.801Z",
          "wordCount": 616,
          "title": "Iterative, Deep, and Unsupervised Synthetic Aperture Sonar Image Segmentation. (arXiv:2107.14563v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hayat_H/0/1/0/all/0/1\">Hassan Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ventura_C/0/1/0/all/0/1\">Carles Ventura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapedriza_A/0/1/0/all/0/1\">Agata Lapedriza</a>",
          "description": "Understanding the emotional impact of movies has become important for\naffective movie analysis, ranking, and indexing. Methods for recognizing evoked\nemotions are usually trained on human annotated data. Concretely, viewers watch\nvideo clips and have to manually annotate the emotions they experienced while\nwatching the videos. Then, the common practice is to aggregate the different\nannotations, by computing average scores or majority voting, and train and test\nmodels on these aggregated annotations. With this procedure a single aggregated\nevoked emotion annotation is obtained per each video. However, emotions\nexperienced while watching a video are subjective: different individuals might\nexperience different emotions. In this paper, we model the emotions evoked by\nvideos in a different manner: instead of modeling the aggregated value we\njointly model the emotions experienced by each viewer and the aggregated value\nusing a multi-task learning approach. Concretely, we propose two deep learning\narchitectures: a Single-Task (ST) architecture and a Multi-Task (MT)\narchitecture. Our results show that the MT approach can more accurately model\neach viewer and the aggregated annotation when compared to methods that are\ndirectly trained on the aggregated annotations. Furthermore, our approach\noutperforms the current state-of-the-art results on the COGNIMUSE benchmark.",
          "link": "http://arxiv.org/abs/2107.14529",
          "publishedOn": "2021-08-02T01:58:23.794Z",
          "wordCount": 643,
          "title": "Recognizing Emotions evoked by Movies using Multitask Learning. (arXiv:2107.14529v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadekar_K/0/1/0/all/0/1\">Kaustubh Sadekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1\">Ashish Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1\">Shanmuganathan Raman</a>",
          "description": "While recent learning based methods have been observed to be superior for\nseveral vision-related applications, their potential in generating artistic\neffects has not been explored much. One such interesting application is Shadow\nArt - a unique form of sculptural art where 2D shadows cast by a 3D sculpture\nproduce artistic effects. In this work, we revisit shadow art using\ndifferentiable rendering based optimization frameworks to obtain the 3D\nsculpture from a set of shadow (binary) images and their corresponding\nprojection information. Specifically, we discuss shape optimization through\nvoxel as well as mesh-based differentiable renderers. Our choice of using\ndifferentiable rendering for generating shadow art sculptures can be attributed\nto its ability to learn the underlying 3D geometry solely from image data, thus\nreducing the dependence on 3D ground truth. The qualitative and quantitative\nresults demonstrate the potential of the proposed framework in generating\ncomplex 3D sculptures that go beyond those seen in contemporary art pieces\nusing just a set of shadow images as input. Further, we demonstrate the\ngeneration of 3D sculptures to cast shadows of faces, animated movie\ncharacters, and applicability of the framework to sketch-based 3D\nreconstruction of underlying shapes.",
          "link": "http://arxiv.org/abs/2107.14539",
          "publishedOn": "2021-08-02T01:58:23.778Z",
          "wordCount": 630,
          "title": "Shadow Art Revisited: A Differentiable Rendering Based Approach. (arXiv:2107.14539v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14476",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1\">Caifeng Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouwman_R/0/1/0/all/0/1\">R. Arthur Bouwman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dekker_L/0/1/0/all/0/1\">Lukas R. C. Dekker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolen_A/0/1/0/all/0/1\">Alexander F. Kolen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+With_P/0/1/0/all/0/1\">Peter H. N. de With</a>",
          "description": "Medical instrument segmentation in 3D ultrasound is essential for\nimage-guided intervention. However, to train a successful deep neural network\nfor instrument segmentation, a large number of labeled images are required,\nwhich is expensive and time-consuming to obtain. In this article, we propose a\nsemi-supervised learning (SSL) framework for instrument segmentation in 3D US,\nwhich requires much less annotation effort than the existing methods. To\nachieve the SSL learning, a Dual-UNet is proposed to segment the instrument.\nThe Dual-UNet leverages unlabeled data using a novel hybrid loss function,\nconsisting of uncertainty and contextual constraints. Specifically, the\nuncertainty constraints leverage the uncertainty estimation of the predictions\nof the UNet, and therefore improve the unlabeled information for SSL training.\nIn addition, contextual constraints exploit the contextual information of the\ntraining images, which are used as the complementary information for voxel-wise\nuncertainty estimation. Extensive experiments on multiple ex-vivo and in-vivo\ndatasets show that our proposed method achieves Dice score of about 68.6%-69.1%\nand the inference time of about 1 sec. per volume. These results are better\nthan the state-of-the-art SSL methods and the inference time is comparable to\nthe supervised approaches.",
          "link": "http://arxiv.org/abs/2107.14476",
          "publishedOn": "2021-08-02T01:58:23.770Z",
          "wordCount": 645,
          "title": "Medical Instrument Segmentation in 3D US by Hybrid Constrained Semi-Supervised Learning. (arXiv:2107.14476v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Casamitjana_A/0/1/0/all/0/1\">Adri&#xe0; Casamitjana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1\">Matteo Mancini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iglesias_J/0/1/0/all/0/1\">Juan Eugenio Iglesias</a>",
          "description": "Nonlinear inter-modality registration is often challenging due to the lack of\nobjective functions that are good proxies for alignment. Here we propose a\nsynthesis-by-registration method to convert this problem into an easier\nintra-modality task. We introduce a registration loss for weakly supervised\nimage translation between domains that does not require perfectly aligned\ntraining data. This loss capitalises on a registration U-Net with frozen\nweights, to drive a synthesis CNN towards the desired translation. We\ncomplement this loss with a structure preserving constraint based on\ncontrastive learning, which prevents blurring and content shifts due to\noverfitting. We apply this method to the registration of histological sections\nto MRI slices, a key step in 3D histology reconstruction. Results on two\ndifferent public datasets show improvements over registration based on mutual\ninformation (13% reduction in landmark error) and synthesis-based algorithms\nsuch as CycleGAN (11% reduction), and are comparable to a registration CNN with\nlabel supervision.",
          "link": "http://arxiv.org/abs/2107.14449",
          "publishedOn": "2021-08-02T01:58:23.762Z",
          "wordCount": 610,
          "title": "Synth-by-Reg (SbR): Contrastive learning for synthesis-based registration of paired images. (arXiv:2107.14449v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14659",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Concha_A/0/1/0/all/0/1\">Alejo Concha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burri_M/0/1/0/all/0/1\">Michael Burri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briales_J/0/1/0/all/0/1\">Jes&#xfa;s Briales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forster_C/0/1/0/all/0/1\">Christian Forster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oth_L/0/1/0/all/0/1\">Luc Oth</a>",
          "description": "Mobile AR applications benefit from fast initialization to display\nworld-locked effects instantly. However, standard visual odometry or SLAM\nalgorithms require motion parallax to initialize (see Figure 1) and, therefore,\nsuffer from delayed initialization. In this paper, we present a 6-DoF monocular\nvisual odometry that initializes instantly and without motion parallax. Our\nmain contribution is a pose estimator that decouples estimating the 5-DoF\nrelative rotation and translation direction from the 1-DoF translation\nmagnitude. While scale is not observable in a monocular vision-only setting, it\nis still paramount to estimate a consistent scale over the whole trajectory\n(even if not physically accurate) to avoid AR effects moving erroneously along\ndepth. In our approach, we leverage the fact that depth errors are not\nperceivable to the user during rotation-only motion. However, as the user\nstarts translating the device, depth becomes perceivable and so does the\ncapability to estimate consistent scale. Our proposed algorithm naturally\ntransitions between these two modes. We perform extensive validations of our\ncontributions with both a publicly available dataset and synthetic data. We\nshow that the proposed pose estimator outperforms the classical approaches for\n6-DoF pose estimation used in the literature in low-parallax configurations. We\nrelease a dataset for the relative pose problem using real data to facilitate\nthe comparison with future solutions for the relative pose problem. Our\nsolution is either used as a full odometry or as a preSLAM component of any\nsupported SLAM system (ARKit, ARCore) in world-locked AR effects on platforms\nsuch as Instagram and Facebook.",
          "link": "http://arxiv.org/abs/2107.14659",
          "publishedOn": "2021-08-02T01:58:23.752Z",
          "wordCount": 692,
          "title": "Instant Visual Odometry Initialization for Mobile AR. (arXiv:2107.14659v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14611",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_H/0/1/0/all/0/1\">Haosong Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1\">Jinyu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1\">Fanghong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengguo Li</a>",
          "description": "Localizing pre-visited places during long-term simultaneous localization and\nmapping, i.e. loop closure detection (LCD), is a crucial technique to correct\naccumulated inconsistencies. As one of the most effective and efficient\nsolutions, Bag-of-Words (BoW) builds a visual vocabulary to associate features\nand then detect loops. Most existing approaches that build vocabularies\noff-line determine scales of the vocabulary by trial-and-error, which often\nresults in unreasonable feature association. Moreover, the accuracy of the\nalgorithm usually declines due to perceptual aliasing, as the BoW-based method\nignores the positions of visual features. To overcome these disadvantages, we\npropose a natural convergence criterion based on the comparison between the\nradii of nodes and the drifts of feature descriptors, which is then utilized to\nbuild the optimal vocabulary automatically. Furthermore, we present a novel\ntopological graph verification method for validating candidate loops so that\ngeometrical positions of the words can be involved with a negligible increase\nin complexity, which can significantly improve the accuracy of LCD. Experiments\non various public datasets and comparisons against several state-of-the-art\nalgorithms verify the performance of our proposed approach.",
          "link": "http://arxiv.org/abs/2107.14611",
          "publishedOn": "2021-08-02T01:58:23.746Z",
          "wordCount": 629,
          "title": "Automatic Vocabulary and Graph Verification for Accurate Loop Closure Detection. (arXiv:2107.14611v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xunlin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yangxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Minlong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>",
          "description": "Nowadays, customer's demands for E-commerce are more diversified, which\nintroduces more complications to the product retrieval industry. Previous\nmethods are either subject to single-modal input or perform supervised\nimage-level product retrieval, thus fail to accommodate real-life scenarios\nwhere enormous weakly annotated multi-modal data are present. In this paper, we\ninvestigate a more realistic setting that aims to perform weakly-supervised\nmulti-modal instance-level product retrieval among fine-grained product\ncategories. To promote the study of this challenging task, we contribute\nProduct1M, one of the largest multi-modal cosmetic datasets for real-world\ninstance-level retrieval. Notably, Product1M contains over 1 million\nimage-caption pairs and consists of two sample types, i.e., single-product and\nmulti-product samples, which encompass a wide variety of cosmetics brands. In\naddition to the great diversity, Product1M enjoys several appealing\ncharacteristics including fine-grained categories, complex combinations, and\nfuzzy correspondence that well mimic the real-world scenes. Moreover, we\npropose a novel model named Cross-modal contrAstive Product Transformer for\ninstance-level prodUct REtrieval (CAPTURE), that excels in capturing the\npotential synergy between multi-modal inputs via a hybrid-stream transformer in\na self-supervised manner.CAPTURE generates discriminative instance features via\nmasked multi-modal learning as well as cross-modal contrastive pretraining and\nit outperforms several SOTA cross-modal baselines. Extensive ablation studies\nwell demonstrate the effectiveness and the generalization capacity of our\nmodel.",
          "link": "http://arxiv.org/abs/2107.14572",
          "publishedOn": "2021-08-02T01:58:23.738Z",
          "wordCount": 657,
          "title": "Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-modal Pretraining. (arXiv:2107.14572v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yousong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chaoyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1\">Guosheng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wei Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Ming Tang</a>",
          "description": "Transformer has achieved great success in computer vision, while how to split\npatches in an image remains a problem. Existing methods usually use a\nfixed-size patch embedding which might destroy the semantics of objects. To\naddress this problem, we propose a new Deformable Patch (DePatch) module which\nlearns to adaptively split the images into patches with different positions and\nscales in a data-driven way rather than using predefined fixed patches. In this\nway, our method can well preserve the semantics in patches. The DePatch module\ncan work as a plug-and-play module, which can easily be incorporated into\ndifferent transformers to achieve an end-to-end training. We term this\nDePatch-embedded transformer as Deformable Patch-based Transformer (DPT) and\nconduct extensive evaluations of DPT on image classification and object\ndetection. Results show DPT can achieve 81.9% top-1 accuracy on ImageNet\nclassification, and 43.7% box mAP with RetinaNet, 44.3% with Mask R-CNN on\nMSCOCO object detection. Code has been made available at:\nhttps://github.com/CASIA-IVA-Lab/DPT .",
          "link": "http://arxiv.org/abs/2107.14467",
          "publishedOn": "2021-08-02T01:58:23.720Z",
          "wordCount": 616,
          "title": "DPT: Deformable Patch-based Transformer for Visual Recognition. (arXiv:2107.14467v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14531",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Araujo_R/0/1/0/all/0/1\">R. J. Ara&#xfa;jo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardoso_J/0/1/0/all/0/1\">J. S. Cardoso</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_H/0/1/0/all/0/1\">H. P. Oliveira</a>",
          "description": "Blood vessel segmentation is one of the most studied topics in computer\nvision, due to its relevance in daily clinical practice. Despite the evolution\nthe field has been facing, especially after the dawn of deep learning,\nimportant challenges are still not solved. One of them concerns the consistency\nof the topological properties of the vascular trees, given that the best\nperforming methodologies do not directly penalize mistakes such as broken\nsegments and end up producing predictions with disconnected trees. This is\nparticularly relevant in graph-like structures, such as blood vessel trees,\ngiven that it puts at risk the characterization steps that follow the\nsegmentation task. In this paper, we propose a similarity index which captures\nthe topological consistency of the predicted segmentations having as reference\nthe ground truth. We also design a novel loss function based on the\nmorphological closing operator and show how it allows to learn deep neural\nnetwork models which produce more topologically coherent masks. Our experiments\ntarget well known retinal benchmarks and a coronary angiogram database.",
          "link": "http://arxiv.org/abs/2107.14531",
          "publishedOn": "2021-08-02T01:58:23.713Z",
          "wordCount": 635,
          "title": "Topological Similarity Index and Loss Function for Blood Vessel Segmentation. (arXiv:2107.14531v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14443",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Galetto_F/0/1/0/all/0/1\">Fernando J. Galetto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_G/0/1/0/all/0/1\">Guang Deng</a>",
          "description": "The depth information is useful in many image processing applications.\nHowever, since taking a picture is a process of projection of a 3D scene onto a\n2D imaging sensor, the depth information is embedded in the image. Extracting\nthe depth information from the image is a challenging task. A guiding principle\nis that the level of blurriness due to defocus is related to the distance\nbetween the object and the focal plane. Based on this principle and the widely\nused assumption that Gaussian blur is a good model for defocus blur, we\nformulate the problem of estimating the spatially varying defocus blurriness as\na Gaussian blur classification problem. We solved the problem by training a\ndeep neural network to classify image patches into one of the 20 levels of\nblurriness. We have created a dataset of more than 500000 image patches of size\n32x32 which are used to train and test several well-known network models. We\nfind that MobileNetV2 is suitable for this application due to its low memory\nrequirement and high accuracy. The trained model is used to determine the patch\nblurriness which is then refined by applying an iterative weighted guided\nfilter. The result is a defocus map that carries the information of the degree\nof blurriness for each pixel. We compare the proposed method with\nstate-of-the-art techniques and we demonstrate its successful applications in\nadaptive image enhancement, defocus magnification, and multi-focus image\nfusion.",
          "link": "http://arxiv.org/abs/2107.14443",
          "publishedOn": "2021-08-02T01:58:23.692Z",
          "wordCount": 689,
          "title": "Single image deep defocus estimation and its applications. (arXiv:2107.14443v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung-Nghia Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huy H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1\">Isao Echizen</a>",
          "description": "The proliferation of deepfake media is raising concerns among the public and\nrelevant authorities. It has become essential to develop countermeasures\nagainst forged faces in social media. This paper presents a comprehensive study\non two new countermeasure tasks: multi-face forgery detection and segmentation\nin-the-wild. Localizing forged faces among multiple human faces in unrestricted\nnatural scenes is far more challenging than the traditional deepfake\nrecognition task. To promote these new tasks, we have created the first\nlarge-scale dataset posing a high level of challenges that is designed with\nface-wise rich annotations explicitly for face forgery detection and\nsegmentation, namely OpenForensics. With its rich annotations, our\nOpenForensics dataset has great potentials for research in both deepfake\nprevention and general human face detection. We have also developed a suite of\nbenchmarks for these tasks by conducting an extensive evaluation of\nstate-of-the-art instance detection and segmentation methods on our newly\nconstructed dataset in various scenarios. The dataset, benchmark results,\ncodes, and supplementary materials will be publicly available on our project\npage: https://sites.google.com/view/ltnghia/research/openforensics",
          "link": "http://arxiv.org/abs/2107.14480",
          "publishedOn": "2021-08-02T01:58:23.652Z",
          "wordCount": 622,
          "title": "OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery Detection And Segmentation In-The-Wild. (arXiv:2107.14480v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yongxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianlei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiaolin Qin</a>",
          "description": "Nowadays, plenty of deep learning technologies are being applied to all\naspects of autonomous driving with promising results. Among them, object\ndetection is the key to improve the ability of an autonomous agent to perceive\nits environment so that it can (re)act. However, previous vision-based object\ndetectors cannot achieve satisfactory performance under real-time driving\nscenarios. To remedy this, we present the real-time steaming perception system\nin this paper, which is also the 2nd Place solution of Streaming Perception\nChallenge (Workshop on Autonomous Driving at CVPR 2021) for the detection-only\ntrack. Unlike traditional object detection challenges, which focus mainly on\nthe absolute performance, streaming perception task requires achieving a\nbalance of accuracy and latency, which is crucial for real-time autonomous\ndriving. We adopt YOLOv5 as our basic framework, data augmentation,\nBag-of-Freebies, and Transformer are adopted to improve streaming object\ndetection performance with negligible extra inference cost. On the Argoverse-HD\ntest set, our method achieves 33.2 streaming AP (34.6 streaming AP verified by\nthe organizer) under the required hardware. Its performance significantly\nsurpasses the fixed baseline of 13.6 (host team), demonstrating the\npotentiality of application.",
          "link": "http://arxiv.org/abs/2107.14388",
          "publishedOn": "2021-08-02T01:58:23.644Z",
          "wordCount": 617,
          "title": "Real-time Streaming Perception System for Autonomous Driving. (arXiv:2107.14388v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14425",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaotian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_H/0/1/0/all/0/1\">Hanling Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Ling Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>",
          "description": "There has been a recent surge of research interest in attacking the problem\nof social relation inference based on images. Existing works classify social\nrelations mainly by creating complicated graphs of human interactions, or\nlearning the foreground and/or background information of persons and objects,\nbut ignore holistic scene context. The holistic scene refers to the\nfunctionality of a place in images, such as dinning room, playground and\noffice. In this paper, by mimicking human understanding on images, we propose\nan approach of \\textbf{PR}actical \\textbf{I}nference in \\textbf{S}ocial\nr\\textbf{E}lation (PRISE), which concisely learns interactive features of\npersons and discriminative features of holistic scenes. Technically, we develop\na simple and fast relational graph convolutional network to capture interactive\nfeatures of all persons in one image. To learn the holistic scene feature, we\nelaborately design a contrastive learning task based on image scene\nclassification. To further boost the performance in social relation inference,\nwe collect and distribute a new large-scale dataset, which consists of about\n240 thousand unlabeled images. The extensive experimental results show that our\nnovel learning framework significantly beats the state-of-the-art methods,\ne.g., PRISE achieves 6.8$\\%$ improvement for domain classification in PIPA\ndataset.",
          "link": "http://arxiv.org/abs/2107.14425",
          "publishedOn": "2021-08-02T01:58:23.636Z",
          "wordCount": 642,
          "title": "Enhancing Social Relation Inference with Concise Interaction Graph and Discriminative Scene Representation. (arXiv:2107.14425v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14519",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zongben Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>",
          "description": "It has been shown that equivariant convolution is very helpful for many types\nof computer vision tasks. Recently, the 2D filter parametrization technique\nplays an important role when designing equivariant convolutions. However, the\ncurrent filter parametrization method still has its evident drawbacks, where\nthe most critical one lies in the accuracy problem of filter representation.\nAgainst this issue, in this paper we modify the classical Fourier series\nexpansion for 2D filters, and propose a new set of atomic basis functions for\nfilter parametrization. The proposed filter parametrization method not only\nfinely represents 2D filters with zero error when the filter is not rotated,\nbut also substantially alleviates the fence-effect-caused quality degradation\nwhen the filter is rotated. Accordingly, we construct a new equivariant\nconvolution method based on the proposed filter parametrization method, named\nF-Conv. We prove that the equivariance of the proposed F-Conv is exact in the\ncontinuous domain, which becomes approximate only after discretization.\nExtensive experiments show the superiority of the proposed method.\nParticularly, we adopt rotation equivariant convolution methods to image\nsuper-resolution task, and F-Conv evidently outperforms previous filter\nparametrization based method in this task, reflecting its intrinsic capability\nof faithfully preserving rotation symmetries in local image features.",
          "link": "http://arxiv.org/abs/2107.14519",
          "publishedOn": "2021-08-02T01:58:23.629Z",
          "wordCount": 642,
          "title": "Fourier Series Expansion Based Filter Parametrization for Equivariant Convolutions. (arXiv:2107.14519v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leroy_R/0/1/0/all/0/1\">R&#xe9;my Leroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trouve_Peloux_P/0/1/0/all/0/1\">Pauline Trouv&#xe9;-Peloux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Champagnat_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Champagnat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saux_B/0/1/0/all/0/1\">Bertrand Le Saux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_M/0/1/0/all/0/1\">Marcela Carvalho</a>",
          "description": "Good quality reconstruction and comprehension of a scene rely on 3D\nestimation methods. The 3D information was usually obtained from images by\nstereo-photogrammetry, but deep learning has recently provided us with\nexcellent results for monocular depth estimation. Building up a sufficiently\nlarge and rich training dataset to achieve these results requires onerous\nprocessing. In this paper, we address the problem of learning outdoor 3D point\ncloud from monocular data using a sparse ground-truth dataset. We propose\nPix2Point, a deep learning-based approach for monocular 3D point cloud\nprediction, able to deal with complete and challenging outdoor scenes. Our\nmethod relies on a 2D-3D hybrid neural network architecture, and a supervised\nend-to-end minimisation of an optimal transport divergence between point\nclouds. We show that, when trained on sparse point clouds, our simple promising\napproach achieves a better coverage of 3D outdoor scenes than efficient\nmonocular depth methods.",
          "link": "http://arxiv.org/abs/2107.14498",
          "publishedOn": "2021-08-02T01:58:23.621Z",
          "wordCount": 610,
          "title": "Pix2Point: Learning Outdoor 3D Using Sparse Point Clouds and Optimal Transport. (arXiv:2107.14498v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiajun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanyong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>",
          "description": "As an emerging data modal with precise distance sensing, LiDAR point clouds\nhave been placed great expectations on 3D scene understanding. However, point\nclouds are always sparsely distributed in the 3D space, and with unstructured\nstorage, which makes it difficult to represent them for effective 3D object\ndetection. To this end, in this work, we regard point clouds as hollow-3D data\nand propose a new architecture, namely Hallucinated Hollow-3D R-CNN\n($\\text{H}^2$3D R-CNN), to address the problem of 3D object detection. In our\napproach, we first extract the multi-view features by sequentially projecting\nthe point clouds into the perspective view and the bird-eye view. Then, we\nhallucinate the 3D representation by a novel bilaterally guided multi-view\nfusion block. Finally, the 3D objects are detected via a box refinement module\nwith a novel Hierarchical Voxel RoI Pooling operation. The proposed\n$\\text{H}^2$3D R-CNN provides a new angle to take full advantage of\ncomplementary information in the perspective view and the bird-eye view with an\nefficient framework. We evaluate our approach on the public KITTI Dataset and\nWaymo Open Dataset. Extensive experiments demonstrate the superiority of our\nmethod over the state-of-the-art algorithms with respect to both effectiveness\nand efficiency. The code will be made available at\n\\url{https://github.com/djiajunustc/H-23D_R-CNN}.",
          "link": "http://arxiv.org/abs/2107.14391",
          "publishedOn": "2021-08-02T01:58:23.612Z",
          "wordCount": 656,
          "title": "From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection. (arXiv:2107.14391v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>",
          "description": "Semantic segmentation requires per-pixel prediction for a given image.\nTypically, the output resolution of a segmentation network is severely reduced\ndue to the downsampling operations in the CNN backbone. Most previous methods\nemploy upsampling decoders to recover the spatial resolution. Various decoders\nwere designed in the literature. Here, we propose a novel decoder, termed\ndynamic neural representational decoder (NRD), which is simple yet\nsignificantly more efficient. As each location on the encoder's output\ncorresponds to a local patch of the semantic labels, in this work, we represent\nthese local patches of labels with compact neural networks. This neural\nrepresentation enables our decoder to leverage the smoothness prior in the\nsemantic label space, and thus makes our decoder more efficient. Furthermore,\nthese neural representations are dynamically generated and conditioned on the\noutputs of the encoder networks. The desired semantic labels can be efficiently\ndecoded from the neural representations, resulting in high-resolution semantic\nsegmentation predictions. We empirically show that our proposed decoder can\noutperform the decoder in DeeplabV3+ with only 30% computational complexity,\nand achieve competitive performance with the methods using dilated encoders\nwith only 15% computation. Experiments on the Cityscapes, ADE20K, and PASCAL\nContext datasets demonstrate the effectiveness and efficiency of our proposed\nmethod.",
          "link": "http://arxiv.org/abs/2107.14428",
          "publishedOn": "2021-08-02T01:58:23.605Z",
          "wordCount": 638,
          "title": "Dynamic Neural Representational Decoders for High-Resolution Semantic Segmentation. (arXiv:2107.14428v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guangze Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>",
          "description": "Recent years have witnessed the fast evolution and promising performance of\nthe convolutional neural network (CNN)-based trackers, which aim at imitating\nbiological visual systems. However, current CNN-based trackers can hardly\ngeneralize well to low-light scenes that are commonly lacked in the existing\ntraining set. In indistinguishable night scenarios frequently encountered in\nunmanned aerial vehicle (UAV) tracking-based applications, the robustness of\nthe state-of-the-art (SOTA) trackers drops significantly. To facilitate aerial\ntracking in the dark through a general fashion, this work proposes a low-light\nimage enhancer namely DarkLighter, which dedicates to alleviate the impact of\npoor illumination and noise iteratively. A lightweight map estimation network,\ni.e., ME-Net, is trained to efficiently estimate illumination maps and noise\nmaps jointly. Experiments are conducted with several SOTA trackers on numerous\nUAV dark tracking scenes. Exhaustive evaluations demonstrate the reliability\nand universality of DarkLighter, with high efficiency. Moreover, DarkLighter\nhas further been implemented on a typical UAV system. Real-world tests at night\nscenes have verified its practicability and dependability.",
          "link": "http://arxiv.org/abs/2107.14389",
          "publishedOn": "2021-08-02T01:58:23.597Z",
          "wordCount": 609,
          "title": "DarkLighter: Light Up the Darkness for UAV Tracking. (arXiv:2107.14389v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14292",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shafique_A/0/1/0/all/0/1\">Abubakr Shafique</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Babaie_M/0/1/0/all/0/1\">Morteza Babaie</a> (1 and 3), <a href=\"http://arxiv.org/find/eess/1/au:+Sajadi_M/0/1/0/all/0/1\">Mahjabin Sajadi</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Batten_A/0/1/0/all/0/1\">Adrian Batten</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Skdar_S/0/1/0/all/0/1\">Soma Skdar</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Tizhoosh_H/0/1/0/all/0/1\">H.R. Tizhoosh</a> (1 and 3) ((1) Kimia Lab, University of Waterloo, Waterloo, ON, Canada., (2) Department of Pathology, Grand River Hospital, Kitchener, ON, Canada., and (3) Vector Institute, MaRS Centre, Toronto, Canada.)",
          "description": "Joint analysis of multiple biomarker images and tissue morphology is\nimportant for disease diagnosis, treatment planning and drug development. It\nrequires cross-staining comparison among Whole Slide Images (WSIs) of\nimmuno-histochemical and hematoxylin and eosin (H&E) microscopic slides.\nHowever, automatic, and fast cross-staining alignment of enormous gigapixel\nWSIs at single-cell precision is challenging. In addition to morphological\ndeformations introduced during slide preparation, there are large variations in\ncell appearance and tissue morphology across different staining. In this paper,\nwe propose a two-step automatic feature-based cross-staining WSI alignment to\nassist localization of even tiny metastatic foci in the assessment of lymph\nnode. Image pairs were aligned allowing for translation, rotation, and scaling.\nThe registration was performed automatically by first detecting landmarks in\nboth images, using the scale-invariant image transform (SIFT), followed by the\nfast sample consensus (FSC) protocol for finding point correspondences and\nfinally aligned the images. The Registration results were evaluated using both\nvisual and quantitative criteria using the Jaccard index. The average Jaccard\nsimilarity index of the results produced by the proposed system is 0.942 when\ncompared with the manual registration.",
          "link": "http://arxiv.org/abs/2107.14292",
          "publishedOn": "2021-08-02T01:58:23.581Z",
          "wordCount": 686,
          "title": "Automatic Multi-Stain Registration of Whole Slide Images in Histopathology. (arXiv:2107.14292v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tongzhou Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1\">Fanbo Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Derek Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Stone Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhiwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>",
          "description": "Learning generalizable manipulation skills is central for robots to achieve\ntask automation in environments with endless scene and object variations.\nHowever, existing robot learning environments are limited in both scale and\ndiversity of 3D assets (especially of articulated objects), making it difficult\nto train and evaluate the generalization ability of agents over novel objects.\nIn this work, we focus on object-level generalization and propose SAPIEN\nManipulation Skill Benchmark (abbreviated as ManiSkill), a large-scale\nlearning-from-demonstrations benchmark for articulated object manipulation with\nvisual input (point cloud and image). ManiSkill supports object-level\nvariations by utilizing a rich and diverse set of articulated objects, and each\ntask is carefully designed for learning manipulations on a single category of\nobjects. We equip ManiSkill with high-quality demonstrations to facilitate\nlearning-from-demonstrations approaches and perform evaluations on common\nbaseline algorithms. We believe ManiSkill can encourage the robot learning\ncommunity to explore more on learning generalizable object manipulation skills.",
          "link": "http://arxiv.org/abs/2107.14483",
          "publishedOn": "2021-08-02T01:58:23.571Z",
          "wordCount": 606,
          "title": "ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills. (arXiv:2107.14483v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruihuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianzhong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuaijun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>",
          "description": "Most existing domain adaptation methods focus on adaptation from only one\nsource domain, however, in practice there are a number of relevant sources that\ncould be leveraged to help improve performance on target domain. We propose a\nnovel approach named T-SVDNet to address the task of Multi-source Domain\nAdaptation (MDA), which is featured by incorporating Tensor Singular Value\nDecomposition (T-SVD) into a neural network's training pipeline. Overall,\nhigh-order correlations among multiple domains and categories are fully\nexplored so as to better bridge the domain gap. Specifically, we impose\nTensor-Low-Rank (TLR) constraint on a tensor obtained by stacking up a group of\nprototypical similarity matrices, aiming at capturing consistent data structure\nacross different domains. Furthermore, to avoid negative transfer brought by\nnoisy source data, we propose a novel uncertainty-aware weighting strategy to\nadaptively assign weights to different source domains and samples based on the\nresult of uncertainty estimation. Extensive experiments conducted on public\nbenchmarks demonstrate the superiority of our model in addressing the task of\nMDA compared to state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.14447",
          "publishedOn": "2021-08-02T01:58:23.561Z",
          "wordCount": 623,
          "title": "T-SVDNet: Exploring High-Order Prototypical Correlations for Multi-Source Domain Adaptation. (arXiv:2107.14447v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hanxiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qingnan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Youyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">C. Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>",
          "description": "We describe an unsupervised domain adaptation method for image content shift\ncaused by viewpoint changes for a semantic segmentation task. Most existing\nmethods perform domain alignment in a shared space and assume that the mapping\nfrom the aligned space to the output is transferable. However, the novel\ncontent induced by viewpoint changes may nullify such a space for effective\nalignments, thus resulting in negative adaptation. Our method works without\naligning any statistics of the images between the two domains. Instead, it\nutilizes a view transformation network trained only on color images to\nhallucinate the semantic images for the target. Despite the lack of\nsupervision, the view transformation network can still generalize to semantic\nimages thanks to the inductive bias introduced by the attention mechanism.\nFurthermore, to resolve ambiguities in converting the semantic images to\nsemantic labels, we treat the view transformation network as a functional\nrepresentation of an unknown mapping implied by the color images and propose\nfunctional label hallucination to generate pseudo-labels in the target domain.\nOur method surpasses baselines built on state-of-the-art correspondence\nestimation and view synthesis methods. Moreover, it outperforms the\nstate-of-the-art unsupervised domain adaptation methods that utilize\nself-training and adversarial domain alignment. Our code and dataset will be\nmade publicly available.",
          "link": "http://arxiv.org/abs/2107.14285",
          "publishedOn": "2021-08-02T01:58:23.521Z",
          "wordCount": 658,
          "title": "ADeLA: Automatic Dense Labeling with Attention for Viewpoint Adaptation in Semantic Segmentation. (arXiv:2107.14285v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Margapuri_V/0/1/0/all/0/1\">Venkat Margapuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Penumajji_N/0/1/0/all/0/1\">Niketa Penumajji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neilsen_M/0/1/0/all/0/1\">Mitchell Neilsen</a>",
          "description": "Smart environments are environments where digital devices are connected to\neach other over the Internet and operate in sync. Security is of paramount\nimportance in such environments. This paper addresses aspects of authorized\naccess and intruder detection for smart environments. Proposed is PiBase, an\nInternet of Things (IoT)-based app that aids in detecting intruders and\nproviding security. The hardware for the application consists of a Raspberry\nPi, a PIR motion sensor to detect motion from infrared radiation in the\nenvironment, an Android mobile phone and a camera. The software for the\napplication is written in Java, Python and NodeJS. The PIR sensor and Pi camera\nmodule connected to the Raspberry Pi aid in detecting human intrusion. Machine\nlearning algorithms, namely Haar-feature based cascade classifiers and Linear\nBinary Pattern Histograms (LBPH), are used for face detection and face\nrecognition, respectively. The app lets the user create a list of non-intruders\nand anyone that is not on the list is identified as an intruder. The app alerts\nthe user only in the event of an intrusion by using the Google Firebase Cloud\nMessaging service to trigger a notification to the app. The user may choose to\nadd the detected intruder to the list of non-intruders through the app to avoid\nfurther detections as intruder. Face detection by the Haar Cascade algorithm\nyields a recall of 94.6%. Thus, the system is both highly effective and\nrelatively low cost.",
          "link": "http://arxiv.org/abs/2107.14325",
          "publishedOn": "2021-08-02T01:58:23.489Z",
          "wordCount": 682,
          "title": "PiBase: An IoT-based Security System using Raspberry Pi and Google Firebase. (arXiv:2107.14325v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jingwei Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingjing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunmao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>",
          "description": "Automatic facial action unit (AU) recognition is a challenging task due to\nthe scarcity of manual annotations. To alleviate this problem, a large amount\nof efforts has been dedicated to exploiting various methods which leverage\nnumerous unlabeled data. However, many aspects with regard to some unique\nproperties of AUs, such as the regional and relational characteristics, are not\nsufficiently explored in previous works. Motivated by this, we take the AU\nproperties into consideration and propose two auxiliary AU related tasks to\nbridge the gap between limited annotations and the model performance in a\nself-supervised manner via the unlabeled data. Specifically, to enhance the\ndiscrimination of regional features with AU relation embedding, we design a\ntask of RoI inpainting to recover the randomly cropped AU patches. Meanwhile, a\nsingle image based optical flow estimation task is proposed to leverage the\ndynamic change of facial muscles and encode the motion information into the\nglobal feature representation. Based on these two self-supervised auxiliary\ntasks, local features, mutual relation and motion cues of AUs are better\ncaptured in the backbone network with the proposed regional and temporal based\nauxiliary task learning (RTATL) framework. Extensive experiments on BP4D and\nDISFA demonstrate the superiority of our method and new state-of-the-art\nperformances are achieved.",
          "link": "http://arxiv.org/abs/2107.14399",
          "publishedOn": "2021-08-02T01:58:23.463Z",
          "wordCount": 674,
          "title": "Self-Supervised Regional and Temporal Auxiliary Tasks for Facial Action Unit Recognition. (arXiv:2107.14399v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1\">Runzhou Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuangzhuang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yihan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wenxin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Li Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>",
          "description": "In this report, we introduce our winning solution to the Real-time 3D\nDetection and also the \"Most Efficient Model\" in the Waymo Open Dataset\nChallenges at CVPR 2021. Extended from our last year's award-winning model\nAFDet, we have made a handful of modifications to the base model, to improve\nthe accuracy and at the same time to greatly reduce the latency. The modified\nmodel, named as AFDetV2, is featured with a lite 3D Feature Extractor, an\nimproved RPN with extended receptive field and an added sub-head that produces\nan IoU-aware confidence score. These model enhancements, together with enriched\ndata augmentation, stochastic weights averaging, and a GPU-based implementation\nof voxelization, lead to a winning accuracy of 73.12 mAPH/L2 for our AFDetV2\nwith a latency of 60.06 ms, and an accuracy of 72.57 mAPH/L2 for our\nAFDetV2-base, entitled as the \"Most Efficient Model\" by the challenge sponsor,\nwith a winning latency of 55.86 ms.",
          "link": "http://arxiv.org/abs/2107.14342",
          "publishedOn": "2021-08-02T01:58:23.457Z",
          "wordCount": 592,
          "title": "Real-Time Anchor-Free Single-Stage 3D Detection with IoU-Awareness. (arXiv:2107.14342v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Safarani_S/0/1/0/all/0/1\">Shahd Safarani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nix_A/0/1/0/all/0/1\">Arne Nix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willeke_K/0/1/0/all/0/1\">Konstantin Willeke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadena_S/0/1/0/all/0/1\">Santiago A. Cadena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Restivo_K/0/1/0/all/0/1\">Kelli Restivo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denfield_G/0/1/0/all/0/1\">George Denfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_A/0/1/0/all/0/1\">Andreas S. Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinz_F/0/1/0/all/0/1\">Fabian H. Sinz</a>",
          "description": "Deep neural networks set the state-of-the-art across many tasks in computer\nvision, but their generalization ability to image distortions is surprisingly\nfragile. In contrast, the mammalian visual system is robust to a wide range of\nperturbations. Recent work suggests that this generalization ability can be\nexplained by useful inductive biases encoded in the representations of visual\nstimuli throughout the visual cortex. Here, we successfully leveraged these\ninductive biases with a multi-task learning approach: we jointly trained a deep\nnetwork to perform image classification and to predict neural activity in\nmacaque primary visual cortex (V1). We measured the out-of-distribution\ngeneralization abilities of our network by testing its robustness to image\ndistortions. We found that co-training on monkey V1 data leads to increased\nrobustness despite the absence of those distortions during training.\nAdditionally, we showed that our network's robustness is very close to that of\nan Oracle network where parts of the architecture are directly trained on noisy\nimages. Our results also demonstrated that the network's representations become\nmore brain-like as their robustness improves. Using a novel constrained\nreconstruction analysis, we investigated what makes our brain-regularized\nnetwork more robust. We found that our co-trained network is more sensitive to\ncontent than noise when compared to a Baseline network that we trained for\nimage classification alone. Using DeepGaze-predicted saliency maps for ImageNet\nimages, we found that our monkey co-trained network tends to be more sensitive\nto salient regions in a scene, reminiscent of existing theories on the role of\nV1 in the detection of object borders and bottom-up saliency. Overall, our work\nexpands the promising research avenue of transferring inductive biases from the\nbrain, and provides a novel analysis of the effects of our transfer.",
          "link": "http://arxiv.org/abs/2107.14344",
          "publishedOn": "2021-08-02T01:58:23.450Z",
          "wordCount": 745,
          "title": "Towards robust vision by multi-task learning on monkey visual cortex. (arXiv:2107.14344v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1\">Tianxiang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuchen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>",
          "description": "The existence of redundancy in Convolutional Neural Networks (CNNs) enables\nus to remove some filters/channels with acceptable performance drops. However,\nthe training objective of CNNs usually tends to minimize an accuracy-related\nloss function without any attention paid to the redundancy, making the\nredundancy distribute randomly on all the filters, such that removing any of\nthem may trigger information loss and accuracy drop, necessitating a following\nfinetuning step for recovery. In this paper, we propose to manipulate the\nredundancy during training to facilitate network pruning. To this end, we\npropose a novel Centripetal SGD (C-SGD) to make some filters identical,\nresulting in ideal redundancy patterns, as such filters become purely redundant\ndue to their duplicates; hence removing them does not harm the network. As\nshown on CIFAR and ImageNet, C-SGD delivers better performance because the\nredundancy is better organized, compared to the existing methods. The\nefficiency also characterizes C-SGD because it is as fast as regular SGD,\nrequires no finetuning, and can be conducted simultaneously on all the layers\neven in very deep CNNs. Besides, C-SGD can improve the accuracy of CNNs by\nfirst training a model with the same architecture but wider layers then\nsqueezing it into the original width.",
          "link": "http://arxiv.org/abs/2107.14444",
          "publishedOn": "2021-08-02T01:58:23.439Z",
          "wordCount": 666,
          "title": "Manipulating Identical Filter Redundancy for Efficient Pruning on Deep and Complicated CNN. (arXiv:2107.14444v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Winston Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_T/0/1/0/all/0/1\">Tejas Shah</a>",
          "description": "Images acquired by computer vision systems under low light conditions have\nmultiple characteristics like high noise, lousy illumination, reflectance, and\nbad contrast, which make object detection tasks difficult. Much work has been\ndone to enhance images using various pixel manipulation techniques, as well as\ndeep neural networks - some focused on improving the illumination, while some\non reducing the noise. Similarly, considerable research has been done in object\ndetection neural network models. In our work, we break down the problem into\ntwo phases: 1)First, we explore which image enhancement algorithm is more\nsuited for object detection tasks, where accurate feature retrieval is more\nimportant than good image quality. Specifically, we look at basic histogram\nequalization techniques and unpaired image translation techniques. 2)In the\nsecond phase, we explore different object detection models that can be applied\nto the enhanced image. We conclude by comparing all results, calculating mean\naverage precisions (mAP), and giving some directions for future work.",
          "link": "http://arxiv.org/abs/2107.14382",
          "publishedOn": "2021-08-02T01:58:23.426Z",
          "wordCount": 587,
          "title": "Exploring Low-light Object Detection Techniques. (arXiv:2107.14382v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14287",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shilin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hieu Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>",
          "description": "While single image shadow detection has been improving rapidly in recent\nyears, video shadow detection remains a challenging task due to data scarcity\nand the difficulty in modelling temporal consistency. The current video shadow\ndetection method achieves this goal via co-attention, which mostly exploits\ninformation that is temporally coherent but is not robust in detecting moving\nshadows and small shadow regions. In this paper, we propose a simple but\npowerful method to better aggregate information temporally. We use an optical\nflow based warping module to align and then combine features between frames. We\napply this warping module across multiple deep-network layers to retrieve\ninformation from neighboring frames including both local details and high-level\nsemantic information. We train and test our framework on the ViSha dataset.\nExperimental results show that our model outperforms the state-of-the-art video\nshadow detection method by 28%, reducing BER from 16.7 to 12.0.",
          "link": "http://arxiv.org/abs/2107.14287",
          "publishedOn": "2021-08-02T01:58:23.418Z",
          "wordCount": 577,
          "title": "Temporal Feature Warping for Video Shadow Detection. (arXiv:2107.14287v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>",
          "description": "Correspondence selection aims to correctly select the consistent matches\n(inliers) from an initial set of putative correspondences. The selection is\nchallenging since putative matches are typically extremely unbalanced, largely\ndominated by outliers, and the random distribution of such outliers further\ncomplicates the learning process for learning-based methods. To address this\nissue, we propose to progressively prune the correspondences via a\nlocal-to-global consensus learning procedure. We introduce a ``pruning'' block\nthat lets us identify reliable candidates among the initial matches according\nto consensus scores estimated using local-to-global dynamic graphs. We then\nachieve progressive pruning by stacking multiple pruning blocks sequentially.\nOur method outperforms state-of-the-arts on robust line fitting, camera pose\nestimation and retrieval-based image localization benchmarks by significant\nmargins and shows promising generalization ability to different datasets and\ndetector/descriptor combinations.",
          "link": "http://arxiv.org/abs/2101.00591",
          "publishedOn": "2021-07-30T02:13:30.129Z",
          "wordCount": 607,
          "title": "Progressive Correspondence Pruning by Consensus Learning. (arXiv:2101.00591v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valassakis_E/0/1/0/all/0/1\">Eugene Valassakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palo_N/0/1/0/all/0/1\">Norman Di Palo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1\">Edward Johns</a>",
          "description": "In this paper, we study the problem of zero-shot sim-to-real when the task\nrequires both highly precise control with sub-millimetre error tolerance, and\nwide task space generalisation. Our framework involves a coarse-to-fine\ncontroller, where trajectories begin with classical motion planning using\nICP-based pose estimation, and transition to a learned end-to-end controller\nwhich maps images to actions and is trained in simulation with domain\nrandomisation. In this way, we achieve precise control whilst also generalising\nthe controller across wide task spaces, and keeping the robustness of\nvision-based, end-to-end control. Real-world experiments on a range of\ndifferent tasks show that, by exploiting the best of both worlds, our framework\nsignificantly outperforms purely motion planning methods, and purely\nlearning-based methods. Furthermore, we answer a range of questions on best\npractices for precise sim-to-real transfer, such as how different image sensor\nmodalities and image feature representations perform.",
          "link": "http://arxiv.org/abs/2105.11283",
          "publishedOn": "2021-07-30T02:13:29.558Z",
          "wordCount": 626,
          "title": "Coarse-to-Fine for Sim-to-Real: Sub-Millimetre Precision Across Wide Task Spaces. (arXiv:2105.11283v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Shuquan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Songfang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>",
          "description": "Point cloud segmentation is a fundamental task in 3D. Despite recent progress\non point cloud segmentation with the power of deep networks, current deep\nlearning methods based on the clean label assumptions may fail with noisy\nlabels. Yet, object class labels are often mislabeled in real-world point cloud\ndatasets. In this work, we take the lead in solving this issue by proposing a\nnovel Point Noise-Adaptive Learning (PNAL) framework. Compared to existing\nnoise-robust methods on image tasks, our PNAL is noise-rate blind, to cope with\nthe spatially variant noise rate problem specific to point clouds.\nSpecifically, we propose a novel point-wise confidence selection to obtain\nreliable labels based on the historical predictions of each point. A novel\ncluster-wise label correction is proposed with a voting strategy to generate\nthe best possible label taking the neighbor point correlations into\nconsideration. We conduct extensive experiments to demonstrate the\neffectiveness of PNAL on both synthetic and real-world noisy datasets. In\nparticular, even with $60\\%$ symmetric noisy labels, our proposed method\nproduces much better results than its baseline counterpart without PNAL and is\ncomparable to the ideal upper bound trained on a completely clean dataset.\nMoreover, we fully re-labeled the test set of a popular but noisy real-world\nscene dataset ScanNetV2 to make it clean, for rigorous experiment and future\nresearch. Our code and data will be available at\n\\url{https://shuquanye.com/PNAL_website/}.",
          "link": "http://arxiv.org/abs/2107.14230",
          "publishedOn": "2021-07-30T02:13:29.197Z",
          "wordCount": 683,
          "title": "Learning with Noisy Labels for Robust Point Cloud Segmentation. (arXiv:2107.14230v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luczynski_T/0/1/0/all/0/1\">Tomasz Luczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willners_J/0/1/0/all/0/1\">Jonatan Scharff Willners</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vargas_E/0/1/0/all/0/1\">Elizabeth Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roe_J/0/1/0/all/0/1\">Joshua Roe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shida Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petillot_Y/0/1/0/all/0/1\">Yvan Petillot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>",
          "description": "This paper presents a novel dataset for the development of visual navigation\nand simultaneous localisation and mapping (SLAM) algorithms as well as for\nunderwater intervention tasks. It differs from existing datasets as it contains\nground truth for the vehicle's position captured by an underwater motion\ntracking system. The dataset contains distortion-free and rectified stereo\nimages along with the calibration parameters of the stereo camera setup.\nFurthermore, the experiments were performed and recorded in a controlled\nenvironment, where current and waves could be generated allowing the dataset to\ncover a wide range of conditions - from calm water to waves and currents of\nsignificant strength.",
          "link": "http://arxiv.org/abs/2107.13628",
          "publishedOn": "2021-07-30T02:13:29.140Z",
          "wordCount": 542,
          "title": "Underwater inspection and intervention dataset. (arXiv:2107.13628v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shkodrani_S/0/1/0/all/0/1\">Sindi Shkodrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manfredi_M/0/1/0/all/0/1\">Marco Manfredi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baka_N/0/1/0/all/0/1\">N&#xf3;ra Baka</a>",
          "description": "Attempts of learning from hierarchical taxonomies in computer vision have\nbeen mostly focusing on image classification. Though ways of best harvesting\nlearning improvements from hierarchies in classification are far from being\nsolved, there is a need to target these problems in other vision tasks such as\nobject detection. As progress on the classification side is often dependent on\nhierarchical cross-entropy losses, novel detection architectures using sigmoid\nas an output function instead of softmax cannot easily apply these advances,\nrequiring novel methods in detection. In this work we establish a theoretical\nframework based on probability and set theory for extracting parent predictions\nand a hierarchical loss that can be used across tasks, showing results across\nclassification and detection benchmarks and opening up the possibility of\nhierarchical learning for sigmoid-based detection architectures.",
          "link": "http://arxiv.org/abs/2107.13627",
          "publishedOn": "2021-07-30T02:13:29.104Z",
          "wordCount": 574,
          "title": "United We Learn Better: Harvesting Learning Improvements From Class Hierarchies Across Tasks. (arXiv:2107.13627v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiangmiao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>",
          "description": "3D object detection is an important capability needed in various practical\napplications such as driver assistance systems. Monocular 3D detection, as an\neconomical solution compared to conventional settings relying on binocular\nvision or LiDAR, has drawn increasing attention recently but still yields\nunsatisfactory results. This paper first presents a systematic study on this\nproblem and observes that the current monocular 3D detection problem can be\nsimplified as an instance depth estimation problem: The inaccurate instance\ndepth blocks all the other 3D attribute predictions from improving the overall\ndetection performance. However, recent methods directly estimate the depth\nbased on isolated instances or pixels while ignoring the geometric relations\nacross different objects, which can be valuable constraints as the key\ninformation about depth is not directly manifest in the monocular image.\nTherefore, we construct geometric relation graphs across predicted objects and\nuse the graph to facilitate depth estimation. As the preliminary depth\nestimation of each instance is usually inaccurate in this ill-posed setting, we\nincorporate a probabilistic representation to capture the uncertainty. It\nprovides an important indicator to identify confident predictions and further\nguide the depth propagation. Despite the simplicity of the basic idea, our\nmethod obtains significant improvements on KITTI and nuScenes benchmarks,\nachieving the 1st place out of all monocular vision-only methods while still\nmaintaining real-time efficiency. Code and models will be released at\nhttps://github.com/open-mmlab/mmdetection3d.",
          "link": "http://arxiv.org/abs/2107.14160",
          "publishedOn": "2021-07-30T02:13:29.036Z",
          "wordCount": 662,
          "title": "Probabilistic and Geometric Depth: Detecting Objects in Perspective. (arXiv:2107.14160v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sabater_A/0/1/0/all/0/1\">Alberto Sabater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_L/0/1/0/all/0/1\">Laura Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_Victor_J/0/1/0/all/0/1\">Jose Santos-Victor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardino_A/0/1/0/all/0/1\">Alexandre Bernardino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1\">Luis Montesano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1\">Ana C. Murillo</a>",
          "description": "One-shot action recognition aims to recognize new action categories from a\nsingle reference example, typically referred to as the anchor example. This\nwork presents a novel approach for one-shot action recognition in the wild that\ncomputes motion representations robust to variable kinematic conditions.\nOne-shot action recognition is then performed by evaluating anchor and target\nmotion representations. We also develop a set of complementary steps that boost\nthe action recognition performance in the most challenging scenarios. Our\napproach is evaluated on the public NTU-120 one-shot action recognition\nbenchmark, outperforming previous action recognition models. Besides, we\nevaluate our framework on a real use-case of therapy with autistic people.\nThese recordings are particularly challenging due to high-level artifacts from\nthe patient motion. Our results provide not only quantitative but also online\nqualitative measures, essential for the patient evaluation and monitoring\nduring the actual therapy.",
          "link": "http://arxiv.org/abs/2102.08997",
          "publishedOn": "2021-07-30T02:13:28.989Z",
          "wordCount": 627,
          "title": "One-shot action recognition in challenging therapy scenarios. (arXiv:2102.08997v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Baobei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>",
          "description": "Depth completion deals with the problem of recovering dense depth maps from\nsparse ones, where color images are often used to facilitate this completion.\nRecent approaches mainly focus on image guided learning to predict dense\nresults. However, blurry image guidance and object structures in depth still\nimpede the performance of image guided frameworks. To tackle these problems, we\nexplore a repetitive design in our image guided network to sufficiently and\ngradually recover depth values. Specifically, the repetition is embodied in a\ncolor image guidance branch and a depth generation branch. In the former\nbranch, we design a repetitive hourglass network to extract higher-level image\nfeatures of complex environments, which can provide powerful context guidance\nfor depth prediction. In the latter branch, we design a repetitive guidance\nmodule based on dynamic convolution where the convolution factorization is\napplied to simultaneously reduce its complexity and progressively model\nhigh-frequency structures, e.g., boundaries. Further, in this module, we\npropose an adaptive fusion mechanism to effectively aggregate multi-step depth\nfeatures. Extensive experiments show that our method achieves state-of-the-art\nresult on the NYUv2 dataset and ranks 1st on the KITTI benchmark at the time of\nsubmission.",
          "link": "http://arxiv.org/abs/2107.13802",
          "publishedOn": "2021-07-30T02:13:28.982Z",
          "wordCount": 639,
          "title": "RigNet: Repetitive Image Guided Network for Depth Completion. (arXiv:2107.13802v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zeyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xuyang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jiaxiang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Runze Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jiayu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangyuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongbo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_C/0/1/0/all/0/1\">Chiew-Lan Tai</a>",
          "description": "In recent years, sparse voxel-based methods have become the state-of-the-arts\nfor 3D semantic segmentation of indoor scenes, thanks to the powerful 3D CNNs.\nNevertheless, being oblivious to the underlying geometry, voxel-based methods\nsuffer from ambiguous features on spatially close objects and struggle with\nhandling complex and irregular geometries due to the lack of geodesic\ninformation. In view of this, we present Voxel-Mesh Network (VMNet), a novel 3D\ndeep architecture that operates on the voxel and mesh representations\nleveraging both the Euclidean and geodesic information. Intuitively, the\nEuclidean information extracted from voxels can offer contextual cues\nrepresenting interactions between nearby objects, while the geodesic\ninformation extracted from meshes can help separate objects that are spatially\nclose but have disconnected surfaces. To incorporate such information from the\ntwo domains, we design an intra-domain attentive module for effective feature\naggregation and an inter-domain attentive module for adaptive feature fusion.\nExperimental results validate the effectiveness of VMNet: specifically, on the\nchallenging ScanNet dataset for large-scale segmentation of indoor scenes, it\noutperforms the state-of-the-art SparseConvNet and MinkowskiNet (74.6% vs 72.5%\nand 73.6% in mIoU) with a simpler network structure (17M vs 30M and 38M\nparameters). Code release: https://github.com/hzykent/VMNet",
          "link": "http://arxiv.org/abs/2107.13824",
          "publishedOn": "2021-07-30T02:13:28.957Z",
          "wordCount": 646,
          "title": "VMNet: Voxel-Mesh Network for Geodesic-Aware 3D Semantic Segmentation. (arXiv:2107.13824v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.05101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>",
          "description": "Ternary Neural Networks (TNNs) have received much attention due to being\npotentially orders of magnitude faster in inference, as well as more power\nefficient, than full-precision counterparts. However, 2 bits are required to\nencode the ternary representation with only 3 quantization levels leveraged. As\na result, conventional TNNs have similar memory consumption and speed compared\nwith the standard 2-bit models, but have worse representational capability.\nMoreover, there is still a significant gap in accuracy between TNNs and\nfull-precision networks, hampering their deployment to real applications. To\ntackle these two challenges, in this work, we first show that, under some mild\nconstraints, computational complexity of the ternary inner product can be\nreduced by a factor of 2. Second, to mitigate the performance gap, we\nelaborately design an implementation-dependent ternary quantization algorithm.\nThe proposed framework is termed Fast and Accurate Ternary Neural Networks\n(FATNN). Experiments on image classification demonstrate that our FATNN\nsurpasses the state-of-the-arts by a significant margin in accuracy. More\nimportantly, speedup evaluation compared with various precisions is analyzed on\nseveral platforms, which serves as a strong benchmark for further research.",
          "link": "http://arxiv.org/abs/2008.05101",
          "publishedOn": "2021-07-30T02:13:28.948Z",
          "wordCount": 669,
          "title": "FATNN: Fast and Accurate Ternary Neural Networks. (arXiv:2008.05101v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samani_E/0/1/0/all/0/1\">Ekta U. Samani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xingjian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Ashis G. Banerjee</a>",
          "description": "Object recognition in unseen indoor environments remains a challenging\nproblem for visual perception of mobile robots. In this letter, we propose the\nuse of topologically persistent features, which rely on the objects' shape\ninformation, to address this challenge. In particular, we extract two kinds of\nfeatures, namely, sparse persistence image (PI) and amplitude, by applying\npersistent homology to multi-directional height function-based filtrations of\nthe cubical complexes representing the object segmentation maps. The features\nare then used to train a fully connected network for recognition. For\nperformance evaluation, in addition to a widely used shape dataset and a\nbenchmark indoor scenes dataset, we collect a new dataset, comprising scene\nimages from two different environments, namely, a living room and a mock\nwarehouse. The scenes are captured using varying camera poses under different\nillumination conditions and include up to five different objects from a given\nset of fourteen objects. On the benchmark indoor scenes dataset, sparse PI\nfeatures show better recognition performance in unseen environments than the\nfeatures learned using the widely used ResNetV2-56 and EfficientNet-B4 models.\nFurther, they provide slightly higher recall and accuracy values than Faster\nR-CNN, an end-to-end object detection method, and its state-of-the-art variant,\nDomain Adaptive Faster R-CNN. The performance of our methods also remains\nrelatively unchanged from the training environment (living room) to the unseen\nenvironment (mock warehouse) in the new dataset. In contrast, the performance\nof the object detection methods drops substantially. We also implement the\nproposed method on a real-world robot to demonstrate its usefulness.",
          "link": "http://arxiv.org/abs/2010.03196",
          "publishedOn": "2021-07-30T02:13:28.943Z",
          "wordCount": 766,
          "title": "Visual Object Recognition in Indoor Environments Using Topologically Persistent Features. (arXiv:2010.03196v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaopeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chris Xiaoxuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengdi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peijun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1\">Niki Trigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1\">Andrew Markham</a>",
          "description": "Accurately describing and detecting 2D and 3D keypoints is crucial to\nestablishing correspondences across images and point clouds. Despite a plethora\nof learning-based 2D or 3D local feature descriptors and detectors having been\nproposed, the derivation of a shared descriptor and joint keypoint detector\nthat directly matches pixels and points remains under-explored by the\ncommunity. This work takes the initiative to establish fine-grained\ncorrespondences between 2D images and 3D point clouds. In order to directly\nmatch pixels and points, a dual fully convolutional framework is presented that\nmaps 2D and 3D inputs into a shared latent representation space to\nsimultaneously describe and detect keypoints. Furthermore, an ultra-wide\nreception mechanism in combination with a novel loss function are designed to\nmitigate the intrinsic information variations between pixel and point local\nregions. Extensive experimental results demonstrate that our framework shows\ncompetitive performance in fine-grained matching between images and point\nclouds and achieves state-of-the-art results for the task of indoor visual\nlocalization. Our source code will be available at [no-name-for-blind-review].",
          "link": "http://arxiv.org/abs/2103.01055",
          "publishedOn": "2021-07-30T02:13:28.934Z",
          "wordCount": 663,
          "title": "P2-Net: Joint Description and Detection of Local Features for Pixel and Point Matching. (arXiv:2103.01055v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01288",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tiange Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianhui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>",
          "description": "Discrete point cloud objects lack sufficient shape descriptors of 3D\ngeometries. In this paper, we present a novel method for aggregating\nhypothetical curves in point clouds. Sequences of connected points (curves) are\ninitially grouped by taking guided walks in the point clouds, and then\nsubsequently aggregated back to augment their point-wise features. We provide\nan effective implementation of the proposed aggregation strategy including a\nnovel curve grouping operator followed by a curve aggregation operator. Our\nmethod was benchmarked on several point cloud analysis tasks where we achieved\nthe state-of-the-art classification accuracy of 94.2% on the ModelNet40\nclassification task, instance IoU of 86.8 on the ShapeNetPart segmentation\ntask, and cosine error of 0.11 on the ModelNet40 normal estimation task.",
          "link": "http://arxiv.org/abs/2105.01288",
          "publishedOn": "2021-07-30T02:13:28.914Z",
          "wordCount": 592,
          "title": "Walk in the Cloud: Learning Curves for Point Clouds Shape Analysis. (arXiv:2105.01288v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wehrbein_T/0/1/0/all/0/1\">Tom Wehrbein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1\">Marco Rudolph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wandt_B/0/1/0/all/0/1\">Bastian Wandt</a>",
          "description": "3D human pose estimation from monocular images is a highly ill-posed problem\ndue to depth ambiguities and occlusions. Nonetheless, most existing works\nignore these ambiguities and only estimate a single solution. In contrast, we\ngenerate a diverse set of hypotheses that represents the full posterior\ndistribution of feasible 3D poses. To this end, we propose a normalizing flow\nbased method that exploits the deterministic 3D-to-2D mapping to solve the\nambiguous inverse 2D-to-3D problem. Additionally, uncertain detections and\nocclusions are effectively modeled by incorporating uncertainty information of\nthe 2D detector as condition. Further keys to success are a learned 3D pose\nprior and a generalization of the best-of-M loss. We evaluate our approach on\nthe two benchmark datasets Human3.6M and MPI-INF-3DHP, outperforming all\ncomparable methods in most metrics. The implementation is available on GitHub.",
          "link": "http://arxiv.org/abs/2107.13788",
          "publishedOn": "2021-07-30T02:13:28.908Z",
          "wordCount": 575,
          "title": "Probabilistic Monocular 3D Human Pose Estimation with Normalizing Flows. (arXiv:2107.13788v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianxiao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongbin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shane Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinmei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qiuda Yu</a>",
          "description": "Monocular depth estimation and semantic segmentation are two fundamental\ngoals of scene understanding. Due to the advantages of task interaction, many\nworks study the joint task learning algorithm. However, most existing methods\nfail to fully leverage the semantic labels, ignoring the provided context\nstructures and only using them to supervise the prediction of segmentation\nsplit. In this paper, we propose a network injected with contextual information\n(CI-Net) to solve the problem. Specifically, we introduce self-attention block\nin the encoder to generate attention map. With supervision from the ground\ntruth created by semantic labels, the network is embedded with contextual\ninformation so that it could understand the scene better, utilizing dependent\nfeatures to make accurate prediction. Besides, a feature sharing module is\nconstructed to make the task-specific features deeply fused and a consistency\nloss is devised to make the features mutually guided. We evaluate the proposed\nCI-Net on the NYU-Depth-v2 and SUN-RGBD datasets. The experimental results\nvalidate that our proposed CI-Net is competitive with the state-of-the-arts.",
          "link": "http://arxiv.org/abs/2107.13800",
          "publishedOn": "2021-07-30T02:13:28.902Z",
          "wordCount": 615,
          "title": "CI-Net: Contextual Information for Joint Semantic Segmentation and Depth Estimation. (arXiv:2107.13800v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.10860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muyao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yehui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>",
          "description": "With the rapid development of measurement technology, LiDAR and depth cameras\nare widely used in the perception of the 3D environment. Recent learning based\nmethods for robot perception most focus on the image or video, but deep\nlearning methods for dynamic 3D point cloud sequences are underexplored.\nTherefore, developing efficient and accurate perception method compatible with\nthese advanced instruments is pivotal to autonomous driving and service robots.\nAn Anchor-based Spatio-Temporal Attention 3D Convolution operation (ASTA3DConv)\nis proposed in this paper to process dynamic 3D point cloud sequences. The\nproposed convolution operation builds a regular receptive field around each\npoint by setting several virtual anchors around each point. The features of\nneighborhood points are firstly aggregated to each anchor based on the\nspatio-temporal attention mechanism. Then, anchor-based 3D convolution is\nadopted to aggregate these anchors' features to the core points. The proposed\nmethod makes better use of the structured information within the local region\nand learns spatio-temporal embedding features from dynamic 3D point cloud\nsequences. Anchor-based Spatio-Temporal Attention 3D Convolutional Neural\nNetworks (ASTA3DCNNs) are built for classification and segmentation tasks based\non the proposed ASTA3DConv and evaluated on action recognition and semantic\nsegmentation tasks. The experiments and ablation studies on MSRAction3D and\nSynthia datasets demonstrate the superior performance and effectiveness of our\nmethod for dynamic 3D point cloud sequences. Our method achieves the\nstate-of-the-art performance among the methods with dynamic 3D point cloud\nsequences as input on MSRAction3D and Synthia datasets.",
          "link": "http://arxiv.org/abs/2012.10860",
          "publishedOn": "2021-07-30T02:13:28.895Z",
          "wordCount": 724,
          "title": "Anchor-Based Spatio-Temporal Attention 3D Convolutional Networks for Dynamic 3D Point Cloud Sequences. (arXiv:2012.10860v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhihao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yinghong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruimao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>",
          "description": "Compared with the visual grounding on 2D images, the natural-language-guided\n3D object localization on point clouds is more challenging. In this paper, we\npropose a new model, named InstanceRefer, to achieve a superior 3D visual\ngrounding through the grounding-by-matching strategy. In practice, our model\nfirst predicts the target category from the language descriptions using a\nsimple language classification model. Then, based on the category, our model\nsifts out a small number of instance candidates (usually less than 20) from the\npanoptic segmentation of point clouds. Thus, the non-trivial 3D visual\ngrounding task has been effectively re-formulated as a simplified\ninstance-matching problem, considering that instance-level candidates are more\nrational than the redundant 3D object proposals. Subsequently, for each\ncandidate, we perform the multi-level contextual inference, i.e., referring\nfrom instance attribute perception, instance-to-instance relation perception,\nand instance-to-background global localization perception, respectively.\nEventually, the most relevant candidate is selected and localized by ranking\nconfidence scores, which are obtained by the cooperative holistic\nvisual-language feature matching. Experiments confirm that our method\noutperforms previous state-of-the-arts on ScanRefer online benchmark and\nNr3D/Sr3D datasets.",
          "link": "http://arxiv.org/abs/2103.01128",
          "publishedOn": "2021-07-30T02:13:28.879Z",
          "wordCount": 675,
          "title": "InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring. (arXiv:2103.01128v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiongchao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatemi_A/0/1/0/all/0/1\">Arezou Fatemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lira_W/0/1/0/all/0/1\">Wallace Lira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fenggen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_B/0/1/0/all/0/1\">Biao Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1\">Ali Mahdavi-Amiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "We introduce RaidaR, a rich annotated image dataset of rainy street scenes,\nto support autonomous driving research. The new dataset contains the largest\nnumber of rainy images (58,542) to date, 5,000 of which provide semantic\nsegmentations and 3,658 provide object instance segmentations. The RaidaR\nimages cover a wide range of realistic rain-induced artifacts, including fog,\ndroplets, and road reflections, which can effectively augment existing street\nscene datasets to improve data-driven machine perception during rainy weather.\nTo facilitate efficient annotation of a large volume of images, we develop a\nsemi-automatic scheme combining manual segmentation and an automated processing\nakin to cross validation, resulting in 10-20 fold reduction on annotation time.\nWe demonstrate the utility of our new dataset by showing how data augmentation\nwith RaidaR can elevate the accuracy of existing segmentation algorithms. We\nalso present a novel unpaired image-to-image translation algorithm for\nadding/removing rain artifacts, which directly benefits from RaidaR.",
          "link": "http://arxiv.org/abs/2104.04606",
          "publishedOn": "2021-07-30T02:13:28.873Z",
          "wordCount": 635,
          "title": "RaidaR: A Rich Annotated Image Dataset of Rainy Street Scenes. (arXiv:2104.04606v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Willes_J/0/1/0/all/0/1\">John Willes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_J/0/1/0/all/0/1\">James Harrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harakeh_A/0/1/0/all/0/1\">Ali Harakeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1\">Steven Waslander</a>",
          "description": "As autonomous decision-making agents move from narrow operating environments\nto unstructured worlds, learning systems must move from a closed-world\nformulation to an open-world and few-shot setting in which agents continuously\nlearn new classes from small amounts of information. This stands in stark\ncontrast to modern machine learning systems that are typically designed with a\nknown set of classes and a large number of examples for each class. In this\nwork we extend embedding-based few-shot learning algorithms to the open-world\nrecognition setting. We combine Bayesian non-parametric class priors with an\nembedding-based pre-training scheme to yield a highly flexible framework which\nwe refer to as few-shot learning for open world recognition (FLOWR). We\nbenchmark our framework on open-world extensions of the common MiniImageNet and\nTieredImageNet few-shot learning datasets. Our results show, compared to prior\nmethods, strong classification accuracy performance and up to a 12% improvement\nin H-measure (a measure of novel class detection) from our non-parametric\nopen-world few-shot learning scheme.",
          "link": "http://arxiv.org/abs/2107.13682",
          "publishedOn": "2021-07-30T02:13:28.867Z",
          "wordCount": 596,
          "title": "Bayesian Embeddings for Few-Shot Open World Recognition. (arXiv:2107.13682v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wenpeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuemiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>",
          "description": "Existing GAN inversion methods are stuck in a paradox that the inverted codes\ncan either achieve high-fidelity reconstruction, or retain the editing\ncapability. Having only one of them clearly cannot realize real image editing.\nIn this paper, we resolve this paradox by introducing consecutive images (\\eg,\nvideo frames or the same person with different poses) into the inversion\nprocess. The rationale behind our solution is that the continuity of\nconsecutive images leads to inherent editable directions. This inborn property\nis used for two unique purposes: 1) regularizing the joint inversion process,\nsuch that each of the inverted code is semantically accessible from one of the\nother and fastened in a editable domain; 2) enforcing inter-image coherence,\nsuch that the fidelity of each inverted code can be maximized with the\ncomplement of other images. Extensive experiments demonstrate that our\nalternative significantly outperforms state-of-the-art methods in terms of\nreconstruction fidelity and editability on both the real image dataset and\nsynthesis dataset. Furthermore, our method provides the first support of\nvideo-based GAN inversion, and an interesting application of unsupervised\nsemantic transfer from consecutive images. Source code can be found at:\n\\url{https://github.com/Qingyang-Xu/InvertingGANs_with_ConsecutiveImgs}.",
          "link": "http://arxiv.org/abs/2107.13812",
          "publishedOn": "2021-07-30T02:13:28.852Z",
          "wordCount": 636,
          "title": "From Continuity to Editability: Inverting GANs with Consecutive Images. (arXiv:2107.13812v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raman_C/0/1/0/all/0/1\">Chirag Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_H/0/1/0/all/0/1\">Hayley Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loog_M/0/1/0/all/0/1\">Marco Loog</a>",
          "description": "The default paradigm for the forecasting of human behavior in social\nconversations is characterized by top-down approaches. These involve\nidentifying predictive relationships between low level nonverbal cues and\nfuture semantic events of interest (e.g. turn changes, group leaving). A common\nhurdle however, is the limited availability of labeled data for supervised\nlearning. In this work, we take the first step in the direction of a bottom-up\nself-supervised approach in the domain. We formulate the task of Social Cue\nForecasting to leverage the larger amount of unlabeled low-level behavior cues,\nand characterize the modeling challenges involved. To address these, we take a\nmeta-learning approach and propose the Social Process (SP) models--socially\naware sequence-to-sequence (Seq2Seq) models within the Neural Process (NP)\nfamily. SP models learn extractable representations of non-semantic future cues\nfor each participant, while capturing global uncertainty by jointly reasoning\nabout the future for all members of the group. Evaluation on synthesized and\nreal-world behavior data shows that our SP models achieve higher log-likelihood\nthan the NP baselines, and also highlights important considerations for\napplying such techniques within the domain of social human interactions.",
          "link": "http://arxiv.org/abs/2107.13576",
          "publishedOn": "2021-07-30T02:13:28.836Z",
          "wordCount": 628,
          "title": "Social Processes: Self-Supervised Forecasting of Nonverbal Cues in Social Conversations. (arXiv:2107.13576v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03725",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Ping Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yujin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangyin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1\">Qingpei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yong Tan</a>",
          "description": "Reconstructing a high-precision and high-fidelity 3D human hand from a color\nimage plays a central role in replicating a realistic virtual hand in\nhuman-computer interaction and virtual reality applications. The results of\ncurrent methods are lacking in accuracy and fidelity due to various hand poses\nand severe occlusions. In this study, we propose an I2UV-HandNet model for\naccurate hand pose and shape estimation as well as 3D hand super-resolution\nreconstruction. Specifically, we present the first UV-based 3D hand shape\nrepresentation. To recover a 3D hand mesh from an RGB image, we design an\nAffineNet to predict a UV position map from the input in an image-to-image\ntranslation fashion. To obtain a higher fidelity shape, we exploit an\nadditional SRNet to transform the low-resolution UV map outputted by AffineNet\ninto a high-resolution one. For the first time, we demonstrate the\ncharacterization capability of the UV-based hand shape representation. Our\nexperiments show that the proposed method achieves state-of-the-art performance\non several challenging benchmarks.",
          "link": "http://arxiv.org/abs/2102.03725",
          "publishedOn": "2021-07-30T02:13:28.830Z",
          "wordCount": 648,
          "title": "I2UV-HandNet: Image-to-UV Prediction Network for Accurate and High-fidelity 3D Hand Mesh Modeling. (arXiv:2102.03725v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.08218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xianyu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1\">Ziwei Liao</a>",
          "description": "Plane feature is a kind of stable landmark to reduce drift error in SLAM\nsystem. It is easy and fast to extract planes from dense point cloud, which is\ncommonly acquired from RGB-D camera or lidar. But for stereo camera, it is hard\nto compute dense point cloud accurately and efficiently. In this paper, we\npropose a novel method to compute plane parameters using intersecting lines\nwhich are extracted from the stereo image. The plane features commonly exist on\nthe surface of man-made objects and structure, which have regular shape and\nstraight edge lines. In 3D space, two intersecting lines can determine such a\nplane. Thus we extract line segments from both stereo left and right image. By\nstereo matching, we compute the endpoints and line directions in 3D space, and\nthen the planes from two intersecting lines. We discard those inaccurate plane\nfeatures in the frame tracking. Adding such plane features in stereo SLAM\nsystem reduces the drift error and refines the performance. We test our\nproposed system on public datasets and demonstrate its robust and accurate\nestimation results, compared with state-of-the-art SLAM systems. To benefit the\nresearch of plane-based SLAM, we release our codes at\nhttps://github.com/fishmarch/Stereo-Plane-SLAM.",
          "link": "http://arxiv.org/abs/2008.08218",
          "publishedOn": "2021-07-30T02:13:28.808Z",
          "wordCount": 675,
          "title": "Stereo Plane SLAM Based on Intersecting Lines. (arXiv:2008.08218v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qingtian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_C/0/1/0/all/0/1\">Chen Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zizhuang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yisong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoping Wang</a>",
          "description": "3D reconstruction has lately attracted increasing attention due to its wide\napplication in many areas, such as autonomous driving, robotics and virtual\nreality. As a dominant technique in artificial intelligence, deep learning has\nbeen successfully adopted to solve various computer vision problems. However,\ndeep learning for 3D reconstruction is still at its infancy due to its unique\nchallenges and varying pipelines. To stimulate future research, this paper\npresents a review of recent progress in deep learning methods for Multi-view\nStereo (MVS), which is considered as a crucial task of image-based 3D\nreconstruction. It also presents comparative results on several publicly\navailable datasets, with insightful observations and inspiring future research\ndirections.",
          "link": "http://arxiv.org/abs/2106.15328",
          "publishedOn": "2021-07-30T02:13:28.801Z",
          "wordCount": 568,
          "title": "Deep Learning for Multi-View Stereo via Plane Sweep: A Survey. (arXiv:2106.15328v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuen_J/0/1/0/all/0/1\">Jason Kuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "We introduce a new image segmentation task, termed Entity Segmentation (ES)\nwith the aim to segment all visual entities in an image without considering\nsemantic category labels. It has many practical applications in image\nmanipulation/editing where the segmentation mask quality is typically crucial\nbut category labels are less important. In this setting, all\nsemantically-meaningful segments are equally treated as categoryless entities\nand there is no thing-stuff distinction. Based on our unified entity\nrepresentation, we propose a center-based entity segmentation framework with\ntwo novel modules to improve mask quality. Experimentally, both our new task\nand framework demonstrate superior advantages as against existing work. In\nparticular, ES enables the following: (1) merging multiple datasets to form a\nlarge training set without the need to resolve label conflicts; (2) any model\ntrained on one dataset can generalize exceptionally well to other datasets with\nunseen domains. Our code is made publicly available at\nhttps://github.com/dvlab-research/Entity.",
          "link": "http://arxiv.org/abs/2107.14228",
          "publishedOn": "2021-07-30T02:13:28.778Z",
          "wordCount": 595,
          "title": "Open-World Entity Segmentation. (arXiv:2107.14228v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.14119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1\">Emanuel Ben-Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1\">Tal Ridnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_N/0/1/0/all/0/1\">Nadav Zamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_I/0/1/0/all/0/1\">Itamar Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protter_M/0/1/0/all/0/1\">Matan Protter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1\">Lihi Zelnik-Manor</a>",
          "description": "In a typical multi-label setting, a picture contains on average few positive\nlabels, and many negative ones. This positive-negative imbalance dominates the\noptimization process, and can lead to under-emphasizing gradients from positive\nlabels during training, resulting in poor accuracy. In this paper, we introduce\na novel asymmetric loss (\"ASL\"), which operates differently on positive and\nnegative samples. The loss enables to dynamically down-weights and\nhard-thresholds easy negative samples, while also discarding possibly\nmislabeled samples. We demonstrate how ASL can balance the probabilities of\ndifferent samples, and how this balancing is translated to better mAP scores.\nWith ASL, we reach state-of-the-art results on multiple popular multi-label\ndatasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate\nASL applicability for other tasks, such as single-label classification and\nobject detection. ASL is effective, easy to implement, and does not increase\nthe training time or complexity.\n\nImplementation is available at: https://github.com/Alibaba-MIIL/ASL.",
          "link": "http://arxiv.org/abs/2009.14119",
          "publishedOn": "2021-07-30T02:13:28.754Z",
          "wordCount": 650,
          "title": "Asymmetric Loss For Multi-Label Classification. (arXiv:2009.14119v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pizzati_F/0/1/0/all/0/1\">Fabio Pizzati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerri_P/0/1/0/all/0/1\">Pietro Cerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>",
          "description": "Image-to-image translation (i2i) networks suffer from entanglement effects in\npresence of physics-related phenomena in target domain (such as occlusions,\nfog, etc), thus lowering the translation quality and variability. In this\npaper, we present a comprehensive method for disentangling physics-based traits\nin the translation, guiding the learning process with neural or physical\nmodels. For the latter, we integrate adversarial estimation and genetic\nalgorithms to correctly achieve disentanglement. The results show our approach\ndramatically increase performances in many challenging scenarios for image\ntranslation.",
          "link": "http://arxiv.org/abs/2107.14229",
          "publishedOn": "2021-07-30T02:13:28.747Z",
          "wordCount": 524,
          "title": "Guided Disentanglement in Generative Networks. (arXiv:2107.14229v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1\">Nuoxing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liangliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>",
          "description": "Trajectory prediction is confronted with the dilemma to capture the\nmulti-modal nature of future dynamics with both diversity and accuracy. In this\npaper, we present a distribution discrimination (DisDis) method to predict\npersonalized motion patterns by distinguishing the potential distributions.\nMotivated by that the motion pattern of each person is personalized due to\nhis/her habit, our DisDis learns the latent distribution to represent different\nmotion patterns and optimize it by the contrastive discrimination. This\ndistribution discrimination encourages latent distributions to be more\ndiscriminative. Our method can be integrated with existing multi-modal\nstochastic predictive models as a plug-and-play module to learn the more\ndiscriminative latent distribution. To evaluate the latent distribution, we\nfurther propose a new metric, probability cumulative minimum distance (PCMD)\ncurve, which cumulatively calculates the minimum distance on the sorted\nprobabilities. Experimental results on the ETH and UCY datasets show the\neffectiveness of our method.",
          "link": "http://arxiv.org/abs/2107.14204",
          "publishedOn": "2021-07-30T02:13:28.729Z",
          "wordCount": 589,
          "title": "Personalized Trajectory Prediction via Distribution Discrimination. (arXiv:2107.14204v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.08773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_E/0/1/0/all/0/1\">Eddie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>",
          "description": "Data augmentations are important ingredients in the recipe for training\nrobust neural networks, especially in computer vision. A fundamental question\nis whether neural network features encode data augmentation transformations. To\nanswer this question, we introduce a systematic approach to investigate which\nlayers of neural networks are the most predictive of augmentation\ntransformations. Our approach uses features in pre-trained vision models with\nminimal additional processing to predict common properties transformed by\naugmentation (scale, aspect ratio, hue, saturation, contrast, and brightness).\nSurprisingly, neural network features not only predict data augmentation\ntransformations, but they predict many transformations with high accuracy.\nAfter validating that neural networks encode features corresponding to\naugmentation transformations, we show that these features are encoded in the\nearly layers of modern CNNs, though the augmentation signal fades in deeper\nlayers.",
          "link": "http://arxiv.org/abs/2003.08773",
          "publishedOn": "2021-07-30T02:13:28.721Z",
          "wordCount": 601,
          "title": "Do CNNs Encode Data Augmentations?. (arXiv:2003.08773v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thoduka_S/0/1/0/all/0/1\">Santosh Thoduka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ploger_P/0/1/0/all/0/1\">Paul G. Pl&#xf6;ger</a>",
          "description": "Execution monitoring is essential for robots to detect and respond to\nfailures. Since it is impossible to enumerate all failures for a given task, we\nlearn from successful executions of the task to detect visual anomalies during\nruntime. Our method learns to predict the motions that occur during the nominal\nexecution of a task, including camera and robot body motion. A probabilistic\nU-Net architecture is used to learn to predict optical flow, and the robot's\nkinematics and 3D model are used to model camera and body motion. The errors\nbetween the observed and predicted motion are used to calculate an anomaly\nscore. We evaluate our method on a dataset of a robot placing a book on a\nshelf, which includes anomalies such as falling books, camera occlusions, and\nrobot disturbances. We find that modeling camera and body motion, in addition\nto the learning-based optical flow prediction, results in an improvement of the\narea under the receiver operating characteristic curve from 0.752 to 0.804, and\nthe area under the precision-recall curve from 0.467 to 0.549.",
          "link": "http://arxiv.org/abs/2107.14206",
          "publishedOn": "2021-07-30T02:13:28.698Z",
          "wordCount": 624,
          "title": "Using Visual Anomaly Detection for Task Execution Monitoring. (arXiv:2107.14206v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00179",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunhui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1\">Davide Modolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>",
          "description": "Most action recognition solutions rely on dense sampling to precisely cover\nthe informative temporal clip. Extensively searching temporal region is\nexpensive for a real-world application. In this work, we focus on improving the\ninference efficiency of current action recognition backbones on trimmed videos,\nand illustrate that one action model can also cover then informative region by\ndropping non-informative features. We present Selective Feature Compression\n(SFC), an action recognition inference strategy that greatly increase model\ninference efficiency without any accuracy compromise. Differently from previous\nworks that compress kernel sizes and decrease the channel dimension, we propose\nto compress feature flow at spatio-temporal dimension without changing any\nbackbone parameters. Our experiments on Kinetics-400, UCF101 and ActivityNet\nshow that SFC is able to reduce inference speed by 6-7x and memory usage by\n5-6x compared with the commonly used 30 crops dense sampling procedure, while\nalso slightly improving Top1 Accuracy. We thoroughly quantitatively and\nqualitatively evaluate SFC and all its components and show how does SFC learn\nto attend to important video regions and to drop temporal features that are\nuninformative for the task of action recognition.",
          "link": "http://arxiv.org/abs/2104.00179",
          "publishedOn": "2021-07-30T02:13:28.681Z",
          "wordCount": 654,
          "title": "Selective Feature Compression for Efficient Activity Recognition Inference. (arXiv:2104.00179v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02303",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sabater_A/0/1/0/all/0/1\">Alberto Sabater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_I/0/1/0/all/0/1\">I&#xf1;igo Alonso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1\">Luis Montesano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1\">Ana C. Murillo</a>",
          "description": "Hand action recognition is a special case of action recognition with\napplications in human-robot interaction, virtual reality or life-logging\nsystems. Building action classifiers able to work for such heterogeneous action\ndomains is very challenging. There are very subtle changes across different\nactions from a given application but also large variations across domains (e.g.\nvirtual reality vs life-logging). This work introduces a novel skeleton-based\nhand motion representation model that tackles this problem. The framework we\npropose is agnostic to the application domain or camera recording view-point.\nWhen working on a single domain (intra-domain action classification) our\napproach performs better or similar to current state-of-the-art methods on\nwell-known hand action recognition benchmarks. And, more importantly, when\nperforming hand action recognition for action domains and camera perspectives\nwhich our approach has not been trained for (cross-domain action\nclassification), our proposed framework achieves comparable performance to\nintra-domain state-of-the-art methods. These experiments show the robustness\nand generalization capabilities of our framework.",
          "link": "http://arxiv.org/abs/2103.02303",
          "publishedOn": "2021-07-30T02:13:28.658Z",
          "wordCount": 619,
          "title": "Domain and View-point Agnostic Hand Action Recognition. (arXiv:2103.02303v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1\">Aditya Jyoti Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Smaranjit Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1\">Kanishka Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nethaji_N/0/1/0/all/0/1\">Niketha Nethaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Shivam Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purkayastha_A/0/1/0/all/0/1\">Arnab Dutta Purkayastha</a>",
          "description": "Tourism in India plays a quintessential role in the country's economy with an\nestimated 9.2% GDP share for the year 2018. With a yearly growth rate of 6.2%,\nthe industry holds a huge potential for being the primary driver of the economy\nas observed in the nations of the Middle East like the United Arab Emirates.\nThe historical and cultural diversity exhibited throughout the geography of the\nnation is a unique spectacle for people around the world and therefore serves\nto attract tourists in tens of millions in number every year. Traditionally,\ntour guides or academic professionals who study these heritage monuments were\nresponsible for providing information to the visitors regarding their\narchitectural and historical significance. However, unfortunately this system\nhas several caveats when considered on a large scale such as unavailability of\nsufficient trained people, lack of accurate information, failure to convey the\nrichness of details in an attractive format etc. Recently, machine learning\napproaches revolving around the usage of monument pictures have been shown to\nbe useful for rudimentary analysis of heritage sights. This paper serves as a\nsurvey of the research endeavors undertaken in this direction which would\neventually provide insights for building an automated decision system that\ncould be utilized to make the experience of tourism in India more modernized\nfor visitors.",
          "link": "http://arxiv.org/abs/2107.14070",
          "publishedOn": "2021-07-30T02:13:28.638Z",
          "wordCount": 690,
          "title": "Machine Learning Advances aiding Recognition and Classification of Indian Monuments and Landmarks. (arXiv:2107.14070v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuewen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingru Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>",
          "description": "Image captioning is shown to be able to achieve a better performance by using\nscene graphs to represent the relations of objects in the image. The current\ncaptioning encoders generally use a Graph Convolutional Net (GCN) to represent\nthe relation information and merge it with the object region features via\nconcatenation or convolution to get the final input for sentence decoding.\nHowever, the GCN-based encoders in the existing methods are less effective for\ncaptioning due to two reasons. First, using the image captioning as the\nobjective (i.e., Maximum Likelihood Estimation) rather than a relation-centric\nloss cannot fully explore the potential of the encoder. Second, using a\npre-trained model instead of the encoder itself to extract the relationships is\nnot flexible and cannot contribute to the explainability of the model. To\nimprove the quality of image captioning, we propose a novel architecture\nReFormer -- a RElational transFORMER to generate features with relation\ninformation embedded and to explicitly express the pair-wise relationships\nbetween objects in the image. ReFormer incorporates the objective of scene\ngraph generation with that of image captioning using one modified Transformer\nmodel. This design allows ReFormer to generate not only better image captions\nwith the bene-fit of extracting strong relational image features, but also\nscene graphs to explicitly describe the pair-wise relation-ships. Experiments\non publicly available datasets show that our model significantly outperforms\nstate-of-the-art methods on image captioning and scene graph generation",
          "link": "http://arxiv.org/abs/2107.14178",
          "publishedOn": "2021-07-30T02:13:28.600Z",
          "wordCount": 664,
          "title": "ReFormer: The Relational Transformer for Image Captioning. (arXiv:2107.14178v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">Eugene Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Cheng-Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yi Lee</a>",
          "description": "Deep neural networks (DNNs) are known to perform well when deployed to test\ndistributions that shares high similarity with the training distribution.\nFeeding DNNs with new data sequentially that were unseen in the training\ndistribution has two major challenges -- fast adaptation to new tasks and\ncatastrophic forgetting of old tasks. Such difficulties paved way for the\non-going research on few-shot learning and continual learning. To tackle these\nproblems, we introduce Attentive Independent Mechanisms (AIM). We incorporate\nthe idea of learning using fast and slow weights in conjunction with the\ndecoupling of the feature extraction and higher-order conceptual learning of a\nDNN. AIM is designed for higher-order conceptual learning, modeled by a mixture\nof experts that compete to learn independent concepts to solve a new task. AIM\nis a modular component that can be inserted into existing deep learning\nframeworks. We demonstrate its capability for few-shot learning by adding it to\nSIB and trained on MiniImageNet and CIFAR-FS, showing significant improvement.\nAIM is also applied to ANML and OML trained on Omniglot, CIFAR-100 and\nMiniImageNet to demonstrate its capability in continual learning. Code made\npublicly available at https://github.com/huang50213/AIM-Fewshot-Continual.",
          "link": "http://arxiv.org/abs/2107.14053",
          "publishedOn": "2021-07-30T02:13:28.594Z",
          "wordCount": 643,
          "title": "Few-Shot and Continual Learning with Attentive Independent Mechanisms. (arXiv:2107.14053v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.10143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tomen_N/0/1/0/all/0/1\">Nergis Tomen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan van Gemert</a>",
          "description": "Convolutional layers in CNNs implement linear filters which decompose the\ninput into different frequency bands. However, most modern architectures\nneglect standard principles of filter design when optimizing their model\nchoices regarding the size and shape of the convolutional kernel. In this work,\nwe consider the well-known problem of spectral leakage caused by windowing\nartifacts in filtering operations in the context of CNNs. We show that the\nsmall size of CNN kernels make them susceptible to spectral leakage, which may\ninduce performance-degrading artifacts. To address this issue, we propose the\nuse of larger kernel sizes along with the Hamming window function to alleviate\nleakage in CNN architectures. We demonstrate improved classification accuracy\non multiple benchmark datasets including Fashion-MNIST, CIFAR-10, CIFAR-100 and\nImageNet with the simple use of a standard window function in convolutional\nlayers. Finally, we show that CNNs employing the Hamming window display\nincreased robustness against various adversarial attacks.",
          "link": "http://arxiv.org/abs/2101.10143",
          "publishedOn": "2021-07-30T02:13:28.535Z",
          "wordCount": 615,
          "title": "Spectral Leakage and Rethinking the Kernel Size in CNNs. (arXiv:2101.10143v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1\">Juan C. P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeanneret_G/0/1/0/all/0/1\">Guillaume Jeanneret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueda_L/0/1/0/all/0/1\">Laura Rueda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1\">Pablo Arbel&#xe1;ez</a>",
          "description": "Deep learning models are prone to being fooled by imperceptible perturbations\nknown as adversarial attacks. In this work, we study how equipping models with\nTest-time Transformation Ensembling (TTE) can work as a reliable defense\nagainst such attacks. While transforming the input data, both at train and test\ntimes, is known to enhance model performance, its effects on adversarial\nrobustness have not been studied. Here, we present a comprehensive empirical\nstudy of the impact of TTE, in the form of widely-used image transforms, on\nadversarial robustness. We show that TTE consistently improves model robustness\nagainst a variety of powerful attacks without any need for re-training, and\nthat this improvement comes at virtually no trade-off with accuracy on clean\nsamples. Finally, we show that the benefits of TTE transfer even to the\ncertified robustness domain, in which TTE provides sizable and consistent\nimprovements.",
          "link": "http://arxiv.org/abs/2107.14110",
          "publishedOn": "2021-07-30T02:13:28.493Z",
          "wordCount": 588,
          "title": "Enhancing Adversarial Robustness via Test-time Transformation Ensembling. (arXiv:2107.14110v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Houwen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Minghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1\">Hongyang Chao</a>",
          "description": "Relative position encoding (RPE) is important for transformer to capture\nsequence ordering of input tokens. General efficacy has been proven in natural\nlanguage processing. However, in computer vision, its efficacy is not well\nstudied and even remains controversial, e.g., whether relative position\nencoding can work equally well as absolute position? In order to clarify this,\nwe first review existing relative position encoding methods and analyze their\npros and cons when applied in vision transformers. We then propose new relative\nposition encoding methods dedicated to 2D images, called image RPE (iRPE). Our\nmethods consider directional relative distance modeling as well as the\ninteractions between queries and relative position embeddings in self-attention\nmechanism. The proposed iRPE methods are simple and lightweight. They can be\neasily plugged into transformer blocks. Experiments demonstrate that solely due\nto the proposed encoding methods, DeiT and DETR obtain up to 1.5% (top-1 Acc)\nand 1.3% (mAP) stable improvements over their original versions on ImageNet and\nCOCO respectively, without tuning any extra hyperparameters such as learning\nrate and weight decay. Our ablation and analysis also yield interesting\nfindings, some of which run counter to previous understanding. Code and models\nare open-sourced at https://github.com/microsoft/Cream/tree/main/iRPE.",
          "link": "http://arxiv.org/abs/2107.14222",
          "publishedOn": "2021-07-30T02:13:28.486Z",
          "wordCount": 641,
          "title": "Rethinking and Improving Relative Position Encoding for Vision Transformer. (arXiv:2107.14222v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_N/0/1/0/all/0/1\">Nikhil Woodruff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Enshaei_A/0/1/0/all/0/1\">Amir Enshaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_B/0/1/0/all/0/1\">Bashar Awwad Shiekh Hasan</a>",
          "description": "Signatures present on corporate documents are often used in investigations of\nrelationships between persons of interest, and prior research into the task of\noffline signature verification has evaluated a wide range of methods on\nstandard signature datasets. However, such tasks often benefit from prior human\nsupervision in the collection, adjustment and labelling of isolated signature\nimages from which all real-world context has been removed. Signatures found in\nonline document repositories such as the United Kingdom Companies House\nregularly contain high variation in location, size, quality and degrees of\nobfuscation under stamps. We propose an integrated pipeline of signature\nextraction and curation, with no human assistance from the obtaining of company\ndocuments to the clustering of individual signatures. We use a sequence of\nheuristic methods, convolutional neural networks, generative adversarial\nnetworks and convolutional Siamese networks for signature extraction,\nfiltering, cleaning and embedding respectively. We evaluate both the\neffectiveness of the pipeline at matching obscured same-author signature pairs\nand the effectiveness of the entire pipeline against a human baseline for\ndocument signature analysis, as well as presenting uses for such a pipeline in\nthe field of real-world anti-money laundering investigation.",
          "link": "http://arxiv.org/abs/2107.14091",
          "publishedOn": "2021-07-30T02:13:28.479Z",
          "wordCount": 629,
          "title": "Fully-Automatic Pipeline for Document Signature Analysis to Detect Money Laundering Activities. (arXiv:2107.14091v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fangrui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chongruo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>",
          "description": "Semantic segmentation is a challenging problem due to difficulties in\nmodeling context in complex scenes and class confusions along boundaries. Most\nliterature either focuses on context modeling or boundary refinement, which is\nless generalizable in open-world scenarios. In this work, we advocate a unified\nframework(UN-EPT) to segment objects by considering both context information\nand boundary artifacts. We first adapt a sparse sampling strategy to\nincorporate the transformer-based attention mechanism for efficient context\nmodeling. In addition, a separate spatial branch is introduced to capture image\ndetails for boundary refinement. The whole model can be trained in an\nend-to-end manner. We demonstrate promising performance on three popular\nbenchmarks for semantic segmentation with low memory footprint. Code will be\nreleased soon.",
          "link": "http://arxiv.org/abs/2107.14209",
          "publishedOn": "2021-07-30T02:13:28.472Z",
          "wordCount": 558,
          "title": "A Unified Efficient Pyramid Transformer for Semantic Segmentation. (arXiv:2107.14209v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.03972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_H/0/1/0/all/0/1\">Haizhou Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Rui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1\">Zijie Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuang Liu</a>",
          "description": "Estimating 3D poses of multiple humans in real-time is a classic but still\nchallenging task in computer vision. Its major difficulty lies in the ambiguity\nin cross-view association of 2D poses and the huge state space when there are\nmultiple people in multiple views. In this paper, we present a novel solution\nfor multi-human 3D pose estimation from multiple calibrated camera views. It\ntakes 2D poses in different camera coordinates as inputs and aims for the\naccurate 3D poses in the global coordinate. Unlike previous methods that\nassociate 2D poses among all pairs of views from scratch at every frame, we\nexploit the temporal consistency in videos to match the 2D inputs with 3D poses\ndirectly in 3-space. More specifically, we propose to retain the 3D pose for\neach person and update them iteratively via the cross-view multi-human\ntracking. This novel formulation improves both accuracy and efficiency, as we\ndemonstrated on widely-used public datasets. To further verify the scalability\nof our method, we propose a new large-scale multi-human dataset with 12 to 28\ncamera views. Without bells and whistles, our solution achieves 154 FPS on 12\ncameras and 34 FPS on 28 cameras, indicating its ability to handle large-scale\nreal-world applications. The proposed dataset is released at\nhttps://github.com/longcw/crossview_3d_pose_tracking.",
          "link": "http://arxiv.org/abs/2003.03972",
          "publishedOn": "2021-07-30T02:13:28.456Z",
          "wordCount": 703,
          "title": "Cross-View Tracking for Multi-Human 3D Pose Estimation at over 100 FPS. (arXiv:2003.03972v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14175",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Basty_N/0/1/0/all/0/1\">Nicolas Basty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thanaj_M/0/1/0/all/0/1\">Marjola Thanaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cule_M/0/1/0/all/0/1\">Madeleine Cule</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sorokin_E/0/1/0/all/0/1\">Elena P. Sorokin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bell_J/0/1/0/all/0/1\">Jimmy D. Bell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thomas_E/0/1/0/all/0/1\">E. Louise Thomas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Whitcher_B/0/1/0/all/0/1\">Brandon Whitcher</a>",
          "description": "Dixon MRI is widely used for body composition studies. Current processing\nmethods associated with large whole-body volumes are time intensive and prone\nto artifacts during fat-water separation performed on the scanner, making the\ndata difficult to analyse. The most common artifact are fat-water swaps, where\nthe labels are inverted at the voxel level. It is common for researchers to\ndiscard swapped data (generally around 10%), which can be wasteful and lead to\nunintended biases. The UK Biobank is acquiring Dixon MRI for over 100,000\nparticipants, and thousands of swaps will occur. If those go undetected, errors\nwill propagate into processes such as abdominal organ segmentation and dilute\nthe results in population-based analyses. There is a clear need for a fast and\nrobust method to accurately separate fat and water channels. In this work we\npropose such a method based on style transfer using a conditional generative\nadversarial network. We also introduce a new Dixon loss function for the\ngenerator model. Using data from the UK Biobank Dixon MRI, our model is able to\npredict highly accurate fat and water channels that are free from artifacts. We\nshow that the model separates fat and water channels using either single input\n(in-phase) or dual input (in-phase and opposed-phase), with the latter\nproducing improved results. Our proposed method enables faster and more\naccurate downstream analysis of body composition from Dixon MRI in population\nstudies by eliminating the need for visual inspection or discarding data due to\nfat-water swaps.",
          "link": "http://arxiv.org/abs/2107.14175",
          "publishedOn": "2021-07-30T02:13:28.449Z",
          "wordCount": 707,
          "title": "Swap-Free Fat-Water Separation in Dixon MRI using Conditional Generative Adversarial Networks. (arXiv:2107.14175v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1\">Chun-Han Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_W/0/1/0/all/0/1\">Wei-Chih Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>",
          "description": "Reasoning 3D shapes from 2D images is an essential yet challenging task,\nespecially when only single-view images are at our disposal. While an object\ncan have a complicated shape, individual parts are usually close to geometric\nprimitives and thus are easier to model. Furthermore, parts provide a mid-level\nrepresentation that is robust to appearance variations across objects in a\nparticular category. In this work, we tackle the problem of 3D part discovery\nfrom only 2D image collections. Instead of relying on manually annotated parts\nfor supervision, we propose a self-supervised approach, latent part discovery\n(LPD). Our key insight is to learn a novel part shape prior that allows each\npart to fit an object shape faithfully while constrained to have simple\ngeometry. Extensive experiments on the synthetic ShapeNet, PartNet, and\nreal-world Pascal 3D+ datasets show that our method discovers consistent object\nparts and achieves favorable reconstruction accuracy compared to the existing\nmethods with the same level of supervision.",
          "link": "http://arxiv.org/abs/2107.13629",
          "publishedOn": "2021-07-30T02:13:28.442Z",
          "wordCount": 599,
          "title": "Discovering 3D Parts from Image Collections. (arXiv:2107.13629v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Beihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qinmu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Generalized zero-shot learning (GZSL) has achieved significant progress, with\nmany efforts dedicated to overcoming the problems of visual-semantic domain gap\nand seen-unseen bias. However, most existing methods directly use feature\nextraction models trained on ImageNet alone, ignoring the cross-dataset bias\nbetween ImageNet and GZSL benchmarks. Such a bias inevitably results in\npoor-quality visual features for GZSL tasks, which potentially limits the\nrecognition performance on both seen and unseen classes. In this paper, we\npropose a simple yet effective GZSL method, termed feature refinement for\ngeneralized zero-shot learning (FREE), to tackle the above problem. FREE\nemploys a feature refinement (FR) module that incorporates\n\\textit{semantic$\\rightarrow$visual} mapping into a unified generative model to\nrefine the visual features of seen and unseen class samples. Furthermore, we\npropose a self-adaptive margin center loss (SAMC-loss) that cooperates with a\nsemantic cycle-consistency loss to guide FR to learn class- and\nsemantically-relevant representations, and concatenate the features in FR to\nextract the fully refined features. Extensive experiments on five benchmark\ndatasets demonstrate the significant performance gain of FREE over its baseline\nand current state-of-the-art methods. Our codes are available at\nhttps://github.com/shiming-chen/FREE .",
          "link": "http://arxiv.org/abs/2107.13807",
          "publishedOn": "2021-07-30T02:13:28.436Z",
          "wordCount": 633,
          "title": "FREE: Feature Refinement for Generalized Zero-Shot Learning. (arXiv:2107.13807v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+LeBauer_D/0/1/0/all/0/1\">David LeBauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnette_M/0/1/0/all/0/1\">Max Burnette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahlgren_N/0/1/0/all/0/1\">Noah Fahlgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kooper_R/0/1/0/all/0/1\">Rob Kooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McHenry_K/0/1/0/all/0/1\">Kenton McHenry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1\">Abby Stylianou</a>",
          "description": "A core objective of the TERRA-REF project was to generate an open-access\nreference dataset for the study of evaluation of sensing technology to study\nplants under field conditions. The TERRA-REF program deployed a suite of high\nresolution, cutting edge technology sensors on a gantry system with the aim of\nscanning 1 hectare (~$10^4$ m) at around $1 mm^2$ spatial resolution multiple\ntimes per week. The system contains co-located sensors including a stereo-pair\nRGB camera, a thermal imager, a laser scanner to capture 3D structure, and two\nhyperspectral cameras covering wavelengths of 300-2500nm. This sensor data is\nprovided alongside over sixty types of traditional plant measurements that can\nbe used to train new machine learning models. Associated weather and\nenvironmental measurements, information about agronomic management and\nexperimental design, and the genomic sequences of hundreds of plant varieties\nhave been collected and are available alongside the sensor and plant trait\n(phenotype) data.\n\nOver the course of four years and ten growing seasons, the TERRA-REF system\ngenerated over 1 PB of sensor data and almost 45 million files. The subset that\nhas been released to the public domain accounts for two seasons and about half\nof the total data volume. This provides an unprecedented opportunity for\ninvestigations far beyond the core biological scope of the project.\n\nThis focus of this paper is to provide the Computer Vision and Machine\nLearning communities an overview of the available data and some potential\napplications of this one of a kind data.",
          "link": "http://arxiv.org/abs/2107.14072",
          "publishedOn": "2021-07-30T02:13:28.427Z",
          "wordCount": 713,
          "title": "What Does TERRA-REF's High Resolution, Multi Sensor Plant Sensing Public Domain Data Offer the Computer Vision Community?. (arXiv:2107.14072v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jizhizi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Sihan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Recently, there has been an increasing concern about the privacy issue raised\nby using personally identifiable information in machine learning. However,\nprevious portrait matting methods were all based on identifiable portrait\nimages. To fill the gap, we present P3M-10k in this paper, which is the first\nlarge-scale anonymized benchmark for Privacy-Preserving Portrait Matting.\nP3M-10k consists of 10,000 high-resolution face-blurred portrait images along\nwith high-quality alpha mattes. We systematically evaluate both trimap-free and\ntrimap-based matting methods on P3M-10k and find that existing matting methods\nshow different generalization capabilities when following the\nPrivacy-Preserving Training (PPT) setting, i.e., training on face-blurred\nimages and testing on arbitrary images. To devise a better trimap-free portrait\nmatting model, we propose P3M-Net, which leverages the power of a unified\nframework for both semantic perception and detail matting, and specifically\nemphasizes the interaction between them and the encoder to facilitate the\nmatting process. Extensive experiments on P3M-10k demonstrate that P3M-Net\noutperforms the state-of-the-art methods in terms of both objective metrics and\nsubjective visual quality. Besides, it shows good generalization capacity under\nthe PPT setting, confirming the value of P3M-10k for facilitating future\nresearch and enabling potential real-world applications. The source code and\ndataset are available at https://github.com/JizhiziLi/P3M",
          "link": "http://arxiv.org/abs/2104.14222",
          "publishedOn": "2021-07-30T02:13:28.420Z",
          "wordCount": 672,
          "title": "Privacy-Preserving Portrait Matting. (arXiv:2104.14222v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">TianYang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">XiaoJun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1\">Josef Kittler</a>",
          "description": "The Transformer architecture has achieved rapiddevelopment in recent years,\noutperforming the CNN archi-tectures in many computer vision tasks, such as the\nVisionTransformers (ViT) for image classification. However, existingvisual\ntransformer models aim to extract semantic informationfor high-level tasks such\nas classification and detection, distortingthe spatial resolution of the input\nimage, thus sacrificing thecapacity in reconstructing the input or generating\nhigh-resolutionimages. In this paper, therefore, we propose a Patch\nPyramidTransformer(PPT) to effectively address the above issues. Specif-ically,\nwe first design a Patch Transformer to transform theimage into a sequence of\npatches, where transformer encodingis performed for each patch to extract local\nrepresentations.In addition, we construct a Pyramid Transformer to\neffectivelyextract the non-local information from the entire image.\nAfterobtaining a set of multi-scale, multi-dimensional, and multi-anglefeatures\nof the original image, we design the image reconstructionnetwork to ensure that\nthe features can be reconstructed intothe original input. To validate the\neffectiveness, we apply theproposed Patch Pyramid Transformer to the image\nfusion task.The experimental results demonstrate its superior\nperformanceagainst the state-of-the-art fusion approaches, achieving the\nbestresults on several evaluation indicators. The underlying capacityof the PPT\nnetwork is reflected by its universal power in featureextraction and image\nreconstruction, which can be directlyapplied to different image fusion tasks\nwithout redesigning orretraining the network.",
          "link": "http://arxiv.org/abs/2107.13967",
          "publishedOn": "2021-07-30T02:13:28.404Z",
          "wordCount": 671,
          "title": "PPT Fusion: Pyramid Patch Transformerfor a Case Study in Image Fusion. (arXiv:2107.13967v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13718",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Luchuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>",
          "description": "Crowd counting is a challenging task due to the issues such as scale\nvariation and perspective variation in real crowd scenes. In this paper, we\npropose a novel Cascaded Residual Density Network (CRDNet) in a coarse-to-fine\napproach to generate the high-quality density map for crowd counting more\naccurately. (1) We estimate the residual density maps by multi-scale pyramidal\nfeatures through cascaded residual density modules. It can improve the quality\nof density map layer by layer effectively. (2) A novel additional local count\nloss is presented to refine the accuracy of crowd counting, which reduces the\nerrors of pixel-wise Euclidean loss by restricting the number of people in the\nlocal crowd areas. Experiments on two public benchmark datasets show that the\nproposed method achieves effective improvement compared with the\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.13718",
          "publishedOn": "2021-07-30T02:13:28.394Z",
          "wordCount": 566,
          "title": "Cascaded Residual Density Network for Crowd Counting. (arXiv:2107.13718v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chang-Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng-Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_F/0/1/0/all/0/1\">Feng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>",
          "description": "Semantic segmentation models trained on public datasets have achieved great\nsuccess in recent years. However, these models didn't consider the\npersonalization issue of segmentation though it is important in practice. In\nthis paper, we address the problem of personalized image segmentation. The\nobjective is to generate more accurate segmentation results on unlabeled\npersonalized images by investigating the data's personalized traits. To open up\nfuture research in this area, we collect a large dataset containing various\nusers' personalized images called PIS (Personalized Image Semantic\nSegmentation). We also survey some recent researches related to this problem\nand report their performance on our dataset. Furthermore, by observing the\ncorrelation among a user's personalized images, we propose a baseline method\nthat incorporates the inter-image context when segmenting certain images.\nExtensive experiments show that our method outperforms the existing methods on\nthe proposed dataset. The code and the PIS dataset will be made publicly\navailable.",
          "link": "http://arxiv.org/abs/2107.13978",
          "publishedOn": "2021-07-30T02:13:28.375Z",
          "wordCount": 582,
          "title": "Personalized Image Semantic Segmentation. (arXiv:2107.13978v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scabini_L/0/1/0/all/0/1\">Leonardo F. S. Scabini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruno_O/0/1/0/all/0/1\">Odemir M. Bruno</a>",
          "description": "Understanding the behavior of Artificial Neural Networks is one of the main\ntopics in the field recently, as black-box approaches have become usual since\nthe widespread of deep learning. Such high-dimensional models may manifest\ninstabilities and weird properties that resemble complex systems. Therefore, we\npropose Complex Network (CN) techniques to analyze the structure and\nperformance of fully connected neural networks. For that, we build a dataset\nwith 4 thousand models and their respective CN properties. They are employed in\na supervised classification setup considering four vision benchmarks. Each\nneural network is approached as a weighted and undirected graph of neurons and\nsynapses, and centrality measures are computed after training. Results show\nthat these measures are highly related to the network classification\nperformance. We also propose the concept of Bag-Of-Neurons (BoN), a CN-based\napproach for finding topological signatures linking similar neurons. Results\nsuggest that six neuronal types emerge in such networks, independently of the\ntarget domain, and are distributed differently according to classification\naccuracy. We also tackle specific CN properties related to performance, such as\nhigher subgraph centrality on lower-performing models. Our findings suggest\nthat CN properties play a critical role in the performance of fully connected\nneural networks, with topological patterns emerging independently on a wide\nrange of models.",
          "link": "http://arxiv.org/abs/2107.14062",
          "publishedOn": "2021-07-30T02:13:28.368Z",
          "wordCount": 686,
          "title": "Structure and Performance of Fully Connected Neural Networks: Emerging Complex Network Properties. (arXiv:2107.14062v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14185",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hengchang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhifei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhan Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1\">Kui Ren</a>",
          "description": "Transferability of adversarial examples is of central importance for\nattacking an unknown model, which facilitates adversarial attacks in more\npractical scenarios, e.g., blackbox attacks. Existing transferable attacks tend\nto craft adversarial examples by indiscriminately distorting features to\ndegrade prediction accuracy in a source model without aware of intrinsic\nfeatures of objects in the images. We argue that such brute-force degradation\nwould introduce model-specific local optimum into adversarial examples, thus\nlimiting the transferability. By contrast, we propose the Feature\nImportance-aware Attack (FIA), which disrupts important object-aware features\nthat dominate model decisions consistently. More specifically, we obtain\nfeature importance by introducing the aggregate gradient, which averages the\ngradients with respect to feature maps of the source model, computed on a batch\nof random transforms of the original clean image. The gradients will be highly\ncorrelated to objects of interest, and such correlation presents invariance\nacross different models. Besides, the random transforms will preserve intrinsic\nfeatures of objects and suppress model-specific information. Finally, the\nfeature importance guides to search for adversarial examples towards disrupting\ncritical features, achieving stronger transferability. Extensive experimental\nevaluation demonstrates the effectiveness and superior performance of the\nproposed FIA, i.e., improving the success rate by 8.4% against normally trained\nmodels and 11.7% against defense models as compared to the state-of-the-art\ntransferable attacks. Code is available at: https://github.com/hcguoO0/FIA",
          "link": "http://arxiv.org/abs/2107.14185",
          "publishedOn": "2021-07-30T02:13:28.353Z",
          "wordCount": 658,
          "title": "Feature Importance-aware Transferable Adversarial Attacks. (arXiv:2107.14185v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazaheri_A/0/1/0/all/0/1\">Amir Mazaheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>",
          "description": "Video generation is one of the most challenging tasks in Machine Learning and\nComputer Vision fields of study. In this paper, we tackle the text to video\ngeneration problem, which is a conditional form of video generation. Humans can\nlisten/read natural language sentences, and can imagine or visualize what is\nbeing described; therefore, we believe that video generation from natural\nlanguage sentences will have an important impact on Artificial Intelligence.\nVideo generation is relatively a new field of study in Computer Vision, which\nis far from being solved. The majority of recent works deal with synthetic\ndatasets or real datasets with very limited types of objects, scenes, and\nemotions. To the best of our knowledge, this is the very first work on the text\n(free-form sentences) to video generation on more realistic video datasets like\nActor and Action Dataset (A2D) or UCF101. We tackle the complicated problem of\nvideo generation by regressing the latent representations of the first and last\nframes and employing a context-aware interpolation method to build the latent\nrepresentations of in-between frames. We propose a stacking ``upPooling'' block\nto sequentially generate RGB frames out of each latent representation and\nprogressively increase the resolution. Moreover, our proposed Discriminator\nencodes videos based on single and multiple frames. We provide quantitative and\nqualitative results to support our arguments and show the superiority of our\nmethod over well-known baselines like Recurrent Neural Network (RNN) and\nDeconvolution (as known as Convolutional Transpose) based video generation\nmethods.",
          "link": "http://arxiv.org/abs/2107.13766",
          "publishedOn": "2021-07-30T02:13:28.342Z",
          "wordCount": 682,
          "title": "Video Generation from Text Employing Latent Path Construction for Temporal Modeling. (arXiv:2107.13766v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kellenberger_B/0/1/0/all/0/1\">Benjamin Kellenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vargas_Munoz_J/0/1/0/all/0/1\">John E. Vargas-Mu&#xf1;oz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuia_D/0/1/0/all/0/1\">Devis Tuia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daudt_R/0/1/0/all/0/1\">Rodrigo C. Daudt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whelan_T/0/1/0/all/0/1\">Thao T-T Whelan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayo_B/0/1/0/all/0/1\">Brenda Ayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1\">Ferda Ofli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1\">Muhammad Imran</a>",
          "description": "Humanitarian actions require accurate information to efficiently delegate\nsupport operations. Such information can be maps of building footprints,\nbuilding functions, and population densities. While the access to this\ninformation is comparably easy in industrialized countries thanks to reliable\ncensus data and national geo-data infrastructures, this is not the case for\ndeveloping countries, where that data is often incomplete or outdated. Building\nmaps derived from remote sensing images may partially remedy this challenge in\nsuch countries, but are not always accurate due to different landscape\nconfigurations and lack of validation data. Even when they exist, building\nfootprint layers usually do not reveal more fine-grained building properties,\nsuch as the number of stories or the building's function (e.g., office,\nresidential, school, etc.). In this project we aim to automate building\nfootprint and function mapping using heterogeneous data sources. In a first\nstep, we intend to delineate buildings from satellite data, using deep learning\nmodels for semantic image segmentation. Building functions shall be retrieved\nby parsing social media data like for instance tweets, as well as ground-based\nimagery, to automatically identify different buildings functions and retrieve\nfurther information such as the number of building stories. Building maps\naugmented with those additional attributes make it possible to derive more\naccurate population density maps, needed to support the targeted provision of\nhumanitarian aid.",
          "link": "http://arxiv.org/abs/2107.14123",
          "publishedOn": "2021-07-30T02:13:28.335Z",
          "wordCount": 667,
          "title": "Mapping Vulnerable Populations with AI. (arXiv:2107.14123v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wenhang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chunyan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Ancong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hongwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-Shi Zheng</a>",
          "description": "Person re-identification (Re-ID) aims to match person images across\nnon-overlapping camera views. The majority of Re-ID methods focus on\nsmall-scale surveillance systems in which each pedestrian is captured in\ndifferent camera views of adjacent scenes. However, in large-scale surveillance\nsystems that cover larger areas, it is required to track a pedestrian of\ninterest across distant scenes (e.g., a criminal suspect escapes from one city\nto another). Since most pedestrians appear in limited local areas, it is\ndifficult to collect training data with cross-camera pairs of the same person.\nIn this work, we study intra-camera supervised person re-identification across\ndistant scenes (ICS-DS Re-ID), which uses cross-camera unpaired data with\nintra-camera identity labels for training. It is challenging as cross-camera\npaired data plays a crucial role for learning camera-invariant features in most\nexisting Re-ID methods. To learn camera-invariant representation from\ncross-camera unpaired training data, we propose a cross-camera feature\nprediction method to mine cross-camera self supervision information from\ncamera-specific feature distribution by transforming fake cross-camera positive\nfeature pairs and minimize the distances of the fake pairs. Furthermore, we\nautomatically localize and extract local-level feature by a transformer. Joint\nlearning of global-level and local-level features forms a global-local\ncross-camera feature prediction scheme for mining fine-grained cross-camera\nself supervision information. Finally, cross-camera self supervision and\nintra-camera supervision are aggregated in a framework. The experiments are\nconducted in the ICS-DS setting on Market-SCT, Duke-SCT and MSMT17-SCT\ndatasets. The evaluation results demonstrate the superiority of our method,\nwhich gains significant improvements of 15.4 Rank-1 and 22.3 mAP on Market-SCT\nas compared to the second best method.",
          "link": "http://arxiv.org/abs/2107.13904",
          "publishedOn": "2021-07-30T02:13:28.325Z",
          "wordCount": 722,
          "title": "Cross-Camera Feature Prediction for Intra-Camera Supervised Person Re-identification across Distant Scenes. (arXiv:2107.13904v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13833",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Noort_F/0/1/0/all/0/1\">Frieda van den Noort</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sirmacek_B/0/1/0/all/0/1\">Beril Sirmacek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Slump_C/0/1/0/all/0/1\">Cornelis H. Slump</a>",
          "description": "The prevalance of pelvic floor problems is high within the female population.\nTransperineal ultrasound (TPUS) is the main imaging modality used to\ninvestigate these problems. Automating the analysis of TPUS data will help in\ngrowing our understanding of pelvic floor related problems. In this study we\npresent a U-net like neural network with some convolutional long short term\nmemory (CLSTM) layers to automate the 3D segmentation of the levator ani muscle\n(LAM) in TPUS volumes. The CLSTM layers are added to preserve the inter-slice\n3D information. We reach human level performance on this segmentation task.\nTherefore, we conclude that we successfully automated the segmentation of the\nLAM on 3D TPUS data. This paves the way towards automatic in-vivo analysis of\nthe LAM mechanics in the context of large study populations.",
          "link": "http://arxiv.org/abs/2107.13833",
          "publishedOn": "2021-07-30T02:13:28.318Z",
          "wordCount": 587,
          "title": "Recurrent U-net for automatic pelvic floor muscle segmentation on 3D ultrasound. (arXiv:2107.13833v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Luchuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Huihui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>",
          "description": "Abnormal behavior detection in surveillance video is a pivotal part of the\nintelligent city. Most existing methods only consider how to detect anomalies,\nwith less considering to explain the reason of the anomalies. We investigate an\northogonal perspective based on the reason of these abnormal behaviors. To this\nend, we propose a multivariate fusion method that analyzes each target through\nthree branches: object, action and motion. The object branch focuses on the\nappearance information, the motion branch focuses on the distribution of the\nmotion features, and the action branch focuses on the action category of the\ntarget. The information that these branches focus on is different, and they can\ncomplement each other and jointly detect abnormal behavior. The final abnormal\nscore can then be obtained by combining the abnormal scores of the three\nbranches.",
          "link": "http://arxiv.org/abs/2107.13706",
          "publishedOn": "2021-07-30T02:13:28.312Z",
          "wordCount": 569,
          "title": "Abnormal Behavior Detection Based on Target Analysis. (arXiv:2107.13706v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyons_M/0/1/0/all/0/1\">Michael J. Lyons</a>",
          "description": "Twenty-five years ago, my colleagues Miyuki Kamachi and Jiro Gyoba and I\ndesigned and photographed JAFFE, a set of facial expression images intended for\nuse in a study of face perception. In 2019, without seeking permission or\ninforming us, Kate Crawford and Trevor Paglen exhibited JAFFE in two widely\npublicized art shows. In addition, they published a nonfactual account of the\nimages in the essay \"Excavating AI: The Politics of Images in Machine Learning\nTraining Sets.\" The present article recounts the creation of the JAFFE dataset\nand unravels each of Crawford and Paglen's fallacious statements. I also\ndiscuss JAFFE more broadly in connection with research on facial expression,\naffective computing, and human-computer interaction.",
          "link": "http://arxiv.org/abs/2107.13998",
          "publishedOn": "2021-07-30T02:13:28.297Z",
          "wordCount": 572,
          "title": "\"Excavating AI\" Re-excavated: Debunking a Fallacious Account of the JAFFE Dataset. (arXiv:2107.13998v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Loginov_V/0/1/0/all/0/1\">Vladimir Loginov</a>",
          "description": "Recent works in the text recognition area have pushed forward the recognition\nresults to the new horizons. But for a long time a lack of large human-labeled\nnatural text recognition datasets has been forcing researchers to use synthetic\ndata for training text recognition models. Even though synthetic datasets are\nvery large (MJSynth and SynthTest, two most famous synthetic datasets, have\nseveral million images each), their diversity could be insufficient, compared\nto natural datasets like ICDAR and others. Fortunately, the recently released\ntext-recognition annotation for OpenImages V5 dataset has comparable with\nsynthetic dataset number of instances and more diverse examples. We have used\nthis annotation with a Text Recognition head architecture from the Yet Another\nMask Text Spotter and got comparable to the SOTA results. On some datasets we\nhave even outperformed previous SOTA models. In this paper we also introduce a\ntext recognition model. The model's code is available.",
          "link": "http://arxiv.org/abs/2107.13938",
          "publishedOn": "2021-07-30T02:13:28.291Z",
          "wordCount": 586,
          "title": "Why You Should Try the Real Data for the Scene Text Recognition. (arXiv:2107.13938v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1\">Aditya Jyoti Paul</a>",
          "description": "For over hundreds of millions of years, sea turtles and their ancestors have\nswum in the vast expanses of the ocean. They have undergone a number of\nevolutionary changes, leading to speciation and sub-speciation. However, in the\npast few decades, some of the most notable forces driving the genetic variance\nand population decline have been global warming and anthropogenic impact\nranging from large-scale poaching, collecting turtle eggs for food, besides\ndumping trash including plastic waste into the ocean. This leads to severe\ndetrimental effects in the sea turtle population, driving them to extinction.\nThis research focusses on the forces causing the decline in sea turtle\npopulation, the necessity for the global conservation efforts along with its\nsuccesses and failures, followed by an in-depth analysis of the modern advances\nin detection and recognition of sea turtles, involving Machine Learning and\nComputer Vision systems, aiding the conservation efforts.",
          "link": "http://arxiv.org/abs/2107.14061",
          "publishedOn": "2021-07-30T02:13:28.233Z",
          "wordCount": 610,
          "title": "The Need and Status of Sea Turtle Conservation and Survey of Associated Computer Vision Advances. (arXiv:2107.14061v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huan_J/0/1/0/all/0/1\">Jun Huan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>",
          "description": "While deep learning succeeds in a wide range of tasks, it highly depends on\nthe massive collection of annotated data which is expensive and time-consuming.\nTo lower the cost of data annotation, active learning has been proposed to\ninteractively query an oracle to annotate a small proportion of informative\nsamples in an unlabeled dataset. Inspired by the fact that the samples with\nhigher loss are usually more informative to the model than the samples with\nlower loss, in this paper we present a novel deep active learning approach that\nqueries the oracle for data annotation when the unlabeled sample is believed to\nincorporate high loss. The core of our approach is a measurement Temporal\nOutput Discrepancy (TOD) that estimates the sample loss by evaluating the\ndiscrepancy of outputs given by models at different optimization steps. Our\ntheoretical investigation shows that TOD lower-bounds the accumulated sample\nloss thus it can be used to select informative unlabeled samples. On basis of\nTOD, we further develop an effective unlabeled data sampling strategy as well\nas an unsupervised learning criterion that enhances model performance by\nincorporating the unlabeled data. Due to the simplicity of TOD, our active\nlearning approach is efficient, flexible, and task-agnostic. Extensive\nexperimental results demonstrate that our approach achieves superior\nperformances than the state-of-the-art active learning methods on image\nclassification and semantic segmentation tasks.",
          "link": "http://arxiv.org/abs/2107.14153",
          "publishedOn": "2021-07-30T02:13:28.218Z",
          "wordCount": 673,
          "title": "Semi-Supervised Active Learning with Temporal Output Discrepancy. (arXiv:2107.14153v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jizong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1\">Chrisitian Desrosiers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>",
          "description": "Pre-training a recognition model with contrastive learning on a large dataset\nof unlabeled data has shown great potential to boost the performance of a\ndownstream task, e.g., image classification. However, in domains such as\nmedical imaging, collecting unlabeled data can be challenging and expensive. In\nthis work, we propose to adapt contrastive learning to work with meta-label\nannotations, for improving the model's performance in medical image\nsegmentation even when no additional unlabeled data is available. Meta-labels\nsuch as the location of a 2D slice in a 3D MRI scan or the type of device used,\noften come for free during the acquisition process. We use the meta-labels for\npre-training the image encoder as well as to regularize a semi-supervised\ntraining, in which a reduced set of annotated data is used for training.\nFinally, to fully exploit the weak annotations, a self-paced learning approach\nis used to help the learning and discriminate useful labels from noise. Results\non three different medical image segmentation datasets show that our approach:\ni) highly boosts the performance of a model trained on a few scans, ii)\noutperforms previous contrastive and semi-supervised approaches, and iii)\nreaches close to the performance of a model trained on the full data.",
          "link": "http://arxiv.org/abs/2107.13741",
          "publishedOn": "2021-07-30T02:13:28.205Z",
          "wordCount": 639,
          "title": "Self-Paced Contrastive Learning for Semi-supervisedMedical Image Segmentation with Meta-labels. (arXiv:2107.13741v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Forecasting human trajectories in complex dynamic environments plays a\ncritical role in autonomous vehicles and intelligent robots. Most existing\nmethods learn to predict future trajectories by behavior clues from history\ntrajectories and interaction clues from environments. However, the inherent\nbias between training and deployment environments is ignored. Hence, we propose\na counterfactual analysis method for human trajectory prediction to investigate\nthe causality between the predicted trajectories and input clues and alleviate\nthe negative effects brought by environment bias. We first build a causal graph\nfor trajectory forecasting with history trajectory, future trajectory, and the\nenvironment interactions. Then, we cut off the inference from environment to\ntrajectory by constructing the counterfactual intervention on the trajectory\nitself. Finally, we compare the factual and counterfactual trajectory clues to\nalleviate the effects of environment bias and highlight the trajectory clues.\nOur counterfactual analysis is a plug-and-play module that can be applied to\nany baseline prediction methods including RNN- and CNN-based ones. We show that\nour method achieves consistent improvement for different baselines and obtains\nthe state-of-the-art results on public pedestrian trajectory forecasting\nbenchmarks.",
          "link": "http://arxiv.org/abs/2107.14202",
          "publishedOn": "2021-07-30T02:13:28.151Z",
          "wordCount": 620,
          "title": "Human Trajectory Prediction via Counterfactual Analysis. (arXiv:2107.14202v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13820",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ching/0/1/0/all/0/1\">Ching</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_K/0/1/0/all/0/1\">Kai Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao/0/1/0/all/0/1\">Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_J/0/1/0/all/0/1\">Jerry Chang</a>, Yun, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_C/0/1/0/all/0/1\">Chien Cheng</a>",
          "description": "The purpose of this study is to differentiate malignant and benign\nmediastinal lesions by using the three-dimensional convolutional neural network\nthrough the endobronchial ultrasound (EBUS) image. Compared with previous\nstudy, our proposed model is robust to noise and able to fuse various imaging\nfeatures and spatiotemporal features of EBUS videos. Endobronchial\nultrasound-guided transbronchial needle aspiration (EBUS-TBNA) is a diagnostic\ntool for intrathoracic lymph nodes. Physician can observe the characteristics\nof the lesion using grayscale mode, doppler mode, and elastography during the\nprocedure. To process the EBUS data in the form of a video and appropriately\nintegrate the features of multiple imaging modes, we used a time-series\nthree-dimensional convolutional neural network (3D CNN) to learn the\nspatiotemporal features and design a variety of architectures to fuse each\nimaging mode. Our model (Res3D_UDE) took grayscale mode, Doppler mode, and\nelastography as training data and achieved an accuracy of 82.00% and area under\nthe curve (AUC) of 0.83 on the validation set. Compared with previous study, we\ndirectly used videos recorded during procedure as training and validation data,\nwithout additional manual selection, which might be easier for clinical\napplication. In addition, model designed with 3D CNN can also effectively learn\nspatiotemporal features and improve accuracy. In the future, our model may be\nused to guide physicians to quickly and correctly find the target lesions for\nslice sampling during the inspection process, reduce the number of slices of\nbenign lesions, and shorten the inspection time.",
          "link": "http://arxiv.org/abs/2107.13820",
          "publishedOn": "2021-07-30T02:13:28.133Z",
          "wordCount": 707,
          "title": "The interpretation of endobronchial ultrasound image using 3D convolutional neural network for differentiating malignant and benign mediastinal lesions. (arXiv:2107.13820v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yanqing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_B/0/1/0/all/0/1\">Bangning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Miaogen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yanlong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chunliu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Juan Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Changyu Shen</a>",
          "description": "Multiple optical scattering occurs when light propagates in a non-uniform\nmedium. During the multiple scattering, images were distorted and the spatial\ninformation they carried became scrambled. However, the image information is\nnot lost but presents in the form of speckle patterns (SPs). In this study, we\nbuilt up an optical random scattering system based on an LCD and an RGB laser\nsource. We found that the image classification can be improved by the help of\nrandom scattering which is considered as a feedforward neural network to\nextracts features from image. Along with the ridge classification deployed on\ncomputer, we achieved excellent classification accuracy higher than 94%, for a\nvariety of data sets covering medical, agricultural, environmental protection\nand other fields. In addition, the proposed optical scattering system has the\nadvantages of high speed, low power consumption, and miniaturization, which is\nsuitable for deploying in edge computing applications.",
          "link": "http://arxiv.org/abs/2107.14051",
          "publishedOn": "2021-07-30T02:13:27.993Z",
          "wordCount": 600,
          "title": "Improvement of image classification by multiple optical scattering. (arXiv:2107.14051v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinzhu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianzhu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yating Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>",
          "description": "Geometry Projection is a powerful depth estimation method in monocular 3D\nobject detection. It estimates depth dependent on heights, which introduces\nmathematical priors into the deep model. But projection process also introduces\nthe error amplification problem, in which the error of the estimated height\nwill be amplified and reflected greatly at the output depth. This property\nleads to uncontrollable depth inferences and also damages the training\nefficiency. In this paper, we propose a Geometry Uncertainty Projection Network\n(GUP Net) to tackle the error amplification problem at both inference and\ntraining stages. Specifically, a GUP module is proposed to obtains the\ngeometry-guided uncertainty of the inferred depth, which not only provides high\nreliable confidence for each depth but also benefits depth learning.\nFurthermore, at the training stage, we propose a Hierarchical Task Learning\nstrategy to reduce the instability caused by error amplification. This learning\nalgorithm monitors the learning situation of each task by a proposed indicator\nand adaptively assigns the proper loss weights for different tasks according to\ntheir pre-tasks situation. Based on that, each task starts learning only when\nits pre-tasks are learned well, which can significantly improve the stability\nand efficiency of the training process. Extensive experiments demonstrate the\neffectiveness of the proposed method. The overall model can infer more reliable\nobject depth than existing methods and outperforms the state-of-the-art\nimage-based monocular 3D detectors by 3.74% and 4.7% AP40 of the car and\npedestrian categories on the KITTI benchmark.",
          "link": "http://arxiv.org/abs/2107.13774",
          "publishedOn": "2021-07-30T02:13:27.987Z",
          "wordCount": 691,
          "title": "Geometry Uncertainty Projection Network for Monocular 3D Object Detection. (arXiv:2107.13774v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elhagry_A/0/1/0/all/0/1\">Ahmed Elhagry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gla_R/0/1/0/all/0/1\">Rawan Gla</a>",
          "description": "Sign language is a set of gestures that deaf people use to communicate.\nUnfortunately, normal people don't understand it, which creates a communication\ngap that needs to be filled. Because of the variations in (Egyptian Sign\nLanguage) ESL from one region to another, ESL provides a challenging research\nproblem. In this work, we are providing applied research with its video-based\nEgyptian sign language recognition system that serves the local community of\ndeaf people in Egypt, with a moderate and reasonable accuracy. We present a\ncomputer vision system with two different neural networks architectures. The\nfirst is a Convolutional Neural Network (CNN) for extracting spatial features.\nThe CNN model was retrained on the inception mod. The second architecture is a\nCNN followed by a Long Short-Term Memory (LSTM) for extracting both spatial and\ntemporal features. The two models achieved an accuracy of 90% and 72%,\nrespectively. We examined the power of these two architectures to distinguish\nbetween 9 common words (with similar signs) among some deaf people community in\nEgypt.",
          "link": "http://arxiv.org/abs/2107.13647",
          "publishedOn": "2021-07-30T02:13:27.981Z",
          "wordCount": 599,
          "title": "Egyptian Sign Language Recognition Using CNN and LSTM. (arXiv:2107.13647v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13703",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Afshari_M/0/1/0/all/0/1\">Mehdi Afshari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tizhoosh_H/0/1/0/all/0/1\">H.R. Tizhoosh</a>",
          "description": "Histopathology digital scans are large-size images that contain valuable\ninformation at the pixel level. Content-based comparison of these images is a\nchallenging task. This study proposes a content-based similarity measure for\nhigh-resolution gigapixel histopathology images. The proposed similarity\nmeasure is an expansion of cosine vector similarity to a matrix. Each image is\ndivided into same-size patches with a meaningful amount of information (i.e.,\ncontained enough tissue). The similarity is measured by the extraction of\npatch-level deep embeddings of the last pooling layer of a pre-trained deep\nmodel at four different magnification levels, namely, 1x, 2.5x, 5x, and 10x\nmagnifications. In addition, for faster measurement, embedding reduction is\ninvestigated. Finally, to assess the proposed method, an image search method is\nimplemented. Results show that the similarity measure represents the slide\nlabels with a maximum accuracy of 93.18\\% for top-5 search at 5x magnification.",
          "link": "http://arxiv.org/abs/2107.13703",
          "publishedOn": "2021-07-30T02:13:27.974Z",
          "wordCount": 589,
          "title": "A Similarity Measure of Histopathology Images by Deep Embeddings. (arXiv:2107.13703v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shan_W/0/1/0/all/0/1\">Wenkang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>",
          "description": "Most of the existing 3D human pose estimation approaches mainly focus on\npredicting 3D positional relationships between the root joint and other human\njoints (local motion) instead of the overall trajectory of the human body\n(global motion). Despite the great progress achieved by these approaches, they\nare not robust to global motion, and lack the ability to accurately predict\nlocal motion with a small movement range. To alleviate these two problems, we\npropose a relative information encoding method that yields positional and\ntemporal enhanced representations. Firstly, we encode positional information by\nutilizing relative coordinates of 2D poses to enhance the consistency between\nthe input and output distribution. The same posture with different absolute 2D\npositions can be mapped to a common representation. It is beneficial to resist\nthe interference of global motion on the prediction results. Second, we encode\ntemporal information by establishing the connection between the current pose\nand other poses of the same person within a period of time. More attention will\nbe paid to the movement changes before and after the current pose, resulting in\nbetter prediction performance on local motion with a small movement range. The\nablation studies validate the effectiveness of the proposed relative\ninformation encoding method. Besides, we introduce a multi-stage optimization\nmethod to the whole framework to further exploit the positional and temporal\nenhanced representations. Our method outperforms state-of-the-art methods on\ntwo public datasets. Code is available at\nhttps://github.com/paTRICK-swk/Pose3D-RIE.",
          "link": "http://arxiv.org/abs/2107.13994",
          "publishedOn": "2021-07-30T02:13:27.957Z",
          "wordCount": 705,
          "title": "Improving Robustness and Accuracy via Relative Information Encoding in 3D Human Pose Estimation. (arXiv:2107.13994v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruicong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Feng Lu</a>",
          "description": "Deep neural networks have significantly improved appearance-based gaze\nestimation accuracy. However, it still suffers from unsatisfactory performance\nwhen generalizing the trained model to new domains, e.g., unseen environments\nor persons. In this paper, we propose a plug-and-play gaze adaptation framework\n(PnP-GA), which is an ensemble of networks that learn collaboratively with the\nguidance of outliers. Since our proposed framework does not require\nground-truth labels in the target domain, the existing gaze estimation networks\ncan be directly plugged into PnP-GA and generalize the algorithms to new\ndomains. We test PnP-GA on four gaze domain adaptation tasks, ETH-to-MPII,\nETH-to-EyeDiap, Gaze360-to-MPII, and Gaze360-to-EyeDiap. The experimental\nresults demonstrate that the PnP-GA framework achieves considerable performance\nimprovements of 36.9%, 31.6%, 19.4%, and 11.8% over the baseline system. The\nproposed framework also outperforms the state-of-the-art domain adaptation\napproaches on gaze domain adaptation tasks. Code has been released at\nhttps://github.com/DreamtaleCore/PnP-GA.",
          "link": "http://arxiv.org/abs/2107.13780",
          "publishedOn": "2021-07-30T02:13:27.942Z",
          "wordCount": 586,
          "title": "Generalizing Gaze Estimation with Outlier-guided Collaborative Adaptation. (arXiv:2107.13780v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chuanguang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1\">Zhulin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Linhang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongjun Xu</a>",
          "description": "Knowledge distillation often involves how to define and transfer knowledge\nfrom teacher to student effectively. Although recent self-supervised\ncontrastive knowledge achieves the best performance, forcing the network to\nlearn such knowledge may damage the representation learning of the original\nclass recognition task. We therefore adopt an alternative self-supervised\naugmented task to guide the network to learn the joint distribution of the\noriginal recognition task and self-supervised auxiliary task. It is\ndemonstrated as a richer knowledge to improve the representation power without\nlosing the normal classification capability. Moreover, it is incomplete that\nprevious methods only transfer the probabilistic knowledge between the final\nlayers. We propose to append several auxiliary classifiers to hierarchical\nintermediate feature maps to generate diverse self-supervised knowledge and\nperform the one-to-one transfer to teach the student network thoroughly. Our\nmethod significantly surpasses the previous SOTA SSKD with an average\nimprovement of 2.56\\% on CIFAR-100 and an improvement of 0.77\\% on ImageNet\nacross widely used network pairs. Codes are available at\nhttps://github.com/winycg/HSAKD.",
          "link": "http://arxiv.org/abs/2107.13715",
          "publishedOn": "2021-07-30T02:13:27.915Z",
          "wordCount": 599,
          "title": "Hierarchical Self-supervised Augmented Knowledge Distillation. (arXiv:2107.13715v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iwanowski_M/0/1/0/all/0/1\">Marcin Iwanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzabka_M/0/1/0/all/0/1\">Marcin Grzabka</a>",
          "description": "The paper describes a method for measuring the similarity and symmetry of an\nimage annotated with bounding boxes indicating image objects. The latter\nrepresentation became popular recently due to the rapid development of fast and\nefficient deep-learning-based object-detection methods. The proposed approach\nallows for comparing sets of bounding boxes to estimate the degree of\nsimilarity of their underlying images. It is based on the fuzzy approach that\nuses the fuzzy mutual position (FMP) matrix to describe spatial composition and\nrelations between bounding boxes within an image. A method of computing the\nsimilarity of two images described by their FMP matrices is proposed and the\nalgorithm of its computation. It outputs the single scalar value describing the\ndegree of content-based image similarity. By modifying the method`s parameters,\ninstead of similarity, the reflectional symmetry of object composition may also\nbe measured. The proposed approach allows for measuring differences in objects`\ncomposition of various intensities. It is also invariant to translation and\nscaling and - in case of symmetry detection - position and orientation of the\nsymmetry axis. A couple of examples illustrate the method.",
          "link": "http://arxiv.org/abs/2107.13651",
          "publishedOn": "2021-07-30T02:13:27.909Z",
          "wordCount": 649,
          "title": "Similarity and symmetry measures based on fuzzy descriptors of image objects` composition. (arXiv:2107.13651v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1\">Chongyang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1\">Xiaoxue Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkara_S/0/1/0/all/0/1\">Srinivas Sunkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Abhinav Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jindong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arcas_B/0/1/0/all/0/1\">Blaise Aguera y Arcas</a>",
          "description": "To improve the accessibility of smart devices and to simplify their usage,\nbuilding models which understand user interfaces (UIs) and assist users to\ncomplete their tasks is critical. However, unique challenges are proposed by\nUI-specific characteristics, such as how to effectively leverage multimodal UI\nfeatures that involve image, text, and structural metadata and how to achieve\ngood performance when high-quality labeled data is unavailable. To address such\nchallenges we introduce UIBert, a transformer-based joint image-text model\ntrained through novel pre-training tasks on large-scale unlabeled UI data to\nlearn generic feature representations for a UI and its components. Our key\nintuition is that the heterogeneous features in a UI are self-aligned, i.e.,\nthe image and text features of UI components, are predictive of each other. We\npropose five pretraining tasks utilizing this self-alignment among different\nfeatures of a UI component and across various components in the same UI. We\nevaluate our method on nine real-world downstream UI tasks where UIBert\noutperforms strong multimodal baselines by up to 9.26% accuracy.",
          "link": "http://arxiv.org/abs/2107.13731",
          "publishedOn": "2021-07-30T02:13:27.894Z",
          "wordCount": 620,
          "title": "UIBert: Learning Generic Multimodal Representations for UI Understanding. (arXiv:2107.13731v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13742",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taherkhani_F/0/1/0/all/0/1\">Fariborz Taherkhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talreja_V/0/1/0/all/0/1\">Veeru Talreja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valenti_M/0/1/0/all/0/1\">Matthew C. Valenti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "In recent years, with the advent of deep-learning, face recognition has\nachieved exceptional success. However, many of these deep face recognition\nmodels perform much better in handling frontal faces compared to profile faces.\nThe major reason for poor performance in handling of profile faces is that it\nis inherently difficult to learn pose-invariant deep representations that are\nuseful for profile face recognition. In this paper, we hypothesize that the\nprofile face domain possesses a latent connection with the frontal face domain\nin a latent feature subspace. We look to exploit this latent connection by\nprojecting the profile faces and frontal faces into a common latent subspace\nand perform verification or retrieval in the latent domain. We leverage a\ncoupled conditional generative adversarial network (cpGAN) structure to find\nthe hidden relationship between the profile and frontal images in a latent\ncommon embedding subspace. Specifically, the cpGAN framework consists of two\nconditional GAN-based sub-networks, one dedicated to the frontal domain and the\nother dedicated to the profile domain. Each sub-network tends to find a\nprojection that maximizes the pair-wise correlation between the two feature\ndomains in a common embedding feature subspace. The efficacy of our approach\ncompared with the state-of-the-art is demonstrated using the CFP, CMU\nMulti-PIE, IJB-A, and IJB-C datasets. Additionally, we have also implemented a\ncoupled convolutional neural network (cpCNN) and an adversarial discriminative\ndomain adaptation network (ADDA) for profile to frontal face recognition. We\nhave evaluated the performance of cpCNN and ADDA and compared it with the\nproposed cpGAN. Finally, we have also evaluated our cpGAN for reconstruction of\nfrontal faces from input profile faces contained in the VGGFace2 dataset.",
          "link": "http://arxiv.org/abs/2107.13742",
          "publishedOn": "2021-07-30T02:13:27.888Z",
          "wordCount": 731,
          "title": "Profile to Frontal Face Recognition in the Wild Using Coupled Conditional GAN. (arXiv:2107.13742v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kloeker_L/0/1/0/all/0/1\">Laurent Kloeker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kloeker_A/0/1/0/all/0/1\">Amarin Kloeker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomsen_F/0/1/0/all/0/1\">Fabian Thomsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erraji_A/0/1/0/all/0/1\">Armin Erraji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_L/0/1/0/all/0/1\">Lutz Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamberty_S/0/1/0/all/0/1\">Serge Lamberty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_A/0/1/0/all/0/1\">Adrian Fazekas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kallo_E/0/1/0/all/0/1\">Eszter Kall&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oeser_M/0/1/0/all/0/1\">Markus Oeser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flechon_C/0/1/0/all/0/1\">Charlotte Fl&#xe9;chon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lohmiller_J/0/1/0/all/0/1\">Jochen Lohmiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_P/0/1/0/all/0/1\">Pascal Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sommer_M/0/1/0/all/0/1\">Martin Sommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winter_H/0/1/0/all/0/1\">Helen Winter</a>",
          "description": "With the Corridor for New Mobility Aachen - D\\\"usseldorf, an integrated\ndevelopment environment is created, incorporating existing test capabilities,\nto systematically test and validate automated vehicles in interaction with\nconnected Intelligent Transport Systems Stations (ITS-Ss). This is achieved\nthrough a time- and cost-efficient toolchain and methodology, in which\nsimulation, closed test sites as well as test fields in public transport are\nlinked in the best possible way. By implementing a digital twin, the recorded\ntraffic events can be visualized in real-time and driving functions can be\ntested in the simulation based on real data. In order to represent diverse\ntraffic scenarios, the corridor contains a highway section, a rural area, and\nurban areas. First, this paper outlines the project goals before describing the\nindividual project contents in more detail. These include the concepts of\ntraffic detection, driving function development, digital twin development, and\npublic involvement.",
          "link": "http://arxiv.org/abs/2107.14048",
          "publishedOn": "2021-07-30T02:13:27.880Z",
          "wordCount": 613,
          "title": "Corridor for new mobility Aachen-D\\\"usseldorf: Methods and concepts of the research project ACCorD. (arXiv:2107.14048v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhiyuan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yaohai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ruisong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yayu Gao</a>",
          "description": "In this paper, we propose an efficient human pose estimation network -- SFM\n(slender fusion model) by fusing multi-level features and adding lightweight\nattention blocks -- HSA (High-Level Spatial Attention). Many existing methods\non efficient network have already taken feature fusion into consideration,\nwhich largely boosts the performance. However, its performance is far inferior\nto large network such as ResNet and HRNet due to its limited fusion operation\nin the network. Specifically, we expand the number of fusion operation by\nbuilding bridges between two pyramid frameworks without adding layers.\nMeanwhile, to capture long-range dependency, we propose a lightweight attention\nblock -- HSA, which computes second-order attention map. In summary, SFM\nmaximizes the number of feature fusion in a limited number of layers. HSA\nlearns high precise spatial information by computing the attention of spatial\nattention map. With the help of SFM and HSA, our network is able to generate\nmulti-level feature and extract precise global spatial information with little\ncomputing resource. Thus, our method achieve comparable or even better accuracy\nwith less parameters and computational cost. Our SFM achieve 89.0 in PCKh@0.5,\n42.0 in PCKh@0.1 on MPII validation set and 71.7 in AP, 90.7 in AP@0.5 on COCO\nvalidation with only 1.7G FLOPs and 1.5M parameters. The source code will be\npublic soon.",
          "link": "http://arxiv.org/abs/2107.13693",
          "publishedOn": "2021-07-30T02:13:27.858Z",
          "wordCount": 657,
          "title": "Efficient Human Pose Estimation by Maximizing Fusion and High-Level Spatial Attention. (arXiv:2107.13693v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yu Cheng Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsougenis_E/0/1/0/all/0/1\">Efstratios Tsougenis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsui_K/0/1/0/all/0/1\">Kwok-Leung Tsui</a>",
          "description": "Counting the repetition of human exercise and physical rehabilitation is a\ncommon task in rehabilitation and exercise training. The existing vision-based\nrepetition counting methods less emphasize the concurrent motions in the same\nvideo. This work presents a vision-based human motion repetition counting\napplicable to counting concurrent motions through the skeleton location\nextracted from various pose estimation methods. The presented method was\nvalidated on the University of Idaho Physical Rehabilitation Movements Data Set\n(UI-PRMD), and MM-fit dataset. The overall mean absolute error (MAE) for mm-fit\nwas 0.06 with off-by-one Accuracy (OBOA) 0.94. Overall MAE for UI-PRMD dataset\nwas 0.06 with OBOA 0.95. We have also tested the performance in a variety of\ncamera locations and concurrent motions with conveniently collected video with\noverall MAE 0.06 and OBOA 0.88. The proposed method provides a view-angle and\nmotion agnostic concurrent motion counting. This method can potentially use in\nlarge-scale remote rehabilitation and exercise training with only one camera.",
          "link": "http://arxiv.org/abs/2107.13760",
          "publishedOn": "2021-07-30T02:13:27.846Z",
          "wordCount": 589,
          "title": "Viewpoint-Invariant Exercise Repetition Counting. (arXiv:2107.13760v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13643",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elhagry_A/0/1/0/all/0/1\">Ahmed Elhagry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeed_M/0/1/0/all/0/1\">Mohamed Saeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araia_M/0/1/0/all/0/1\">Musie Araia</a>",
          "description": "Human pose estimation (HPE) is one of the most challenging tasks in computer\nvision as humans are deformable by nature and thus their pose has so much\nvariance. HPE aims to correctly identify the main joint locations of a single\nperson or multiple people in a given image or video. Locating joints of a\nperson in images or videos is an important task that can be applied in action\nrecognition and object tracking. As have many computer vision tasks, HPE has\nadvanced massively with the introduction of deep learning to the field. In this\npaper, we focus on one of the deep learning-based approaches of HPE proposed by\nNewell et al., which they named the stacked hourglass network. Their approach\nis widely used in many applications and is regarded as one of the best works in\nthis area. The main focus of their approach is to capture as much information\nas it can at all possible scales so that a coherent understanding of the local\nfeatures and full-body location is achieved. Their findings demonstrate that\nimportant cues such as orientation of a person, arrangement of limbs, and\nadjacent joints' relative location can be identified from multiple scales at\ndifferent resolutions. To do so, they makes use of a single pipeline to process\nimages in multiple resolutions, which comprises a skip layer to not lose\nspatial information at each resolution. The resolution of the images stretches\nas lower as 4x4 to make sure that a smaller spatial feature is included. In\nthis study, we study the effect of architectural modifications on the\ncomputational speed and accuracy of the network.",
          "link": "http://arxiv.org/abs/2107.13643",
          "publishedOn": "2021-07-30T02:13:27.840Z",
          "wordCount": 703,
          "title": "Lighter Stacked Hourglass Human Pose Estimation. (arXiv:2107.13643v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xinyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dongjin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuncong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengzhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingchao Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>",
          "description": "Detecting abnormal activities in real-world surveillance videos is an\nimportant yet challenging task as the prior knowledge about video anomalies is\nusually limited or unavailable. Despite that many approaches have been\ndeveloped to resolve this problem, few of them can capture the normal\nspatio-temporal patterns effectively and efficiently. Moreover, existing works\nseldom explicitly consider the local consistency at frame level and global\ncoherence of temporal dynamics in video sequences. To this end, we propose\nConvolutional Transformer based Dual Discriminator Generative Adversarial\nNetworks (CT-D2GAN) to perform unsupervised video anomaly detection.\nSpecifically, we first present a convolutional transformer to perform future\nframe prediction. It contains three key components, i.e., a convolutional\nencoder to capture the spatial information of the input video clips, a temporal\nself-attention module to encode the temporal dynamics, and a convolutional\ndecoder to integrate spatio-temporal features and predict the future frame.\nNext, a dual discriminator based adversarial training procedure, which jointly\nconsiders an image discriminator that can maintain the local consistency at\nframe-level and a video discriminator that can enforce the global coherence of\ntemporal dynamics, is employed to enhance the future frame prediction. Finally,\nthe prediction error is used to identify abnormal video frames. Thoroughly\nempirical studies on three public video anomaly detection datasets, i.e., UCSD\nPed2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the effectiveness of\nthe proposed adversarial spatio-temporal modeling framework.",
          "link": "http://arxiv.org/abs/2107.13720",
          "publishedOn": "2021-07-30T02:13:27.812Z",
          "wordCount": 708,
          "title": "Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for Video Anomaly Detection. (arXiv:2107.13720v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chengkuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Ming Y. Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_D/0/1/0/all/0/1\">Drew F. K. Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tiffany Y. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaumberg_A/0/1/0/all/0/1\">Andrew J. Schaumberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1\">Faisal Mahmood</a>",
          "description": "The expanding adoption of digital pathology has enabled the curation of large\nrepositories of histology whole slide images (WSIs), which contain a wealth of\ninformation. Similar pathology image search offers the opportunity to comb\nthrough large historical repositories of gigapixel WSIs to identify cases with\nsimilar morphological features and can be particularly useful for diagnosing\nrare diseases, identifying similar cases for predicting prognosis, treatment\noutcomes, and potential clinical trial success. A critical challenge in\ndeveloping a WSI search and retrieval system is scalability, which is uniquely\nchallenging given the need to search a growing number of slides that each can\nconsist of billions of pixels and are several gigabytes in size. Such systems\nare typically slow and retrieval speed often scales with the size of the\nrepository they search through, making their clinical adoption tedious and are\nnot feasible for repositories that are constantly growing. Here we present Fast\nImage Search for Histopathology (FISH), a histology image search pipeline that\nis infinitely scalable and achieves constant search speed that is independent\nof the image database size while being interpretable and without requiring\ndetailed annotations. FISH uses self-supervised deep learning to encode\nmeaningful representations from WSIs and a Van Emde Boas tree for fast search,\nfollowed by an uncertainty-based ranking algorithm to retrieve similar WSIs. We\nevaluated FISH on multiple tasks and datasets with over 22,000 patient cases\nspanning 56 disease subtypes. We additionally demonstrate that FISH can be used\nto assist with the diagnosis of rare cancer types where sufficient cases may\nnot be available to train traditional supervised deep models. FISH is available\nas an easy-to-use, open-source software package\n(https://github.com/mahmoodlab/FISH).",
          "link": "http://arxiv.org/abs/2107.13587",
          "publishedOn": "2021-07-30T02:13:27.802Z",
          "wordCount": 725,
          "title": "Fast and Scalable Image Search For Histology. (arXiv:2107.13587v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadakis_M/0/1/0/all/0/1\">Manolis Fragkiadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putten_P/0/1/0/all/0/1\">Peter van der Putten</a>",
          "description": "Sign language lexica are a useful resource for researchers and people\nlearning sign languages. Current implementations allow a user to search a sign\neither by its gloss or by selecting its primary features such as handshape and\nlocation. This study focuses on exploring a reverse search functionality where\na user can sign a query sign in front of a webcam and retrieve a set of\nmatching signs. By extracting different body joints combinations (upper body,\ndominant hand's arm and wrist) using the pose estimation framework OpenPose, we\ncompare four techniques (PCA, UMAP, DTW and Euclidean distance) as distance\nmetrics between 20 query signs, each performed by eight participants on a 1200\nsign lexicon. The results show that UMAP and DTW can predict a matching sign\nwith an 80\\% and 71\\% accuracy respectively at the top-20 retrieved signs using\nthe movement of the dominant hand arm. Using DTW and adding more sign instances\nfrom other participants in the lexicon, the accuracy can be raised to 90\\% at\nthe top-10 ranking. Our results suggest that our methodology can be used with\nno training in any sign language lexicon regardless of its size.",
          "link": "http://arxiv.org/abs/2107.13637",
          "publishedOn": "2021-07-30T02:13:27.788Z",
          "wordCount": 660,
          "title": "Sign and Search: Sign Search Functionality for Sign Language Lexica. (arXiv:2107.13637v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiali Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>",
          "description": "The fact that there exists a gap between low-level features and semantic\nmeanings of images, called the semantic gap, is known for decades. Resolution\nof the semantic gap is a long standing problem. The semantic gap problem is\nreviewed and a survey on recent efforts in bridging the gap is made in this\nwork. Most importantly, we claim that the semantic gap is primarily bridged\nthrough supervised learning today. Experiences are drawn from two application\ndomains to illustrate this point: 1) object detection and 2) metric learning\nfor content-based image retrieval (CBIR). To begin with, this paper offers a\nhistorical retrospective on supervision, makes a gradual transition to the\nmodern data-driven methodology and introduces commonly used datasets. Then, it\nsummarizes various supervision methods to bridge the semantic gap in the\ncontext of object detection and metric learning.",
          "link": "http://arxiv.org/abs/2107.13757",
          "publishedOn": "2021-07-30T02:13:27.763Z",
          "wordCount": 576,
          "title": "Bridging Gap between Image Pixels and Semantics via Supervision: A Survey. (arXiv:2107.13757v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Breiki_F/0/1/0/all/0/1\">Farha Al Breiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridzuan_M/0/1/0/all/0/1\">Muhammad Ridzuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandhe_R/0/1/0/all/0/1\">Rushali Grandhe</a>",
          "description": "Fine-grained image classification involves identifying different\nsubcategories of a class which possess very subtle discriminatory features.\nFine-grained datasets usually provide bounding box annotations along with class\nlabels to aid the process of classification. However, building large scale\ndatasets with such annotations is a mammoth task. Moreover, this extensive\nannotation is time-consuming and often requires expertise, which is a huge\nbottleneck in building large datasets. On the other hand, self-supervised\nlearning (SSL) exploits the freely available data to generate supervisory\nsignals which act as labels. The features learnt by performing some pretext\ntasks on huge unlabelled data proves to be very helpful for multiple downstream\ntasks.\n\nOur idea is to leverage self-supervision such that the model learns useful\nrepresentations of fine-grained image classes. We experimented with 3 kinds of\nmodels: Jigsaw solving as pretext task, adversarial learning (SRGAN) and\ncontrastive learning based (SimCLR) model. The learned features are used for\ndownstream tasks such as fine-grained image classification. Our code is\navailable at\nthis http URL",
          "link": "http://arxiv.org/abs/2107.13973",
          "publishedOn": "2021-07-30T02:13:27.750Z",
          "wordCount": 608,
          "title": "Self-Supervised Learning for Fine-Grained Image Classification. (arXiv:2107.13973v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1\">William Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1\">Philippe Burlina</a>",
          "description": "When deploying artificial intelligence (AI) in the real world, being able to\ntrust the operation of the AI by characterizing how it performs is an\never-present and important topic. An important and still largely unexplored\ntask in this characterization is determining major factors within the real\nworld that affect the AI's behavior, such as weather conditions or lighting,\nand either a) being able to give justification for why it may have failed or b)\neliminating the influence the factor has. Determining these sensitive factors\nheavily relies on collected data that is diverse enough to cover numerous\ncombinations of these factors, which becomes more onerous when having many\npotential sensitive factors or operating in complex environments. This paper\ninvestigates methods that discover and separate out individual semantic\nsensitive factors from a given dataset to conduct this characterization as well\nas addressing mitigation of these factors' sensitivity. We also broaden\nremediation of fairness, which normally only addresses socially relevant\nfactors, and widen it to deal with the desensitization of AI with regard to all\npossible aspects of variation in the domain. The proposed methods which\ndiscover these major factors reduce the potentially onerous demands of\ncollecting a sufficiently diverse dataset. In experiments using the road sign\n(GTSRB) and facial imagery (CelebA) datasets, we show the promise of using this\nscheme to perform this characterization and remediation and demonstrate that\nour approach outperforms state of the art approaches.",
          "link": "http://arxiv.org/abs/2107.13625",
          "publishedOn": "2021-07-30T02:13:27.728Z",
          "wordCount": 674,
          "title": "Generalizing Fairness: Discovery and Mitigation of Unknown Sensitive Attributes. (arXiv:2107.13625v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinmin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinzhu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jun Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>",
          "description": "As a crucial task of autonomous driving, 3D object detection has made great\nprogress in recent years. However, monocular 3D object detection remains a\nchallenging problem due to the unsatisfactory performance in depth estimation.\nMost existing monocular methods typically directly regress the scene depth\nwhile ignoring important relationships between the depth and various geometric\nelements (e.g. bounding box sizes, 3D object dimensions, and object poses). In\nthis paper, we propose to learn geometry-guided depth estimation with\nprojective modeling to advance monocular 3D object detection. Specifically, a\nprincipled geometry formula with projective modeling of 2D and 3D depth\npredictions in the monocular 3D object detection network is devised. We further\nimplement and embed the proposed formula to enable geometry-aware deep\nrepresentation learning, allowing effective 2D and 3D interactions for boosting\nthe depth estimation. Moreover, we provide a strong baseline through addressing\nsubstantial misalignment between 2D annotation and projected boxes to ensure\nrobust learning with the proposed geometric formula. Experiments on the KITTI\ndataset show that our method remarkably improves the detection performance of\nthe state-of-the-art monocular-based method without extra data by 2.80% on the\nmoderate test setting. The model and code will be released at\nhttps://github.com/YinminZhang/MonoGeo.",
          "link": "http://arxiv.org/abs/2107.13931",
          "publishedOn": "2021-07-30T02:13:27.720Z",
          "wordCount": 650,
          "title": "Learning Geometry-Guided Depth via Projective Modeling for Monocular 3D Object Detection. (arXiv:2107.13931v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korschens_M/0/1/0/all/0/1\">Matthias K&#xf6;rschens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodesheim_P/0/1/0/all/0/1\">Paul Bodesheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romermann_C/0/1/0/all/0/1\">Christine R&#xf6;mermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucher_S/0/1/0/all/0/1\">Solveig Franziska Bucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Migliavacca_M/0/1/0/all/0/1\">Mirco Migliavacca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulrich_J/0/1/0/all/0/1\">Josephine Ulrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>",
          "description": "Monitoring the responses of plants to environmental changes is essential for\nplant biodiversity research. This, however, is currently still being done\nmanually by botanists in the field. This work is very laborious, and the data\nobtained is, though following a standardized method to estimate plant coverage,\nusually subjective and has a coarse temporal resolution. To remedy these\ncaveats, we investigate approaches using convolutional neural networks (CNNs)\nto automatically extract the relevant data from images, focusing on plant\ncommunity composition and species coverages of 9 herbaceous plant species. To\nthis end, we investigate several standard CNN architectures and different\npretraining methods. We find that we outperform our previous approach at higher\nimage resolutions using a custom CNN with a mean absolute error of 5.16%. In\naddition to these investigations, we also conduct an error analysis based on\nthe temporal aspect of the plant cover images. This analysis gives insight into\nwhere problems for automatic approaches lie, like occlusion and likely\nmisclassifications caused by temporal changes.",
          "link": "http://arxiv.org/abs/2106.11154",
          "publishedOn": "2021-07-29T02:00:11.252Z",
          "wordCount": 644,
          "title": "Automatic Plant Cover Estimation with Convolutional Neural Networks. (arXiv:2106.11154v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.03860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pellikka_M/0/1/0/all/0/1\">Matti Pellikka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahtinen_V/0/1/0/all/0/1\">Valtteri Lahtinen</a>",
          "description": "We propose a novel method for large-scale image stitching that is robust\nagainst repetitive patterns and featureless regions in the imagery. In such\ncases, state-of-the-art image stitching methods easily produce image alignment\nartifacts, since they may produce false pairwise image registrations that are\nin conflict within the global connectivity graph. Our method augments the\ncurrent methods by collecting all the plausible pairwise image registration\ncandidates, among which globally consistent candidates are chosen. This enables\nthe stitching process to determine the correct pairwise registrations by\nutilizing all the available information from the whole imagery, such as\nunambiguous registrations outside the repeating pattern and featureless\nregions. We formalize the method as a weighted multigraph whose nodes represent\nthe individual image transformations from the composite image, and whose sets\nof multiple edges between two nodes represent all the plausible transformations\nbetween the pixel coordinates of the two images. The edge weights represent the\nplausibility of the transformations. The image transformations and the edge\nweights are solved from a non-linear minimization problem with linear\nconstraints, for which a projection method is used. As an example, we apply the\nmethod in a large-scale scanning application where the transformations are\nprimarily translations with only slight rotation and scaling component. Despite\nthese simplifications, the state-of-the-art methods do not produce adequate\nresults in such applications, since the image overlap is small, which can be\nfeatureless or repetitive, and misalignment artifacts and their concealment are\nunacceptable.",
          "link": "http://arxiv.org/abs/2004.03860",
          "publishedOn": "2021-07-29T02:00:11.244Z",
          "wordCount": 717,
          "title": "A Robust Method for Image Stitching. (arXiv:2004.03860v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shepley_A/0/1/0/all/0/1\">Andrew Shepley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falzon_G/0/1/0/all/0/1\">Greg Falzon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwan_P/0/1/0/all/0/1\">Paul Kwan</a>",
          "description": "Confluence is a novel non-Intersection over Union (IoU) alternative to\nNon-Maxima Suppression (NMS) in bounding box post-processing in object\ndetection. It overcomes the inherent limitations of IoU-based NMS variants to\nprovide a more stable, consistent predictor of bounding box clustering by using\na normalized Manhattan Distance inspired proximity metric to represent bounding\nbox clustering. Unlike Greedy and Soft NMS, it does not rely solely on\nclassification confidence scores to select optimal bounding boxes, instead\nselecting the box which is closest to every other box within a given cluster\nand removing highly confluent neighboring boxes. Confluence is experimentally\nvalidated on the MS COCO and CrowdHuman benchmarks, improving Average Precision\nby up to 2.3-3.8% and Average Recall by up to 5.3-7.2% when compared against\nde-facto standard and state of the art NMS variants. Quantitative results are\nsupported by extensive qualitative analysis and threshold sensitivity analysis\nexperiments support the conclusion that Confluence is more robust than NMS\nvariants. Confluence represents a paradigm shift in bounding box processing,\nwith potential to replace IoU in bounding box regression processes.",
          "link": "http://arxiv.org/abs/2012.00257",
          "publishedOn": "2021-07-29T02:00:11.228Z",
          "wordCount": 645,
          "title": "Confluence: A Robust Non-IoU Alternative to Non-Maxima Suppression in Object Detection. (arXiv:2012.00257v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hagemann_A/0/1/0/all/0/1\">Annika Hagemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knorr_M/0/1/0/all/0/1\">Moritz Knorr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janssen_H/0/1/0/all/0/1\">Holger Janssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1\">Christoph Stiller</a>",
          "description": "Accurate camera calibration is a precondition for many computer vision\napplications. Calibration errors, such as wrong model assumptions or imprecise\nparameter estimation, can deteriorate a system's overall performance, making\nthe reliable detection and quantification of these errors critical. In this\nwork, we introduce an evaluation scheme to capture the fundamental error\nsources in camera calibration: systematic errors (biases) and uncertainty\n(variance). The proposed bias detection method uncovers smallest systematic\nerrors and thereby reveals imperfections of the calibration setup and provides\nthe basis for camera model selection. A novel resampling-based uncertainty\nestimator enables uncertainty estimation under non-ideal conditions and thereby\nextends the classical covariance estimator. Furthermore, we derive a simple\nuncertainty metric that is independent of the camera model. In combination, the\nproposed methods can be used to assess the accuracy of individual calibrations,\nbut also to benchmark new calibration algorithms, camera models, or calibration\nsetups. We evaluate the proposed methods with simulations and real cameras.",
          "link": "http://arxiv.org/abs/2107.13484",
          "publishedOn": "2021-07-29T02:00:11.192Z",
          "wordCount": 589,
          "title": "Inferring bias and uncertainty in camera calibration. (arXiv:2107.13484v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenhua Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "In this work, we propose an adversarial unsupervised domain adaptation (UDA)\napproach with the inherent conditional and label shifts, in which we aim to\nalign the distributions w.r.t. both $p(x|y)$ and $p(y)$. Since the label is\ninaccessible in the target domain, the conventional adversarial UDA assumes\n$p(y)$ is invariant across domains, and relies on aligning $p(x)$ as an\nalternative to the $p(x|y)$ alignment. To address this, we provide a thorough\ntheoretical and empirical analysis of the conventional adversarial UDA methods\nunder both conditional and label shifts, and propose a novel and practical\nalternative optimization scheme for adversarial UDA. Specifically, we infer the\nmarginal $p(y)$ and align $p(x|y)$ iteratively in the training, and precisely\nalign the posterior $p(y|x)$ in testing. Our experimental results demonstrate\nits effectiveness on both classification and segmentation UDA, and partial UDA.",
          "link": "http://arxiv.org/abs/2107.13469",
          "publishedOn": "2021-07-29T02:00:11.185Z",
          "wordCount": 605,
          "title": "Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate. (arXiv:2107.13469v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03577",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Franco_Barranco_D/0/1/0/all/0/1\">Daniel Franco-Barranco</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munoz_Barrutia_A/0/1/0/all/0/1\">Arrate Mu&#xf1;oz-Barrutia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arganda_Carreras_I/0/1/0/all/0/1\">Ignacio Arganda-Carreras</a>",
          "description": "Electron microscopy (EM) allows the identification of intracellular\norganelles such as mitochondria, providing insights for clinical and scientific\nstudies. In recent years, a number of novel deep learning architectures have\nbeen published reporting superior performance, or even human-level accuracy,\ncompared to previous approaches on public mitochondria segmentation datasets.\nUnfortunately, many of these publications do not make neither the code nor the\nfull training details public to support the results obtained, leading to\nreproducibility issues and dubious model comparisons. For that reason, and\nfollowing a recent code of best practices for reporting experimental results,\nwe present an extensive study of the state-of-the-art deep learning\narchitectures for the segmentation of mitochondria on EM volumes, and evaluate\nthe impact in performance of different variations of 2D and 3D U-Net-like\nmodels for this task. To better understand the contribution of each component,\na common set of pre- and post-processing operations has been implemented and\ntested with each approach. Moreover, an exhaustive sweep of hyperparameters\nvalues for all architectures have been performed and each configuration has\nbeen run multiple times to report the mean and standard deviation values of the\nevaluation metrics. Using this methodology, we found very stable architectures\nand hyperparameter configurations that consistently obtain state-of-the-art\nresults in the well-known EPFL Hippocampus mitochondria segmentation dataset.\nFurthermore, we have benchmarked our proposed models on two other available\ndatasets, Lucchi++ and Kasthuri++, where they outperform all previous works.\nThe code derived from this research and its documentation are publicly\navailable.",
          "link": "http://arxiv.org/abs/2104.03577",
          "publishedOn": "2021-07-29T02:00:11.149Z",
          "wordCount": 711,
          "title": "Stable deep neural network architectures for mitochondria segmentation on electron microscopy volumes. (arXiv:2104.03577v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1\">Maria Pia Del Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiller_D/0/1/0/all/0/1\">Dario Spiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1\">Pierre Philippe Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "In recent years, the growth of Machine Learning (ML) algorithms has raised\nthe number of studies including their applicability in a variety of different\nscenarios. Among all, one of the hardest ones is the aerospace, due to its\npeculiar physical requirements. In this context, a feasibility study and a\nfirst prototype for an Artificial Intelligence (AI) model to be deployed on\nboard satellites are presented in this work. As a case study, the detection of\nvolcanic eruptions has been investigated as a method to swiftly produce alerts\nand allow immediate interventions. Two Convolutional Neural Networks (CNNs)\nhave been proposed and designed, showing how to efficiently implement them for\nidentifying the eruptions and at the same time adapting their complexity in\norder to fit on board requirements.",
          "link": "http://arxiv.org/abs/2106.15281",
          "publishedOn": "2021-07-29T02:00:10.339Z",
          "wordCount": 602,
          "title": "On Board Volcanic Eruption Detection through CNNs and Satellite Multispectral Imagery. (arXiv:2106.15281v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yue Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Liangxiu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleerekoper_A/0/1/0/all/0/1\">Anthony Kleerekoper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Sheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tongle Hu</a>",
          "description": "Late blight disease is one of the most destructive diseases in potato crop,\nleading to serious yield losses globally. Accurate diagnosis of the disease at\nearly stage is critical for precision disease control and management. Current\nfarm practices in crop disease diagnosis are based on manual visual inspection,\nwhich is costly, time consuming, subject to individual bias. Recent advances in\nimaging sensors (e.g. RGB, multiple spectral and hyperspectral cameras), remote\nsensing and machine learning offer the opportunity to address this challenge.\nParticularly, hyperspectral imagery (HSI) combining with machine learning/deep\nlearning approaches is preferable for accurately identifying specific plant\ndiseases because the HSI consists of a wide range of high-quality reflectance\ninformation beyond human vision, capable of capturing both spectral-spatial\ninformation. The proposed method considers the potential disease specific\nreflectance radiation variance caused by the canopy structural diversity,\nintroduces the multiple capsule layers to model the hierarchical structure of\nthe spectral-spatial disease attributes with the encapsulated features to\nrepresent the various classes and the rotation invariance of the disease\nattributes in the feature space. We have evaluated the proposed method with the\nreal UAV-based HSI data under the controlled field conditions. The\neffectiveness of the hierarchical features has been quantitatively assessed and\ncompared with the existing representative machine learning/deep learning\nmethods. The experiment results show that the proposed model significantly\nimproves the accuracy performance when considering hierarchical-structure of\nspectral-spatial features, comparing to the existing methods only using\nspectral, or spatial or spectral-spatial features without consider\nhierarchical-structure of spectral-spatial features.",
          "link": "http://arxiv.org/abs/2107.13277",
          "publishedOn": "2021-07-29T02:00:10.166Z",
          "wordCount": 707,
          "title": "A Novel CropdocNet for Automated Potato Late Blight Disease Detection from the Unmanned Aerial Vehicle-based Hyperspectral Imagery. (arXiv:2107.13277v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feeney_P/0/1/0/all/0/1\">Patrick Feeney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1\">Michael C. Hughes</a>",
          "description": "The pixelwise reconstruction error of deep autoencoders is often utilized for\nimage novelty detection and localization under the assumption that pixels with\nhigh error indicate which parts of the input image are unfamiliar and therefore\nlikely to be novel. This assumed correlation between pixels with high\nreconstruction error and novel regions of input images has not been verified\nand may limit the accuracy of these methods. In this paper we utilize saliency\nmaps to evaluate whether this correlation exists. Saliency maps reveal directly\nhow much a change in each input pixel would affect reconstruction loss, while\neach pixel's reconstruction error may be attributed to many input pixels when\nlayers are fully connected. We compare saliency maps to reconstruction error\nmaps via qualitative visualizations as well as quantitative correspondence\nbetween the top K elements of the maps for both novel and normal images. Our\nresults indicate that reconstruction error maps do not closely correlate with\nthe importance of pixels in the input images, making them insufficient for\nnovelty localization.",
          "link": "http://arxiv.org/abs/2107.13379",
          "publishedOn": "2021-07-29T02:00:10.151Z",
          "wordCount": 606,
          "title": "Evaluating the Use of Reconstruction Error for Novelty Localization. (arXiv:2107.13379v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiufu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Sheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1\">Zhihui Lai</a>",
          "description": "Though widely used in image classification, convolutional neural networks\n(CNNs) are prone to noise interruptions, i.e. the CNN output can be drastically\nchanged by small image noise. To improve the noise robustness, we try to\nintegrate CNNs with wavelet by replacing the common down-sampling (max-pooling,\nstrided-convolution, and average pooling) with discrete wavelet transform\n(DWT). We firstly propose general DWT and inverse DWT (IDWT) layers applicable\nto various orthogonal and biorthogonal discrete wavelets like Haar, Daubechies,\nand Cohen, etc., and then design wavelet integrated CNNs (WaveCNets) by\nintegrating DWT into the commonly used CNNs (VGG, ResNets, and DenseNet).\nDuring the down-sampling, WaveCNets apply DWT to decompose the feature maps\ninto the low-frequency and high-frequency components. Containing the main\ninformation including the basic object structures, the low-frequency component\nis transmitted into the following layers to generate robust high-level\nfeatures. The high-frequency components are dropped to remove most of the data\nnoises. The experimental results show that %wavelet accelerates the CNN\ntraining, and WaveCNets achieve higher accuracy on ImageNet than various\nvanilla CNNs. We have also tested the performance of WaveCNets on the noisy\nversion of ImageNet, ImageNet-C and six adversarial attacks, the results\nsuggest that the proposed DWT/IDWT layers could provide better noise-robustness\nand adversarial robustness. When applying WaveCNets as backbones, the\nperformance of object detectors (i.e., faster R-CNN and RetinaNet) on COCO\ndetection dataset are consistently improved. We believe that suppression of\naliasing effect, i.e. separation of low frequency and high frequency\ninformation, is the main advantages of our approach. The code of our DWT/IDWT\nlayer and different WaveCNets are available at\nhttps://github.com/CVI-SZU/WaveCNet.",
          "link": "http://arxiv.org/abs/2107.13335",
          "publishedOn": "2021-07-29T02:00:10.141Z",
          "wordCount": 720,
          "title": "WaveCNet: Wavelet Integrated CNNs to Suppress Aliasing Effect for Noise-Robust Image Classification. (arXiv:2107.13335v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhigao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yin Zhao</a>",
          "description": "We conduct a subjective experiment to compare the performance of traditional\nimage coding methods and learning-based image coding methods. HEVC and VVC, the\nstate-of-the-art traditional coding methods, are used as the representative\ntraditional methods. The learning-based methods used contain not only CNN-based\nmethods, but also a GAN-based method, all of which are advanced or typical.\nSingle Stimuli (SS), which is also called Absolute Category Rating (ACR), is\nadopted as the methodology of the experiment to obtain perceptual quality of\nimages. Additionally, we utilize some typical and frequently used objective\nquality metrics to evaluate the coding methods in the experiment as comparison.\nThe experiment shows that CNN-based and GAN-based methods can perform better\nthan traditional methods in low bit-rates. In high bit-rates, however, it is\nhard to verify whether CNN-based methods are superior to traditional methods.\nBecause the GAN method does not provide models with high target bit-rates, we\ncannot exactly tell the performance of the GAN method in high bit-rates.\nFurthermore, some popular objective quality metrics have not shown the ability\nwell to measure quality of images generated by learning-based coding methods,\nespecially the GAN-based one.",
          "link": "http://arxiv.org/abs/2107.13122",
          "publishedOn": "2021-07-29T02:00:10.118Z",
          "wordCount": 631,
          "title": "Subjective evaluation of traditional and learning-based image coding methods. (arXiv:2107.13122v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenjiang Liu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zunlei Feng</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chengji Shen</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Ou_K/0/1/0/all/0/1\">Kairi Ou</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haihong Tang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a> (2) ((1) Alibaba Group, (2) Zhejiang University)",
          "description": "Image virtual try-on task has abundant applications and has become a hot\nresearch topic recently. Existing 2D image-based virtual try-on methods aim to\ntransfer a target clothing image onto a reference person, which has two main\ndisadvantages: cannot control the size and length precisely; unable to\naccurately estimate the user's figure in the case of users wearing thick\nclothes, resulting in inaccurate dressing effect. In this paper, we put forward\nan akin task that aims to dress clothing for underwear models. %, which is also\nan urgent need in e-commerce scenarios. To solve the above drawbacks, we\npropose a Shape Controllable Virtual Try-On Network (SC-VTON), where a graph\nattention network integrates the information of model and clothing to generate\nthe warped clothing image. In addition, the control points are incorporated\ninto SC-VTON for the desired clothing shape. Furthermore, by adding a Splitting\nNetwork and a Synthesis Network, we can use clothing/model pair data to help\noptimize the deformation module and generalize the task to the typical virtual\ntry-on task. Extensive experiments show that the proposed method can achieve\naccurate shape control. Meanwhile, compared with other methods, our method can\ngenerate high-resolution results with detailed textures.",
          "link": "http://arxiv.org/abs/2107.13156",
          "publishedOn": "2021-07-29T02:00:10.105Z",
          "wordCount": 658,
          "title": "Shape Controllable Virtual Try-on for Underwear Models. (arXiv:2107.13156v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yanda Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongrun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yitian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xuesheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaowei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yalin Zheng</a>",
          "description": "Semi-supervised approaches for crowd counting attract attention, as the fully\nsupervised paradigm is expensive and laborious due to its request for a large\nnumber of images of dense crowd scenarios and their annotations. This paper\nproposes a spatial uncertainty-aware semi-supervised approach via regularized\nsurrogate task (binary segmentation) for crowd counting problems. Different\nfrom existing semi-supervised learning-based crowd counting methods, to exploit\nthe unlabeled data, our proposed spatial uncertainty-aware teacher-student\nframework focuses on high confident regions' information while addressing the\nnoisy supervision from the unlabeled data in an end-to-end manner.\nSpecifically, we estimate the spatial uncertainty maps from the teacher model's\nsurrogate task to guide the feature learning of the main task (density\nregression) and the surrogate task of the student model at the same time.\nBesides, we introduce a simple yet effective differential transformation layer\nto enforce the inherent spatial consistency regularization between the main\ntask and the surrogate task in the student model, which helps the surrogate\ntask to yield more reliable predictions and generates high-quality uncertainty\nmaps. Thus, our model can also address the task-level perturbation problems\nthat occur spatial inconsistency between the primary and surrogate tasks in the\nstudent model. Experimental results on four challenging crowd counting datasets\ndemonstrate that our method achieves superior performance to the\nstate-of-the-art semi-supervised methods.",
          "link": "http://arxiv.org/abs/2107.13271",
          "publishedOn": "2021-07-29T02:00:10.004Z",
          "wordCount": 657,
          "title": "Spatial Uncertainty-Aware Semi-Supervised Crowd Counting. (arXiv:2107.13271v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1\">Laurent Kneip</a>",
          "description": "Camera calibration is an important prerequisite towards the solution of 3D\ncomputer vision problems. Traditional methods rely on static images of a\ncalibration pattern. This raises interesting challenges towards the practical\nusage of event cameras, which notably require image change to produce\nsufficient measurements. The current standard for event camera calibration\ntherefore consists of using flashing patterns. They have the advantage of\nsimultaneously triggering events in all reprojected pattern feature locations,\nbut it is difficult to construct or use such patterns in the field. We present\nthe first dynamic event camera calibration algorithm. It calibrates directly\nfrom events captured during relative motion between camera and calibration\npattern. The method is propelled by a novel feature extraction mechanism for\ncalibration patterns, and leverages existing calibration tools before\noptimizing all parameters through a multi-segment continuous-time formulation.\nAs demonstrated through our results on real data, the obtained calibration\nmethod is highly convenient and reliably calibrates from data sequences\nspanning less than 10 seconds.",
          "link": "http://arxiv.org/abs/2107.06749",
          "publishedOn": "2021-07-29T02:00:08.913Z",
          "wordCount": 629,
          "title": "Dynamic Event Camera Calibration. (arXiv:2107.06749v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sayak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>",
          "description": "Floods wreak havoc throughout the world, causing billions of dollars in\ndamages, and uprooting communities, ecosystems and economies. Accurate and\nrobust flood detection including delineating open water flood areas and\nidentifying flood levels can aid in disaster response and mitigation. However,\nestimating flood levels remotely is of essence as physical access to flooded\nareas is limited and the ability to deploy instruments in potential flood zones\ncan be dangerous. Aligning flood extent mapping with local topography can\nprovide a plan-of-action that the disaster response team can consider. Thus,\nremote flood level estimation via satellites like Sentinel-1 can prove to be\nremedial. The Emerging Techniques in Computational Intelligence (ETCI)\ncompetition on Flood Detection tasked participants with predicting flooded\npixels after training with synthetic aperture radar (SAR) images in a\nsupervised setting. We use a cyclical approach involving two stages (1)\ntraining an ensemble model of multiple UNet architectures with available high\nand low confidence labeled data and, (2) generating pseudo labels or low\nconfidence labels on the unlabeled test dataset, and then, combining the\ngenerated labels with the previously available high confidence labeled dataset.\nThis assimilated dataset is used for the next round of training ensemble\nmodels. This cyclical process is repeated until the performance improvement\nplateaus. Additionally, we post process our results with Conditional Random\nFields. Our approach sets a high score on the public leaderboard for the ETCI\ncompetition with 0.7654 IoU. Our method, which we release with all the code\nincluding trained models, can also be used as an open science benchmark for the\nSentinel-1 released dataset on GitHub. To the best of our knowledge we believe\nthis the first works to try out semi-supervised learning to improve flood\nsegmentation models.",
          "link": "http://arxiv.org/abs/2107.08369",
          "publishedOn": "2021-07-29T02:00:08.850Z",
          "wordCount": 774,
          "title": "Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-29T02:00:08.842Z",
          "wordCount": 700,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogozhnikov_A/0/1/0/all/0/1\">Alex Rogozhnikov</a>",
          "description": "Without positional information, attention-based transformer neural networks\nare permutation-invariant. Absolute or relative positional embeddings are the\nmost popular ways to feed transformer models positional information. Absolute\npositional embeddings are simple to implement, but suffer from generalization\nissues when evaluating on sequences of different length than those seen at\ntraining time. Relative positions are more robust to length change, but are\nmore complex to implement and yield inferior model throughput. In this paper,\nwe propose an augmentation-based approach (CAPE) for absolute positional\nembeddings, which keeps the advantages of both absolute (simplicity and speed)\nand relative position embeddings (better generalization). In addition, our\nempirical evaluation on state-of-the-art models in machine translation, image\nand speech recognition demonstrates that CAPE leads to better generalization\nperformance as well as increased stability with respect to training\nhyper-parameters.",
          "link": "http://arxiv.org/abs/2106.03143",
          "publishedOn": "2021-07-29T02:00:08.818Z",
          "wordCount": 607,
          "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings. (arXiv:2106.03143v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Di Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "3D point cloud completion is very challenging because it heavily relies on\nthe accurate understanding of the complex 3D shapes (e.g., high-curvature,\nconcave/convex, and hollowed-out 3D shapes) and the unknown & diverse patterns\nof the partially available point clouds. In this paper, we propose a novel\nsolution,i.e., Point-block Carving (PC), for completing the complex 3D point\ncloud completion. Given the partial point cloud as the guidance, we carve a3D\nblock that contains the uniformly distributed 3D points, yielding the entire\npoint cloud. To achieve PC, we propose a new network architecture, i.e.,\nCarveNet. This network conducts the exclusive convolution on each point of the\nblock, where the convolutional kernels are trained on the 3D shape data.\nCarveNet determines which point should be carved, for effectively recovering\nthe details of the complete shapes. Furthermore, we propose a sensor-aware\nmethod for data augmentation,i.e., SensorAug, for training CarveNet on richer\npatterns of partial point clouds, thus enhancing the completion power of the\nnetwork. The extensive evaluations on the ShapeNet and KITTI datasets\ndemonstrate the generality of our approach on the partial point clouds with\ndiverse patterns. On these datasets, CarveNet successfully outperforms the\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.13452",
          "publishedOn": "2021-07-29T02:00:08.799Z",
          "wordCount": 643,
          "title": "CarveNet: Carving Point-Block for Complex 3D Shape Completion. (arXiv:2107.13452v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09405",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Schirris_Y/0/1/0/all/0/1\">Yoni Schirris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gavves_E/0/1/0/all/0/1\">Efstratios Gavves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nederlof_I/0/1/0/all/0/1\">Iris Nederlof</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Horlings_H/0/1/0/all/0/1\">Hugo Mark Horlings</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1\">Jonas Teuwen</a>",
          "description": "We propose a Deep learning-based weak label learning method for analysing\nwhole slide images (WSIs) of Hematoxylin and Eosin (H&E) stained tumorcells not\nrequiring pixel-level or tile-level annotations using Self-supervised\npre-training and heterogeneity-aware deep Multiple Instance LEarning\n(DeepSMILE). We apply DeepSMILE to the task of Homologous recombination\ndeficiency (HRD) and microsatellite instability (MSI) prediction. We utilize\ncontrastive self-supervised learning to pre-train a feature extractor on\nhistopathology tiles of cancer tissue. Additionally, we use variability-aware\ndeep multiple instance learning to learn the tile feature aggregation function\nwhile modeling tumor heterogeneity. Compared to state-of-the-art genomic label\nclassification methods, DeepSMILE improves classification performance for HRD\nfrom $70.43\\pm4.10\\%$ to $83.79\\pm1.25\\%$ AUC and MSI from $78.56\\pm6.24\\%$ to\n$90.32\\pm3.58\\%$ AUC in a multi-center breast and colorectal cancer dataset,\nrespectively. These improvements suggest we can improve genomic label\nclassification performance without collecting larger datasets. In the future,\nthis may reduce the need for expensive genome sequencing techniques, provide\npersonalized therapy recommendations based on widely available WSIs of cancer\ntissue, and improve patient care with quicker treatment decisions - also in\nmedical centers without access to genome sequencing resources.",
          "link": "http://arxiv.org/abs/2107.09405",
          "publishedOn": "2021-07-29T02:00:08.791Z",
          "wordCount": 673,
          "title": "DeepSMILE: Self-supervised heterogeneity-aware multiple instance learning for DNA damage response defect classification directly from H&E whole-slide images. (arXiv:2107.09405v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">Edward S. Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1\">Oleh Rybkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1\">Dinesh Jayaraman</a>",
          "description": "Training visuomotor robot controllers from scratch on a new robot typically\nrequires generating large amounts of robot-specific data. Could we leverage\ndata previously collected on another robot to reduce or even completely remove\nthis need for robot-specific data? We propose a \"robot-aware\" solution paradigm\nthat exploits readily available robot \"self-knowledge\" such as proprioception,\nkinematics, and camera calibration to achieve this. First, we learn modular\ndynamics models that pair a transferable, robot-agnostic world dynamics module\nwith a robot-specific, analytical robot dynamics module. Next, we set up visual\nplanning costs that draw a distinction between the robot self and the world.\nOur experiments on tabletop manipulation tasks in simulation and on real robots\ndemonstrate that these plug-in improvements dramatically boost the\ntransferability of visuomotor controllers, even permitting zero-shot transfer\nonto new robots for the very first time. Project website:\nhttps://hueds.github.io/rac/",
          "link": "http://arxiv.org/abs/2107.09047",
          "publishedOn": "2021-07-29T02:00:08.783Z",
          "wordCount": 603,
          "title": "Know Thyself: Transferable Visuomotor Control Through Robot-Awareness. (arXiv:2107.09047v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "Domain adaptation is to transfer the shared knowledge learned from the source\ndomain to a new environment, i.e., target domain. One common practice is to\ntrain the model on both labeled source-domain data and unlabeled target-domain\ndata. Yet the learned models are usually biased due to the strong supervision\nof the source domain. Most researchers adopt the early-stopping strategy to\nprevent over-fitting, but when to stop training remains a challenging problem\nsince the lack of the target-domain validation set. In this paper, we propose\none efficient bootstrapping method, called Adaboost Student, explicitly\nlearning complementary models during training and liberating users from\nempirical early stopping. Adaboost Student combines the deep model learning\nwith the conventional training strategy, i.e., adaptive boosting, and enables\ninteractions between learned models and the data sampler. We adopt one adaptive\ndata sampler to progressively facilitate learning on hard samples and aggregate\n\"weak\" models to prevent over-fitting. Extensive experiments show that (1)\nWithout the need to worry about the stopping time, AdaBoost Student provides\none robust solution by efficient complementary model learning during training.\n(2) AdaBoost Student is orthogonal to most domain adaptation methods, which can\nbe combined with existing approaches to further improve the state-of-the-art\nperformance. We have achieved competitive results on three widely-used scene\nsegmentation domain adaptation benchmarks.",
          "link": "http://arxiv.org/abs/2103.15685",
          "publishedOn": "2021-07-29T02:00:08.776Z",
          "wordCount": 684,
          "title": "Adaptive Boosting for Domain Adaptation: Towards Robust Predictions in Scene Segmentation. (arXiv:2103.15685v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowakowski_A/0/1/0/all/0/1\">Artur Nowakowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puglisi_E/0/1/0/all/0/1\">Erika Puglisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1\">Maria Pia Del Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mifdal_J/0/1/0/all/0/1\">Jamila Mifdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirri_F/0/1/0/all/0/1\">Fiora Pirri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1\">Pierre Philippe Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "The abundance of clouds, located both spatially and temporally, often makes\nremote sensing (RS) applications with optical images difficult or even\nimpossible to perform. Traditional cloud removing techniques have been studied\nfor years, and recently, Machine Learning (ML)-based approaches have also been\nconsidered. In this manuscript, a novel method for the restoration of\nclouds-corrupted optical images is presented, able to generate the whole\noptical scene of interest, not only the cloudy pixels, and based on a Joint\nData Fusion paradigm, where three deep neural networks are hierarchically\ncombined. Spatio-temporal features are separately extracted by a conditional\nGenerative Adversarial Network (cGAN) and by a Convolutional Long Short-Term\nMemory (ConvLSTM), from Synthetic Aperture Radar (SAR) data and optical\ntime-series of data respectively, and then combined with a U-shaped network.\nThe use of time-series of data has been rarely explored in the state of the art\nfor this peculiar objective, and moreover existing models do not combine both\nspatio-temporal domains and SAR-optical imagery. Quantitative and qualitative\nresults have shown a good ability of the proposed method in producing\ncloud-free images, by also preserving the details and outperforming the cGAN\nand the ConvLSTM when individually used. Both the code and the dataset have\nbeen implemented from scratch and made available to interested researchers for\nfurther analysis and investigation.",
          "link": "http://arxiv.org/abs/2106.12226",
          "publishedOn": "2021-07-29T02:00:08.768Z",
          "wordCount": 710,
          "title": "Spatio-Temporal SAR-Optical Data Fusion for Cloud Removal via a Deep Hierarchical Model. (arXiv:2106.12226v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhengbo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zitang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weilian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamata_S/0/1/0/all/0/1\">Sei-ichiro Kamata</a>",
          "description": "Various deep neural network architectures (DNNs) maintain massive vital\nrecords in computer vision. While drawing attention worldwide, the design of\nthe overall structure lacks general guidance. Based on the relationship between\nDNN design and numerical differential equations, we performed a fair comparison\nof the residual design with higher-order perspectives. We show that the widely\nused DNN design strategy, constantly stacking a small design (usually 2-3\nlayers), could be easily improved, supported by solid theoretical knowledge and\nwith no extra parameters needed. We reorganise the residual design in\nhigher-order ways, which is inspired by the observation that many effective\nnetworks can be interpreted as different numerical discretisations of\ndifferential equations. The design of ResNet follows a relatively simple\nscheme, which is Euler forward; however, the situation becomes complicated\nrapidly while stacking. We suppose that stacked ResNet is somehow equalled to a\nhigher-order scheme; then, the current method of forwarding propagation might\nbe relatively weak compared with a typical high-order method such as\nRunge-Kutta. We propose HO-ResNet to verify the hypothesis of widely used CV\nbenchmarks with sufficient experiments. Stable and noticeable increases in\nperformance are observed, and convergence and robustness are also improved. Our\nstacking strategy improved ResNet-30 by 2.15 per cent and ResNet-58 by 2.35 per\ncent on CIFAR-10, with the same settings and parameters. The proposed strategy\nis fundamental and theoretical and can therefore be applied to any network as a\ngeneral guideline.",
          "link": "http://arxiv.org/abs/2103.15244",
          "publishedOn": "2021-07-29T02:00:08.760Z",
          "wordCount": 727,
          "title": "Rethinking ResNets: Improved Stacking Strategies With High Order Schemes. (arXiv:2103.15244v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Terry_J/0/1/0/all/0/1\">J. K. Terry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayakumar_M/0/1/0/all/0/1\">Mario Jayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alwis_K/0/1/0/all/0/1\">Kusal De Alwis</a>",
          "description": "The general approach taken when training deep learning classifiers is to save\nthe parameters after every few iterations, train until either a human observer\nor a simple metric-based heuristic decides the network isn't learning anymore,\nand then backtrack and pick the saved parameters with the best validation\naccuracy. Simple methods are used to determine if a neural network isn't\nlearning anymore because, as long as it's well after the optimal values are\nfound, the condition doesn't impact the final accuracy of the model. However\nfrom a runtime perspective, this is of great significance to the many cases\nwhere numerous neural networks are trained simultaneously (e.g. hyper-parameter\ntuning). Motivated by this, we introduce a statistical significance test to\ndetermine if a neural network has stopped learning. This stopping criterion\nappears to represent a happy medium compared to other popular stopping\ncriterions, achieving comparable accuracy to the criterions that achieve the\nhighest final accuracies in 77% or fewer epochs, while the criterions which\nstop sooner do so with an appreciable loss to final accuracy. Additionally, we\nuse this as the basis of a new learning rate scheduler, removing the need to\nmanually choose learning rate schedules and acting as a quasi-line search,\nachieving superior or comparable empirical performance to existing methods.",
          "link": "http://arxiv.org/abs/2103.01205",
          "publishedOn": "2021-07-29T02:00:08.752Z",
          "wordCount": 691,
          "title": "Statistically Significant Stopping of Neural Network Training. (arXiv:2103.01205v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dongming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Weakly-supervised temporal action localization (WS-TAL) aims to localize\nactions in untrimmed videos with only video-level labels. Most existing models\nfollow the \"localization by classification\" procedure: locate temporal regions\ncontributing most to the video-level classification. Generally, they process\neach snippet (or frame) individually and thus overlook the fruitful temporal\ncontext relation. Here arises the single snippet cheating issue: \"hard\"\nsnippets are too vague to be classified. In this paper, we argue that learning\nby comparing helps identify these hard snippets and we propose to utilize\nsnippet Contrastive learning to Localize Actions, CoLA for short. Specifically,\nwe propose a Snippet Contrast (SniCo) Loss to refine the hard snippet\nrepresentation in feature space, which guides the network to perceive precise\ntemporal boundaries and avoid the temporal interval interruption. Besides,\nsince it is infeasible to access frame-level annotations, we introduce a Hard\nSnippet Mining algorithm to locate the potential hard snippets. Substantial\nanalyses verify that this mining strategy efficaciously captures the hard\nsnippets and SniCo Loss leads to more informative feature representation.\nExtensive experiments show that CoLA achieves state-of-the-art results on\nTHUMOS'14 and ActivityNet v1.2 datasets. CoLA code is publicly available at\nhttps://github.com/zhang-can/CoLA.",
          "link": "http://arxiv.org/abs/2103.16392",
          "publishedOn": "2021-07-29T02:00:08.728Z",
          "wordCount": 669,
          "title": "CoLA: Weakly-Supervised Temporal Action Localization with Snippet Contrastive Learning. (arXiv:2103.16392v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morales_D/0/1/0/all/0/1\">David Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talavera_E/0/1/0/all/0/1\">Estefania Talavera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remeseiro_B/0/1/0/all/0/1\">Beatriz Remeseiro</a>",
          "description": "The field of deep learning is evolving in different directions, with still\nthe need for more efficient training strategies. In this work, we present a\nnovel and robust training scheme that integrates visual explanation techniques\nin the learning process. Unlike the attention mechanisms that focus on the\nrelevant parts of images, we aim to improve the robustness of the model by\nmaking it pay attention to other regions as well. Broadly speaking, the idea is\nto distract the classifier in the learning process to force it to focus not\nonly on relevant regions but also on those that, a priori, are not so\ninformative for the discrimination of the class. We tested the proposed\napproach by embedding it into the learning process of a convolutional neural\nnetwork for the analysis and classification of two well-known datasets, namely\nStanford cars and FGVC-Aircraft. Furthermore, we evaluated our model on a\nreal-case scenario for the classification of egocentric images, allowing us to\nobtain relevant information about peoples' lifestyles. In particular, we work\non the challenging EgoFoodPlaces dataset, achieving state-of-the-art results\nwith a lower level of complexity. The obtained results indicate the suitability\nof our proposed training scheme for image classification, improving the\nrobustness of the final model.",
          "link": "http://arxiv.org/abs/2012.14173",
          "publishedOn": "2021-07-29T02:00:08.714Z",
          "wordCount": 696,
          "title": "Playing to distraction: towards a robust training of CNN classifiers through visual explanation techniques. (arXiv:2012.14173v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaoqing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>",
          "description": "Crowd counting has drawn much attention due to its importance in\nsafety-critical surveillance systems. Especially, deep neural network (DNN)\nmethods have significantly reduced estimation errors for crowd counting\nmissions. Recent studies have demonstrated that DNNs are vulnerable to\nadversarial attacks, i.e., normal images with human-imperceptible perturbations\ncould mislead DNNs to make false predictions. In this work, we propose a robust\nattack strategy called Adversarial Patch Attack with Momentum (APAM) to\nsystematically evaluate the robustness of crowd counting models, where the\nattacker's goal is to create an adversarial perturbation that severely degrades\ntheir performances, thus leading to public safety accidents (e.g., stampede\naccidents). Especially, the proposed attack leverages the extreme-density\nbackground information of input images to generate robust adversarial patches\nvia a series of transformations (e.g., interpolation, rotation, etc.). We\nobserve that by perturbing less than 6\\% of image pixels, our attacks severely\ndegrade the performance of crowd counting systems, both digitally and\nphysically. To better enhance the adversarial robustness of crowd counting\nmodels, we propose the first regression model-based Randomized Ablation (RA),\nwhich is more sufficient than Adversarial Training (ADT) (Mean Absolute Error\nof RA is 5 lower than ADT on clean samples and 30 lower than ADT on adversarial\nexamples). Extensive experiments on five crowd counting models demonstrate the\neffectiveness and generality of the proposed method. The supplementary\nmaterials and certificate retrained models are available at\n\\url{https://www.dropbox.com/s/hc4fdx133vht0qb/ACM_MM2021_Supp.pdf?dl=0}",
          "link": "http://arxiv.org/abs/2104.10868",
          "publishedOn": "2021-07-29T02:00:08.692Z",
          "wordCount": 719,
          "title": "Towards Adversarial Patch Analysis and Certified Defense against Crowd Counting. (arXiv:2104.10868v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Somanath_G/0/1/0/all/0/1\">Gowri Somanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1\">Daniel Kurz</a>",
          "description": "We present a method to estimate an HDR environment map from a narrow\nfield-of-view LDR camera image in real-time. This enables perceptually\nappealing reflections and shading on virtual objects of any material finish,\nfrom mirror to diffuse, rendered into a real physical environment using\naugmented reality. Our method is based on our efficient convolutional neural\nnetwork architecture, EnvMapNet, trained end-to-end with two novel losses,\nProjectionLoss for the generated image, and ClusterLoss for adversarial\ntraining. Through qualitative and quantitative comparison to state-of-the-art\nmethods, we demonstrate that our algorithm reduces the directional error of\nestimated light sources by more than 50%, and achieves 3.7 times lower Frechet\nInception Distance (FID). We further showcase a mobile application that is able\nto run our neural network model in under 9 ms on an iPhone XS, and render in\nreal-time, visually coherent virtual objects in previously unseen real-world\nenvironments.",
          "link": "http://arxiv.org/abs/2011.10687",
          "publishedOn": "2021-07-29T02:00:08.676Z",
          "wordCount": 658,
          "title": "HDR Environment Map Estimation for Real-Time Augmented Reality. (arXiv:2011.10687v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.03064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1\">Xuefei Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Changcheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenshuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zixuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shuang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huazhong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>",
          "description": "Conducting efficient performance estimations of neural architectures is a\nmajor challenge in neural architecture search (NAS). To reduce the architecture\ntraining costs in NAS, one-shot estimators (OSEs) amortize the architecture\ntraining costs by sharing the parameters of one supernet between all\narchitectures. Recently, zero-shot estimators (ZSEs) that involve no training\nare proposed to further reduce the architecture evaluation cost. Despite the\nhigh efficiency of these estimators, the quality of such estimations has not\nbeen thoroughly studied. In this paper, we conduct an extensive and organized\nassessment of OSEs and ZSEs on three NAS benchmarks: NAS-Bench-101/201/301.\nSpecifically, we employ a set of NAS-oriented criteria to study the behavior of\nOSEs and ZSEs and reveal that they have certain biases and variances. After\nanalyzing how and why the OSE estimations are unsatisfying, we explore how to\nmitigate the correlation gap of OSEs from several perspectives. For ZSEs, we\nfind that current ZSEs are not satisfying enough in these benchmark search\nspaces, and analyze their biases. Through our analysis, we give out suggestions\nfor future application and development of efficient architecture performance\nestimators. Furthermore, the analysis framework proposed in our work could be\nutilized in future research to give a more comprehensive understanding of newly\ndesigned architecture performance estimators. All codes and analysis scripts\nare available at https://github.com/walkerning/aw_nas.",
          "link": "http://arxiv.org/abs/2008.03064",
          "publishedOn": "2021-07-29T02:00:08.658Z",
          "wordCount": 706,
          "title": "Evaluating Efficient Performance Estimators of Neural Architectures. (arXiv:2008.03064v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiunn-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chenxi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achar_M/0/1/0/all/0/1\">Madhav Achar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1\">Maani Ghaffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grizzle_J/0/1/0/all/0/1\">Jessy W. Grizzle</a>",
          "description": "Sensor calibration, which can be intrinsic or extrinsic, is an essential step\nto achieve the measurement accuracy required for modern perception and\nnavigation systems deployed on autonomous robots. To date, intrinsic\ncalibration models for spinning LiDARs have been based on hypothesized based on\ntheir physical mechanisms, resulting in anywhere from three to ten parameters\nto be estimated from data, while no phenomenological models have yet been\nproposed for solid-state LiDARs. Instead of going down that road, we propose to\nabstract away from the physics of a LiDAR type (spinning vs solid-state, for\nexample), and focus on the spatial geometry of the point cloud generated by the\nsensor. By modeling the calibration parameters as an element of a special\nmatrix Lie Group, we achieve a unifying view of calibration for different types\nof LiDARs. We further prove mathematically that the proposed model is\nwell-constrained (has a unique answer) given four appropriately orientated\ntargets. The proof provides a guideline for target positioning in the form of a\ntetrahedron. Moreover, an existing Semidefinite programming global solver for\nSE(3) can be modified to compute efficiently the optimal calibration\nparameters. For solid state LiDARs, we illustrate how the method works in\nsimulation. For spinning LiDARs, we show with experimental data that the\nproposed matrix Lie Group model performs equally well as physics-based models\nin terms of reducing the P2P distance, while being more robust to noise.",
          "link": "http://arxiv.org/abs/2012.03321",
          "publishedOn": "2021-07-29T02:00:08.644Z",
          "wordCount": 701,
          "title": "Global Unifying Intrinsic Calibration for Spinning and Solid-State LiDARs. (arXiv:2012.03321v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13542",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wyburd_M/0/1/0/all/0/1\">Madeleine K. Wyburd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dinsdale_N/0/1/0/all/0/1\">Nicola K. Dinsdale</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Namburete_A/0/1/0/all/0/1\">Ana I.L. Namburete</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jenkinson_M/0/1/0/all/0/1\">Mark Jenkinson</a>",
          "description": "Accurate topology is key when performing meaningful anatomical segmentations,\nhowever, it is often overlooked in traditional deep learning methods. In this\nwork we propose TEDS-Net: a novel segmentation method that guarantees accurate\ntopology. Our method is built upon a continuous diffeomorphic framework, which\nenforces topology preservation. However, in practice, diffeomorphic fields are\nrepresented using a finite number of parameters and sampled using methods such\nas linear interpolation, violating the theoretical guarantees. We therefore\nintroduce additional modifications to more strictly enforce it. Our network\nlearns how to warp a binary prior, with the desired topological\ncharacteristics, to complete the segmentation task. We tested our method on\nmyocardium segmentation from an open-source 2D heart dataset. TEDS-Net\npreserved topology in 100% of the cases, compared to 90% from the U-Net,\nwithout sacrificing on Hausdorff Distance or Dice performance. Code will be\nmade available at: www.github.com/mwyburd/TEDS-Net",
          "link": "http://arxiv.org/abs/2107.13542",
          "publishedOn": "2021-07-29T02:00:08.636Z",
          "wordCount": 613,
          "title": "TEDS-Net: Enforcing Diffeomorphisms in Spatial Transformers to Guarantee Topology Preservation in Segmentations. (arXiv:2107.13542v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.08032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1\">Spandan Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1\">Timothy Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dozier_J/0/1/0/all/0/1\">Jamell Dozier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_H/0/1/0/all/0/1\">Helen Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandari_N/0/1/0/all/0/1\">Nishchal Bhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1\">Fr&#xe9;do Durand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>",
          "description": "Object recognition and viewpoint estimation lie at the heart of visual\nunderstanding. Recent works suggest that convolutional neural networks (CNNs)\nfail to generalize to out-of-distribution (OOD) category-viewpoint\ncombinations, ie. combinations not seen during training. In this paper, we\ninvestigate when and how such OOD generalization may be possible by evaluating\nCNNs trained to classify both object category and 3D viewpoint on OOD\ncombinations, and identifying the neural mechanisms that facilitate such OOD\ngeneralization. We show that increasing the number of in-distribution\ncombinations (ie. data diversity) substantially improves generalization to OOD\ncombinations, even with the same amount of training data. We compare learning\ncategory and viewpoint in separate and shared network architectures, and\nobserve starkly different trends on in-distribution and OOD combinations, ie.\nwhile shared networks are helpful in-distribution, separate networks\nsignificantly outperform shared ones at OOD combinations. Finally, we\ndemonstrate that such OOD generalization is facilitated by the neural mechanism\nof specialization, ie. the emergence of two types of neurons -- neurons\nselective to category and invariant to viewpoint, and vice versa.",
          "link": "http://arxiv.org/abs/2007.08032",
          "publishedOn": "2021-07-29T02:00:08.629Z",
          "wordCount": 654,
          "title": "When and how do CNNs generalize to out-of-distribution category-viewpoint combinations?. (arXiv:2007.08032v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yubin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Pengyi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>",
          "description": "There has been a growing interest in unsupervised domain adaptation (UDA) to\nalleviate the data scalability issue, while the existing works usually focus on\nclassifying independently discrete labels. However, in many tasks (e.g.,\nmedical diagnosis), the labels are discrete and successively distributed. The\nUDA for ordinal classification requires inducing non-trivial ordinal\ndistribution prior to the latent space. Target for this, the partially ordered\nset (poset) is defined for constraining the latent vector. Instead of the\ntypically i.i.d. Gaussian latent prior, in this work, a recursively conditional\nGaussian (RCG) set is proposed for ordered constraint modeling, which admits a\ntractable joint distribution prior. Furthermore, we are able to control the\ndensity of content vectors that violate the poset constraint by a simple\n\"three-sigma rule\". We explicitly disentangle the cross-domain images into a\nshared ordinal prior induced ordinal content space and two separate\nsource/target ordinal-unrelated spaces, and the self-training is worked on the\nshared space exclusively for ordinal-aware domain alignment. Extensive\nexperiments on UDA medical diagnoses and facial age estimation demonstrate its\neffectiveness.",
          "link": "http://arxiv.org/abs/2107.13467",
          "publishedOn": "2021-07-29T02:00:08.588Z",
          "wordCount": 624,
          "title": "Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation. (arXiv:2107.13467v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenhao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eun_K/0/1/0/all/0/1\">Kim Ji Eun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>",
          "description": "Deep Generative Models (DGMs) are known for their superior capability in\ngenerating realistic data. Extending purely data-driven approaches, recent\nspecialized DGMs may satisfy additional controllable requirements such as\nembedding a traffic sign in a driving scene, by manipulating patterns\n\\textit{implicitly} in the neuron or feature level. In this paper, we introduce\na novel method to incorporate domain knowledge \\textit{explicitly} in the\ngeneration process to achieve semantically controllable scene generation. We\ncategorize our knowledge into two types to be consistent with the composition\nof natural scenes, where the first type represents the property of objects and\nthe second type represents the relationship among objects. We then propose a\ntree-structured generative model to learn complex scene representation, whose\nnodes and edges are naturally corresponding to the two types of knowledge\nrespectively. Knowledge can be explicitly integrated to enable semantically\ncontrollable scene generation by imposing semantic rules on properties of nodes\nand edges in the tree structure. We construct a synthetic example to illustrate\nthe controllability and explainability of our method in a clean setting. We\nfurther extend the synthetic example to realistic autonomous vehicle driving\nenvironments and conduct extensive experiments to show that our method\nefficiently identifies adversarial traffic scenes against different\nstate-of-the-art 3D point cloud segmentation models satisfying the traffic\nrules specified as the explicit knowledge.",
          "link": "http://arxiv.org/abs/2106.04066",
          "publishedOn": "2021-07-29T02:00:08.567Z",
          "wordCount": 703,
          "title": "Semantically Controllable Scene Generation with Guidance of Explicit Knowledge. (arXiv:2106.04066v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13407",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mora_Martin_G/0/1/0/all/0/1\">Germ&#xe1;n Mora-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turpin_A/0/1/0/all/0/1\">Alex Turpin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ruget_A/0/1/0/all/0/1\">Alice Ruget</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halimi_A/0/1/0/all/0/1\">Abderrahim Halimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Henderson_R/0/1/0/all/0/1\">Robert Henderson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leach_J/0/1/0/all/0/1\">Jonathan Leach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gyongy_I/0/1/0/all/0/1\">Istvan Gyongy</a>",
          "description": "3D time-of-flight (ToF) imaging is used in a variety of applications such as\naugmented reality (AR), computer interfaces, robotics and autonomous systems.\nSingle-photon avalanche diodes (SPADs) are one of the enabling technologies\nproviding accurate depth data even over long ranges. By developing SPADs in\narray format with integrated processing combined with pulsed, flood-type\nillumination, high-speed 3D capture is possible. However, array sizes tend to\nbe relatively small, limiting the lateral resolution of the resulting depth\nmaps, and, consequently, the information that can be extracted from the image\nfor applications such as object detection. In this paper, we demonstrate that\nthese limitations can be overcome through the use of convolutional neural\nnetworks (CNNs) for high-performance object detection. We present outdoor\nresults from a portable SPAD camera system that outputs 16-bin photon timing\nhistograms with 64x32 spatial resolution. The results, obtained with exposure\ntimes down to 2 ms (equivalent to 500 FPS) and in signal-to-background (SBR)\nratios as low as 0.05, point to the advantages of providing the CNN with full\nhistogram data rather than point clouds alone. Alternatively, a combination of\npoint cloud and active intensity data may be used as input, for a similar level\nof performance. In either case, the GPU-accelerated processing time is less\nthan 1 ms per frame, leading to an overall latency (image acquisition plus\nprocessing) in the millisecond range, making the results relevant for\nsafety-critical computer vision applications which would benefit from faster\nthan human reaction times.",
          "link": "http://arxiv.org/abs/2107.13407",
          "publishedOn": "2021-07-29T02:00:08.547Z",
          "wordCount": 705,
          "title": "High-speed object detection with a single-photon time-of-flight image sensor. (arXiv:2107.13407v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15564",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_Q/0/1/0/all/0/1\">Qingcheng Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_L/0/1/0/all/0/1\">Lin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1\">He Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xing_J/0/1/0/all/0/1\">Jiezhen Xing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jicong Zhang</a>",
          "description": "The novel Coronavirus disease (COVID-19) is a highly contagious virus and has\nspread all over the world, posing an extremely serious threat to all countries.\nAutomatic lung infection segmentation from computed tomography (CT) plays an\nimportant role in the quantitative analysis of COVID-19. However, the major\nchallenge lies in the inadequacy of annotated COVID-19 datasets. Currently,\nthere are several public non-COVID lung lesion segmentation datasets, providing\nthe potential for generalizing useful information to the related COVID-19\nsegmentation task. In this paper, we propose a novel relation-driven\ncollaborative learning model to exploit shared knowledge from non-COVID lesions\nfor annotation-efficient COVID-19 CT lung infection segmentation. The model\nconsists of a general encoder to capture general lung lesion features based on\nmultiple non-COVID lesions, and a target encoder to focus on task-specific\nfeatures based on COVID-19 infections. Features extracted from the two parallel\nencoders are concatenated for the subsequent decoder part. We develop a\ncollaborative learning scheme to regularize feature-level relation consistency\nof given input and encourage the model to learn more general and discriminative\nrepresentation of COVID-19 infections. Extensive experiments demonstrate that\ntrained with limited COVID-19 data, exploiting shared knowledge from non-COVID\nlesions can further improve state-of-the-art performance with up to 3.0% in\ndice similarity coefficient and 4.2% in normalized surface dice. Our proposed\nmethod promotes new insights into annotation-efficient deep learning for\nCOVID-19 infection segmentation and illustrates strong potential for real-world\napplications in the global fight against COVID-19 in the absence of sufficient\nhigh-quality annotations.",
          "link": "http://arxiv.org/abs/2012.15564",
          "publishedOn": "2021-07-29T02:00:08.536Z",
          "wordCount": 768,
          "title": "Exploiting Shared Knowledge from Non-COVID Lesions for Annotation-Efficient COVID-19 CT Lung Infection Segmentation. (arXiv:2012.15564v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Son_W/0/1/0/all/0/1\">Wonchul Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_J/0/1/0/all/0/1\">Jaemin Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Junyong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonjun Hwang</a>",
          "description": "With the success of deep neural networks, knowledge distillation which guides\nthe learning of a small student network from a large teacher network is being\nactively studied for model compression and transfer learning. However, few\nstudies have been performed to resolve the poor learning issue of the student\nnetwork when the student and teacher model sizes significantly differ. In this\npaper, we propose a densely guided knowledge distillation using multiple\nteacher assistants that gradually decreases the model size to efficiently\nbridge the large gap between the teacher and student networks. To stimulate\nmore efficient learning of the student network, we guide each teacher assistant\nto every other smaller teacher assistants iteratively. Specifically, when\nteaching a smaller teacher assistant at the next step, the existing larger\nteacher assistants from the previous step are used as well as the teacher\nnetwork. Moreover, we design stochastic teaching where, for each mini-batch, a\nteacher or teacher assistants are randomly dropped. This acts as a regularizer\nto improve the efficiency of teaching of the student network. Thus, the student\ncan always learn salient distilled knowledge from the multiple sources. We\nverified the effectiveness of the proposed method for a classification task\nusing CIFAR-10, CIFAR-100, and ImageNet. We also achieved significant\nperformance improvements with various backbone architectures such as ResNet,\nWideResNet, and VGG.",
          "link": "http://arxiv.org/abs/2009.08825",
          "publishedOn": "2021-07-29T02:00:08.529Z",
          "wordCount": 684,
          "title": "Densely Guided Knowledge Distillation using Multiple Teacher Assistants. (arXiv:2009.08825v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weiherer_M/0/1/0/all/0/1\">Maximilian Weiherer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eigenberger_A/0/1/0/all/0/1\">Andreas Eigenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brebant_V/0/1/0/all/0/1\">Vanessa Br&#xe9;bant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prantl_L/0/1/0/all/0/1\">Lukas Prantl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palm_C/0/1/0/all/0/1\">Christoph Palm</a>",
          "description": "We present the Regensburg Breast Shape Model (RBSM) - a 3D statistical shape\nmodel of the female breast built from 110 breast scans, and the first ever\npublicly available. Together with the model, a fully automated, pairwise\nsurface registration pipeline used to establish correspondence among 3D breast\nscans is introduced. Our method is computationally efficient and requires only\nfour landmarks to guide the registration process. In order to weaken the strong\ncoupling between breast and thorax, we propose to minimize the variance outside\nthe breast region as much as possible. To achieve this goal, a novel concept\ncalled breast probability masks (BPMs) is introduced. A BPM assigns\nprobabilities to each point of a 3D breast scan, telling how likely it is that\na particular point belongs to the breast area. During registration, we use BPMs\nto align the template to the target as accurately as possible inside the breast\nregion and only roughly outside. This simple yet effective strategy\nsignificantly reduces the unwanted variance outside the breast region, leading\nto better statistical shape models in which breast shapes are quite well\ndecoupled from the thorax. The RBSM is thus able to produce a variety of\ndifferent breast shapes as independently as possible from the shape of the\nthorax. Our systematic experimental evaluation reveals a generalization ability\nof 0.17 mm and a specificity of 2.8 mm for the RBSM. Ultimately, our model is\nseen as a first step towards combining physically motivated deformable models\nof the breast and statistical approaches in order to enable more realistic\nsurgical outcome simulation.",
          "link": "http://arxiv.org/abs/2107.13463",
          "publishedOn": "2021-07-29T02:00:08.502Z",
          "wordCount": 745,
          "title": "Learning the shape of female breasts: an open-access 3D statistical shape model of the female breast built from 110 breast scans. (arXiv:2107.13463v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13431",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ge_S/0/1/0/all/0/1\">Shuang Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_Q/0/1/0/all/0/1\">Qiongyu Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1\">Wenquan Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_D/0/1/0/all/0/1\">Desheng Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Huabin Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaobo Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_K/0/1/0/all/0/1\">Kehong Yuan</a>",
          "description": "Ultrasound is the preferred choice for early screening of dense breast\ncancer. Clinically, doctors have to manually write the screening report which\nis time-consuming and laborious, and it is easy to miss and miswrite.\nTherefore, this paper proposes a method for efficiently generating personalized\nbreast ultrasound screening preliminary reports by AI, especially for benign\nand normal cases which account for the majority. Doctors then make simple\nadjustments or corrections to quickly generate final reports. The proposed\napproach has been tested using a database of 1133 breast tumor instances.\nExperimental results indicate this pipeline improves doctors' work efficiency\nby up to 90%, which greatly reduces repetitive work.",
          "link": "http://arxiv.org/abs/2107.13431",
          "publishedOn": "2021-07-29T02:00:08.487Z",
          "wordCount": 559,
          "title": "AI assisted method for efficiently generating breast ultrasound screening reports. (arXiv:2107.13431v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaodan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1\">Xingxing Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dekui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Ying He</a>",
          "description": "The existing 3D deep learning methods adopt either individual point-based\nfeatures or local-neighboring voxel-based features, and demonstrate great\npotential for processing 3D data. However, the point based models are\ninefficient due to the unordered nature of point clouds and the voxel-based\nmodels suffer from large information loss. Motivated by the success of recent\npoint-voxel representation, such as PVCNN, we propose a new convolutional\nneural network, called Multi Point-Voxel Convolution (MPVConv), for deep\nlearning on point clouds. Integrating both the advantages of voxel and\npoint-based methods, MPVConv can effectively increase the neighboring\ncollection between point-based features and also promote independence among\nvoxel-based features. Moreover, most of the existing approaches aim at solving\none specific task, and only a few of them can handle a variety of tasks. Simply\nreplacing the corresponding convolution module with MPVConv, we show that\nMPVConv can fit in different backbones to solve a wide range of 3D tasks.\nExtensive experiments on benchmark datasets such as ShapeNet Part, S3DIS and\nKITTI for various tasks show that MPVConv improves the accuracy of the backbone\n(PointNet) by up to \\textbf{36\\%}, and achieves higher accuracy than the\nvoxel-based model with up to \\textbf{34}$\\times$ speedups. In addition, MPVConv\noutperforms the state-of-the-art point-based models with up to\n\\textbf{8}$\\times$ speedups. Notably, our MPVConv achieves better accuracy than\nthe newest point-voxel-based model PVCNN (a model more efficient than PointNet)\nwith lower latency.",
          "link": "http://arxiv.org/abs/2107.13152",
          "publishedOn": "2021-07-29T02:00:08.460Z",
          "wordCount": 679,
          "title": "Multi Point-Voxel Convolution (MPVConv) for Deep Learning on Point Clouds. (arXiv:2107.13152v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weixia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>",
          "description": "The computational vision community has recently paid attention to continual\nlearning for blind image quality assessment (BIQA). The primary challenge is to\ncombat catastrophic forgetting of previously-seen IQA datasets (i.e., tasks).\nIn this paper, we present a simple yet effective continual learning method for\nBIQA with improved quality prediction accuracy, plasticity-stability trade-off,\nand task-order/length robustness. The key step in our approach is to freeze all\nconvolution filters of a pre-trained deep neural network (DNN) for an explicit\npromise of stability, and learn task-specific normalization parameters for\nplasticity. We assign each new task a prediction head, and load the\ncorresponding normalization parameters to produce a quality score. The final\nquality estimate is computed by feature fusion and adaptive weighting using\nhierarchical representations, without leveraging the test-time oracle.\nExtensive experiments on six IQA datasets demonstrate the advantages of the\nproposed method in comparison to previous training techniques for BIQA.",
          "link": "http://arxiv.org/abs/2107.13429",
          "publishedOn": "2021-07-29T02:00:08.441Z",
          "wordCount": 591,
          "title": "Task-Specific Normalization for Continual Learning of Blind Image Quality Models. (arXiv:2107.13429v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14331",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Das_A/0/1/0/all/0/1\">Abhranil Das</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Geisler_W/0/1/0/all/0/1\">Wilson S Geisler</a>",
          "description": "Univariate and multivariate normal probability distributions are widely used\nwhen modeling decisions under uncertainty. Computing the performance of such\nmodels requires integrating these distributions over specific domains, which\ncan vary widely across models. Besides some special cases where these integrals\nare easy to calculate, there exist no general analytical expressions, standard\nnumerical methods or software for these integrals. Here we present mathematical\nresults and open-source software that provide (i) the probability in any domain\nof a normal in any dimensions with any parameters, (ii) the probability\ndensity, cumulative distribution, and inverse cumulative distribution of any\nfunction of a normal vector, (iii) the classification errors among any number\nof normal distributions, the Bayes-optimal discriminability index and relation\nto the operating characteristic, (iv) dimension reduction and visualizations\nfor such problems, and (v) tests for how reliably these methods may be used on\ngiven data. We demonstrate these tools with vision research applications of\ndetecting occluding objects in natural scenes, and detecting camouflage.",
          "link": "http://arxiv.org/abs/2012.14331",
          "publishedOn": "2021-07-29T02:00:08.434Z",
          "wordCount": 681,
          "title": "A method to integrate and classify normal distributions. (arXiv:2012.14331v7 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jianing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_F/0/1/0/all/0/1\">Frank P.-W. Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yingnan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_B/0/1/0/all/0/1\">Benny Lo</a>",
          "description": "Accurate prediction of future person location and movement trajectory from an\negocentric wearable camera can benefit a wide range of applications, such as\nassisting visually impaired people in navigation, and the development of\nmobility assistance for people with disability. In this work, a new egocentric\ndataset was constructed using a wearable camera, with 8,250 short clips of a\ntargeted person either walking 1) toward, 2) away, or 3) across the camera\nwearer in indoor environments, or 4) staying still in the scene, and 13,817\nperson bounding boxes were manually labelled. Apart from the bounding boxes,\nthe dataset also contains the estimated pose of the targeted person as well as\nthe IMU signal of the wearable camera at each time point. An LSTM-based\nencoder-decoder framework was designed to predict the future location and\nmovement trajectory of the targeted person in this egocentric setting.\nExtensive experiments have been conducted on the new dataset, and have shown\nthat the proposed method is able to reliably and better predict future person\nlocation and trajectory in egocentric videos captured by the wearable camera\ncompared to three baselines.",
          "link": "http://arxiv.org/abs/2103.04019",
          "publishedOn": "2021-07-29T02:00:08.406Z",
          "wordCount": 671,
          "title": "Indoor Future Person Localization from an Egocentric Wearable Camera. (arXiv:2103.04019v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.00826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gautam_A/0/1/0/all/0/1\">Akshat Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sit_M/0/1/0/all/0/1\">Muhammed Sit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1\">Ibrahim Demir</a>",
          "description": "In this paper, we demonstrated a practical application of realistic river\nimage generation using deep learning. Specifically, we explored a generative\nadversarial network (GAN) model capable of generating high-resolution and\nrealistic river images that can be used to support modeling and analysis in\nsurface water estimation, river meandering, wetland loss, and other\nhydrological research studies. First, we have created an extensive repository\nof overhead river images to be used in training. Second, we incorporated the\nProgressive Growing GAN (PGGAN), a network architecture that iteratively trains\nsmaller-resolution GANs to gradually build up to a very high resolution to\ngenerate high quality (i.e., 1024x1024) synthetic river imagery. With simpler\nGAN architectures, difficulties arose in terms of exponential increase of\ntraining time and vanishing/exploding gradient issues, which the PGGAN\nimplementation seemed to significantly reduce. The results presented in this\nstudy show great promise in generating high-quality images and capturing the\ndetails of river structure and flow to support hydrological research, which\noften requires extensive imagery for model performance.",
          "link": "http://arxiv.org/abs/2003.00826",
          "publishedOn": "2021-07-29T02:00:08.397Z",
          "wordCount": 644,
          "title": "Realistic River Image Synthesis using Deep Generative Adversarial Networks. (arXiv:2003.00826v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hanxiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1\">Helena Kotthaus</a>",
          "description": "In the field of autonomous driving and robotics, point clouds are showing\ntheir excellent real-time performance as raw data from most of the mainstream\n3D sensors. Therefore, point cloud neural networks have become a popular\nresearch direction in recent years. So far, however, there has been little\ndiscussion about the explainability of deep neural networks for point clouds.\nIn this paper, we propose new explainability approaches for point cloud deep\nneural networks based on local surrogate model-based methods to show which\ncomponents make the main contribution to the classification. Moreover, we\npropose a quantitative validation method for explainability methods of point\nclouds which enhances the persuasive power of explainability by dropping the\nmost positive or negative contributing features and monitoring how the\nclassification scores of specific categories change. To enable an intuitive\nexplanation of misclassified instances, we display features with confounding\ncontributions. Our new explainability approach provides a fairly accurate, more\nintuitive and widely applicable explanation for point cloud classification\ntasks. Our code is available at https://github.com/Explain3D/Explainable3D",
          "link": "http://arxiv.org/abs/2107.13459",
          "publishedOn": "2021-07-29T02:00:08.384Z",
          "wordCount": 610,
          "title": "Surrogate Model-Based Explainability Methods for Point Cloud NNs. (arXiv:2107.13459v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Libo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haokui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wei Yin</a>",
          "description": "Road detection is a critically important task for self-driving cars. By\nemploying LiDAR data, recent works have significantly improved the accuracy of\nroad detection. Relying on LiDAR sensors limits the wide application of those\nmethods when only cameras are available. In this paper, we propose a novel road\ndetection approach with RGB being the only input during inference.\nSpecifically, we exploit pseudo-LiDAR using depth estimation, and propose a\nfeature fusion network where RGB and learned depth information are fused for\nimproved road detection. To further optimize the network structure and improve\nthe efficiency of the network. we search for the network structure of the\nfeature fusion module using NAS techniques. Finally, be aware of that\ngenerating pseudo-LiDAR from RGB via depth estimation introduces extra\ncomputational costs and relies on depth estimation networks, we design a\nmodality distillation strategy and leverage it to further free our network from\nthese extra computational cost and dependencies during inference. The proposed\nmethod achieves state-of-the-art performance on two challenging benchmarks,\nKITTI and R2D.",
          "link": "http://arxiv.org/abs/2107.13279",
          "publishedOn": "2021-07-29T02:00:08.377Z",
          "wordCount": 596,
          "title": "Pseudo-LiDAR Based Road Detection. (arXiv:2107.13279v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13200",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_B/0/1/0/all/0/1\">Bo Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_P/0/1/0/all/0/1\">Pengfei Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a> (Alzheimer&#x27;s Disease Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing), <a href=\"http://arxiv.org/find/eess/1/au:+Shen_S/0/1/0/all/0/1\">Shuwei Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_P/0/1/0/all/0/1\">Peng Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1\">Ronald X. Xu</a>",
          "description": "Early and accurate diagnosis of Alzheimer's disease (AD) and its prodromal\nperiod mild cognitive impairment (MCI) is essential for the delayed disease\nprogression and the improved quality of patients'life. The emerging\ncomputer-aided diagnostic methods that combine deep learning with structural\nmagnetic resonance imaging (sMRI) have achieved encouraging results, but some\nof them are limit of issues such as data leakage and unexplainable diagnosis.\nIn this research, we propose a novel end-to-end deep learning approach for\nautomated diagnosis of AD and localization of important brain regions related\nto the disease from sMRI data. This approach is based on a 2D single model\nstrategy and has the following differences from the current approaches: 1)\nConvolutional Neural Network (CNN) models of different structures and\ncapacities are evaluated systemically and the most suitable model is adopted\nfor AD diagnosis; 2) a data augmentation strategy named Two-stage Random\nRandAugment (TRRA) is proposed to alleviate the overfitting issue caused by\nlimited training data and to improve the classification performance in AD\ndiagnosis; 3) an explainable method of Grad-CAM++ is introduced to generate the\nvisually explainable heatmaps that localize and highlight the brain regions\nthat our model focuses on and to make our model more transparent. Our approach\nhas been evaluated on two publicly accessible datasets for two classification\ntasks of AD vs. cognitively normal (CN) and progressive MCI (pMCI) vs. stable\nMCI (sMCI). The experimental results indicate that our approach outperforms the\nstate-of-the-art approaches, including those using multi-model and 3D CNN\nmethods. The resultant localization heatmaps from our approach also highlight\nthe lateral ventricle and some disease-relevant regions of cortex, coincident\nwith the commonly affected regions during the development of AD.",
          "link": "http://arxiv.org/abs/2107.13200",
          "publishedOn": "2021-07-29T02:00:08.356Z",
          "wordCount": 760,
          "title": "An explainable two-dimensional single model deep learning approach for Alzheimer's disease diagnosis and brain atrophy localization. (arXiv:2107.13200v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/1907.01845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruijun Xu</a>",
          "description": "One of the most critical problems in weight-sharing neural architecture\nsearch is the evaluation of candidate models within a predefined search space.\nIn practice, a one-shot supernet is trained to serve as an evaluator. A\nfaithful ranking certainly leads to more accurate searching results. However,\ncurrent methods are prone to making misjudgments. In this paper, we prove that\ntheir biased evaluation is due to inherent unfairness in the supernet training.\nIn view of this, we propose two levels of constraints: expectation fairness and\nstrict fairness. Particularly, strict fairness ensures equal optimization\nopportunities for all choice blocks throughout the training, which neither\noverestimates nor underestimates their capacity. We demonstrate that this is\ncrucial for improving the confidence of models' ranking. Incorporating the\none-shot supernet trained under the proposed fairness constraints with a\nmulti-objective evolutionary search algorithm, we obtain various\nstate-of-the-art models, e.g., FairNAS-A attains 77.5% top-1 validation\naccuracy on ImageNet. The models and their evaluation codes are made publicly\navailable online this http URL .",
          "link": "http://arxiv.org/abs/1907.01845",
          "publishedOn": "2021-07-29T02:00:08.349Z",
          "wordCount": 672,
          "title": "FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search. (arXiv:1907.01845v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.01446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chongwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1\">Yulong Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Caifei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xin Fan</a>",
          "description": "To boost the object grabbing capability of underwater robots for open-sea\nfarming, we propose a new dataset (UDD) consisting of three categories\n(seacucumber, seaurchin, and scallop) with 2,227 images. To the best of our\nknowledge, it is the first 4K HD dataset collected in a real open-sea farm. We\nalso propose a novel Poisson-blending Generative Adversarial Network (Poisson\nGAN) and an efficient object detection network (AquaNet) to address two common\nissues within related datasets: the class-imbalance problem and the problem of\nmass small object, respectively. Specifically, Poisson GAN combines Poisson\nblending into its generator and employs a new loss called Dual Restriction loss\n(DR loss), which supervises both implicit space features and image-level\nfeatures during training to generate more realistic images. By utilizing\nPoisson GAN, objects of minority class like seacucumber or scallop could be\nadded into an image naturally and annotated automatically, which could increase\nthe loss of minority classes during training detectors to eliminate the\nclass-imbalance problem; AquaNet is a high-efficiency detector to address the\nproblem of detecting mass small objects from cloudy underwater pictures. Within\nit, we design two efficient components: a depth-wise-convolution-based\nMulti-scale Contextual Features Fusion (MFF) block and a Multi-scale\nBlursampling (MBP) module to reduce the parameters of the network to 1.3\nmillion. Both two components could provide multi-scale features of small\nobjects under a short backbone configuration without any loss of accuracy. In\naddition, we construct a large-scale augmented dataset (AUDD) and a\npre-training dataset via Poisson GAN from UDD. Extensive experiments show the\neffectiveness of the proposed Poisson GAN, AquaNet, UDD, AUDD, and pre-training\ndataset.",
          "link": "http://arxiv.org/abs/2003.01446",
          "publishedOn": "2021-07-29T02:00:08.335Z",
          "wordCount": 750,
          "title": "A New Dataset, Poisson GAN and AquaNet for Underwater Object Grabbing. (arXiv:2003.01446v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Widya_A/0/1/0/all/0/1\">Aji Resindra Widya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monno_Y/0/1/0/all/0/1\">Yusuke Monno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okutomi_M/0/1/0/all/0/1\">Masatoshi Okutomi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_S/0/1/0/all/0/1\">Sho Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotoda_T/0/1/0/all/0/1\">Takuji Gotoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miki_K/0/1/0/all/0/1\">Kenji Miki</a>",
          "description": "Gastroendoscopy has been a clinical standard for diagnosing and treating\nconditions that affect a part of a patient's digestive system, such as the\nstomach. Despite the fact that gastroendoscopy has a lot of advantages for\npatients, there exist some challenges for practitioners, such as the lack of 3D\nperception, including the depth and the endoscope pose information. Such\nchallenges make navigating the endoscope and localizing any found lesion in a\ndigestive tract difficult. To tackle these problems, deep learning-based\napproaches have been proposed to provide monocular gastroendoscopy with\nadditional yet important depth and pose information. In this paper, we propose\na novel supervised approach to train depth and pose estimation networks using\nconsecutive endoscopy images to assist the endoscope navigation in the stomach.\nWe firstly generate real depth and pose training data using our previously\nproposed whole stomach 3D reconstruction pipeline to avoid poor generalization\nability between computer-generated (CG) models and real data for the stomach.\nIn addition, we propose a novel generalized photometric loss function to avoid\nthe complicated process of finding proper weights for balancing the depth and\nthe pose loss terms, which is required for existing direct depth and pose\nsupervision approaches. We then experimentally show that our proposed\ngeneralized loss performs better than existing direct supervision losses.",
          "link": "http://arxiv.org/abs/2107.13263",
          "publishedOn": "2021-07-29T02:00:08.327Z",
          "wordCount": 664,
          "title": "Learning-Based Depth and Pose Estimation for Monocular Endoscope with Loss Generalization. (arXiv:2107.13263v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramamonjison_R/0/1/0/all/0/1\">Rindra Ramamonjison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xinyu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiaolong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>",
          "description": "This paper presents a Simple and effective unsupervised adaptation method for\nRobust Object Detection (SimROD). To overcome the challenging issues of domain\nshift and pseudo-label noise, our method integrates a novel domain-centric\naugmentation method, a gradual self-labeling adaptation procedure, and a\nteacher-guided fine-tuning mechanism. Using our method, target domain samples\ncan be leveraged to adapt object detection models without changing the model\narchitecture or generating synthetic data. When applied to image corruptions\nand high-level cross-domain adaptation benchmarks, our method outperforms prior\nbaselines on multiple domain adaptation benchmarks. SimROD achieves new\nstate-of-the-art on standard real-to-synthetic and cross-camera setup\nbenchmarks. On the image corruption benchmark, models adapted with our method\nachieved a relative robustness improvement of 15-25% AP50 on Pascal-C and 5-6%\nAP on COCO-C and Cityscapes-C. On the cross-domain benchmark, our method\noutperformed the best baseline performance by up to 8% AP50 on Comic dataset\nand up to 4% on Watercolor dataset.",
          "link": "http://arxiv.org/abs/2107.13389",
          "publishedOn": "2021-07-29T02:00:08.319Z",
          "wordCount": 612,
          "title": "SimROD: A Simple Adaptation Method for Robust Object Detection. (arXiv:2107.13389v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chunxia Xiao</a>",
          "description": "Generating photo-realistic images from a text description is a challenging\nproblem in computer vision. Previous works have shown promising performance to\ngenerate synthetic images conditional on text by Generative Adversarial\nNetworks (GANs). In this paper, we focus on the category-consistent and\nrelativistic diverse constraints to optimize the diversity of synthetic images.\nBased on those constraints, a category-consistent and relativistic diverse\nconditional GAN (CRD-CGAN) is proposed to synthesize $K$ photo-realistic images\nsimultaneously. We use the attention loss and diversity loss to improve the\nsensitivity of the GAN to word attention and noises. Then, we employ the\nrelativistic conditional loss to estimate the probability of relatively real or\nfake for synthetic images, which can improve the performance of basic\nconditional loss. Finally, we introduce a category-consistent loss to alleviate\nthe over-category issues between K synthetic images. We evaluate our approach\nusing the Birds-200-2011, Oxford-102 flower and MSCOCO 2014 datasets, and the\nextensive experiments demonstrate superiority of the proposed method in\ncomparison with state-of-the-art methods in terms of photorealistic and\ndiversity of the generated synthetic images.",
          "link": "http://arxiv.org/abs/2107.13516",
          "publishedOn": "2021-07-29T02:00:08.298Z",
          "wordCount": 609,
          "title": "CRD-CGAN: Category-Consistent and Relativistic Constraints for Diverse Text-to-Image Generation. (arXiv:2107.13516v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ashlesha Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangwan_K/0/1/0/all/0/1\">Kuldip Singh Sangwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhiraj/0/1/0/all/0/1\">Dhiraj</a>",
          "description": "As the proportion of road accidents increases each year, driver distraction\ncontinues to be an important risk component in road traffic injuries and\ndeaths. The distractions caused by the increasing use of mobile phones and\nother wireless devices pose a potential risk to road safety. Our current study\naims to aid the already existing techniques in driver posture recognition by\nimproving the performance in the driver distraction classification problem. We\npresent an approach using a genetic algorithm-based ensemble of six independent\ndeep neural architectures, namely, AlexNet, VGG-16, EfficientNet B0, Vanilla\nCNN, Modified DenseNet, and InceptionV3 + BiLSTM. We test it on two\ncomprehensive datasets, the AUC Distracted Driver Dataset, on which our\ntechnique achieves an accuracy of 96.37%, surpassing the previously obtained\n95.98%, and on the State Farm Driver Distraction Dataset, on which we attain an\naccuracy of 99.75%. The 6-Model Ensemble gave an inference time of 0.024\nseconds as measured on our machine with Ubuntu 20.04(64-bit) and GPU as GeForce\nGTX 1080.",
          "link": "http://arxiv.org/abs/2107.13355",
          "publishedOn": "2021-07-29T02:00:08.291Z",
          "wordCount": 633,
          "title": "A Computer Vision-Based Approach for Driver Distraction Recognition using Deep Learning and Genetic Algorithm Based Ensemble. (arXiv:2107.13355v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jianing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_B/0/1/0/all/0/1\">Benny Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang-Zhong Yang</a>",
          "description": "In this report, the technical details of our submission to the EPIC-Kitchens\nAction Anticipation Challenge 2021 are given. We developed a hierarchical\nattention model for action anticipation, which leverages Transformer-based\nattention mechanism to aggregate features across temporal dimension,\nmodalities, symbiotic branches respectively. In terms of Mean Top-5 Recall of\naction, our submission with team name ICL-SJTU achieved 13.39% for overall\ntesting set, 10.05% for unseen subsets and 11.88% for tailed subsets.\nAdditionally, it is noteworthy that our submission ranked 1st in terms of verb\nclass in all three (sub)sets.",
          "link": "http://arxiv.org/abs/2107.13259",
          "publishedOn": "2021-07-29T02:00:08.283Z",
          "wordCount": 529,
          "title": "TransAction: ICL-SJTU Submission to EPIC-Kitchens Action Anticipation Challenge 2021. (arXiv:2107.13259v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kuiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>",
          "description": "Modelling long-range contextual relationships is critical for pixel-wise\nprediction tasks such as semantic segmentation. However, convolutional neural\nnetworks (CNNs) are inherently limited to model such dependencies due to the\nnaive structure in its building modules (\\eg, local convolution kernel). While\nrecent global aggregation methods are beneficial for long-range structure\ninformation modelling, they would oversmooth and bring noise to the regions\ncontaining fine details (\\eg,~boundaries and small objects), which are very\nmuch cared for the semantic segmentation task. To alleviate this problem, we\npropose to explore the local context for making the aggregated long-range\nrelationship being distributed more accurately in local regions. In particular,\nwe design a novel local distribution module which models the affinity map\nbetween global and local relationship for each pixel adaptively. Integrating\nexisting global aggregation modules, we show that our approach can be\nmodularized as an end-to-end trainable block and easily plugged into existing\nsemantic segmentation networks, giving rise to the \\emph{GALD} networks.\nDespite its simplicity and versatility, our approach allows us to build new\nstate of the art on major semantic segmentation benchmarks including\nCityscapes, ADE20K, Pascal Context, Camvid and COCO-stuff. Code and trained\nmodels are released at \\url{https://github.com/lxtGH/GALD-DGCNet} to foster\nfurther research.",
          "link": "http://arxiv.org/abs/2107.13154",
          "publishedOn": "2021-07-29T02:00:08.276Z",
          "wordCount": 652,
          "title": "Global Aggregation then Local Distribution for Scene Parsing. (arXiv:2107.13154v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeesoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1\">Junsuk Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sangdoo Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>",
          "description": "Weakly-supervised object localization (WSOL) enables finding an object using\na dataset without any localization information. By simply training a\nclassification model using only image-level annotations, the feature map of the\nmodel can be utilized as a score map for localization. In spite of many WSOL\nmethods proposing novel strategies, there has not been any de facto standard\nabout how to normalize the class activation map (CAM). Consequently, many WSOL\nmethods have failed to fully exploit their own capacity because of the misuse\nof a normalization method. In this paper, we review many existing normalization\nmethods and point out that they should be used according to the property of the\ngiven dataset. Additionally, we propose a new normalization method which\nsubstantially enhances the performance of any CAM-based WSOL methods. Using the\nproposed normalization method, we provide a comprehensive evaluation over three\ndatasets (CUB, ImageNet and OpenImages) on three different architectures and\nobserve significant performance gains over the conventional min-max\nnormalization method in all the evaluated cases.",
          "link": "http://arxiv.org/abs/2107.13221",
          "publishedOn": "2021-07-29T02:00:08.264Z",
          "wordCount": 608,
          "title": "Normalization Matters in Weakly Supervised Object Localization. (arXiv:2107.13221v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lomurno_E/0/1/0/all/0/1\">Eugenio Lomurno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romanoni_A/0/1/0/all/0/1\">Andrea Romanoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matteucci_M/0/1/0/all/0/1\">Matteo Matteucci</a>",
          "description": "Today, Multi-View Stereo techniques are able to reconstruct robust and\ndetailed 3D models, especially when starting from high-resolution images.\nHowever, there are cases in which the resolution of input images is relatively\nlow, for instance, when dealing with old photos, or when hardware constrains\nthe amount of data that can be acquired. In this paper, we investigate if, how,\nand how much increasing the resolution of such input images through\nSuper-Resolution techniques reflects in quality improvements of the\nreconstructed 3D models, despite the artifacts that sometimes this may\ngenerate. We show that applying a Super-Resolution step before recovering the\ndepth maps in most cases leads to a better 3D model both in the case of\nPatchMatch-based and deep-learning-based algorithms. The use of\nSuper-Resolution improves especially the completeness of reconstructed models\nand turns out to be particularly effective in the case of textured scenes.",
          "link": "http://arxiv.org/abs/2107.13261",
          "publishedOn": "2021-07-29T02:00:08.240Z",
          "wordCount": 574,
          "title": "Improving Multi-View Stereo via Super-Resolution. (arXiv:2107.13261v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13465",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Ti Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balagopal_A/0/1/0/all/0/1\">Anjali Balagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohopolski_M/0/1/0/all/0/1\">Michael Dohopolski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_H/0/1/0/all/0/1\">Howard E. Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McBeth_R/0/1/0/all/0/1\">Rafe McBeth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jun Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mu-Han Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sher_D/0/1/0/all/0/1\">David J. Sher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Steve Jiang</a>",
          "description": "Automatic segmentation of anatomical structures is critical for many medical\napplications. However, the results are not always clinically acceptable and\nrequire tedious manual revision. Here, we present a novel concept called\nartificial intelligence assisted contour revision (AIACR) and demonstrate its\nfeasibility. The proposed clinical workflow of AIACR is as follows given an\ninitial contour that requires a clinicians revision, the clinician indicates\nwhere a large revision is needed, and a trained deep learning (DL) model takes\nthis input to update the contour. This process repeats until a clinically\nacceptable contour is achieved. The DL model is designed to minimize the\nclinicians input at each iteration and to minimize the number of iterations\nneeded to reach acceptance. In this proof-of-concept study, we demonstrated the\nconcept on 2D axial images of three head-and-neck cancer datasets, with the\nclinicians input at each iteration being one mouse click on the desired\nlocation of the contour segment. The performance of the model is quantified\nwith Dice Similarity Coefficient (DSC) and 95th percentile of Hausdorff\nDistance (HD95). The average DSC/HD95 (mm) of the auto-generated initial\ncontours were 0.82/4.3, 0.73/5.6 and 0.67/11.4 for three datasets, which were\nimproved to 0.91/2.1, 0.86/2.4 and 0.86/4.7 with three mouse clicks,\nrespectively. Each DL-based contour update requires around 20 ms. We proposed a\nnovel AIACR concept that uses DL models to assist clinicians in revising\ncontours in an efficient and effective way, and we demonstrated its feasibility\nby using 2D axial CT images from three head-and-neck cancer datasets.",
          "link": "http://arxiv.org/abs/2107.13465",
          "publishedOn": "2021-07-29T02:00:08.231Z",
          "wordCount": 704,
          "title": "A Proof-of-Concept Study of Artificial Intelligence Assisted Contour Revision. (arXiv:2107.13465v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1\">Javier Naranjo-Alcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1\">Sergi Perez-Castanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Garcia_A/0/1/0/all/0/1\">Aaron Lopez-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1\">Pedro Zuccarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1\">Maximo Cobos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1\">Francesc J. Ferri</a>",
          "description": "The use of multiple and semantically correlated sources can provide\ncomplementary information to each other that may not be evident when working\nwith individual modalities on their own. In this context, multi-modal models\ncan help producing more accurate and robust predictions in machine learning\ntasks where audio-visual data is available. This paper presents a multi-modal\nmodel for automatic scene classification that exploits simultaneously auditory\nand visual information. The proposed approach makes use of two separate\nnetworks which are respectively trained in isolation on audio and visual data,\nso that each network specializes in a given modality. The visual subnetwork is\na pre-trained VGG16 model followed by a bidiretional recurrent layer, while the\nresidual audio subnetwork is based on stacked squeeze-excitation convolutional\nblocks trained from scratch. After training each subnetwork, the fusion of\ninformation from the audio and visual streams is performed at two different\nstages. The early fusion stage combines features resulting from the last\nconvolutional block of the respective subnetworks at different time steps to\nfeed a bidirectional recurrent structure. The late fusion stage combines the\noutput of the early fusion stage with the independent predictions provided by\nthe two subnetworks, resulting in the final prediction. We evaluate the method\nusing the recently published TAU Audio-Visual Urban Scenes 2021, which contains\nsynchronized audio and video recordings from 12 European cities in 10 different\nscene classes. The proposed model has been shown to provide an excellent\ntrade-off between prediction performance (86.5%) and system complexity (15M\nparameters) in the evaluation results of the DCASE 2021 Challenge.",
          "link": "http://arxiv.org/abs/2107.13180",
          "publishedOn": "2021-07-29T02:00:08.204Z",
          "wordCount": 712,
          "title": "Squeeze-Excitation Convolutional Recurrent Neural Networks for Audio-Visual Scene Classification. (arXiv:2107.13180v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13273",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barquero_G/0/1/0/all/0/1\">Germ&#xe1;n Barquero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupont_I/0/1/0/all/0/1\">Isabelle Hupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_C/0/1/0/all/0/1\">Carles Fern&#xe1;ndez</a>",
          "description": "Most current multi-object trackers focus on short-term tracking, and are\nbased on deep and complex systems that often cannot operate in real-time,\nmaking them impractical for video-surveillance. In this paper we present a\nlong-term, multi-face tracking architecture conceived for working in crowded\ncontexts where faces are often the only visible part of a person. Our system\nbenefits from advances in the fields of face detection and face recognition to\nachieve long-term tracking, and is particularly unconstrained to the motion and\nocclusions of people. It follows a tracking-by-detection approach, combining a\nfast short-term visual tracker with a novel online tracklet reconnection\nstrategy grounded on rank-based face verification. The proposed rank-based\nconstraint favours higher inter-class distance among tracklets, and reduces the\npropagation of errors due to wrong reconnections. Additionally, a correction\nmodule is included to correct past assignments with no extra computational\ncost. We present a series of experiments introducing novel specialized metrics\nfor the evaluation of long-term tracking capabilities, and publicly release a\nvideo dataset with 10 manually annotated videos and a total length of 8' 54\".\nOur findings validate the robustness of each of the proposed modules, and\ndemonstrate that, in these challenging contexts, our approach yields up to 50%\nlonger tracks than state-of-the-art deep learning trackers.",
          "link": "http://arxiv.org/abs/2107.13273",
          "publishedOn": "2021-07-29T02:00:08.192Z",
          "wordCount": 664,
          "title": "Rank-based verification for long-term face tracking in crowded scenes. (arXiv:2107.13273v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rodin_I/0/1/0/all/0/1\">Ivan Rodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1\">Antonino Furnari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavroedis_D/0/1/0/all/0/1\">Dimitrios Mavroedis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1\">Giovanni Maria Farinella</a>",
          "description": "Egocentric videos can bring a lot of information about how humans perceive\nthe world and interact with the environment, which can be beneficial for the\nanalysis of human behaviour. The research in egocentric video analysis is\ndeveloping rapidly thanks to the increasing availability of wearable devices\nand the opportunities offered by new large-scale egocentric datasets. As\ncomputer vision techniques continue to develop at an increasing pace, the tasks\nrelated to the prediction of future are starting to evolve from the need of\nunderstanding the present. Predicting future human activities, trajectories and\ninteractions with objects is crucial in applications such as human-robot\ninteraction, assistive wearable technologies for both industrial and daily\nliving scenarios, entertainment and virtual or augmented reality. This survey\nsummarises the evolution of studies in the context of future prediction from\negocentric vision making an overview of applications, devices, existing\nproblems, commonly used datasets, models and input modalities. Our analysis\nhighlights that methods for future prediction from egocentric vision can have a\nsignificant impact in a range of applications and that further research efforts\nshould be devoted to the standardisation of tasks and the proposal of datasets\nconsidering real-world scenarios such as the ones with an industrial vocation.",
          "link": "http://arxiv.org/abs/2107.13411",
          "publishedOn": "2021-07-29T02:00:08.160Z",
          "wordCount": 647,
          "title": "Predicting the Future from First Person (Egocentric) Vision: A Survey. (arXiv:2107.13411v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagong_M/0/1/0/all/0/1\">Min-Cheol Sagong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_Y/0/1/0/all/0/1\">Yoon-Jae Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Seung-Won Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1\">Sung-Jea Ko</a>",
          "description": "Convolutional neural networks (CNNs) have been not only widespread but also\nachieved noticeable results on numerous applications including image\nclassification, restoration, and generation. Although the weight-sharing\nproperty of convolutions makes them widely adopted in various tasks, its\ncontent-agnostic characteristic can also be considered a major drawback. To\nsolve this problem, in this paper, we propose a novel operation, called pixel\nadaptive kernel attention (PAKA). PAKA provides directivity to the filter\nweights by multiplying spatially varying attention from learnable features. The\nproposed method infers pixel-adaptive attention maps along the channel and\nspatial directions separately to address the decomposed model with fewer\nparameters. Our method is trainable in an end-to-end manner and applicable to\nany CNN-based models. In addition, we propose an improved information\naggregation module with PAKA, called the hierarchical PAKA module (HPM). We\ndemonstrate the superiority of our HPM by presenting state-of-the-art\nperformance on semantic segmentation compared to the conventional information\naggregation modules. We validate the proposed method through additional\nablation studies and visualizing the effect of PAKA providing directivity to\nthe weights of convolutions. We also show the generalizability of the proposed\nmethod by applying it to multi-modal tasks especially color-guided depth map\nsuper-resolution.",
          "link": "http://arxiv.org/abs/2107.13144",
          "publishedOn": "2021-07-29T02:00:08.140Z",
          "wordCount": 647,
          "title": "Content-aware Directed Propagation Network with Pixel Adaptive Kernel Attention. (arXiv:2107.13144v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_G/0/1/0/all/0/1\">Guohua Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1\">Xingxing Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xin Cao</a>",
          "description": "The repairing work of terracotta warriors in Emperor Qinshihuang Mausoleum\nSite Museum is handcrafted by experts, and the increasing amounts of unearthed\npieces of terracotta warriors make the archaeologists too challenging to\nconduct the restoration of terracotta warriors efficiently. We hope to segment\nthe 3D point cloud data of the terracotta warriors automatically and store the\nfragment data in the database to assist the archaeologists in matching the\nactual fragments with the ones in the database, which could result in higher\nrepairing efficiency of terracotta warriors. Moreover, the existing 3D neural\nnetwork research is mainly focusing on supervised classification, clustering,\nunsupervised representation, and reconstruction. There are few pieces of\nresearches concentrating on unsupervised point cloud part segmentation. In this\npaper, we present SRG-Net for 3D point clouds of terracotta warriors to address\nthese problems. Firstly, we adopt a customized seed-region-growing algorithm to\nsegment the point cloud coarsely. Then we present a supervised segmentation and\nunsupervised reconstruction networks to learn the characteristics of 3D point\nclouds. Finally, we combine the SRG algorithm with our improved CNN using a\nrefinement method. This pipeline is called SRG-Net, which aims at conducting\nsegmentation tasks on the terracotta warriors. Our proposed SRG-Net is\nevaluated on the terracotta warriors data and ShapeNet dataset by measuring the\naccuracy and the latency. The experimental results show that our SRG-Net\noutperforms the state-of-the-art methods. Our code is shown in Code File\n1~\\cite{Srgnet_2021}.",
          "link": "http://arxiv.org/abs/2107.13167",
          "publishedOn": "2021-07-29T02:00:08.132Z",
          "wordCount": 681,
          "title": "Unsupervised Segmentation for Terracotta Warrior with Seed-Region-Growing CNN(SRG-Net). (arXiv:2107.13167v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1\">Mariella Dimiccoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrido_L/0/1/0/all/0/1\">Llu&#xed;s Garrido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Corominas_G/0/1/0/all/0/1\">Guillem Rodriguez-Corominas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wendt_H/0/1/0/all/0/1\">Herwig Wendt</a>",
          "description": "Recently, transfer subspace learning based approaches have shown to be a\nvalid alternative to unsupervised subspace clustering and temporal data\nclustering for human motion segmentation (HMS). These approaches leverage prior\nknowledge from a source domain to improve clustering performance on a target\ndomain, and currently they represent the state of the art in HMS. Bucking this\ntrend, in this paper, we propose a novel unsupervised model that learns a\nrepresentation of the data and digs clustering information from the data\nitself. Our model is reminiscent of temporal subspace clustering, but presents\ntwo critical differences. First, we learn an auxiliary data matrix that can\ndeviate from the initial data, hence confer more degrees of freedom to the\ncoding matrix. Second, we introduce a regularization term for this auxiliary\ndata matrix that preserves the local geometrical structure present in the\nhigh-dimensional space. The proposed model is efficiently optimized by using an\noriginal Alternating Direction Method of Multipliers (ADMM) formulation\nallowing to learn jointly the auxiliary data representation, a nonnegative\ndictionary and a coding matrix. Experimental results on four benchmark datasets\nfor HMS demonstrate that our approach achieves significantly better clustering\nperformance then state-of-the-art methods, including both unsupervised and more\nrecent semi-supervised transfer learning approaches.",
          "link": "http://arxiv.org/abs/2107.13362",
          "publishedOn": "2021-07-29T02:00:08.124Z",
          "wordCount": 645,
          "title": "Graph Constrained Data Representation Learning for Human Motion Segmentation. (arXiv:2107.13362v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13237",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mukherjee_U/0/1/0/all/0/1\">Uddipan Mukherjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pancholi_S/0/1/0/all/0/1\">Sidharth Pancholi</a>",
          "description": "Heart disease is the most common reason for human mortality that causes\nalmost one-third of deaths throughout the world. Detecting the disease early\nincreases the chances of survival of the patient and there are several ways a\nsign of heart disease can be detected early. This research proposes to convert\ncleansed and normalized heart sound into visual mel scale spectrograms and then\nusing visual domain transfer learning approaches to automatically extract\nfeatures and categorize between heart sounds. Some of the previous studies\nfound that the spectrogram of various types of heart sounds is visually\ndistinguishable to human eyes, which motivated this study to experiment on\nvisual domain classification approaches for automated heart sound\nclassification. It will use convolution neural network-based architectures i.e.\nResNet, MobileNetV2, etc as the automated feature extractors from spectrograms.\nThese well-accepted models in the image domain showed to learn generalized\nfeature representations of cardiac sounds collected from different environments\nwith varying amplitude and noise levels. Model evaluation criteria used were\ncategorical accuracy, precision, recall, and AUROC as the chosen dataset is\nunbalanced. The proposed approach has been implemented on datasets A and B of\nthe PASCAL heart sound collection and resulted in ~ 90% categorical accuracy\nand AUROC of ~0.97 for both sets.",
          "link": "http://arxiv.org/abs/2107.13237",
          "publishedOn": "2021-07-29T02:00:08.116Z",
          "wordCount": 652,
          "title": "A Visual Domain Transfer Learning Approach for Heartbeat Sound Classification. (arXiv:2107.13237v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arora_G/0/1/0/all/0/1\">Geetika Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_R/0/1/0/all/0/1\">Rohit K Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_K/0/1/0/all/0/1\">Kamlesh Tiwari</a>",
          "description": "This paper proposes teeth-photo, a new biometric modality for human\nauthentication on mobile and hand held devices. Biometrics samples are acquired\nusing the camera mounted on mobile device with the help of a mobile application\nhaving specific markers to register the teeth area. Region of interest (RoI) is\nthen extracted using the markers and the obtained sample is enhanced using\ncontrast limited adaptive histogram equalization (CLAHE) for better visual\nclarity. We propose a deep learning architecture and novel regularization\nscheme to obtain highly discriminative embedding for small size RoI. Proposed\ncustom loss function was able to achieve perfect classification for the tiny\nRoI of $75\\times 75$ size. The model is end-to-end and few-shot and therefore\nis very efficient in terms of time and energy requirements. The system can be\nused in many ways including device unlocking and secure authentication. To the\nbest of our understanding, this is the first work on teeth-photo based\nauthentication for mobile device. Experiments have been conducted on an\nin-house teeth-photo database collected using our application. The database is\nmade publicly available. Results have shown that the proposed system has\nperfect accuracy.",
          "link": "http://arxiv.org/abs/2107.13217",
          "publishedOn": "2021-07-29T02:00:08.109Z",
          "wordCount": 632,
          "title": "DeepTeeth: A Teeth-photo Based Human Authentication System for Mobile and Hand-held Devices. (arXiv:2107.13217v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kuiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>",
          "description": "Video Instance Segmentation (VIS) is a new and inherently multi-task problem,\nwhich aims to detect, segment and track each instance in a video sequence.\nExisting approaches are mainly based on single-frame features or single-scale\nfeatures of multiple frames, where temporal information or multi-scale\ninformation is ignored. To incorporate both temporal and scale information, we\npropose a Temporal Pyramid Routing (TPR) strategy to conditionally align and\nconduct pixel-level aggregation from a feature pyramid pair of two adjacent\nframes. Specifically, TPR contains two novel components, including Dynamic\nAligned Cell Routing (DACR) and Cross Pyramid Routing (CPR), where DACR is\ndesigned for aligning and gating pyramid features across temporal dimension,\nwhile CPR transfers temporally aggregated features across scale dimension.\nMoreover, our approach is a plug-and-play module and can be easily applied to\nexisting instance segmentation methods. Extensive experiments on YouTube-VIS\ndataset demonstrate the effectiveness and efficiency of the proposed approach\non several state-of-the-art instance segmentation methods. Codes and trained\nmodels will be publicly available to facilitate future\nresearch.(\\url{https://github.com/lxtGH/TemporalPyramidRouting}).",
          "link": "http://arxiv.org/abs/2107.13155",
          "publishedOn": "2021-07-29T02:00:08.090Z",
          "wordCount": 609,
          "title": "Improving Video Instance Segmentation via Temporal Pyramid Routing. (arXiv:2107.13155v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Sida Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>",
          "description": "We present a new neural representation, called Neural Ray (NeuRay), for the\nnovel view synthesis (NVS) task with multi-view images as input. Existing\nneural scene representations for solving the NVS problem, such as NeRF, cannot\ngeneralize to new scenes and take an excessively long time on training on each\nnew scene from scratch. The other subsequent neural rendering methods based on\nstereo matching, such as PixelNeRF, SRF and IBRNet are designed to generalize\nto unseen scenes but suffer from view inconsistency in complex scenes with\nself-occlusions. To address these issues, our NeuRay method represents every\nscene by encoding the visibility of rays associated with the input views. This\nneural representation can efficiently be initialized from depths estimated by\nexternal MVS methods, which is able to generalize to new scenes and achieves\nsatisfactory rendering images without any training on the scene. Then, the\ninitialized NeuRay can be further optimized on every scene with little training\ntiming to enforce spatial coherence to ensure view consistency in the presence\nof severe self-occlusion. Experiments demonstrate that NeuRay can quickly\ngenerate high-quality novel view images of unseen scenes with little finetuning\nand can handle complex scenes with severe self-occlusions which previous\nmethods struggle with.",
          "link": "http://arxiv.org/abs/2107.13421",
          "publishedOn": "2021-07-29T02:00:08.082Z",
          "wordCount": 647,
          "title": "Neural Rays for Occlusion-aware Image-based Rendering. (arXiv:2107.13421v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kyrkou_C/0/1/0/all/0/1\">Christos Kyrkou</a>",
          "description": "The need for automated real-time visual systems in applications such as smart\ncamera surveillance, smart environments, and drones necessitates the\nimprovement of methods for visual active monitoring and control. Traditionally,\nthe active monitoring task has been handled through a pipeline of modules such\nas detection, filtering, and control. However, such methods are difficult to\njointly optimize and tune their various parameters for real-time processing in\nresource constraint systems. In this paper a deep Convolutional Camera\nController Neural Network is proposed to go directly from visual information to\ncamera movement to provide an efficient solution to the active vision problem.\nIt is trained end-to-end without bounding box annotations to control a camera\nand follow multiple targets from raw pixel values. Evaluation through both a\nsimulation framework and real experimental setup, indicate that the proposed\nsolution is robust to varying conditions and able to achieve better monitoring\nperformance than traditional approaches both in terms of number of targets\nmonitored as well as in effective monitoring time. The advantage of the\nproposed approach is that it is computationally less demanding and can run at\nover 10 FPS (~4x speedup) on an embedded smart camera providing a practical and\naffordable solution to real-time active monitoring.",
          "link": "http://arxiv.org/abs/2107.13233",
          "publishedOn": "2021-07-29T02:00:08.071Z",
          "wordCount": 673,
          "title": "C^3Net: End-to-End deep learning for efficient real-time visual active camera control. (arXiv:2107.13233v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ze Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Feng Lu</a>",
          "description": "Remote photoplethysmography (rPPG) monitors heart rate without requiring\nphysical contact, which allows for a wide variety of applications. Deep\nlearning-based rPPG have demonstrated superior performance over the traditional\napproaches in controlled context. However, the lighting situation in indoor\nspace is typically complex, with uneven light distribution and frequent\nvariations in illumination. It lacks a fair comparison of different methods\nunder different illuminations using the same dataset. In this paper, we present\na public dataset, namely the BH-rPPG dataset, which contains data from twelve\nsubjects under three illuminations: low, medium, and high illumination. We also\nprovide the ground truth heart rate measured by an oximeter. We evaluate the\nperformance of three deep learning-based methods to that of four traditional\nmethods using two public datasets: the UBFC-rPPG dataset and the BH-rPPG\ndataset. The experimental results demonstrate that traditional methods are\ngenerally more resistant to fluctuating illuminations. We found that the\nrPPGNet achieves lowest MAE among deep learning-based method under medium\nillumination, whereas the CHROM achieves 1.5 beats per minute (BPM),\noutperforming the rPPGNet by 60%. These findings suggest that while developing\ndeep learning-based heart rate estimation algorithms, illumination variation\nshould be taken into account. This work serves as a benchmark for rPPG\nperformance evaluation and it opens a pathway for future investigation into\ndeep learning-based rPPG under illumination variations.",
          "link": "http://arxiv.org/abs/2107.13193",
          "publishedOn": "2021-07-29T02:00:08.022Z",
          "wordCount": 666,
          "title": "Assessment of Deep Learning-based Heart Rate Estimation using Remote Photoplethysmography under Different Illuminations. (arXiv:2107.13193v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13048",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_R/0/1/0/all/0/1\">Richard J. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_M/0/1/0/all/0/1\">Ming Y. Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shaban_M/0/1/0/all/0/1\">Muhammad Shaban</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chengkuan Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1\">Tiffany Y. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Williamson_D/0/1/0/all/0/1\">Drew F. K. Williamson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahmood_F/0/1/0/all/0/1\">Faisal Mahmood</a>",
          "description": "Cancer prognostication is a challenging task in computational pathology that\nrequires context-aware representations of histology features to adequately\ninfer patient survival. Despite the advancements made in weakly-supervised deep\nlearning, many approaches are not context-aware and are unable to model\nimportant morphological feature interactions between cell identities and tissue\ntypes that are prognostic for patient survival. In this work, we present\nPatch-GCN, a context-aware, spatially-resolved patch-based graph convolutional\nnetwork that hierarchically aggregates instance-level histology features to\nmodel local- and global-level topological structures in the tumor\nmicroenvironment. We validate Patch-GCN with 4,370 gigapixel WSIs across five\ndifferent cancer types from the Cancer Genome Atlas (TCGA), and demonstrate\nthat Patch-GCN outperforms all prior weakly-supervised approaches by\n3.58-9.46%. Our code and corresponding models are publicly available at\nhttps://github.com/mahmoodlab/Patch-GCN.",
          "link": "http://arxiv.org/abs/2107.13048",
          "publishedOn": "2021-07-29T02:00:07.969Z",
          "wordCount": 606,
          "title": "Whole Slide Images are 2D Point Clouds: Context-Aware Survival Prediction using Patch-based Graph Convolutional Networks. (arXiv:2107.13048v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chenhang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>",
          "description": "Current geometry-based monocular 3D object detection models can efficiently\ndetect objects by leveraging perspective geometry, but their performance is\nlimited due to the absence of accurate depth information. Though this issue can\nbe alleviated in a depth-based model where a depth estimation module is plugged\nto predict depth information before 3D box reasoning, the introduction of such\nmodule dramatically reduces the detection speed. Instead of training a costly\ndepth estimator, we propose a rendering module to augment the training data by\nsynthesizing images with virtual-depths. The rendering module takes as input\nthe RGB image and its corresponding sparse depth image, outputs a variety of\nphoto-realistic synthetic images, from which the detection model can learn more\ndiscriminative features to adapt to the depth changes of the objects. Besides,\nwe introduce an auxiliary module to improve the detection model by jointly\noptimizing it through a depth estimation task. Both modules are working in the\ntraining time and no extra computation will be introduced to the detection\nmodel. Experiments show that by working with our proposed modules, a\ngeometry-based model can represent the leading accuracy on the KITTI 3D\ndetection benchmark.",
          "link": "http://arxiv.org/abs/2107.13269",
          "publishedOn": "2021-07-29T02:00:07.928Z",
          "wordCount": 636,
          "title": "Aug3D-RPN: Improving Monocular 3D Object Detection by Synthetic Images with Virtual Depth. (arXiv:2107.13269v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13157",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Trivedi_A/0/1/0/all/0/1\">Anusua Trivedi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Desbiens_J/0/1/0/all/0/1\">Jocelyn Desbiens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gross_R/0/1/0/all/0/1\">Ron Gross</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dodhia_R/0/1/0/all/0/1\">Rahul Dodhia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferres_J/0/1/0/all/0/1\">Juan Lavista Ferres</a>",
          "description": "Purpose: To demonstrate that retinal microvasculature per se is a reliable\nbiomarker for Diabetic Retinopathy (DR) and, by extension, cardiovascular\ndiseases. Methods: Deep Learning Convolutional Neural Networks (CNN) applied to\ncolor fundus images for semantic segmentation of the blood vessels and severity\nclassification on both vascular and full images. Vessel reconstruction through\nharmonic descriptors is also used as a smoothing and de-noising tool. The\nmathematical background of the theory is also outlined. Results: For diabetic\npatients, at least 93.8% of DR No-Refer vs. Refer classification can be related\nto vasculature defects. As for the Non-Sight Threatening vs. Sight Threatening\ncase, the ratio is as high as 96.7%. Conclusion: In the case of DR, most of the\ndisease biomarkers are related topologically to the vasculature. Translational\nRelevance: Experiments conducted on eye blood vasculature reconstruction as a\nbiomarker shows a strong correlation between vasculature shape and later stages\nof DR.",
          "link": "http://arxiv.org/abs/2107.13157",
          "publishedOn": "2021-07-29T02:00:07.920Z",
          "wordCount": 603,
          "title": "Retinal Microvasculature as Biomarker for Diabetes and Cardiovascular Diseases. (arXiv:2107.13157v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundgaard_K/0/1/0/all/0/1\">Keld T. Lundgaard</a>",
          "description": "By leveraging large amounts of product data collected across hundreds of live\ne-commerce websites, we construct 1000 unique classification tasks that share\nsimilarly-structured input data, comprised of both text and images. These\nclassification tasks focus on learning the product hierarchy of different\ne-commerce websites, causing many of them to be correlated. Adopting a\nmulti-modal transformer model, we solve these tasks in unison using multi-task\nlearning (MTL). Extensive experiments are presented over an initial 100-task\ndataset to reveal best practices for \"large-scale MTL\" (i.e., MTL with more\nthan 100 tasks). From these experiments, a final, unified methodology is\nderived, which is composed of both best practices and new proposals such as\nDyPa, a simple heuristic for automatically allocating task-specific parameters\nto tasks that could benefit from extra capacity. Using our large-scale MTL\nmethodology, we successfully train a single model across all 1000 tasks in our\ndataset while using minimal task specific parameters, thereby showing that it\nis possible to extend several orders of magnitude beyond current efforts in\nMTL.",
          "link": "http://arxiv.org/abs/2107.13054",
          "publishedOn": "2021-07-29T02:00:07.913Z",
          "wordCount": 623,
          "title": "Exceeding the Limits of Visual-Linguistic Multi-Task Learning. (arXiv:2107.13054v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meiling Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>",
          "description": "In this paper, we present a set of extremely efficient and high throughput\nmodels for accurate face verification, MixFaceNets which are inspired by Mixed\nDepthwise Convolutional Kernels. Extensive experiment evaluations on Label Face\nin the Wild (LFW), Age-DB, MegaFace, and IARPA Janus Benchmarks IJB-B and IJB-C\ndatasets have shown the effectiveness of our MixFaceNets for applications\nrequiring extremely low computational complexity. Under the same level of\ncomputation complexity (< 500M FLOPs), our MixFaceNets outperform\nMobileFaceNets on all the evaluated datasets, achieving 99.60% accuracy on LFW,\n97.05% accuracy on AgeDB-30, 93.60 TAR (at FAR1e-6) on MegaFace, 90.94 TAR (at\nFAR1e-4) on IJB-B and 93.08 TAR (at FAR1e-4) on IJB-C. With computational\ncomplexity between 500M and 1G FLOPs, our MixFaceNets achieved results\ncomparable to the top-ranked models, while using significantly fewer FLOPs and\nless computation overhead, which proves the practical value of our proposed\nMixFaceNets. All training codes, pre-trained models, and training logs have\nbeen made available https://github.com/fdbtrs/mixfacenets.",
          "link": "http://arxiv.org/abs/2107.13046",
          "publishedOn": "2021-07-29T02:00:07.905Z",
          "wordCount": 602,
          "title": "MixFaceNets: Extremely Efficient Face Recognition Networks. (arXiv:2107.13046v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiaojie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>",
          "description": "Video prediction methods generally consume substantial computing resources in\ntraining and deployment, among which keypoint-based approaches show promising\nimprovement in efficiency by simplifying dense image prediction to light\nkeypoint prediction. However, keypoint locations are often modeled only as\ncontinuous coordinates, so noise from semantically insignificant deviations in\nvideos easily disrupt learning stability, leading to inaccurate keypoint\nmodeling. In this paper, we design a new grid keypoint learning framework,\naiming at a robust and explainable intermediate keypoint representation for\nlong-term efficient video prediction. We have two major technical\ncontributions. First, we detect keypoints by jumping among candidate locations\nin our raised grid space and formulate a condensation loss to encourage\nmeaningful keypoints with strong representative capability. Second, we\nintroduce a 2D binary map to represent the detected grid keypoints and then\nsuggest propagating keypoint locations with stochasticity by selecting entries\nin the discrete grid space, thus preserving the spatial structure of keypoints\nin the longterm horizon for better future frame generation. Extensive\nexperiments verify that our method outperforms the state-ofthe-art stochastic\nvideo prediction methods while saves more than 98% of computing resources. We\nalso demonstrate our method on a robotic-assisted surgery dataset with\npromising results. Our code is available at\nhttps://github.com/xjgaocs/Grid-Keypoint-Learning.",
          "link": "http://arxiv.org/abs/2107.13170",
          "publishedOn": "2021-07-29T02:00:07.898Z",
          "wordCount": 642,
          "title": "Accurate Grid Keypoint Learning for Efficient Video Prediction. (arXiv:2107.13170v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elhagry_A/0/1/0/all/0/1\">Ahmed Elhagry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadaoui_K/0/1/0/all/0/1\">Karima Kadaoui</a>",
          "description": "Image Captioning is a task that combines computer vision and natural language\nprocessing, where it aims to generate descriptive legends for images. It is a\ntwo-fold process relying on accurate image understanding and correct language\nunderstanding both syntactically and semantically. It is becoming increasingly\ndifficult to keep up with the latest research and findings in the field of\nimage captioning due to the growing amount of knowledge available on the topic.\nThere is not, however, enough coverage of those findings in the available\nreview papers. We perform in this paper a run-through of the current\ntechniques, datasets, benchmarks and evaluation metrics used in image\ncaptioning. The current research on the field is mostly focused on deep\nlearning-based methods, where attention mechanisms along with deep\nreinforcement and adversarial learning appear to be in the forefront of this\nresearch topic. In this paper, we review recent methodologies such as UpDown,\nOSCAR, VIVO, Meta Learning and a model that uses conditional generative\nadversarial nets. Although the GAN-based model achieves the highest score,\nUpDown represents an important basis for image captioning and OSCAR and VIVO\nare more useful as they use novel object captioning. This review paper serves\nas a roadmap for researchers to keep up to date with the latest contributions\nmade in the field of image caption generation.",
          "link": "http://arxiv.org/abs/2107.13114",
          "publishedOn": "2021-07-29T02:00:07.877Z",
          "wordCount": 653,
          "title": "A Thorough Review on Recent Deep Learning Methodologies for Image Captioning. (arXiv:2107.13114v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chaoqiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qiyu Sun</a>",
          "description": "Previous unsupervised monocular depth estimation methods mainly focus on the\nday-time scenario, and their frameworks are driven by warped photometric\nconsistency. While in some challenging environments, like night, rainy night or\nsnowy winter, the photometry of the same pixel on different frames is\ninconsistent because of the complex lighting and reflection, so that the\nday-time unsupervised frameworks cannot be directly applied to these complex\nscenarios. In this paper, we investigate the problem of unsupervised monocular\ndepth estimation in certain highly complex scenarios. We address this\nchallenging problem by using domain adaptation, and a unified image\ntransfer-based adaptation framework is proposed based on monocular videos in\nthis paper. The depth model trained on day-time scenarios is adapted to\ndifferent complex scenarios. Instead of adapting the whole depth network, we\njust consider the encoder network for lower computational complexity. The depth\nmodels adapted by the proposed framework to different scenarios share the same\ndecoder, which is practical. Constraints on both feature space and output space\npromote the framework to learn the key features for depth decoding, and the\nsmoothness loss is introduced into the adaptation framework for better depth\nestimation performance. Extensive experiments show the effectiveness of the\nproposed unsupervised framework in estimating the dense depth map from the\nnight-time, rainy night-time and snowy winter images.",
          "link": "http://arxiv.org/abs/2107.13137",
          "publishedOn": "2021-07-29T02:00:07.868Z",
          "wordCount": 653,
          "title": "Unsupervised Monocular Depth Estimation in Highly Complex Environments. (arXiv:2107.13137v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elhagry_A/0/1/0/all/0/1\">Ahmed Elhagry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadaoui_K/0/1/0/all/0/1\">Karima Kadaoui</a>",
          "description": "Image captioning is a task in the field of Artificial Intelligence that\nmerges between computer vision and natural language processing. It is\nresponsible for generating legends that describe images, and has various\napplications like descriptions used by assistive technology or indexing images\n(for search engines for instance). This makes it a crucial topic in AI that is\nundergoing a lot of research. This task however, like many others, is trained\non large images labeled via human annotation, which can be very cumbersome: it\nneeds manual effort, both financial and temporal costs, it is error-prone and\npotentially difficult to execute in some cases (e.g. medical images). To\nmitigate the need for labels, we attempt to use self-supervised learning, a\ntype of learning where models use the data contained within the images\nthemselves as labels. It is challenging to accomplish though, since the task is\ntwo-fold: the images and captions come from two different modalities and\nusually handled by different types of networks. It is thus not obvious what a\ncompletely self-supervised solution would look like. How it would achieve\ncaptioning in a comparable way to how self-supervision is applied today on\nimage recognition tasks is still an ongoing research topic. In this project, we\nare using an encoder-decoder architecture where the encoder is a convolutional\nneural network (CNN) trained on OpenImages dataset and learns image features in\na self-supervised fashion using the rotation pretext task. The decoder is a\nLong Short-Term Memory (LSTM), and it is trained, along within the image\ncaptioning model, on MS COCO dataset and is responsible of generating captions.\nOur GitHub repository can be found:\nhttps://github.com/elhagry1/SSL_ImageCaptioning_RotationPrediction",
          "link": "http://arxiv.org/abs/2107.13111",
          "publishedOn": "2021-07-29T02:00:07.855Z",
          "wordCount": 704,
          "title": "Experimenting with Self-Supervision using Rotation Prediction for Image Captioning. (arXiv:2107.13111v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dsouza_D/0/1/0/all/0/1\">Daniel D&#x27;souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nussbaum_Z/0/1/0/all/0/1\">Zach Nussbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>",
          "description": "As machine learning models are increasingly employed to assist human\ndecision-makers, it becomes critical to communicate the uncertainty associated\nwith these model predictions. However, the majority of work on uncertainty has\nfocused on traditional probabilistic or ranking approaches - where the model\nassigns low probabilities or scores to uncertain examples. While this captures\nwhat examples are challenging for the model, it does not capture the underlying\nsource of the uncertainty. In this work, we seek to identify examples the model\nis uncertain about and characterize the source of said uncertainty. We explore\nthe benefits of designing a targeted intervention - targeted data augmentation\nof the examples where the model is uncertain over the course of training. We\ninvestigate whether the rate of learning in the presence of additional\ninformation differs between atypical and noisy examples? Our results show that\nthis is indeed the case, suggesting that well-designed interventions over the\ncourse of training can be an effective way to characterize and distinguish\nbetween different sources of uncertainty.",
          "link": "http://arxiv.org/abs/2107.13098",
          "publishedOn": "2021-07-29T02:00:07.847Z",
          "wordCount": 618,
          "title": "A Tale Of Two Long Tails. (arXiv:2107.13098v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jinlei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qiaoyong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Di Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong Zhou</a>",
          "description": "Reconstruction-based methods play an important role in unsupervised anomaly\ndetection in images. Ideally, we expect a perfect reconstruction for normal\nsamples and poor reconstruction for abnormal samples. Since the\ngeneralizability of deep neural networks is difficult to control, existing\nmodels such as autoencoder do not work well. In this work, we interpret the\nreconstruction of an image as a divide-and-assemble procedure. Surprisingly, by\nvarying the granularity of division on feature maps, we are able to modulate\nthe reconstruction capability of the model for both normal and abnormal\nsamples. That is, finer granularity leads to better reconstruction, while\ncoarser granularity leads to poorer reconstruction. With proper granularity,\nthe gap between the reconstruction error of normal and abnormal samples can be\nmaximized. The divide-and-assemble framework is implemented by embedding a\nnovel multi-scale block-wise memory module into an autoencoder network.\nBesides, we introduce adversarial learning and explore the semantic latent\nrepresentation of the discriminator, which improves the detection of subtle\nanomaly. We achieve state-of-the-art performance on the challenging MVTec AD\ndataset. Remarkably, we improve the vanilla autoencoder model by 10.1% in terms\nof the AUROC score.",
          "link": "http://arxiv.org/abs/2107.13118",
          "publishedOn": "2021-07-29T02:00:07.831Z",
          "wordCount": 628,
          "title": "Divide-and-Assemble: Learning Block-wise Memory for Unsupervised Anomaly Detection. (arXiv:2107.13118v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Walsh_R/0/1/0/all/0/1\">Reece Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelpakey_M/0/1/0/all/0/1\">Mohamed H. Abdelpakey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1\">Mohamed S. Shehata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1\">Mostafa M.Mohamed</a>",
          "description": "Classifying and analyzing human cells is a lengthy procedure, often involving\na trained professional. In an attempt to expedite this process, an active area\nof research involves automating cell classification through use of deep\nlearning-based techniques. In practice, a large amount of data is required to\naccurately train these deep learning models. However, due to the sparse human\ncell datasets currently available, the performance of these models is typically\nlow. This study investigates the feasibility of using few-shot learning-based\ntechniques to mitigate the data requirements for accurate training. The study\nis comprised of three parts: First, current state-of-the-art few-shot learning\ntechniques are evaluated on human cell classification. The selected techniques\nare trained on a non-medical dataset and then tested on two out-of-domain,\nhuman cell datasets. The results indicate that, overall, the test accuracy of\nstate-of-the-art techniques decreased by at least 30% when transitioning from a\nnon-medical dataset to a medical dataset. Second, this study evaluates the\npotential benefits, if any, to varying the backbone architecture and training\nschemes in current state-of-the-art few-shot learning techniques when used in\nhuman cell classification. Even with these variations, the overall test\naccuracy decreased from 88.66% on non-medical datasets to 44.13% at best on the\nmedical datasets. Third, this study presents future directions for using\nfew-shot learning in human cell classification. In general, few-shot learning\nin its current state performs poorly on human cell classification. The study\nproves that attempts to modify existing network architectures are not effective\nand concludes that future research effort should be focused on improving\nrobustness towards out-of-domain testing using optimization-based or\nself-supervised few-shot learning techniques.",
          "link": "http://arxiv.org/abs/2107.13093",
          "publishedOn": "2021-07-29T02:00:07.805Z",
          "wordCount": 720,
          "title": "Automated Human Cell Classification in Sparse Datasets using Few-Shot Learning. (arXiv:2107.13093v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13136",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marino_J/0/1/0/all/0/1\">Joseph Marino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mandt_S/0/1/0/all/0/1\">Stephan Mandt</a>",
          "description": "While recent machine learning research has revealed connections between deep\ngenerative models such as VAEs and rate-distortion losses used in learned\ncompression, most of this work has focused on images. In a similar spirit, we\nview recently proposed neural video coding algorithms through the lens of deep\nautoregressive and latent variable modeling. We present recent neural video\ncodecs as instances of a generalized stochastic temporal autoregressive\ntransform, and propose new avenues for further improvements inspired by\nnormalizing flows and structured priors. We propose several architectures that\nyield state-of-the-art video compression performance on full-resolution video\nand discuss their tradeoffs and ablations. In particular, we propose (i)\nimproved temporal autoregressive transforms, (ii) improved entropy models with\nstructured and temporal dependencies, and (iii) variable bitrate versions of\nour algorithms. Since our improvements are compatible with a large class of\nexisting models, we provide further evidence that the generative modeling\nviewpoint can advance the neural video coding field.",
          "link": "http://arxiv.org/abs/2107.13136",
          "publishedOn": "2021-07-29T02:00:07.797Z",
          "wordCount": 637,
          "title": "Insights from Generative Modeling for Neural Video Compression. (arXiv:2107.13136v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>",
          "description": "This thesis presents methods and approaches to image color correction, color\nenhancement, and color editing. To begin, we study the color correction problem\nfrom the standpoint of the camera's image signal processor (ISP). A camera's\nISP is hardware that applies a series of in-camera image processing and color\nmanipulation steps, many of which are nonlinear in nature, to render the\ninitial sensor image to its final photo-finished representation saved in the\n8-bit standard RGB (sRGB) color space. As white balance (WB) is one of the\nmajor procedures applied by the ISP for color correction, this thesis presents\ntwo different methods for ISP white balancing. Afterward, we discuss another\nscenario of correcting and editing image colors, where we present a set of\nmethods to correct and edit WB settings for images that have been improperly\nwhite-balanced by the ISP. Then, we explore another factor that has a\nsignificant impact on the quality of camera-rendered colors, in which we\noutline two different methods to correct exposure errors in camera-rendered\nimages. Lastly, we discuss post-capture auto color editing and manipulation. In\nparticular, we propose auto image recoloring methods to generate different\nrealistic versions of the same camera-rendered image with new colors. Through\nextensive evaluations, we demonstrate that our methods provide superior\nsolutions compared to existing alternatives targeting color correction, color\nenhancement, and color editing.",
          "link": "http://arxiv.org/abs/2107.13117",
          "publishedOn": "2021-07-29T02:00:07.789Z",
          "wordCount": 649,
          "title": "Image color correction, enhancement, and editing. (arXiv:2107.13117v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13108",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1\">Bin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_N/0/1/0/all/0/1\">Nan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianfu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>",
          "description": "This paper presents a neural network built upon Transformers, namely PlaneTR,\nto simultaneously detect and reconstruct planes from a single image. Different\nfrom previous methods, PlaneTR jointly leverages the context information and\nthe geometric structures in a sequence-to-sequence way to holistically detect\nplane instances in one forward pass. Specifically, we represent the geometric\nstructures as line segments and conduct the network with three main components:\n(i) context and line segments encoders, (ii) a structure-guided plane decoder,\n(iii) a pixel-wise plane embedding decoder. Given an image and its detected\nline segments, PlaneTR generates the context and line segment sequences via two\nspecially designed encoders and then feeds them into a Transformers-based\ndecoder to directly predict a sequence of plane instances by simultaneously\nconsidering the context and global structure cues. Finally, the pixel-wise\nembeddings are computed to assign each pixel to one predicted plane instance\nwhich is nearest to it in embedding space. Comprehensive experiments\ndemonstrate that PlaneTR achieves a state-of-the-art performance on the ScanNet\nand NYUv2 datasets.",
          "link": "http://arxiv.org/abs/2107.13108",
          "publishedOn": "2021-07-29T02:00:07.753Z",
          "wordCount": 608,
          "title": "PlaneTR: Structure-Guided Transformers for 3D Plane Recovery. (arXiv:2107.13108v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuefan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Youyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">C. Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>",
          "description": "We describe a method for realistic depth synthesis that learns diverse\nvariations from the real depth scans and ensures geometric consistency for\neffective synthetic-to-real transfer. Unlike general image synthesis pipelines,\nwhere geometries are mostly ignored, we treat geometries carried by the depth\nbased on their own existence. We propose differential contrastive learning that\nexplicitly enforces the underlying geometric properties to be invariant\nregarding the real variations been learned. The resulting depth synthesis\nmethod is task-agnostic and can be used for training any task-specific networks\nwith synthetic labels. We demonstrate the effectiveness of the proposed method\nby extensive evaluations on downstream real-world geometric reasoning tasks. We\nshow our method achieves better synthetic-to-real transfer performance than the\nother state-of-the-art. When fine-tuned on a small number of real-world\nannotations, our method can even surpass the fully supervised baselines.",
          "link": "http://arxiv.org/abs/2107.13087",
          "publishedOn": "2021-07-29T02:00:07.746Z",
          "wordCount": 574,
          "title": "DCL: Differential Contrastive Learning for Geometry-Aware Depth Synthesis. (arXiv:2107.13087v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Ying Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Pei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>",
          "description": "This paper revisits human-object interaction (HOI) recognition at image level\nwithout using supervisions of object location and human pose. We name it\ndetection-free HOI recognition, in contrast to the existing\ndetection-supervised approaches which rely on object and keypoint detections to\nachieve state of the art. With our method, not only the detection supervision\nis evitable, but superior performance can be achieved by properly using\nimage-text pre-training (such as CLIP) and the proposed Log-Sum-Exp Sign\n(LSE-Sign) loss function. Specifically, using text embeddings of class labels\nto initialize the linear classifier is essential for leveraging the CLIP\npre-trained image encoder. In addition, LSE-Sign loss facilitates learning from\nmultiple labels on an imbalanced dataset by normalizing gradients over all\nclasses in a softmax format. Surprisingly, our detection-free solution achieves\n60.5 mAP on the HICO dataset, outperforming the detection-supervised state of\nthe art by 13.4 mAP",
          "link": "http://arxiv.org/abs/2107.13083",
          "publishedOn": "2021-07-29T02:00:07.738Z",
          "wordCount": 588,
          "title": "Is Object Detection Necessary for Human-Object Interaction Recognition?. (arXiv:2107.13083v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1\">Shreyank N Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevilla_Lara_L/0/1/0/all/0/1\">Laura Sevilla-Lara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kiyoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_M/0/1/0/all/0/1\">Marcus Rohrbach</a>",
          "description": "Zero-shot action recognition is the task of classifying action categories\nthat are not available in the training set. In this setting, the standard\nevaluation protocol is to use existing action recognition datasets (e.g.\nUCF101) and randomly split the classes into seen and unseen. However, most\nrecent work builds on representations pre-trained on the Kinetics dataset,\nwhere classes largely overlap with classes in the zero-shot evaluation\ndatasets. As a result, classes which are supposed to be unseen, are present\nduring supervised pre-training, invalidating the condition of the zero-shot\nsetting. A similar concern was previously noted several years ago for image\nbased zero-shot recognition, but has not been considered by the zero-shot\naction recognition community. In this paper, we propose a new split for true\nzero-shot action recognition with no overlap between unseen test classes and\ntraining or pre-training classes. We benchmark several recent approaches on the\nproposed True Zero-Shot (TruZe) Split for UCF101 and HMDB51, with zero-shot and\ngeneralized zero-shot evaluation. In our extensive analysis we find that our\nTruZe splits are significantly harder than comparable random splits as nothing\nis leaking from pre-training, i.e. unseen performance is consistently lower, up\nto 9.4% for zero-shot action recognition. In an additional evaluation we also\nfind that similar issues exist in the splits used in few-shot action\nrecognition, here we see differences of up to 14.1%. We publish our splits and\nhope that our benchmark analysis will change how the field is evaluating zero-\nand few-shot action recognition moving forward.",
          "link": "http://arxiv.org/abs/2107.13029",
          "publishedOn": "2021-07-29T02:00:07.688Z",
          "wordCount": 688,
          "title": "A New Split for Evaluating True Zero-Shot Action Recognition. (arXiv:2107.13029v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qingyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boshen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiayi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>",
          "description": "Recently, the problem of inaccurate learning targets in crowd counting draws\nincreasing attention. Inspired by a few pioneering work, we solve this problem\nby trying to predict the indices of pre-defined interval bins of counts instead\nof the count values themselves. However, an inappropriate interval setting\nmight make the count error contributions from different intervals extremely\nimbalanced, leading to inferior counting performance. Therefore, we propose a\nnovel count interval partition criterion called Uniform Error Partition (UEP),\nwhich always keeps the expected counting error contributions equal for all\nintervals to minimize the prediction risk. Then to mitigate the inevitably\nintroduced discretization errors in the count quantization process, we propose\nanother criterion called Mean Count Proxies (MCP). The MCP criterion selects\nthe best count proxy for each interval to represent its count value during\ninference, making the overall expected discretization error of an image nearly\nnegligible. As far as we are aware, this work is the first to delve into such a\nclassification task and ends up with a promising solution for count interval\npartition. Following the above two theoretically demonstrated criterions, we\npropose a simple yet effective model termed Uniform Error Partition Network\n(UEPNet), which achieves state-of-the-art performance on several challenging\ndatasets. The codes will be available at:\nhttps://github.com/TencentYoutuResearch/CrowdCounting-UEPNet.",
          "link": "http://arxiv.org/abs/2107.12619",
          "publishedOn": "2021-07-28T02:02:34.598Z",
          "wordCount": 674,
          "title": "Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting. (arXiv:2107.12619v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Menghan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuzhen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Simon X. Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Ping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>",
          "description": "For people who ardently love painting but unfortunately have visual\nimpairments, holding a paintbrush to create a work is a very difficult task.\nPeople in this special group are eager to pick up the paintbrush, like Leonardo\nda Vinci, to create and make full use of their own talents. Therefore, to\nmaximally bridge this gap, we propose a painting navigation system to assist\nblind people in painting and artistic creation. The proposed system is composed\nof cognitive system and guidance system. The system adopts drawing board\npositioning based on QR code, brush navigation based on target detection and\nbush real-time positioning. Meanwhile, this paper uses human-computer\ninteraction on the basis of voice and a simple but efficient position\ninformation coding rule. In addition, we design a criterion to efficiently\njudge whether the brush reaches the target or not. According to the\nexperimental results, the thermal curves extracted from the faces of testers\nshow that it is relatively well accepted by blindfolded and even blind testers.\nWith the prompt frequency of 1s, the painting navigation system performs best\nwith the completion degree of 89% with SD of 8.37% and overflow degree of 347%\nwith SD of 162.14%. Meanwhile, the excellent and good types of brush tip\ntrajectory account for 74%, and the relative movement distance is 4.21 with SD\nof 2.51. This work demonstrates that it is practicable for the blind people to\nfeel the world through the brush in their hands. In the future, we plan to\ndeploy Angle's Eyes on the phone to make it more portable. The demo video of\nthe proposed painting navigation system is available at:\nhttps://doi.org/10.6084/m9.figshare.9760004.v1.",
          "link": "http://arxiv.org/abs/2107.12921",
          "publishedOn": "2021-07-28T02:02:34.546Z",
          "wordCount": 739,
          "title": "Angel's Girl for Blind Painters: an Efficient Painting Navigation System Validated by Multimodal Evaluation Approach. (arXiv:2107.12921v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drozdowski_P/0/1/0/all/0/1\">Pawel Drozdowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stockhardt_F/0/1/0/all/0/1\">Fabian Stockhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osorio_Roig_D/0/1/0/all/0/1\">Dail&#xe9; Osorio-Roig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>",
          "description": "Computationally efficient, accurate, and privacy-preserving data storage and\nretrieval are among the key challenges faced by practical deployments of\nbiometric identification systems worldwide. In this work, a method of protected\nindexing of biometric data is presented. By utilising feature-level fusion of\nintelligently paired templates, a multi-stage search structure is created.\nDuring retrieval, the list of potential candidate identities is successively\npre-filtered, thereby reducing the number of template comparisons necessary for\na biometric identification transaction. Protection of the biometric probe\ntemplates, as well as the stored reference templates and the created index is\ncarried out using homomorphic encryption. The proposed method is extensively\nevaluated in closed-set and open-set identification scenarios on publicly\navailable databases using two state-of-the-art open-source face recognition\nsystems. With respect to a typical baseline algorithm utilising an exhaustive\nsearch-based retrieval algorithm, the proposed method enables a reduction of\nthe computational workload associated with a biometric identification\ntransaction by 90%, while simultaneously suffering no degradation of the\nbiometric performance. Furthermore, by facilitating a seamless integration of\ntemplate protection with open-source homomorphic encryption libraries, the\nproposed method guarantees unlinkability, irreversibility, and renewability of\nthe protected biometric data.",
          "link": "http://arxiv.org/abs/2107.12675",
          "publishedOn": "2021-07-28T02:02:34.530Z",
          "wordCount": 643,
          "title": "Feature Fusion Methods for Indexing and Retrieval of Biometric Data: Application to Face Recognition with Privacy Protection. (arXiv:2107.12675v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12040",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Du_X/0/1/0/all/0/1\">Xuefeng Du</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_H/0/1/0/all/0/1\">Haohan Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenxi Zhu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangrui Zeng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chang_Y/0/1/0/all/0/1\">Yi-Wei Chang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>",
          "description": "Motivation: Cryo-Electron Tomography (cryo-ET) is a 3D bioimaging tool that\nvisualizes the structural and spatial organization of macromolecules at a\nnear-native state in single cells, which has broad applications in life\nscience. However, the systematic structural recognition and recovery of\nmacromolecules captured by cryo-ET are difficult due to high structural\ncomplexity and imaging limits. Deep learning based subtomogram classification\nhave played critical roles for such tasks. As supervised approaches, however,\ntheir performance relies on sufficient and laborious annotation on a large\ntraining dataset.\n\nResults: To alleviate this major labeling burden, we proposed a Hybrid Active\nLearning (HAL) framework for querying subtomograms for labelling from a large\nunlabeled subtomogram pool. Firstly, HAL adopts uncertainty sampling to select\nthe subtomograms that have the most uncertain predictions. Moreover, to\nmitigate the sampling bias caused by such strategy, a discriminator is\nintroduced to judge if a certain subtomogram is labeled or unlabeled and\nsubsequently the model queries the subtomogram that have higher probabilities\nto be unlabeled. Additionally, HAL introduces a subset sampling strategy to\nimprove the diversity of the query set, so that the information overlap is\ndecreased between the queried batches and the algorithmic efficiency is\nimproved. Our experiments on subtomogram classification tasks using both\nsimulated and real data demonstrate that we can achieve comparable testing\nperformance (on average only 3% accuracy drop) by using less than 30% of the\nlabeled subtomograms, which shows a very promising result for subtomogram\nclassification task with limited labeling resources.",
          "link": "http://arxiv.org/abs/2102.12040",
          "publishedOn": "2021-07-28T02:02:34.402Z",
          "wordCount": 783,
          "title": "Active Learning to Classify Macromolecular Structures in situ for Less Supervision in Cryo-Electron Tomography. (arXiv:2102.12040v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1\">Rundong Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loianno_G/0/1/0/all/0/1\">Giuseppe Loianno</a>",
          "description": "Estimating the 6D pose of objects is beneficial for robotics tasks such as\ntransportation, autonomous navigation, manipulation as well as in scenarios\nbeyond robotics like virtual and augmented reality. With respect to single\nimage pose estimation, pose tracking takes into account the temporal\ninformation across multiple frames to overcome possible detection\ninconsistencies and to improve the pose estimation efficiency. In this work, we\nintroduce a novel Deep Neural Network (DNN) called VIPose, that combines\ninertial and camera data to address the object pose tracking problem in\nreal-time. The key contribution is the design of a novel DNN architecture which\nfuses visual and inertial features to predict the objects' relative 6D pose\nbetween consecutive image frames. The overall 6D pose is then estimated by\nconsecutively combining relative poses. Our approach shows remarkable pose\nestimation results for heavily occluded objects that are well known to be very\nchallenging to handle by existing state-of-the-art solutions. The effectiveness\nof the proposed approach is validated on a new dataset called VIYCB with RGB\nimage, IMU data, and accurate 6D pose annotations created by employing an\nautomated labeling technique. The approach presents accuracy performances\ncomparable to state-of-the-art techniques, but with additional benefit to be\nreal-time.",
          "link": "http://arxiv.org/abs/2107.12617",
          "publishedOn": "2021-07-28T02:02:34.381Z",
          "wordCount": 641,
          "title": "VIPose: Real-time Visual-Inertial 6D Object Pose Tracking. (arXiv:2107.12617v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.",
          "link": "http://arxiv.org/abs/2107.12651",
          "publishedOn": "2021-07-28T02:02:34.325Z",
          "wordCount": 604,
          "title": "Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12689",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Byrne_N/0/1/0/all/0/1\">Nick Byrne</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Clough_J/0/1/0/all/0/1\">James R Clough</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valverde_I/0/1/0/all/0/1\">Isra Valverde</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Montana_G/0/1/0/all/0/1\">Giovanni Montana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+King_A/0/1/0/all/0/1\">Andrew P King</a>",
          "description": "Multi-class segmentation of cardiac magnetic resonance (CMR) images seeks a\nseparation of data into anatomical components with known structure and\nconfiguration. The most popular CNN-based methods are optimised using pixel\nwise loss functions, ignorant of the spatially extended features that\ncharacterise anatomy. Therefore, whilst sharing a high spatial overlap with the\nground truth, inferred CNN-based segmentations can lack coherence, including\nspurious connected components, holes and voids. Such results are implausible,\nviolating anticipated anatomical topology. In response, (single-class)\npersistent homology-based loss functions have been proposed to capture global\nanatomical features. Our work extends these approaches to the task of\nmulti-class segmentation. Building an enriched topological description of all\nclass labels and class label pairs, our loss functions make predictable and\nstatistically significant improvements in segmentation topology using a\nCNN-based post-processing framework. We also present (and make available) a\nhighly efficient implementation based on cubical complexes and parallel\nexecution, enabling practical application within high resolution 3D data for\nthe first time. We demonstrate our approach on 2D short axis and 3D whole heart\nCMR segmentation, advancing a detailed and faithful analysis of performance on\ntwo publicly available datasets.",
          "link": "http://arxiv.org/abs/2107.12689",
          "publishedOn": "2021-07-28T02:02:34.306Z",
          "wordCount": 641,
          "title": "A persistent homology-based topological loss for CNN-based multi-class segmentation of CMR. (arXiv:2107.12689v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Li Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lefei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Unsupervised domain adaptation (UDA) for semantic segmentation aims to adapt\na segmentation model trained on the labeled source domain to the unlabeled\ntarget domain. Existing methods try to learn domain invariant features while\nsuffering from large domain gaps that make it difficult to correctly align\ndiscrepant features, especially in the initial training phase. To address this\nissue, we propose a novel Dual Soft-Paste (DSP) method in this paper.\nSpecifically, DSP selects some classes from a source domain image using a\nlong-tail class first sampling strategy and softly pastes the corresponding\nimage patch on both the source and target training images with a fusion weight.\nTechnically, we adopt the mean teacher framework for domain adaptation, where\nthe pasted source and target images go through the student network while the\noriginal target image goes through the teacher network. Output-level alignment\nis carried out by aligning the probability maps of the target fused image from\nboth networks using a weighted cross-entropy loss. In addition, feature-level\nalignment is carried out by aligning the feature maps of the source and target\nimages from student network using a weighted maximum mean discrepancy loss. DSP\nfacilitates the model learning domain-invariant features from the intermediate\ndomains, leading to faster convergence and better performance. Experiments on\ntwo challenging benchmarks demonstrate the superiority of DSP over\nstate-of-the-art methods. Code is available at\n\\url{https://github.com/GaoLii/DSP}.",
          "link": "http://arxiv.org/abs/2107.09600",
          "publishedOn": "2021-07-28T02:02:34.299Z",
          "wordCount": 695,
          "title": "DSP: Dual Soft-Paste for Unsupervised Domain Adaptive Semantic Segmentation. (arXiv:2107.09600v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramon_E/0/1/0/all/0/1\">Eduard Ramon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triginer_G/0/1/0/all/0/1\">Gil Triginer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escur_J/0/1/0/all/0/1\">Janna Escur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pumarola_A/0/1/0/all/0/1\">Albert Pumarola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1\">Jaime Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1\">Xavier Giro-i-Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>",
          "description": "Recent learning approaches that implicitly represent surface geometry using\ncoordinate-based neural representations have shown impressive results in the\nproblem of multi-view 3D reconstruction. The effectiveness of these techniques\nis, however, subject to the availability of a large number (several tens) of\ninput views of the scene, and computationally demanding optimizations. In this\npaper, we tackle these limitations for the specific problem of few-shot full 3D\nhead reconstruction, by endowing coordinate-based representations with a\nprobabilistic shape prior that enables faster convergence and better\ngeneralization when using few input images (down to three). First, we learn a\nshape model of 3D heads from thousands of incomplete raw scans using implicit\nrepresentations. At test time, we jointly overfit two coordinate-based neural\nnetworks to the scene, one modeling the geometry and another estimating the\nsurface radiance, using implicit differentiable rendering. We devise a\ntwo-stage optimization strategy in which the learned prior is used to\ninitialize and constrain the geometry during an initial optimization phase.\nThen, the prior is unfrozen and fine-tuned to the scene. By doing this, we\nachieve high-fidelity head reconstructions, including hair and shoulders, and\nwith a high level of detail that consistently outperforms both state-of-the-art\n3D Morphable Models methods in the few-shot scenario, and non-parametric\nmethods when large sets of views are available.",
          "link": "http://arxiv.org/abs/2107.12512",
          "publishedOn": "2021-07-28T02:02:34.240Z",
          "wordCount": 654,
          "title": "H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction. (arXiv:2107.12512v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuangjie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaoqing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jin Ye</a>",
          "description": "Recent deep networks have convincingly demonstrated high capability in crowd\ncounting, which is a critical task attracting widespread attention due to its\nvarious industrial applications. Despite such progress, trained data-dependent\nmodels usually can not generalize well to unseen scenarios because of the\ninherent domain shift. To facilitate this issue, this paper proposes a novel\nadversarial scoring network (ASNet) to gradually bridge the gap across domains\nfrom coarse to fine granularity. In specific, at the coarse-grained stage, we\ndesign a dual-discriminator strategy to adapt source domain to be close to the\ntargets from the perspectives of both global and local feature space via\nadversarial learning. The distributions between two domains can thus be aligned\nroughly. At the fine-grained stage, we explore the transferability of source\ncharacteristics by scoring how similar the source samples are to target ones\nfrom multiple levels based on generative probability derived from coarse stage.\nGuided by these hierarchical scores, the transferable source features are\nproperly selected to enhance the knowledge transfer during the adaptation\nprocess. With the coarse-to-fine design, the generalization bottleneck induced\nfrom the domain discrepancy can be effectively alleviated. Three sets of\nmigration experiments show that the proposed methods achieve state-of-the-art\ncounting performance compared with major unsupervised methods.",
          "link": "http://arxiv.org/abs/2107.12858",
          "publishedOn": "2021-07-28T02:02:33.959Z",
          "wordCount": 657,
          "title": "Coarse to Fine: Domain Adaptive Crowd Counting via Adversarial Scoring Network. (arXiv:2107.12858v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dam_T/0/1/0/all/0/1\">Tanmoy Dam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anavatti_S/0/1/0/all/0/1\">Sreenatha G. Anavatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbass_H/0/1/0/all/0/1\">Hussein A. Abbass</a> (Fellow, IEEESchool of Engineering and Information Technology, University of New South Wales Canberra, Australia)",
          "description": "The Latent Space Clustering in Generative adversarial networks (ClusterGAN)\nmethod has been successful with high-dimensional data. However, the method\nassumes uniformlydistributed priors during the generation of modes, which isa\nrestrictive assumption in real-world data and cause loss ofdiversity in the\ngenerated modes. In this paper, we proposeself-augmentation information\nmaximization improved Clus-terGAN (SIMI-ClusterGAN) to learn the distinctive\npriorsfrom the data. The proposed SIMI-ClusterGAN consists offour deep neural\nnetworks: self-augmentation prior network,generator, discriminator and\nclustering inference autoencoder.The proposed method has been validated using\nseven bench-mark data sets and has shown improved performance overstate-of-the\nart methods. To demonstrate the superiority ofSIMI-ClusterGAN performance on\nimbalanced dataset, wehave discussed two imbalanced conditions on MNIST\ndatasetswith one-class imbalance and three classes imbalanced cases.The results\nhighlight the advantages of SIMI-ClusterGAN.",
          "link": "http://arxiv.org/abs/2107.12706",
          "publishedOn": "2021-07-28T02:02:33.823Z",
          "wordCount": 582,
          "title": "Improving ClusterGAN Using Self-AugmentedInformation Maximization of Disentangling LatentSpaces. (arXiv:2107.12706v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhaoyu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Pin Siang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Hsing Wang</a>",
          "description": "In this work, we propose a novel two-stage framework for the efficient 3D\npoint cloud object detection. Instead of transforming point clouds into 2D bird\neye view projections, we parse the raw point cloud data directly in the 3D\nspace yet achieve impressive efficiency and accuracy. To achieve this goal, we\npropose dynamic voxelization, a method that voxellizes points at local scale\non-the-fly. By doing so, we preserve the point cloud geometry with 3D voxels,\nand therefore waive the dependence on expensive MLPs to learn from point\ncoordinates. On the other hand, we inherently still follow the same processing\npattern as point-wise methods (e.g., PointNet) and no longer suffer from the\nquantization issue like conventional convolutions. For further speed\noptimization, we propose the grid-based downsampling and voxelization method,\nand provide different CUDA implementations to accommodate to the discrepant\nrequirements during training and inference phases. We highlight our efficiency\non KITTI 3D object detection dataset with 75 FPS and on Waymo Open dataset with\n25 FPS inference speed with satisfactory accuracy.",
          "link": "http://arxiv.org/abs/2107.12707",
          "publishedOn": "2021-07-28T02:02:33.541Z",
          "wordCount": null,
          "title": "DV-Det: Efficient 3D Point Cloud Object Detection with Dynamic Voxelization. (arXiv:2107.12707v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12978",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1\">Brennan Nichyporuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1\">Justin Szeto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1\">Douglas L. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>",
          "description": "There are many clinical contexts which require accurate detection and\nsegmentation of all focal pathologies (e.g. lesions, tumours) in patient\nimages. In cases where there are a mix of small and large lesions, standard\nbinary cross entropy loss will result in better segmentation of large lesions\nat the expense of missing small ones. Adjusting the operating point to\naccurately detect all lesions generally leads to oversegmentation of large\nlesions. In this work, we propose a novel reweighing strategy to eliminate this\nperformance gap, increasing small pathology detection performance while\nmaintaining segmentation accuracy. We show that our reweighing strategy vastly\noutperforms competing strategies based on experiments on a large scale,\nmulti-scanner, multi-center dataset of Multiple Sclerosis patient images.",
          "link": "http://arxiv.org/abs/2107.12978",
          "publishedOn": "2021-07-28T02:02:33.039Z",
          "wordCount": null,
          "title": "Optimizing Operating Points for High Performance Lesion Detection and Segmentation Using Lesion Size Reweighting. (arXiv:2107.12978v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.05105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhipeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peirong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niethammer_M/0/1/0/all/0/1\">Marc Niethammer</a>",
          "description": "For semantic segmentation, label probabilities are often uncalibrated as they\nare typically only the by-product of a segmentation task. Intersection over\nUnion (IoU) and Dice score are often used as criteria for segmentation success,\nwhile metrics related to label probabilities are not often explored. However,\nprobability calibration approaches have been studied, which match probability\noutputs with experimentally observed errors. These approaches mainly focus on\nclassification tasks, but not on semantic segmentation. Thus, we propose a\nlearning-based calibration method that focuses on multi-label semantic\nsegmentation. Specifically, we adopt a convolutional neural network to predict\nlocal temperature values for probability calibration. One advantage of our\napproach is that it does not change prediction accuracy, hence allowing for\ncalibration as a post-processing step. Experiments on the COCO, CamVid, and\nLPBA40 datasets demonstrate improved calibration performance for a range of\ndifferent metrics. We also demonstrate the good performance of our method for\nmulti-atlas brain segmentation from magnetic resonance images.",
          "link": "http://arxiv.org/abs/2008.05105",
          "publishedOn": "2021-07-28T02:02:33.038Z",
          "wordCount": 619,
          "title": "Local Temperature Scaling for Probability Calibration. (arXiv:2008.05105v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grassucci_E/0/1/0/all/0/1\">Eleonora Grassucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cicero_E/0/1/0/all/0/1\">Edoardo Cicero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comminiello_D/0/1/0/all/0/1\">Danilo Comminiello</a>",
          "description": "Latest Generative Adversarial Networks (GANs) are gathering outstanding\nresults through a large-scale training, thus employing models composed of\nmillions of parameters requiring extensive computational capabilities. Building\nsuch huge models undermines their replicability and increases the training\ninstability. Moreover, multi-channel data, such as images or audio, are usually\nprocessed by realvalued convolutional networks that flatten and concatenate the\ninput, often losing intra-channel spatial relations. To address these issues\nrelated to complexity and information loss, we propose a family of\nquaternion-valued generative adversarial networks (QGANs). QGANs exploit the\nproperties of quaternion algebra, e.g., the Hamilton product, that allows to\nprocess channels as a single entity and capture internal latent relations,\nwhile reducing by a factor of 4 the overall number of parameters. We show how\nto design QGANs and to extend the proposed approach even to advanced models.We\ncompare the proposed QGANs with real-valued counterparts on several image\ngeneration benchmarks. Results show that QGANs are able to obtain better FID\nscores than real-valued GANs and to generate visually pleasing images.\nFurthermore, QGANs save up to 75% of the training parameters. We believe these\nresults may pave the way to novel, more accessible, GANs capable of improving\nperformance and saving computational resources.",
          "link": "http://arxiv.org/abs/2104.09630",
          "publishedOn": "2021-07-28T02:02:32.872Z",
          "wordCount": 678,
          "title": "Quaternion Generative Adversarial Networks. (arXiv:2104.09630v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingze Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Hui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>",
          "description": "We propose a new method to detect deepfake images using the cue of the source\nfeature inconsistency within the forged images. It is based on the hypothesis\nthat images' distinct source features can be preserved and extracted after\ngoing through state-of-the-art deepfake generation processes. We introduce a\nnovel representation learning approach, called pair-wise self-consistency\nlearning (PCL), for training ConvNets to extract these source features and\ndetect deepfake images. It is accompanied by a new image synthesis approach,\ncalled inconsistency image generator (I2G), to provide richly annotated\ntraining data for PCL. Experimental results on seven popular datasets show that\nour models improve averaged AUC over the state of the art from 96.45% to 98.05%\nin the in-dataset evaluation and from 86.03% to 92.18% in the cross-dataset\nevaluation.",
          "link": "http://arxiv.org/abs/2012.09311",
          "publishedOn": "2021-07-28T02:02:32.807Z",
          "wordCount": 594,
          "title": "Learning Self-Consistency for Deepfake Detection. (arXiv:2012.09311v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shihao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1\">Dylan Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1\">Richard Hartley</a>",
          "description": "Occlusions pose a significant challenge to optical flow algorithms that rely\non local evidences. We consider an occluded point to be one that is imaged in\nthe first frame but not in the next, a slight overloading of the standard\ndefinition since it also includes points that move out-of-frame. Estimating the\nmotion of these points is extremely difficult, particularly in the two-frame\nsetting. Previous work relies on CNNs to learn occlusions, without much\nsuccess, or requires multiple frames to reason about occlusions using temporal\nsmoothness. In this paper, we argue that the occlusion problem can be better\nsolved in the two-frame case by modelling image self-similarities. We introduce\na global motion aggregation module, a transformer-based approach to find\nlong-range dependencies between pixels in the first image, and perform global\naggregation on the corresponding motion features. We demonstrate that the\noptical flow estimates in the occluded regions can be significantly improved\nwithout damaging the performance in non-occluded regions. This approach obtains\nnew state-of-the-art results on the challenging Sintel dataset, improving the\naverage end-point error by 13.6% on Sintel Final and 13.7% on Sintel Clean. At\nthe time of submission, our method ranks first on these benchmarks among all\npublished and unpublished approaches. Code is available at\nhttps://github.com/zacjiang/GMA",
          "link": "http://arxiv.org/abs/2104.02409",
          "publishedOn": "2021-07-28T02:02:32.750Z",
          "wordCount": 683,
          "title": "Learning to Estimate Hidden Motions with Global Motion Aggregation. (arXiv:2104.02409v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1\">Zixuan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haizhou Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>",
          "description": "The challenge of the Class Incremental Learning (CIL) lies in difficulty for\na learner to discern the old classes' data from the new while no previous data\nis preserved. Namely, the representation distribution of different phases\noverlaps with each other. In this paper, to alleviate the phenomenon of\nrepresentation overlapping for both memory-based and memory-free methods, we\npropose a new CIL framework, Contrastive Class Concentration for CIL (C4IL).\nOur framework leverages the class concentration effect of contrastive\nrepresentation learning, therefore yielding a representation distribution with\nbetter intra-class compactibility and inter-class separability. Quantitative\nexperiments showcase our framework that is effective in both memory-based and\nmemory-free cases: it outperforms the baseline methods of both cases by 5% in\nterms of the average and top-1 accuracy in 10-phase and 20-phase CIL.\nQualitative results also demonstrate that our method generates a more compact\nrepresentation distribution that alleviates the overlapping problem.",
          "link": "http://arxiv.org/abs/2107.12308",
          "publishedOn": "2021-07-28T02:02:32.735Z",
          "wordCount": 605,
          "title": "Alleviate Representation Overlapping in Class Incremental Learning by Contrastive Class Concentration. (arXiv:2107.12308v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">Robin Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1\">Matthias Rottmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1\">Hanno Gottschalk</a>",
          "description": "Deep neural networks (DNNs) for the semantic segmentation of images are\nusually trained to operate on a predefined closed set of object classes. This\nis in contrast to the \"open world\" setting where DNNs are envisioned to be\ndeployed to. From a functional safety point of view, the ability to detect\nso-called \"out-of-distribution\" (OoD) samples, i.e., objects outside of a DNN's\nsemantic space, is crucial for many applications such as automated driving. A\nnatural baseline approach to OoD detection is to threshold on the pixel-wise\nsoftmax entropy. We present a two-step procedure that significantly improves\nthat approach. Firstly, we utilize samples from the COCO dataset as OoD proxy\nand introduce a second training objective to maximize the softmax entropy on\nthese samples. Starting from pretrained semantic segmentation networks we\nre-train a number of DNNs on different in-distribution datasets and\nconsistently observe improved OoD detection performance when evaluating on\ncompletely disjoint OoD datasets. Secondly, we perform a transparent\npost-processing step to discard false positive OoD samples by so-called \"meta\nclassification\". To this end, we apply linear models to a set of hand-crafted\nmetrics derived from the DNN's softmax probabilities. In our experiments we\nconsistently observe a clear additional gain in OoD detection performance,\ncutting down the number of detection errors by up to 52% when comparing the\nbest baseline with our results. We achieve this improvement sacrificing only\nmarginally in original segmentation performance. Therefore, our method\ncontributes to safer DNNs with more reliable overall system performance.",
          "link": "http://arxiv.org/abs/2012.06575",
          "publishedOn": "2021-07-28T02:02:32.609Z",
          "wordCount": 728,
          "title": "Entropy Maximization and Meta Classification for Out-Of-Distribution Detection in Semantic Segmentation. (arXiv:2012.06575v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1711.05535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrett_M/0/1/0/all/0/1\">Michael Garrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yi-Dong Shen</a>",
          "description": "Matching images and sentences demands a fine understanding of both\nmodalities. In this paper, we propose a new system to discriminatively embed\nthe image and text to a shared visual-textual space. In this field, most\nexisting works apply the ranking loss to pull the positive image / text pairs\nclose and push the negative pairs apart from each other. However, directly\ndeploying the ranking loss is hard for network learning, since it starts from\nthe two heterogeneous features to build inter-modal relationship. To address\nthis problem, we propose the instance loss which explicitly considers the\nintra-modal data distribution. It is based on an unsupervised assumption that\neach image / text group can be viewed as a class. So the network can learn the\nfine granularity from every image/text group. The experiment shows that the\ninstance loss offers better weight initialization for the ranking loss, so that\nmore discriminative embeddings can be learned. Besides, existing works usually\napply the off-the-shelf features, i.e., word2vec and fixed visual feature. So\nin a minor contribution, this paper constructs an end-to-end dual-path\nconvolutional network to learn the image and text representations. End-to-end\nlearning allows the system to directly learn from the data and fully utilize\nthe supervision. On two generic retrieval datasets (Flickr30k and MSCOCO),\nexperiments demonstrate that our method yields competitive accuracy compared to\nstate-of-the-art methods. Moreover, in language based person retrieval, we\nimprove the state of the art by a large margin. The code has been made publicly\navailable.",
          "link": "http://arxiv.org/abs/1711.05535",
          "publishedOn": "2021-07-28T02:02:32.601Z",
          "wordCount": 743,
          "title": "Dual-Path Convolutional Image-Text Embeddings with Instance Loss. (arXiv:1711.05535v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zehui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenhongyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiaofei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Feng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>",
          "description": "Deep learning-based dense object detectors have achieved great success in the\npast few years and have been applied to numerous multimedia applications such\nas video understanding. However, the current training pipeline for dense\ndetectors is compromised to lots of conjunctions that may not hold. In this\npaper, we investigate three such important conjunctions: 1) only samples\nassigned as positive in classification head are used to train the regression\nhead; 2) classification and regression share the same input feature and\ncomputational fields defined by the parallel head architecture; and 3) samples\ndistributed in different feature pyramid layers are treated equally when\ncomputing the loss. We first carry out a series of pilot experiments to show\ndisentangling such conjunctions can lead to persistent performance improvement.\nThen, based on these findings, we propose Disentangled Dense Object Detector\n(DDOD), in which simple and effective disentanglement mechanisms are designed\nand integrated into the current state-of-the-art dense object detectors.\nExtensive experiments on MS COCO benchmark show that our approach can lead to\n2.0 mAP, 2.4 mAP and 2.2 mAP absolute improvements on RetinaNet, FCOS, and ATSS\nbaselines with negligible extra overhead. Notably, our best model reaches 55.0\nmAP on the COCO test-dev set and 93.5 AP on the hard subset of WIDER FACE,\nachieving new state-of-the-art performance on these two competitive benchmarks.\nCode is available at https://github.com/zehuichen123/DDOD.",
          "link": "http://arxiv.org/abs/2107.02963",
          "publishedOn": "2021-07-28T02:02:32.594Z",
          "wordCount": 676,
          "title": "Disentangle Your Dense Object Detector. (arXiv:2107.02963v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dupont_R/0/1/0/all/0/1\">Robin Dupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahbi_H/0/1/0/all/0/1\">Hichem Sahbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_G/0/1/0/all/0/1\">Guillaume Michel</a>",
          "description": "Pruning seeks to design lightweight architectures by removing redundant\nweights in overparameterized networks. Most of the existing techniques first\nremove structured sub-networks (filters, channels,...) and then fine-tune the\nresulting networks to maintain a high accuracy. However, removing a whole\nstructure is a strong topological prior and recovering the accuracy, with\nfine-tuning, is highly cumbersome. In this paper, we introduce an \"end-to-end\"\nlightweight network design that achieves training and pruning simultaneously\nwithout fine-tuning. The design principle of our method relies on\nreparametrization that learns not only the weights but also the topological\nstructure of the lightweight sub-network. This reparametrization acts as a\nprior (or regularizer) that defines pruning masks implicitly from the weights\nof the underlying network, without increasing the number of training\nparameters. Sparsity is induced with a budget loss that provides an accurate\npruning. Extensive experiments conducted on the CIFAR10 and the TinyImageNet\ndatasets, using standard architectures (namely Conv4, VGG19 and ResNet18), show\ncompelling results without fine-tuning.",
          "link": "http://arxiv.org/abs/2107.03909",
          "publishedOn": "2021-07-28T02:02:32.587Z",
          "wordCount": 615,
          "title": "Weight Reparametrization for Budget-Aware Network Pruning. (arXiv:2107.03909v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12461",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zunair_H/0/1/0/all/0/1\">Hasib Zunair</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamza_A/0/1/0/all/0/1\">A. Ben Hamza</a>",
          "description": "The U-Net architecture, built upon the fully convolutional network, has\nproven to be effective in biomedical image segmentation. However, U-Net applies\nskip connections to merge semantically different low- and high-level\nconvolutional features, resulting in not only blurred feature maps, but also\nover- and under-segmented target regions. To address these limitations, we\npropose a simple, yet effective end-to-end depthwise encoder-decoder fully\nconvolutional network architecture, called Sharp U-Net, for binary and\nmulti-class biomedical image segmentation. The key rationale of Sharp U-Net is\nthat instead of applying a plain skip connection, a depthwise convolution of\nthe encoder feature map with a sharpening kernel filter is employed prior to\nmerging the encoder and decoder features, thereby producing a sharpened\nintermediate feature map of the same size as the encoder map. Using this\nsharpening filter layer, we are able to not only fuse semantically less\ndissimilar features, but also to smooth out artifacts throughout the network\nlayers during the early stages of training. Our extensive experiments on six\ndatasets show that the proposed Sharp U-Net model consistently outperforms or\nmatches the recent state-of-the-art baselines in both binary and multi-class\nsegmentation tasks, while adding no extra learnable parameters. Furthermore,\nSharp U-Net outperforms baselines that have more than three times the number of\nlearnable parameters.",
          "link": "http://arxiv.org/abs/2107.12461",
          "publishedOn": "2021-07-28T02:02:32.568Z",
          "wordCount": 652,
          "title": "Sharp U-Net: Depthwise Convolutional Network for Biomedical Image Segmentation. (arXiv:2107.12461v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12889",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Felfeliyan_B/0/1/0/all/0/1\">Banafshe Felfeliyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hareendranathan_A/0/1/0/all/0/1\">Abhilash Hareendranathan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuntze_G/0/1/0/all/0/1\">Gregor Kuntze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jaremko_J/0/1/0/all/0/1\">Jacob L. Jaremko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ronsky_J/0/1/0/all/0/1\">Janet L. Ronsky</a>",
          "description": "Objective assessment of Magnetic Resonance Imaging (MRI) scans of\nosteoarthritis (OA) can address the limitation of the current OA assessment.\nSegmentation of bone, cartilage, and joint fluid is necessary for the OA\nobjective assessment. Most of the proposed segmentation methods are not\nperforming instance segmentation and suffer from class imbalance problems. This\nstudy deployed Mask R-CNN instance segmentation and improved it (improved-Mask\nR-CNN (iMaskRCNN)) to obtain a more accurate generalized segmentation for\nOA-associated tissues. Training and validation of the method were performed\nusing 500 MRI knees from the Osteoarthritis Initiative (OAI) dataset and 97 MRI\nscans of patients with symptomatic hip OA. Three modifications to Mask R-CNN\nyielded the iMaskRCNN: adding a 2nd ROIAligned block, adding an extra decoder\nlayer to the mask-header, and connecting them by a skip connection. The results\nwere assessed using Hausdorff distance, dice score, and coefficients of\nvariation (CoV). The iMaskRCNN led to improved bone and cartilage segmentation\ncompared to Mask RCNN as indicated with the increase in dice score from 95% to\n98% for the femur, 95% to 97% for tibia, 71% to 80% for femoral cartilage, and\n81% to 82% for tibial cartilage. For the effusion detection, dice improved with\niMaskRCNN 72% versus MaskRCNN 71%. The CoV values for effusion detection\nbetween Reader1 and Mask R-CNN (0.33), Reader1 and iMaskRCNN (0.34), Reader2\nand Mask R-CNN (0.22), Reader2 and iMaskRCNN (0.29) are close to CoV between\ntwo readers (0.21), indicating a high agreement between the human readers and\nboth Mask R-CNN and iMaskRCNN. Mask R-CNN and iMaskRCNN can reliably and\nsimultaneously extract different scale articular tissues involved in OA,\nforming the foundation for automated assessment of OA. The iMaskRCNN results\nshow that the modification improved the network performance around the edges.",
          "link": "http://arxiv.org/abs/2107.12889",
          "publishedOn": "2021-07-28T02:02:32.560Z",
          "wordCount": 755,
          "title": "Improved-Mask R-CNN: Towards an Accurate Generic MSK MRI instance segmentation platform (Data from the Osteoarthritis Initiative). (arXiv:2107.12889v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1\">Da Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chow_V/0/1/0/all/0/1\">Vincent Chow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popuri_K/0/1/0/all/0/1\">Karteek Popuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beg_M/0/1/0/all/0/1\">Mirza Faisal Beg</a>",
          "description": "The latest advances in computer-assisted precision medicine are making it\nfeasible to move from population-wide models that are useful to discover\naggregate patterns that hold for group-based analysis to patient-specific\nmodels that can drive patient-specific decisions with regard to treatment\nchoices, and predictions of outcomes of treatment. Body Composition is\nrecognized as an important driver and risk factor for a wide variety of\ndiseases, as well as a predictor of individual patient-specific clinical\noutcomes to treatment choices or surgical interventions. 3D CT images are\nroutinely acquired in the oncological worklows and deliver accurate rendering\nof internal anatomy and therefore can be used opportunistically to assess the\namount of skeletal muscle and adipose tissue compartments. Powerful tools of\nartificial intelligence such as deep learning are making it feasible now to\nsegment the entire 3D image and generate accurate measurements of all internal\nanatomy. These will enable the overcoming of the severe bottleneck that existed\npreviously, namely, the need for manual segmentation, which was prohibitive to\nscale to the hundreds of 2D axial slices that made up a 3D volumetric image.\nAutomated tools such as presented here will now enable harvesting whole-body\nmeasurements from 3D CT or MRI images, leading to a new era of discovery of the\ndrivers of various diseases based on individual tissue, organ volume, shape,\nand functional status. These measurements were hitherto unavailable thereby\nlimiting the field to a very small and limited subset. These discoveries and\nthe potential to perform individual image segmentation with high speed and\naccuracy are likely to lead to the incorporation of these 3D measures into\nindividual specific treatment planning models related to nutrition, aging,\nchemotoxicity, surgery and survival after the onset of a major disease such as\ncancer.",
          "link": "http://arxiv.org/abs/2106.00652",
          "publishedOn": "2021-07-28T02:02:32.553Z",
          "wordCount": 849,
          "title": "Comprehensive Validation of Automated Whole Body Skeletal Muscle, Adipose Tissue, and Bone Segmentation from 3D CT images for Body Composition Analysis: Towards Extended Body Composition. (arXiv:2106.00652v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.10343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Ke Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoou Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>",
          "description": "Very deep Convolutional Neural Networks (CNNs) have greatly improved the\nperformance on various image restoration tasks. However, this comes at a price\nof increasing computational burden, hence limiting their practical usages. We\nobserve that some corrupted image regions are inherently easier to restore than\nothers since the distortion and content vary within an image. To leverage this,\nwe propose Path-Restore, a multi-path CNN with a pathfinder that can\ndynamically select an appropriate route for each image region. We train the\npathfinder using reinforcement learning with a difficulty-regulated reward.\nThis reward is related to the performance, complexity and \"the difficulty of\nrestoring a region\". A policy mask is further investigated to jointly process\nall the image regions. We conduct experiments on denoising and mixed\nrestoration tasks. The results show that our method achieves comparable or\nsuperior performance to existing approaches with less computational cost. In\nparticular, Path-Restore is effective for real-world denoising, where the noise\ndistribution varies across different regions on a single image. Compared to the\nstate-of-the-art RIDNet, our method achieves comparable performance and runs\n2.7x faster on the realistic Darmstadt Noise Dataset.",
          "link": "http://arxiv.org/abs/1904.10343",
          "publishedOn": "2021-07-28T02:02:32.545Z",
          "wordCount": 664,
          "title": "Path-Restore: Learning Network Path Selection for Image Restoration. (arXiv:1904.10343v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raumanns_R/0/1/0/all/0/1\">Ralf Raumanns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schouten_G/0/1/0/all/0/1\">Gerard Schouten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joosten_M/0/1/0/all/0/1\">Max Joosten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pluim_J/0/1/0/all/0/1\">Josien P. W. Pluim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>",
          "description": "We present ENHANCE, an open dataset with multiple annotations to complement\nthe existing ISIC and PH2 skin lesion classification datasets. This dataset\ncontains annotations of visual ABC (asymmetry, border, colour) features from\nnon-expert annotation sources: undergraduate students, crowd workers from\nAmazon MTurk and classic image processing algorithms. In this paper we first\nanalyse the correlations between the annotations and the diagnostic label of\nthe lesion, as well as study the agreement between different annotation\nsources. Overall we find weak correlations of non-expert annotations with the\ndiagnostic label, and low agreement between different annotation sources. We\nthen study multi-task learning (MTL) with the annotations as additional labels,\nand show that non-expert annotations can improve (ensembles of)\nstate-of-the-art convolutional neural networks via MTL. We hope that our\ndataset can be used in further research into multiple annotations and/or MTL.\nAll data and models are available on Github:\nhttps://github.com/raumannsr/ENHANCE.",
          "link": "http://arxiv.org/abs/2107.12734",
          "publishedOn": "2021-07-28T02:02:32.525Z",
          "wordCount": 614,
          "title": "ENHANCE (ENriching Health data by ANnotations of Crowd and Experts): A case study for skin lesion classification. (arXiv:2107.12734v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wimmer_P/0/1/0/all/0/1\">Paul Wimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehnert_J/0/1/0/all/0/1\">Jens Mehnert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1\">Alexandru Condurache</a>",
          "description": "State-of-the-art deep neural network (DNN) pruning techniques, applied\none-shot before training starts, evaluate sparse architectures with the help of\na single criterion -- called pruning score. Pruning weights based on a solitary\nscore works well for some architectures and pruning rates but may also fail for\nother ones. As a common baseline for pruning scores, we introduce the notion of\na generalized synaptic score (GSS). In this work we do not concentrate on a\nsingle pruning criterion, but provide a framework for combining arbitrary GSSs\nto create more powerful pruning strategies. These COmbined Pruning Scores\n(COPS) are obtained by solving a constrained optimization problem. Optimizing\nfor more than one score prevents the sparse network to overly specialize on an\nindividual task, thus COntrols Pruning before training Starts. The\ncombinatorial optimization problem given by COPS is relaxed on a linear program\n(LP). This LP is solved analytically and determines a solution for COPS.\nFurthermore, an algorithm to compute it for two scores numerically is proposed\nand evaluated. Solving COPS in such a way has lower complexity than the best\ngeneral LP solver. In our experiments we compared pruning with COPS against\nstate-of-the-art methods for different network architectures and image\nclassification tasks and obtained improved results.",
          "link": "http://arxiv.org/abs/2107.12673",
          "publishedOn": "2021-07-28T02:02:32.519Z",
          "wordCount": 647,
          "title": "COPS: Controlled Pruning Before Training Starts. (arXiv:2107.12673v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_M/0/1/0/all/0/1\">Mingyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Honghui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Teli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baochang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shumin Han</a>",
          "description": "Transformers with remarkable global representation capacities achieve\ncompetitive results for visual tasks, but fail to consider high-level local\npattern information in input images. In this paper, we present a generic\nDual-stream Network (DS-Net) to fully explore the representation capacity of\nlocal and global pattern features for image classification. Our DS-Net can\nsimultaneously calculate fine-grained and integrated features and efficiently\nfuse them. Specifically, we propose an Intra-scale Propagation module to\nprocess two different resolutions in each block and an Inter-Scale Alignment\nmodule to perform information interaction across features at dual scales.\nBesides, we also design a Dual-stream FPN (DS-FPN) to further enhance\ncontextual information for downstream dense predictions. Without bells and\nwhistles, the propsed DS-Net outperforms Deit-Small by 2.4% in terms of top-1\naccuracy on ImageNet-1k and achieves state-of-the-art performance over other\nVision Transformers and ResNets. For object detection and instance\nsegmentation, DS-Net-Small respectively outperforms ResNet-50 by 6.4% and 5.5 %\nin terms of mAP on MSCOCO 2017, and surpasses the previous state-of-the-art\nscheme, which significantly demonstrates its potential to be a general backbone\nin vision tasks. The code will be released soon.",
          "link": "http://arxiv.org/abs/2105.14734",
          "publishedOn": "2021-07-28T02:02:32.504Z",
          "wordCount": 660,
          "title": "Dual-stream Network for Visual Recognition. (arXiv:2105.14734v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kosman_E/0/1/0/all/0/1\">Eitan Kosman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Dotan Di Castro</a>",
          "description": "Autonomous driving gained huge traction in recent years, due to its potential\nto change the way we commute. Much effort has been put into trying to estimate\nthe state of a vehicle. Meanwhile, learning to forecast the state of a vehicle\nahead introduces new capabilities, such as predicting dangerous situations.\nMoreover, forecasting brings new supervision opportunities by learning to\npredict richer a context, expressed by multiple horizons. Intuitively, a video\nstream originated from a front-facing camera is necessary because it encodes\ninformation about the upcoming road. Besides, historical traces of the\nvehicle's states give more context. In this paper, we tackle multi-horizon\nforecasting of vehicle states by fusing the two modalities. We design and\nexperiment with 3 end-to-end architectures that exploit 3D convolutions for\nvisual features extraction and 1D convolutions for features extraction from\nspeed and steering angle traces. To demonstrate the effectiveness of our\nmethod, we perform extensive experiments on two publicly available real-world\ndatasets, Comma2k19 and the Udacity challenge. We show that we are able to\nforecast a vehicle's state to various horizons, while outperforming the current\nstate-of-the-art results on the related task of driving state estimation. We\nexamine the contribution of vision features, and find that a model fed with\nvision features achieves an error that is 56.6% and 66.9% of the error of a\nmodel that doesn't use those features, on the Udacity and Comma2k19 datasets\nrespectively.",
          "link": "http://arxiv.org/abs/2107.12674",
          "publishedOn": "2021-07-28T02:02:32.496Z",
          "wordCount": 672,
          "title": "Vision-Guided Forecasting -- Visual Context for Multi-Horizon Time Series Forecasting. (arXiv:2107.12674v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gothankar_R/0/1/0/all/0/1\">Ruchira Gothankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troia_F/0/1/0/all/0/1\">Fabio Di Troia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1\">Mark Stamp</a>",
          "description": "YouTube videos often include captivating descriptions and intriguing\nthumbnails designed to increase the number of views, and thereby increase the\nrevenue for the person who posted the video. This creates an incentive for\npeople to post clickbait videos, in which the content might deviate\nsignificantly from the title, description, or thumbnail. In effect, users are\ntricked into clicking on clickbait videos. In this research, we consider the\nchallenging problem of detecting clickbait YouTube videos. We experiment with\nmultiple state-of-the-art machine learning techniques using a variety of\ntextual features.",
          "link": "http://arxiv.org/abs/2107.12791",
          "publishedOn": "2021-07-28T02:02:32.478Z",
          "wordCount": 517,
          "title": "Clickbait Detection in YouTube Videos. (arXiv:2107.12791v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1\">Sreyas Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_J/0/1/0/all/0/1\">Joshua L. Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzorro_R/0/1/0/all/0/1\">Ramon Manzorro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crozier_P/0/1/0/all/0/1\">Peter A. Crozier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simoncelli_E/0/1/0/all/0/1\">Eero P. Simoncelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>",
          "description": "Deep convolutional neural networks (CNNs) for image denoising are usually\ntrained on large datasets. These models achieve the current state of the art,\nbut they have difficulties generalizing when applied to data that deviate from\nthe training distribution. Recent work has shown that it is possible to train\ndenoisers on a single noisy image. These models adapt to the features of the\ntest image, but their performance is limited by the small amount of information\nused to train them. Here we propose \"GainTuning\", in which CNN models\npre-trained on large datasets are adaptively and selectively adjusted for\nindividual test images. To avoid overfitting, GainTuning optimizes a single\nmultiplicative scaling parameter (the \"Gain\") of each channel in the\nconvolutional layers of the CNN. We show that GainTuning improves\nstate-of-the-art CNNs on standard image-denoising benchmarks, boosting their\ndenoising performance on nearly every image in a held-out test set. These\nadaptive improvements are even more substantial for test images differing\nsystematically from the training data, either in noise level or image type. We\nillustrate the potential of adaptive denoising in a scientific application, in\nwhich a CNN is trained on synthetic data, and tested on real\ntransmission-electron-microscope images. In contrast to the existing\nmethodology, GainTuning is able to faithfully reconstruct the structure of\ncatalytic nanoparticles from these data at extremely low signal-to-noise\nratios.",
          "link": "http://arxiv.org/abs/2107.12815",
          "publishedOn": "2021-07-28T02:02:32.472Z",
          "wordCount": 655,
          "title": "Adaptive Denoising via GainTuning. (arXiv:2107.12815v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Darafsh_S/0/1/0/all/0/1\">Sahar Darafsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghidary_S/0/1/0/all/0/1\">Saeed Shiry Ghidary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_M/0/1/0/all/0/1\">Morteza Saheb Zamani</a>",
          "description": "With the rapid increase in digital technologies, most fields of study include\nrecognition of human activity and intention recognition, which are important in\nsmart environments. In this research, we introduce a real-time activity\nrecognition to recognize people's intentions to pass or not pass a door. This\nsystem, if applied in elevators and automatic doors will save energy and\nincrease efficiency. For this study, data preparation is applied to combine the\nspatial and temporal features with the help of digital image processing\nprinciples. Nevertheless, unlike previous studies, only one AlexNet neural\nnetwork is used instead of two-stream convolutional neural networks. Our\nembedded system was implemented with an accuracy of 98.78% on our Intention\nRecognition dataset. We also examined our data representation approach on other\ndatasets, including HMDB-51, KTH, and Weizmann, and obtained accuracy of\n78.48%, 97.95%, and 100%, respectively. The image recognition and neural\nnetwork models were simulated and implemented using Xilinx simulators for\nZCU102 board. The operating frequency of this embedded system is 333 MHz, and\nit works in real-time with 120 frames per second (fps).",
          "link": "http://arxiv.org/abs/2107.12744",
          "publishedOn": "2021-07-28T02:02:32.464Z",
          "wordCount": 618,
          "title": "Real-Time Activity Recognition and Intention Recognition Using a Vision-based Embedded System. (arXiv:2107.12744v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baird_A/0/1/0/all/0/1\">Alice Baird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stappen_L/0/1/0/all/0/1\">Lukas Stappen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christ_L/0/1/0/all/0/1\">Lukas Christ</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schumann_L/0/1/0/all/0/1\">Lea Schumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messner_E/0/1/0/all/0/1\">Eva-Maria Me&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>",
          "description": "Emotion is an inherently subjective psychophysiological human-state and to\nproduce an agreed-upon representation (gold standard) for continuous emotion\nrequires a time-consuming and costly training procedure of multiple human\nannotators. There is strong evidence in the literature that physiological\nsignals are sufficient objective markers for states of emotion, particularly\narousal. In this contribution, we utilise a dataset which includes continuous\nemotion and physiological signals - Heartbeats per Minute (BPM), Electrodermal\nActivity (EDA), and Respiration-rate - captured during a stress induced\nscenario (Trier Social Stress Test). We utilise a Long Short-Term Memory,\nRecurrent Neural Network to explore the benefit of fusing these physiological\nsignals with arousal as the target, learning from various audio, video, and\ntextual based features. We utilise the state-of-the-art MuSe-Toolbox to\nconsider both annotation delay and inter-rater agreement weighting when fusing\nthe target signals. An improvement in Concordance Correlation Coefficient (CCC)\nis seen across features sets when fusing EDA with arousal, compared to the\narousal only gold standard results. Additionally, BERT-based textual features'\nresults improved for arousal plus all physiological signals, obtaining up to\n.3344 CCC compared to .2118 CCC for arousal only. Multimodal fusion also\nimproves overall CCC with audio plus video features obtaining up to .6157 CCC\nto recognize arousal plus EDA and BPM.",
          "link": "http://arxiv.org/abs/2107.12964",
          "publishedOn": "2021-07-28T02:02:32.456Z",
          "wordCount": 660,
          "title": "A Physiologically-adapted Gold Standard for Arousal During a Stress Induced Scenario. (arXiv:2107.12964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xuan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xizhou Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>",
          "description": "As a kind of generative self-supervised learning methods, generative\nadversarial nets have been widely studied in the field of anomaly detection.\nHowever, the representation learning ability of the generator is limited since\nit pays too much attention to pixel-level details, and generator is difficult\nto learn abstract semantic representations from label prediction pretext tasks\nas effective as discriminator. In order to improve the representation learning\nability of generator, we propose a self-supervised learning framework combining\ngenerative methods and discriminative methods. The generator no longer learns\nrepresentation by reconstruction error, but the guidance of discriminator, and\ncould benefit from pretext tasks designed for discriminative methods. Our\ndiscriminative-generative representation learning method has performance close\nto discriminative methods and has a great advantage in speed. Our method used\nin one-class anomaly detection task significantly outperforms several\nstate-of-the-arts on multiple benchmark data sets, increases the performance of\nthe top-performing GAN-based baseline by 6% on CIFAR-10 and 2% on MVTAD.",
          "link": "http://arxiv.org/abs/2107.12753",
          "publishedOn": "2021-07-28T02:02:32.449Z",
          "wordCount": 598,
          "title": "Discriminative-Generative Representation Learning for One-Class Anomaly Detection. (arXiv:2107.12753v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Harish_A/0/1/0/all/0/1\">Abhinav Narayan Harish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagar_R/0/1/0/all/0/1\">Rajendra Nagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1\">Shanmuganathan Raman</a>",
          "description": "Autonomous assembly of objects is an essential task in robotics and 3D\ncomputer vision. It has been studied extensively in robotics as a problem of\nmotion planning, actuator control and obstacle avoidance. However, the task of\ndeveloping a generalized framework for assembly robust to structural variants\nremains relatively unexplored. In this work, we tackle this problem using a\nrecurrent graph learning framework considering inter-part relations and the\nprogressive update of the part pose. Our network can learn more plausible\npredictions of shape structure by accounting for priorly assembled parts.\nCompared to the current state-of-the-art, our network yields up to 10%\nimprovement in part accuracy and up to 15% improvement in connectivity accuracy\non the PartNet dataset. Moreover, our resulting latent space facilitates\nexciting applications such as shape recovery from the point-cloud components.\nWe conduct extensive experiments to justify our design choices and demonstrate\nthe effectiveness of the proposed framework.",
          "link": "http://arxiv.org/abs/2107.12859",
          "publishedOn": "2021-07-28T02:02:32.441Z",
          "wordCount": 598,
          "title": "RGL-NET: A Recurrent Graph Learning framework for Progressive Part Assembly. (arXiv:2107.12859v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengyi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaohui Hu</a>",
          "description": "Since the superiority of Transformer in learning long-term dependency, the\nsign language Transformer model achieves remarkable progress in Sign Language\nRecognition (SLR) and Translation (SLT). However, there are several issues with\nthe Transformer that prevent it from better sign language understanding. The\nfirst issue is that the self-attention mechanism learns sign video\nrepresentation in a frame-wise manner, neglecting the temporal semantic\nstructure of sign gestures. Secondly, the attention mechanism with absolute\nposition encoding is direction and distance unaware, thus limiting its ability.\nTo address these issues, we propose a new model architecture, namely PiSLTRc,\nwith two distinctive characteristics: (i) content-aware and position-aware\nconvolution layers. Specifically, we explicitly select relevant features using\na novel content-aware neighborhood gathering method. Then we aggregate these\nfeatures with position-informed temporal convolution layers, thus generating\nrobust neighborhood-enhanced sign representation. (ii) injecting the relative\nposition information to the attention mechanism in the encoder, decoder, and\neven encoder-decoder cross attention. Compared with the vanilla Transformer\nmodel, our model performs consistently better on three large-scale sign\nlanguage benchmarks: PHOENIX-2014, PHOENIX-2014-T and CSL. Furthermore,\nextensive experiments demonstrate that the proposed method achieves\nstate-of-the-art performance on translation quality with $+1.6$ BLEU\nimprovements.",
          "link": "http://arxiv.org/abs/2107.12600",
          "publishedOn": "2021-07-28T02:02:32.420Z",
          "wordCount": 624,
          "title": "PiSLTRc: Position-informed Sign Language Transformer with Content-aware Convolution. (arXiv:2107.12600v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.11091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schlett_T/0/1/0/all/0/1\">Torsten Schlett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>",
          "description": "Face recognition can benefit from the utilization of depth data captured\nusing low-cost cameras, in particular for presentation attack detection\npurposes. Depth video output from these capture devices can however contain\ndefects such as holes or general depth inaccuracies. This work proposes a deep\nlearning face depth enhancement method in this context of facial biometrics,\nwhich adds a security aspect to the topic. U-Net-like architectures are\nutilized, and the networks are compared against hand-crafted enhancer types, as\nwell as a similar depth enhancer network from related work trained for an\nadjacent application scenario. All tested enhancer types exclusively use depth\ndata as input, which differs from methods that enhance depth based on\nadditional input data such as visible light color images. Synthetic face depth\nground truth images and degraded forms thereof are created with help of PRNet,\nto train multiple deep learning enhancer models with different network sizes\nand training configurations. Evaluations are carried out on the synthetic data,\non Kinect v1 images from the KinectFaceDB, and on in-house RealSense D435\nimages. These evaluations include an assessment of the falsification for\noccluded face depth input, which is relevant to biometric security. The\nproposed deep learning enhancers yield noticeably better results than the\ntested preexisting enhancers, without overly falsifying depth data when\nnon-face input is provided, and are shown to reduce the error of a simple\nlandmark-based PAD method.",
          "link": "http://arxiv.org/abs/2006.11091",
          "publishedOn": "2021-07-28T02:02:32.413Z",
          "wordCount": 698,
          "title": "Deep Learning-based Single Image Face Depth Data Enhancement. (arXiv:2006.11091v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuda Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xin Du</a>",
          "description": "Image enhancement is a subjective process whose targets vary with user\npreferences. In this paper, we propose a deep learning-based image enhancement\nmethod covering multiple tonal styles using only a single model dubbed\nStarEnhancer. It can transform an image from one tonal style to another, even\nif that style is unseen. With a simple one-time setting, users can customize\nthe model to make the enhanced images more in line with their aesthetics. To\nmake the method more practical, we propose a well-designed enhancer that can\nprocess a 4K-resolution image over 200 FPS but surpasses the contemporaneous\nsingle style image enhancement methods in terms of PSNR, SSIM, and LPIPS.\nFinally, our proposed enhancement method has good interactability, which allows\nthe user to fine-tune the enhanced image using intuitive options.",
          "link": "http://arxiv.org/abs/2107.12898",
          "publishedOn": "2021-07-28T02:02:32.384Z",
          "wordCount": 560,
          "title": "StarEnhancer: Learning Real-Time and Style-Aware Image Enhancement. (arXiv:2107.12898v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1\">Akshay Rangesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deo_N/0/1/0/all/0/1\">Nachiket Deo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greer_R/0/1/0/all/0/1\">Ross Greer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunaratne_P/0/1/0/all/0/1\">Pujitha Gunaratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1\">Mohan M. Trivedi</a>",
          "description": "Understanding occupant-vehicle interactions by modeling control transitions\nis important to ensure safe approaches to passenger vehicle automation. Models\nwhich contain contextual, semantically meaningful representations of driver\nstates can be used to determine the appropriate timing and conditions for\ntransfer of control between driver and vehicle. However, such models rely on\nreal-world control take-over data from drivers engaged in distracting\nactivities, which is costly to collect. Here, we introduce a scheme for data\naugmentation for such a dataset. Using the augmented dataset, we develop and\ntrain take-over time (TOT) models that operate sequentially on mid and\nhigh-level features produced by computer vision algorithms operating on\ndifferent driver-facing camera views, showing models trained on the augmented\ndataset to outperform the initial dataset. The demonstrated model features\nencode different aspects of the driver state, pertaining to the face, hands,\nfoot and upper body of the driver. We perform ablative experiments on feature\ncombinations as well as model architectures, showing that a TOT model supported\nby augmented data can be used to produce continuous estimates of take-over\ntimes without delay, suitable for complex real-world scenarios.",
          "link": "http://arxiv.org/abs/2107.12932",
          "publishedOn": "2021-07-28T02:02:32.345Z",
          "wordCount": 639,
          "title": "Predicting Take-over Time for Autonomous Driving with Real-World Data: Robust Data Augmentation, Models, and Evaluation. (arXiv:2107.12932v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rukundo_O/0/1/0/all/0/1\">Olivier Rukundo</a>",
          "description": "This paper presents the evaluation of effects of image size on deep learning\nperformance via semantic segmentation of magnetic resonance heart images with\nU-net for fully automated quantification of myocardial infarction. Both\nnon-extra pixel and extra pixel interpolation algorithms are used to change the\nsize of images in datasets of interest. Extra class labels, in interpolated\nground truth segmentation images, are removed using thresholding, median\nfiltering, and subtraction strategies. Common class metrics are used to\nevaluate the quality of semantic segmentation with U-net against the ground\ntruth segmentation while arbitrary threshold, comparison of the sums, and sums\nof differences between medical experts and fully automated results are options\nused to estimate the relationship between medical experts-based quantification\nand fully automated quantification results.",
          "link": "http://arxiv.org/abs/2101.11508",
          "publishedOn": "2021-07-28T02:02:32.326Z",
          "wordCount": 592,
          "title": "Effects of Image Size on Deep Learning. (arXiv:2101.11508v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Runze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1\">Srikrishna Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1\">Bir Bhanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyan Wu</a>",
          "description": "We consider the problem of estimating frame-level full human body meshes\ngiven a video of a person with natural motion dynamics. While much progress in\nthis field has been in single image-based mesh estimation, there has been a\nrecent uptick in efforts to infer mesh dynamics from video given its role in\nalleviating issues such as depth ambiguity and occlusions. However, a key\nlimitation of existing work is the assumption that all the observed motion\ndynamics can be modeled using one dynamical/recurrent model. While this may\nwork well in cases with relatively simplistic dynamics, inference with\nin-the-wild videos presents many challenges. In particular, it is typically the\ncase that different body parts of a person undergo different dynamics in the\nvideo, e.g., legs may move in a way that may be dynamically different from\nhands (e.g., a person dancing). To address these issues, we present a new\nmethod for video mesh recovery that divides the human mesh into several local\nparts following the standard skeletal model. We then model the dynamics of each\nlocal part with separate recurrent models, with each model conditioned\nappropriately based on the known kinematic structure of the human body. This\nresults in a structure-informed local recurrent learning architecture that can\nbe trained in an end-to-end fashion with available annotations. We conduct a\nvariety of experiments on standard video mesh recovery benchmark datasets such\nas Human3.6M, MPI-INF-3DHP, and 3DPW, demonstrating the efficacy of our design\nof modeling local dynamics as well as establishing state-of-the-art results\nbased on standard evaluation metrics.",
          "link": "http://arxiv.org/abs/2107.12847",
          "publishedOn": "2021-07-28T02:02:32.316Z",
          "wordCount": 708,
          "title": "Learning Local Recurrent Models for Human Mesh Recovery. (arXiv:2107.12847v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laousy_O/0/1/0/all/0/1\">Othmane Laousy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chassagnon_G/0/1/0/all/0/1\">Guillaume Chassagnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1\">Edouard Oyallon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paragios_N/0/1/0/all/0/1\">Nikos Paragios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revel_M/0/1/0/all/0/1\">Marie-Pierre Revel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>",
          "description": "Sarcopenia is a medical condition characterized by a reduction in muscle mass\nand function. A quantitative diagnosis technique consists of localizing the CT\nslice passing through the middle of the third lumbar area (L3) and segmenting\nmuscles at this level. In this paper, we propose a deep reinforcement learning\nmethod for accurate localization of the L3 CT slice. Our method trains a\nreinforcement learning agent by incentivizing it to discover the right\nposition. Specifically, a Deep Q-Network is trained to find the best policy to\nfollow for this problem. Visualizing the training process shows that the agent\nmimics the scrolling of an experienced radiologist. Extensive experiments\nagainst other state-of-the-art deep learning based methods for L3 localization\nprove the superiority of our technique which performs well even with limited\namount of data and annotations.",
          "link": "http://arxiv.org/abs/2107.12800",
          "publishedOn": "2021-07-28T02:02:32.301Z",
          "wordCount": 577,
          "title": "Deep Reinforcement Learning for L3 Slice Localization in Sarcopenia Assessment. (arXiv:2107.12800v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1\">Andr&#xe9;s G&#xf3;mez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genevois_T/0/1/0/all/0/1\">Thomas Genevois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lussereau_J/0/1/0/all/0/1\">Jerome Lussereau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laugier_C/0/1/0/all/0/1\">Christian Laugier</a>",
          "description": "Object detection is a critical problem for the safe interaction between\nautonomous vehicles and road users. Deep-learning methodologies allowed the\ndevelopment of object detection approaches with better performance. However,\nthere is still the challenge to obtain more characteristics from the objects\ndetected in real-time. The main reason is that more information from the\nenvironment's objects can improve the autonomous vehicle capacity to face\ndifferent urban situations. This paper proposes a new approach to detect static\nand dynamic objects in front of an autonomous vehicle. Our approach can also\nget other characteristics from the objects detected, like their position,\nvelocity, and heading. We develop our proposal fusing results of the\nenvironment's interpretations achieved of YoloV3 and a Bayesian filter. To\ndemonstrate our proposal's performance, we asses it through a benchmark dataset\nand real-world data obtained from an autonomous platform. We compared the\nresults achieved with another approach.",
          "link": "http://arxiv.org/abs/2107.12692",
          "publishedOn": "2021-07-28T02:02:31.716Z",
          "wordCount": 599,
          "title": "Dynamic and Static Object Detection Considering Fusion Regions and Point-wise Features. (arXiv:2107.12692v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cesaire_M/0/1/0/all/0/1\">Manon C&#xe9;saire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajri_H/0/1/0/all/0/1\">Hatem Hajri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1\">Sylvain Lamprier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>",
          "description": "This paper introduces stochastic sparse adversarial attacks (SSAA), simple,\nfast and purely noise-based targeted and untargeted $L_0$ attacks of neural\nnetwork classifiers (NNC). SSAA are devised by exploiting a simple small-time\nexpansion idea widely used for Markov processes and offer new examples of $L_0$\nattacks whose studies have been limited. They are designed to solve the known\nscalability issue of the family of Jacobian-based saliency maps attacks to\nlarge datasets and they succeed in solving it. Experiments on small and large\ndatasets (CIFAR-10 and ImageNet) illustrate further advantages of SSAA in\ncomparison with the-state-of-the-art methods. For instance, in the untargeted\ncase, our method called Voting Folded Gaussian Attack (VFGA) scales efficiently\nto ImageNet and achieves a significantly lower $L_0$ score than SparseFool (up\nto $\\frac{2}{5}$ lower) while being faster. Moreover, VFGA achieves better\n$L_0$ scores on ImageNet than Sparse-RS when both attacks are fully successful\non a large number of samples. Codes are publicly available through the link\nhttps://github.com/SSAA3/stochastic-sparse-adv-attacks",
          "link": "http://arxiv.org/abs/2011.12423",
          "publishedOn": "2021-07-28T02:02:31.697Z",
          "wordCount": 644,
          "title": "Stochastic sparse adversarial attacks. (arXiv:2011.12423v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.02704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1\">Vincent Labatut</a> (LIA), <a href=\"http://arxiv.org/find/cs/1/au:+Bost_X/0/1/0/all/0/1\">Xavier Bost</a> (LIA)",
          "description": "A character network is a graph extracted from a narrative, in which vertices\nrepresent characters and edges correspond to interactions between them. A\nnumber of narrative-related problems can be addressed automatically through the\nanalysis of character networks, such as summarization, classification, or role\ndetection. Character networks are particularly relevant when considering works\nof fictions (e.g. novels, plays, movies, TV series), as their exploitation\nallows developing information retrieval and recommendation systems. However,\nworks of fiction possess specific properties making these tasks harder. This\nsurvey aims at presenting and organizing the scientific literature related to\nthe extraction of character networks from works of fiction, as well as their\nanalysis. We first describe the extraction process in a generic way, and\nexplain how its constituting steps are implemented in practice, depending on\nthe medium of the narrative, the goal of the network analysis, and other\nfactors. We then review the descriptive tools used to characterize character\nnetworks, with a focus on the way they are interpreted in this context. We\nillustrate the relevance of character networks by also providing a review of\napplications derived from their analysis. Finally, we identify the limitations\nof the existing approaches, and the most promising perspectives.",
          "link": "http://arxiv.org/abs/1907.02704",
          "publishedOn": "2021-07-28T02:02:31.671Z",
          "wordCount": 714,
          "title": "Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v4 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhi Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengyi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jianwei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaohui Hu</a>",
          "description": "Continuous sign language recognition (cSLR) is a public significant task that\ntranscribes a sign language video into an ordered gloss sequence. It is\nimportant to capture the fine-grained gloss-level details, since there is no\nexplicit alignment between sign video frames and the corresponding glosses.\nAmong the past works, one promising way is to adopt a one-dimensional\nconvolutional network (1D-CNN) to temporally fuse the sequential frames.\nHowever, CNNs are agnostic to similarity or dissimilarity, and thus are unable\nto capture local consistent semantics within temporally neighboring frames. To\naddress the issue, we propose to adaptively fuse local features via temporal\nsimilarity for this task. Specifically, we devise a Multi-scale Local-Temporal\nSimilarity Fusion Network (mLTSF-Net) as follows: 1) In terms of a specific\nvideo frame, we firstly select its similar neighbours with multi-scale\nreceptive regions to accommodate different lengths of glosses. 2) To ensure\ntemporal consistency, we then use position-aware convolution to temporally\nconvolve each scale of selected frames. 3) To obtain a local-temporally\nenhanced frame-wise representation, we finally fuse the results of different\nscales using a content-dependent aggregator. We train our model in an\nend-to-end fashion, and the experimental results on RWTH-PHOENIX-Weather 2014\ndatasets (RWTH) demonstrate that our model achieves competitive performance\ncompared with several state-of-the-art models.",
          "link": "http://arxiv.org/abs/2107.12762",
          "publishedOn": "2021-07-28T02:02:31.663Z",
          "wordCount": 649,
          "title": "Multi-Scale Local-Temporal Similarity Fusion for Continuous Sign Language Recognition. (arXiv:2107.12762v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.16074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhirui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zekun Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yabang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_A/0/1/0/all/0/1\">Andrew Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>",
          "description": "3D deep learning has been increasingly more popular for a variety of tasks\nincluding many safety-critical applications. However, recently several works\nraise the security issues of 3D deep nets. Although most of these works\nconsider adversarial attacks, we identify that backdoor attack is indeed a more\nserious threat to 3D deep learning systems but remains unexplored. We present\nthe backdoor attacks in 3D with a unified framework that exploits the unique\nproperties of 3D data and networks. In particular, we design two attack\napproaches: the poison-label attack and the clean-label attack. The first one\nis straightforward and effective in practice, while the second one is more\nsophisticated assuming there are certain data inspections. The attack\nalgorithms are mainly motivated and developed by 1) the recent discovery of 3D\nadversarial samples which demonstrate the vulnerability of 3D deep nets under\nspatial transformations; 2) the proposed feature disentanglement technique that\nmanipulates the feature of the data through optimization methods and its\npotential to embed a new task. Extensive experiments show the efficacy of the\npoison-label attack with over 95% success rate across several 3D datasets and\nmodels, and the ability of clean-label attack against data filtering with\naround 50% success rate. Our proposed backdoor attack in 3D point cloud is\nexpected to perform as a baseline for improving the robustness of 3D deep\nmodels.",
          "link": "http://arxiv.org/abs/2103.16074",
          "publishedOn": "2021-07-28T02:02:31.649Z",
          "wordCount": 702,
          "title": "PointBA: Towards Backdoor Attacks in 3D Point Cloud. (arXiv:2103.16074v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_P/0/1/0/all/0/1\">P.S. Sastry</a>",
          "description": "Deep Neural Networks (DNNs) have been shown to be susceptible to memorization\nor overfitting in the presence of noisily labelled data. For the problem of\nrobust learning under such noisy data, several algorithms have been proposed. A\nprominent class of algorithms rely on sample selection strategies, motivated by\ncurriculum learning. For example, many algorithms use the `small loss trick'\nwherein a fraction of samples with loss values below a certain threshold are\nselected for training. These algorithms are sensitive to such thresholds, and\nit is difficult to fix or learn these thresholds. Often, these algorithms also\nrequire information such as label noise rates which are typically unavailable\nin practice. In this paper, we propose a data-dependent, adaptive sample\nselection strategy that relies only on batch statistics of a given mini-batch\nto provide robustness against label noise. The algorithm does not have any\nadditional hyperparameters for sample selection, does not need any information\non noise rates, and does not need access to separate data with clean labels. We\nempirically demonstrate the effectiveness of our algorithm on benchmark\ndatasets.",
          "link": "http://arxiv.org/abs/2106.15292",
          "publishedOn": "2021-07-28T02:02:31.629Z",
          "wordCount": 641,
          "title": "Adaptive Sample Selection for Robust Learning under Label Noise. (arXiv:2106.15292v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12654",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Da-Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>",
          "description": "Traditional learning systems are trained in closed-world for a fixed number\nof classes, and need pre-collected datasets in advance. However, new classes\noften emerge in real-world applications and should be learned incrementally.\nFor example, in electronic commerce, new types of products appear daily, and in\na social media community, new topics emerge frequently. Under such\ncircumstances, incremental models should learn several new classes at a time\nwithout forgetting. We find a strong correlation between old and new classes in\nincremental learning, which can be applied to relate and facilitate different\nlearning stages mutually. As a result, we propose CO-transport for class\nIncremental Learning (COIL), which learns to relate across incremental tasks\nwith the class-wise semantic relationship. In detail, co-transport has two\naspects: prospective transport tries to augment the old classifier with optimal\ntransported knowledge as fast model adaptation. Retrospective transport aims to\ntransport new class classifiers backward as old ones to overcome forgetting.\nWith these transports, COIL efficiently adapts to new tasks, and stably resists\nforgetting. Experiments on benchmark and real-world multimedia datasets\nvalidate the effectiveness of our proposed method.",
          "link": "http://arxiv.org/abs/2107.12654",
          "publishedOn": "2021-07-28T02:02:31.622Z",
          "wordCount": 616,
          "title": "Co-Transport for Class-Incremental Learning. (arXiv:2107.12654v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.02871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ponomarev_E/0/1/0/all/0/1\">Evgeny Ponomarev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matveev_S/0/1/0/all/0/1\">Sergey Matveev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1\">Ivan Oseledets</a>",
          "description": "A lot of deep learning applications are desired to be run on mobile devices.\nBoth accuracy and inference time are meaningful for a lot of them. While the\nnumber of FLOPs is usually used as a proxy for neural network latency, it may\nbe not the best choice. In order to obtain a better approximation of latency,\nresearch community uses look-up tables of all possible layers for latency\ncalculation for the final prediction of the inference on mobile CPU. It\nrequires only a small number of experiments. Unfortunately, on mobile GPU this\nmethod is not applicable in a straight-forward way and shows low precision. In\nthis work, we consider latency approximation on mobile GPU as a data and\nhardware-specific problem. Our main goal is to construct a convenient latency\nestimation tool for investigation(LETI) of neural network inference and\nbuilding robust and accurate latency prediction models for each specific task.\nTo achieve this goal, we build open-source tools which provide a convenient way\nto conduct massive experiments on different target devices focusing on mobile\nGPU. After evaluation of the dataset, we learn the regression model on\nexperimental data and use it for future latency prediction and analysis. We\nexperimentally demonstrate the applicability of such an approach on a subset of\npopular NAS-Benchmark 101 dataset and also evaluate the most popular neural\nnetwork architectures for two mobile GPUs. As a result, we construct latency\nprediction model with good precision on the target evaluation subset. We\nconsider LETI as a useful tool for neural architecture search or massive\nlatency evaluation. The project is available at https://github.com/leti-ai",
          "link": "http://arxiv.org/abs/2010.02871",
          "publishedOn": "2021-07-28T02:02:31.615Z",
          "wordCount": 758,
          "title": "LETI: Latency Estimation Tool and Investigation of Neural Networks inference on Mobile GPU. (arXiv:2010.02871v2 [cs.PF] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noh_B/0/1/0/all/0/1\">Byeongjoon Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hansaem Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1\">Hwasoo Yeo</a>",
          "description": "Traffic accidents are a threat to human lives, particularly pedestrians\ncausing premature deaths. Therefore, it is necessary to devise systems to\nprevent accidents in advance and respond proactively, using potential risky\nsituations as one of the surrogate safety measurements. This study introduces a\nnew concept of a pedestrian safety system that combines the field and the\ncentralized processes. The system can warn of upcoming risks immediately in the\nfield and improve the safety of risk frequent areas by assessing the safety\nlevels of roads without actual collisions. In particular, this study focuses on\nthe latter by introducing a new analytical framework for a crosswalk safety\nassessment with behaviors of vehicle/pedestrian and environmental features. We\nobtain these behavioral features from actual traffic video footage in the city\nwith complete automatic processing. The proposed framework mainly analyzes\nthese behaviors in multidimensional perspectives by constructing a data cube\nstructure, which combines the LSTM based predictive collision risk estimation\nmodel and the on line analytical processing operations. From the PCR estimation\nmodel, we categorize the severity of risks as four levels and apply the\nproposed framework to assess the crosswalk safety with behavioral features. Our\nanalytic experiments are based on two scenarios, and the various descriptive\nresults are harvested the movement patterns of vehicles and pedestrians by road\nenvironment and the relationships between risk levels and car speeds. Thus, the\nproposed framework can support decision makers by providing valuable\ninformation to improve pedestrian safety for future accidents, and it can help\nus better understand their behaviors near crosswalks proactively. In order to\nconfirm the feasibility and applicability of the proposed framework, we\nimplement and apply it to actual operating CCTVs in Osan City, Korea.",
          "link": "http://arxiv.org/abs/2107.12507",
          "publishedOn": "2021-07-28T02:02:31.608Z",
          "wordCount": 734,
          "title": "Analyzing vehicle pedestrian interactions combining data cube structure and predictive collision risk estimation model. (arXiv:2107.12507v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.03449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thai-Son Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stueker_S/0/1/0/all/0/1\">Sebastian Stueker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alex Waibel</a>",
          "description": "Achieving super-human performance in recognizing human speech has been a goal\nfor several decades, as researchers have worked on increasingly challenging\ntasks. In the 1990's it was discovered, that conversational speech between two\nhumans turns out to be considerably more difficult than read speech as\nhesitations, disfluencies, false starts and sloppy articulation complicate\nacoustic processing and require robust handling of acoustic, lexical and\nlanguage context, jointly. Early attempts with statistical models could only\nreach error rates over 50% and far from human performance (WER of around 5.5%).\nNeural hybrid models and recent attention-based encoder-decoder models have\nconsiderably improved performance as such contexts can now be learned in an\nintegral fashion. However, processing such contexts requires an entire\nutterance presentation and thus introduces unwanted delays before a recognition\nresult can be output. In this paper, we address performance as well as latency.\nWe present results for a system that can achieve super-human performance (at a\nWER of 5.0%, over the Switchboard conversational benchmark) at a word based\nlatency of only 1 second behind a speaker's speech. The system uses multiple\nattention-based encoder-decoder networks integrated within a novel low latency\nincremental inference approach.",
          "link": "http://arxiv.org/abs/2010.03449",
          "publishedOn": "2021-07-28T02:02:31.599Z",
          "wordCount": 689,
          "title": "Super-Human Performance in Online Low-latency Recognition of Conversational Speech. (arXiv:2010.03449v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Secci_F/0/1/0/all/0/1\">Francesco Secci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceccarelli_A/0/1/0/all/0/1\">Andrea Ceccarelli</a>",
          "description": "RGB cameras are one of the most relevant sensors for autonomous driving\napplications. It is undeniable that failures of vehicle cameras may compromise\nthe autonomous driving task, possibly leading to unsafe behaviors when images\nthat are subsequently processed by the driving system are altered. To support\nthe definition of safe and robust vehicle architectures and intelligent\nsystems, in this paper we define the failure modes of a vehicle camera,\ntogether with an analysis of effects and known mitigations. Further, we build a\nsoftware library for the generation of the corresponding failed images and we\nfeed them to six object detectors for mono and stereo cameras and to the\nself-driving agent of an autonomous driving simulator. The resulting\nmisbehaviors with respect to operating with clean images allow a better\nunderstanding of failures effects and the related safety risks in image-based\napplications.",
          "link": "http://arxiv.org/abs/2008.05938",
          "publishedOn": "2021-07-28T02:02:31.581Z",
          "wordCount": 611,
          "title": "RGB cameras failures and their effects in autonomous driving applications. (arXiv:2008.05938v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08158",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gadermayr_M/0/1/0/all/0/1\">Michael Gadermayr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tschuchnig_M/0/1/0/all/0/1\">Maximilian Tschuchnig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stangassinger_L/0/1/0/all/0/1\">Lea Maria Stangassinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kreutzer_C/0/1/0/all/0/1\">Christina Kreutzer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Couillard_Despres_S/0/1/0/all/0/1\">Sebastien Couillard-Despres</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oostingh_G/0/1/0/all/0/1\">Gertie Janneke Oostingh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hittmair_A/0/1/0/all/0/1\">Anton Hittmair</a>",
          "description": "In contrast to paraffin sections, frozen sections can be quickly generated\nduring surgical interventions. This procedure allows surgeons to wait for\nhistological findings during the intervention to base intra-operative decisions\non the outcome of the histology. However, compared to paraffin sections, the\nquality of frozen sections is typically lower, leading to a higher ratio of\nmiss-classification. In this work, we investigated the effect of the section\ntype on automated decision support approaches for classification of thyroid\ncancer. This was enabled by a data set consisting of pairs of sections for\nindividual patients. Moreover, we investigated, whether a frozen-to-paraffin\ntranslation could help to optimize classification scores. Finally, we propose a\nspecific data augmentation strategy to deal with a small amount of training\ndata and to increase classification accuracy even further.",
          "link": "http://arxiv.org/abs/2012.08158",
          "publishedOn": "2021-07-28T02:02:31.574Z",
          "wordCount": 622,
          "title": "Frozen-to-Paraffin: Categorization of Histological Frozen Sections by the Aid of Paraffin Sections and Generative Adversarial Networks. (arXiv:2012.08158v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sohee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungkyu Lee</a>",
          "description": "Continual learning is a concept of online learning with multiple sequential\ntasks. One of the critical barriers of continual learning is that a network\nshould learn a new task keeping the knowledge of old tasks without access to\nany data of the old tasks. In this paper, we propose a neuron activation\nimportance-based regularization method for stable continual learning regardless\nof the order of tasks. We conduct comprehensive experiments on existing\nbenchmark data sets to evaluate not just the stability and plasticity of our\nmethod with improved classification accuracy also the robustness of the\nperformance along the changes of task order.",
          "link": "http://arxiv.org/abs/2107.12657",
          "publishedOn": "2021-07-28T02:02:31.567Z",
          "wordCount": 529,
          "title": "Continual Learning with Neuron Activation Importance. (arXiv:2107.12657v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xia Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chunxia Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>",
          "description": "The combination of a small unmanned ground vehicle (UGV) and a large unmanned\ncarrier vehicle allows more flexibility in real applications such as rescue in\ndangerous scenarios. The autonomous recovery system, which is used to guide the\nsmall UGV back to the carrier vehicle, is an essential component to achieve a\nseamless combination of the two vehicles. This paper proposes a novel\nautonomous recovery framework with a low-cost monocular vision system to\nprovide accurate positioning and attitude estimation of the UGV during\nnavigation. First, we introduce a light-weight convolutional neural network\ncalled UGV-KPNet to detect the keypoints of the small UGV from the images\ncaptured by a monocular camera. UGV-KPNet is computationally efficient with a\nsmall number of parameters and provides pixel-level accurate keypoints\ndetection results in real-time. Then, six degrees of freedom pose is estimated\nusing the detected keypoints to obtain positioning and attitude information of\nthe UGV. Besides, we are the first to create a large-scale real-world keypoints\ndataset of the UGV. The experimental results demonstrate that the proposed\nsystem achieves state-of-the-art performance in terms of both accuracy and\nspeed on UGV keypoint detection, and can further boost the 6-DoF pose\nestimation for the UGV.",
          "link": "http://arxiv.org/abs/2107.12852",
          "publishedOn": "2021-07-28T02:02:31.560Z",
          "wordCount": 653,
          "title": "Real-time Keypoints Detection for Autonomous Recovery of the Unmanned Ground Vehicle. (arXiv:2107.12852v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.09405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benny_Y/0/1/0/all/0/1\">Yaniv Benny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pekar_N/0/1/0/all/0/1\">Niv Pekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "We consider the abstract relational reasoning task, which is commonly used as\nan intelligence test. Since some patterns have spatial rationales, while others\nare only semantic, we propose a multi-scale architecture that processes each\nquery in multiple resolutions. We show that indeed different rules are solved\nby different resolutions and a combined multi-scale approach outperforms the\nexisting state of the art in this task on all benchmarks by 5-54%. The success\nof our method is shown to arise from multiple novelties. First, it searches for\nrelational patterns in multiple resolutions, which allows it to readily detect\nvisual relations, such as location, in higher resolution, while allowing the\nlower resolution module to focus on semantic relations, such as shape type.\nSecond, we optimize the reasoning network of each resolution proportionally to\nits performance, hereby we motivate each resolution to specialize on the rules\nfor which it performs better than the others and ignore cases that are already\nsolved by the other resolutions. Third, we propose a new way to pool\ninformation along the rows and the columns of the illustration-grid of the\nquery. Our work also analyses the existing benchmarks, demonstrating that the\nRAVEN dataset selects the negative examples in a way that is easily exploited.\nWe, therefore, propose a modified version of the RAVEN dataset, named\nRAVEN-FAIR. Our code and pretrained models are available at\nhttps://github.com/yanivbenny/MRNet.",
          "link": "http://arxiv.org/abs/2009.09405",
          "publishedOn": "2021-07-28T02:02:31.553Z",
          "wordCount": 689,
          "title": "Scale-Localized Abstract Reasoning. (arXiv:2009.09405v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.02918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhaoyu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Pin Siang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chow_J/0/1/0/all/0/1\">Junkang Chow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jimmy Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheong_Y/0/1/0/all/0/1\">Yehur Cheong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Hsing Wang</a>",
          "description": "3D point cloud interpretation is a challenging task due to the randomness and\nsparsity of the component points. Many of the recently proposed methods like\nPointNet and PointCNN have been focusing on learning shape descriptions from\npoint coordinates as point-wise input features, which usually involves\ncomplicated network architectures. In this work, we draw attention back to the\nstandard 3D convolutions towards an efficient 3D point cloud interpretation.\nInstead of converting the entire point cloud into voxel representations like\nthe other volumetric methods, we voxelize the sub-portions of the point cloud\nonly at necessary locations within each convolution layer on-the-fly, using our\ndynamic voxelization operation with self-adaptive voxelization resolution. In\naddition, we incorporate 3D group convolution into our dense convolution kernel\nimplementation to further exploit the rotation invariant features of point\ncloud. Benefiting from its simple fully-convolutional architecture, our network\nis able to run and converge at a considerably fast speed, while yields on-par\nor even better performance compared with the state-of-the-art methods on\nseveral benchmark datasets.",
          "link": "http://arxiv.org/abs/2009.02918",
          "publishedOn": "2021-07-28T02:02:31.534Z",
          "wordCount": 647,
          "title": "DV-ConvNet: Fully Convolutional Deep Learning on Point Clouds with Dynamic Voxelization and 3D Group Convolution. (arXiv:2009.02918v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sabo_A/0/1/0/all/0/1\">Andrea Sabo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdizadeh_S/0/1/0/all/0/1\">Sina Mehdizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iaboni_A/0/1/0/all/0/1\">Andrea Iaboni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taati_B/0/1/0/all/0/1\">Babak Taati</a>",
          "description": "Drug-induced parkinsonism affects many older adults with dementia, often\ncausing gait disturbances. New advances in vision-based human pose-estimation\nhave opened possibilities for frequent and unobtrusive analysis of gait in\nresidential settings. This work proposes novel spatial-temporal graph\nconvolutional network (ST-GCN) architectures and training procedures to predict\nclinical scores of parkinsonism in gait from video of individuals with\ndementia. We propose a two-stage training approach consisting of a\nself-supervised pretraining stage that encourages the ST-GCN model to learn\nabout gait patterns before predicting clinical scores in the finetuning stage.\nThe proposed ST-GCN models are evaluated on joint trajectories extracted from\nvideo and are compared against traditional (ordinal, linear, random forest)\nregression models and temporal convolutional network baselines. Three 2D human\npose-estimation libraries (OpenPose, Detectron, AlphaPose) and the Microsoft\nKinect (2D and 3D) are used to extract joint trajectories of 4787 natural\nwalking bouts from 53 older adults with dementia. A subset of 399 walks from 14\nparticipants is annotated with scores of parkinsonism severity on the gait\ncriteria of the Unified Parkinson's Disease Rating Scale (UPDRS) and the\nSimpson-Angus Scale (SAS). Our results demonstrate that ST-GCN models operating\non 3D joint trajectories extracted from the Kinect consistently outperform all\nother models and feature sets. Prediction of parkinsonism scores in natural\nwalking bouts of unseen participants remains a challenging task, with the best\nmodels achieving macro-averaged F1-scores of 0.53 +/- 0.03 and 0.40 +/- 0.02\nfor UPDRS-gait and SAS-gait, respectively. Pre-trained model and demo code for\nthis work is available:\nhttps://github.com/TaatiTeam/stgcn_parkinsonism_prediction.",
          "link": "http://arxiv.org/abs/2105.03464",
          "publishedOn": "2021-07-28T02:02:31.527Z",
          "wordCount": 723,
          "title": "Estimating Parkinsonism Severity in Natural Gait Videos of Older Adults with Dementia. (arXiv:2105.03464v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>",
          "description": "Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.",
          "link": "http://arxiv.org/abs/2106.13948",
          "publishedOn": "2021-07-28T02:02:31.518Z",
          "wordCount": 654,
          "title": "Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12666",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zefeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhiyin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Text-to-image person re-identification (ReID) aims to search for images\ncontaining a person of interest using textual descriptions. However, due to the\nsignificant modality gap and the large intra-class variance in textual\ndescriptions, text-to-image ReID remains a challenging problem. Accordingly, in\nthis paper, we propose a Semantically Self-Aligned Network (SSAN) to handle the\nabove problems. First, we propose a novel method that automatically extracts\nsemantically aligned part-level features from the two modalities. Second, we\ndesign a multi-view non-local network that captures the relationships between\nbody parts, thereby establishing better correspondences between body parts and\nnoun phrases. Third, we introduce a Compound Ranking (CR) loss that makes use\nof textual descriptions for other images of the same identity to provide extra\nsupervision, thereby effectively reducing the intra-class variance in textual\nfeatures. Finally, to expedite future research in text-to-image ReID, we build\na new database named ICFG-PEDES. Extensive experiments demonstrate that SSAN\noutperforms state-of-the-art approaches by significant margins. Both the new\nICFG-PEDES database and the SSAN code are available at\nhttps://github.com/zifyloo/SSAN.",
          "link": "http://arxiv.org/abs/2107.12666",
          "publishedOn": "2021-07-28T02:02:31.511Z",
          "wordCount": 616,
          "title": "Semantically Self-Aligned Network for Text-to-Image Part-aware Person Re-identification. (arXiv:2107.12666v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12664",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shi-Xue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaobin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xu-Cheng Yin</a>",
          "description": "Arbitrary shape text detection is a challenging task due to the high\ncomplexity and variety of scene texts. In this work, we propose a novel\nadaptive boundary proposal network for arbitrary shape text detection, which\ncan learn to directly produce accurate boundary for arbitrary shape text\nwithout any post-processing. Our method mainly consists of a boundary proposal\nmodel and an innovative adaptive boundary deformation model. The boundary\nproposal model constructed by multi-layer dilated convolutions is adopted to\nproduce prior information (including classification map, distance field, and\ndirection field) and coarse boundary proposals. The adaptive boundary\ndeformation model is an encoder-decoder network, in which the encoder mainly\nconsists of a Graph Convolutional Network (GCN) and a Recurrent Neural Network\n(RNN). It aims to perform boundary deformation in an iterative way for\nobtaining text instance shape guided by prior information from the boundary\nproposal model.In this way, our method can directly and efficiently generate\naccurate text boundaries without complex post-processing. Extensive experiments\non publicly available datasets demonstrate the state-of-the-art performance of\nour method.",
          "link": "http://arxiv.org/abs/2107.12664",
          "publishedOn": "2021-07-28T02:02:31.489Z",
          "wordCount": 633,
          "title": "Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection. (arXiv:2107.12664v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12746",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qingyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengkai Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>",
          "description": "Localizing individuals in crowds is more in accordance with the practical\ndemands of subsequent high-level crowd analysis tasks than simply counting.\nHowever, existing localization based methods relying on intermediate\nrepresentations (\\textit{i.e.}, density maps or pseudo boxes) serving as\nlearning targets are counter-intuitive and error-prone. In this paper, we\npropose a purely point-based framework for joint crowd counting and individual\nlocalization. For this framework, instead of merely reporting the absolute\ncounting error at image level, we propose a new metric, called density\nNormalized Average Precision (nAP), to provide more comprehensive and more\nprecise performance evaluation. Moreover, we design an intuitive solution under\nthis framework, which is called Point to Point Network (P2PNet). P2PNet\ndiscards superfluous steps and directly predicts a set of point proposals to\nrepresent heads in an image, being consistent with the human annotation\nresults. By thorough analysis, we reveal the key step towards implementing such\na novel idea is to assign optimal learning targets for these proposals.\nTherefore, we propose to conduct this crucial association in an one-to-one\nmatching manner using the Hungarian algorithm. The P2PNet not only\nsignificantly surpasses state-of-the-art methods on popular counting\nbenchmarks, but also achieves promising localization accuracy. The codes will\nbe available at: https://github.com/TencentYoutuResearch/CrowdCounting-P2PNet.",
          "link": "http://arxiv.org/abs/2107.12746",
          "publishedOn": "2021-07-28T02:02:31.462Z",
          "wordCount": 658,
          "title": "Rethinking Counting and Localization in Crowds:A Purely Point-Based Framework. (arXiv:2107.12746v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Daping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiayu Cao</a>",
          "description": "Agriculture is an essential industry in the both society and economy of a\ncountry. However, the pests and diseases cause a great amount of reduction in\nagricultural production while there is not sufficient guidance for farmers to\navoid this disaster. To address this problem, we apply CNNs to plant disease\nrecognition by building a classification model. Within the dataset of 3,642\nimages of apple leaves, We use a pre-trained image classification model\nRestnet34 based on a Convolutional neural network (CNN) with the Fastai\nframework in order to save the training time. Overall, the accuracy of\nclassification is 93.765%.",
          "link": "http://arxiv.org/abs/2107.12598",
          "publishedOn": "2021-07-28T02:02:31.454Z",
          "wordCount": 535,
          "title": "Identify Apple Leaf Diseases Using Deep Learning Algorithm. (arXiv:2107.12598v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gudovskiy_D/0/1/0/all/0/1\">Denis Gudovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishizaka_S/0/1/0/all/0/1\">Shun Ishizaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozuka_K/0/1/0/all/0/1\">Kazuki Kozuka</a>",
          "description": "Unsupervised anomaly detection with localization has many practical\napplications when labeling is infeasible and, moreover, when anomaly examples\nare completely missing in the train data. While recently proposed models for\nsuch data setup achieve high accuracy metrics, their complexity is a limiting\nfactor for real-time processing. In this paper, we propose a real-time model\nand analytically derive its relationship to prior methods. Our CFLOW-AD model\nis based on a conditional normalizing flow framework adopted for anomaly\ndetection with localization. In particular, CFLOW-AD consists of a\ndiscriminatively pretrained encoder followed by a multi-scale generative\ndecoders where the latter explicitly estimate likelihood of the encoded\nfeatures. Our approach results in a computationally and memory-efficient model:\nCFLOW-AD is faster and smaller by a factor of 10x than prior state-of-the-art\nwith the same input setting. Our experiments on the MVTec dataset show that\nCFLOW-AD outperforms previous methods by 0.36% AUROC in detection task, by\n1.12% AUROC and 2.5% AUPRO in localization task, respectively. We open-source\nour code with fully reproducible experiments.",
          "link": "http://arxiv.org/abs/2107.12571",
          "publishedOn": "2021-07-28T02:02:31.447Z",
          "wordCount": 620,
          "title": "CFLOW-AD: Real-Time Unsupervised Anomaly Detection with Localization via Conditional Normalizing Flows. (arXiv:2107.12571v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zixin Zhu</a> (Xi&#x27;an jiaotong University), <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wei Tang</a> (University of Illinois at Chicago), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a> (Xi&#x27;an Jiaotong University), <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a> (Xi&#x27;an Jiaotong University), <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a> (Wormpex AI Research)",
          "description": "Effectively tackling the problem of temporal action localization (TAL)\nnecessitates a visual representation that jointly pursues two confounding\ngoals, i.e., fine-grained discrimination for temporal localization and\nsufficient visual invariance for action classification. We address this\nchallenge by enriching both the local and global contexts in the popular\ntwo-stage temporal localization framework, where action proposals are first\ngenerated followed by action classification and temporal boundary regression.\nOur proposed model, dubbed ContextLoc, can be divided into three sub-networks:\nL-Net, G-Net and P-Net. L-Net enriches the local context via fine-grained\nmodeling of snippet-level features, which is formulated as a\nquery-and-retrieval process. G-Net enriches the global context via higher-level\nmodeling of the video-level representation. In addition, we introduce a novel\ncontext adaptation module to adapt the global context to different proposals.\nP-Net further models the context-aware inter-proposal relations. We explore two\nexisting models to be the P-Net in our experiments. The efficacy of our\nproposed method is validated by experimental results on the THUMOS14 (54.3\\% at\nIoU@0.5) and ActivityNet v1.3 (51.24\\% at IoU@0.5) datasets, which outperforms\nrecent states of the art.",
          "link": "http://arxiv.org/abs/2107.12960",
          "publishedOn": "2021-07-28T02:02:31.433Z",
          "wordCount": 639,
          "title": "Enriching Local and Global Contexts for Temporal Action Localization. (arXiv:2107.12960v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Riqiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mirza S. Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yucheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaiwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deppen_S/0/1/0/all/0/1\">Steve Deppen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandler_K/0/1/0/all/0/1\">Kim L. Sandler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massion_P/0/1/0/all/0/1\">Pierre P. Massion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>",
          "description": "Image Quality Assessment (IQA) is important for scientific inquiry,\nespecially in medical imaging and machine learning. Potential data quality\nissues can be exacerbated when human-based workflows use limited views of the\ndata that may obscure digital artifacts. In practice, multiple factors such as\nnetwork issues, accelerated acquisitions, motion artifacts, and imaging\nprotocol design can impede the interpretation of image collections. The medical\nimage processing community has developed a wide variety of tools for the\ninspection and validation of imaging data. Yet, IQA of computed tomography (CT)\nremains an under-recognized challenge, and no user-friendly tool is commonly\navailable to address these potential issues. Here, we create and illustrate a\npipeline specifically designed to identify and resolve issues encountered with\nlarge-scale data mining of clinically acquired CT data. Using the widely\nstudied National Lung Screening Trial (NLST), we have identified approximately\n4% of image volumes with quality concerns out of 17,392 scans. To assess\nrobustness, we applied the proposed pipeline to our internal datasets where we\nfind our tool is generalizable to clinically acquired medical images. In\nconclusion, the tool has been useful and time-saving for research study of\nclinical data, and the code and tutorials are publicly available at\nhttps://github.com/MASILab/QA_tool.",
          "link": "http://arxiv.org/abs/2107.12842",
          "publishedOn": "2021-07-28T02:02:31.423Z",
          "wordCount": 662,
          "title": "Technical Report: Quality Assessment Tool for Machine Learning with Clinical CT. (arXiv:2107.12842v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sungmin Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dogyoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junhyeop Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sangwon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Woojin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>",
          "description": "Despite the remarkable success of deep learning, optimal convolution\noperation on point cloud remains indefinite due to its irregular data\nstructure. In this paper, we present Cubic Kernel Convolution (CKConv) that\nlearns to voxelize the features of local points by exploiting both continuous\nand discrete convolutions. Our continuous convolution uniquely employs a 3D\ncubic form of kernel weight representation that splits a feature into voxels in\nembedding space. By consecutively applying discrete 3D convolutions on the\nvoxelized features in a spatial manner, preceding continuous convolution is\nforced to learn spatial feature mapping, i.e., feature voxelization. In this\nway, geometric information can be detailed by encoding with subdivided\nfeatures, and our 3D convolutions on these fixed structured data do not suffer\nfrom discretization artifacts thanks to voxelization in embedding space.\nFurthermore, we propose a spatial attention module, Local Set Attention (LSA),\nto provide comprehensive structure awareness within the local point set and\nhence produce representative features. By learning feature voxelization with\nLSA, CKConv can extract enriched features for effective point cloud analysis.\nWe show that CKConv has great applicability to point cloud processing tasks\nincluding object classification, object part segmentation, and scene semantic\nsegmentation with state-of-the-art results.",
          "link": "http://arxiv.org/abs/2107.12655",
          "publishedOn": "2021-07-28T02:02:31.412Z",
          "wordCount": 636,
          "title": "CKConv: Learning Feature Voxelization for Point Cloud Analysis. (arXiv:2107.12655v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xompero_A/0/1/0/all/0/1\">Alessio Xompero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donaher_S/0/1/0/all/0/1\">Santiago Donaher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iashin_V/0/1/0/all/0/1\">Vladimir Iashin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palermo_F/0/1/0/all/0/1\">Francesca Palermo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solak_G/0/1/0/all/0/1\">G&#xf6;khan Solak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coppola_C/0/1/0/all/0/1\">Claudio Coppola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_R/0/1/0/all/0/1\">Reina Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagao_Y/0/1/0/all/0/1\">Yuichi Nagao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hachiuma_R/0/1/0/all/0/1\">Ryo Hachiuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Chuanlin Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">Rosa H. M. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christmann_G/0/1/0/all/0/1\">Guilherme Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jyun-Ting Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neeharika_G/0/1/0/all/0/1\">Gonuguntla Neeharika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chinnakotla Krishna Teja Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1\">Dinesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehman_B/0/1/0/all/0/1\">Bakhtawar Ur Rehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1\">Andrea Cavallaro</a>",
          "description": "Acoustic and visual sensing can support the contactless estimation of the\nweight of a container and the amount of its content when the container is\nmanipulated by a person. However, transparencies (both of the container and of\nthe content) and the variability of materials, shapes and sizes make this\nproblem challenging. In this paper, we present an open benchmarking framework\nand an in-depth comparative analysis of recent methods that estimate the\ncapacity of a container, as well as the type, mass, and amount of its content.\nThese methods use learned and handcrafted features, such as mel-frequency\ncepstrum coefficients, zero-crossing rate, spectrograms, with different types\nof classifiers to estimate the type and amount of the content with acoustic\ndata, and geometric approaches with visual data to determine the capacity of\nthe container. Results on a newly distributed dataset show that audio alone is\na strong modality and methods achieves a weighted average F1-score up to 81%\nand 97% for content type and level classification, respectively. Estimating the\ncontainer capacity with vision-only approaches and filling mass with\nmulti-modal, multi-stage algorithms reaches up to 65% weighted average capacity\nand mass scores.",
          "link": "http://arxiv.org/abs/2107.12719",
          "publishedOn": "2021-07-28T02:02:31.386Z",
          "wordCount": 692,
          "title": "Multi-modal estimation of the properties of containers and their content: survey and evaluation. (arXiv:2107.12719v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiangxi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>",
          "description": "Image manipulation with natural language, which aims to manipulate images\nwith the guidance of language descriptions, has been a challenging problem in\nthe fields of computer vision and natural language processing (NLP). Currently,\na number of efforts have been made for this task, but their performances are\nstill distant away from generating realistic and text-conformed manipulated\nimages. Therefore, in this paper, we propose a memory-based Image Manipulation\nNetwork (MIM-Net), where a set of memories learned from images is introduced to\nsynthesize the texture information with the guidance of the textual\ndescription. We propose a two-stage network with an additional reconstruction\nstage to learn the latent memories efficiently. To avoid the unnecessary\nbackground changes, we propose a Target Localization Unit (TLU) to focus on the\nmanipulation of the region mentioned by the text. Moreover, to learn a robust\nmemory, we further propose a novel randomized memory training loss. Experiments\non the four popular datasets show the better performance of our method compared\nto the existing ones.",
          "link": "http://arxiv.org/abs/2107.12579",
          "publishedOn": "2021-07-28T02:02:31.377Z",
          "wordCount": 607,
          "title": "Remember What You have drawn: Semantic Image Manipulation with Memory. (arXiv:2107.12579v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yezhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_T/0/1/0/all/0/1\">Tong Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>",
          "description": "Confidence calibration is of great importance to the reliability of decisions\nmade by machine learning systems. However, discriminative classifiers based on\ndeep neural networks are often criticized for producing overconfident\npredictions that fail to reflect the true correctness likelihood of\nclassification accuracy. We argue that such an inability to model uncertainty\nis mainly caused by the closed-world nature in softmax: a model trained by the\ncross-entropy loss will be forced to classify input into one of $K$ pre-defined\ncategories with high probability. To address this problem, we for the first\ntime propose a novel $K$+1-way softmax formulation, which incorporates the\nmodeling of open-world uncertainty as the extra dimension. To unify the\nlearning of the original $K$-way classification task and the extra dimension\nthat models uncertainty, we propose a novel energy-based objective function,\nand moreover, theoretically prove that optimizing such an objective essentially\nforces the extra dimension to capture the marginal data distribution. Extensive\nexperiments show that our approach, Energy-based Open-World Softmax\n(EOW-Softmax), is superior to existing state-of-the-art methods in improving\nconfidence calibration.",
          "link": "http://arxiv.org/abs/2107.12628",
          "publishedOn": "2021-07-28T02:02:31.369Z",
          "wordCount": 614,
          "title": "Energy-Based Open-World Uncertainty Modeling for Confidence Calibration. (arXiv:2107.12628v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huyan_N/0/1/0/all/0/1\">Ning Huyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_D/0/1/0/all/0/1\">Dou Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xuefeng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Licheng Jiao</a>",
          "description": "Outlier detection is one of the most important processes taken to create\ngood, reliable data in machine learning. The most methods of outlier detection\nleverage an auxiliary reconstruction task by assuming that outliers are more\ndifficult to be recovered than normal samples (inliers). However, it is not\nalways true, especially for auto-encoder (AE) based models. They may recover\ncertain outliers even outliers are not in the training data, because they do\nnot constrain the feature learning. Instead, we think outlier detection can be\ndone in the feature space by measuring the feature distance between outliers\nand inliers. We then propose a framework, MCOD, using a memory module and a\ncontrastive learning module. The memory module constrains the consistency of\nfeatures, which represent the normal data. The contrastive learning module\nlearns more discriminating features, which boosts the distinction between\noutliers and inliers. Extensive experiments on four benchmark datasets show\nthat our proposed MCOD achieves a considerable performance and outperforms nine\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.12642",
          "publishedOn": "2021-07-28T02:02:31.350Z",
          "wordCount": 601,
          "title": "Unsupervised Outlier Detection using Memory and Contrastive Learning. (arXiv:2107.12642v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12636",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Fengxiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Detection transformers have recently shown promising object detection results\nand attracted increasing attention. However, how to develop effective domain\nadaptation techniques to improve its cross-domain performance remains\nunexplored and unclear. In this paper, we delve into this topic and empirically\nfind that direct feature distribution alignment on the CNN backbone only brings\nlimited improvements, as it does not guarantee domain-invariant sequence\nfeatures in the transformer for prediction. To address this issue, we propose a\nnovel Sequence Feature Alignment (SFA) method that is specially designed for\nthe adaptation of detection transformers. Technically, SFA consists of a domain\nquery-based feature alignment (DQFA) module and a token-wise feature alignment\n(TDA) module. In DQFA, a novel domain query is used to aggregate and align\nglobal context from the token sequence of both domains. DQFA reduces the domain\ndiscrepancy in global feature representations and object relations when\ndeploying in the transformer encoder and decoder, respectively. Meanwhile, TDA\naligns token features in the sequence from both domains, which reduces the\ndomain gaps in local and instance-level feature representations in the\ntransformer encoder and decoder, respectively. Besides, a novel bipartite\nmatching consistency loss is proposed to enhance the feature discriminability\nfor robust object detection. Experiments on three challenging benchmarks show\nthat SFA outperforms state-of-the-art domain adaptive object detection methods.\nCode has been made available at: https://github.com/encounter1997/SFA.",
          "link": "http://arxiv.org/abs/2107.12636",
          "publishedOn": "2021-07-28T02:02:31.341Z",
          "wordCount": 693,
          "title": "Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Turkoz_E/0/1/0/all/0/1\">Erkin T&#xfc;rk&#xf6;z</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olcay_E/0/1/0/all/0/1\">Ertug Olcay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oksanen_T/0/1/0/all/0/1\">Timo Oksanen</a>",
          "description": "This paper proposes a concept of computer vision-based guidance assistance\nfor agricultural vehicles to increase the accuracy in plowing and reduce\ndriver's cognitive burden in long-lasting tillage operations. Plowing is a\ncommon agricultural practice to prepare the soil for planting in many countries\nand it can take place both in the spring and the fall. Since plowing operation\nrequires high traction forces, it causes increased energy consumption.\nMoreover, longer operation time due to unnecessary maneuvers leads to higher\nfuel consumption. To provide necessary information for the driver and the\ncontrol unit of the tractor, a first concept of furrow detection system based\non an RGB-D camera was developed.",
          "link": "http://arxiv.org/abs/2107.12646",
          "publishedOn": "2021-07-28T02:02:31.318Z",
          "wordCount": 566,
          "title": "Computer Vision-Based Guidance Assistance Concept for Plowing Using RGB-D Camera. (arXiv:2107.12646v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kompella_R/0/1/0/all/0/1\">Ramana Kompella</a>",
          "description": "Deep Neural Network (DNN) trained object detectors are widely deployed in\nmany mission-critical systems for real time video analytics at the edge, such\nas autonomous driving and video surveillance. A common performance requirement\nin these mission-critical edge services is the near real-time latency of online\nobject detection on edge devices. However, even with well-trained DNN object\ndetectors, the online detection quality at edge may deteriorate for a number of\nreasons, such as limited capacity to run DNN object detection models on\nheterogeneous edge devices, and detection quality degradation due to random\nframe dropping when the detection processing rate is significantly slower than\nthe incoming video frame rate. This paper addresses these problems by\nexploiting multi-model multi-device detection parallelism for fast object\ndetection in edge systems with heterogeneous edge devices. First, we analyze\nthe performance bottleneck of running a well-trained DNN model at edge for real\ntime online object detection. We use the offline detection as a reference\nmodel, and examine the root cause by analyzing the mismatch among the incoming\nvideo streaming rate, video processing rate for object detection, and output\nrate for real time detection visualization of video streaming. Second, we study\nperformance optimizations by exploiting multi-model detection parallelism. We\nshow that the model-parallel detection approach can effectively speed up the\nFPS detection processing rate, minimizing the FPS disparity with the incoming\nvideo frame rate on heterogeneous edge devices. We evaluate the proposed\napproach using SSD300 and YOLOv3 on benchmark videos of different video stream\nrates. The results show that exploiting multi-model detection parallelism can\nspeed up the online object detection processing rate and deliver near real-time\nobject detection performance for efficient video analytics at edge.",
          "link": "http://arxiv.org/abs/2107.12563",
          "publishedOn": "2021-07-28T02:02:31.304Z",
          "wordCount": 720,
          "title": "Parallel Detection for Efficient Video Analytics at the Edge. (arXiv:2107.12563v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1\">Qi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_R/0/1/0/all/0/1\">Runmin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_R/0/1/0/all/0/1\">Ronghui Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lingzhi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>",
          "description": "Depth map super-resolution is a task with high practical application\nrequirements in the industry. Existing color-guided depth map super-resolution\nmethods usually necessitate an extra branch to extract high-frequency detail\ninformation from RGB image to guide the low-resolution depth map\nreconstruction. However, because there are still some differences between the\ntwo modalities, direct information transmission in the feature dimension or\nedge map dimension cannot achieve satisfactory result, and may even trigger\ntexture copying in areas where the structures of the RGB-D pair are\ninconsistent. Inspired by the multi-task learning, we propose a joint learning\nnetwork of depth map super-resolution (DSR) and monocular depth estimation\n(MDE) without introducing additional supervision labels. For the interaction of\ntwo subnetworks, we adopt a differentiated guidance strategy and design two\nbridges correspondingly. One is the high-frequency attention bridge (HABdg)\ndesigned for the feature encoding process, which learns the high-frequency\ninformation of the MDE task to guide the DSR task. The other is the content\nguidance bridge (CGBdg) designed for the depth map reconstruction process,\nwhich provides the content guidance learned from DSR task for MDE task. The\nentire network architecture is highly portable and can provide a paradigm for\nassociating the DSR and MDE tasks. Extensive experiments on benchmark datasets\ndemonstrate that our method achieves competitive performance. Our code and\nmodels are available at https://rmcong.github.io/proj_BridgeNet.html.",
          "link": "http://arxiv.org/abs/2107.12541",
          "publishedOn": "2021-07-28T02:02:31.295Z",
          "wordCount": 682,
          "title": "BridgeNet: A Joint Learning Network of Depth Map Super-Resolution and Monocular Depth Estimation. (arXiv:2107.12541v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Song Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhiyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrich_N/0/1/0/all/0/1\">Norman Hendrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1\">Fanyu Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shuzhi Sam Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianwei Zhang</a>",
          "description": "In the classic setting of unsupervised domain adaptation (UDA), the labeled\nsource data are available in the training phase. However, in many real-world\nscenarios, owing to some reasons such as privacy protection and information\nsecurity, the source data is inaccessible, and only a model trained on the\nsource domain is available. This paper proposes a novel deep clustering method\nfor this challenging task. Aiming at the dynamical clustering at feature-level,\nwe introduce extra constraints hidden in the geometric structure between data\nto assist the process. Concretely, we propose a geometry-based constraint,\nnamed semantic consistency on the nearest neighborhood (SCNNH), and use it to\nencourage robust clustering. To reach this goal, we construct the nearest\nneighborhood for every target data and take it as the fundamental clustering\nunit by building our objective on the geometry. Also, we develop a more\nSCNNH-compliant structure with an additional semantic credibility constraint,\nnamed semantic hyper-nearest neighborhood (SHNNH). After that, we extend our\nmethod to this new geometry. Extensive experiments on three challenging UDA\ndatasets indicate that our method achieves state-of-the-art results. The\nproposed method has significant improvement on all datasets (as we adopt SHNNH,\nthe average accuracy increases by over 3.0\\% on the large-scaled dataset). Code\nis available at https://github.com/tntek/N2DCX.",
          "link": "http://arxiv.org/abs/2107.12585",
          "publishedOn": "2021-07-28T02:02:31.288Z",
          "wordCount": 660,
          "title": "Nearest Neighborhood-Based Deep Clustering for Source Data-absent Unsupervised Domain Adaptation. (arXiv:2107.12585v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12560",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jinchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xian Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junnan Liu</a>",
          "description": "Effective fusion of different types of features is the key to salient object\ndetection. The majority of existing network structure design is based on the\nsubjective experience of scholars and the process of feature fusion does not\nconsider the relationship between the fused features and highest-level\nfeatures. In this paper, we focus on the feature relationship and propose a\nnovel global attention unit, which we term the \"perception- and-regulation\"\n(PR) block, that adaptively regulates the feature fusion process by explicitly\nmodeling interdependencies between features. The perception part uses the\nstructure of fully-connected layers in classification networks to learn the\nsize and shape of objects. The regulation part selectively strengthens and\nweakens the features to be fused. An imitating eye observation module (IEO) is\nfurther employed for improving the global perception ability of the network.\nThe imitation of foveal vision and peripheral vision enables IEO to scrutinize\nhighly detailed objects and to organize the broad spatial scene to better\nsegment objects. Sufficient experiments conducted on SOD datasets demonstrate\nthat the proposed method performs favorably against 22 state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2107.12560",
          "publishedOn": "2021-07-28T02:02:31.238Z",
          "wordCount": 609,
          "title": "Perception-and-Regulation Network for Salient Object Detection. (arXiv:2107.12560v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Santamaria_Pang_A/0/1/0/all/0/1\">Alberto Santamaria-Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jianwei Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1\">Aritra Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubricht_J/0/1/0/all/0/1\">James Kubricht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_P/0/1/0/all/0/1\">Peter Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naresh_I/0/1/0/all/0/1\">Iyer Naresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virani_N/0/1/0/all/0/1\">Nurali Virani</a>",
          "description": "We propose a novel framework for real-time black-box universal attacks which\ndisrupts activations of early convolutional layers in deep learning models. Our\nhypothesis is that perturbations produced in the wavelet space disrupt early\nconvolutional layers more effectively than perturbations performed in the time\ndomain. The main challenge in adversarial attacks is to preserve low frequency\nimage content while minimally changing the most meaningful high frequency\ncontent. To address this, we formulate an optimization problem using time-scale\n(wavelet) representations as a dual space in three steps. First, we project\noriginal images into orthonormal sub-spaces for low and high scales via wavelet\ncoefficients. Second, we perturb wavelet coefficients for high scale projection\nusing a generator network. Third, we generate new adversarial images by\nprojecting back the original coefficients from the low scale and the perturbed\ncoefficients from the high scale sub-space. We provide a theoretical framework\nthat guarantees a dual mapping from time and time-scale domain representations.\nWe compare our results with state-of-the-art black-box attacks from\ngenerative-based and gradient-based models. We also verify efficacy against\nmultiple defense methods such as JPEG compression, Guided Denoiser and\nComdefend. Our results show that wavelet-based perturbations consistently\noutperform time-based attacks thus providing new insights into vulnerabilities\nof deep learning models and could potentially lead to robust architectures or\nnew defense and attack mechanisms by leveraging time-scale representations.",
          "link": "http://arxiv.org/abs/2107.12473",
          "publishedOn": "2021-07-28T02:02:31.230Z",
          "wordCount": 665,
          "title": "Adversarial Attacks with Time-Scale Representations. (arXiv:2107.12473v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Runze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1\">Bir Bhanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>",
          "description": "Self-supervised depth estimation for indoor environments is more challenging\nthan its outdoor counterpart in at least the following two aspects: (i) the\ndepth range of indoor sequences varies a lot across different frames, making it\ndifficult for the depth network to induce consistent depth cues, whereas the\nmaximum distance in outdoor scenes mostly stays the same as the camera usually\nsees the sky; (ii) the indoor sequences contain much more rotational motions,\nwhich cause difficulties for the pose network, while the motions of outdoor\nsequences are pre-dominantly translational, especially for driving datasets\nsuch as KITTI. In this paper, special considerations are given to those\nchallenges and a set of good practices are consolidated for improving the\nperformance of self-supervised monocular depth estimation in indoor\nenvironments. The proposed method mainly consists of two novel modules, \\ie, a\ndepth factorization module and a residual pose estimation module, each of which\nis designed to respectively tackle the aforementioned challenges. The\neffectiveness of each module is shown through a carefully conducted ablation\nstudy and the demonstration of the state-of-the-art performance on two indoor\ndatasets, \\ie, EuRoC and NYUv2.",
          "link": "http://arxiv.org/abs/2107.12429",
          "publishedOn": "2021-07-28T02:02:31.221Z",
          "wordCount": 632,
          "title": "MonoIndoor: Towards Good Practice of Self-Supervised Monocular Depth Estimation for Indoor Environments. (arXiv:2107.12429v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miao_B/0/1/0/all/0/1\">Bo Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>",
          "description": "We propose a self-supervised spatio-temporal matching method coined\nMotion-Aware Mask Propagation (MAMP) for semi-supervised video object\nsegmentation. During training, MAMP leverages the frame reconstruction task to\ntrain the model without the need for annotations. During inference, MAMP\nextracts high-resolution features from each frame to build a memory bank from\nthe features as well as the predicted masks of selected past frames. MAMP then\npropagates the masks from the memory bank to subsequent frames according to our\nmotion-aware spatio-temporal matching module, also proposed in this paper.\nEvaluation on DAVIS-2017 and YouTube-VOS datasets show that MAMP achieves\nstate-of-the-art performance with stronger generalization ability compared to\nexisting self-supervised methods, i.e. 4.9\\% higher mean\n$\\mathcal{J}\\&\\mathcal{F}$ on DAVIS-2017 and 4.85\\% higher mean\n$\\mathcal{J}\\&\\mathcal{F}$ on the unseen categories of YouTube-VOS than the\nnearest competitor. Moreover, MAMP performs on par with many supervised video\nobject segmentation methods. Our code is available at:\n\\url{https://github.com/bo-miao/MAMP}.",
          "link": "http://arxiv.org/abs/2107.12569",
          "publishedOn": "2021-07-28T02:02:31.204Z",
          "wordCount": 588,
          "title": "Self-Supervised Video Object Segmentation by Motion-Aware Mask Propagation. (arXiv:2107.12569v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azari_B/0/1/0/all/0/1\">Bahar Azari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdogmus_D/0/1/0/all/0/1\">Deniz Erdogmus</a>",
          "description": "Despite the vast success of standard planar convolutional neural networks,\nthey are not the most efficient choice for analyzing signals that lie on an\narbitrarily curved manifold, such as a cylinder. The problem arises when one\nperforms a planar projection of these signals and inevitably causes them to be\ndistorted or broken where there is valuable information. We propose a\nCircular-symmetric Correlation Layer (CCL) based on the formalism of\nroto-translation equivariant correlation on the continuous group $S^1 \\times\n\\mathbb{R}$, and implement it efficiently using the well-known Fast Fourier\nTransform (FFT) algorithm. We showcase the performance analysis of a general\nnetwork equipped with CCL on various recognition and classification tasks and\ndatasets. The PyTorch package implementation of CCL is provided online.",
          "link": "http://arxiv.org/abs/2107.12480",
          "publishedOn": "2021-07-28T02:02:30.923Z",
          "wordCount": 552,
          "title": "Circular-Symmetric Correlation Layer based on FFT. (arXiv:2107.12480v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1\">Mohit Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "Seemingly simple natural language requests to a robot are generally\nunderspecified, for example \"Can you bring me the wireless mouse?\" When viewing\nmice on the shelf, the number of buttons or presence of a wire may not be\nvisible from certain angles or positions. Flat images of candidate mice may not\nprovide the discriminative information needed for \"wireless\". The world, and\nobjects in it, are not flat images but complex 3D shapes. If a human requests\nan object based on any of its basic properties, such as color, shape, or\ntexture, robots should perform the necessary exploration to accomplish the\ntask. In particular, while substantial effort and progress has been made on\nunderstanding explicitly visual attributes like color and category,\ncomparatively little progress has been made on understanding language about\nshapes and contours. In this work, we introduce a novel reasoning task that\ntargets both visual and non-visual language about 3D objects. Our new\nbenchmark, ShapeNet Annotated with Referring Expressions (SNARE), requires a\nmodel to choose which of two objects is being referenced by a natural language\ndescription. We introduce several CLIP-based models for distinguishing objects\nand demonstrate that while recent advances in jointly modeling vision and\nlanguage are useful for robotic language understanding, it is still the case\nthat these models are weaker at understanding the 3D nature of objects --\nproperties which play a key role in manipulation. In particular, we find that\nadding view estimation to language grounding models improves accuracy on both\nSNARE and when identifying objects referred to in language on a robot platform.",
          "link": "http://arxiv.org/abs/2107.12514",
          "publishedOn": "2021-07-28T02:02:30.915Z",
          "wordCount": 709,
          "title": "Language Grounding with 3D Objects. (arXiv:2107.12514v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Haisheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_P/0/1/0/all/0/1\">Peiqin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yukun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Weihao Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>",
          "description": "This technical report presents an overview of our solution used in the\nsubmission to 2021 HACS Temporal Action Localization Challenge on both\nSupervised Learning Track and Weakly-Supervised Learning Track. Temporal Action\nLocalization (TAL) requires to not only precisely locate the temporal\nboundaries of action instances, but also accurately classify the untrimmed\nvideos into specific categories. However, Weakly-Supervised TAL indicates\nlocating the action instances using only video-level class labels. In this\npaper, to train a supervised temporal action localizer, we adopt Temporal\nContext Aggregation Network (TCANet) to generate high-quality action proposals\nthrough ``local and global\" temporal context aggregation and complementary as\nwell as progressive boundary refinement. As for the WSTAL, a novel framework is\nproposed to handle the poor quality of CAS generated by simple classification\nnetwork, which can only focus on local discriminative parts, rather than locate\nthe entire interval of target actions. Further inspired by the transfer\nlearning method, we also adopt an additional module to transfer the knowledge\nfrom trimmed videos (HACS Clips dataset) to untrimmed videos (HACS Segments\ndataset), aiming at promoting the classification performance on untrimmed\nvideos. Finally, we employ a boundary regression module embedded with\nOuter-Inner-Contrastive (OIC) loss to automatically predict the boundaries\nbased on the enhanced CAS. Our proposed scheme achieves 39.91 and 29.78 average\nmAP on the challenge testing set of supervised and weakly-supervised temporal\naction localization track respectively.",
          "link": "http://arxiv.org/abs/2107.12618",
          "publishedOn": "2021-07-28T02:02:30.805Z",
          "wordCount": 699,
          "title": "Transferable Knowledge-Based Multi-Granularity Aggregation Network for Temporal Action Localization: Submission to ActivityNet Challenge 2021. (arXiv:2107.12618v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yilin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1\">Taku Komura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>",
          "description": "6D pose estimation of rigid objects from a single RGB image has seen\ntremendous improvements recently by using deep learning to combat complex\nreal-world variations, but a majority of methods build models on the per-object\nlevel, failing to scale to multiple objects simultaneously. In this paper, we\npresent a novel approach for scalable 6D pose estimation, by self-supervised\nlearning on synthetic data of multiple objects using a single autoencoder. To\nhandle multiple objects and generalize to unseen objects, we disentangle the\nlatent object shape and pose representations, so that the latent shape space\nmodels shape similarities, and the latent pose code is used for rotation\nretrieval by comparison with canonical rotations. To encourage shape space\nconstruction, we apply contrastive metric learning and enable the processing of\nunseen objects by referring to similar training objects. The different\nsymmetries across objects induce inconsistent latent pose spaces, which we\ncapture with a conditioned block producing shape-dependent pose codebooks by\nre-entangling shape and pose representations. We test our method on two\nmulti-object benchmarks with real data, T-LESS and NOCS REAL275, and show it\noutperforms existing RGB-based methods in terms of pose estimation accuracy and\ngeneralization.",
          "link": "http://arxiv.org/abs/2107.12549",
          "publishedOn": "2021-07-28T02:02:30.796Z",
          "wordCount": 639,
          "title": "Disentangled Implicit Shape and Pose Learning for Scalable 6D Pose Estimation. (arXiv:2107.12549v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Miao Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1\">Yang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Siyu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bo Yuan</a>",
          "description": "Advanced tensor decomposition, such as Tensor train (TT) and Tensor ring\n(TR), has been widely studied for deep neural network (DNN) model compression,\nespecially for recurrent neural networks (RNNs). However, compressing\nconvolutional neural networks (CNNs) using TT/TR always suffers significant\naccuracy loss. In this paper, we propose a systematic framework for tensor\ndecomposition-based model compression using Alternating Direction Method of\nMultipliers (ADMM). By formulating TT decomposition-based model compression to\nan optimization problem with constraints on tensor ranks, we leverage ADMM\ntechnique to systemically solve this optimization problem in an iterative way.\nDuring this procedure, the entire DNN model is trained in the original\nstructure instead of TT format, but gradually enjoys the desired low tensor\nrank characteristics. We then decompose this uncompressed model to TT format\nand fine-tune it to finally obtain a high-accuracy TT-format DNN model. Our\nframework is very general, and it works for both CNNs and RNNs, and can be\neasily modified to fit other tensor decomposition approaches. We evaluate our\nproposed framework on different DNN models for image classification and video\nrecognition tasks. Experimental results show that our ADMM-based TT-format\nmodels demonstrate very high compression performance with high accuracy.\nNotably, on CIFAR-100, with 2.3X and 2.4X compression ratios, our models have\n1.96% and 2.21% higher top-1 accuracy than the original ResNet-20 and\nResNet-32, respectively. For compressing ResNet-18 on ImageNet, our model\nachieves 2.47X FLOPs reduction without accuracy loss.",
          "link": "http://arxiv.org/abs/2107.12422",
          "publishedOn": "2021-07-28T02:02:30.772Z",
          "wordCount": 682,
          "title": "Towards Efficient Tensor Decomposition-Based DNN Model Compression with Optimization Framework. (arXiv:2107.12422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12518",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pakhomov_D/0/1/0/all/0/1\">Daniil Pakhomov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hira_S/0/1/0/all/0/1\">Sanchit Hira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagle_N/0/1/0/all/0/1\">Narayani Wagle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_K/0/1/0/all/0/1\">Kemar E. Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>",
          "description": "We introduce a method that allows to automatically segment images into\nsemantically meaningful regions without human supervision. Derived regions are\nconsistent across different images and coincide with human-defined semantic\nclasses on some datasets. In cases where semantic regions might be hard for\nhuman to define and consistently label, our method is still able to find\nmeaningful and consistent semantic classes. In our work, we use pretrained\nStyleGAN2~\\cite{karras2020analyzing} generative model: clustering in the\nfeature space of the generative model allows to discover semantic classes. Once\nclasses are discovered, a synthetic dataset with generated images and\ncorresponding segmentation masks can be created. After that a segmentation\nmodel is trained on the synthetic dataset and is able to generalize to real\nimages. Additionally, by using CLIP~\\cite{radford2021learning} we are able to\nuse prompts defined in a natural language to discover some desired semantic\nclasses. We test our method on publicly available datasets and show\nstate-of-the-art results.",
          "link": "http://arxiv.org/abs/2107.12518",
          "publishedOn": "2021-07-28T02:02:30.749Z",
          "wordCount": 597,
          "title": "Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP. (arXiv:2107.12518v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1\">Fa-Ting Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jia-Chang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-Shi Zheng</a>",
          "description": "Weakly supervised temporal action localization (WS-TAL) is a challenging task\nthat aims to localize action instances in the given video with video-level\ncategorical supervision. Both appearance and motion features are used in\nprevious works, while they do not utilize them in a proper way but apply simple\nconcatenation or score-level fusion. In this work, we argue that the features\nextracted from the pretrained extractor, e.g., I3D, are not the\nWS-TALtask-specific features, thus the feature re-calibration is needed for\nreducing the task-irrelevant information redundancy. Therefore, we propose a\ncross-modal consensus network (CO2-Net) to tackle this problem. In CO2-Net, we\nmainly introduce two identical proposed cross-modal consensus modules (CCM)\nthat design a cross-modal attention mechanism to filter out the task-irrelevant\ninformation redundancy using the global information from the main modality and\nthe cross-modal local information of the auxiliary modality. Moreover, we treat\nthe attention weights derived from each CCMas the pseudo targets of the\nattention weights derived from another CCM to maintain the consistency between\nthe predictions derived from two CCMs, forming a mutual learning manner.\nFinally, we conduct extensive experiments on two common used temporal action\nlocalization datasets, THUMOS14 and ActivityNet1.2, to verify our method and\nachieve the state-of-the-art results. The experimental results show that our\nproposed cross-modal consensus module can produce more representative features\nfor temporal action localization.",
          "link": "http://arxiv.org/abs/2107.12589",
          "publishedOn": "2021-07-28T02:02:30.724Z",
          "wordCount": 665,
          "title": "Cross-modal Consensus Network for Weakly Supervised Temporal Action Localization. (arXiv:2107.12589v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12604",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaotian Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Houdong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>",
          "description": "There is a surge of interest in image scene graph generation (object,\nattribute and relationship detection) due to the need of building fine-grained\nimage understanding models that go beyond object detection. Due to the lack of\na good benchmark, the reported results of different scene graph generation\nmodels are not directly comparable, impeding the research progress. We have\ndeveloped a much-needed scene graph generation benchmark based on the\nmaskrcnn-benchmark and several popular models. This paper presents main\nfeatures of our benchmark and a comprehensive ablation study of scene graph\ngeneration models using the Visual Genome and OpenImages Visual relationship\ndetection datasets. Our codebase is made publicly available at\nhttps://github.com/microsoft/scene_graph_benchmark.",
          "link": "http://arxiv.org/abs/2107.12604",
          "publishedOn": "2021-07-28T02:02:30.684Z",
          "wordCount": 546,
          "title": "Image Scene Graph Generation (SGG) Benchmark. (arXiv:2107.12604v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1\">Debesh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smedsrud_P/0/1/0/all/0/1\">Pia H. Smedsrud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansen_D/0/1/0/all/0/1\">Dag Johansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_T/0/1/0/all/0/1\">Thomas de Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansen_H/0/1/0/all/0/1\">H&#xe5;vard D. Johansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>",
          "description": "Colonoscopy is considered the gold standard for detection of colorectal\ncancer and its precursors. Existing examination methods are, however, hampered\nby high overall miss-rate, and many abnormalities are left undetected.\nComputer-Aided Diagnosis systems based on advanced machine learning algorithms\nare touted as a game-changer that can identify regions in the colon overlooked\nby the physicians during endoscopic examinations, and help detect and\ncharacterize lesions. In previous work, we have proposed the ResUNet++\narchitecture and demonstrated that it produces more efficient results compared\nwith its counterparts U-Net and ResUNet. In this paper, we demonstrate that\nfurther improvements to the overall prediction performance of the ResUNet++\narchitecture can be achieved by using conditional random field and test-time\naugmentation. We have performed extensive evaluations and validated the\nimprovements using six publicly available datasets: Kvasir-SEG, CVC-ClinicDB,\nCVC-ColonDB, ETIS-Larib Polyp DB, ASU-Mayo Clinic Colonoscopy Video Database,\nand CVC-VideoClinicDB. Moreover, we compare our proposed architecture and\nresulting model with other State-of-the-art methods. To explore the\ngeneralization capability of ResUNet++ on different publicly available polyp\ndatasets, so that it could be used in a real-world setting, we performed an\nextensive cross-dataset evaluation. The experimental results show that applying\nCRF and TTA improves the performance on various polyp segmentation datasets\nboth on the same dataset and cross-dataset.",
          "link": "http://arxiv.org/abs/2107.12435",
          "publishedOn": "2021-07-28T02:02:30.676Z",
          "wordCount": 678,
          "title": "A Comprehensive Study on Colorectal Polyp Segmentation with ResUNet++, Conditional Random Field and Test-Time Augmentation. (arXiv:2107.12435v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Rahul Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravirathinam_P/0/1/0/all/0/1\">Praveen Ravirathinam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaowei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Ankush Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulla_D/0/1/0/all/0/1\">David Mulla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vipin Kumar</a>",
          "description": "Mapping and monitoring crops is a key step towards sustainable\nintensification of agriculture and addressing global food security. A dataset\nlike ImageNet that revolutionized computer vision applications can accelerate\ndevelopment of novel crop mapping techniques. Currently, the United States\nDepartment of Agriculture (USDA) annually releases the Cropland Data Layer\n(CDL) which contains crop labels at 30m resolution for the entire United States\nof America. While CDL is state of the art and is widely used for a number of\nagricultural applications, it has a number of limitations (e.g., pixelated\nerrors, labels carried over from previous errors and absence of input imagery\nalong with class labels). In this work, we create a new semantic segmentation\nbenchmark dataset, which we call CalCROP21, for the diverse crops in the\nCentral Valley region of California at 10m spatial resolution using a Google\nEarth Engine based robust image processing pipeline and a novel attention based\nspatio-temporal semantic segmentation algorithm STATT. STATT uses re-sampled\n(interpolated) CDL labels for training, but is able to generate a better\nprediction than CDL by leveraging spatial and temporal patterns in Sentinel2\nmulti-spectral image series to effectively capture phenologic differences\namongst crops and uses attention to reduce the impact of clouds and other\natmospheric disturbances. We also present a comprehensive evaluation to show\nthat STATT has significantly better results when compared to the resampled CDL\nlabels. We have released the dataset and the processing pipeline code for\ngenerating the benchmark dataset.",
          "link": "http://arxiv.org/abs/2107.12499",
          "publishedOn": "2021-07-28T02:02:30.668Z",
          "wordCount": 690,
          "title": "CalCROP21: A Georeferenced multi-spectral dataset of Satellite Imagery and Crop Labels. (arXiv:2107.12499v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12469",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thoreau_M/0/1/0/all/0/1\">Michael Thoreau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wilson_F/0/1/0/all/0/1\">Frazer Wilson</a>",
          "description": "Access to high resolution satellite imagery has dramatically increased in\nrecent years as several new constellations have entered service. High revisit\nfrequencies as well as improved resolution has widened the use cases of\nsatellite imagery to areas such as humanitarian relief and even Search and\nRescue (SaR). We propose a novel remote sensing object detection dataset for\ndeep learning assisted SaR. This dataset contains only small objects that have\nbeen identified as potential targets as part of a live SaR response. We\nevaluate the application of popular object detection models to this dataset as\na baseline to inform further research. We also propose a novel object detection\nmetric, specifically designed to be used in a deep learning assisted SaR\nsetting.",
          "link": "http://arxiv.org/abs/2107.12469",
          "publishedOn": "2021-07-28T02:02:30.657Z",
          "wordCount": 571,
          "title": "SaRNet: A Dataset for Deep Learning Assisted Search and Rescue with Satellite Imagery. (arXiv:2107.12469v1 [eess.IV])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2103.16329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_W/0/1/0/all/0/1\">Wai Weng Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Layeghy_S/0/1/0/all/0/1\">Siamak Layeghy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarhan_M/0/1/0/all/0/1\">Mohanad Sarhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallagher_M/0/1/0/all/0/1\">Marcus Gallagher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portmann_M/0/1/0/all/0/1\">Marius Portmann</a>",
          "description": "This paper presents a new Network Intrusion Detection System (NIDS) based on\nGraph Neural Networks (GNNs). GNNs are a relatively new sub-field of deep\nneural networks, which can leverage the inherent structure of graph-based data.\nTraining and evaluation data for NIDSs are typically represented as flow\nrecords, which can naturally be represented in a graph format. This establishes\nthe potential and motivation for exploring GNNs for network intrusion\ndetection, which is the focus of this paper. Current approaches to graph\nrepresentation learning can only consider topological information and/or node\nfeatures, but not edge features. This is a key limitation for the use of\ncurrent GNN models for network intrusion detection, since critical flow\ninformation for the detection of anomalous or malicious traffic, e.g. flow\nsize, flow duration, etc., is represented as edge features in a graph\nrepresentation. In this paper, we propose E-GraphSAGE, a first GNN approach\nwhich overcomes this limitation and which allows capturing the edge features of\na graph, in addition to node features and topological information. We present a\nnovel NIDS based on E-GraphSAGE, and our extensive experimental evaluation on\nsix recent NIDS benchmark datasets shows that it outperforms the\nstate-of-the-art in regards to key classification metrics in four out of six\ncases, and closely matches it in the other two cases. Our research and initial\nbasic system demonstrates the potential of GNNs for network intrusion\ndetection, and provides motivation for further research.",
          "link": "http://arxiv.org/abs/2103.16329",
          "publishedOn": "2021-08-02T01:58:25.434Z",
          "wordCount": 750,
          "title": "E-GraphSAGE: A Graph Neural Network based Intrusion Detection System. (arXiv:2103.16329v5 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.00509",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pulver_H/0/1/0/all/0/1\">Henry Pulver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eiras_F/0/1/0/all/0/1\">Francisco Eiras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carozza_L/0/1/0/all/0/1\">Ludovico Carozza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawasly_M/0/1/0/all/0/1\">Majd Hawasly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1\">Stefano V. Albrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthy_S/0/1/0/all/0/1\">Subramanian Ramamoorthy</a>",
          "description": "Achieving a proper balance between planning quality, safety and efficiency is\na major challenge for autonomous driving. Optimisation-based motion planners\nare capable of producing safe, smooth and comfortable plans, but often at the\ncost of runtime efficiency. On the other hand, naively deploying trajectories\nproduced by efficient-to-run deep imitation learning approaches might risk\ncompromising safety. In this paper, we present PILOT -- a planning framework\nthat comprises an imitation neural network followed by an efficient optimiser\nthat actively rectifies the network's plan, guaranteeing fulfilment of safety\nand comfort requirements. The objective of the efficient optimiser is the same\nas the objective of an expensive-to-run optimisation-based planning system that\nthe neural network is trained offline to imitate. This efficient optimiser\nprovides a key layer of online protection from learning failures or deficiency\nin out-of-distribution situations that might compromise safety or comfort.\nUsing a state-of-the-art, runtime-intensive optimisation-based method as the\nexpert, we demonstrate in simulated autonomous driving experiments in CARLA\nthat PILOT achieves a seven-fold reduction in runtime when compared to the\nexpert it imitates without sacrificing planning quality.",
          "link": "http://arxiv.org/abs/2011.00509",
          "publishedOn": "2021-08-02T01:58:25.414Z",
          "wordCount": 676,
          "title": "PILOT: Efficient Planning by Imitation Learning and Optimisation for Safe Autonomous Driving. (arXiv:2011.00509v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11730",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_S/0/1/0/all/0/1\">Sheng Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Sian Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dingwen Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zizhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cappello_F/0/1/0/all/0/1\">Franck Cappello</a>",
          "description": "Error-bounded lossy compression is becoming an indispensable technique for\nthe success of today's scientific projects with vast volumes of data produced\nduring the simulations or instrument data acquisitions. Not only can it\nsignificantly reduce data size, but it also can control the compression errors\nbased on user-specified error bounds. Autoencoder (AE) models have been widely\nused in image compression, but few AE-based compression approaches support\nerror-bounding features, which are highly required by scientific applications.\nTo address this issue, we explore using convolutional autoencoders to improve\nerror-bounded lossy compression for scientific data, with the following three\nkey contributions. (1) We provide an in-depth investigation of the\ncharacteristics of various autoencoder models and develop an error-bounded\nautoencoder-based framework in terms of the SZ model. (2) We optimize the\ncompression quality for main stages in our designed AE-based error-bounded\ncompression framework, fine-tuning the block sizes and latent sizes and also\noptimizing the compression efficiency of latent vectors. (3) We evaluate our\nproposed solution using five real-world scientific datasets and comparing them\nwith six other related works. Experiments show that our solution exhibits a\nvery competitive compression quality from among all the compressors in our\ntests. In absolute terms, it can obtain a much better compression quality (100%\n~ 800% improvement in compression ratio with the same data distortion) compared\nwith SZ2.1 and ZFP in cases with a high compression ratio.",
          "link": "http://arxiv.org/abs/2105.11730",
          "publishedOn": "2021-08-02T01:58:25.387Z",
          "wordCount": 723,
          "title": "Exploring Autoencoder-based Error-bounded Compression for Scientific Data. (arXiv:2105.11730v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hammar_K/0/1/0/all/0/1\">Kim Hammar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stadler_R/0/1/0/all/0/1\">Rolf Stadler</a>",
          "description": "We study automated intrusion prevention using reinforcement learning. In a\nnovel approach, we formulate the problem of intrusion prevention as an optimal\nstopping problem. This formulation allows us insight into the structure of the\noptimal policies, which turn out to be threshold based. Since the computation\nof the optimal defender policy using dynamic programming is not feasible for\npractical cases, we approximate the optimal policy through reinforcement\nlearning in a simulation environment. To define the dynamics of the simulation,\nwe emulate the target infrastructure and collect measurements. Our evaluations\nshow that the learned policies are close to optimal and that they indeed can be\nexpressed using thresholds.",
          "link": "http://arxiv.org/abs/2106.07160",
          "publishedOn": "2021-08-02T01:58:25.365Z",
          "wordCount": 582,
          "title": "Learning Intrusion Prevention Policies through Optimal Stopping. (arXiv:2106.07160v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01531",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nistal_J/0/1/0/all/0/1\">Javier Nistal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aouameur_C/0/1/0/all/0/1\">Cyran Aouameur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lattner_S/0/1/0/all/0/1\">Stefan Lattner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richard_G/0/1/0/all/0/1\">Ga&#xeb;l Richard</a>",
          "description": "Influenced by the field of Computer Vision, Generative Adversarial Networks\n(GANs) are often adopted for the audio domain using fixed-size two-dimensional\nspectrogram representations as the \"image data\". However, in the (musical)\naudio domain, it is often desired to generate output of variable duration. This\npaper presents VQCPC-GAN, an adversarial framework for synthesizing\nvariable-length audio by exploiting Vector-Quantized Contrastive Predictive\nCoding (VQCPC). A sequence of VQCPC tokens extracted from real audio data\nserves as conditional input to a GAN architecture, providing step-wise\ntime-dependent features of the generated content. The input noise z\n(characteristic in adversarial architectures) remains fixed over time, ensuring\ntemporal consistency of global features. We evaluate the proposed model by\ncomparing a diverse set of metrics against various strong baselines. Results\nshow that, even though the baselines score best, VQCPC-GAN achieves comparable\nperformance even when generating variable-length audio. Numerous sound examples\nare provided in the accompanying website, and we release the code for\nreproducibility.",
          "link": "http://arxiv.org/abs/2105.01531",
          "publishedOn": "2021-08-02T01:58:25.319Z",
          "wordCount": 665,
          "title": "VQCPC-GAN: Variable-Length Adversarial Audio Synthesis Using Vector-Quantized Contrastive Predictive Coding. (arXiv:2105.01531v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.06402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allein_L/0/1/0/all/0/1\">Liesbeth Allein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>",
          "description": "Truth can vary over time. Fact-checking decisions on claim veracity should\ntherefore take into account temporal information of both the claim and\nsupporting or refuting evidence. In this work, we investigate the hypothesis\nthat the timestamp of a Web page is crucial to how it should be ranked for a\ngiven claim. We delineate four temporal ranking methods that constrain evidence\nranking differently and simulate hypothesis-specific evidence rankings given\nthe evidence timestamps as gold standard. Evidence ranking in three\nfact-checking models is ultimately optimized using a learning-to-rank loss\nfunction. Our study reveals that time-aware evidence ranking not only surpasses\nrelevance assumptions based purely on semantic similarity or position in a\nsearch results list, but also improves veracity predictions of time-sensitive\nclaims in particular.",
          "link": "http://arxiv.org/abs/2009.06402",
          "publishedOn": "2021-08-02T01:58:25.301Z",
          "wordCount": 595,
          "title": "Time-Aware Evidence Ranking for Fact-Checking. (arXiv:2009.06402v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11259",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Barrat_Charlaix_P/0/1/0/all/0/1\">Pierre Barrat-Charlaix</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Muntoni_A/0/1/0/all/0/1\">Anna Paola Muntoni</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shimagaki_K/0/1/0/all/0/1\">Kai Shimagaki</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Weigt_M/0/1/0/all/0/1\">Martin Weigt</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zamponi_F/0/1/0/all/0/1\">Francesco Zamponi</a>",
          "description": "Boltzmann machines (BM) are widely used as generative models. For example,\npairwise Potts models (PM), which are instances of the BM class, provide\naccurate statistical models of families of evolutionarily related protein\nsequences. Their parameters are the local fields, which describe site-specific\npatterns of amino-acid conservation, and the two-site couplings, which mirror\nthe coevolution between pairs of sites. This coevolution reflects structural\nand functional constraints acting on protein sequences during evolution. The\nmost conservative choice to describe the coevolution signal is to include all\npossible two-site couplings into the PM. This choice, typical of what is known\nas Direct Coupling Analysis, has been successful for predicting residue\ncontacts in the three-dimensional structure, mutational effects, and in\ngenerating new functional sequences. However, the resulting PM suffers from\nimportant over-fitting effects: many couplings are small, noisy and hardly\ninterpretable; the PM is close to a critical point, meaning that it is highly\nsensitive to small parameter perturbations. In this work, we introduce a\ngeneral parameter-reduction procedure for BMs, via a controlled iterative\ndecimation of the less statistically significant couplings, identified by an\ninformation-based criterion that selects either weak or statistically\nunsupported couplings. For several protein families, our procedure allows one\nto remove more than $90\\%$ of the PM couplings, while preserving the predictive\nand generative properties of the original dense PM, and the resulting model is\nfar away from criticality, hence more robust to noise.",
          "link": "http://arxiv.org/abs/2011.11259",
          "publishedOn": "2021-08-02T01:58:25.285Z",
          "wordCount": 715,
          "title": "Sparse generative modeling via parameter-reduction of Boltzmann machines: application to protein-sequence families. (arXiv:2011.11259v3 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiangshan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xuejing Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shanqing Guo</a>",
          "description": "With the wide use of Automatic Speech Recognition (ASR) in applications such\nas human machine interaction, simultaneous interpretation, audio transcription,\netc., its security protection becomes increasingly important. Although recent\nstudies have brought to light the weaknesses of popular ASR systems that enable\nout-of-band signal attack, adversarial attack, etc., and further proposed\nvarious remedies (signal smoothing, adversarial training, etc.), a systematic\nunderstanding of ASR security (both attacks and defenses) is still missing,\nespecially on how realistic such threats are and how general existing\nprotection could be. In this paper, we present our systematization of knowledge\nfor ASR security and provide a comprehensive taxonomy for existing work based\non a modularized workflow. More importantly, we align the research in this\ndomain with that on security in Image Recognition System (IRS), which has been\nextensively studied, using the domain knowledge in the latter to help\nunderstand where we stand in the former. Generally, both IRS and ASR are\nperceptual systems. Their similarities allow us to systematically study\nexisting literature in ASR security based on the spectrum of attacks and\ndefense solutions proposed for IRS, and pinpoint the directions of more\nadvanced attacks and the directions potentially leading to more effective\nprotection in ASR. In contrast, their differences, especially the complexity of\nASR compared with IRS, help us learn unique challenges and opportunities in ASR\nsecurity. Particularly, our experimental study shows that transfer learning\nacross ASR models is feasible, even in the absence of knowledge about models\n(even their types) and training data.",
          "link": "http://arxiv.org/abs/2103.10651",
          "publishedOn": "2021-08-02T01:58:25.279Z",
          "wordCount": 744,
          "title": "SoK: A Modularized Approach to Study the Security of Automatic Speech Recognition Systems. (arXiv:2103.10651v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.06775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1\">Peide Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hengli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "Traditional decision and planning frameworks for self-driving vehicles (SDVs)\nscale poorly in new scenarios, thus they require tedious hand-tuning of rules\nand parameters to maintain acceptable performance in all foreseeable cases.\nRecently, self-driving methods based on deep learning have shown promising\nresults with better generalization capability but less hand engineering effort.\nHowever, most of the previous learning-based methods are trained and evaluated\nin limited driving scenarios with scattered tasks, such as lane-following,\nautonomous braking, and conditional driving. In this paper, we propose a\ngraph-based deep network to achieve scalable self-driving that can handle\nmassive traffic scenarios. Specifically, more than 7,000 km of evaluation is\nconducted in a high-fidelity driving simulator, in which our method can obey\nthe traffic rules and safely navigate the vehicle in a large variety of urban,\nrural, and highway environments, including unprotected left turns, narrow\nroads, roundabouts, and pedestrian-rich intersections. Demonstration videos are\navailable at https://caipeide.github.io/dignet/.",
          "link": "http://arxiv.org/abs/2011.06775",
          "publishedOn": "2021-08-02T01:58:25.261Z",
          "wordCount": 646,
          "title": "DiGNet: Learning Scalable Self-Driving Policies for Generic Traffic Scenarios with Graph Neural Networks. (arXiv:2011.06775v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guowen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Han Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shangwei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Run Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>",
          "description": "This paper presents a novel fingerprinting scheme for the Intellectual\nProperty (IP) protection of Generative Adversarial Networks (GANs). Prior\nsolutions for classification models adopt adversarial examples as the\nfingerprints, which can raise stealthiness and robustness problems when they\nare applied to the GAN models. Our scheme constructs a composite deep learning\nmodel from the target GAN and a classifier. Then we generate stealthy\nfingerprint samples from this composite model, and register them to the\nclassifier for effective ownership verification. This scheme inspires three\nconcrete methodologies to practically protect the modern GAN models.\nTheoretical analysis proves that these methods can satisfy different security\nrequirements necessary for IP protection. We also conduct extensive experiments\nto show that our solutions outperform existing strategies in terms of\nstealthiness, functionality-preserving and unremovability.",
          "link": "http://arxiv.org/abs/2106.11760",
          "publishedOn": "2021-08-02T01:58:25.255Z",
          "wordCount": 609,
          "title": "A Novel Verifiable Fingerprinting Scheme for Generative Adversarial Networks. (arXiv:2106.11760v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Toth_C/0/1/0/all/0/1\">Csaba Toth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonnier_P/0/1/0/all/0/1\">Patric Bonnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberhauser_H/0/1/0/all/0/1\">Harald Oberhauser</a>",
          "description": "Sequential data such as time series, video, or text can be challenging to\nanalyse as the ordered structure gives rise to complex dependencies. At the\nheart of this is non-commutativity, in the sense that reordering the elements\nof a sequence can completely change its meaning. We use a classical\nmathematical object -- the tensor algebra -- to capture such dependencies. To\naddress the innate computational complexity of high degree tensors, we use\ncompositions of low-rank tensor projections. This yields modular and scalable\nbuilding blocks for neural networks that give state-of-the-art performance on\nstandard benchmarks such as multivariate time series classification and\ngenerative models for video.",
          "link": "http://arxiv.org/abs/2006.07027",
          "publishedOn": "2021-08-02T01:58:25.248Z",
          "wordCount": 577,
          "title": "Seq2Tens: An Efficient Representation of Sequences by Low-Rank Tensor Projections. (arXiv:2006.07027v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1\">Javier Naranjo-Alcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1\">Sergi Perez-Castanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1\">Maximo Cobos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1\">Francesc J. Ferri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1\">Pedro Zuccarello</a>",
          "description": "Acoustic scene classification (ASC) is one of the most popular problems in\nthe field of machine listening. The objective of this problem is to classify an\naudio clip into one of the predefined scenes using only the audio data. This\nproblem has considerably progressed over the years in the different editions of\nDCASE. It usually has several subtasks that allow to tackle this problem with\ndifferent approaches. The subtask presented in this report corresponds to a ASC\nproblem that is constrained by the complexity of the model as well as having\naudio recorded from different devices, known as mismatch devices (real and\nsimulated). The work presented in this report follows the research line carried\nout by the team in previous years. Specifically, a system based on two steps is\nproposed: a two-dimensional representation of the audio using the Gamamtone\nfilter bank and a convolutional neural network using squeeze-excitation\ntechniques. The presented system outperforms the baseline by about 17\npercentage points.",
          "link": "http://arxiv.org/abs/2107.14658",
          "publishedOn": "2021-08-02T01:58:25.239Z",
          "wordCount": 625,
          "title": "Task 1A DCASE 2021: Acoustic Scene Classification with mismatch-devices using squeeze-excitation technique and low-complexity constraint. (arXiv:2107.14658v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09286",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingzhong Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lirong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Liangjian Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>",
          "description": "Recent studies show that advanced priors play a major role in deep generative\nmodels. Exemplar VAE, as a variant of VAE with an exemplar-based prior, has\nachieved impressive results. However, due to the nature of model design, an\nexemplar-based model usually requires vast amounts of data to participate in\ntraining, which leads to huge computational complexity. To address this issue,\nwe propose Bayesian Pseudocoresets Exemplar VAE (ByPE-VAE), a new variant of\nVAE with a prior based on Bayesian pseudocoreset. The proposed prior is\nconditioned on a small-scale pseudocoreset rather than the whole dataset for\nreducing the computational cost and avoiding overfitting. Simultaneously, we\nobtain the optimal pseudocoreset via a stochastic optimization algorithm during\nVAE training aiming to minimize the Kullback-Leibler divergence between the\nprior based on the pseudocoreset and that based on the whole dataset.\nExperimental results show that ByPE-VAE can achieve competitive improvements\nover the state-of-the-art VAEs in the tasks of density estimation,\nrepresentation learning, and generative data augmentation. Particularly, on a\nbasic VAE architecture, ByPE-VAE is up to 3 times faster than Exemplar VAE\nwhile almost holding the performance. Code is available at our supplementary\nmaterials.",
          "link": "http://arxiv.org/abs/2107.09286",
          "publishedOn": "2021-08-02T01:58:25.232Z",
          "wordCount": 631,
          "title": "ByPE-VAE: Bayesian Pseudocoresets Exemplar VAE. (arXiv:2107.09286v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07576",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Greenbank_S/0/1/0/all/0/1\">Samuel Greenbank</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Howey_D/0/1/0/all/0/1\">David A. Howey</a>",
          "description": "The complex nature of lithium-ion battery degradation has led to many machine\nlearning based approaches to health forecasting being proposed in literature.\nHowever, machine learning can be computationally intensive. Linear approaches\nare faster but have previously been too inflexible for successful prognosis.\nFor both techniques, the choice and quality of the inputs is a limiting factor\nof performance. Piecewise-linear models, combined with automated feature\nselection, offer a fast and flexible alternative without being as\ncomputationally intensive as machine learning. Here, a piecewise-linear\napproach to battery health forecasting was compared to a Gaussian process\nregression tool and found to perform equally well. The input feature selection\nprocess demonstrated the benefit of limiting the correlation between inputs.\nFurther trials found that the piecewise-linear approach was robust to changing\ninput size and availability of training data.",
          "link": "http://arxiv.org/abs/2104.07576",
          "publishedOn": "2021-08-02T01:58:25.216Z",
          "wordCount": 601,
          "title": "Piecewise-linear modelling with feature selection for Li-ion battery end of life prognosis. (arXiv:2104.07576v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chaowei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiazhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1\">Tao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1\">Huasong Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Houpu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1\">Peng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1\">Liefeng Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanqing Chen</a>",
          "description": "In this paper, we present Fedlearn-Algo, an open-source privacy preserving\nmachine learning platform. We use this platform to demonstrate our research and\ndevelopment results on privacy preserving machine learning algorithms. As the\nfirst batch of novel FL algorithm examples, we release vertical federated\nkernel binary classification model and vertical federated random forest model.\nThey have been tested to be more efficient than existing vertical federated\nlearning models in our practice. Besides the novel FL algorithm examples, we\nalso release a machine communication module. The uniform data transfer\ninterface supports transferring widely used data formats between machines. We\nwill maintain this platform by adding more functional modules and algorithm\nexamples. The code is available at https://github.com/fedlearnAI/fedlearn-algo.",
          "link": "http://arxiv.org/abs/2107.04129",
          "publishedOn": "2021-08-02T01:58:25.204Z",
          "wordCount": 587,
          "title": "Fedlearn-Algo: A flexible open-source privacy-preserving machine learning platform. (arXiv:2107.04129v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.08903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1\">Nan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sichen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_K/0/1/0/all/0/1\">Kyle Kai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabowo_A/0/1/0/all/0/1\">Arian Prabowo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Mohammad Saiedur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1\">Flora D. Salim</a>",
          "description": "Generative Adversarial Networks (GANs) have shown remarkable success in\nproducing realistic-looking images in the computer vision area. Recently,\nGAN-based techniques are shown to be promising for spatio-temporal-based\napplications such as trajectory prediction, events generation and time-series\ndata imputation. While several reviews for GANs in computer vision have been\npresented, no one has considered addressing the practical applications and\nchallenges relevant to spatio-temporal data. In this paper, we have conducted a\ncomprehensive review of the recent developments of GANs for spatio-temporal\ndata. We summarise the application of popular GAN architectures for\nspatio-temporal data and the common practices for evaluating the performance of\nspatio-temporal applications with GANs. Finally, we point out future research\ndirections to benefit researchers in this area.",
          "link": "http://arxiv.org/abs/2008.08903",
          "publishedOn": "2021-08-02T01:58:25.198Z",
          "wordCount": 636,
          "title": "Generative Adversarial Networks for Spatio-temporal Data: A Survey. (arXiv:2008.08903v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tolstaya_E/0/1/0/all/0/1\">Ekaterina Tolstaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butler_L/0/1/0/all/0/1\">Landon Butler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mox_D/0/1/0/all/0/1\">Daniel Mox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulos_J/0/1/0/all/0/1\">James Paulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vijay Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1\">Alejandro Ribeiro</a>",
          "description": "Many algorithms for control of multi-robot teams operate under the assumption\nthat low-latency, global state information necessary to coordinate agent\nactions can readily be disseminated among the team. However, in harsh\nenvironments with no existing communication infrastructure, robots must form\nad-hoc networks, forcing the team to operate in a distributed fashion. To\novercome this challenge, we propose a task-agnostic, decentralized, low-latency\nmethod for data distribution in ad-hoc networks using Graph Neural Networks\n(GNN). Our approach enables multi-agent algorithms based on global state\ninformation to function by ensuring it is available at each robot. To do this,\nagents glean information about the topology of the network from packet\ntransmissions and feed it to a GNN running locally which instructs the agent\nwhen and where to transmit the latest state information. We train the\ndistributed GNN communication policies via reinforcement learning using the\naverage Age of Information as the reward function and show that it improves\ntraining stability compared to task-specific reward functions. Our approach\nperforms favorably compared to industry-standard methods for data distribution\nsuch as random flooding and round robin. We also show that the trained policies\ngeneralize to larger teams of both static and mobile agents.",
          "link": "http://arxiv.org/abs/2103.05091",
          "publishedOn": "2021-08-02T01:58:25.192Z",
          "wordCount": 672,
          "title": "Learning Connectivity for Data Distribution in Robot Teams. (arXiv:2103.05091v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.03133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wenbo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shroff_N/0/1/0/all/0/1\">Ness B. Shroff</a>",
          "description": "This paper studies the sample complexity (aka number of comparisons) bounds\nfor the active best-$k$ items selection from pairwise comparisons. From a given\nset of items, the learner can make pairwise comparisons on every pair of items,\nand each comparison returns an independent noisy result about the preferred\nitem. At any time, the learner can adaptively choose a pair of items to compare\naccording to past observations (i.e., active learning). The learner's goal is\nto find the (approximately) best-$k$ items with a given confidence, while\ntrying to use as few comparisons as possible. In this paper, we study two\nproblems: (i) finding the probably approximately correct (PAC) best-$k$ items\nand (ii) finding the exact best-$k$ items, both under strong stochastic\ntransitivity and stochastic triangle inequality. For PAC best-$k$ items\nselection, we first show a lower bound and then propose an algorithm whose\nsample complexity upper bound matches the lower bound up to a constant factor.\nFor the exact best-$k$ items selection, we first prove a worst-instance lower\nbound. We then propose two algorithms based on our PAC best items selection\nalgorithms: one works for $k=1$ and is sample complexity optimal up to a loglog\nfactor, and the other works for all values of $k$ and is sample complexity\noptimal up to a log factor.",
          "link": "http://arxiv.org/abs/2007.03133",
          "publishedOn": "2021-08-02T01:58:25.185Z",
          "wordCount": 681,
          "title": "The Sample Complexity of Best-$k$ Items Selection from Pairwise Comparisons. (arXiv:2007.03133v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.11972",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1\">Wanfang Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1\">Yuxiao Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Reich_B/0/1/0/all/0/1\">Brian J Reich</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_Y/0/1/0/all/0/1\">Ying Sun</a>",
          "description": "In spatial statistics, a common objective is to predict the values of a\nspatial process at unobserved locations by exploiting spatial dependence. In\ngeostatistics, Kriging provides the best linear unbiased predictor using\ncovariance functions and is often associated with Gaussian processes. However,\nwhen considering non-linear prediction for non-Gaussian and categorical data,\nthe Kriging prediction is not necessarily optimal, and the associated variance\nis often overly optimistic. We propose to use deep neural networks (DNNs) for\nspatial prediction. Although DNNs are widely used for general classification\nand prediction, they have not been studied thoroughly for data with spatial\ndependence. In this work, we propose a novel neural network structure for\nspatial prediction by adding an embedding layer of spatial coordinates with\nbasis functions. We show in theory that the proposed DeepKriging method has\nmultiple advantages over Kriging and classical DNNs only with spatial\ncoordinates as features. We also provide density prediction for uncertainty\nquantification without any distributional assumption and apply the method to\nPM$_{2.5}$ concentrations across the continental United States.",
          "link": "http://arxiv.org/abs/2007.11972",
          "publishedOn": "2021-08-02T01:58:25.166Z",
          "wordCount": 637,
          "title": "DeepKriging: Spatially Dependent Deep Neural Networks for Spatial Prediction. (arXiv:2007.11972v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10596",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lederer_A/0/1/0/all/0/1\">Armin Lederer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Capone_A/0/1/0/all/0/1\">Alexandre Capone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beckers_T/0/1/0/all/0/1\">Thomas Beckers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Umlauft_J/0/1/0/all/0/1\">Jonas Umlauft</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hirche_S/0/1/0/all/0/1\">Sandra Hirche</a>",
          "description": "Despite the existence of formal guarantees for learning-based control\napproaches, the relationship between data and control performance is still\npoorly understood. In this paper, we propose a Lyapunov-based measure for\nquantifying the impact of data on the certifiable control performance. By\nmodeling unknown system dynamics through Gaussian processes, we can determine\nthe interrelation between model uncertainty and satisfaction of stability\nconditions. This allows us to directly asses the impact of data on the provable\nstationary control performance, and thereby the value of the data for the\nclosed-loop system performance. Our approach is applicable to a wide variety of\nunknown nonlinear systems that are to be controlled by a generic learning-based\ncontrol law, and the results obtained in numerical simulations indicate the\nefficacy of the proposed measure.",
          "link": "http://arxiv.org/abs/2011.10596",
          "publishedOn": "2021-08-02T01:58:25.148Z",
          "wordCount": 595,
          "title": "The Impact of Data on the Stability of Learning-Based Control- Extended Version. (arXiv:2011.10596v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zijian Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>",
          "description": "In this paper, we propose MINE to perform novel view synthesis and depth\nestimation via dense 3D reconstruction from a single image. Our approach is a\ncontinuous depth generalization of the Multiplane Images (MPI) by introducing\nthe NEural radiance fields (NeRF). Given a single image as input, MINE predicts\na 4-channel image (RGB and volume density) at arbitrary depth values to jointly\nreconstruct the camera frustum and fill in occluded contents. The reconstructed\nand inpainted frustum can then be easily rendered into novel RGB or depth views\nusing differentiable rendering. Extensive experiments on RealEstate10K, KITTI\nand Flowers Light Fields show that our MINE outperforms state-of-the-art by a\nlarge margin in novel view synthesis. We also achieve competitive results in\ndepth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our\nsource code is available at https://github.com/vincentfung13/MINE",
          "link": "http://arxiv.org/abs/2103.14910",
          "publishedOn": "2021-08-02T01:58:25.136Z",
          "wordCount": 639,
          "title": "MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis. (arXiv:2103.14910v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.00241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vadera_S/0/1/0/all/0/1\">Sunil Vadera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ameen_S/0/1/0/all/0/1\">Salem Ameen</a>",
          "description": "This paper presents a survey of methods for pruning deep neural networks. It\nbegins by categorising over 150 studies based on the underlying approach used\nand then focuses on three categories: methods that use magnitude based pruning,\nmethods that utilise clustering to identify redundancy, and methods that use\nsensitivity analysis to assess the effect of pruning. Some of the key\ninfluencing studies within these categories are presented to highlight the\nunderlying approaches and results achieved. Most studies present results which\nare distributed in the literature as new architectures, algorithms and data\nsets have developed with time, making comparison across different studied\ndifficult. The paper therefore provides a resource for the community that can\nbe used to quickly compare the results from many different methods on a variety\nof data sets, and a range of architectures, including AlexNet, ResNet, DenseNet\nand VGG. The resource is illustrated by comparing the results published for\npruning AlexNet and ResNet50 on ImageNet and ResNet56 and VGG16 on the CIFAR10\ndata to reveal which pruning methods work well in terms of retaining accuracy\nwhilst achieving good compression rates. The paper concludes by identifying\nsome promising directions for future research.",
          "link": "http://arxiv.org/abs/2011.00241",
          "publishedOn": "2021-08-02T01:58:25.131Z",
          "wordCount": 662,
          "title": "Methods for Pruning Deep Neural Networks. (arXiv:2011.00241v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.01005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Safran_I/0/1/0/all/0/1\">Itay Safran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yehudai_G/0/1/0/all/0/1\">Gilad Yehudai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1\">Ohad Shamir</a>",
          "description": "We study the effects of mild over-parameterization on the optimization\nlandscape of a simple ReLU neural network of the form\n$\\mathbf{x}\\mapsto\\sum_{i=1}^k\\max\\{0,\\mathbf{w}_i^{\\top}\\mathbf{x}\\}$, in a\nwell-studied teacher-student setting where the target values are generated by\nthe same architecture, and when directly optimizing over the population squared\nloss with respect to Gaussian inputs. We prove that while the objective is\nstrongly convex around the global minima when the teacher and student networks\npossess the same number of neurons, it is not even \\emph{locally convex} after\nany amount of over-parameterization. Moreover, related desirable properties\n(e.g., one-point strong convexity and the Polyak-{\\L}ojasiewicz condition) also\ndo not hold even locally. On the other hand, we establish that the objective\nremains one-point strongly convex in \\emph{most} directions (suitably defined),\nand show an optimization guarantee under this property. For the non-global\nminima, we prove that adding even just a single neuron will turn a non-global\nminimum into a saddle point. This holds under some technical conditions which\nwe validate empirically. These results provide a possible explanation for why\nrecovering a global minimum becomes significantly easier when we\nover-parameterize, even if the amount of over-parameterization is very\nmoderate.",
          "link": "http://arxiv.org/abs/2006.01005",
          "publishedOn": "2021-08-02T01:58:25.115Z",
          "wordCount": 662,
          "title": "The Effects of Mild Over-parameterization on the Optimization Landscape of Shallow ReLU Neural Networks. (arXiv:2006.01005v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lederer_A/0/1/0/all/0/1\">Armin Lederer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conejo_A/0/1/0/all/0/1\">Alejandro Jose Ordonez Conejo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_K/0/1/0/all/0/1\">Korbinian Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wenxin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umlauft_J/0/1/0/all/0/1\">Jonas Umlauft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirche_S/0/1/0/all/0/1\">Sandra Hirche</a>",
          "description": "The increased demand for online prediction and the growing availability of\nlarge data sets drives the need for computationally efficient models. While\nexact Gaussian process regression shows various favorable theoretical\nproperties (uncertainty estimate, unlimited expressive power), the poor scaling\nwith respect to the training set size prohibits its application in big data\nregimes in real-time. Therefore, this paper proposes dividing local Gaussian\nprocesses, which are a novel, computationally efficient modeling approach based\non Gaussian process regression. Due to an iterative, data-driven division of\nthe input space, they achieve a sublinear computational complexity in the total\nnumber of training points in practice, while providing excellent predictive\ndistributions. A numerical evaluation on real-world data sets shows their\nadvantages over other state-of-the-art methods in terms of accuracy as well as\nprediction and update speed.",
          "link": "http://arxiv.org/abs/2006.09446",
          "publishedOn": "2021-08-02T01:58:25.097Z",
          "wordCount": 601,
          "title": "Real-Time Regression with Dividing Local Gaussian Processes. (arXiv:2006.09446v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berube_C/0/1/0/all/0/1\">Charles L. B&#xe9;rub&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berube_P/0/1/0/all/0/1\">Pierre B&#xe9;rub&#xe9;</a>",
          "description": "We present a novel approach for data-driven modeling of the time-domain\ninduced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs\nare Bayesian neural networks that aim to learn a latent statistical\ndistribution to encode extensive data sets as lower dimension representations.\nWe collected 1 600 319 IP decay curves in various regions of Canada, the United\nStates and Kazakhstan, and compiled them to train a deep VAE. The proposed deep\nlearning approach is strictly unsupervised and data-driven: it does not require\nmanual processing or ground truth labeling of IP data. Moreover, our VAE\napproach avoids the pitfalls of IP parametrization with the empirical Cole-Cole\nand Debye decomposition models, simple power-law models, or other sophisticated\nmechanistic models. We demonstrate four applications of VAEs to model and\nprocess IP data: (1) representative synthetic data generation, (2) unsupervised\nBayesian denoising and data uncertainty estimation, (3) quantitative evaluation\nof the signal-to-noise ratio, and (4) automated outlier detection. We also\ninterpret the IP compilation's latent representation and reveal a strong\ncorrelation between its first dimension and the average chargeability of IP\ndecays. Finally, we experiment with varying VAE latent space dimensions and\ndemonstrate that a single real-valued scalar parameter contains sufficient\ninformation to encode our extensive IP data compilation. This new finding\nsuggests that modeling time-domain IP data using mathematical models governed\nby more than one free parameter is ambiguous, whereas modeling only the average\nchargeability is justified. A pre-trained implementation of our model --\nreadily applicable to new IP data from any geolocation -- is available as\nopen-source Python code for the applied geophysics community.",
          "link": "http://arxiv.org/abs/2107.14796",
          "publishedOn": "2021-08-02T01:58:25.066Z",
          "wordCount": 728,
          "title": "Data-driven modeling of time-domain induced polarization. (arXiv:2107.14796v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Damak_K/0/1/0/all/0/1\">Khalil Damak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khenissi_S/0/1/0/all/0/1\">Sami Khenissi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasraoui_O/0/1/0/all/0/1\">Olfa Nasraoui</a>",
          "description": "Recent work in recommender systems has emphasized the importance of fairness,\nwith a particular interest in bias and transparency, in addition to predictive\naccuracy. In this paper, we focus on the state of the art pairwise ranking\nmodel, Bayesian Personalized Ranking (BPR), which has previously been found to\noutperform pointwise models in predictive accuracy, while also being able to\nhandle implicit feedback. Specifically, we address two limitations of BPR: (1)\nBPR is a black box model that does not explain its outputs, thus limiting the\nuser's trust in the recommendations, and the analyst's ability to scrutinize a\nmodel's outputs; and (2) BPR is vulnerable to exposure bias due to the data\nbeing Missing Not At Random (MNAR). This exposure bias usually translates into\nan unfairness against the least popular items because they risk being\nunder-exposed by the recommender system. In this work, we first propose a novel\nexplainable loss function and a corresponding Matrix Factorization-based model\ncalled Explainable Bayesian Personalized Ranking (EBPR) that generates\nrecommendations along with item-based explanations. Then, we theoretically\nquantify additional exposure bias resulting from the explainability, and use it\nas a basis to propose an unbiased estimator for the ideal EBPR loss. The result\nis a ranking model that aptly captures both debiased and explainable user\npreferences. Finally, we perform an empirical study on three real-world\ndatasets that demonstrate the advantages of our proposed models.",
          "link": "http://arxiv.org/abs/2107.14768",
          "publishedOn": "2021-08-02T01:58:25.056Z",
          "wordCount": 695,
          "title": "Debiased Explainable Pairwise Ranking from Implicit Feedback. (arXiv:2107.14768v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soutif__Cormerais_A/0/1/0/all/0/1\">Albin Soutif--Cormerais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masana_M/0/1/0/all/0/1\">Marc Masana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost Van de Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1\">Bart&#x142;omiej Twardowski</a>",
          "description": "In class-incremental learning, an agent with limited resources needs to learn\na sequence of classification tasks, forming an ever growing classification\nproblem, with the constraint of not being able to access data from previous\ntasks. The main difference with task-incremental learning, where a task-ID is\navailable at inference time, is that the learner also needs to perform\ncross-task discrimination, i.e. distinguish between classes that have not been\nseen together. Approaches to tackle this problem are numerous and mostly make\nuse of an external memory (buffer) of non-negligible size. In this paper, we\nablate the learning of cross-task features and study its influence on the\nperformance of basic replay strategies used for class-IL. We also define a new\nforgetting measure for class-incremental learning, and see that forgetting is\nnot the principal cause of low performance. Our experimental results show that\nfuture algorithms for class-incremental learning should not only prevent\nforgetting, but also aim to improve the quality of the cross-task features, and\nthe knowledge transfer between tasks. This is especially important when tasks\ncontain limited amount of data.",
          "link": "http://arxiv.org/abs/2106.11930",
          "publishedOn": "2021-08-02T01:58:25.046Z",
          "wordCount": 651,
          "title": "On the importance of cross-task features for class-incremental learning. (arXiv:2106.11930v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xuezhong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jingyu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_C/0/1/0/all/0/1\">Cheng Zhuo</a>",
          "description": "As technology scaling is approaching the physical limit, lithography hotspot\ndetection has become an essential task in design for manufacturability. While\nthe deployment of pattern matching or machine learning in hotspot detection can\nhelp save significant simulation time, such methods typically demand for\nnon-trivial quality data to build the model, which most design houses are short\nof. Moreover, the design houses are also unwilling to directly share such data\nwith the other houses to build a unified model, which can be ineffective for\nthe design house with unique design patterns due to data insufficiency. On the\nother hand, with data homogeneity in each design house, the locally trained\nmodels can be easily over-fitted, losing generalization ability and robustness.\nIn this paper, we propose a heterogeneous federated learning framework for\nlithography hotspot detection that can address the aforementioned issues. On\none hand, the framework can build a more robust centralized global sub-model\nthrough heterogeneous knowledge sharing while keeping local data private. On\nthe other hand, the global sub-model can be combined with a local sub-model to\nbetter adapt to local data heterogeneity. The experimental results show that\nthe proposed framework can overcome the challenge of non-independent and\nidentically distributed (non-IID) data and heterogeneous communication to\nachieve very high performance in comparison to other state-of-the-art methods\nwhile guaranteeing a good convergence rate in various scenarios.",
          "link": "http://arxiv.org/abs/2107.04367",
          "publishedOn": "2021-08-02T01:58:25.022Z",
          "wordCount": 690,
          "title": "Lithography Hotspot Detection via Heterogeneous Federated Learning with Local Adaptation. (arXiv:2107.04367v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mozo_A/0/1/0/all/0/1\">Alberto Mozo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Prieto_A/0/1/0/all/0/1\">&#xc1;ngel Gonz&#xe1;lez-Prieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pastor_A/0/1/0/all/0/1\">Antonio Pastor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Canaval_S/0/1/0/all/0/1\">Sandra G&#xf3;mez-Canaval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talavera_E/0/1/0/all/0/1\">Edgar Talavera</a>",
          "description": "Due to the growing rise of cyber attacks in the Internet, flow-based data\nsets are crucial to increase the performance of the Machine Learning (ML)\ncomponents that run in network-based intrusion detection systems (IDS). To\novercome the existing network traffic data shortage in attack analysis, recent\nworks propose Generative Adversarial Networks (GANs) for synthetic flow-based\nnetwork traffic generation. Data privacy is appearing more and more as a strong\nrequirement when processing such network data, which suggests to find solutions\nwhere synthetic data can fully replace real data. Because of the\nill-convergence of the GAN training, none of the existing solutions can\ngenerate high-quality fully synthetic data that can totally substitute real\ndata in the training of IDS ML components. Therefore, they mix real with\nsynthetic data, which acts only as data augmentation components, leading to\nprivacy breaches as real data is used. In sharp contrast, in this work we\npropose a novel deterministic way to measure the quality of the synthetic data\nproduced by a GAN both with respect to the real data and to its performance\nwhen used for ML tasks. As a byproduct, we present a heuristic that uses these\nmetrics for selecting the best performing generator during GAN training,\nleading to a stopping criterion. An additional heuristic is proposed to select\nthe best performing GANs when different types of synthetic data are to be used\nin the same ML task. We demonstrate the adequacy of our proposal by generating\nsynthetic cryptomining attack traffic and normal traffic flow-based data using\nan enhanced version of a Wasserstein GAN. We show that the generated synthetic\nnetwork traffic can completely replace real data when training a ML-based\ncryptomining detector, obtaining similar performance and avoiding privacy\nviolations, since real data is not used in the training of the ML-based\ndetector.",
          "link": "http://arxiv.org/abs/2107.14776",
          "publishedOn": "2021-08-02T01:58:25.005Z",
          "wordCount": 748,
          "title": "Synthetic flow-based cryptomining attack generation through Generative Adversarial Networks. (arXiv:2107.14776v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Baihe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason D. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>",
          "description": "This paper considers two-player zero-sum finite-horizon Markov games with\nsimultaneous moves. The study focuses on the challenging settings where the\nvalue function or the model is parameterized by general function classes.\nProvably efficient algorithms for both decoupled and {coordinated} settings are\ndeveloped. In the {decoupled} setting where the agent controls a single player\nand plays against an arbitrary opponent, we propose a new model-free algorithm.\nThe sample complexity is governed by the Minimax Eluder dimension -- a new\ndimension of the function class in Markov games. As a special case, this method\nimproves the state-of-the-art algorithm by a $\\sqrt{d}$ factor in the regret\nwhen the reward function and transition kernel are parameterized with\n$d$-dimensional linear features. In the {coordinated} setting where both\nplayers are controlled by the agent, we propose a model-based algorithm and a\nmodel-free algorithm. In the model-based algorithm, we prove that sample\ncomplexity can be bounded by a generalization of Witness rank to Markov games.\nThe model-free algorithm enjoys a $\\sqrt{K}$-regret upper bound where $K$ is\nthe number of episodes. Our algorithms are based on new techniques of alternate\noptimism.",
          "link": "http://arxiv.org/abs/2107.14702",
          "publishedOn": "2021-08-02T01:58:24.998Z",
          "wordCount": 629,
          "title": "Towards General Function Approximation in Zero-Sum Markov Games. (arXiv:2107.14702v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xiaomin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lihang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jieqiong Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Donglong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanzhuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>",
          "description": "Effective molecular representation learning is of great importance to\nfacilitate molecular property prediction, which is a fundamental task for the\ndrug and material industry. Recent advances in graph neural networks (GNNs)\nhave shown great promise in applying GNNs for molecular representation\nlearning. Moreover, a few recent studies have also demonstrated successful\napplications of self-supervised learning methods to pre-train the GNNs to\novercome the problem of insufficient labeled molecules. However, existing GNNs\nand pre-training strategies usually treat molecules as topological graph data\nwithout fully utilizing the molecular geometry information. Whereas, the\nthree-dimensional (3D) spatial structure of a molecule, a.k.a molecular\ngeometry, is one of the most critical factors for determining molecular\nphysical, chemical, and biological properties. To this end, we propose a novel\nGeometry Enhanced Molecular representation learning method (GEM) for Chemical\nRepresentation Learning (ChemRL). At first, we design a geometry-based GNN\narchitecture that simultaneously models atoms, bonds, and bond angles in a\nmolecule. To be specific, we devised double graphs for a molecule: The first\none encodes the atom-bond relations; The second one encodes bond-angle\nrelations. Moreover, on top of the devised GNN architecture, we propose several\nnovel geometry-level self-supervised learning strategies to learn spatial\nknowledge by utilizing the local and global molecular 3D structures. We compare\nChemRL-GEM with various state-of-the-art (SOTA) baselines on different\nmolecular benchmarks and exhibit that ChemRL-GEM can significantly outperform\nall baselines in both regression and classification tasks. For example, the\nexperimental results show an overall improvement of 8.8% on average compared to\nSOTA baselines on the regression tasks, demonstrating the superiority of the\nproposed method.",
          "link": "http://arxiv.org/abs/2106.06130",
          "publishedOn": "2021-08-02T01:58:24.993Z",
          "wordCount": 751,
          "title": "ChemRL-GEM: Geometry Enhanced Molecular Representation Learning for Property Prediction. (arXiv:2106.06130v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14608",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rusu_C/0/1/0/all/0/1\">Cristian Rusu</a>",
          "description": "In this paper, we describe a new algorithm to build a few sparse principal\ncomponents from a given data matrix. Our approach does not explicitly create\nthe covariance matrix of the data and can be viewed as an extension of the\nKogbetliantz algorithm to build an approximate singular value decomposition for\na few principal components. We show the performance of the proposed algorithm\nto recover sparse principal components on various datasets from the literature\nand perform dimensionality reduction for classification applications.",
          "link": "http://arxiv.org/abs/2107.14608",
          "publishedOn": "2021-08-02T01:58:24.987Z",
          "wordCount": 527,
          "title": "An iterative coordinate descent algorithm to compute sparse low-rank approximations. (arXiv:2107.14608v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kassis_A/0/1/0/all/0/1\">Andre Kassis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengartner_U/0/1/0/all/0/1\">Urs Hengartner</a>",
          "description": "Voice authentication has become an integral part in security-critical\noperations, such as bank transactions and call center conversations. The\nvulnerability of automatic speaker verification systems (ASVs) to spoofing\nattacks instigated the development of countermeasures (CMs), whose task is to\ntell apart bonafide and spoofed speech. Together, ASVs and CMs form today's\nvoice authentication platforms, advertised as an impregnable access control\nmechanism. We develop the first practical attack on CMs, and show how a\nmalicious actor may efficiently craft audio samples to bypass voice\nauthentication in its strictest form. Previous works have primarily focused on\nnon-proactive attacks or adversarial strategies against ASVs that do not\nproduce speech in the victim's voice. The repercussions of our attacks are far\nmore severe, as the samples we generate sound like the victim, eliminating any\nchance of plausible deniability. Moreover, the few existing adversarial attacks\nagainst CMs mistakenly optimize spoofed speech in the feature space and do not\ntake into account the existence of ASVs, resulting in inferior synthetic audio\nthat fails in realistic settings. We eliminate these obstacles through our key\ntechnical contribution: a novel joint loss function that enables mounting\nadvanced adversarial attacks against combined ASV/CM deployments directly in\nthe time domain. Our adversarials achieve concerning black-box success rates\nagainst state-of-the-art authentication platforms (up to 93.57\\%). Finally, we\nperform the first targeted, over-telephony-network attack on CMs, bypassing\nseveral challenges and enabling various potential threats, given the increased\nuse of voice biometrics in call centers. Our results call into question the\nsecurity of modern voice authentication systems in light of the real threat of\nattackers bypassing these measures to gain access to users' most valuable\nresources.",
          "link": "http://arxiv.org/abs/2107.14642",
          "publishedOn": "2021-08-02T01:58:24.982Z",
          "wordCount": 710,
          "title": "Practical Attacks on Voice Spoofing Countermeasures. (arXiv:2107.14642v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14747",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Coifman_R/0/1/0/all/0/1\">Ronald R. Coifman</a>, <a href=\"http://arxiv.org/find/math/1/au:+Marshall_N/0/1/0/all/0/1\">Nicholas F. Marshall</a>, <a href=\"http://arxiv.org/find/math/1/au:+Steinerberger_S/0/1/0/all/0/1\">Stefan Steinerberger</a>",
          "description": "Let $\\mathcal{G} = \\{G_1 = (V, E_1), \\dots, G_m = (V, E_m)\\}$ be a collection\nof $m$ graphs defined on a common set of vertices $V$ but with different edge\nsets $E_1, \\dots, E_m$. Informally, a function $f :V \\rightarrow \\mathbb{R}$ is\nsmooth with respect to $G_k = (V,E_k)$ if $f(u) \\sim f(v)$ whenever $(u, v) \\in\nE_k$. We study the problem of understanding whether there exists a nonconstant\nfunction that is smooth with respect to all graphs in $\\mathcal{G}$,\nsimultaneously, and how to find it if it exists.",
          "link": "http://arxiv.org/abs/2107.14747",
          "publishedOn": "2021-08-02T01:58:24.976Z",
          "wordCount": 526,
          "title": "A common variable minimax theorem for graphs. (arXiv:2107.14747v1 [math.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2005.04954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_T/0/1/0/all/0/1\">Tatsuya Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_A/0/1/0/all/0/1\">Atsuyoshi Nakamura</a>",
          "description": "Various things propagate through the medium of individuals. Some individuals\nfollow the others and take the states similar to their states a small number of\ntime steps later. In this paper, we study the problem of estimating the state\npropagation order of individuals from the real-valued state sequences of all\nthe individuals. We propose a method to estimate the propagation direction\nbetween individuals by the sum of the time delay of one individual's state\npositions from the other individual's matched state position averaged over the\nminimum cost alignments and show how to calculate it efficiently. The\npropagation order estimated by our proposed method is demonstrated to be\nsignificantly more accurate than that by a baseline method for our synthetic\ndatasets, and also to be consistent with visually recognizable propagation\norders for the dataset of Japanese stock price time series and biological cell\nfiring state sequences.",
          "link": "http://arxiv.org/abs/2005.04954",
          "publishedOn": "2021-08-02T01:58:24.958Z",
          "wordCount": 615,
          "title": "Propagation Graph Estimation from Individual's Time Series of Observed States. (arXiv:2005.04954v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.12377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Landi_F/0/1/0/all/0/1\">Federico Landi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corsini_M/0/1/0/all/0/1\">Massimiliano Corsini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Vision-and-Language Navigation (VLN) is a challenging task in which an agent\nneeds to follow a language-specified path to reach a target destination. The\ngoal gets even harder as the actions available to the agent get simpler and\nmove towards low-level, atomic interactions with the environment. This setting\ntakes the name of low-level VLN. In this paper, we strive for the creation of\nan agent able to tackle three key issues: multi-modality, long-term\ndependencies, and adaptability towards different locomotive settings. To that\nend, we devise \"Perceive, Transform, and Act\" (PTA): a fully-attentive VLN\narchitecture that leaves the recurrent approach behind and the first\nTransformer-like architecture incorporating three different modalities -\nnatural language, images, and low-level actions for the agent control. In\nparticular, we adopt an early fusion strategy to merge lingual and visual\ninformation efficiently in our encoder. We then propose to refine the decoding\nphase with a late fusion extension between the agent's history of actions and\nthe perceptual modalities. We experimentally validate our model on two\ndatasets: PTA achieves promising results in low-level VLN on R2R and achieves\ngood performance in the recently proposed R4R benchmark. Our code is publicly\navailable at https://github.com/aimagelab/perceive-transform-and-act.",
          "link": "http://arxiv.org/abs/1911.12377",
          "publishedOn": "2021-08-02T01:58:24.951Z",
          "wordCount": 687,
          "title": "Multimodal Attention Networks for Low-Level Vision-and-Language Navigation. (arXiv:1911.12377v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.15106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McLaughlin_C/0/1/0/all/0/1\">Connor J. McLaughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokkotou_E/0/1/0/all/0/1\">Efi G. Kokkotou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_J/0/1/0/all/0/1\">Jean A. King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conboy_L/0/1/0/all/0/1\">Lisa A. Conboy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousefi_A/0/1/0/all/0/1\">Ali Yousefi</a>",
          "description": "The analysis of clinical questionnaire data comes with many inherent\nchallenges. These challenges include the handling of data with missing fields,\nas well as the overall interpretation of a dataset with many fields of\ndifferent scales and forms. While numerous methods have been developed to\naddress these challenges, they are often not robust, statistically sound, or\neasily interpretable. Here, we propose a latent factor modeling framework that\nextends the principal component analysis for both categorical and quantitative\ndata with missing elements. The model simultaneously provides the principal\ncomponents (basis) and each patients' projections on these bases in a latent\nspace. We show an application of our modeling framework through Irritable Bowel\nSyndrome (IBS) symptoms, where we find correlations between these projections\nand other standardized patient symptom scales. This latent factor model can be\neasily applied to different clinical questionnaire datasets for clustering\nanalysis and interpretable inference.",
          "link": "http://arxiv.org/abs/2104.15106",
          "publishedOn": "2021-08-02T01:58:24.944Z",
          "wordCount": 628,
          "title": "Latent Factor Decomposition Model: Applications for Questionnaire Data. (arXiv:2104.15106v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14695",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Ekapure_S/0/1/0/all/0/1\">Shubham Ekapure</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Jiruwala_N/0/1/0/all/0/1\">Nuruddin Jiruwala</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Patnaik_S/0/1/0/all/0/1\">Sohan Patnaik</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+SenGupta_I/0/1/0/all/0/1\">Indranil SenGupta</a>",
          "description": "In this paper, we implement a combination of technical analysis and\nmachine/deep learning-based analysis to build a trend classification model. The\ngoal of the paper is to apprehend short-term market movement, and incorporate\nit to improve the underlying stochastic model. Also, the analysis presented in\nthis paper can be implemented in a \\emph{model-independent} fashion. We execute\na data-science-driven technique that makes short-term forecasts dependent on\nthe price trends of current stock market data. Based on the analysis, three\ndifferent labels are generated for a data set: $+1$ (buy signal), $0$ (hold\nsignal), or $-1$ (sell signal). We propose a detailed analysis of four major\nstocks- Amazon, Apple, Google, and Microsoft. We implement various technical\nindicators to label the data set according to the trend and train various\nmodels for trend estimation. Statistical analysis of the outputs and\nclassification results are obtained.",
          "link": "http://arxiv.org/abs/2107.14695",
          "publishedOn": "2021-08-02T01:58:24.936Z",
          "wordCount": 602,
          "title": "A data-science-driven short-term analysis of Amazon, Apple, Google, and Microsoft stocks. (arXiv:2107.14695v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2101.07240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kutuzova_S/0/1/0/all/0/1\">Svetlana Kutuzova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1\">Oswin Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCloskey_D/0/1/0/all/0/1\">Douglas McCloskey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_M/0/1/0/all/0/1\">Mads Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1\">Christian Igel</a>",
          "description": "Multimodal generative models should be able to learn a meaningful latent\nrepresentation that enables a coherent joint generation of all modalities\n(e.g., images and text). Many applications also require the ability to\naccurately sample modalities conditioned on observations of a subset of the\nmodalities. Often not all modalities may be observed for all training data\npoints, so semi-supervised learning should be possible. In this study, we\npropose a novel product-of-experts (PoE) based variational autoencoder that\nhave these desired properties. We benchmark it against a mixture-of-experts\n(MoE) approach and an approach of combining the modalities with an additional\nencoder network. An empirical evaluation shows that the PoE based models can\noutperform the contrasted models. Our experiments support the intuition that\nPoE models are more suited for a conjunctive combination of modalities.",
          "link": "http://arxiv.org/abs/2101.07240",
          "publishedOn": "2021-08-02T01:58:24.930Z",
          "wordCount": 597,
          "title": "Multimodal Variational Autoencoders for Semi-Supervised Learning: In Defense of Product-of-Experts. (arXiv:2101.07240v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yun-Yun Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>",
          "description": "Learning to classify time series with limited data is a practical yet\nchallenging problem. Current methods are primarily based on hand-designed\nfeature extraction rules or domain-specific data augmentation. Motivated by the\nadvances in deep speech processing models and the fact that voice data are\nunivariate temporal signals, in this paper, we propose Voice2Series (V2S), a\nnovel end-to-end approach that reprograms acoustic models for time series\nclassification, through input transformation learning and output label mapping.\nLeveraging the representation learning power of a large-scale pre-trained\nspeech processing model, on 30 different time series tasks we show that V2S\neither outperforms or is tied with state-of-the-art methods on 20 tasks, and\nimproves their average accuracy by 1.84%. We further provide a theoretical\njustification of V2S by proving its population risk is upper bounded by the\nsource risk and a Wasserstein distance accounting for feature alignment via\nreprogramming. Our results offer new and effective means to time series\nclassification.",
          "link": "http://arxiv.org/abs/2106.09296",
          "publishedOn": "2021-08-02T01:58:24.913Z",
          "wordCount": 672,
          "title": "Voice2Series: Reprogramming Acoustic Models for Time Series Classification. (arXiv:2106.09296v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scholler_C/0/1/0/all/0/1\">Christoph Sch&#xf6;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a>",
          "description": "The future motion of traffic participants is inherently uncertain. To plan\nsafely, therefore, an autonomous agent must take into account multiple possible\ntrajectory outcomes and prioritize them. Recently, this problem has been\naddressed with generative neural networks. However, most generative models\neither do not learn the true underlying trajectory distribution reliably, or do\nnot allow predictions to be associated with likelihoods. In our work, we model\nmotion prediction directly as a density estimation problem with a normalizing\nflow between a noise distribution and the future motion distribution. Our\nmodel, named FloMo, allows likelihoods to be computed in a single network pass\nand can be trained directly with maximum likelihood estimation. Furthermore, we\npropose a method to stabilize training flows on trajectory datasets and a new\ndata augmentation transformation that improves the performance and\ngeneralization of our model. Our method achieves state-of-the-art performance\non three popular prediction datasets, with a significant gap to most competing\nmodels.",
          "link": "http://arxiv.org/abs/2103.03614",
          "publishedOn": "2021-08-02T01:58:24.907Z",
          "wordCount": 634,
          "title": "FloMo: Tractable Motion Prediction with Normalizing Flows. (arXiv:2103.03614v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14664",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Antunes_L/0/1/0/all/0/1\">Luis M. Antunes</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Grau_Crespo_R/0/1/0/all/0/1\">Ricardo Grau-Crespo</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Butler_K/0/1/0/all/0/1\">Keith T. Butler</a>",
          "description": "The use of machine learning is becoming increasingly common in computational\nmaterials science. To build effective models of the chemistry of materials,\nuseful machine-based representations of atoms and their compounds are required.\nWe derive distributed representations of compounds from their chemical formulas\nonly, via pooling operations of distributed representations of atoms. These\ncompound representations are evaluated on ten different tasks, such as the\nprediction of formation energy and band gap, and are found to be competitive\nwith existing benchmarks that make use of structure, and even superior in cases\nwhere only composition is available. Finally, we introduce a new approach for\nlearning distributed representations of atoms, named SkipAtom, which makes use\nof the growing information in materials structure databases.",
          "link": "http://arxiv.org/abs/2107.14664",
          "publishedOn": "2021-08-02T01:58:24.902Z",
          "wordCount": 556,
          "title": "Distributed Representations of Atoms and Materials for Machine Learning. (arXiv:2107.14664v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2007.03767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozdayi_M/0/1/0/all/0/1\">Mustafa Safa Ozdayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kantarcioglu_M/0/1/0/all/0/1\">Murat Kantarcioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gel_Y/0/1/0/all/0/1\">Yulia R. Gel</a>",
          "description": "Federated learning (FL) allows a set of agents to collaboratively train a\nmodel without sharing their potentially sensitive data. This makes FL suitable\nfor privacy-preserving applications. At the same time, FL is susceptible to\nadversarial attacks due to decentralized and unvetted data. One important line\nof attacks against FL is the backdoor attacks. In a backdoor attack, an\nadversary tries to embed a backdoor functionality to the model during training\nthat can later be activated to cause a desired misclassification. To prevent\nbackdoor attacks, we propose a lightweight defense that requires minimal change\nto the FL protocol. At a high level, our defense is based on carefully\nadjusting the aggregation server's learning rate, per dimension and per round,\nbased on the sign information of agents' updates. We first conjecture the\nnecessary steps to carry a successful backdoor attack in FL setting, and then,\nexplicitly formulate the defense based on our conjecture. Through experiments,\nwe provide empirical evidence that supports our conjecture, and we test our\ndefense against backdoor attacks under different settings. We observe that\neither backdoor is completely eliminated, or its accuracy is significantly\nreduced. Overall, our experiments suggest that our defense significantly\noutperforms some of the recently proposed defenses in the literature. We\nachieve this by having minimal influence over the accuracy of the trained\nmodels. In addition, we also provide convergence rate analysis for our proposed\nscheme.",
          "link": "http://arxiv.org/abs/2007.03767",
          "publishedOn": "2021-08-02T01:58:24.896Z",
          "wordCount": 725,
          "title": "Defending against Backdoors in Federated Learning with Robust Learning Rate. (arXiv:2007.03767v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.04591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1\">Sergi Perez-Castanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1\">Javier Naranjo-Alcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1\">Pedro Zuccarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1\">Maximo Cobos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1\">Frances J. Ferri</a>",
          "description": "Acoustic scene classification (ASC) has been approached in the last years\nusing deep learning techniques such as convolutional neural networks or\nrecurrent neural networks. Many state-of-the-art solutions are based on image\nclassification frameworks and, as such, a 2D representation of the audio signal\nis considered for training these networks. Finding the most suitable audio\nrepresentation is still a research area of interest. In this paper, different\nlog-Mel representations and combinations are analyzed. Experiments show that\nthe best results are obtained using the harmonic and percussive components plus\nthe difference between left and right stereo channels, (L-R). On the other\nhand, it is a common strategy to ensemble different models in order to increase\nthe final accuracy. Even though averaging different model predictions is a\ncommon choice, an exhaustive analysis of different ensemble techniques has not\nbeen presented in ASC problems. In this paper, geometric and arithmetic mean\nplus the Ordered Weighted Averaging (OWA) operator are studied as aggregation\noperators for the output of the different models of the ensemble. Finally, the\nwork carried out in this paper is highly oriented towards real-time\nimplementations. In this context, as the number of applications for audio\nclassification on edge devices is increasing exponentially, we also analyze\ndifferent network depths and efficient solutions for aggregating ensemble\npredictions.",
          "link": "http://arxiv.org/abs/1906.04591",
          "publishedOn": "2021-08-02T01:58:24.890Z",
          "wordCount": 707,
          "title": "CNN depth analysis with different channel inputs for Acoustic Scene Classification. (arXiv:1906.04591v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pillai_N/0/1/0/all/0/1\">Nisha Pillai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matuszek_C/0/1/0/all/0/1\">Cynthia Matuszek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1\">Francis Ferraro</a>",
          "description": "We propose a learning system in which language is grounded in visual percepts\nwithout specific pre-defined categories of terms. We present a unified\ngenerative method to acquire a shared semantic/visual embedding that enables\nthe learning of language about a wide range of real-world objects. We evaluate\nthe efficacy of this learning by predicting the semantics of objects and\ncomparing the performance with neural and non-neural inputs. We show that this\ngenerative approach exhibits promising results in language grounding without\npre-specifying visual categories under low resource settings. Our experiments\ndemonstrate that this approach is generalizable to multilingual, highly varied\ndatasets.",
          "link": "http://arxiv.org/abs/2107.14593",
          "publishedOn": "2021-08-02T01:58:24.883Z",
          "wordCount": 552,
          "title": "Neural Variational Learning for Grounded Language Acquisition. (arXiv:2107.14593v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Galinkin_E/0/1/0/all/0/1\">Erick Galinkin</a>",
          "description": "In many cases, neural networks perform well on test data, but tend to\noverestimate their confidence on out-of-distribution data. This has led to\nadoption of Bayesian neural networks, which better capture uncertainty and\ntherefore more accurately reflect the model's confidence. For machine learning\nsecurity researchers, this raises the natural question of how making a model\nBayesian affects the security of the model. In this work, we explore the\ninterplay between Bayesianism and two measures of security: model privacy and\nadversarial robustness. We demonstrate that Bayesian neural networks are more\nvulnerable to membership inference attacks in general, but are at least as\nrobust as their non-Bayesian counterparts to adversarial examples.",
          "link": "http://arxiv.org/abs/2107.14601",
          "publishedOn": "2021-08-02T01:58:24.865Z",
          "wordCount": 531,
          "title": "Who's Afraid of Thomas Bayes?. (arXiv:2107.14601v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.05079",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dewaskar_M/0/1/0/all/0/1\">Miheer Dewaskar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Palowitch_J/0/1/0/all/0/1\">John Palowitch</a>, <a href=\"http://arxiv.org/find/stat/1/au:+He_M/0/1/0/all/0/1\">Mark He</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Love_M/0/1/0/all/0/1\">Michael I. Love</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nobel_A/0/1/0/all/0/1\">Andrew B. Nobel</a>",
          "description": "Data sets in which measurements of different types are obtained from a common\nset of samples appear in many scientific applications. In the analysis of such\ndata, an important problem is to identify groups of features from different\ndata types that are strongly associated. Given two data types, a bimodule is a\npair $(A,B)$ of feature sets from the two types such that the aggregate\ncross-correlation between the features in $A$ and those in $B$ is large. A\nbimodule $(A,B)$ is stable if $A$ coincides with the set of features that have\nsignificant aggregate correlation with the features in $B$, and vice-versa. We\ndevelop an, iterative, testing-based procedure called BSP to identify stable\nbimodules. BSP relies on approximate p-values derived from the permutation\nmoments of sums of squared sample correlations between a single feature of one\ntype and a group of features of the second type. We carry out a thorough\nsimulation study to assess the performance of BSP, and present an extended\napplication to the problem of expression quantitative trait loci (eQTL)\nanalysis using recent data from the GTEx project. In addition, we apply BSP to\nclimatology data to identify regions in North America where annual temperature\nvariation affects precipitation.",
          "link": "http://arxiv.org/abs/2009.05079",
          "publishedOn": "2021-08-02T01:58:24.860Z",
          "wordCount": 686,
          "title": "Finding Stable Groups of Cross-Correlated Features in Two Data Sets With Common Samples. (arXiv:2009.05079v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01174",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Choi_Y/0/1/0/all/0/1\">Yeunju Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_Y/0/1/0/all/0/1\">Youngmoon Jung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suh_Y/0/1/0/all/0/1\">Youngjoo Suh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Hoirin Kim</a>",
          "description": "Although recent end-to-end text-to-speech (TTS) systems have achieved\nhigh-quality synthesized speech, there are still several factors that degrade\nthe quality of synthesized speech, including lack of training data or\ninformation loss during knowledge distillation. To address the problem, we\npropose a novel way to train a TTS model under the supervision of perceptual\nloss, which measures the distance between the maximum speech quality score and\nthe predicted one. We first pre-train a mean opinion score (MOS) prediction\nmodel and then train a TTS model in the direction of maximizing the MOS of\nsynthesized speech predicted by the pre-trained MOS prediction model. Through\nthis method, we can improve the quality of synthesized speech universally\n(i.e., regardless of the network architecture or the cause of the speech\nquality degradation) and efficiently (i.e., without increasing the inference\ntime or the model complexity). The evaluation results for MOS and phoneme error\nrate demonstrate that our proposed approach improves previous models in terms\nof both naturalness and intelligibility.",
          "link": "http://arxiv.org/abs/2011.01174",
          "publishedOn": "2021-08-02T01:58:24.854Z",
          "wordCount": 630,
          "title": "Perceptually Guided End-to-End Text-to-Speech With MOS Prediction. (arXiv:2011.01174v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Disabato_S/0/1/0/all/0/1\">Simone Disabato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roveri_M/0/1/0/all/0/1\">Manuel Roveri</a>",
          "description": "Tiny Machine Learning (TML) is a new research area whose goal is to design\nmachine and deep learning techniques able to operate in Embedded Systems and\nIoT units, hence satisfying the severe technological constraints on memory,\ncomputation, and energy characterizing these pervasive devices. Interestingly,\nthe related literature mainly focused on reducing the computational and memory\ndemand of the inference phase of machine and deep learning models. At the same\ntime, the training is typically assumed to be carried out in Cloud or edge\ncomputing systems (due to the larger memory and computational requirements).\nThis assumption results in TML solutions that might become obsolete when the\nprocess generating the data is affected by concept drift (e.g., due to\nperiodicity or seasonality effect, faults or malfunctioning affecting sensors\nor actuators, or changes in the users' behavior), a common situation in\nreal-world application scenarios. For the first time in the literature, this\npaper introduces a Tiny Machine Learning for Concept Drift (TML-CD) solution\nbased on deep learning feature extractors and a k-nearest neighbors classifier\nintegrating a hybrid adaptation module able to deal with concept drift\naffecting the data-generating process. This adaptation module continuously\nupdates (in a passive way) the knowledge base of TML-CD and, at the same time,\nemploys a Change Detection Test to inspect for changes (in an active way) to\nquickly adapt to concept drift by removing the obsolete knowledge. Experimental\nresults on both image and audio benchmarks show the effectiveness of the\nproposed solution, whilst the porting of TML-CD on three off-the-shelf\nmicro-controller units shows the feasibility of what is proposed in real-world\npervasive systems.",
          "link": "http://arxiv.org/abs/2107.14759",
          "publishedOn": "2021-08-02T01:58:24.848Z",
          "wordCount": 697,
          "title": "Tiny Machine Learning for Concept Drift. (arXiv:2107.14759v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_H/0/1/0/all/0/1\">Hugo Flores Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguilar_A/0/1/0/all/0/1\">Aldo Aguilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manilow_E/0/1/0/all/0/1\">Ethan Manilow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardo_B/0/1/0/all/0/1\">Bryan Pardo</a>",
          "description": "Deep learning work on musical instrument recognition has generally focused on\ninstrument classes for which we have abundant data. In this work, we exploit\nhierarchical relationships between instruments in a few-shot learning setup to\nenable classification of a wider set of musical instruments, given a few\nexamples at inference. We apply a hierarchical loss function to the training of\nprototypical networks, combined with a method to aggregate prototypes\nhierarchically, mirroring the structure of a predefined musical instrument\nhierarchy. These extensions require no changes to the network architecture and\nnew levels can be easily added or removed. Compared to a non-hierarchical\nfew-shot baseline, our method leads to a significant increase in classification\naccuracy and significant decrease mistake severity on instrument classes unseen\nin training.",
          "link": "http://arxiv.org/abs/2107.07029",
          "publishedOn": "2021-08-02T01:58:24.836Z",
          "wordCount": 580,
          "title": "Leveraging Hierarchical Structures for Few-Shot Musical Instrument Recognition. (arXiv:2107.07029v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1\">Jean-Baptiste Alayrac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doersch_C/0/1/0/all/0/1\">Carl Doersch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_C/0/1/0/all/0/1\">Catalin Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">David Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1\">Skanda Koppula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andrew Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1\">Evan Shelhamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1\">Olivier H&#xe9;naff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1\">Matthew M. Botvinick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Jo&#xe3;o Carreira</a>",
          "description": "The recently-proposed Perceiver model obtains good results on several domains\n(images, audio, multimodal, point clouds) while scaling linearly in compute and\nmemory with the input size. While the Perceiver supports many kinds of inputs,\nit can only produce very simple outputs such as class scores. Perceiver IO\novercomes this limitation without sacrificing the original's appealing\nproperties by learning to flexibly query the model's latent space to produce\noutputs of arbitrary size and semantics. Perceiver IO still decouples model\ndepth from data size and still scales linearly with data size, but now with\nrespect to both input and output sizes. The full Perceiver IO model achieves\nstrong results on tasks with highly structured output spaces, such as natural\nlanguage and visual understanding, StarCraft II, and multi-task and multi-modal\ndomains. As highlights, Perceiver IO matches a Transformer-based BERT baseline\non the GLUE language benchmark without the need for input tokenization and\nachieves state-of-the-art performance on Sintel optical flow estimation.",
          "link": "http://arxiv.org/abs/2107.14795",
          "publishedOn": "2021-08-02T01:58:24.821Z",
          "wordCount": 639,
          "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs. (arXiv:2107.14795v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haizhou Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youcai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenjie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>",
          "description": "It is a consensus that small models perform quite poorly under the paradigm\nof self-supervised contrastive learning. Existing methods usually adopt a large\noff-the-shelf model to transfer knowledge to the small one via knowledge\ndistillation. Despite their effectiveness, distillation-based methods may not\nbe suitable for some resource-restricted scenarios due to the huge\ncomputational expenses of deploying a large model. In this paper, we study the\nissue of training self-supervised small models without distillation signals. We\nfirst evaluate the representation spaces of the small models and make two\nnon-negligible observations: (i) small models can complete the pretext task\nwithout overfitting despite its limited capacity; (ii) small models universally\nsuffer the problem of over-clustering. Then we verify multiple assumptions that\nare considered to alleviate the over-clustering phenomenon. Finally, we combine\nthe validated techniques and improve the baseline of five small architectures\nwith considerable margins, which indicates that training small self-supervised\ncontrastive models is feasible even without distillation signals.",
          "link": "http://arxiv.org/abs/2107.14762",
          "publishedOn": "2021-08-02T01:58:24.805Z",
          "wordCount": 610,
          "title": "On the Efficacy of Small Self-Supervised Contrastive Models without Distillation Signals. (arXiv:2107.14762v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gefei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yuling Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Can Yang</a>",
          "description": "We propose to learn a generative model via entropy interpolation with a\nSchr\\\"{o}dinger Bridge. The generative learning task can be formulated as\ninterpolating between a reference distribution and a target distribution based\non the Kullback-Leibler divergence. At the population level, this entropy\ninterpolation is characterized via an SDE on $[0,1]$ with a time-varying drift\nterm. At the sample level, we derive our Schr\\\"{o}dinger Bridge algorithm by\nplugging the drift term estimated by a deep score estimator and a deep density\nratio estimator into the Euler-Maruyama method. Under some mild smoothness\nassumptions of the target distribution, we prove the consistency of both the\nscore estimator and the density ratio estimator, and then establish the\nconsistency of the proposed Schr\\\"{o}dinger Bridge approach. Our theoretical\nresults guarantee that the distribution learned by our approach converges to\nthe target distribution. Experimental results on multimodal synthetic data and\nbenchmark data support our theoretical findings and indicate that the\ngenerative model via Schr\\\"{o}dinger Bridge is comparable with state-of-the-art\nGANs, suggesting a new formulation of generative learning. We demonstrate its\nusefulness in image interpolation and image inpainting.",
          "link": "http://arxiv.org/abs/2106.10410",
          "publishedOn": "2021-08-02T01:58:24.799Z",
          "wordCount": 648,
          "title": "Deep Generative Learning via Schr\\\"{o}dinger Bridge. (arXiv:2106.10410v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14742",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Alt_T/0/1/0/all/0/1\">Tobias Alt</a>, <a href=\"http://arxiv.org/find/math/1/au:+Schrader_K/0/1/0/all/0/1\">Karl Schrader</a>, <a href=\"http://arxiv.org/find/math/1/au:+Augustin_M/0/1/0/all/0/1\">Matthias Augustin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Peter_P/0/1/0/all/0/1\">Pascal Peter</a>, <a href=\"http://arxiv.org/find/math/1/au:+Weickert_J/0/1/0/all/0/1\">Joachim Weickert</a>",
          "description": "We investigate numerous structural connections between numerical algorithms\nfor partial differential equations (PDEs) and neural architectures. Our goal is\nto transfer the rich set of mathematical foundations from the world of PDEs to\nneural networks. Besides structural insights we provide concrete examples and\nexperimental evaluations of the resulting architectures. Using the example of\ngeneralised nonlinear diffusion in 1D, we consider explicit schemes,\nacceleration strategies thereof, implicit schemes, and multigrid approaches. We\nconnect these concepts to residual networks, recurrent neural networks, and\nU-net architectures. Our findings inspire a symmetric residual network design\nwith provable stability guarantees and justify the effectiveness of skip\nconnections in neural networks from a numerical perspective. Moreover, we\npresent U-net architectures that implement multigrid techniques for learning\nefficient solutions of partial differential equation models, and motivate\nuncommon design choices such as trainable nonmonotone activation functions.\nExperimental evaluations show that the proposed architectures save half of the\ntrainable parameters and can thus outperform standard ones with the same model\ncomplexity. Our considerations serve as a basis for explaining the success of\npopular neural architectures and provide a blueprint for developing new\nmathematically well-founded neural building blocks.",
          "link": "http://arxiv.org/abs/2107.14742",
          "publishedOn": "2021-08-02T01:58:24.793Z",
          "wordCount": 627,
          "title": "Connections between Numerical Algorithms for PDEs and Neural Networks. (arXiv:2107.14742v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14803",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Herbreteau_S/0/1/0/all/0/1\">S&#xe9;bastien Herbreteau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kervrann_C/0/1/0/all/0/1\">Charles Kervrann</a>",
          "description": "This work tackles the issue of noise removal from images, focusing on the\nwell-known DCT image denoising algorithm. The latter, stemming from signal\nprocessing, has been well studied over the years. Though very simple, it is\nstill used in crucial parts of state-of-the-art \"traditional\" denoising\nalgorithms such as BM3D. Since a few years however, deep convolutional neural\nnetworks (CNN) have outperformed their traditional counterparts, making signal\nprocessing methods less attractive. In this paper, we demonstrate that a DCT\ndenoiser can be seen as a shallow CNN and thereby its original linear transform\ncan be tuned through gradient descent in a supervised manner, improving\nconsiderably its performance. This gives birth to a fully interpretable CNN\ncalled DCT2net. To deal with remaining artifacts induced by DCT2net, an\noriginal hybrid solution between DCT and DCT2net is proposed combining the best\nthat these two methods can offer; DCT2net is selected to process non-stationary\nimage patches while DCT is optimal for piecewise smooth patches. Experiments on\nartificially noisy images demonstrate that two-layer DCT2net provides\ncomparable results to BM3D and is as fast as DnCNN algorithm composed of more\nthan a dozen of layers.",
          "link": "http://arxiv.org/abs/2107.14803",
          "publishedOn": "2021-08-02T01:58:24.786Z",
          "wordCount": 632,
          "title": "DCT2net: an interpretable shallow CNN for image denoising. (arXiv:2107.14803v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarap_A/0/1/0/all/0/1\">Abien Fred Agarap</a>",
          "description": "We define disentanglement as how far class-different data points from each\nother are, relative to the distances among class-similar data points. When\nmaximizing disentanglement during representation learning, we obtain a\ntransformed feature representation where the class memberships of the data\npoints are preserved. If the class memberships of the data points are\npreserved, we would have a feature representation space in which a nearest\nneighbour classifier or a clustering algorithm would perform well. We take\nadvantage of this method to learn better natural language representation, and\nemploy it on text classification and text clustering tasks. Through\ndisentanglement, we obtain text representations with better-defined clusters\nand improve text classification performance. Our approach had a test\nclassification accuracy of as high as 90.11% and test clustering accuracy of\n88% on the AG News dataset, outperforming our baseline models -- without any\nother training tricks or regularization.",
          "link": "http://arxiv.org/abs/2107.14597",
          "publishedOn": "2021-08-02T01:58:24.769Z",
          "wordCount": 584,
          "title": "Text Classification and Clustering with Annealing Soft Nearest Neighbor Loss. (arXiv:2107.14597v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_S/0/1/0/all/0/1\">Sakshi Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1\">Vinay Kumar Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1\">Srijith P K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1\">Piyush Rai</a>",
          "description": "We present a continual learning approach for generative adversarial networks\n(GANs), by designing and leveraging parameter-efficient feature map\ntransformations. Our approach is based on learning a set of global and\ntask-specific parameters. The global parameters are fixed across tasks whereas\nthe task-specific parameters act as local adapters for each task, and help in\nefficiently obtaining task-specific feature maps. Moreover, we propose an\nelement-wise addition of residual bias in the transformed feature space, which\nfurther helps stabilize GAN training in such settings. Our approach also\nleverages task similarity information based on the Fisher information matrix.\nLeveraging this knowledge from previous tasks significantly improves the model\nperformance. In addition, the similarity measure also helps reduce the\nparameter growth in continual adaptation and helps to learn a compact model. In\ncontrast to the recent approaches for continually-learned GANs, the proposed\napproach provides a memory-efficient way to perform effective continual data\ngeneration. Through extensive experiments on challenging and diverse datasets,\nwe show that the feature-map-transformation approach outperforms\nstate-of-the-art methods for continually-learned GANs, with substantially fewer\nparameters. The proposed method generates high-quality samples that can also\nimprove the generative-replay-based continual learning for discriminative\ntasks.",
          "link": "http://arxiv.org/abs/2103.04032",
          "publishedOn": "2021-08-02T01:58:24.710Z",
          "wordCount": 667,
          "title": "CAM-GAN: Continual Adaptation Modules for Generative Adversarial Networks. (arXiv:2103.04032v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pujol_Perich_D/0/1/0/all/0/1\">David Pujol-Perich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Varela_J/0/1/0/all/0/1\">Jos&#xe9; Su&#xe1;rez-Varela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabellos_Aparicio_A/0/1/0/all/0/1\">Albert Cabellos-Aparicio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barlet_Ros_P/0/1/0/all/0/1\">Pere Barlet-Ros</a>",
          "description": "The last few years have seen an increasing wave of attacks with serious\neconomic and privacy damages, which evinces the need for accurate Network\nIntrusion Detection Systems (NIDS). Recent works propose the use of Machine\nLearning (ML) techniques for building such systems (e.g., decision trees,\nneural networks). However, existing ML-based NIDS are barely robust to common\nadversarial attacks, which limits their applicability to real networks. A\nfundamental problem of these solutions is that they treat and classify flows\nindependently. In contrast, in this paper we argue the importance of focusing\non the structural patterns of attacks, by capturing not only the individual\nflow features, but also the relations between different flows (e.g., the\nsource/destination hosts they share). To this end, we use a graph\nrepresentation that keeps flow records and their relationships, and propose a\nnovel Graph Neural Network (GNN) model tailored to process and learn from such\ngraph-structured information. In our evaluation, we first show that the\nproposed GNN model achieves state-of-the-art results in the well-known\nCIC-IDS2017 dataset. Moreover, we assess the robustness of our solution under\ntwo common adversarial attacks, that intentionally modify the packet size and\ninter-arrival times to avoid detection. The results show that our model is able\nto maintain the same level of accuracy as in previous experiments, while\nstate-of-the-art ML techniques degrade up to 50% their accuracy (F1-score)\nunder these attacks. This unprecedented level of robustness is mainly induced\nby the capability of our GNN model to learn flow patterns of attacks structured\nas graphs.",
          "link": "http://arxiv.org/abs/2107.14756",
          "publishedOn": "2021-08-02T01:58:24.703Z",
          "wordCount": 710,
          "title": "Unveiling the potential of Graph Neural Networks for robust Intrusion Detection. (arXiv:2107.14756v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14698",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Loftin_R/0/1/0/all/0/1\">Robert Loftin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Aadirupa Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1\">Sam Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>",
          "description": "High sample complexity remains a barrier to the application of reinforcement\nlearning (RL), particularly in multi-agent systems. A large body of work has\ndemonstrated that exploration mechanisms based on the principle of optimism\nunder uncertainty can significantly improve the sample efficiency of RL in\nsingle agent tasks. This work seeks to understand the role of optimistic\nexploration in non-cooperative multi-agent settings. We will show that, in\nzero-sum games, optimistic exploration can cause the learner to waste time\nsampling parts of the state space that are irrelevant to strategic play, as\nthey can only be reached through cooperation between both players. To address\nthis issue, we introduce a formal notion of strategically efficient exploration\nin Markov games, and use this to develop two strategically efficient learning\nalgorithms for finite Markov games. We demonstrate that these methods can be\nsignificantly more sample efficient than their optimistic counterparts.",
          "link": "http://arxiv.org/abs/2107.14698",
          "publishedOn": "2021-08-02T01:58:24.697Z",
          "wordCount": 603,
          "title": "Strategically Efficient Exploration in Competitive Multi-agent Reinforcement Learning. (arXiv:2107.14698v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Budd_S/0/1/0/all/0/1\">Samuel Budd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Day_T/0/1/0/all/0/1\">Thomas Day</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simpson_J/0/1/0/all/0/1\">John Simpson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lloyd_K/0/1/0/all/0/1\">Karen Lloyd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthew_J/0/1/0/all/0/1\">Jacqueline Matthew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skelton_E/0/1/0/all/0/1\">Emily Skelton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_R/0/1/0/all/0/1\">Reza Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>",
          "description": "Probably yes. -- Supervised Deep Learning dominates performance scores for\nmany computer vision tasks and defines the state-of-the-art. However, medical\nimage analysis lags behind natural image applications. One of the many reasons\nis the lack of well annotated medical image data available to researchers. One\nof the first things researchers are told is that we require significant\nexpertise to reliably and accurately interpret and label such data. We see\nsignificant inter- and intra-observer variability between expert annotations of\nmedical images. Still, it is a widely held assumption that novice annotators\nare unable to provide useful annotations for use by clinical Deep Learning\nmodels. In this work we challenge this assumption and examine the implications\nof using a minimally trained novice labelling workforce to acquire annotations\nfor a complex medical image dataset. We study the time and cost implications of\nusing novice annotators, the raw performance of novice annotators compared to\ngold-standard expert annotators, and the downstream effects on a trained Deep\nLearning segmentation model's performance for detecting a specific congenital\nheart disease (hypoplastic left heart syndrome) in fetal ultrasound imaging.",
          "link": "http://arxiv.org/abs/2107.14682",
          "publishedOn": "2021-08-02T01:58:24.691Z",
          "wordCount": 647,
          "title": "Can non-specialists provide high quality gold standard labels in challenging modalities?. (arXiv:2107.14682v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bengar_J/0/1/0/all/0/1\">Javad Zolfaghari Bengar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raducanu_B/0/1/0/all/0/1\">Bogdan Raducanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>",
          "description": "Active learning aims to select samples to be annotated that yield the largest\nperformance improvement for the learning algorithm. Many methods approach this\nproblem by measuring the informativeness of samples and do this based on the\ncertainty of the network predictions for samples. However, it is well-known\nthat neural networks are overly confident about their prediction and are\ntherefore an untrustworthy source to assess sample informativeness. In this\npaper, we propose a new informativeness-based active learning method. Our\nmeasure is derived from the learning dynamics of a neural network. More\nprecisely we track the label assignment of the unlabeled data pool during the\ntraining of the algorithm. We capture the learning dynamics with a metric\ncalled label-dispersion, which is low when the network consistently assigns the\nsame label to the sample during the training of the network and high when the\nassigned label changes frequently. We show that label-dispersion is a promising\npredictor of the uncertainty of the network, and show on two benchmark datasets\nthat an active learning algorithm based on label-dispersion obtains excellent\nresults.",
          "link": "http://arxiv.org/abs/2107.14707",
          "publishedOn": "2021-08-02T01:58:24.676Z",
          "wordCount": 635,
          "title": "When Deep Learners Change Their Mind: Learning Dynamics for Active Learning. (arXiv:2107.14707v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1\">Guangfeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shao-Lun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1\">Tian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linqi Song</a>",
          "description": "Gradient quantization is an emerging technique in reducing communication\ncosts in distributed learning. Existing gradient quantization algorithms often\nrely on engineering heuristics or empirical observations, lacking a systematic\napproach to dynamically quantize gradients. This paper addresses this issue by\nproposing a novel dynamically quantized SGD (DQ-SGD) framework, enabling us to\ndynamically adjust the quantization scheme for each gradient descent step by\nexploring the trade-off between communication cost and convergence error. We\nderive an upper bound, tight in some cases, of the convergence error for a\nrestricted family of quantization schemes and loss functions. We design our\nDQ-SGD algorithm via minimizing the communication cost under the convergence\nerror constraints. Finally, through extensive experiments on large-scale\nnatural language processing and computer vision tasks on AG-News, CIFAR-10, and\nCIFAR-100 datasets, we demonstrate that our quantization scheme achieves better\ntradeoffs between the communication cost and learning performance than other\nstate-of-the-art gradient quantization methods.",
          "link": "http://arxiv.org/abs/2107.14575",
          "publishedOn": "2021-08-02T01:58:24.670Z",
          "wordCount": 590,
          "title": "DQ-SGD: Dynamic Quantization in SGD for Communication-Efficient Distributed Learning. (arXiv:2107.14575v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarmento_P/0/1/0/all/0/1\">Pedro Sarmento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Adarsh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carr_C/0/1/0/all/0/1\">CJ Carr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zukowski_Z/0/1/0/all/0/1\">Zack Zukowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barthet_M/0/1/0/all/0/1\">Mathieu Barthet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Hsuan Yang</a>",
          "description": "Originating in the Renaissance and burgeoning in the digital era, tablatures\nare a commonly used music notation system which provides explicit\nrepresentations of instrument fingerings rather than pitches. GuitarPro has\nestablished itself as a widely used tablature format and software enabling\nmusicians to edit and share songs for musical practice, learning, and\ncomposition. In this work, we present DadaGP, a new symbolic music dataset\ncomprising 26,181 song scores in the GuitarPro format covering 739 musical\ngenres, along with an accompanying tokenized format well-suited for generative\nsequence models such as the Transformer. The tokenized format is inspired by\nevent-based MIDI encodings, often used in symbolic music generation models. The\ndataset is released with an encoder/decoder which converts GuitarPro files to\ntokens and back. We present results of a use case in which DadaGP is used to\ntrain a Transformer-based model to generate new songs in GuitarPro format. We\ndiscuss other relevant use cases for the dataset (guitar-bass transcription,\nmusic style transfer and artist/genre classification) as well as ethical\nimplications. DadaGP opens up the possibility to train GuitarPro score\ngenerators, fine-tune models on custom data, create new styles of music,\nAI-powered songwriting apps, and human-AI improvisation.",
          "link": "http://arxiv.org/abs/2107.14653",
          "publishedOn": "2021-08-02T01:58:24.663Z",
          "wordCount": 640,
          "title": "DadaGP: A Dataset of Tokenized GuitarPro Songs for Sequence Models. (arXiv:2107.14653v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14551",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Munasinghe_T/0/1/0/all/0/1\">Thilanka Munasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasindu_H/0/1/0/all/0/1\">HR Pasindu</a>",
          "description": "We propose how a developing country like Sri Lanka can benefit from\nprivacy-enabled machine learning techniques such as Federated Learning to\ndetect road conditions using crowd-sourced data collection and proposed the\nidea of implementing a Digital Twin for the national road system in Sri Lanka.\nDeveloping countries such as Sri Lanka are far behind in implementing smart\nroad systems and smart cities compared to the developed countries. The proposed\nwork discussed in this paper matches the UN Sustainable Development Goal (SDG)\n9: \"Build Resilient Infrastructure, Promote Inclusive and Sustainable\nIndustrialization and Foster Innovation\". Our proposed work discusses how the\ngovernment and private sector vehicles that conduct routine trips to collect\ncrowd-sourced data using smartphone devices to identify the road conditions and\ndetect where the potholes, surface unevenness (roughness), and other major\ndistresses are located on the roads. We explore Mobile Edge Computing (MEC)\ntechniques that can bring machine learning intelligence closer to the edge\ndevices where produced data is stored and show how the applications of\nFederated Learning can be made to detect and improve road conditions. During\nthe second phase of this study, we plan to implement a Digital Twin for the\nroad system in Sri Lanka. We intend to use data provided by both Dedicated and\nNon-Dedicated systems in the proposed Digital Twin for the road system. As of\nwriting this paper, and best to our knowledge, there is no Digital Twin system\nimplemented for roads and other infrastructure systems in Sri Lanka. The\nproposed Digital Twin will be one of the first implementations of such systems\nin Sri Lanka. Lessons learned from this pilot project will benefit other\ndeveloping countries who wish to follow the same path and make data-driven\ndecisions.",
          "link": "http://arxiv.org/abs/2107.14551",
          "publishedOn": "2021-08-02T01:58:24.654Z",
          "wordCount": 790,
          "title": "Sensing and Mapping for Better Roads: Initial Plan for Using Federated Learning and Implementing a Digital Twin to Identify the Road Conditions in a Developing Country -- Sri Lanka. (arXiv:2107.14551v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uglov_A/0/1/0/all/0/1\">Arsenii Uglov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaev_S/0/1/0/all/0/1\">Sergei Nikolaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belov_S/0/1/0/all/0/1\">Sergei Belov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padalitsa_D/0/1/0/all/0/1\">Daniil Padalitsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenkina_T/0/1/0/all/0/1\">Tatiana Greenkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biagio_M/0/1/0/all/0/1\">Marco San Biagio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cacciatori_F/0/1/0/all/0/1\">Fabio Cacciatori</a>",
          "description": "Injection molding is one of the most popular manufacturing methods for the\nmodeling of complex plastic objects. Faster numerical simulation of the\ntechnological process would allow for faster and cheaper design cycles of new\nproducts. In this work, we propose a baseline for a data processing pipeline\nthat includes the extraction of data from Moldflow simulation projects and the\nprediction of the fill time and deflection distributions over 3-dimensional\nsurfaces using machine learning models. We propose algorithms for engineering\nof features, including information of injector gates parameters that will\nmostly affect the time for plastic to reach the particular point of the form\nfor fill time prediction, and geometrical features for deflection prediction.\nWe propose and evaluate baseline machine learning models for fill time and\ndeflection distribution prediction and provide baseline values of MSE and RMSE\nmetrics. Finally, we measure the execution time of our solution and show that\nit significantly exceeds the time of simulation with Moldflow software:\napproximately 17 times and 14 times faster for mean and median total times\nrespectively, comparing the times of all analysis stages for deflection\nprediction. Our solution has been implemented in a prototype web application\nthat was approved by the management board of Fiat Chrysler Automobiles and\nIllogic SRL. As one of the promising applications of this surrogate modelling\napproach, we envision the use of trained models as a fast objective function in\nthe task of optimization of technological parameters of the injection molding\nprocess (meaning optimal placement of gates), which could significantly aid\nengineers in this task, or even automate it.",
          "link": "http://arxiv.org/abs/2107.14574",
          "publishedOn": "2021-08-02T01:58:24.647Z",
          "wordCount": 706,
          "title": "Surrogate Modelling for Injection Molding Processes using Machine Learning. (arXiv:2107.14574v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koffas_S/0/1/0/all/0/1\">Stefanos Koffas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Mauro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picek_S/0/1/0/all/0/1\">Stjepan Picek</a>",
          "description": "Deep neural networks represent a powerful option for many real-world\napplications due to their ability to model even complex data relations.\nHowever, such neural networks can also be prohibitively expensive to train,\nmaking it common to either outsource the training process to third parties or\nuse pretrained neural networks. Unfortunately, such practices make neural\nnetworks vulnerable to various attacks, where one attack is the backdoor\nattack. In such an attack, the third party training the model may maliciously\ninject hidden behaviors into the model. Still, if a particular input (called\ntrigger) is fed into a neural network, the network will respond with a wrong\nresult.\n\nIn this work, we explore the option of backdoor attacks to automatic speech\nrecognition systems where we inject inaudible triggers. By doing so, we make\nthe backdoor attack challenging to detect for legitimate users, and thus,\npotentially more dangerous. We conduct experiments on two versions of datasets\nand three neural networks and explore the performance of our attack concerning\nthe duration, position, and type of the trigger. Our results indicate that less\nthan 1% of poisoned data is sufficient to deploy a backdoor attack and reach a\n100% attack success rate. What is more, while the trigger is inaudible, making\nit without limitations with respect to the duration of the signal, we observed\nthat even short, non-continuous triggers result in highly successful attacks.",
          "link": "http://arxiv.org/abs/2107.14569",
          "publishedOn": "2021-08-02T01:58:24.640Z",
          "wordCount": 665,
          "title": "Can You Hear It? Backdoor Attacks via Ultrasonic Triggers. (arXiv:2107.14569v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14586",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dupuy_C/0/1/0/all/0/1\">Christophe Dupuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arava_R/0/1/0/all/0/1\">Radhika Arava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>",
          "description": "Recent advances in deep learning have drastically improved performance on\nmany Natural Language Understanding (NLU) tasks. However, the data used to\ntrain NLU models may contain private information such as addresses or phone\nnumbers, particularly when drawn from human subjects. It is desirable that\nunderlying models do not expose private information contained in the training\ndata. Differentially Private Stochastic Gradient Descent (DP-SGD) has been\nproposed as a mechanism to build privacy-preserving models. However, DP-SGD can\nbe prohibitively slow to train. In this work, we propose a more efficient\nDP-SGD for training using a GPU infrastructure and apply it to fine-tuning\nmodels based on LSTM and transformer architectures. We report faster training\ntimes, alongside accuracy, theoretical privacy guarantees and success of\nMembership inference attacks for our models and observe that fine-tuning with\nproposed variant of DP-SGD can yield competitive models without significant\ndegradation in training time and improvement in privacy protection. We also\nmake observations such as looser theoretical $\\epsilon, \\delta$ can translate\ninto significant practical privacy gains.",
          "link": "http://arxiv.org/abs/2107.14586",
          "publishedOn": "2021-08-02T01:58:24.623Z",
          "wordCount": 611,
          "title": "An Efficient DP-SGD Mechanism for Large Scale NLP Models. (arXiv:2107.14586v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karn_A/0/1/0/all/0/1\">Aryan Karn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1\">Ashutosh Acharya</a>",
          "description": "We present a study of the manners by which Domain information has been\nincorporated when building models with Neural Networks. Integrating space data\nis uniquely important to the development of Knowledge understanding model, as\nwell as other fields that aid in understanding information by utilizing the\nhuman-machine interface and Reinforcement Learning. On numerous such occasions,\nmachine-based model development may profit essentially from the human\ninformation on the world encoded in an adequately exact structure. This paper\ninspects expansive ways to affect encode such information as sensible and\nmathematical limitations and portrays methods and results that came to a couple\nof subcategories under all of those methodologies.",
          "link": "http://arxiv.org/abs/2107.14613",
          "publishedOn": "2021-08-02T01:58:24.616Z",
          "wordCount": 549,
          "title": "Incorporation of Deep Neural Network & Reinforcement Learning with Domain Knowledge. (arXiv:2107.14613v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">GuoLiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyang Li</a>",
          "description": "Although attention-based Neural Machine Translation has achieved remarkable\nprogress in recent layers, it still suffers from issue of making insufficient\nuse of the output of each layer. In transformer, it only uses the top layer of\nencoder and decoder in the subsequent process, which makes it impossible to\ntake advantage of the useful information in other layers. To address this\nissue, we propose a residual tree aggregation of layers for Transformer(RTAL),\nwhich helps to fuse information across layers. Specifically, we try to fuse the\ninformation across layers by constructing a post-order binary tree. In\nadditional to the last node, we add the residual connection to the process of\ngenerating child nodes. Our model is based on the Neural Machine Translation\nmodel Transformer and we conduct our experiments on WMT14 English-to-German and\nWMT17 English-to-France translation tasks. Experimental results across language\npairs show that the proposed approach outperforms the strong baseline model\nsignificantly",
          "link": "http://arxiv.org/abs/2107.14590",
          "publishedOn": "2021-08-02T01:58:24.609Z",
          "wordCount": 584,
          "title": "Residual Tree Aggregation of Layers for Neural Machine Translation. (arXiv:2107.14590v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Attias_I/0/1/0/all/0/1\">Idan Attias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_E/0/1/0/all/0/1\">Edith Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechner_M/0/1/0/all/0/1\">Moshe Shechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stemmer_U/0/1/0/all/0/1\">Uri Stemmer</a>",
          "description": "Streaming algorithms are algorithms for processing large data streams, using\nonly a limited amount of memory. Classical streaming algorithms operate under\nthe assumption that the input stream is fixed in advance. Recently, there is a\ngrowing interest in studying streaming algorithms that provide provable\nguarantees even when the input stream is chosen by an adaptive adversary. Such\nstreaming algorithms are said to be {\\em adversarially-robust}. We propose a\nnovel framework for adversarial streaming that hybrids two recently suggested\nframeworks by Hassidim et al. (2020) and by Woodruff and Zhou (2021). These\nrecently suggested frameworks rely on very different ideas, each with its own\nstrengths and weaknesses. We combine these two frameworks (in a non-trivial\nway) into a single hybrid framework that gains from both approaches to obtain\nsuperior performances for turnstile streams.",
          "link": "http://arxiv.org/abs/2107.14527",
          "publishedOn": "2021-08-02T01:58:24.593Z",
          "wordCount": 577,
          "title": "A Framework for Adversarial Streaming via Differential Privacy and Difference Estimators. (arXiv:2107.14527v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mukhtar_S/0/1/0/all/0/1\">Shakeeb A. M. Mukhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joglekar_P/0/1/0/all/0/1\">Pushkar S. Joglekar</a>",
          "description": "One of the major problems writers and poets face is the writer's block. It is\na condition in which an author loses the ability to produce new work or\nexperiences a creative slowdown. The problem is more difficult in the context\nof poetry than prose, as in the latter case authors need not be very concise\nwhile expressing their ideas, also the various aspects such as rhyme, poetic\nmeters are not relevant for prose. One of the most effective ways to overcome\nthis writing block for poets can be, to have a prompt system, which would help\ntheir imagination and open their minds for new ideas. A prompt system can\npossibly generate one liner, two liner or full ghazals. The purpose of this\nwork is to give an ode to the Urdu, Hindi poets, and helping them start their\nnext line of poetry, a couplet or a complete ghazal considering various factors\nlike rhymes, refrain, and meters. The result will help aspiring poets to get\nnew ideas and help them overcome writer's block by auto-generating pieces of\npoetry using Deep Learning techniques. A concern with creative works like this,\nespecially in the literary context, is to ensure that the output is not\nplagiarized. This work also addresses the concern and makes sure that the\nresulting odes are not exact match with input data using parameters like\ntemperature and manual plagiarism check against input corpus. To the best of\nour knowledge, although the automatic text generation problem has been studied\nquite extensively in the literature, the specific problem of Urdu, Hindi poetry\ngeneration has not been explored much. Apart from developing system to\nauto-generate Urdu, Hindi poetry, another key contribution of our work is to\ncreate a cleaned and preprocessed corpus of Urdu, Hindi poetry (derived from\nauthentic resources) and making it freely available for researchers in the\narea.",
          "link": "http://arxiv.org/abs/2107.14587",
          "publishedOn": "2021-08-02T01:58:24.587Z",
          "wordCount": 741,
          "title": "Urdu & Hindi Poetry Generation using Neural Networks. (arXiv:2107.14587v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Apostolova_E/0/1/0/all/0/1\">Emilia Apostolova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karim_F/0/1/0/all/0/1\">Fazle Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muscioni_G/0/1/0/all/0/1\">Guido Muscioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_A/0/1/0/all/0/1\">Anubhav Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clyman_J/0/1/0/all/0/1\">Jeffrey Clyman</a>",
          "description": "In this work, we modify and apply self-supervision techniques to the domain\nof medical health insurance claims. We model patients' healthcare claims\nhistory analogous to free-text narratives, and introduce pre-trained `prior\nknowledge', later utilized for patient outcome predictions on a challenging\ntask: predicting Covid-19 hospitalization, given a patient's pre-Covid-19\ninsurance claims history. Results suggest that pre-training on insurance claims\nnot only produces better prediction performance, but, more importantly,\nimproves the model's `clinical trustworthiness' and model\nstability/reliability.",
          "link": "http://arxiv.org/abs/2107.14591",
          "publishedOn": "2021-08-02T01:58:24.580Z",
          "wordCount": 569,
          "title": "Self-supervision for health insurance claims data: a Covid-19 use case. (arXiv:2107.14591v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1\">Moritz Knolle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1\">Dmitrii Usynin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1\">Alexander Ziller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus R. Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>",
          "description": "The application of differential privacy to the training of deep neural\nnetworks holds the promise of allowing large-scale (decentralized) use of\nsensitive data while providing rigorous privacy guarantees to the individual.\nThe predominant approach to differentially private training of neural networks\nis DP-SGD, which relies on norm-based gradient clipping as a method for\nbounding sensitivity, followed by the addition of appropriately calibrated\nGaussian noise. In this work we propose NeuralDP, a technique for privatising\nactivations of some layer within a neural network, which by the post-processing\nproperties of differential privacy yields a differentially private network. We\nexperimentally demonstrate on two datasets (MNIST and Pediatric Pneumonia\nDataset (PPD)) that our method offers substantially improved privacy-utility\ntrade-offs compared to DP-SGD.",
          "link": "http://arxiv.org/abs/2107.14582",
          "publishedOn": "2021-08-02T01:58:24.574Z",
          "wordCount": 555,
          "title": "NeuralDP Differentially private neural networks by design. (arXiv:2107.14582v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korzeniowski_F/0/1/0/all/0/1\">Filip Korzeniowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oramas_S/0/1/0/all/0/1\">Sergio Oramas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouyon_F/0/1/0/all/0/1\">Fabien Gouyon</a>",
          "description": "Artist similarity plays an important role in organizing, understanding, and\nsubsequently, facilitating discovery in large collections of music. In this\npaper, we present a hybrid approach to computing similarity between artists\nusing graph neural networks trained with triplet loss. The novelty of using a\ngraph neural network architecture is to combine the topology of a graph of\nartist connections with content features to embed artists into a vector space\nthat encodes similarity. To evaluate the proposed method, we compile the new\nOLGA dataset, which contains artist similarities from AllMusic, together with\ncontent features from AcousticBrainz. With 17,673 artists, this is the largest\nacademic artist similarity dataset that includes content-based features to\ndate. Moreover, we also showcase the scalability of our approach by\nexperimenting with a much larger proprietary dataset. Results show the\nsuperiority of the proposed approach over current state-of-the-art methods for\nmusic similarity. Finally, we hope that the OLGA dataset will facilitate\nresearch on data-driven models for artist similarity.",
          "link": "http://arxiv.org/abs/2107.14541",
          "publishedOn": "2021-08-02T01:58:24.568Z",
          "wordCount": 614,
          "title": "Artist Similarity with Graph Neural Networks. (arXiv:2107.14541v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kobler_R/0/1/0/all/0/1\">Reinmar J. Kobler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirayama_J/0/1/0/all/0/1\">Jun-Ichiro Hirayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopes_Dias_L/0/1/0/all/0/1\">Lea Hehenberger Catarina Lopes-Dias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Putz_G/0/1/0/all/0/1\">Gernot R. M&#xfc;ller-Putz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawanabe_M/0/1/0/all/0/1\">Motoaki Kawanabe</a>",
          "description": "Riemannian tangent space methods offer state-of-the-art performance in\nmagnetoencephalography (MEG) and electroencephalography (EEG) based\napplications such as brain-computer interfaces and biomarker development. One\nlimitation, particularly relevant for biomarker development, is limited model\ninterpretability compared to established component-based methods. Here, we\npropose a method to transform the parameters of linear tangent space models\ninto interpretable patterns. Using typical assumptions, we show that this\napproach identifies the true patterns of latent sources, encoding a target\nsignal. In simulations and two real MEG and EEG datasets, we demonstrate the\nvalidity of the proposed approach and investigate its behavior when the model\nassumptions are violated. Our results confirm that Riemannian tangent space\nmethods are robust to differences in the source patterns across observations.\nWe found that this robustness property also transfers to the associated\npatterns.",
          "link": "http://arxiv.org/abs/2107.14398",
          "publishedOn": "2021-08-02T01:58:24.561Z",
          "wordCount": 580,
          "title": "On the interpretation of linear Riemannian tangent space model parameters in M/EEG. (arXiv:2107.14398v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14280",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Taylor_M/0/1/0/all/0/1\">Michael G. Taylor</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Nandy_A/0/1/0/all/0/1\">Aditya Nandy</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Lu_C/0/1/0/all/0/1\">Connie C. Lu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kulik_H/0/1/0/all/0/1\">Heather J. Kulik</a>",
          "description": "The rational tailoring of transition metal complexes is necessary to address\noutstanding challenges in energy utilization and storage. Heterobimetallic\ntransition metal complexes that exhibit metal-metal bonding in stacked \"double\ndecker\" ligand structures are an emerging, attractive platform for catalysis,\nbut their properties are challenging to predict prior to laborious synthetic\nefforts. We demonstrate an alternative, data-driven approach to uncovering\nstructure-property relationships for rational bimetallic complex design. We\ntailor graph-based representations of the metal-local environment for these\nheterobimetallic complexes for use in training of multiple linear regression\nand kernel ridge regression (KRR) models. Focusing on oxidation potentials, we\nobtain a set of 28 experimentally characterized complexes to develop a multiple\nlinear regression model. On this training set, we achieve good accuracy (mean\nabsolute error, MAE, of 0.25 V) and preserve transferability to unseen\nexperimental data with a new ligand structure. We trained a KRR model on a\nsubset of 330 structurally characterized heterobimetallics to predict the\ndegree of metal-metal bonding. This KRR model predicts relative metal-metal\nbond lengths in the test set to within 5%, and analysis of key features reveals\nthe fundamental atomic contributions (e.g., the valence electron configuration)\nthat most strongly influence the behavior of complexes. Our work provides\nguidance for rational bimetallic design, suggesting that properties including\nthe formal shortness ratio should be transferable from one period to another.",
          "link": "http://arxiv.org/abs/2107.14280",
          "publishedOn": "2021-08-02T01:58:24.551Z",
          "wordCount": 668,
          "title": "Deciphering Cryptic Behavior in Bimetallic Transition Metal Complexes with Machine Learning. (arXiv:2107.14280v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14465",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1\">Quoc Phong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhaoxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Low_B/0/1/0/all/0/1\">Bryan Kian Hsiang Low</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaillet_P/0/1/0/all/0/1\">Patrick Jaillet</a>",
          "description": "Information-based Bayesian optimization (BO) algorithms have achieved\nstate-of-the-art performance in optimizing a black-box objective function.\nHowever, they usually require several approximations or simplifying assumptions\n(without clearly understanding their effects on the BO performance) and/or\ntheir generalization to batch BO is computationally unwieldy, especially with\nan increasing batch size. To alleviate these issues, this paper presents a\nnovel trusted-maximizers entropy search (TES) acquisition function: It measures\nhow much an input query contributes to the information gain on the maximizer\nover a finite set of trusted maximizers, i.e., inputs optimizing functions that\nare sampled from the Gaussian process posterior belief of the objective\nfunction. Evaluating TES requires either only a stochastic approximation with\nsampling or a deterministic approximation with expectation propagation, both of\nwhich are investigated and empirically evaluated using synthetic benchmark\nobjective functions and real-world optimization problems, e.g., hyperparameter\ntuning of a convolutional neural network and synthesizing 'physically\nrealizable' faces to fool a black-box face recognition system. Though TES can\nnaturally be generalized to a batch variant with either approximation, the\nlatter is amenable to be scaled to a much larger batch size in our experiments.",
          "link": "http://arxiv.org/abs/2107.14465",
          "publishedOn": "2021-08-02T01:58:24.545Z",
          "wordCount": 634,
          "title": "Trusted-Maximizers Entropy Search for Efficient Bayesian Optimization. (arXiv:2107.14465v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akman_A/0/1/0/all/0/1\">Alican Akman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coppock_H/0/1/0/all/0/1\">Harry Coppock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaskell_A/0/1/0/all/0/1\">Alexander Gaskell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzirakis_P/0/1/0/all/0/1\">Panagiotis Tzirakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_L/0/1/0/all/0/1\">Lyn Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>",
          "description": "We report on cross-running the recent COVID-19 Identification ResNet (CIdeR)\non the two Interspeech 2021 COVID-19 diagnosis from cough and speech audio\nchallenges: ComParE and DiCOVA. CIdeR is an end-to-end deep learning neural\nnetwork originally designed to classify whether an individual is COVID-positive\nor COVID-negative based on coughing and breathing audio recordings from a\npublished crowdsourced dataset. In the current study, we demonstrate the\npotential of CIdeR at binary COVID-19 diagnosis from both the COVID-19 Cough\nand Speech Sub-Challenges of INTERSPEECH 2021, ComParE and DiCOVA. CIdeR\nachieves significant improvements over several baselines.",
          "link": "http://arxiv.org/abs/2107.14549",
          "publishedOn": "2021-08-02T01:58:24.525Z",
          "wordCount": 596,
          "title": "Evaluating the COVID-19 Identification ResNet (CIdeR) on the INTERSPEECH COVID-19 from Audio Challenges. (arXiv:2107.14549v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1\">Javier Naranjo-Alcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1\">Sergi Perez-Castanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1\">Pedro Zuccarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1\">Francesc J. Ferri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1\">Maximo Cobos</a>",
          "description": "Sound event localisation and detection (SELD) is a problem in the field of\nautomatic listening that aims at the temporal detection and localisation\n(direction of arrival estimation) of sound events within an audio clip, usually\nof long duration. Due to the amount of data present in the datasets related to\nthis problem, solutions based on deep learning have positioned themselves at\nthe top of the state of the art. Most solutions are based on 2D representations\nof the audio (different spectrograms) that are processed by a\nconvolutional-recurrent network. The motivation of this submission is to study\nthe squeeze-excitation technique in the convolutional part of the network and\nhow it improves the performance of the system. This study is based on the one\ncarried out by the same team last year. This year, it has been decided to study\nhow this technique improves each of the datasets (last year only the MIC\ndataset was studied). This modification shows an improvement in the performance\nof the system compared to the baseline using MIC dataset.",
          "link": "http://arxiv.org/abs/2107.14561",
          "publishedOn": "2021-08-02T01:58:24.506Z",
          "wordCount": 629,
          "title": "TASK3 DCASE2021 Challenge: Sound event localization and detection using squeeze-excitation residual CNNs. (arXiv:2107.14561v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+ONeill_L/0/1/0/all/0/1\">Lachlan O&#x27;Neill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angus_S/0/1/0/all/0/1\">Simon Angus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgohain_S/0/1/0/all/0/1\">Satya Borgohain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chmait_N/0/1/0/all/0/1\">Nader Chmait</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dowe_D/0/1/0/all/0/1\">David L. Dowe</a>",
          "description": "As the discipline has evolved, research in machine learning has been focused\nmore and more on creating more powerful neural networks, without regard for the\ninterpretability of these networks. Such \"black-box models\" yield\nstate-of-the-art results, but we cannot understand why they make a particular\ndecision or prediction. Sometimes this is acceptable, but often it is not.\n\nWe propose a novel architecture, Regression Networks, which combines the\npower of neural networks with the understandability of regression analysis.\nWhile some methods for combining these exist in the literature, our\narchitecture generalizes these approaches by taking interactions into account,\noffering the power of a dense neural network without forsaking\ninterpretability. We demonstrate that the models exceed the state-of-the-art\nperformance of interpretable models on several benchmark datasets, matching the\npower of a dense neural network. Finally, we discuss how these techniques can\nbe generalized to other neural architectures, such as convolutional and\nrecurrent neural networks.",
          "link": "http://arxiv.org/abs/2107.14417",
          "publishedOn": "2021-08-02T01:58:24.479Z",
          "wordCount": 586,
          "title": "Creating Powerful and Interpretable Models withRegression Networks. (arXiv:2107.14417v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomes_G/0/1/0/all/0/1\">Gecynalda Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "Neural Networks have been applied for time series prediction with good\nexperimental results that indicate the high capacity to approximate functions\nwith good precision. Most neural models used in these applications use\nactivation functions with fixed parameters. However, it is known that the\nchoice of activation function strongly influences the complexity and\nperformance of the neural network and that a limited number of activation\nfunctions have been used. In this work, we propose the use of a family of free\nparameter asymmetric activation functions for neural networks and show that\nthis family of defined activation functions satisfies the requirements of the\nuniversal approximation theorem. A methodology for the global optimization of\nthis family of activation functions with free parameter and the weights of the\nconnections between the processing units of the neural network is used. The\ncentral idea of the proposed methodology is to simultaneously optimize the\nweights and the activation function used in a multilayer perceptron network\n(MLP), through an approach that combines the advantages of simulated annealing,\ntabu search and a local learning algorithm, with the purpose of improving\nperformance in the adjustment and forecasting of time series. We chose two\nlearning algorithms: backpropagation with the term momentum (BPM) and\nLevenbergMarquardt (LM).",
          "link": "http://arxiv.org/abs/2107.14370",
          "publishedOn": "2021-08-02T01:58:24.469Z",
          "wordCount": 654,
          "title": "Otimizacao de pesos e funcoes de ativacao de redes neurais aplicadas na previsao de series temporais. (arXiv:2107.14370v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14412",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bajcsy_A/0/1/0/all/0/1\">Andrea Bajcsy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leung_K/0/1/0/all/0/1\">Karen Leung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmerling_E/0/1/0/all/0/1\">Edward Schmerling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>",
          "description": "As safety-critical autonomous vehicles (AVs) will soon become pervasive in\nour society, a number of safety concepts for trusted AV deployment have been\nrecently proposed throughout industry and academia. Yet, agreeing upon an\n\"appropriate\" safety concept is still an elusive task. In this paper, we\nadvocate for the use of Hamilton Jacobi (HJ) reachability as a unifying\nmathematical framework for comparing existing safety concepts, and propose ways\nto expand its modeling premises in a data-driven fashion. Specifically, we show\nthat (i) existing predominant safety concepts can be embedded in the HJ\nreachability framework, thereby enabling a common language for comparing and\ncontrasting modeling assumptions, and (ii) HJ reachability can serve as an\ninductive bias to effectively reason, in a data-driven context, about two\ncritical, yet often overlooked aspects of safety: responsibility and\ncontext-dependency.",
          "link": "http://arxiv.org/abs/2107.14412",
          "publishedOn": "2021-08-02T01:58:24.416Z",
          "wordCount": 578,
          "title": "Towards the Unification and Data-Driven Synthesis of Autonomous Vehicle Safety Concepts. (arXiv:2107.14412v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Safarani_S/0/1/0/all/0/1\">Shahd Safarani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nix_A/0/1/0/all/0/1\">Arne Nix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willeke_K/0/1/0/all/0/1\">Konstantin Willeke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadena_S/0/1/0/all/0/1\">Santiago A. Cadena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Restivo_K/0/1/0/all/0/1\">Kelli Restivo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denfield_G/0/1/0/all/0/1\">George Denfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_A/0/1/0/all/0/1\">Andreas S. Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinz_F/0/1/0/all/0/1\">Fabian H. Sinz</a>",
          "description": "Deep neural networks set the state-of-the-art across many tasks in computer\nvision, but their generalization ability to image distortions is surprisingly\nfragile. In contrast, the mammalian visual system is robust to a wide range of\nperturbations. Recent work suggests that this generalization ability can be\nexplained by useful inductive biases encoded in the representations of visual\nstimuli throughout the visual cortex. Here, we successfully leveraged these\ninductive biases with a multi-task learning approach: we jointly trained a deep\nnetwork to perform image classification and to predict neural activity in\nmacaque primary visual cortex (V1). We measured the out-of-distribution\ngeneralization abilities of our network by testing its robustness to image\ndistortions. We found that co-training on monkey V1 data leads to increased\nrobustness despite the absence of those distortions during training.\nAdditionally, we showed that our network's robustness is very close to that of\nan Oracle network where parts of the architecture are directly trained on noisy\nimages. Our results also demonstrated that the network's representations become\nmore brain-like as their robustness improves. Using a novel constrained\nreconstruction analysis, we investigated what makes our brain-regularized\nnetwork more robust. We found that our co-trained network is more sensitive to\ncontent than noise when compared to a Baseline network that we trained for\nimage classification alone. Using DeepGaze-predicted saliency maps for ImageNet\nimages, we found that our monkey co-trained network tends to be more sensitive\nto salient regions in a scene, reminiscent of existing theories on the role of\nV1 in the detection of object borders and bottom-up saliency. Overall, our work\nexpands the promising research avenue of transferring inductive biases from the\nbrain, and provides a novel analysis of the effects of our transfer.",
          "link": "http://arxiv.org/abs/2107.14344",
          "publishedOn": "2021-08-02T01:58:24.397Z",
          "wordCount": 745,
          "title": "Towards robust vision by multi-task learning on monkey visual cortex. (arXiv:2107.14344v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tongzhou Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1\">Fanbo Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Derek Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Stone Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhiwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>",
          "description": "Learning generalizable manipulation skills is central for robots to achieve\ntask automation in environments with endless scene and object variations.\nHowever, existing robot learning environments are limited in both scale and\ndiversity of 3D assets (especially of articulated objects), making it difficult\nto train and evaluate the generalization ability of agents over novel objects.\nIn this work, we focus on object-level generalization and propose SAPIEN\nManipulation Skill Benchmark (abbreviated as ManiSkill), a large-scale\nlearning-from-demonstrations benchmark for articulated object manipulation with\nvisual input (point cloud and image). ManiSkill supports object-level\nvariations by utilizing a rich and diverse set of articulated objects, and each\ntask is carefully designed for learning manipulations on a single category of\nobjects. We equip ManiSkill with high-quality demonstrations to facilitate\nlearning-from-demonstrations approaches and perform evaluations on common\nbaseline algorithms. We believe ManiSkill can encourage the robot learning\ncommunity to explore more on learning generalizable object manipulation skills.",
          "link": "http://arxiv.org/abs/2107.14483",
          "publishedOn": "2021-08-02T01:58:24.381Z",
          "wordCount": 606,
          "title": "ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills. (arXiv:2107.14483v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14235",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Iaquinta_A/0/1/0/all/0/1\">Amanda Ferrari Iaquinta</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Silva_A/0/1/0/all/0/1\">Ana Carolina de Sousa Silva</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Junior_A/0/1/0/all/0/1\">Aldrumont Ferraz J&#xfa;nior</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Toledo_J/0/1/0/all/0/1\">Jessica Monique de Toledo</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Atzingen_G/0/1/0/all/0/1\">Gustavo Voltani von Atzingen</a>",
          "description": "The electrical signal emitted by the eyes movement produces a very strong\nartifact on EEG signaldue to its close proximity to the sensors and abundance\nof occurrence. In the context of detectingeye blink artifacts in EEG waveforms\nfor further removal and signal purification, multiple strategieswhere proposed\nin the literature. Most commonly applied methods require the use of a large\nnumberof electrodes, complex equipment for sampling and processing data. The\ngoal of this work is to createa reliable and user independent algorithm for\ndetecting and removing eye blink in EEG signals usingCNN (convolutional neural\nnetwork). For training and validation, three sets of public EEG data wereused.\nAll three sets contain samples obtained while the recruited subjects performed\nassigned tasksthat included blink voluntarily in specific moments, watch a\nvideo and read an article. The modelused in this study was able to have an\nembracing understanding of all the features that distinguish atrivial EEG\nsignal from a signal contaminated with eye blink artifacts without being\noverfitted byspecific features that only occurred in the situations when the\nsignals were registered.",
          "link": "http://arxiv.org/abs/2107.14235",
          "publishedOn": "2021-08-02T01:58:24.374Z",
          "wordCount": 628,
          "title": "EEG multipurpose eye blink detector using convolutional neural network. (arXiv:2107.14235v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yun Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongchao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1\">Suo Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1\">Chunyang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Huanjun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1\">Lihong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjie Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yixiang Mu</a>",
          "description": "We develop a novel framework that adds the regularizers of the sparse group\nlasso to a family of adaptive optimizers in deep learning, such as Momentum,\nAdagrad, Adam, AMSGrad, AdaHessian, and create a new class of optimizers, which\nare named Group Momentum, Group Adagrad, Group Adam, Group AMSGrad and Group\nAdaHessian, etc., accordingly. We establish theoretically proven convergence\nguarantees in the stochastic convex settings, based on primal-dual methods. We\nevaluate the regularized effect of our new optimizers on three large-scale\nreal-world ad click datasets with state-of-the-art deep learning models. The\nexperimental results reveal that compared with the original optimizers with the\npost-processing procedure which uses the magnitude pruning method, the\nperformance of the models can be significantly improved on the same sparsity\nlevel. Furthermore, in comparison to the cases without magnitude pruning, our\nmethods can achieve extremely high sparsity with significantly better or highly\ncompetitive performance.",
          "link": "http://arxiv.org/abs/2107.14432",
          "publishedOn": "2021-08-02T01:58:24.368Z",
          "wordCount": 599,
          "title": "Adaptive Optimizers with Sparse Group Lasso for Neural Networks in CTR Prediction. (arXiv:2107.14432v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1\">Tianxiang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuchen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>",
          "description": "The existence of redundancy in Convolutional Neural Networks (CNNs) enables\nus to remove some filters/channels with acceptable performance drops. However,\nthe training objective of CNNs usually tends to minimize an accuracy-related\nloss function without any attention paid to the redundancy, making the\nredundancy distribute randomly on all the filters, such that removing any of\nthem may trigger information loss and accuracy drop, necessitating a following\nfinetuning step for recovery. In this paper, we propose to manipulate the\nredundancy during training to facilitate network pruning. To this end, we\npropose a novel Centripetal SGD (C-SGD) to make some filters identical,\nresulting in ideal redundancy patterns, as such filters become purely redundant\ndue to their duplicates; hence removing them does not harm the network. As\nshown on CIFAR and ImageNet, C-SGD delivers better performance because the\nredundancy is better organized, compared to the existing methods. The\nefficiency also characterizes C-SGD because it is as fast as regular SGD,\nrequires no finetuning, and can be conducted simultaneously on all the layers\neven in very deep CNNs. Besides, C-SGD can improve the accuracy of CNNs by\nfirst training a model with the same architecture but wider layers then\nsqueezing it into the original width.",
          "link": "http://arxiv.org/abs/2107.14444",
          "publishedOn": "2021-08-02T01:58:24.348Z",
          "wordCount": 666,
          "title": "Manipulating Identical Filter Redundancy for Efficient Pruning on Deep and Complicated CNN. (arXiv:2107.14444v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14442",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1\">Marina Meil&#x103;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1\">Hanyu Zhang</a>",
          "description": "We address the problem of validating the ouput of clustering algorithms.\nGiven data $\\mathcal{D}$ and a partition $\\mathcal{C}$ of these data into $K$\nclusters, when can we say that the clusters obtained are correct or meaningful\nfor the data? This paper introduces a paradigm in which a clustering\n$\\mathcal{C}$ is considered meaningful if it is good with respect to a loss\nfunction such as the K-means distortion, and stable, i.e. the only good\nclustering up to small perturbations. Furthermore, we present a generic method\nto obtain post-inference guarantees of near-optimality and stability for a\nclustering $\\mathcal{C}$. The method can be instantiated for a variety of\nclustering criteria (also called loss functions) for which convex relaxations\nexist. Obtaining the guarantees amounts to solving a convex optimization\nproblem. We demonstrate the practical relevance of this method by obtaining\nguarantees for the K-means and the Normalized Cut clustering criteria on\nrealistic data sets. We also prove that asymptotic instability implies finite\nsample instability w.h.p., allowing inferences about the population\nclusterability from a sample. The guarantees do not depend on any\ndistributional assumptions, but they depend on the data set $\\mathcal{D}$\nadmitting a stable clustering.",
          "link": "http://arxiv.org/abs/2107.14442",
          "publishedOn": "2021-08-02T01:58:24.323Z",
          "wordCount": 617,
          "title": "Distribution free optimality intervals for clustering. (arXiv:2107.14442v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bothmann_L/0/1/0/all/0/1\">Ludwig Bothmann</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Strickroth_S/0/1/0/all/0/1\">Sven Strickroth</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Casalicchio_G/0/1/0/all/0/1\">Giuseppe Casalicchio</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1\">David R&#xfc;gamer</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1\">Marius Lindauer</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Scheipl_F/0/1/0/all/0/1\">Fabian Scheipl</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a> (1) ((1) Department of Statistics, Ludwig-Maximilians-Universit&#xe4;t M&#xfc;nchen, Germany, (2) Institute of Computer Science, Ludwig-Maximilians-Universit&#xe4;t M&#xfc;nchen, Germany, (3) Institute of Information Process, Leibniz University Hannover, Germany)",
          "description": "Education should not be a privilege but a common good. It should be openly\naccessible to everyone, with as few barriers as possible; even more so for key\ntechnologies such as Machine Learning (ML) and Data Science (DS). Open\nEducational Resources (OER) are a crucial factor for greater educational\nequity. In this paper, we describe the specific requirements for OER in ML and\nDS and argue that it is especially important for these fields to make source\nfiles publicly available, leading to Open Source Educational Resources (OSER).\nWe present our view on the collaborative development of OSER, the challenges\nthis poses, and first steps towards their solutions. We outline how OSER can be\nused for blended learning scenarios and share our experiences in university\neducation. Finally, we discuss additional challenges such as credit assignment\nor granting certificates.",
          "link": "http://arxiv.org/abs/2107.14330",
          "publishedOn": "2021-08-02T01:58:24.310Z",
          "wordCount": 619,
          "title": "Developing Open Source Educational Resources for Machine Learning and Data Science. (arXiv:2107.14330v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14261",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Convery_O/0/1/0/all/0/1\">Owen Convery</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Smith_L/0/1/0/all/0/1\">Lewis Smith</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hanuka_A/0/1/0/all/0/1\">Adi Hanuka</a>",
          "description": "Virtual Diagnostic (VD) is a deep learning tool that can be used to predict a\ndiagnostic output. VDs are especially useful in systems where measuring the\noutput is invasive, limited, costly or runs the risk of damaging the output.\nGiven a prediction, it is necessary to relay how reliable that prediction is.\nThis is known as 'uncertainty quantification' of a prediction. In this paper,\nwe use ensemble methods and quantile regression neural networks to explore\ndifferent ways of creating and analyzing prediction's uncertainty on\nexperimental data from the Linac Coherent Light Source at SLAC. We aim to\naccurately and confidently predict the current profile or longitudinal phase\nspace images of the electron beam. The ability to make informed decisions under\nuncertainty is crucial for reliable deployment of deep learning tools on\nsafety-critical systems as particle accelerators.",
          "link": "http://arxiv.org/abs/2107.14261",
          "publishedOn": "2021-08-02T01:58:24.285Z",
          "wordCount": 578,
          "title": "Quantifying Uncertainty for Machine Learning Based Diagnostic. (arXiv:2107.14261v1 [physics.acc-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14309",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Revay_M/0/1/0/all/0/1\">Max Revay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Umenberger_J/0/1/0/all/0/1\">Jack Umenberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manchester_I/0/1/0/all/0/1\">Ian R. Manchester</a>",
          "description": "This paper proposes methods for identification of large-scale networked\nsystems with guarantees that the resulting model will be contracting -- a\nstrong form of nonlinear stability -- and/or monotone, i.e. order relations\nbetween states are preserved. The main challenges that we address are:\nsimultaneously searching for model parameters and a certificate of stability,\nand scalability to networks with hundreds or thousands of nodes. We propose a\nmodel set that admits convex constraints for stability and monotonicity, and\nhas a separable structure that allows distributed identification via the\nalternating directions method of multipliers (ADMM). The performance and\nscalability of the approach is illustrated on a variety of linear and\nnon-linear case studies, including a nonlinear traffic network with a\n200-dimensional state space.",
          "link": "http://arxiv.org/abs/2107.14309",
          "publishedOn": "2021-08-02T01:58:24.279Z",
          "wordCount": 584,
          "title": "Distributed Identification of Contracting and/or Monotone Network Dynamics. (arXiv:2107.14309v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Casamitjana_A/0/1/0/all/0/1\">Adri&#xe0; Casamitjana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1\">Matteo Mancini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iglesias_J/0/1/0/all/0/1\">Juan Eugenio Iglesias</a>",
          "description": "Nonlinear inter-modality registration is often challenging due to the lack of\nobjective functions that are good proxies for alignment. Here we propose a\nsynthesis-by-registration method to convert this problem into an easier\nintra-modality task. We introduce a registration loss for weakly supervised\nimage translation between domains that does not require perfectly aligned\ntraining data. This loss capitalises on a registration U-Net with frozen\nweights, to drive a synthesis CNN towards the desired translation. We\ncomplement this loss with a structure preserving constraint based on\ncontrastive learning, which prevents blurring and content shifts due to\noverfitting. We apply this method to the registration of histological sections\nto MRI slices, a key step in 3D histology reconstruction. Results on two\ndifferent public datasets show improvements over registration based on mutual\ninformation (13% reduction in landmark error) and synthesis-based algorithms\nsuch as CycleGAN (11% reduction), and are comparable to a registration CNN with\nlabel supervision.",
          "link": "http://arxiv.org/abs/2107.14449",
          "publishedOn": "2021-08-02T01:58:24.267Z",
          "wordCount": 610,
          "title": "Synth-by-Reg (SbR): Contrastive learning for synthesis-based registration of paired images. (arXiv:2107.14449v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mathur_L/0/1/0/all/0/1\">Leena Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spitale_M/0/1/0/all/0/1\">Micol Spitale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_H/0/1/0/all/0/1\">Hao Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jieyun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mataric_M/0/1/0/all/0/1\">Maja J Matari&#x107;</a>",
          "description": "Virtual and robotic agents capable of perceiving human empathy have the\npotential to participate in engaging and meaningful human-machine interactions\nthat support human well-being. Prior research in computational empathy has\nfocused on designing empathic agents that use verbal and nonverbal behaviors to\nsimulate empathy and attempt to elicit empathic responses from humans. The\nchallenge of developing agents with the ability to automatically perceive\nelicited empathy in humans remains largely unexplored. Our paper presents the\nfirst approach to modeling user empathy elicited during interactions with a\nrobotic agent. We collected a new dataset from the novel interaction context of\nparticipants listening to a robot storyteller (46 participants, 6.9 hours of\nvideo). After each storytelling interaction, participants answered a\nquestionnaire that assessed their level of elicited empathy during the\ninteraction with the robot. We conducted experiments with 8 classical machine\nlearning models and 2 deep learning models (long short-term memory networks and\ntemporal convolutional networks) to detect empathy by leveraging patterns in\nparticipants' visual behaviors while they were listening to the robot\nstoryteller. Our highest-performing approach, based on XGBoost, achieved an\naccuracy of 69% and AUC of 72% when detecting empathy in videos. We contribute\ninsights regarding modeling approaches and visual features for automated\nempathy detection. Our research informs and motivates future development of\nempathy perception models that can be leveraged by virtual and robotic agents\nduring human-machine interactions.",
          "link": "http://arxiv.org/abs/2107.14345",
          "publishedOn": "2021-08-02T01:58:24.212Z",
          "wordCount": 685,
          "title": "Modeling User Empathy Elicited by a Robot Storyteller. (arXiv:2107.14345v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14457",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nadali_A/0/1/0/all/0/1\">Alireza Nadali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebadzadeh_M/0/1/0/all/0/1\">Mohammad Mehdi Ebadzadeh</a>",
          "description": "In recent years, there have been many deep structures for Reinforcement\nLearning, mainly for value function estimation and representations. These\nmethods achieved great success in Atari 2600 domain. In this paper, we propose\nan improved architecture based upon Dueling Networks, in this architecture,\nthere are two separate estimators, one approximate the state value function and\nthe other, state advantage function. This improvement based on Maximum Entropy,\nshows better policy evaluation compared to the original network and other\nvalue-based architectures in Atari domain.",
          "link": "http://arxiv.org/abs/2107.14457",
          "publishedOn": "2021-08-02T01:58:24.116Z",
          "wordCount": 506,
          "title": "Maximum Entropy Dueling Network Architecture. (arXiv:2107.14457v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14410",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Zhu_L/0/1/0/all/0/1\">Liao Zhu</a>",
          "description": "Modern evolvements of the technologies have been leading to a profound\ninfluence on the financial market. The introduction of constituents like\nExchange-Traded Funds, and the wide-use of advanced technologies such as\nalgorithmic trading, results in a boom of the data which provides more\nopportunities to reveal deeper insights. However, traditional statistical\nmethods always suffer from the high-dimensional, high-correlation, and\ntime-varying instinct of the financial data. In this dissertation, we focus on\ndeveloping techniques to stress these difficulties. With the proposed\nmethodologies, we can have more interpretable models, clearer explanations, and\nbetter predictions.",
          "link": "http://arxiv.org/abs/2107.14410",
          "publishedOn": "2021-08-02T01:58:24.110Z",
          "wordCount": 533,
          "title": "The Adaptive Multi-Factor Model and the Financial Market. (arXiv:2107.14410v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Razavi_M/0/1/0/all/0/1\">Moein Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janfaza_V/0/1/0/all/0/1\">Vahid Janfaza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamauchi_T/0/1/0/all/0/1\">Takashi Yamauchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leontyev_A/0/1/0/all/0/1\">Anton Leontyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Longmire_Monford_S/0/1/0/all/0/1\">Shanle Longmire-Monford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orr_J/0/1/0/all/0/1\">Joseph Orr</a>",
          "description": "Background: The human mind is multimodal. Yet most behavioral studies rely on\ncentury-old measures such as task accuracy and latency. To create a better\nunderstanding of human behavior and brain functionality, we should introduce\nother measures and analyze behavior from various aspects. However, it is\ntechnically complex and costly to design and implement the experiments that\nrecord multiple measures. To address this issue, a platform that allows\nsynchronizing multiple measures from human behavior is needed. Method: This\npaper introduces an opensource platform named OpenSync, which can be used to\nsynchronize multiple measures in neuroscience experiments. This platform helps\nto automatically integrate, synchronize and record physiological measures\n(e.g., electroencephalogram (EEG), galvanic skin response (GSR), eye-tracking,\nbody motion, etc.), user input response (e.g., from mouse, keyboard, joystick,\netc.), and task-related information (stimulus markers). In this paper, we\nexplain the structure and details of OpenSync, provide two case studies in\nPsychoPy and Unity. Comparison with existing tools: Unlike proprietary systems\n(e.g., iMotions), OpenSync is free and it can be used inside any opensource\nexperiment design software (e.g., PsychoPy, OpenSesame, Unity, etc.,\nhttps://pypi.org/project/OpenSync/ and\nhttps://github.com/moeinrazavi/OpenSync_Unity). Results: Our experimental\nresults show that the OpenSync platform is able to synchronize multiple\nmeasures with microsecond resolution.",
          "link": "http://arxiv.org/abs/2107.14367",
          "publishedOn": "2021-08-02T01:58:24.103Z",
          "wordCount": 659,
          "title": "OpenSync: An opensource platform for synchronizing multiple measures in neuroscience experiments. (arXiv:2107.14367v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14324",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_T/0/1/0/all/0/1\">Tingran Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Buchanan_S/0/1/0/all/0/1\">Sam Buchanan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gilboa_D/0/1/0/all/0/1\">Dar Gilboa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wright_J/0/1/0/all/0/1\">John Wright</a>",
          "description": "Data with low-dimensional nonlinear structure are ubiquitous in engineering\nand scientific problems. We study a model problem with such structure -- a\nbinary classification task that uses a deep fully-connected neural network to\nclassify data drawn from two disjoint smooth curves on the unit sphere. Aside\nfrom mild regularity conditions, we place no restrictions on the configuration\nof the curves. We prove that when (i) the network depth is large relative to\ncertain geometric properties that set the difficulty of the problem and (ii)\nthe network width and number of samples is polynomial in the depth,\nrandomly-initialized gradient descent quickly learns to correctly classify all\npoints on the two curves with high probability. To our knowledge, this is the\nfirst generalization guarantee for deep networks with nonlinear data that\ndepends only on intrinsic data properties. Our analysis proceeds by a reduction\nto dynamics in the neural tangent kernel (NTK) regime, where the network depth\nplays the role of a fitting resource in solving the classification problem. In\nparticular, via fine-grained control of the decay properties of the NTK, we\ndemonstrate that when the network is sufficiently deep, the NTK can be locally\napproximated by a translationally invariant operator on the manifolds and\nstably inverted over smooth functions, which guarantees convergence and\ngeneralization.",
          "link": "http://arxiv.org/abs/2107.14324",
          "publishedOn": "2021-08-02T01:58:24.096Z",
          "wordCount": 650,
          "title": "Deep Networks Provably Classify Data on Curves. (arXiv:2107.14324v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruihuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianzhong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuaijun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>",
          "description": "Most existing domain adaptation methods focus on adaptation from only one\nsource domain, however, in practice there are a number of relevant sources that\ncould be leveraged to help improve performance on target domain. We propose a\nnovel approach named T-SVDNet to address the task of Multi-source Domain\nAdaptation (MDA), which is featured by incorporating Tensor Singular Value\nDecomposition (T-SVD) into a neural network's training pipeline. Overall,\nhigh-order correlations among multiple domains and categories are fully\nexplored so as to better bridge the domain gap. Specifically, we impose\nTensor-Low-Rank (TLR) constraint on a tensor obtained by stacking up a group of\nprototypical similarity matrices, aiming at capturing consistent data structure\nacross different domains. Furthermore, to avoid negative transfer brought by\nnoisy source data, we propose a novel uncertainty-aware weighting strategy to\nadaptively assign weights to different source domains and samples based on the\nresult of uncertainty estimation. Extensive experiments conducted on public\nbenchmarks demonstrate the superiority of our model in addressing the task of\nMDA compared to state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.14447",
          "publishedOn": "2021-08-02T01:58:24.077Z",
          "wordCount": 623,
          "title": "T-SVDNet: Exploring High-Order Prototypical Correlations for Multi-Source Domain Adaptation. (arXiv:2107.14447v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruobin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Liang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suganthan_P/0/1/0/all/0/1\">P.N. Suganthan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuen_K/0/1/0/all/0/1\">Kum Fai Yuen</a>",
          "description": "Electricity load forecasting is crucial for the power systems' planning and\nmaintenance. However, its un-stationary and non-linear characteristics impose\nsignificant difficulties in anticipating future demand. This paper proposes a\nnovel ensemble deep Random Vector Functional Link (edRVFL) network for\nelectricity load forecasting. The weights of hidden layers are randomly\ninitialized and kept fixed during the training process. The hidden layers are\nstacked to enforce deep representation learning. Then, the model generates the\nforecasts by ensembling the outputs of each layer. Moreover, we also propose to\naugment the random enhancement features by empirical wavelet transformation\n(EWT). The raw load data is decomposed by EWT in a walk-forward fashion, not\nintroducing future data leakage problems in the decomposition process. Finally,\nall the sub-series generated by the EWT, including raw data, are fed into the\nedRVFL for forecasting purposes. The proposed model is evaluated on twenty\npublicly available time series from the Australian Energy Market Operator of\nthe year 2020. The simulation results demonstrate the proposed model's superior\nperformance over eleven forecasting methods in three error metrics and\nstatistical tests on electricity load forecasting tasks.",
          "link": "http://arxiv.org/abs/2107.14385",
          "publishedOn": "2021-08-02T01:58:24.071Z",
          "wordCount": 636,
          "title": "Random vector functional link neural network based ensemble deep learning for short-term load forecasting. (arXiv:2107.14385v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14368",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1\">Akash Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aich_A/0/1/0/all/0/1\">Abhishek Aich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodriguez_K/0/1/0/all/0/1\">Kevin Rodriguez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reddy_G/0/1/0/all/0/1\">G. Venugopala Reddy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>",
          "description": "While machine learning approaches have shown remarkable performance in\nbiomedical image analysis, most of these methods rely on high-quality and\naccurate imaging data. However, collecting such data requires intensive and\ncareful manual effort. One of the major challenges in imaging the Shoot Apical\nMeristem (SAM) of Arabidopsis thaliana, is that the deeper slices in the\nz-stack suffer from different perpetual quality-related problems like poor\ncontrast and blurring. These quality-related issues often lead to the disposal\nof the painstakingly collected data with little to no control on quality while\ncollecting the data. Therefore, it becomes necessary to employ and design\ntechniques that can enhance the images to make them more suitable for further\nanalysis. In this paper, we propose a data-driven Deep Quantized Latent\nRepresentation (DQLR) methodology for high-quality image reconstruction in the\nShoot Apical Meristem (SAM) of Arabidopsis thaliana. Our proposed framework\nutilizes multiple consecutive slices in the z-stack to learn a low dimensional\nlatent space, quantize it and subsequently perform reconstruction using the\nquantized representation to obtain sharper images. Experiments on a publicly\navailable dataset validate our methodology showing promising results.",
          "link": "http://arxiv.org/abs/2107.14368",
          "publishedOn": "2021-08-02T01:58:24.064Z",
          "wordCount": 629,
          "title": "Deep Quantized Representation for Enhanced Reconstruction. (arXiv:2107.14368v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14372",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huppertz_R/0/1/0/all/0/1\">Robert Huppertz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakalembe_C/0/1/0/all/0/1\">Catherine Nakalembe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerner_H/0/1/0/all/0/1\">Hannah Kerner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lachyan_R/0/1/0/all/0/1\">Ramani Lachyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rischard_M/0/1/0/all/0/1\">Maxime Rischard</a>",
          "description": "With the global refugee crisis at a historic high, there is a growing need to\nassess the impact of refugee settlements on their hosting countries and\nsurrounding environments. Because fires are an important land management\npractice in smallholder agriculture in sub-Saharan Africa, burned area (BA)\nmappings can help provide information about the impacts of land management\npractices on local environments. However, a lack of BA ground-truth data in\nmuch of sub-Saharan Africa limits the use of highly scalable deep learning (DL)\ntechniques for such BA mappings. In this work, we propose a scalable transfer\nlearning approach to study BA dynamics in areas with little to no ground-truth\ndata such as the West Nile region in Northern Uganda. We train a deep learning\nmodel on BA ground-truth data in Portugal and propose the application of that\nmodel on refugee-hosting districts in West Nile between 2015 and 2020. By\ncomparing the district-level BA dynamic with the wider West Nile region, we aim\nto add understanding of the land management impacts of refugee settlements on\ntheir surrounding environments.",
          "link": "http://arxiv.org/abs/2107.14372",
          "publishedOn": "2021-08-02T01:58:24.055Z",
          "wordCount": 633,
          "title": "Using transfer learning to study burned area dynamics: A case study of refugee settlements in West Nile, Northern Uganda. (arXiv:2107.14372v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Piyush K. Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1\">Rolando Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaroukian_E/0/1/0/all/0/1\">Erin Zaroukian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorothy_M/0/1/0/all/0/1\">Michael Dorothy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basak_A/0/1/0/all/0/1\">Anjon Basak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asher_D/0/1/0/all/0/1\">Derrik E. Asher</a>",
          "description": "Much work has been dedicated to the exploration of Multi-Agent Reinforcement\nLearning (MARL) paradigms implementing a centralized learning with\ndecentralized execution (CLDE) approach to achieve human-like collaboration in\ncooperative tasks. Here, we discuss variations of centralized training and\ndescribe a recent survey of algorithmic approaches. The goal is to explore how\ndifferent implementations of information sharing mechanism in centralized\nlearning may give rise to distinct group coordinated behaviors in multi-agent\nsystems performing cooperative tasks.",
          "link": "http://arxiv.org/abs/2107.14316",
          "publishedOn": "2021-08-02T01:58:24.039Z",
          "wordCount": 561,
          "title": "Survey of Recent Multi-Agent Reinforcement Learning Algorithms Utilizing Centralized Training. (arXiv:2107.14316v1 [cs.MA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Citovsky_G/0/1/0/all/0/1\">Gui Citovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeSalvo_G/0/1/0/all/0/1\">Giulia DeSalvo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gentile_C/0/1/0/all/0/1\">Claudio Gentile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karydas_L/0/1/0/all/0/1\">Lazaros Karydas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopalan_A/0/1/0/all/0/1\">Anand Rajagopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostamizadeh_A/0/1/0/all/0/1\">Afshin Rostamizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>",
          "description": "The ability to train complex and highly effective models often requires an\nabundance of training data, which can easily become a bottleneck in cost, time,\nand computational resources. Batch active learning, which adaptively issues\nbatched queries to a labeling oracle, is a common approach for addressing this\nproblem. The practical benefits of batch sampling come with the downside of\nless adaptivity and the risk of sampling redundant examples within a batch -- a\nrisk that grows with the batch size. In this work, we analyze an efficient\nactive learning algorithm, which focuses on the large batch setting. In\nparticular, we show that our sampling method, which combines notions of\nuncertainty and diversity, easily scales to batch sizes (100K-1M) several\norders of magnitude larger than used in previous studies and provides\nsignificant improvements in model training efficiency compared to recent\nbaselines. Finally, we provide an initial theoretical analysis, proving label\ncomplexity guarantees for a related sampling method, which we show is\napproximately equivalent to our sampling method in specific settings.",
          "link": "http://arxiv.org/abs/2107.14263",
          "publishedOn": "2021-08-02T01:58:24.017Z",
          "wordCount": 602,
          "title": "Batch Active Learning at Scale. (arXiv:2107.14263v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atzberger_P/0/1/0/all/0/1\">Paul J. Atzberger</a>",
          "description": "We discuss a software package for incorporating into simulations data-driven\nmodels trained using machine learning methods. These can be used for (i)\nmodeling dynamics and time-step integration, (ii) modeling interactions between\nsystem components, and (iii) computing quantities of interest characterizing\nsystem state. The package allows for use of machine learning methods with\ngeneral model classes including Neural Networks, Gaussian Process Regression,\nKernel Models, and other approaches. We discuss in this whitepaper our\nprototype C++ package, aims, and example usage.",
          "link": "http://arxiv.org/abs/2107.14362",
          "publishedOn": "2021-08-02T01:58:24.007Z",
          "wordCount": 526,
          "title": "MLMOD Package: Machine Learning Methods for Data-Driven Modeling in LAMMPS. (arXiv:2107.14362v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tipirneni_S/0/1/0/all/0/1\">Sindhu Tipirneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chandan K. Reddy</a>",
          "description": "Multivariate time-series (MVTS) data are frequently observed in critical care\nsettings and are typically characterized by excessive missingness and irregular\ntime intervals. Existing approaches for learning representations in this domain\nhandle such issues by either aggregation or imputation of values, which in-turn\nsuppresses the fine-grained information and adds undesirable noise/overhead\ninto the machine learning model. To tackle this challenge, we propose STraTS\n(Self-supervised Transformer for TimeSeries) model which bypasses these\npitfalls by treating time-series as a set of observation triplets instead of\nusing the traditional dense matrix representation. It employs a novel\nContinuous Value Embedding (CVE) technique to encode continuous time and\nvariable values without the need for discretization. It is composed of a\nTransformer component with Multi-head attention layers which enables it to\nlearn contextual triplet embeddings while avoiding problems of recurrence and\nvanishing gradients that occur in recurrent architectures. Many healthcare\ndatasets also suffer from the limited availability of labeled data. Our model\nutilizes self-supervision by leveraging unlabeled data to learn better\nrepresentations by performing time-series forecasting as a self-supervision\ntask. Experiments on real-world multivariate clinical time-series benchmark\ndatasets show that STraTS shows better prediction performance than\nstate-of-the-art methods for mortality prediction, especially when labeled data\nis limited. Finally, we also present an interpretable version of STraTS which\ncan identify important measurements in the time-series data.",
          "link": "http://arxiv.org/abs/2107.14293",
          "publishedOn": "2021-08-02T01:58:23.987Z",
          "wordCount": 650,
          "title": "Self-supervised Transformer for Multivariate Clinical Time-Series with Missing Values. (arXiv:2107.14293v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rooke_C/0/1/0/all/0/1\">Clayton Rooke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">Jonathan Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leung_K/0/1/0/all/0/1\">Kin Kwan Leung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volkovs_M/0/1/0/all/0/1\">Maksims Volkovs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuberi_S/0/1/0/all/0/1\">Saba Zuberi</a>",
          "description": "Explanation methods applied to sequential models for multivariate time series\nprediction are receiving more attention in machine learning literature. While\ncurrent methods perform well at providing instance-wise explanations, they\nstruggle to efficiently and accurately make attributions over long periods of\ntime and with complex feature interactions. We propose WinIT, a framework for\nevaluating feature importance in time series prediction settings by quantifying\nthe shift in predictive distribution over multiple instances in a windowed\nsetting. Comprehensive empirical evidence shows our method improves on the\nprevious state-of-the-art, FIT, by capturing temporal dependencies in feature\nimportance. We also demonstrate how the solution improves the appropriate\nattribution of features within time steps, which existing interpretability\nmethods often fail to do. We compare with baselines on simulated and real-world\nclinical data. WinIT achieves 2.47x better performance than FIT and other\nfeature importance methods on real-world clinical MIMIC-mortality task. The\ncode for this work is available at https://github.com/layer6ai-labs/WinIT.",
          "link": "http://arxiv.org/abs/2107.14317",
          "publishedOn": "2021-08-02T01:58:23.970Z",
          "wordCount": 588,
          "title": "Temporal Dependencies in Feature Importance for Time Series Predictions. (arXiv:2107.14317v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14257",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Kotthoff_L/0/1/0/all/0/1\">Lars Kotthoff</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dey_S/0/1/0/all/0/1\">Sourin Dey</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jain_V/0/1/0/all/0/1\">Vivek Jain</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tyrrell_A/0/1/0/all/0/1\">Alexander Tyrrell</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wahab_H/0/1/0/all/0/1\">Hud Wahab</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Johnson_P/0/1/0/all/0/1\">Patrick Johnson</a>",
          "description": "A lot of technological advances depend on next-generation materials, such as\ngraphene, which enables a raft of new applications, for example better\nelectronics. Manufacturing such materials is often difficult; in particular,\nproducing graphene at scale is an open problem. We provide a series of datasets\nthat describe the optimization of the production of laser-induced graphene, an\nestablished manufacturing method that has shown great promise. We pose three\nchallenges based on the datasets we provide -- modeling the behavior of\nlaser-induced graphene production with respect to parameters of the production\nprocess, transferring models and knowledge between different precursor\nmaterials, and optimizing the outcome of the transformation over the space of\npossible production parameters. We present illustrative results, along with the\ncode used to generate them, as a starting point for interested users. The data\nwe provide represents an important real-world application of machine learning;\nto the best of our knowledge, no similar datasets are available.",
          "link": "http://arxiv.org/abs/2107.14257",
          "publishedOn": "2021-08-02T01:58:23.952Z",
          "wordCount": 585,
          "title": "Modeling and Optimizing Laser-Induced Graphene. (arXiv:2107.14257v1 [physics.app-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chris Junchi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "Independent component analysis (ICA) has been a popular dimension reduction\ntool in statistical machine learning and signal processing. In this paper, we\npresent a convergence analysis for an online tensorial ICA algorithm, by\nviewing the problem as a nonconvex stochastic approximation problem. For\nestimating one component, we provide a dynamics-based analysis to prove that\nour online tensorial ICA algorithm with a specific choice of stepsize achieves\na sharp finite-sample error bound. In particular, under a mild assumption on\nthe data-generating distribution and a scaling condition such that $d^4/T$ is\nsufficiently small up to a polylogarithmic factor of data dimension $d$ and\nsample size $T$, a sharp finite-sample error bound of $\\tilde{O}(\\sqrt{d/T})$\ncan be obtained.",
          "link": "http://arxiv.org/abs/2012.14415",
          "publishedOn": "2021-07-30T02:13:30.636Z",
          "wordCount": 592,
          "title": "Stochastic Approximation for Online Tensorial Independent Component Analysis. (arXiv:2012.14415v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Mingyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasanbeig_M/0/1/0/all/0/1\">Mohammadhosein Hasanbeig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shaoping Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1\">Alessandro Abate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_Z/0/1/0/all/0/1\">Zhen Kan</a>",
          "description": "This paper investigates the motion planning of autonomous dynamical systems\nmodeled by Markov decision processes (MDP) with unknown transition\nprobabilities over continuous state and action spaces. Linear temporal logic\n(LTL) is used to specify high-level tasks over infinite horizon, which can be\nconverted into a limit deterministic generalized B\\\"uchi automaton (LDGBA) with\nseveral accepting sets. The novelty is to design an embedded product MDP\n(EP-MDP) between the LDGBA and the MDP by incorporating a synchronous\ntracking-frontier function to record unvisited accepting sets of the automaton,\nand to facilitate the satisfaction of the accepting conditions. The proposed\nLDGBA-based reward shaping and discounting schemes for the model-free\nreinforcement learning (RL) only depend on the EP-MDP states and can overcome\nthe issues of sparse rewards. Rigorous analysis shows that any RL method that\noptimizes the expected discounted return is guaranteed to find an optimal\npolicy whose traces maximize the satisfaction probability. A modular deep\ndeterministic policy gradient (DDPG) is then developed to generate such\npolicies over continuous state and action spaces. The performance of our\nframework is evaluated via an array of OpenAI gym environments.",
          "link": "http://arxiv.org/abs/2102.12855",
          "publishedOn": "2021-07-30T02:13:30.616Z",
          "wordCount": 682,
          "title": "Modular Deep Reinforcement Learning for Continuous Motion Planning with Temporal Logic. (arXiv:2102.12855v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Christopher D. Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1\">Heejin Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1\">George J. Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1\">Pratik Chaudhari</a>",
          "description": "We develop a Multi-Agent Reinforcement Learning (MARL) method to learn\nscalable control policies for target tracking. Our method can handle an\narbitrary number of pursuers and targets; we show results for tasks consisting\nup to 1000 pursuers tracking 1000 targets. We use a decentralized,\npartially-observable Markov Decision Process framework to model pursuers as\nagents receiving partial observations (range and bearing) about targets which\nmove using fixed, unknown policies. An attention mechanism is used to\nparameterize the value function of the agents; this mechanism allows us to\nhandle an arbitrary number of targets. Entropy-regularized off-policy RL\nmethods are used to train a stochastic policy, and we discuss how it enables a\nhedging behavior between pursuers that leads to a weak form of cooperation in\nspite of completely decentralized control execution. We further develop a\nmasking heuristic that allows training on smaller problems with few\npursuers-targets and execution on much larger problems. Thorough simulation\nexperiments, ablation studies, and comparisons to state of the art algorithms\nare performed to study the scalability of the approach and robustness of\nperformance to varying numbers of agents and targets.",
          "link": "http://arxiv.org/abs/2011.08055",
          "publishedOn": "2021-07-30T02:13:30.599Z",
          "wordCount": 656,
          "title": "Scalable Reinforcement Learning Policies for Multi-Agent Control. (arXiv:2011.08055v2 [cs.MA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scheinert_D/0/1/0/all/0/1\">Dominik Scheinert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acker_A/0/1/0/all/0/1\">Alexander Acker</a>",
          "description": "Deployment, operation and maintenance of large IT systems becomes\nincreasingly complex and puts human experts under extreme stress when problems\noccur. Therefore, utilization of machine learning (ML) and artificial\nintelligence (AI) is applied on IT system operation and maintenance -\nsummarized in the term AIOps. One specific direction aims at the recognition of\nre-occurring anomaly types to enable remediation automation. However, due to IT\nsystem specific properties, especially their frequent changes (e.g. software\nupdates, reconfiguration or hardware modernization), recognition of reoccurring\nanomaly types is challenging. Current methods mainly assume a static\ndimensionality of provided data. We propose a method that is invariant to\ndimensionality changes of given data. Resource metric data such as CPU\nutilization, allocated memory and others are modelled as multivariate time\nseries. The extraction of temporal and spatial features together with the\nsubsequent anomaly classification is realized by utilizing TELESTO, our novel\ngraph convolutional neural network (GCNN) architecture. The experimental\nevaluation is conducted in a real-world cloud testbed deployment that is\nhosting two applications. Classification results of injected anomalies on a\ncassandra database node show that TELESTO outperforms the alternative GCNNs and\nachieves an overall classification accuracy of 85.1%. Classification results\nfor the other nodes show accuracy values between 85% and 60%.",
          "link": "http://arxiv.org/abs/2102.12877",
          "publishedOn": "2021-07-30T02:13:30.516Z",
          "wordCount": 684,
          "title": "TELESTO: A Graph Neural Network Model for Anomaly Classification in Cloud Services. (arXiv:2102.12877v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14151",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rao_A/0/1/0/all/0/1\">Aniruddha Rajendra Rao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Reimherr_M/0/1/0/all/0/1\">Matthew Reimherr</a>",
          "description": "We introduce a new class of non-linear function-on-function regression models\nfor functional data using neural networks. We propose a framework using a\nhidden layer consisting of continuous neurons, called a continuous hidden\nlayer, for functional response modeling and give two model fitting strategies,\nFunctional Direct Neural Network (FDNN) and Functional Basis Neural Network\n(FBNN). Both are designed explicitly to exploit the structure inherent in\nfunctional data and capture the complex relations existing between the\nfunctional predictors and the functional response. We fit these models by\nderiving functional gradients and implement regularization techniques for more\nparsimonious results. We demonstrate the power and flexibility of our proposed\nmethod in handling complex functional models through extensive simulation\nstudies as well as real data examples.",
          "link": "http://arxiv.org/abs/2107.14151",
          "publishedOn": "2021-07-30T02:13:30.509Z",
          "wordCount": 566,
          "title": "Modern Non-Linear Function-on-Function Regression. (arXiv:2107.14151v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iranfar_A/0/1/0/all/0/1\">Arman Iranfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arza_A/0/1/0/all/0/1\">Adriana Arza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atienza_D/0/1/0/all/0/1\">David Atienza</a>",
          "description": "Continuous and multimodal stress detection has been performed recently\nthrough wearable devices and machine learning algorithms. However, a well-known\nand important challenge of working on physiological signals recorded by\nconventional monitoring devices is missing data due to sensors insufficient\ncontact and interference by other equipment. This challenge becomes more\nproblematic when the user/patient is mentally or physically active or stressed\nbecause of more frequent conscious or subconscious movements. In this paper, we\npropose ReLearn, a robust machine learning framework for stress detection from\nbiomarkers extracted from multimodal physiological signals. ReLearn effectively\ncopes with missing data and outliers both at training and inference phases.\nReLearn, composed of machine learning models for feature selection, outlier\ndetection, data imputation, and classification, allows us to classify all\nsamples, including those with missing values at inference. In particular,\naccording to our experiments and stress database, while by discarding all\nmissing data, as a simplistic yet common approach, no prediction can be made\nfor 34% of the data at inference, our approach can achieve accurate\npredictions, as high as 78%, for missing samples. Also, our experiments show\nthat the proposed framework obtains a cross-validation accuracy of 86.8% even\nif more than 50% of samples within the features are missing.",
          "link": "http://arxiv.org/abs/2104.14278",
          "publishedOn": "2021-07-30T02:13:30.484Z",
          "wordCount": 680,
          "title": "ReLearn: A Robust Machine Learning Framework in Presence of Missing Data for Multimodal Stress Detection from Physiological Signals. (arXiv:2104.14278v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1801.02982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1\">Zeyuan Allen-Zhu</a>",
          "description": "Stochastic gradient descent (SGD) gives an optimal convergence rate when\nminimizing convex stochastic objectives $f(x)$. However, in terms of making the\ngradients small, the original SGD does not give an optimal rate, even when\n$f(x)$ is convex.\n\nIf $f(x)$ is convex, to find a point with gradient norm $\\varepsilon$, we\ndesign an algorithm SGD3 with a near-optimal rate\n$\\tilde{O}(\\varepsilon^{-2})$, improving the best known rate\n$O(\\varepsilon^{-8/3})$ of [18].\n\nIf $f(x)$ is nonconvex, to find its $\\varepsilon$-approximate local minimum,\nwe design an algorithm SGD5 with rate $\\tilde{O}(\\varepsilon^{-3.5})$, where\npreviously SGD variants only achieve $\\tilde{O}(\\varepsilon^{-4})$ [6, 15, 33].\nThis is no slower than the best known stochastic version of Newton's method in\nall parameter regimes [30].",
          "link": "http://arxiv.org/abs/1801.02982",
          "publishedOn": "2021-07-30T02:13:30.469Z",
          "wordCount": 620,
          "title": "How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex SGD. (arXiv:1801.02982v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhanpeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qirui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>",
          "description": "This paper aims to formulate the problem of estimating the optimal baseline\nvalues for the Shapley value in game theory. The Shapley value measures the\nattribution of each input variable of a complex model, which is computed as the\nmarginal benefit from the presence of this variable w.r.t.its absence under\ndifferent contexts. To this end, people usually set the input variable to its\nbaseline value to represent the absence of this variable (i.e.the no-signal\nstate of this variable). Previous studies usually determine the baseline values\nin an empirical manner, which hurts the trustworthiness of the Shapley value.\nIn this paper, we revisit the feature representation of a deep model from the\nperspective of game theory, and define the multi-variate interaction patterns\nof input variables to define the no-signal state of an input variable. Based on\nthe multi-variate interaction, we learn the optimal baseline value of each\ninput variable. Experimental results have demonstrated the effectiveness of our\nmethod.",
          "link": "http://arxiv.org/abs/2105.10719",
          "publishedOn": "2021-07-30T02:13:30.463Z",
          "wordCount": 614,
          "title": "Learning Baseline Values for Shapley Values. (arXiv:2105.10719v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thoma_N/0/1/0/all/0/1\">Nils Thoma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhongjie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ventola_F/0/1/0/all/0/1\">Fabrizio Ventola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>",
          "description": "Time series forecasting is a relevant task that is performed in several\nreal-world scenarios such as product sales analysis and prediction of energy\ndemand. Given their accuracy performance, currently, Recurrent Neural Networks\n(RNNs) are the models of choice for this task. Despite their success in time\nseries forecasting, less attention has been paid to make the RNNs trustworthy.\nFor example, RNNs can not naturally provide an uncertainty measure to their\npredictions. This could be extremely useful in practice in several cases e.g.\nto detect when a prediction might be completely wrong due to an unusual pattern\nin the time series. Whittle Sum-Product Networks (WSPNs), prominent deep\ntractable probabilistic circuits (PCs) for time series, can assist an RNN with\nproviding meaningful probabilities as uncertainty measure. With this aim, we\npropose RECOWN, a novel architecture that employs RNNs and a discriminant\nvariant of WSPNs called Conditional WSPNs (CWSPNs). We also formulate a\nLog-Likelihood Ratio Score as better estimation of uncertainty that is tailored\nto time series and Whittle likelihoods. In our experiments, we show that\nRECOWNs are accurate and trustworthy time series predictors, able to \"know when\nthey do not know\".",
          "link": "http://arxiv.org/abs/2106.04148",
          "publishedOn": "2021-07-30T02:13:30.458Z",
          "wordCount": 662,
          "title": "RECOWNs: Probabilistic Circuits for Trustworthy Time Series Forecasting. (arXiv:2106.04148v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.03194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wenbo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shroff_N/0/1/0/all/0/1\">Ness B. Shroff</a>",
          "description": "This paper studies the problem of finding the exact ranking from noisy\ncomparisons. A comparison over a set of $m$ items produces a noisy outcome\nabout the most preferred item, and reveals some information about the ranking.\nBy repeatedly and adaptively choosing items to compare, we want to fully rank\nthe items with a certain confidence, and use as few comparisons as possible.\nDifferent from most previous works, in this paper, we have three main\nnovelties: (i) compared to prior works, our upper bounds (algorithms) and lower\nbounds on the sample complexity (aka number of comparisons) require the minimal\nassumptions on the instances, and are not restricted to specific models; (ii)\nwe give lower bounds and upper bounds on instances with unequal noise levels;\nand (iii) this paper aims at the exact ranking without knowledge on the\ninstances, while most of the previous works either focus on approximate\nrankings or study exact ranking but require prior knowledge. We first derive\nlower bounds for pairwise ranking (i.e., compare two items each time), and then\npropose (nearly) optimal pairwise ranking algorithms. We further make\nextensions to listwise ranking (i.e., comparing multiple items each time).\nNumerical results also show our improvements against the state of the art.",
          "link": "http://arxiv.org/abs/1909.03194",
          "publishedOn": "2021-07-30T02:13:30.452Z",
          "wordCount": 686,
          "title": "On Sample Complexity Upper and Lower Bounds for Exact Ranking from Noisy Comparisons. (arXiv:1909.03194v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.00814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wan-Lei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Peng-Cheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>",
          "description": "k-nearest neighbor graph is a fundamental data structure in many disciplines\nsuch as information retrieval, data-mining, pattern recognition, and machine\nlearning, etc. In the literature, considerable research has been focusing on\nhow to efficiently build an approximate k-nearest neighbor graph (k-NN graph)\nfor a fixed dataset. Unfortunately, a closely related issue of how to merge two\nexisting k-NN graphs has been overlooked. In this paper, we address the issue\nof k-NN graph merging in two different scenarios. In the first scenario, a\nsymmetric merge algorithm is proposed to combine two approximate k-NN graphs.\nThe algorithm facilitates large-scale processing by the efficient merging of\nk-NN graphs that are produced in parallel. In the second scenario, a joint\nmerge algorithm is proposed to expand an existing k-NN graph with a raw\ndataset. The algorithm enables the incremental construction of a hierarchical\napproximate k-NN graph. Superior performance is attained when leveraging the\nhierarchy for NN search of various data types, dimensionality, and distance\nmeasures.",
          "link": "http://arxiv.org/abs/1908.00814",
          "publishedOn": "2021-07-30T02:13:30.446Z",
          "wordCount": 660,
          "title": "On the Merge of k-NN Graph. (arXiv:1908.00814v6 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsiaousis_M/0/1/0/all/0/1\">Michail Tsiaousis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghouts_G/0/1/0/all/0/1\">Gertjan Burghouts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hillerstrom_F/0/1/0/all/0/1\">Fieke Hillerstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putten_P/0/1/0/all/0/1\">Peter van der Putten</a>",
          "description": "The dominant paradigm in spatiotemporal action detection is to classify\nactions using spatiotemporal features learned by 2D or 3D Convolutional\nNetworks. We argue that several actions are characterized by their context,\nsuch as relevant objects and actors present in the video. To this end, we\nintroduce an architecture based on self-attention and Graph Convolutional\nNetworks in order to model contextual cues, such as actor-actor and\nactor-object interactions, to improve human action detection in video. We are\ninterested in achieving this in a weakly-supervised setting, i.e. using as less\nannotations as possible in terms of action bounding boxes. Our model aids\nexplainability by visualizing the learned context as an attention map, even for\nactions and objects unseen during training. We evaluate how well our model\nhighlights the relevant context by introducing a quantitative metric based on\nrecall of objects retrieved by attention maps. Our model relies on a 3D\nconvolutional RGB stream, and does not require expensive optical flow\ncomputation. We evaluate our models on the DALY dataset, which consists of\nhuman-object interaction actions. Experimental results show that our\ncontextualized approach outperforms a baseline action detection approach by\nmore than 2 points in Video-mAP. Code is available at\n\\url{https://github.com/micts/acgcn}",
          "link": "http://arxiv.org/abs/2107.13648",
          "publishedOn": "2021-07-30T02:13:30.432Z",
          "wordCount": 678,
          "title": "Spot What Matters: Learning Context Using Graph Convolutional Networks for Weakly-Supervised Action Detection. (arXiv:2107.13648v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.10143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tomen_N/0/1/0/all/0/1\">Nergis Tomen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan van Gemert</a>",
          "description": "Convolutional layers in CNNs implement linear filters which decompose the\ninput into different frequency bands. However, most modern architectures\nneglect standard principles of filter design when optimizing their model\nchoices regarding the size and shape of the convolutional kernel. In this work,\nwe consider the well-known problem of spectral leakage caused by windowing\nartifacts in filtering operations in the context of CNNs. We show that the\nsmall size of CNN kernels make them susceptible to spectral leakage, which may\ninduce performance-degrading artifacts. To address this issue, we propose the\nuse of larger kernel sizes along with the Hamming window function to alleviate\nleakage in CNN architectures. We demonstrate improved classification accuracy\non multiple benchmark datasets including Fashion-MNIST, CIFAR-10, CIFAR-100 and\nImageNet with the simple use of a standard window function in convolutional\nlayers. Finally, we show that CNNs employing the Hamming window display\nincreased robustness against various adversarial attacks.",
          "link": "http://arxiv.org/abs/2101.10143",
          "publishedOn": "2021-07-30T02:13:30.426Z",
          "wordCount": 615,
          "title": "Spectral Leakage and Rethinking the Kernel Size in CNNs. (arXiv:2101.10143v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>",
          "description": "Ternary Neural Networks (TNNs) have received much attention due to being\npotentially orders of magnitude faster in inference, as well as more power\nefficient, than full-precision counterparts. However, 2 bits are required to\nencode the ternary representation with only 3 quantization levels leveraged. As\na result, conventional TNNs have similar memory consumption and speed compared\nwith the standard 2-bit models, but have worse representational capability.\nMoreover, there is still a significant gap in accuracy between TNNs and\nfull-precision networks, hampering their deployment to real applications. To\ntackle these two challenges, in this work, we first show that, under some mild\nconstraints, computational complexity of the ternary inner product can be\nreduced by a factor of 2. Second, to mitigate the performance gap, we\nelaborately design an implementation-dependent ternary quantization algorithm.\nThe proposed framework is termed Fast and Accurate Ternary Neural Networks\n(FATNN). Experiments on image classification demonstrate that our FATNN\nsurpasses the state-of-the-arts by a significant margin in accuracy. More\nimportantly, speedup evaluation compared with various precisions is analyzed on\nseveral platforms, which serves as a strong benchmark for further research.",
          "link": "http://arxiv.org/abs/2008.05101",
          "publishedOn": "2021-07-30T02:13:30.421Z",
          "wordCount": 669,
          "title": "FATNN: Fast and Accurate Ternary Neural Networks. (arXiv:2008.05101v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Acevedo_Viloria_J/0/1/0/all/0/1\">Jaime D. Acevedo-Viloria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roa_L/0/1/0/all/0/1\">Luisa Roa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeshina_S/0/1/0/all/0/1\">Soji Adeshina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olazo_C/0/1/0/all/0/1\">Cesar Charalla Olazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Rey_A/0/1/0/all/0/1\">Andr&#xe9;s Rodr&#xed;guez-Rey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramos_J/0/1/0/all/0/1\">Jose Alberto Ramos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correa_Bahnsen_A/0/1/0/all/0/1\">Alejandro Correa-Bahnsen</a>",
          "description": "Large digital platforms create environments where different types of user\ninteractions are captured, these relationships offer a novel source of\ninformation for fraud detection problems. In this paper we propose a framework\nof relational graph convolutional networks methods for fraudulent behaviour\nprevention in the financial services of a Super-App. To this end, we apply the\nframework on different heterogeneous graphs of users, devices, and credit\ncards; and finally use an interpretability algorithm for graph neural networks\nto determine the most important relations to the classification task of the\nusers. Our results show that there is an added value when considering models\nthat take advantage of the alternative data of the Super-App and the\ninteractions found in their high connectivity, further proofing how they can\nleverage that into better decisions and fraud detection strategies.",
          "link": "http://arxiv.org/abs/2107.13673",
          "publishedOn": "2021-07-30T02:13:30.415Z",
          "wordCount": 597,
          "title": "Relational Graph Neural Networks for Fraud Detection in a Super-Appe nvironment. (arXiv:2107.13673v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_K/0/1/0/all/0/1\">Kushankur Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellinger_C/0/1/0/all/0/1\">Colin Bellinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corizzo_R/0/1/0/all/0/1\">Roberto Corizzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krawczyk_B/0/1/0/all/0/1\">Bartosz Krawczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Japkowicz_N/0/1/0/all/0/1\">Nathalie Japkowicz</a>",
          "description": "Structural concept complexity, class overlap, and data scarcity are some of\nthe most important factors influencing the performance of classifiers under\nclass imbalance conditions. When these effects were uncovered in the early\n2000s, understandably, the classifiers on which they were demonstrated belonged\nto the classical rather than Deep Learning categories of approaches. As Deep\nLearning is gaining ground over classical machine learning and is beginning to\nbe used in critical applied settings, it is important to assess systematically\nhow well they respond to the kind of challenges their classical counterparts\nhave struggled with in the past two decades. The purpose of this paper is to\nstudy the behavior of deep learning systems in settings that have previously\nbeen deemed challenging to classical machine learning systems to find out\nwhether the depth of the systems is an asset in such settings. The results in\nboth artificial and real-world image datasets (MNIST Fashion, CIFAR-10) show\nthat these settings remain mostly challenging for Deep Learning systems and\nthat deeper architectures seem to help with structural concept complexity but\nnot with overlap challenges in simple artificial domains. Data scarcity is not\novercome by deeper layers, either. In the real-world image domains, where\noverfitting is a greater concern than in the artificial domains, the advantage\nof deeper architectures is less obvious: while it is observed in certain cases,\nit is quickly cancelled as models get deeper and perform worse than their\nshallower counterparts.",
          "link": "http://arxiv.org/abs/2107.14194",
          "publishedOn": "2021-07-30T02:13:30.410Z",
          "wordCount": 680,
          "title": "On the combined effect of class imbalance and concept complexity in deep learning. (arXiv:2107.14194v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14094",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Krishnakumari_P/0/1/0/all/0/1\">Panchamy Krishnakumari</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Cats_O/0/1/0/all/0/1\">Oded Cats</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lint_H/0/1/0/all/0/1\">Hans van Lint</a>",
          "description": "In an effort to improve user satisfaction and transit image, transit service\nproviders worldwide offer delay compensations. Smart card data enables the\nestimation of passenger delays throughout the network and aid in monitoring\nservice performance. Notwithstanding, in order to prioritize measures for\nimproving service reliability and hence reducing passenger delays, it is\nparamount to identify the system components - stations and track segments -\nwhere most passenger delay occurs. To this end, we propose a novel method for\nestimating network passenger delay from individual trajectories. We decompose\nthe delay along a passenger trajectory into its corresponding track segment\ndelay, initial waiting time and transfer delay. We distinguish between two\ndifferent types of passenger delay in relation to the public transit network:\naverage passenger delay and total passenger delay. We employ temporal\nclustering on these two quantities to reveal daily and seasonal regularity in\ndelay patterns of the transit network. The estimation and clustering methods\nare demonstrated on one year of data from Washington metro network. The data\nconsists of schedule information and smart card data which includes\npassenger-train assignment of the metro network for the months of August 2017\nto August 2018. Our findings show that the average passenger delay is\nrelatively stable throughout the day. The temporal clustering reveals\npronounced and recurrent and thus predictable daily and weekly patterns with\ndistinct characteristics for certain months.",
          "link": "http://arxiv.org/abs/2107.14094",
          "publishedOn": "2021-07-30T02:13:30.393Z",
          "wordCount": 674,
          "title": "Day-to-day and seasonal regularity of network passenger delay for metro networks. (arXiv:2107.14094v1 [physics.soc-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Strouse_D/0/1/0/all/0/1\">DJ Strouse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumli_K/0/1/0/all/0/1\">Kate Baumli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warde_Farley_D/0/1/0/all/0/1\">David Warde-Farley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mnih_V/0/1/0/all/0/1\">Vlad Mnih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_S/0/1/0/all/0/1\">Steven Hansen</a>",
          "description": "Unsupervised skill learning objectives (Gregor et al., 2016, Eysenbach et\nal., 2018) allow agents to learn rich repertoires of behavior in the absence of\nextrinsic rewards. They work by simultaneously training a policy to produce\ndistinguishable latent-conditioned trajectories, and a discriminator to\nevaluate distinguishability by trying to infer latents from trajectories. The\nhope is for the agent to explore and master the environment by encouraging each\nskill (latent) to reliably reach different states. However, an inherent\nexploration problem lingers: when a novel state is actually encountered, the\ndiscriminator will necessarily not have seen enough training data to produce\naccurate and confident skill classifications, leading to low intrinsic reward\nfor the agent and effective penalization of the sort of exploration needed to\nactually maximize the objective. To combat this inherent pessimism towards\nexploration, we derive an information gain auxiliary objective that involves\ntraining an ensemble of discriminators and rewarding the policy for their\ndisagreement. Our objective directly estimates the epistemic uncertainty that\ncomes from the discriminator not having seen enough training examples, thus\nproviding an intrinsic reward more tailored to the true objective compared to\npseudocount-based methods (Burda et al., 2019). We call this exploration bonus\ndiscriminator disagreement intrinsic reward, or DISDAIN. We demonstrate\nempirically that DISDAIN improves skill learning both in a tabular grid world\n(Four Rooms) and the 57 games of the Atari Suite (from pixels). Thus, we\nencourage researchers to treat pessimism with DISDAIN.",
          "link": "http://arxiv.org/abs/2107.14226",
          "publishedOn": "2021-07-30T02:13:30.387Z",
          "wordCount": 684,
          "title": "Learning more skills through optimistic exploration. (arXiv:2107.14226v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guoliang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Ting Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jin Song Dong</a>",
          "description": "Although deep learning has demonstrated astonishing performance in many\napplications, there are still concerns about its dependability. One desirable\nproperty of deep learning applications with societal impact is fairness (i.e.,\nnon-discrimination). Unfortunately, discrimination might be intrinsically\nembedded into the models due to the discrimination in the training data. As a\ncountermeasure, fairness testing systemically identifies discriminatory\nsamples, which can be used to retrain the model and improve the model's\nfairness. Existing fairness testing approaches however have two major\nlimitations. Firstly, they only work well on traditional machine learning\nmodels and have poor performance (e.g., effectiveness and efficiency) on deep\nlearning models. Secondly, they only work on simple structured (e.g., tabular)\ndata and are not applicable for domains such as text. In this work, we bridge\nthe gap by proposing a scalable and effective approach for systematically\nsearching for discriminatory samples while extending existing fairness testing\napproaches to address a more challenging domain, i.e., text classification.\nCompared with state-of-the-art methods, our approach only employs lightweight\nprocedures like gradient computation and clustering, which is significantly\nmore scalable and effective. Experimental results show that on average, our\napproach explores the search space much more effectively (9.62 and 2.38 times\nmore than the state-of-the-art methods respectively on tabular and text\ndatasets) and generates much more discriminatory samples (24.95 and 2.68 times)\nwithin a same reasonable time. Moreover, the retrained models reduce\ndiscrimination by 57.2% and 60.2% respectively on average.",
          "link": "http://arxiv.org/abs/2107.08176",
          "publishedOn": "2021-07-30T02:13:30.380Z",
          "wordCount": 702,
          "title": "Automatic Fairness Testing of Neural Classifiers through Adversarial Sampling. (arXiv:2107.08176v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_R/0/1/0/all/0/1\">Raman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravorty_S/0/1/0/all/0/1\">Suman Chakravorty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1\">Mohamed Naveed Gul Mohamed</a>",
          "description": "We consider the problem of Reinforcement Learning for nonlinear stochastic\ndynamical systems. We show that in the RL setting, there is an inherent ``Curse\nof Variance\" in addition to Bellman's infamous ``Curse of Dimensionality\", in\nparticular, we show that the variance in the solution grows\nfactorial-exponentially in the order of the approximation. A fundamental\nconsequence is that this precludes the search for anything other than ``local\"\nfeedback solutions in RL, in order to control the explosive variance growth,\nand thus, ensure accuracy. We further show that the deterministic optimal\ncontrol has a perturbation structure, in that the higher order terms do not\naffect the calculation of lower order terms, which can be utilized in RL to get\naccurate local solutions.",
          "link": "http://arxiv.org/abs/2011.10829",
          "publishedOn": "2021-07-30T02:13:30.374Z",
          "wordCount": 593,
          "title": "On the Convergence of Reinforcement Learning in Nonlinear Continuous State Space Problems. (arXiv:2011.10829v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14135",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">YunPeng Li</a>",
          "description": "Convolutive blind source separation (BSS) is intended to recover the unknown\ncomponents from their convolutive mixtures. Contrary to the contrast functions\nused in instantaneous cases, the spatial-temporal prewhitening stage and the\npara-unitary filters constraint are difficult to implement in a convolutive\ncontext. In this paper, we propose several modifications of FastICA to\nalleviate these difficulties. Our method performs the simple prewhitening step\non convolutive mixtures prior to the separation and optimizes the contrast\nfunction under the diagonalization constraint implemented by single value\ndecomposition (SVD). Numerical simulations are implemented to verify the\nperformance of the proposed method.",
          "link": "http://arxiv.org/abs/2107.14135",
          "publishedOn": "2021-07-30T02:13:30.369Z",
          "wordCount": 527,
          "title": "Modifications of FastICA in Convolutive Blind Source Separation. (arXiv:2107.14135v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11815",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liang_T/0/1/0/all/0/1\">Tengyuan Liang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Recht_B/0/1/0/all/0/1\">Benjamin Recht</a>",
          "description": "This paper provides elementary analyses of the regret and generalization of\nminimum-norm interpolating classifiers (MNIC). The MNIC is the function of\nsmallest Reproducing Kernel Hilbert Space norm that perfectly interpolates a\nlabel pattern on a finite data set. We derive a mistake bound for MNIC and a\nregularized variant that holds for all data sets. This bound follows from\nelementary properties of matrix inverses. Under the assumption that the data is\nindependently and identically distributed, the mistake bound implies that MNIC\ngeneralizes at a rate proportional to the norm of the interpolating solution\nand inversely proportional to the number of data points. This rate matches\nsimilar rates derived for margin classifiers and perceptrons. We derive several\nplausible generative models where the norm of the interpolating classifier is\nbounded or grows at a rate sublinear in $n$. We also show that as long as the\npopulation class conditional distributions are sufficiently separable in total\nvariation, then MNIC generalizes with a fast rate.",
          "link": "http://arxiv.org/abs/2101.11815",
          "publishedOn": "2021-07-30T02:13:30.350Z",
          "wordCount": 618,
          "title": "Interpolating Classifiers Make Few Mistakes. (arXiv:2101.11815v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.01845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drori_Y/0/1/0/all/0/1\">Yoel Drori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1\">Ohad Shamir</a>",
          "description": "We study the iteration complexity of stochastic gradient descent (SGD) for\nminimizing the gradient norm of smooth, possibly nonconvex functions. We\nprovide several results, implying that the $\\mathcal{O}(\\epsilon^{-4})$ upper\nbound of Ghadimi and Lan~\\cite{ghadimi2013stochastic} (for making the average\ngradient norm less than $\\epsilon$) cannot be improved upon, unless a\ncombination of additional assumptions is made. Notably, this holds even if we\nlimit ourselves to convex quadratic functions. We also show that for nonconvex\nfunctions, the feasibility of minimizing gradients with SGD is surprisingly\nsensitive to the choice of optimality criteria.",
          "link": "http://arxiv.org/abs/1910.01845",
          "publishedOn": "2021-07-30T02:13:30.345Z",
          "wordCount": 577,
          "title": "The Complexity of Finding Stationary Points with Stochastic Gradient Descent. (arXiv:1910.01845v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gaurav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Chenzhong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_J/0/1/0/all/0/1\">Jyotirmoy V. Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogdan_P/0/1/0/all/0/1\">Paul Bogdan</a>",
          "description": "Reinforcement learning (RL) is a technique to learn the control policy for an\nagent that interacts with a stochastic environment. In any given state, the\nagent takes some action, and the environment determines the probability\ndistribution over the next state as well as gives the agent some reward. Most\nRL algorithms typically assume that the environment satisfies Markov\nassumptions (i.e. the probability distribution over the next state depends only\non the current state). In this paper, we propose a model-based RL technique for\na system that has non-Markovian dynamics. Such environments are common in many\nreal-world applications such as in human physiology, biological systems,\nmaterial science, and population dynamics. Model-based RL (MBRL) techniques\ntypically try to simultaneously learn a model of the environment from the data,\nas well as try to identify an optimal policy for the learned model. We propose\na technique where the non-Markovianity of the system is modeled through a\nfractional dynamical system. We show that we can quantify the difference in the\nperformance of an MBRL algorithm that uses bounded horizon model predictive\ncontrol from the optimal policy. Finally, we demonstrate our proposed framework\non a pharmacokinetic model of human blood glucose dynamics and show that our\nfractional models can capture distant correlations on real-world datasets.",
          "link": "http://arxiv.org/abs/2107.13790",
          "publishedOn": "2021-07-30T02:13:30.336Z",
          "wordCount": 642,
          "title": "Non-Markovian Reinforcement Learning using Fractional Dynamics. (arXiv:2107.13790v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomerantz_R/0/1/0/all/0/1\">Roi Pomerantz</a>",
          "description": "We present Translatotron 2, a neural direct speech-to-speech translation\nmodel that can be trained end-to-end. Translatotron 2 consists of a speech\nencoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention\nmodule that connects all the previous three components. Experimental results\nsuggest that Translatotron 2 outperforms the original Translatotron by a large\nmargin in terms of translation quality and predicted speech naturalness, and\ndrastically improves the robustness of the predicted speech by mitigating\nover-generation, such as babbling or long pause. We also propose a new method\nfor retaining the source speaker's voice in the translated speech. The trained\nmodel is restricted to retain the source speaker's voice, and unlike the\noriginal Translatotron, it is not able to generate speech in a different\nspeaker's voice, making the model more robust for production deployment, by\nmitigating potential misuse for creating spoofing audio artifacts. When the new\nmethod is used together with a simple concatenation-based data augmentation,\nthe trained Translatotron 2 model is able to retain each speaker's voice for\ninput with speaker turns.",
          "link": "http://arxiv.org/abs/2107.08661",
          "publishedOn": "2021-07-30T02:13:30.331Z",
          "wordCount": 631,
          "title": "Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14203",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1\">Lingjiao Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cai_T/0/1/0/all/0/1\">Tracy Cai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>",
          "description": "Machine learning (ML) prediction APIs are increasingly widely used. An ML API\ncan change over time due to model updates or retraining. This presents a key\nchallenge in the usage of the API because it is often not clear to the user if\nand how the ML model has changed. Model shifts can affect downstream\napplication performance and also create oversight issues (e.g. if consistency\nis desired). In this paper, we initiate a systematic investigation of ML API\nshifts. We first quantify the performance shifts from 2020 to 2021 of popular\nML APIs from Google, Microsoft, Amazon, and others on a variety of datasets. We\nidentified significant model shifts in 12 out of 36 cases we investigated.\nInterestingly, we found several datasets where the API's predictions became\nsignificantly worse over time. This motivated us to formulate the API shift\nassessment problem at a more fine-grained level as estimating how the API\nmodel's confusion matrix changes over time when the data distribution is\nconstant. Monitoring confusion matrix shifts using standard random sampling can\nrequire a large number of samples, which is expensive as each API call costs a\nfee. We propose a principled adaptive sampling algorithm, MASA, to efficiently\nestimate confusion matrix shifts. MASA can accurately estimate the confusion\nmatrix shifts in commercial ML APIs using up to 90% fewer samples compared to\nrandom sampling. This work establishes ML API shifts as an important problem to\nstudy and provides a cost-effective approach to monitor such shifts.",
          "link": "http://arxiv.org/abs/2107.14203",
          "publishedOn": "2021-07-30T02:13:30.326Z",
          "wordCount": 697,
          "title": "Did the Model Change? Efficiently Assessing Machine Learning API Shifts. (arXiv:2107.14203v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2103.15990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rex Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramli_A/0/1/0/all/0/1\">Albara Ah Ramli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huanle Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_E/0/1/0/all/0/1\">Esha Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henricson_E/0/1/0/all/0/1\">Erik Henricson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>",
          "description": "With the rapid development of the internet of things (IoT) and artificial\nintelligence (AI) technologies, human activity recognition (HAR) has been\napplied in a variety of domains such as security and surveillance, human-robot\ninteraction, and entertainment. Even though a number of surveys and review\npapers have been published, there is a lack of HAR overview papers focusing on\nhealthcare applications that use wearable sensors. Therefore, we fill in the\ngap by presenting this overview paper. In particular, we present our projects\nto illustrate the system design of HAR applications for healthcare. Our\nprojects include early mobility identification of human activities for\nintensive care unit (ICU) patients and gait analysis of Duchenne muscular\ndystrophy (DMD) patients. We cover essential components of designing HAR\nsystems including sensor factors (e.g., type, number, and placement location),\nAI model selection (e.g., classical machine learning models versus deep\nlearning models), and feature engineering. In addition, we highlight the\nchallenges of such healthcare-oriented HAR systems and propose several research\nopportunities for both the medical and the computer science community.",
          "link": "http://arxiv.org/abs/2103.15990",
          "publishedOn": "2021-07-30T02:13:30.313Z",
          "wordCount": 671,
          "title": "An Overview of Human Activity Recognition Using Wearable Sensors: Healthcare and Artificial Intelligence. (arXiv:2103.15990v4 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheuk_K/0/1/0/all/0/1\">Kin Wai Cheuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1\">Dorien Herremans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Li Su</a>",
          "description": "Most of the current supervised automatic music transcription (AMT) models\nlack the ability to generalize. This means that they have trouble transcribing\nreal-world music recordings from diverse musical genres that are not presented\nin the labelled training data. In this paper, we propose a semi-supervised\nframework, ReconVAT, which solves this issue by leveraging the huge amount of\navailable unlabelled music recordings. The proposed ReconVAT uses\nreconstruction loss and virtual adversarial training. When combined with\nexisting U-net models for AMT, ReconVAT achieves competitive results on common\nbenchmark datasets such as MAPS and MusicNet. For example, in the few-shot\nsetting for the string part version of MusicNet, ReconVAT achieves F1-scores of\n61.0% and 41.6% for the note-wise and note-with-offset-wise metrics\nrespectively, which translates into an improvement of 22.2% and 62.5% compared\nto the supervised baseline model. Our proposed framework also demonstrates the\npotential of continual learning on new data, which could be useful in\nreal-world applications whereby new data is constantly available.",
          "link": "http://arxiv.org/abs/2107.04954",
          "publishedOn": "2021-07-30T02:13:30.307Z",
          "wordCount": 632,
          "title": "ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data. (arXiv:2107.04954v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1\">Juan C. P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeanneret_G/0/1/0/all/0/1\">Guillaume Jeanneret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueda_L/0/1/0/all/0/1\">Laura Rueda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1\">Pablo Arbel&#xe1;ez</a>",
          "description": "Deep learning models are prone to being fooled by imperceptible perturbations\nknown as adversarial attacks. In this work, we study how equipping models with\nTest-time Transformation Ensembling (TTE) can work as a reliable defense\nagainst such attacks. While transforming the input data, both at train and test\ntimes, is known to enhance model performance, its effects on adversarial\nrobustness have not been studied. Here, we present a comprehensive empirical\nstudy of the impact of TTE, in the form of widely-used image transforms, on\nadversarial robustness. We show that TTE consistently improves model robustness\nagainst a variety of powerful attacks without any need for re-training, and\nthat this improvement comes at virtually no trade-off with accuracy on clean\nsamples. Finally, we show that the benefits of TTE transfer even to the\ncertified robustness domain, in which TTE provides sizable and consistent\nimprovements.",
          "link": "http://arxiv.org/abs/2107.14110",
          "publishedOn": "2021-07-30T02:13:30.302Z",
          "wordCount": 588,
          "title": "Enhancing Adversarial Robustness via Test-time Transformation Ensembling. (arXiv:2107.14110v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.11270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muller_R/0/1/0/all/0/1\">Roger Alexander M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laflamme_Janssen_J/0/1/0/all/0/1\">Jonathan Laflamme-Janssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacaro_J/0/1/0/all/0/1\">Jaime Camacaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bessega_C/0/1/0/all/0/1\">Carolina Bessega</a>",
          "description": "In this article we address the question whether it is possible to learn the\ndifferential equations describing the physical properties of a dynamical\nsystem, subject to non-conservative forces, from observations of its realspace\ntrajectory(ies) only. We introduce a network that incorporates a difference\napproximation for the second order derivative in terms of residual connections\nbetween convolutional blocks, whose shared weights represent the coefficients\nof a second order ordinary differential equation. We further combine this\nsolver-like architecture with a convolutional network, capable of learning the\nrelation between trajectories of coupled oscillators and therefore allows us to\nmake a stable forecast even if the system is only partially observed. We\noptimize this map together with the solver network, while sharing their\nweights, to form a powerful framework capable of learning the complex physical\nproperties of a dissipative dynamical system.",
          "link": "http://arxiv.org/abs/2010.11270",
          "publishedOn": "2021-07-30T02:13:30.296Z",
          "wordCount": 605,
          "title": "Learning second order coupled differential equations that are subject to non-conservative forces. (arXiv:2010.11270v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pizzati_F/0/1/0/all/0/1\">Fabio Pizzati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerri_P/0/1/0/all/0/1\">Pietro Cerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>",
          "description": "Image-to-image translation (i2i) networks suffer from entanglement effects in\npresence of physics-related phenomena in target domain (such as occlusions,\nfog, etc), thus lowering the translation quality and variability. In this\npaper, we present a comprehensive method for disentangling physics-based traits\nin the translation, guiding the learning process with neural or physical\nmodels. For the latter, we integrate adversarial estimation and genetic\nalgorithms to correctly achieve disentanglement. The results show our approach\ndramatically increase performances in many challenging scenarios for image\ntranslation.",
          "link": "http://arxiv.org/abs/2107.14229",
          "publishedOn": "2021-07-30T02:13:30.290Z",
          "wordCount": 524,
          "title": "Guided Disentanglement in Generative Networks. (arXiv:2107.14229v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.06254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1\">Valerie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jeffrey Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joon Sik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plumb_G/0/1/0/all/0/1\">Gregory Plumb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>",
          "description": "Despite increasing interest in the field of Interpretable Machine Learning\n(IML), a significant gap persists between the technical objectives targeted by\nresearchers' methods and the high-level goals of consumers' use cases. In this\nwork, we synthesize foundational work on IML methods and evaluation into an\nactionable taxonomy. This taxonomy serves as a tool to conceptualize the gap\nbetween researchers and consumers, illustrated by the lack of connections\nbetween its methods and use cases components. It also provides the foundation\nfrom which we describe a three-step workflow to better enable researchers and\nconsumers to work together to discover what types of methods are useful for\nwhat use cases. Eventually, by building on the results generated from this\nworkflow, a more complete version of the taxonomy will increasingly allow\nconsumers to find relevant methods for their target use cases and researchers\nto identify applicable use cases for their proposed methods.",
          "link": "http://arxiv.org/abs/2103.06254",
          "publishedOn": "2021-07-30T02:13:30.276Z",
          "wordCount": 617,
          "title": "Interpretable Machine Learning: Moving From Mythos to Diagnostics. (arXiv:2103.06254v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">Eugene Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Cheng-Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yi Lee</a>",
          "description": "Deep neural networks (DNNs) are known to perform well when deployed to test\ndistributions that shares high similarity with the training distribution.\nFeeding DNNs with new data sequentially that were unseen in the training\ndistribution has two major challenges -- fast adaptation to new tasks and\ncatastrophic forgetting of old tasks. Such difficulties paved way for the\non-going research on few-shot learning and continual learning. To tackle these\nproblems, we introduce Attentive Independent Mechanisms (AIM). We incorporate\nthe idea of learning using fast and slow weights in conjunction with the\ndecoupling of the feature extraction and higher-order conceptual learning of a\nDNN. AIM is designed for higher-order conceptual learning, modeled by a mixture\nof experts that compete to learn independent concepts to solve a new task. AIM\nis a modular component that can be inserted into existing deep learning\nframeworks. We demonstrate its capability for few-shot learning by adding it to\nSIB and trained on MiniImageNet and CIFAR-FS, showing significant improvement.\nAIM is also applied to ANML and OML trained on Omniglot, CIFAR-100 and\nMiniImageNet to demonstrate its capability in continual learning. Code made\npublicly available at https://github.com/huang50213/AIM-Fewshot-Continual.",
          "link": "http://arxiv.org/abs/2107.14053",
          "publishedOn": "2021-07-30T02:13:30.270Z",
          "wordCount": 643,
          "title": "Few-Shot and Continual Learning with Attentive Independent Mechanisms. (arXiv:2107.14053v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.02469",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Subramani_K/0/1/0/all/0/1\">Krishna Subramani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smaragdis_P/0/1/0/all/0/1\">Paris Smaragdis</a>",
          "description": "Most audio processing pipelines involve transformations that act on\nfixed-dimensional input representations of audio. For example, when using the\nShort Time Fourier Transform (STFT) the DFT size specifies a fixed dimension\nfor the input representation. As a consequence, most audio machine learning\nmodels are designed to process fixed-size vector inputs which often prohibits\nthe repurposing of learned models on audio with different sampling rates or\nalternative representations. We note, however, that the intrinsic spectral\ninformation in the audio signal is invariant to the choice of the input\nrepresentation or the sampling rate. Motivated by this, we introduce a novel\nway of processing audio signals by treating them as a collection of points in\nfeature space, and we use point cloud machine learning models that give us\ninvariance to the choice of representation parameters, such as DFT size or the\nsampling rate. Additionally, we observe that these methods result in smaller\nmodels, and allow us to significantly subsample the input representation with\nminimal effects to a trained model performance.",
          "link": "http://arxiv.org/abs/2105.02469",
          "publishedOn": "2021-07-30T02:13:30.177Z",
          "wordCount": 627,
          "title": "Point Cloud Audio Processing. (arXiv:2105.02469v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13822",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Esche_E/0/1/0/all/0/1\">Erik Esche</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Talis_T/0/1/0/all/0/1\">Torben Talis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weigert_J/0/1/0/all/0/1\">Joris Weigert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brand_Rihm_G/0/1/0/all/0/1\">Gerardo Brand-Rihm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+You_B/0/1/0/all/0/1\">Byungjun You</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_C/0/1/0/all/0/1\">Christian Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Repke_J/0/1/0/all/0/1\">Jens-Uwe Repke</a>",
          "description": "Continuously operated (bio-)chemical processes increasingly suffer from\nexternal disturbances, such as feed fluctuations or changes in market\nconditions. Product quality often hinges on control of rarely measured\nconcentrations, which are expensive to measure. Semi-supervised regression is a\npossible building block and method from machine learning to construct\nsoft-sensors for such infrequently measured states. Using two case studies,\ni.e., the Williams-Otto process and a bioethanol production process,\nsemi-supervised regression is compared against standard regression to evaluate\nits merits and its possible scope of application for process control in the\n(bio-)chemical industry.",
          "link": "http://arxiv.org/abs/2107.13822",
          "publishedOn": "2021-07-30T02:13:30.166Z",
          "wordCount": 545,
          "title": "Semi-supervised Learning for Data-driven Soft-sensing of Biological and Chemical Processes. (arXiv:2107.13822v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valassakis_E/0/1/0/all/0/1\">Eugene Valassakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palo_N/0/1/0/all/0/1\">Norman Di Palo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1\">Edward Johns</a>",
          "description": "In this paper, we study the problem of zero-shot sim-to-real when the task\nrequires both highly precise control with sub-millimetre error tolerance, and\nwide task space generalisation. Our framework involves a coarse-to-fine\ncontroller, where trajectories begin with classical motion planning using\nICP-based pose estimation, and transition to a learned end-to-end controller\nwhich maps images to actions and is trained in simulation with domain\nrandomisation. In this way, we achieve precise control whilst also generalising\nthe controller across wide task spaces, and keeping the robustness of\nvision-based, end-to-end control. Real-world experiments on a range of\ndifferent tasks show that, by exploiting the best of both worlds, our framework\nsignificantly outperforms purely motion planning methods, and purely\nlearning-based methods. Furthermore, we answer a range of questions on best\npractices for precise sim-to-real transfer, such as how different image sensor\nmodalities and image feature representations perform.",
          "link": "http://arxiv.org/abs/2105.11283",
          "publishedOn": "2021-07-30T02:13:30.159Z",
          "wordCount": 626,
          "title": "Coarse-to-Fine for Sim-to-Real: Sub-Millimetre Precision Across Wide Task Spaces. (arXiv:2105.11283v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huan_J/0/1/0/all/0/1\">Jun Huan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>",
          "description": "While deep learning succeeds in a wide range of tasks, it highly depends on\nthe massive collection of annotated data which is expensive and time-consuming.\nTo lower the cost of data annotation, active learning has been proposed to\ninteractively query an oracle to annotate a small proportion of informative\nsamples in an unlabeled dataset. Inspired by the fact that the samples with\nhigher loss are usually more informative to the model than the samples with\nlower loss, in this paper we present a novel deep active learning approach that\nqueries the oracle for data annotation when the unlabeled sample is believed to\nincorporate high loss. The core of our approach is a measurement Temporal\nOutput Discrepancy (TOD) that estimates the sample loss by evaluating the\ndiscrepancy of outputs given by models at different optimization steps. Our\ntheoretical investigation shows that TOD lower-bounds the accumulated sample\nloss thus it can be used to select informative unlabeled samples. On basis of\nTOD, we further develop an effective unlabeled data sampling strategy as well\nas an unsupervised learning criterion that enhances model performance by\nincorporating the unlabeled data. Due to the simplicity of TOD, our active\nlearning approach is efficient, flexible, and task-agnostic. Extensive\nexperimental results demonstrate that our approach achieves superior\nperformances than the state-of-the-art active learning methods on image\nclassification and semantic segmentation tasks.",
          "link": "http://arxiv.org/abs/2107.14153",
          "publishedOn": "2021-07-30T02:13:30.124Z",
          "wordCount": 673,
          "title": "Semi-Supervised Active Learning with Temporal Output Discrepancy. (arXiv:2107.14153v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.08773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_E/0/1/0/all/0/1\">Eddie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>",
          "description": "Data augmentations are important ingredients in the recipe for training\nrobust neural networks, especially in computer vision. A fundamental question\nis whether neural network features encode data augmentation transformations. To\nanswer this question, we introduce a systematic approach to investigate which\nlayers of neural networks are the most predictive of augmentation\ntransformations. Our approach uses features in pre-trained vision models with\nminimal additional processing to predict common properties transformed by\naugmentation (scale, aspect ratio, hue, saturation, contrast, and brightness).\nSurprisingly, neural network features not only predict data augmentation\ntransformations, but they predict many transformations with high accuracy.\nAfter validating that neural networks encode features corresponding to\naugmentation transformations, we show that these features are encoded in the\nearly layers of modern CNNs, though the augmentation signal fades in deeper\nlayers.",
          "link": "http://arxiv.org/abs/2003.08773",
          "publishedOn": "2021-07-30T02:13:30.095Z",
          "wordCount": 601,
          "title": "Do CNNs Encode Data Augmentations?. (arXiv:2003.08773v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oueida_S/0/1/0/all/0/1\">Soraia Oueida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_S/0/1/0/all/0/1\">Soaad Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotb_Y/0/1/0/all/0/1\">Yehia Kotb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Syed Ishtiaque Ahmed</a>",
          "description": "This paper presents a new approach to prevent transportation accidents and\nmonitor driver's behavior using a healthcare AI system that incorporates\nfairness and ethics. Dangerous medical cases and unusual behavior of the driver\nare detected. Fairness algorithm is approached in order to improve\ndecision-making and address ethical issues such as privacy issues, and to\nconsider challenges that appear in the wild within AI in healthcare and\ndriving. A healthcare professional will be alerted about any unusual activity,\nand the driver's location when necessary, is provided in order to enable the\nhealthcare professional to immediately help to the unstable driver. Therefore,\nusing the healthcare AI system allows for accidents to be predicted and thus\nprevented and lives may be saved based on the built-in AI system inside the\nvehicle which interacts with the ER system.",
          "link": "http://arxiv.org/abs/2107.14077",
          "publishedOn": "2021-07-30T02:13:30.090Z",
          "wordCount": 618,
          "title": "A Fair and Ethical Healthcare Artificial Intelligence System for Monitoring Driver Behavior and Preventing Road Accidents. (arXiv:2107.14077v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daghaghi_S/0/1/0/all/0/1\">Shabnam Daghaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medini_T/0/1/0/all/0/1\">Tharun Medini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meisburger_N/0/1/0/all/0/1\">Nicholas Meisburger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Beidi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengnan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Anshumali Shrivastava</a>",
          "description": "Softmax classifiers with a very large number of classes naturally occur in\nmany applications such as natural language processing and information\nretrieval. The calculation of full softmax is costly from the computational and\nenergy perspective. There have been various sampling approaches to overcome\nthis challenge, popularly known as negative sampling (NS). Ideally, NS should\nsample negative classes from a distribution that is dependent on the input\ndata, the current parameters, and the correct positive class. Unfortunately,\ndue to the dynamically updated parameters and data samples, there is no\nsampling scheme that is provably adaptive and samples the negative classes\nefficiently. Therefore, alternative heuristics like random sampling, static\nfrequency-based sampling, or learning-based biased sampling, which primarily\ntrade either the sampling cost or the adaptivity of samples per iteration are\nadopted. In this paper, we show two classes of distributions where the sampling\nscheme is truly adaptive and provably generates negative samples in\nnear-constant time. Our implementation in C++ on CPU is significantly superior,\nboth in terms of wall-clock time and accuracy, compared to the most optimized\nTensorFlow implementations of other popular negative sampling approaches on\npowerful NVIDIA V100 GPU.",
          "link": "http://arxiv.org/abs/2012.15843",
          "publishedOn": "2021-07-30T02:13:30.077Z",
          "wordCount": 674,
          "title": "A Tale of Two Efficient and Informative Negative Sampling Distributions. (arXiv:2012.15843v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13833",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Noort_F/0/1/0/all/0/1\">Frieda van den Noort</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sirmacek_B/0/1/0/all/0/1\">Beril Sirmacek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Slump_C/0/1/0/all/0/1\">Cornelis H. Slump</a>",
          "description": "The prevalance of pelvic floor problems is high within the female population.\nTransperineal ultrasound (TPUS) is the main imaging modality used to\ninvestigate these problems. Automating the analysis of TPUS data will help in\ngrowing our understanding of pelvic floor related problems. In this study we\npresent a U-net like neural network with some convolutional long short term\nmemory (CLSTM) layers to automate the 3D segmentation of the levator ani muscle\n(LAM) in TPUS volumes. The CLSTM layers are added to preserve the inter-slice\n3D information. We reach human level performance on this segmentation task.\nTherefore, we conclude that we successfully automated the segmentation of the\nLAM on 3D TPUS data. This paves the way towards automatic in-vivo analysis of\nthe LAM mechanics in the context of large study populations.",
          "link": "http://arxiv.org/abs/2107.13833",
          "publishedOn": "2021-07-30T02:13:30.062Z",
          "wordCount": 587,
          "title": "Recurrent U-net for automatic pelvic floor muscle segmentation on 3D ultrasound. (arXiv:2107.13833v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.05261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1\">Fay&#xe7;al Ait Aoudia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1\">Jakob Hoydis</a>",
          "description": "Previous studies have demonstrated that end-to-end learning enables\nsignificant shaping gains over additive white Gaussian noise (AWGN) channels.\nHowever, its benefits have not yet been quantified over realistic wireless\nchannel models. This work aims to fill this gap by exploring the gains of\nend-to-end learning over a frequency- and time-selective fading channel using\northogonal frequency division multiplexing (OFDM). With imperfect channel\nknowledge at the receiver, the shaping gains observed on AWGN channels vanish.\nNonetheless, we identify two other sources of performance improvements. The\nfirst comes from a neural network (NN)-based receiver operating over a large\nnumber of subcarriers and OFDM symbols which allows to significantly reduce the\nnumber of orthogonal pilots without loss of bit error rate (BER). The second\ncomes from entirely eliminating orthognal pilots by jointly learning a neural\nreceiver together with either superimposed pilots (SIPs), linearly combined\nwith conventional quadrature amplitude modulation (QAM), or an optimized\nconstellation geometry. The learned geometry works for a wide range of\nsignal-to-noise ratios (SNRs), Doppler and delay spreads, has zero mean and\ndoes hence not contain any form of superimposed pilots. Both schemes achieve\nthe same BER as the pilot-based baseline with around 7% higher throughput.\nThus, we believe that a jointly learned transmitter and receiver are a very\ninteresting component for beyond-5G communication systems which could remove\nthe need and associated control overhead for demodulation reference signals\n(DMRSs).",
          "link": "http://arxiv.org/abs/2009.05261",
          "publishedOn": "2021-07-30T02:13:30.056Z",
          "wordCount": 708,
          "title": "End-to-end Learning for OFDM: From Neural Receivers to Pilotless Communication. (arXiv:2009.05261v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1\">Peter Kairouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinke_T/0/1/0/all/0/1\">Thomas Steinke</a>",
          "description": "We consider training models on private data that are distributed across user\ndevices. To ensure privacy, we add on-device noise and use secure aggregation\nso that only the noisy sum is revealed to the server. We present a\ncomprehensive end-to-end system, which appropriately discretizes the data and\nadds discrete Gaussian noise before performing secure aggregation. We provide a\nnovel privacy analysis for sums of discrete Gaussians and carefully analyze the\neffects of data quantization and modular summation arithmetic. Our theoretical\nguarantees highlight the complex tension between communication, privacy, and\naccuracy. Our extensive experimental results demonstrate that our solution is\nessentially able to match the accuracy to central differential privacy with\nless than 16 bits of precision per value.",
          "link": "http://arxiv.org/abs/2102.06387",
          "publishedOn": "2021-07-30T02:13:30.006Z",
          "wordCount": 604,
          "title": "The Distributed Discrete Gaussian Mechanism for Federated Learning with Secure Aggregation. (arXiv:2102.06387v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Langosco_L/0/1/0/all/0/1\">Lauro Langosco di Langosco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strathmann_H/0/1/0/all/0/1\">Heiko Strathmann</a>",
          "description": "Particle-based approximate Bayesian inference approaches such as Stein\nVariational Gradient Descent (SVGD) combine the flexibility and convergence\nguarantees of sampling methods with the computational benefits of variational\ninference. In practice, SVGD relies on the choice of an appropriate kernel\nfunction, which impacts its ability to model the target distribution -- a\nchallenging problem with only heuristic solutions. We propose Neural\nVariational Gradient Descent (NVGD), which is based on parameterizing the\nwitness function of the Stein discrepancy by a deep neural network whose\nparameters are learned in parallel to the inference, mitigating the necessity\nto make any kernel choices whatsoever. We empirically evaluate our method on\npopular synthetic inference problems, real-world Bayesian linear regression,\nand Bayesian neural network inference.",
          "link": "http://arxiv.org/abs/2107.10731",
          "publishedOn": "2021-07-30T02:13:29.987Z",
          "wordCount": 567,
          "title": "Neural Variational Gradient Descent. (arXiv:2107.10731v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weichert_D/0/1/0/all/0/1\">Dorina Weichert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kister_A/0/1/0/all/0/1\">Alexander Kister</a>",
          "description": "A solution that is only reliable under favourable conditions is hardly a safe\nsolution. Min Max Optimization is an approach that returns optima that are\nrobust against worst case conditions. We propose algorithms that perform Min\nMax Optimization in a setting where the function that should be optimized is\nnot known a priori and hence has to be learned by experiments. Therefore we\nextend the Bayesian Optimization setting, which is tailored to maximization\nproblems, to Min Max Optimization problems. While related work extends the two\nacquisition functions Expected Improvement and Gaussian Process Upper\nConfidence Bound; we extend the two acquisition functions Entropy Search and\nKnowledge Gradient. These acquisition functions are able to gain knowledge\nabout the optimum instead of just looking for points that are supposed to be\noptimal. In our evaluation we show that these acquisition functions allow for\nbetter solutions - converging faster to the optimum than the benchmark\nsettings.",
          "link": "http://arxiv.org/abs/2107.13772",
          "publishedOn": "2021-07-30T02:13:29.981Z",
          "wordCount": 586,
          "title": "Bayesian Optimization for Min Max Optimization. (arXiv:2107.13772v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07832",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaini_P/0/1/0/all/0/1\">Priyank Jaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holdijk_L/0/1/0/all/0/1\">Lars Holdijk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1\">Max Welling</a>",
          "description": "We focus on the problem of efficient sampling and learning of probability\ndensities by incorporating symmetries in probabilistic models. We first\nintroduce Equivariant Stein Variational Gradient Descent algorithm -- an\nequivariant sampling method based on Stein's identity for sampling from\ndensities with symmetries. Equivariant SVGD explicitly incorporates symmetry\ninformation in a density through equivariant kernels which makes the resultant\nsampler efficient both in terms of sample complexity and the quality of\ngenerated samples. Subsequently, we define equivariant energy based models to\nmodel invariant densities that are learned using contrastive divergence. By\nutilizing our equivariant SVGD for training equivariant EBMs, we propose new\nways of improving and scaling up training of energy based models. We apply\nthese equivariant energy models for modelling joint densities in regression and\nclassification tasks for image datasets, many-body particle systems and\nmolecular structure generation.",
          "link": "http://arxiv.org/abs/2106.07832",
          "publishedOn": "2021-07-30T02:13:29.922Z",
          "wordCount": 608,
          "title": "Learning Equivariant Energy Based Models with Equivariant Stein Variational Gradient Descent. (arXiv:2106.07832v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1\">Aditya Jyoti Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Smaranjit Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1\">Kanishka Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nethaji_N/0/1/0/all/0/1\">Niketha Nethaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Shivam Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purkayastha_A/0/1/0/all/0/1\">Arnab Dutta Purkayastha</a>",
          "description": "Tourism in India plays a quintessential role in the country's economy with an\nestimated 9.2% GDP share for the year 2018. With a yearly growth rate of 6.2%,\nthe industry holds a huge potential for being the primary driver of the economy\nas observed in the nations of the Middle East like the United Arab Emirates.\nThe historical and cultural diversity exhibited throughout the geography of the\nnation is a unique spectacle for people around the world and therefore serves\nto attract tourists in tens of millions in number every year. Traditionally,\ntour guides or academic professionals who study these heritage monuments were\nresponsible for providing information to the visitors regarding their\narchitectural and historical significance. However, unfortunately this system\nhas several caveats when considered on a large scale such as unavailability of\nsufficient trained people, lack of accurate information, failure to convey the\nrichness of details in an attractive format etc. Recently, machine learning\napproaches revolving around the usage of monument pictures have been shown to\nbe useful for rudimentary analysis of heritage sights. This paper serves as a\nsurvey of the research endeavors undertaken in this direction which would\neventually provide insights for building an automated decision system that\ncould be utilized to make the experience of tourism in India more modernized\nfor visitors.",
          "link": "http://arxiv.org/abs/2107.14070",
          "publishedOn": "2021-07-30T02:13:29.905Z",
          "wordCount": 690,
          "title": "Machine Learning Advances aiding Recognition and Classification of Indian Monuments and Landmarks. (arXiv:2107.14070v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scheinert_D/0/1/0/all/0/1\">Dominik Scheinert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thamsen_L/0/1/0/all/0/1\">Lauritz Thamsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Houkun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Will_J/0/1/0/all/0/1\">Jonathan Will</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acker_A/0/1/0/all/0/1\">Alexander Acker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wittkopp_T/0/1/0/all/0/1\">Thorsten Wittkopp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_O/0/1/0/all/0/1\">Odej Kao</a>",
          "description": "Distributed dataflow systems enable the use of clusters for scalable data\nanalytics. However, selecting appropriate cluster resources for a processing\njob is often not straightforward. Performance models trained on historical\nexecutions of a concrete job are helpful in such situations, yet they are\nusually bound to a specific job execution context (e.g. node type, software\nversions, job parameters) due to the few considered input parameters. Even in\ncase of slight context changes, such supportive models need to be retrained and\ncannot benefit from historical execution data from related contexts.\n\nThis paper presents Bellamy, a novel modeling approach that combines\nscale-outs, dataset sizes, and runtimes with additional descriptive properties\nof a dataflow job. It is thereby able to capture the context of a job\nexecution. Moreover, Bellamy is realizing a two-step modeling approach. First,\na general model is trained on all the available data for a specific scalable\nanalytics algorithm, hereby incorporating data from different contexts.\nSubsequently, the general model is optimized for the specific situation at\nhand, based on the available data for the concrete context. We evaluate our\napproach on two publicly available datasets consisting of execution data from\nvarious dataflow jobs carried out in different environments, showing that\nBellamy outperforms state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.13921",
          "publishedOn": "2021-07-30T02:13:29.887Z",
          "wordCount": 661,
          "title": "Bellamy: Reusing Performance Models for Distributed Dataflow Jobs Across Contexts. (arXiv:2107.13921v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2006.09858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tabaghi_P/0/1/0/all/0/1\">Puoya Tabaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jianhao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1\">Olgica Milenkovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokmanic_I/0/1/0/all/0/1\">Ivan Dokmani&#x107;</a>",
          "description": "Many data analysis problems can be cast as distance geometry problems in\n\\emph{space forms} -- Euclidean, spherical, or hyperbolic spaces. Often,\nabsolute distance measurements are often unreliable or simply unavailable and\nonly proxies to absolute distances in the form of similarities are available.\nHence we ask the following: Given only \\emph{comparisons} of similarities\namongst a set of entities, what can be said about the geometry of the\nunderlying space form? To study this question, we introduce the notions of the\n\\textit{ordinal capacity} of a target space form and \\emph{ordinal spread} of\nthe similarity measurements. The latter is an indicator of complex patterns in\nthe measurements, while the former quantifies the capacity of a space form to\naccommodate a set of measurements with a specific ordinal spread profile. We\nprove that the ordinal capacity of a space form is related to its dimension and\nthe sign of its curvature. This leads to a lower bound on the Euclidean and\nspherical embedding dimension of what we term similarity graphs. More\nimportantly, we show that the statistical behavior of the ordinal spread random\nvariables defined on a similarity graph can be used to identify its underlying\nspace form. We support our theoretical claims with experiments on weighted\ntrees, single-cell RNA expression data and spherical cartographic measurements.",
          "link": "http://arxiv.org/abs/2006.09858",
          "publishedOn": "2021-07-30T02:13:29.875Z",
          "wordCount": 690,
          "title": "Geometry of Similarity Comparisons. (arXiv:2006.09858v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_i/0/1/0/all/0/1\">ing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yiyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Huaxiong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaoshuang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shixin Xu</a>",
          "description": "Stroke is the top leading causes of death in China (Zhou et al. The Lancet\n2019). A dataset from Shanxi Province is used to identify the risk of each\npatient's at four states low/medium/high/attack and provide the state\ntransition tendency through a SHAP DeepExplainer. To improve the accuracy on an\nimbalance sample set, the Quadratic Interactive Deep Neural Network (QIDNN)\nmodel is first proposed by flexible selecting and appending of quadratic\ninteractive features. The experimental results showed that the QIDNN model with\n7 interactive features achieve the state-of-art accuracy $83.25\\%$. Blood\npressure, physical inactivity, smoking, weight and total cholesterol are the\ntop five important features. Then, for the sake of high recall on the most\nurgent state, attack state, the stroke occurrence prediction is taken as an\nauxiliary objective to benefit from multi-objective optimization. The\nprediction accuracy was promoted, meanwhile the recall of the attack state was\nimproved by $24.9\\%$ (to $84.83\\%$) compared to QIDNN (from $67.93\\%$) with\nsame features. The prediction model and analysis tool in this paper not only\ngave the theoretical optimized prediction method, but also provided the\nattribution explanation of risk states and transition direction of each\npatient, which provided a favorable tool for doctors to analyze and diagnose\nthe disease.",
          "link": "http://arxiv.org/abs/2107.14060",
          "publishedOn": "2021-07-30T02:13:29.869Z",
          "wordCount": 651,
          "title": "Multi-objective optimization and explanation for stroke risk assessment in Shanxi province. (arXiv:2107.14060v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farhat_H/0/1/0/all/0/1\">Hikmat Farhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rammouz_V/0/1/0/all/0/1\">Veronica Rammouz</a>",
          "description": "With the rapid growth of the number of devices on the Internet, malware poses\na threat not only to the affected devices but also their ability to use said\ndevices to launch attacks on the Internet ecosystem. Rapid malware\nclassification is an important tools to combat that threat. One of the\nsuccessful approaches to classification is based on malware images and deep\nlearning. While many deep learning architectures are very accurate they usually\ntake a long time to train. In this work we perform experiments on multiple well\nknown, pre-trained, deep network architectures in the context of transfer\nlearning. We show that almost all them classify malware accurately with a very\nshort training period.",
          "link": "http://arxiv.org/abs/2107.13743",
          "publishedOn": "2021-07-30T02:13:29.853Z",
          "wordCount": 543,
          "title": "Malware Classification Using Transfer Learning. (arXiv:2107.13743v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13735",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1\">Yubin Lu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maulik_R/0/1/0/all/0/1\">Romit Maulik</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gao_T/0/1/0/all/0/1\">Ting Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dietrich_F/0/1/0/all/0/1\">Felix Dietrich</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kevrekidis_I/0/1/0/all/0/1\">Ioannis G. Kevrekidis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Duan_J/0/1/0/all/0/1\">Jinqiao Duan</a>",
          "description": "In this work, we propose a method to learn probability distributions using\nsample path data from stochastic differential equations. Specifically, we\nconsider temporally evolving probability distributions (e.g., those produced by\nintegrating local or nonlocal Fokker-Planck equations). We analyze this\nevolution through machine learning assisted construction of a time-dependent\nmapping that takes a reference distribution (say, a Gaussian) to each and every\ninstance of our evolving distribution. If the reference distribution is the\ninitial condition of a Fokker-Planck equation, what we learn is the time-T map\nof the corresponding solution. Specifically, the learned map is a normalizing\nflow that deforms the support of the reference density to the support of each\nand every density snapshot in time. We demonstrate that this approach can learn\nsolutions to non-local Fokker-Planck equations, such as those arising in\nsystems driven by both Brownian and L\\'evy noise. We present examples with two-\nand three-dimensional, uni- and multimodal distributions to validate the\nmethod.",
          "link": "http://arxiv.org/abs/2107.13735",
          "publishedOn": "2021-07-30T02:13:29.848Z",
          "wordCount": 607,
          "title": "Learning the temporal evolution of multivariate densities via normalizing flows. (arXiv:2107.13735v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banna_V/0/1/0/all/0/1\">Vishnu Banna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinnakotla_A/0/1/0/all/0/1\">Akhil Chinnakotla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhengxin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vegesana_A/0/1/0/all/0/1\">Anirudh Vegesana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vivek_N/0/1/0/all/0/1\">Naveen Vivek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnappa_K/0/1/0/all/0/1\">Kruthi Krishnappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yung-Hsiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiruvathukal_G/0/1/0/all/0/1\">George K. Thiruvathukal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">James C. Davis</a>",
          "description": "Machine learning techniques are becoming a fundamental tool for scientific\nand engineering progress. These techniques are applied in contexts as diverse\nas astronomy and spam filtering. However, correctly applying these techniques\nrequires careful engineering. Much attention has been paid to the technical\npotential; relatively little attention has been paid to the software\nengineering process required to bring research-based machine learning\ntechniques into practical utility. Technology companies have supported the\nengineering community through machine learning frameworks such as TensorFLow\nand PyTorch, but the details of how to engineer complex machine learning models\nin these frameworks have remained hidden.\n\nTo promote best practices within the engineering community, academic\ninstitutions and Google have partnered to launch a Special Interest Group on\nMachine Learning Models (SIGMODELS) whose goal is to develop exemplary\nimplementations of prominent machine learning models in community locations\nsuch as the TensorFlow Model Garden (TFMG). The purpose of this report is to\ndefine a process for reproducing a state-of-the-art machine learning model at a\nlevel of quality suitable for inclusion in the TFMG. We define the engineering\nprocess and elaborate on each step, from paper analysis to model release. We\nreport on our experiences implementing the YOLO model family with a team of 26\nstudent researchers, share the tools we developed, and describe the lessons we\nlearned along the way.",
          "link": "http://arxiv.org/abs/2107.00821",
          "publishedOn": "2021-07-30T02:13:29.842Z",
          "wordCount": 705,
          "title": "An Experience Report on Machine Learning Reproducibility: Guidance for Practitioners and TensorFlow Model Garden Contributors. (arXiv:2107.00821v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.14119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1\">Emanuel Ben-Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1\">Tal Ridnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_N/0/1/0/all/0/1\">Nadav Zamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_I/0/1/0/all/0/1\">Itamar Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protter_M/0/1/0/all/0/1\">Matan Protter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1\">Lihi Zelnik-Manor</a>",
          "description": "In a typical multi-label setting, a picture contains on average few positive\nlabels, and many negative ones. This positive-negative imbalance dominates the\noptimization process, and can lead to under-emphasizing gradients from positive\nlabels during training, resulting in poor accuracy. In this paper, we introduce\na novel asymmetric loss (\"ASL\"), which operates differently on positive and\nnegative samples. The loss enables to dynamically down-weights and\nhard-thresholds easy negative samples, while also discarding possibly\nmislabeled samples. We demonstrate how ASL can balance the probabilities of\ndifferent samples, and how this balancing is translated to better mAP scores.\nWith ASL, we reach state-of-the-art results on multiple popular multi-label\ndatasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate\nASL applicability for other tasks, such as single-label classification and\nobject detection. ASL is effective, easy to implement, and does not increase\nthe training time or complexity.\n\nImplementation is available at: https://github.com/Alibaba-MIIL/ASL.",
          "link": "http://arxiv.org/abs/2009.14119",
          "publishedOn": "2021-07-30T02:13:29.837Z",
          "wordCount": 650,
          "title": "Asymmetric Loss For Multi-Label Classification. (arXiv:2009.14119v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1\">Aditya Jyoti Paul</a>",
          "description": "For over hundreds of millions of years, sea turtles and their ancestors have\nswum in the vast expanses of the ocean. They have undergone a number of\nevolutionary changes, leading to speciation and sub-speciation. However, in the\npast few decades, some of the most notable forces driving the genetic variance\nand population decline have been global warming and anthropogenic impact\nranging from large-scale poaching, collecting turtle eggs for food, besides\ndumping trash including plastic waste into the ocean. This leads to severe\ndetrimental effects in the sea turtle population, driving them to extinction.\nThis research focusses on the forces causing the decline in sea turtle\npopulation, the necessity for the global conservation efforts along with its\nsuccesses and failures, followed by an in-depth analysis of the modern advances\nin detection and recognition of sea turtles, involving Machine Learning and\nComputer Vision systems, aiding the conservation efforts.",
          "link": "http://arxiv.org/abs/2107.14061",
          "publishedOn": "2021-07-30T02:13:29.830Z",
          "wordCount": 610,
          "title": "The Need and Status of Sea Turtle Conservation and Survey of Associated Computer Vision Advances. (arXiv:2107.14061v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1906.01005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jordan_I/0/1/0/all/0/1\">Ian D. Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokol_P/0/1/0/all/0/1\">Piotr Aleksander Sokol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_I/0/1/0/all/0/1\">Il Memming Park</a>",
          "description": "Gated recurrent units (GRUs) are specialized memory elements for building\nrecurrent neural networks. Despite their incredible success on various tasks,\nincluding extracting dynamics underlying neural data, little is understood\nabout the specific dynamics representable in a GRU network. As a result, it is\nboth difficult to know a priori how successful a GRU network will perform on a\ngiven task, and also their capacity to mimic the underlying behavior of their\nbiological counterparts. Using a continuous time analysis, we gain intuition on\nthe inner workings of GRU networks. We restrict our presentation to low\ndimensions, allowing for a comprehensive visualization. We found a surprisingly\nrich repertoire of dynamical features that includes stable limit cycles\n(nonlinear oscillations), multi-stable dynamics with various topologies, and\nhomoclinic bifurcations. At the same time we were unable to train GRU networks\nto produce continuous attractors, which are hypothesized to exist in biological\nneural networks. We contextualize the usefulness of different kinds of observed\ndynamics and support our claims experimentally.",
          "link": "http://arxiv.org/abs/1906.01005",
          "publishedOn": "2021-07-30T02:13:29.816Z",
          "wordCount": 647,
          "title": "Gated recurrent units viewed through the lens of continuous time dynamical systems. (arXiv:1906.01005v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.10848",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1\">Hangjian Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Padilla_O/0/1/0/all/0/1\">Oscar Hernan Madrid Padilla</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_Q/0/1/0/all/0/1\">Qing Zhou</a>",
          "description": "Structural learning of directed acyclic graphs (DAGs) or Bayesian networks\nhas been studied extensively under the assumption that data are independent. We\npropose a new Gaussian DAG model for dependent data which assumes the\nobservations are correlated according to an undirected network. Under this\nmodel, we develop a method to estimate the DAG structure given a topological\nordering of the nodes. The proposed method jointly estimates the Bayesian\nnetwork and the correlations among observations by optimizing a scoring\nfunction based on penalized likelihood. We show that under some mild\nconditions, the proposed method produces consistent estimators after one\niteration. Extensive numerical experiments also demonstrate that by jointly\nestimating the DAG structure and the sample correlation, our method achieves\nmuch higher accuracy in structure learning. When the node ordering is unknown,\nthrough experiments on synthetic and real data, we show that our algorithm can\nbe used to estimate the correlations between samples, with which we can\nde-correlate the dependent data to significantly improve the performance of\nclassical DAG learning methods.",
          "link": "http://arxiv.org/abs/1905.10848",
          "publishedOn": "2021-07-30T02:13:29.810Z",
          "wordCount": 622,
          "title": "Learning Gaussian DAGs from Network Data. (arXiv:1905.10848v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.01656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Disabato_S/0/1/0/all/0/1\">Simone Disabato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roveri_M/0/1/0/all/0/1\">Manuel Roveri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alippi_C/0/1/0/all/0/1\">Cesare Alippi</a>",
          "description": "Severe constraints on memory and computation characterizing the\nInternet-of-Things (IoT) units may prevent the execution of Deep Learning\n(DL)-based solutions, which typically demand large memory and high processing\nload. In order to support a real-time execution of the considered DL model at\nthe IoT unit level, DL solutions must be designed having in mind constraints on\nmemory and processing capability exposed by the chosen IoT technology. In this\npaper, we introduce a design methodology aiming at allocating the execution of\nConvolutional Neural Networks (CNNs) on a distributed IoT application. Such a\nmethodology is formalized as an optimization problem where the latency between\nthe data-gathering phase and the subsequent decision-making one is minimized,\nwithin the given constraints on memory and processing load at the units level.\nThe methodology supports multiple sources of data as well as multiple CNNs in\nexecution on the same IoT system allowing the design of CNN-based applications\ndemanding autonomy, low decision-latency, and high Quality-of-Service.",
          "link": "http://arxiv.org/abs/1908.01656",
          "publishedOn": "2021-07-30T02:13:29.804Z",
          "wordCount": 643,
          "title": "Distributed Deep Convolutional Neural Networks for the Internet-of-Things. (arXiv:1908.01656v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.10692",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Harris_K/0/1/0/all/0/1\">Kameron Decker Harris</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1\">Yizhe Zhu</a>",
          "description": "We provide a novel analysis of low-rank tensor completion based on hypergraph\nexpanders. As a proxy for rank, we minimize the max-quasinorm of the tensor,\nwhich generalizes the max-norm for matrices. Our analysis is deterministic and\nshows that the number of samples required to approximately recover an order-$t$\ntensor with at most $n$ entries per dimension is linear in $n$, under the\nassumption that the rank and order of the tensor are $O(1)$. As steps in our\nproof, we find a new expander mixing lemma for a $t$-partite, $t$-uniform\nregular hypergraph model, and prove several new properties about tensor\nmax-quasinorm. To the best of our knowledge, this is the first deterministic\nanalysis of tensor completion. We develop a practical algorithm that solves a\nrelaxed version of the max-quasinorm minimization problem, and we demonstrate\nits efficacy with numerical experiments.",
          "link": "http://arxiv.org/abs/1910.10692",
          "publishedOn": "2021-07-30T02:13:29.798Z",
          "wordCount": 626,
          "title": "Deterministic tensor completion with hypergraph expanders. (arXiv:1910.10692v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuen_J/0/1/0/all/0/1\">Jason Kuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "We introduce a new image segmentation task, termed Entity Segmentation (ES)\nwith the aim to segment all visual entities in an image without considering\nsemantic category labels. It has many practical applications in image\nmanipulation/editing where the segmentation mask quality is typically crucial\nbut category labels are less important. In this setting, all\nsemantically-meaningful segments are equally treated as categoryless entities\nand there is no thing-stuff distinction. Based on our unified entity\nrepresentation, we propose a center-based entity segmentation framework with\ntwo novel modules to improve mask quality. Experimentally, both our new task\nand framework demonstrate superior advantages as against existing work. In\nparticular, ES enables the following: (1) merging multiple datasets to form a\nlarge training set without the need to resolve label conflicts; (2) any model\ntrained on one dataset can generalize exceptionally well to other datasets with\nunseen domains. Our code is made publicly available at\nhttps://github.com/dvlab-research/Entity.",
          "link": "http://arxiv.org/abs/2107.14228",
          "publishedOn": "2021-07-30T02:13:29.792Z",
          "wordCount": 595,
          "title": "Open-World Entity Segmentation. (arXiv:2107.14228v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raman_C/0/1/0/all/0/1\">Chirag Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_H/0/1/0/all/0/1\">Hayley Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loog_M/0/1/0/all/0/1\">Marco Loog</a>",
          "description": "The default paradigm for the forecasting of human behavior in social\nconversations is characterized by top-down approaches. These involve\nidentifying predictive relationships between low level nonverbal cues and\nfuture semantic events of interest (e.g. turn changes, group leaving). A common\nhurdle however, is the limited availability of labeled data for supervised\nlearning. In this work, we take the first step in the direction of a bottom-up\nself-supervised approach in the domain. We formulate the task of Social Cue\nForecasting to leverage the larger amount of unlabeled low-level behavior cues,\nand characterize the modeling challenges involved. To address these, we take a\nmeta-learning approach and propose the Social Process (SP) models--socially\naware sequence-to-sequence (Seq2Seq) models within the Neural Process (NP)\nfamily. SP models learn extractable representations of non-semantic future cues\nfor each participant, while capturing global uncertainty by jointly reasoning\nabout the future for all members of the group. Evaluation on synthesized and\nreal-world behavior data shows that our SP models achieve higher log-likelihood\nthan the NP baselines, and also highlights important considerations for\napplying such techniques within the domain of social human interactions.",
          "link": "http://arxiv.org/abs/2107.13576",
          "publishedOn": "2021-07-30T02:13:29.711Z",
          "wordCount": 628,
          "title": "Social Processes: Self-Supervised Forecasting of Nonverbal Cues in Social Conversations. (arXiv:2107.13576v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13721",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengwu Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Saparbayeva_B/0/1/0/all/0/1\">Bayan Saparbayeva</a>",
          "description": "Mainfold-valued functional data analysis (FDA) recently becomes an active\narea of research motivated by the raising availability of trajectories or\nlongitudinal data observed on non-linear manifolds. The challenges of analyzing\nsuch data comes from many aspects, including infinite dimensionality and\nnonlinearity, as well as time domain or phase variability. In this paper, we\nstudy the amplitude part of manifold-valued functions on $\\S^2$, which is\ninvariant to random time warping or re-parameterization of the function.\nUtilizing the nice geometry of $\\S^2$, we develop a set of efficient and\naccurate tools for temporal alignment of functions, geodesic and sample mean\ncalculation. At the heart of these tools, they rely on gradient descent\nalgorithms with carefully derived gradients. We show the advantages of these\nnewly developed tools over its competitors with extensive simulations and real\ndata, and demonstrate the importance of considering the amplitude part of\nfunctions instead of mixing it with phase variability in mainfold-valued FDA.",
          "link": "http://arxiv.org/abs/2107.13721",
          "publishedOn": "2021-07-30T02:13:29.706Z",
          "wordCount": 590,
          "title": "Amplitude Mean of Functional Data on $\\mathbb{S}^2$. (arXiv:2107.13721v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1\">William Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1\">Philippe Burlina</a>",
          "description": "When deploying artificial intelligence (AI) in the real world, being able to\ntrust the operation of the AI by characterizing how it performs is an\never-present and important topic. An important and still largely unexplored\ntask in this characterization is determining major factors within the real\nworld that affect the AI's behavior, such as weather conditions or lighting,\nand either a) being able to give justification for why it may have failed or b)\neliminating the influence the factor has. Determining these sensitive factors\nheavily relies on collected data that is diverse enough to cover numerous\ncombinations of these factors, which becomes more onerous when having many\npotential sensitive factors or operating in complex environments. This paper\ninvestigates methods that discover and separate out individual semantic\nsensitive factors from a given dataset to conduct this characterization as well\nas addressing mitigation of these factors' sensitivity. We also broaden\nremediation of fairness, which normally only addresses socially relevant\nfactors, and widen it to deal with the desensitization of AI with regard to all\npossible aspects of variation in the domain. The proposed methods which\ndiscover these major factors reduce the potentially onerous demands of\ncollecting a sufficiently diverse dataset. In experiments using the road sign\n(GTSRB) and facial imagery (CelebA) datasets, we show the promise of using this\nscheme to perform this characterization and remediation and demonstrate that\nour approach outperforms state of the art approaches.",
          "link": "http://arxiv.org/abs/2107.13625",
          "publishedOn": "2021-07-30T02:13:29.700Z",
          "wordCount": 674,
          "title": "Generalizing Fairness: Discovery and Mitigation of Unknown Sensitive Attributes. (arXiv:2107.13625v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elwood_A/0/1/0/all/0/1\">Adam Elwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasparin_A/0/1/0/all/0/1\">Alberto Gasparin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozza_A/0/1/0/all/0/1\">Alessandro Rozza</a>",
          "description": "With the rise in use of social media to promote branded products, the demand\nfor effective influencer marketing has increased. Brands are looking for\nimproved ways to identify valuable influencers among a vast catalogue; this is\neven more challenging with \"micro-influencers\", which are more affordable than\nmainstream ones but difficult to discover. In this paper, we propose a novel\nmulti-task learning framework to improve the state of the art in\nmicro-influencer ranking based on multimedia content. Moreover, since the\nvisual congruence between a brand and influencer has been shown to be good\nmeasure of compatibility, we provide an effective visual method for\ninterpreting our models' decisions, which can also be used to inform brands'\nmedia strategies. We compare with the current state-of-the-art on a recently\nconstructed public dataset and we show significant improvement both in terms of\naccuracy and model complexity. The techniques for ranking and interpretation\npresented in this work can be generalised to arbitrary multimedia ranking tasks\nthat have datasets with a similar structure.",
          "link": "http://arxiv.org/abs/2107.13943",
          "publishedOn": "2021-07-30T02:13:29.659Z",
          "wordCount": 599,
          "title": "Ranking Micro-Influencers: a Novel Multi-Task Learning and Interpretable Framework. (arXiv:2107.13943v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13841",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Goetz_A/0/1/0/all/0/1\">Aur&#xe8;le Goetz</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Durmaz_A/0/1/0/all/0/1\">Ali Riza Durmaz</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Muller_M/0/1/0/all/0/1\">Martin M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Thomas_A/0/1/0/all/0/1\">Akhil Thomas</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Britz_D/0/1/0/all/0/1\">Dominik Britz</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kerfriden_P/0/1/0/all/0/1\">Pierre Kerfriden</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Eberl_C/0/1/0/all/0/1\">Chris Eberl</a>",
          "description": "Materials' microstructures are signatures of their alloying composition and\nprocessing history. Therefore, microstructures exist in a wide variety. As\nmaterials become increasingly complex to comply with engineering demands,\nadvanced computer vision (CV) approaches such as deep learning (DL) inevitably\ngain relevance for quantifying microstrucutures' constituents from micrographs.\nWhile DL can outperform classical CV techniques for many tasks, shortcomings\nare poor data efficiency and generalizability across datasets. This is\ninherently in conflict with the expense associated with annotating materials\ndata through experts and extensive materials diversity. To tackle poor domain\ngeneralizability and the lack of labeled data simultaneously, we propose to\napply a sub-class of transfer learning methods called unsupervised domain\nadaptation (UDA). These algorithms address the task of finding domain-invariant\nfeatures when supplied with annotated source data and unannotated target data,\nsuch that performance on the latter distribution is optimized despite the\nabsence of annotations. Exemplarily, this study is conducted on a lath-shaped\nbainite segmentation task in complex phase steel micrographs. Here, the domains\nto bridge are selected to be different metallographic specimen preparations\n(surface etchings) and distinct imaging modalities. We show that a\nstate-of-the-art UDA approach surpasses the na\\\"ive application of source\ndomain trained models on the target domain (generalization baseline) to a large\nextent. This holds true independent of the domain shift, despite using little\ndata, and even when the baseline models were pre-trained or employed data\naugmentation. Through UDA, mIoU was improved over generalization baselines from\n82.2%, 61.0%, 49.7% to 84.7%, 67.3%, 73.3% on three target datasets,\nrespectively. This underlines this techniques' potential to cope with materials\nvariance.",
          "link": "http://arxiv.org/abs/2107.13841",
          "publishedOn": "2021-07-30T02:13:29.652Z",
          "wordCount": 705,
          "title": "Addressing materials' microstructure diversity using transfer learning. (arXiv:2107.13841v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14038",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kashefi_A/0/1/0/all/0/1\">Ali Kashefi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukerji_T/0/1/0/all/0/1\">Tapan Mukerji</a>",
          "description": "We propose a novel deep learning framework for predicting permeability of\nporous media from their digital images. Unlike convolutional neural networks,\ninstead of feeding the whole image volume as inputs to the network, we model\nthe boundary between solid matrix and pore spaces as point clouds and feed them\nas inputs to a neural network based on the PointNet architecture. This approach\novercomes the challenge of memory restriction of graphics processing units and\nits consequences on the choice of batch size, and convergence. Compared to\nconvolutional neural networks, the proposed deep learning methodology provides\nfreedom to select larger batch sizes, due to reducing significantly the size of\nnetwork inputs. Specifically, we use the classification branch of PointNet and\nadjust it for a regression task. As a test case, two and three dimensional\nsynthetic digital rock images are considered. We investigate the effect of\ndifferent components of our neural network on its performance. We compare our\ndeep learning strategy with a convolutional neural network from various\nperspectives, specifically for maximum possible batch size. We inspect the\ngeneralizability of our network by predicting the permeability of real-world\nrock samples as well as synthetic digital rocks that are statistically\ndifferent from the samples used during training. The network predicts the\npermeability of digital rocks a few thousand times faster than a Lattice\nBoltzmann solver with a high level of prediction accuracy.",
          "link": "http://arxiv.org/abs/2107.14038",
          "publishedOn": "2021-07-30T02:13:29.632Z",
          "wordCount": 668,
          "title": "Point-Cloud Deep Learning of Porous Media for Permeability Prediction. (arXiv:2107.14038v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1\">Jiayi Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huayu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1\">Dong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_K/0/1/0/all/0/1\">Kaichao You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duburcq_A/0/1/0/all/0/1\">Alexis Duburcq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "We present Tianshou, a highly modularized python library for deep\nreinforcement learning (DRL) that uses PyTorch as its backend. Tianshou aims to\nprovide building blocks to replicate common RL experiments and has officially\nsupported more than 15 classic algorithms succinctly. To facilitate related\nresearch and prove Tianshou's reliability, we release Tianshou's benchmark of\nMuJoCo environments, covering 9 classic algorithms and 9/13 Mujoco tasks with\nstate-of-the-art performance. We open-sourced Tianshou at\nhttps://github.com/thu-ml/tianshou/, which has received over 3k stars and\nbecome one of the most popular PyTorch-based DRL libraries.",
          "link": "http://arxiv.org/abs/2107.14171",
          "publishedOn": "2021-07-30T02:13:29.614Z",
          "wordCount": 538,
          "title": "Tianshou: a Highly Modularized Deep Reinforcement Learning Library. (arXiv:2107.14171v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zarafshan_P/0/1/0/all/0/1\">Pejman Zarafshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javadi_S/0/1/0/all/0/1\">Saman Javadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roozbahani_A/0/1/0/all/0/1\">Abbas Roozbahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemy_S/0/1/0/all/0/1\">Seyed Mehdi Hashemy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarafshan_P/0/1/0/all/0/1\">Payam Zarafshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etezadi_H/0/1/0/all/0/1\">Hamed Etezadi</a>",
          "description": "Groundwater is the largest storage of freshwater resources, which serves as\nthe major inventory for most of the human consumption through agriculture,\nindustrial, and domestic water supply. In the fields of hydrological, some\nresearchers applied a neural network to forecast rainfall intensity in\nspace-time and introduced the advantages of neural networks compared to\nnumerical models. Then, many researches have been conducted applying\ndata-driven models. Some of them extended an Artificial Neural Networks (ANNs)\nmodel to forecast groundwater level in semi-confined glacial sand and gravel\naquifer under variable state, pumping extraction and climate conditions with\nsignificant accuracy. In this paper, a multi-layer perceptron is applied to\nsimulate groundwater level. The adaptive moment estimation optimization\nalgorithm is also used to this matter. The root mean squared error, mean\nabsolute error, mean squared error and the coefficient of determination ( ) are\nused to evaluate the accuracy of the simulated groundwater level. Total value\nof and RMSE are 0.9458 and 0.7313 respectively which are obtained from the\nmodel output. Results indicate that deep learning algorithms can demonstrate a\nhigh accuracy prediction. Although the optimization of parameters is\ninsignificant in numbers, but due to the value of time in modelling setup, it\nis highly recommended to apply an optimization algorithm in modelling.",
          "link": "http://arxiv.org/abs/2107.13870",
          "publishedOn": "2021-07-30T02:13:29.589Z",
          "wordCount": 653,
          "title": "Artificial Intelligence Hybrid Deep Learning Model for Groundwater Level Prediction Using MLP-ADAM. (arXiv:2107.13870v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeddi_A/0/1/0/all/0/1\">Ashkan B. Jeddi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_N/0/1/0/all/0/1\">Nariman L. Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafieezadeh_A/0/1/0/all/0/1\">Abdollah Shafieezadeh</a>",
          "description": "Reinforcement learning (RL) has shown a promising performance in learning\noptimal policies for a variety of sequential decision-making tasks. However, in\nmany real-world RL problems, besides optimizing the main objectives, the agent\nis expected to satisfy a certain level of safety (e.g., avoiding collisions in\nautonomous driving). While RL problems are commonly formalized as Markov\ndecision processes (MDPs), safety constraints are incorporated via constrained\nMarkov decision processes (CMDPs). Although recent advances in safe RL have\nenabled learning safe policies in CMDPs, these safety requirements should be\nsatisfied during both training and in the deployment process. Furthermore, it\nis shown that in memory-based and partially observable environments, these\nmethods fail to maintain safety over unseen out-of-distribution observations.\nTo address these limitations, we propose a Lyapunov-based uncertainty-aware\nsafe RL model. The introduced model adopts a Lyapunov function that converts\ntrajectory-based constraints to a set of local linear constraints. Furthermore,\nto ensure the safety of the agent in highly uncertain environments, an\nuncertainty quantification method is developed that enables identifying\nrisk-averse actions through estimating the probability of constraint\nviolations. Moreover, a Transformers model is integrated to provide the agent\nwith memory to process long time horizons of information via the self-attention\nmechanism. The proposed model is evaluated in grid-world navigation tasks where\nsafety is defined as avoiding static and dynamic obstacles in fully and\npartially observable environments. The results of these experiments show a\nsignificant improvement in the performance of the agent both in achieving\noptimality and satisfying safety constraints.",
          "link": "http://arxiv.org/abs/2107.13944",
          "publishedOn": "2021-07-30T02:13:29.584Z",
          "wordCount": 691,
          "title": "Lyapunov-based uncertainty-aware safe reinforcement learning. (arXiv:2107.13944v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rambhatla_S/0/1/0/all/0/1\">Sai Saketh Rambhatla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1\">Michael Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>",
          "description": "Boosting is a method for finding a highly accurate hypothesis by linearly\ncombining many ``weak\" hypotheses, each of which may be only moderately\naccurate. Thus, boosting is a method for learning an ensemble of classifiers.\nWhile boosting has been shown to be very effective for decision trees, its\nimpact on neural networks has not been extensively studied. We prove one\nimportant difference between sums of decision trees compared to sums of\nconvolutional neural networks (CNNs) which is that a sum of decision trees\ncannot be represented by a single decision tree with the same number of\nparameters while a sum of CNNs can be represented by a single CNN. Next, using\nstandard object recognition datasets, we verify experimentally the well-known\nresult that a boosted ensemble of decision trees usually generalizes much\nbetter on testing data than a single decision tree with the same number of\nparameters. In contrast, using the same datasets and boosting algorithms, our\nexperiments show the opposite to be true when using neural networks (both CNNs\nand multilayer perceptrons (MLPs)). We find that a single neural network\nusually generalizes better than a boosted ensemble of smaller neural networks\nwith the same total number of parameters.",
          "link": "http://arxiv.org/abs/2107.13600",
          "publishedOn": "2021-07-30T02:13:29.572Z",
          "wordCount": 636,
          "title": "To Boost or not to Boost: On the Limits of Boosted Neural Networks. (arXiv:2107.13600v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahate_A/0/1/0/all/0/1\">Anil Rahate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walambe_R/0/1/0/all/0/1\">Rahee Walambe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanna_S/0/1/0/all/0/1\">Sheela Ramanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotecha_K/0/1/0/all/0/1\">Ketan Kotecha</a>",
          "description": "Multimodal deep learning systems which employ multiple modalities like text,\nimage, audio, video, etc., are showing better performance in comparison with\nindividual modalities (i.e., unimodal) systems. Multimodal machine learning\ninvolves multiple aspects: representation, translation, alignment, fusion, and\nco-learning. In the current state of multimodal machine learning, the\nassumptions are that all modalities are present, aligned, and noiseless during\ntraining and testing time. However, in real-world tasks, typically, it is\nobserved that one or more modalities are missing, noisy, lacking annotated\ndata, have unreliable labels, and are scarce in training or testing and or\nboth. This challenge is addressed by a learning paradigm called multimodal\nco-learning. The modeling of a (resource-poor) modality is aided by exploiting\nknowledge from another (resource-rich) modality using transfer of knowledge\nbetween modalities, including their representations and predictive models.\nCo-learning being an emerging area, there are no dedicated reviews explicitly\nfocusing on all challenges addressed by co-learning. To that end, in this work,\nwe provide a comprehensive survey on the emerging area of multimodal\nco-learning that has not been explored in its entirety yet. We review\nimplementations that overcome one or more co-learning challenges without\nexplicitly considering them as co-learning challenges. We present the\ncomprehensive taxonomy of multimodal co-learning based on the challenges\naddressed by co-learning and associated implementations. The various techniques\nemployed to include the latest ones are reviewed along with some of the\napplications and datasets. Our final goal is to discuss challenges and\nperspectives along with the important ideas and directions for future work that\nwe hope to be beneficial for the entire research community focusing on this\nexciting domain.",
          "link": "http://arxiv.org/abs/2107.13782",
          "publishedOn": "2021-07-30T02:13:29.552Z",
          "wordCount": 713,
          "title": "Multimodal Co-learning: Challenges, Applications with Datasets, Recent Advances and Future Directions. (arXiv:2107.13782v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdary_G/0/1/0/all/0/1\">G Jignesh Chowdary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_S/0/1/0/all/0/1\">Suganya G</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M_P/0/1/0/all/0/1\">Premalatha M</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Y_A/0/1/0/all/0/1\">Asnath Victy Phamila Y</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1\">Karunamurthy K</a>",
          "description": "With the advancements in computer technology, there is a rapid development of\nintelligent systems to understand the complex relationships in data to make\npredictions and classifications. Artificail Intelligence based framework is\nrapidly revolutionizing the healthcare industry. These intelligent systems are\nbuilt with machine learning and deep learning based robust models for early\ndiagnosis of diseases and demonstrates a promising supplementary diagnostic\nmethod for frontline clinical doctors and surgeons. Machine Learning and Deep\nLearning based systems can streamline and simplify the steps involved in\ndiagnosis of diseases from clinical and image-based data, thus providing\nsignificant clinician support and workflow optimization. They mimic human\ncognition and are even capable of diagnosing diseases that cannot be diagnosed\nwith human intelligence. This paper focuses on the survey of machine learning\nand deep learning applications in across 16 medical specialties, namely Dental\nmedicine, Haematology, Surgery, Cardiology, Pulmonology, Orthopedics,\nRadiology, Oncology, General medicine, Psychiatry, Endocrinology, Neurology,\nDermatology, Hepatology, Nephrology, Ophthalmology, and Drug discovery. In this\npaper along with the survey, we discuss the advancements of medical practices\nwith these systems and also the impact of these systems on medical\nprofessionals.",
          "link": "http://arxiv.org/abs/2107.14037",
          "publishedOn": "2021-07-30T02:13:29.521Z",
          "wordCount": 644,
          "title": "Machine Learning and Deep Learning Methods for Building Intelligent Systems in Medicine and Drug Discovery: A Comprehensive Survey. (arXiv:2107.14037v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozkara_K/0/1/0/all/0/1\">Kaan Ozkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Navjot Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Data_D/0/1/0/all/0/1\">Deepesh Data</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diggavi_S/0/1/0/all/0/1\">Suhas Diggavi</a>",
          "description": "Traditionally, federated learning (FL) aims to train a single global model\nwhile collaboratively using multiple clients and a server. Two natural\nchallenges that FL algorithms face are heterogeneity in data across clients and\ncollaboration of clients with {\\em diverse resources}. In this work, we\nintroduce a \\textit{quantized} and \\textit{personalized} FL algorithm QuPeD\nthat facilitates collective (personalized model compression) training via\n\\textit{knowledge distillation} (KD) among clients who have access to\nheterogeneous data and resources. For personalization, we allow clients to\nlearn \\textit{compressed personalized models} with different quantization\nparameters and model dimensions/structures. Towards this, first we propose an\nalgorithm for learning quantized models through a relaxed optimization problem,\nwhere quantization values are also optimized over. When each client\nparticipating in the (federated) learning process has different requirements\nfor the compressed model (both in model dimension and precision), we formulate\na compressed personalization framework by introducing knowledge distillation\nloss for local client objectives collaborating through a global model. We\ndevelop an alternating proximal gradient update for solving this compressed\npersonalization problem, and analyze its convergence properties. Numerically,\nwe validate that QuPeD outperforms competing personalized FL methods, FedAvg,\nand local training of clients in various heterogeneous settings.",
          "link": "http://arxiv.org/abs/2107.13892",
          "publishedOn": "2021-07-30T02:13:29.461Z",
          "wordCount": 635,
          "title": "QuPeD: Quantized Personalization via Distillation with Applications to Federated Learning. (arXiv:2107.13892v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14035",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mike Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piech_C/0/1/0/all/0/1\">Chris Piech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>",
          "description": "High-quality computer science education is limited by the difficulty of\nproviding instructor feedback to students at scale. While this feedback could\nin principle be automated, supervised approaches to predicting the correct\nfeedback are bottlenecked by the intractability of annotating large quantities\nof student code. In this paper, we instead frame the problem of providing\nfeedback as few-shot classification, where a meta-learner adapts to give\nfeedback to student code on a new programming question from just a few examples\nannotated by instructors. Because data for meta-training is limited, we propose\na number of amendments to the typical few-shot learning framework, including\ntask augmentation to create synthetic tasks, and additional side information to\nbuild stronger priors about each task. These additions are combined with a\ntransformer architecture to embed discrete sequences (e.g. code) to a\nprototypical representation of a feedback class label. On a suite of few-shot\nnatural language processing tasks, we match or outperform state-of-the-art\nperformance. Then, on a collection of student solutions to exam questions from\nan introductory university course, we show that our approach reaches an average\nprecision of 88% on unseen questions, surpassing the 82% precision of teaching\nassistants. Our approach was successfully deployed to deliver feedback to\n16,000 student exam-solutions in a programming course offered by a tier 1\nuniversity. This is, to the best of our knowledge, the first successful\ndeployment of a machine learning based feedback to open-ended student code.",
          "link": "http://arxiv.org/abs/2107.14035",
          "publishedOn": "2021-07-30T02:13:29.453Z",
          "wordCount": 677,
          "title": "ProtoTransformer: A Meta-Learning Approach to Providing Student Feedback. (arXiv:2107.14035v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scabini_L/0/1/0/all/0/1\">Leonardo F. S. Scabini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruno_O/0/1/0/all/0/1\">Odemir M. Bruno</a>",
          "description": "Understanding the behavior of Artificial Neural Networks is one of the main\ntopics in the field recently, as black-box approaches have become usual since\nthe widespread of deep learning. Such high-dimensional models may manifest\ninstabilities and weird properties that resemble complex systems. Therefore, we\npropose Complex Network (CN) techniques to analyze the structure and\nperformance of fully connected neural networks. For that, we build a dataset\nwith 4 thousand models and their respective CN properties. They are employed in\na supervised classification setup considering four vision benchmarks. Each\nneural network is approached as a weighted and undirected graph of neurons and\nsynapses, and centrality measures are computed after training. Results show\nthat these measures are highly related to the network classification\nperformance. We also propose the concept of Bag-Of-Neurons (BoN), a CN-based\napproach for finding topological signatures linking similar neurons. Results\nsuggest that six neuronal types emerge in such networks, independently of the\ntarget domain, and are distributed differently according to classification\naccuracy. We also tackle specific CN properties related to performance, such as\nhigher subgraph centrality on lower-performing models. Our findings suggest\nthat CN properties play a critical role in the performance of fully connected\nneural networks, with topological patterns emerging independently on a wide\nrange of models.",
          "link": "http://arxiv.org/abs/2107.14062",
          "publishedOn": "2021-07-30T02:13:29.434Z",
          "wordCount": 686,
          "title": "Structure and Performance of Fully Connected Neural Networks: Emerging Complex Network Properties. (arXiv:2107.14062v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiaodian Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wanhang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shuihai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>",
          "description": "In recent years, federated learning (FL) has been widely applied for\nsupporting decentralized collaborative learning scenarios. Among existing FL\nmodels, federated logistic regression (FLR) is a widely used statistic model\nand has been used in various industries. To ensure data security and user\nprivacy, FLR leverages homomorphic encryption (HE) to protect the exchanged\ndata among different collaborative parties. However, HE introduces significant\ncomputational overhead (i.e., the cost of data encryption/decryption and\ncalculation over encrypted data), which eventually becomes the performance\nbottleneck of the whole system. In this paper, we propose HAFLO, a GPU-based\nsolution to improve the performance of FLR. The core idea of HAFLO is to\nsummarize a set of performance-critical homomorphic operators (HO) used by FLR\nand accelerate the execution of these operators through a joint optimization of\nstorage, IO, and computation. The preliminary results show that our\nacceleration on FATE, a popular FL framework, achieves a 49.9$\\times$ speedup\nfor heterogeneous LR and 88.4$\\times$ for homogeneous LR.",
          "link": "http://arxiv.org/abs/2107.13797",
          "publishedOn": "2021-07-30T02:13:29.428Z",
          "wordCount": 589,
          "title": "HAFLO: GPU-Based Acceleration for Federated Logistic Regression. (arXiv:2107.13797v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Simeunovic_J/0/1/0/all/0/1\">Jelena Simeunovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubnel_B/0/1/0/all/0/1\">Baptiste Schubnel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alet_P/0/1/0/all/0/1\">Pierre-Jean Alet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrillo_R/0/1/0/all/0/1\">Rafael E. Carrillo</a>",
          "description": "Accurate forecasting of solar power generation with fine temporal and spatial\nresolution is vital for the operation of the power grid. However,\nstate-of-the-art approaches that combine machine learning with numerical\nweather predictions (NWP) have coarse resolution. In this paper, we take a\ngraph signal processing perspective and model multi-site photovoltaic (PV)\nproduction time series as signals on a graph to capture their spatio-temporal\ndependencies and achieve higher spatial and temporal resolution forecasts. We\npresent two novel graph neural network models for deterministic multi-site PV\nforecasting dubbed the graph-convolutional long short term memory (GCLSTM) and\nthe graph-convolutional transformer (GCTrafo) models. These methods rely solely\non production data and exploit the intuition that PV systems provide a dense\nnetwork of virtual weather stations. The proposed methods were evaluated in two\ndata sets for an entire year: 1) production data from 304 real PV systems, and\n2) simulated production of 1000 PV systems, both distributed over Switzerland.\nThe proposed models outperform state-of-the-art multi-site forecasting methods\nfor prediction horizons of six hours ahead. Furthermore, the proposed models\noutperform state-of-the-art single-site methods with NWP as inputs on horizons\nup to four hours ahead.",
          "link": "http://arxiv.org/abs/2107.13875",
          "publishedOn": "2021-07-30T02:13:29.421Z",
          "wordCount": 638,
          "title": "Spatio-temporal graph neural networks for multi-site PV power forecasting. (arXiv:2107.13875v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13657",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Goel_G/0/1/0/all/0/1\">Gautam Goel</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hassibi_B/0/1/0/all/0/1\">Babak Hassibi</a>",
          "description": "We consider control from the perspective of competitive analysis. Unlike much\nprior work on learning-based control, which focuses on minimizing regret\nagainst the best controller selected in hindsight from some specific class, we\nfocus on designing an online controller which competes against a clairvoyant\noffline optimal controller. A natural performance metric in this setting is\ncompetitive ratio, which is the ratio between the cost incurred by the online\ncontroller and the cost incurred by the offline optimal controller. Using\noperator-theoretic techniques from robust control, we derive a computationally\nefficient state-space description of the the controller with optimal\ncompetitive ratio in both finite-horizon and infinite-horizon settings. We\nextend competitive control to nonlinear systems using Model Predictive Control\n(MPC) and present numerical experiments which show that our competitive\ncontroller can significantly outperform standard $H_2$ and $H_{\\infty}$\ncontrollers in the MPC setting.",
          "link": "http://arxiv.org/abs/2107.13657",
          "publishedOn": "2021-07-30T02:13:29.415Z",
          "wordCount": 559,
          "title": "Competitive Control. (arXiv:2107.13657v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14112",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Baucas_M/0/1/0/all/0/1\">Marc Jayson Baucas</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Spachos_P/0/1/0/all/0/1\">Petros Spachos</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gregori_S/0/1/0/all/0/1\">Stefano Gregori</a>",
          "description": "Medical conditions and cases are growing at a rapid pace, where physical\nspace is starting to be constrained. Hospitals and clinics no longer have the\nability to accommodate large numbers of incoming patients. It is clear that the\ncurrent state of the health industry needs to improve its valuable and limited\nresources. The evolution of the Internet of Things (IoT) devices along with\nassistive technologies can alleviate the problem in healthcare, by being a\nconvenient and easy means of accessing healthcare services wirelessly. There is\na plethora of IoT devices and potential applications that can take advantage of\nthe unique characteristics that these technologies can offer. However, at the\nsame time, these services pose novel challenges that need to be properly\naddressed. In this article, we review some popular categories of IoT-based\napplications for healthcare along with their devices. Then, we describe the\nchallenges and discuss how research can properly address the open issues and\nimprove the already existing implementations in healthcare. Further possible\nsolutions are also discussed to show their potential in being viable solutions\nfor future healthcare applications",
          "link": "http://arxiv.org/abs/2107.14112",
          "publishedOn": "2021-07-30T02:13:29.409Z",
          "wordCount": 647,
          "title": "Internet-of-Things Devices and Assistive Technologies for Healthcare: Applications, Challenges, and Opportunities. (arXiv:2107.14112v1 [physics.soc-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13586",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weizhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengbao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1\">Hiroaki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "This paper surveys and organizes research works in a new paradigm in natural\nlanguage processing, which we dub \"prompt-based learning\". Unlike traditional\nsupervised learning, which trains a model to take in an input x and predict an\noutput y as P(y|x), prompt-based learning is based on language models that\nmodel the probability of text directly. To use these models to perform\nprediction tasks, the original input x is modified using a template into a\ntextual string prompt x' that has some unfilled slots, and then the language\nmodel is used to probabilistically fill the unfilled information to obtain a\nfinal string x, from which the final output y can be derived. This framework is\npowerful and attractive for a number of reasons: it allows the language model\nto be pre-trained on massive amounts of raw text, and by defining a new\nprompting function the model is able to perform few-shot or even zero-shot\nlearning, adapting to new scenarios with few or no labeled data. In this paper\nwe introduce the basics of this promising paradigm, describe a unified set of\nmathematical notations that can cover a wide variety of existing work, and\norganize existing work along several dimensions, e.g.the choice of pre-trained\nmodels, prompts, and tuning strategies. To make the field more accessible to\ninterested beginners, we not only make a systematic review of existing works\nand a highly structured typology of prompt-based concepts, but also release\nother resources, e.g., a website this http URL including\nconstantly-updated survey, and paperlist.",
          "link": "http://arxiv.org/abs/2107.13586",
          "publishedOn": "2021-07-30T02:13:29.395Z",
          "wordCount": 710,
          "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. (arXiv:2107.13586v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raschka_S/0/1/0/all/0/1\">Sebastian Raschka</a>",
          "description": "Machine learning has seen a vast increase of interest in recent years, along\nwith an abundance of learning resources. While conventional lectures provide\nstudents with important information and knowledge, we also believe that\nadditional project-based learning components can motivate students to engage in\ntopics more deeply. In addition to incorporating project-based learning in our\ncourses, we aim to develop project-based learning components aligned with\nreal-world tasks, including experimental design and execution, report writing,\noral presentation, and peer-reviewing. This paper describes the organization of\nour project-based machine learning courses with a particular emphasis on the\nclass project components and shares our resources with instructors who would\nlike to include similar elements in their courses.",
          "link": "http://arxiv.org/abs/2107.13671",
          "publishedOn": "2021-07-30T02:13:29.389Z",
          "wordCount": 579,
          "title": "Deeper Learning By Doing: Integrating Hands-On Research Projects Into a Machine Learning Course. (arXiv:2107.13671v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyons_M/0/1/0/all/0/1\">Michael J. Lyons</a>",
          "description": "Twenty-five years ago, my colleagues Miyuki Kamachi and Jiro Gyoba and I\ndesigned and photographed JAFFE, a set of facial expression images intended for\nuse in a study of face perception. In 2019, without seeking permission or\ninforming us, Kate Crawford and Trevor Paglen exhibited JAFFE in two widely\npublicized art shows. In addition, they published a nonfactual account of the\nimages in the essay \"Excavating AI: The Politics of Images in Machine Learning\nTraining Sets.\" The present article recounts the creation of the JAFFE dataset\nand unravels each of Crawford and Paglen's fallacious statements. I also\ndiscuss JAFFE more broadly in connection with research on facial expression,\naffective computing, and human-computer interaction.",
          "link": "http://arxiv.org/abs/2107.13998",
          "publishedOn": "2021-07-30T02:13:29.381Z",
          "wordCount": 572,
          "title": "\"Excavating AI\" Re-excavated: Debunking a Fallacious Account of the JAFFE Dataset. (arXiv:2107.13998v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lordelo_C/0/1/0/all/0/1\">Carlos Lordelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1\">Emmanouil Benetos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixon_S/0/1/0/all/0/1\">Simon Dixon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahlback_S/0/1/0/all/0/1\">Sven Ahlb&#xe4;ck</a>",
          "description": "This paper proposes a deep convolutional neural network for performing\nnote-level instrument assignment. Given a polyphonic multi-instrumental music\nsignal along with its ground truth or predicted notes, the objective is to\nassign an instrumental source for each note. This problem is addressed as a\npitch-informed classification task where each note is analysed individually. We\nalso propose to utilise several kernel shapes in the convolutional layers in\norder to facilitate learning of efficient timbre-discriminative feature maps.\nExperiments on the MusicNet dataset using 7 instrument classes show that our\napproach is able to achieve an average F-score of 0.904 when the original\nmulti-pitch annotations are used as the pitch information for the system, and\nthat it also excels if the note information is provided using third-party\nmulti-pitch estimation algorithms. We also include ablation studies\ninvestigating the effects of the use of multiple kernel shapes and comparing\ndifferent input representations for the audio and the note-related information.",
          "link": "http://arxiv.org/abs/2107.13617",
          "publishedOn": "2021-07-30T02:13:29.376Z",
          "wordCount": 625,
          "title": "Pitch-Informed Instrument Assignment Using a Deep Convolutional Network with Multiple Kernel Shapes. (arXiv:2107.13617v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Koushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishmam_A/0/1/0/all/0/1\">Abtahi Ishmam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taher_K/0/1/0/all/0/1\">Kazi Abu Taher</a>",
          "description": "Demand forecasting in power sector has become an important part of modern\ndemand management and response systems with the rise of smart metering enabled\ngrids. Long Short-Term Memory (LSTM) shows promising results in predicting time\nseries data which can also be applied to power load demand in smart grids. In\nthis paper, an LSTM based model using neural network architecture is proposed\nto forecast power demand. The model is trained with hourly energy and power\nusage data of four years from a smart grid. After training and prediction, the\naccuracy of the model is compared against the traditional statistical time\nseries analysis algorithms, such as Auto-Regressive (AR), to determine the\nefficiency. The mean absolute percentile error is found to be 1.22 in the\nproposed LSTM model, which is the lowest among the other models. From the\nfindings, it is clear that the inclusion of neural network in predicting power\ndemand reduces the error of prediction significantly. Thus, the application of\nLSTM can enable a more efficient demand response system.",
          "link": "http://arxiv.org/abs/2107.13653",
          "publishedOn": "2021-07-30T02:13:29.370Z",
          "wordCount": 621,
          "title": "Demand Forecasting in Smart Grid Using Long Short-Term Memory. (arXiv:2107.13653v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antaris_S/0/1/0/all/0/1\">Stefanos Antaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafailidis_D/0/1/0/all/0/1\">Dimitrios Rafailidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girdzijauskas_S/0/1/0/all/0/1\">Sarunas Girdzijauskas</a>",
          "description": "In this paper we present a deep graph reinforcement learning model to predict\nand improve the user experience during a live video streaming event,\norchestrated by an agent/tracker. We first formulate the user experience\nprediction problem as a classification task, accounting for the fact that most\nof the viewers at the beginning of an event have poor quality of experience due\nto low-bandwidth connections and limited interactions with the tracker. In our\nmodel we consider different factors that influence the quality of user\nexperience and train the proposed model on diverse state-action transitions\nwhen viewers interact with the tracker. In addition, provided that past events\nhave various user experience characteristics we follow a gradient boosting\nstrategy to compute a global model that learns from different events. Our\nexperiments with three real-world datasets of live video streaming events\ndemonstrate the superiority of the proposed model against several baseline\nstrategies. Moreover, as the majority of the viewers at the beginning of an\nevent has poor experience, we show that our model can significantly increase\nthe number of viewers with high quality experience by at least 75% over the\nfirst streaming minutes. Our evaluation datasets and implementation are\npublicly available at https://publicresearch.z13.web.core.windows.net",
          "link": "http://arxiv.org/abs/2107.13619",
          "publishedOn": "2021-07-30T02:13:29.355Z",
          "wordCount": 644,
          "title": "A Deep Graph Reinforcement Learning Model for Improving User Experience in Live Video Streaming. (arXiv:2107.13619v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaulwar_A/0/1/0/all/0/1\">Amit Chaulwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huth_M/0/1/0/all/0/1\">Michael Huth</a>",
          "description": "Federated analytics has many applications in edge computing, its use can lead\nto better decision making for service provision, product development, and user\nexperience. We propose a Bayesian approach to trend detection in which the\nprobability of a keyword being trendy, given a dataset, is computed via Bayes'\nTheorem; the probability of a dataset, given that a keyword is trendy, is\ncomputed through secure aggregation of such conditional probabilities over\nlocal datasets of users. We propose a protocol, named SAFE, for Bayesian\nfederated analytics that offers sufficient privacy for production grade use\ncases and reduces the computational burden of users and an aggregator. We\nillustrate this approach with a trend detection experiment and discuss how this\napproach could be extended further to make it production-ready.",
          "link": "http://arxiv.org/abs/2107.13640",
          "publishedOn": "2021-07-30T02:13:29.348Z",
          "wordCount": 566,
          "title": "Secure Bayesian Federated Analytics for Privacy-Preserving Trend Detection. (arXiv:2107.13640v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deldjoo_Y/0/1/0/all/0/1\">Yashar Deldjoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merra_F/0/1/0/all/0/1\">Felice Antonio Merra</a>",
          "description": "Recommender systems (RSs) employ user-item feedback, e.g., ratings, to match\ncustomers to personalized lists of products. Approaches to top-k recommendation\nmainly rely on Learning-To-Rank algorithms and, among them, the most widely\nadopted is Bayesian Personalized Ranking (BPR), which bases on a pair-wise\noptimization approach. Recently, BPR has been found vulnerable against\nadversarial perturbations of its model parameters. Adversarial Personalized\nRanking (APR) mitigates this issue by robustifying BPR via an adversarial\ntraining procedure. The empirical improvements of APR's accuracy performance on\nBPR have led to its wide use in several recommender models. However, a key\noverlooked aspect has been the beyond-accuracy performance of APR, i.e.,\nnovelty, coverage, and amplification of popularity bias, considering that\nrecent results suggest that BPR, the building block of APR, is sensitive to the\nintensification of biases and reduction of recommendation novelty. In this\nwork, we model the learning characteristics of the BPR and APR optimization\nframeworks to give mathematical evidence that, when the feedback data have a\ntailed distribution, APR amplifies the popularity bias more than BPR due to an\nunbalanced number of received positive updates from short-head items. Using\nmatrix factorization (MF), we empirically validate the theoretical results by\nperforming preliminary experiments on two public datasets to compare BPR-MF and\nAPR-MF performance on accuracy and beyond-accuracy metrics. The experimental\nresults consistently show the degradation of novelty and coverage measures and\na worrying amplification of bias.",
          "link": "http://arxiv.org/abs/2107.13876",
          "publishedOn": "2021-07-30T02:13:29.341Z",
          "wordCount": 687,
          "title": "Understanding the Effects of Adversarial Personalized Ranking Optimization Method on Recommendation Quality. (arXiv:2107.13876v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yichun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>",
          "description": "Pre-trained language models (PLMs) have achieved great success in natural\nlanguage processing. Most of PLMs follow the default setting of architecture\nhyper-parameters (e.g., the hidden dimension is a quarter of the intermediate\ndimension in feed-forward sub-networks) in BERT (Devlin et al., 2019). Few\nstudies have been conducted to explore the design of architecture\nhyper-parameters in BERT, especially for the more efficient PLMs with tiny\nsizes, which are essential for practical deployment on resource-constrained\ndevices. In this paper, we adopt the one-shot Neural Architecture Search (NAS)\nto automatically search architecture hyper-parameters. Specifically, we\ncarefully design the techniques of one-shot learning and the search space to\nprovide an adaptive and efficient development way of tiny PLMs for various\nlatency constraints. We name our method AutoTinyBERT and evaluate its\neffectiveness on the GLUE and SQuAD benchmarks. The extensive experiments show\nthat our method outperforms both the SOTA search-based baseline (NAS-BERT) and\nthe SOTA distillation-based methods (such as DistilBERT, TinyBERT, MiniLM and\nMobileBERT). In addition, based on the obtained architectures, we propose a\nmore efficient development method that is even faster than the development of a\nsingle PLM.",
          "link": "http://arxiv.org/abs/2107.13686",
          "publishedOn": "2021-07-30T02:13:29.334Z",
          "wordCount": 639,
          "title": "AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models. (arXiv:2107.13686v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aitio_A/0/1/0/all/0/1\">Antti Aitio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howey_D/0/1/0/all/0/1\">David A. Howey</a>",
          "description": "Hundreds of millions of people lack access to electricity. Decentralised\nsolar-battery systems are key for addressing this whilst avoiding carbon\nemissions and air pollution, but are hindered by relatively high costs and\nrural locations that inhibit timely preventative maintenance. Accurate\ndiagnosis of battery health and prediction of end of life from operational data\nimproves user experience and reduces costs. But lack of controlled validation\ntests and variable data quality mean existing lab-based techniques fail to\nwork. We apply a scaleable probabilistic machine learning approach to diagnose\nhealth in 1027 solar-connected lead-acid batteries, each running for 400-760\ndays, totalling 620 million data rows. We demonstrate 73% accurate prediction\nof end of life, eight weeks in advance, rising to 82% at the point of failure.\nThis work highlights the opportunity to estimate health from existing\nmeasurements using `big data' techniques, without additional equipment,\nextending lifetime and improving performance in real-world applications.",
          "link": "http://arxiv.org/abs/2107.13856",
          "publishedOn": "2021-07-30T02:13:29.328Z",
          "wordCount": 599,
          "title": "Predicting battery end of life from solar off-grid system field data using machine learning. (arXiv:2107.13856v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wentao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Han Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaorui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thuraisingham_B/0/1/0/all/0/1\">Bhavani Thuraisingham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>",
          "description": "Adversarial training has been empirically proven to be one of the most\neffective and reliable defense methods against adversarial attacks. However,\nalmost all existing studies about adversarial training are focused on balanced\ndatasets, where each class has an equal amount of training examples. Research\non adversarial training with imbalanced training datasets is rather limited. As\nthe initial effort to investigate this problem, we reveal the facts that\nadversarially trained models present two distinguished behaviors from naturally\ntrained models in imbalanced datasets: (1) Compared to natural training,\nadversarially trained models can suffer much worse performance on\nunder-represented classes, when the training dataset is extremely imbalanced.\n(2) Traditional reweighting strategies may lose efficacy to deal with the\nimbalance issue for adversarial training. For example, upweighting the\nunder-represented classes will drastically hurt the model's performance on\nwell-represented classes, and as a result, finding an optimal reweighting value\ncan be tremendously challenging. In this paper, to further understand our\nobservations, we theoretically show that the poor data separability is one key\nreason causing this strong tension between under-represented and\nwell-represented classes. Motivated by this finding, we propose Separable\nReweighted Adversarial Training (SRAT) to facilitate adversarial training under\nimbalanced scenarios, by learning more separable features for different\nclasses. Extensive experiments on various datasets verify the effectiveness of\nthe proposed framework.",
          "link": "http://arxiv.org/abs/2107.13639",
          "publishedOn": "2021-07-30T02:13:29.313Z",
          "wordCount": 643,
          "title": "Imbalanced Adversarial Training with Reweighting. (arXiv:2107.13639v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bachinger_F/0/1/0/all/0/1\">Florian Bachinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1\">Gabriel Kronberger</a>",
          "description": "With the increasing number of created and deployed prediction models and the\ncomplexity of machine learning workflows we require so called model management\nsystems to support data scientists in their tasks. In this work we describe our\ntechnological concept for such a model management system. This concept includes\nversioned storage of data, support for different machine learning algorithms,\nfine tuning of models, subsequent deployment of models and monitoring of model\nperformance after deployment. We describe this concept with a close focus on\nmodel lifecycle requirements stemming from our industry application cases, but\ngeneralize key features that are relevant for all applications of machine\nlearning.",
          "link": "http://arxiv.org/abs/2107.13821",
          "publishedOn": "2021-07-30T02:13:29.306Z",
          "wordCount": 583,
          "title": "Concept for a Technical Infrastructure for Management of Predictive Models in Industrial Applications. (arXiv:2107.13821v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xinyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dongjin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuncong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengzhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingchao Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>",
          "description": "Detecting abnormal activities in real-world surveillance videos is an\nimportant yet challenging task as the prior knowledge about video anomalies is\nusually limited or unavailable. Despite that many approaches have been\ndeveloped to resolve this problem, few of them can capture the normal\nspatio-temporal patterns effectively and efficiently. Moreover, existing works\nseldom explicitly consider the local consistency at frame level and global\ncoherence of temporal dynamics in video sequences. To this end, we propose\nConvolutional Transformer based Dual Discriminator Generative Adversarial\nNetworks (CT-D2GAN) to perform unsupervised video anomaly detection.\nSpecifically, we first present a convolutional transformer to perform future\nframe prediction. It contains three key components, i.e., a convolutional\nencoder to capture the spatial information of the input video clips, a temporal\nself-attention module to encode the temporal dynamics, and a convolutional\ndecoder to integrate spatio-temporal features and predict the future frame.\nNext, a dual discriminator based adversarial training procedure, which jointly\nconsiders an image discriminator that can maintain the local consistency at\nframe-level and a video discriminator that can enforce the global coherence of\ntemporal dynamics, is employed to enhance the future frame prediction. Finally,\nthe prediction error is used to identify abnormal video frames. Thoroughly\nempirical studies on three public video anomaly detection datasets, i.e., UCSD\nPed2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the effectiveness of\nthe proposed adversarial spatio-temporal modeling framework.",
          "link": "http://arxiv.org/abs/2107.13720",
          "publishedOn": "2021-07-30T02:13:29.299Z",
          "wordCount": 708,
          "title": "Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for Video Anomaly Detection. (arXiv:2107.13720v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goh_H/0/1/0/all/0/1\">Hoe-Han Goh</a>",
          "description": "This perspective illustrates some of the AI applications that can accelerate\nthe achievement of SDGs and also highlights some of the considerations that\ncould hinder the efforts towards them. This emphasizes the importance of\nestablishing standard AI guidelines and regulations for the beneficial\napplications of AI.",
          "link": "http://arxiv.org/abs/2107.13966",
          "publishedOn": "2021-07-30T02:13:29.293Z",
          "wordCount": 487,
          "title": "Artificial Intelligence in Achieving Sustainable Development Goals. (arXiv:2107.13966v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aminian_G/0/1/0/all/0/1\">Gholamali Aminian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_Y/0/1/0/all/0/1\">Yuheng Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toni_L/0/1/0/all/0/1\">Laura Toni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_M/0/1/0/all/0/1\">Miguel R. D. Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wornell_G/0/1/0/all/0/1\">Gregory Wornell</a>",
          "description": "Bounding the generalization error of a supervised learning algorithm is one\nof the most important problems in learning theory, and various approaches have\nbeen developed. However, existing bounds are often loose and lack of\nguarantees. As a result, they may fail to characterize the exact generalization\nability of a learning algorithm. Our main contribution is an exact\ncharacterization of the expected generalization error of the well-known Gibbs\nalgorithm in terms of symmetrized KL information between the input training\nsamples and the output hypothesis. Such a result can be applied to tighten\nexisting expected generalization error bound. Our analysis provides more\ninsight on the fundamental role the symmetrized KL information plays in\ncontrolling the generalization error of the Gibbs algorithm.",
          "link": "http://arxiv.org/abs/2107.13656",
          "publishedOn": "2021-07-30T02:13:29.277Z",
          "wordCount": 608,
          "title": "Characterizing the Generalization Error of Gibbs Algorithm with Symmetrized KL information. (arXiv:2107.13656v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Otles_E/0/1/0/all/0/1\">Erkin &#xd6;tle&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jeeheh Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Benjamin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bochinski_M/0/1/0/all/0/1\">Michelle Bochinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1\">Hyeon Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortwine_J/0/1/0/all/0/1\">Justin Ortwine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_E/0/1/0/all/0/1\">Erica Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washer_L/0/1/0/all/0/1\">Laraine Washer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_V/0/1/0/all/0/1\">Vincent B. Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">Krishna Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1\">Jenna Wiens</a>",
          "description": "Once integrated into clinical care, patient risk stratification models may\nperform worse compared to their retrospective performance. To date, it is\nwidely accepted that performance will degrade over time due to changes in care\nprocesses and patient populations. However, the extent to which this occurs is\npoorly understood, in part because few researchers report prospective\nvalidation performance. In this study, we compare the 2020-2021 ('20-'21)\nprospective performance of a patient risk stratification model for predicting\nhealthcare-associated infections to a 2019-2020 ('19-'20) retrospective\nvalidation of the same model. We define the difference in retrospective and\nprospective performance as the performance gap. We estimate how i) \"temporal\nshift\", i.e., changes in clinical workflows and patient populations, and ii)\n\"infrastructure shift\", i.e., changes in access, extraction and transformation\nof data, both contribute to the performance gap. Applied prospectively to\n26,864 hospital encounters during a twelve-month period from July 2020 to June\n2021, the model achieved an area under the receiver operating characteristic\ncurve (AUROC) of 0.767 (95% confidence interval (CI): 0.737, 0.801) and a Brier\nscore of 0.189 (95% CI: 0.186, 0.191). Prospective performance decreased\nslightly compared to '19-'20 retrospective performance, in which the model\nachieved an AUROC of 0.778 (95% CI: 0.744, 0.815) and a Brier score of 0.163\n(95% CI: 0.161, 0.165). The resulting performance gap was primarily due to\ninfrastructure shift and not temporal shift. So long as we continue to develop\nand validate models using data stored in large research data warehouses, we\nmust consider differences in how and when data are accessed, measure how these\ndifferences may affect prospective performance, and work to mitigate those\ndifferences.",
          "link": "http://arxiv.org/abs/2107.13964",
          "publishedOn": "2021-07-30T02:13:29.247Z",
          "wordCount": 733,
          "title": "Mind the Performance Gap: Examining Dataset Shift During Prospective Validation. (arXiv:2107.13964v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grespan_M/0/1/0/all/0/1\">Mattia Medina Grespan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ashim Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>",
          "description": "Symbolic knowledge can provide crucial inductive bias for training neural\nmodels, especially in low data regimes. A successful strategy for incorporating\nsuch knowledge involves relaxing logical statements into sub-differentiable\nlosses for optimization. In this paper, we study the question of how best to\nrelax logical expressions that represent labeled examples and knowledge about a\nproblem; we focus on sub-differentiable t-norm relaxations of logic. We present\ntheoretical and empirical criteria for characterizing which relaxation would\nperform best in various scenarios. In our theoretical study driven by the goal\nof preserving tautologies, the Lukasiewicz t-norm performs best. However, in\nour empirical analysis on the text chunking and digit recognition tasks, the\nproduct t-norm achieves best predictive performance. We analyze this apparent\ndiscrepancy, and conclude with a list of best practices for defining loss\nfunctions via logic.",
          "link": "http://arxiv.org/abs/2107.13646",
          "publishedOn": "2021-07-30T02:13:29.191Z",
          "wordCount": 578,
          "title": "Evaluating Relaxations of Logic for Neural Networks: A Comprehensive Study. (arXiv:2107.13646v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trillos_N/0/1/0/all/0/1\">Nicolas Garcia Trillos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengfei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenghui Li</a>",
          "description": "In this work we study statistical properties of graph-based algorithms for\nmulti-manifold clustering (MMC). In MMC the goal is to retrieve the\nmulti-manifold structure underlying a given Euclidean data set when this one is\nassumed to be obtained by sampling a distribution on a union of manifolds\n$\\mathcal{M} = \\mathcal{M}_1 \\cup\\dots \\cup \\mathcal{M}_N$ that may intersect\nwith each other and that may have different dimensions. We investigate\nsufficient conditions that similarity graphs on data sets must satisfy in order\nfor their corresponding graph Laplacians to capture the right geometric\ninformation to solve the MMC problem. Precisely, we provide high probability\nerror bounds for the spectral approximation of a tensorized Laplacian on\n$\\mathcal{M}$ with a suitable graph Laplacian built from the observations; the\nrecovered tensorized Laplacian contains all geometric information of all the\nindividual underlying manifolds. We provide an example of a family of\nsimilarity graphs, which we call annular proximity graphs with angle\nconstraints, satisfying these sufficient conditions. We contrast our family of\ngraphs with other constructions in the literature based on the alignment of\ntangent planes. Extensive numerical experiments expand the insights that our\ntheory provides on the MMC problem.",
          "link": "http://arxiv.org/abs/2107.13610",
          "publishedOn": "2021-07-30T02:13:29.165Z",
          "wordCount": 635,
          "title": "Large sample spectral analysis of graph-based multi-manifold clustering. (arXiv:2107.13610v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Agni Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_V/0/1/0/all/0/1\">Vikramjit Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliver_C/0/1/0/all/0/1\">Carolyn Oliver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullal_A/0/1/0/all/0/1\">Adeeti Ullal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biddulph_M/0/1/0/all/0/1\">Matt Biddulph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mance_I/0/1/0/all/0/1\">Irida Mance</a>",
          "description": "Respiratory rate (RR) is a clinical metric used to assess overall health and\nphysical fitness. An individual's RR can change from their baseline due to\nchronic illness symptoms (e.g., asthma, congestive heart failure), acute\nillness (e.g., breathlessness due to infection), and over the course of the day\ndue to physical exhaustion during heightened exertion. Remote estimation of RR\ncan offer a cost-effective method to track disease progression and\ncardio-respiratory fitness over time. This work investigates a model-driven\napproach to estimate RR from short audio segments obtained after physical\nexertion in healthy adults. Data was collected from 21 individuals using\nmicrophone-enabled, near-field headphones before, during, and after strenuous\nexercise. RR was manually annotated by counting perceived inhalations and\nexhalations. A multi-task Long-Short Term Memory (LSTM) network with\nconvolutional layers was implemented to process mel-filterbank energies,\nestimate RR in varying background noise conditions, and predict heavy\nbreathing, indicated by an RR of more than 25 breaths per minute. The\nmulti-task model performs both classification and regression tasks and\nleverages a mixture of loss functions. It was observed that RR can be estimated\nwith a concordance correlation coefficient (CCC) of 0.76 and a mean squared\nerror (MSE) of 0.2, demonstrating that audio can be a viable signal for\napproximating RR.",
          "link": "http://arxiv.org/abs/2107.14028",
          "publishedOn": "2021-07-30T02:13:29.158Z",
          "wordCount": 671,
          "title": "Estimating Respiratory Rate From Breath Audio Obtained Through Wearable Microphones. (arXiv:2107.14028v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dumpala_S/0/1/0/all/0/1\">Sri Harsha Dumpala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_S/0/1/0/all/0/1\">Sebastian Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rempel_S/0/1/0/all/0/1\">Sheri Rempel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uher_R/0/1/0/all/0/1\">Rudolf Uher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oore_S/0/1/0/all/0/1\">Sageev Oore</a>",
          "description": "Depression detection from speech has attracted a lot of attention in recent\nyears. However, the significance of speaker-specific information in depression\ndetection has not yet been explored. In this work, we analyze the significance\nof speaker embeddings for the task of depression detection from speech.\nExperimental results show that the speaker embeddings provide important cues to\nachieve state-of-the-art performance in depression detection. We also show that\ncombining conventional OpenSMILE and COVAREP features, which carry\ncomplementary information, with speaker embeddings further improves the\ndepression detection performance. The significance of temporal context in the\ntraining of deep learning models for depression detection is also analyzed in\nthis paper.",
          "link": "http://arxiv.org/abs/2107.13969",
          "publishedOn": "2021-07-30T02:13:29.151Z",
          "wordCount": 560,
          "title": "Significance of Speaker Embeddings and Temporal Context for Depression Detection. (arXiv:2107.13969v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Breiki_F/0/1/0/all/0/1\">Farha Al Breiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridzuan_M/0/1/0/all/0/1\">Muhammad Ridzuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandhe_R/0/1/0/all/0/1\">Rushali Grandhe</a>",
          "description": "Fine-grained image classification involves identifying different\nsubcategories of a class which possess very subtle discriminatory features.\nFine-grained datasets usually provide bounding box annotations along with class\nlabels to aid the process of classification. However, building large scale\ndatasets with such annotations is a mammoth task. Moreover, this extensive\nannotation is time-consuming and often requires expertise, which is a huge\nbottleneck in building large datasets. On the other hand, self-supervised\nlearning (SSL) exploits the freely available data to generate supervisory\nsignals which act as labels. The features learnt by performing some pretext\ntasks on huge unlabelled data proves to be very helpful for multiple downstream\ntasks.\n\nOur idea is to leverage self-supervision such that the model learns useful\nrepresentations of fine-grained image classes. We experimented with 3 kinds of\nmodels: Jigsaw solving as pretext task, adversarial learning (SRGAN) and\ncontrastive learning based (SimCLR) model. The learned features are used for\ndownstream tasks such as fine-grained image classification. Our code is\navailable at\nthis http URL",
          "link": "http://arxiv.org/abs/2107.13973",
          "publishedOn": "2021-07-30T02:13:29.096Z",
          "wordCount": 608,
          "title": "Self-Supervised Learning for Fine-Grained Image Classification. (arXiv:2107.13973v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14033",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Cui_C/0/1/0/all/0/1\">Chaoran Cui</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Li_X/0/1/0/all/0/1\">Xiaojie Li</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Du_J/0/1/0/all/0/1\">Juan Du</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zhang_C/0/1/0/all/0/1\">Chunyun Zhang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Nie_X/0/1/0/all/0/1\">Xiushan Nie</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Yin_Y/0/1/0/all/0/1\">Yilong Yin</a>",
          "description": "Predicting the future price trends of stocks is a challenging yet intriguing\nproblem given its critical role to help investors make profitable decisions. In\nthis paper, we present a collaborative temporal-relational modeling framework\nfor end-to-end stock trend prediction. The temporal dynamics of stocks is\nfirstly captured with an attention-based recurrent neural network. Then,\ndifferent from existing studies relying on the pairwise correlations between\nstocks, we argue that stocks are naturally connected as a collective group, and\nintroduce the hypergraph structures to jointly characterize the stock\ngroup-wise relationships of industry-belonging and fund-holding. A novel\nhypergraph tri-attention network (HGTAN) is proposed to augment the hypergraph\nconvolutional networks with a hierarchical organization of intra-hyperedge,\ninter-hyperedge, and inter-hypergraph attention modules. In this manner, HGTAN\nadaptively determines the importance of nodes, hyperedges, and hypergraphs\nduring the information propagation among stocks, so that the potential\nsynergies between stock movements can be fully exploited. Extensive experiments\non real-world data demonstrate the effectiveness of our approach. Also, the\nresults of investment simulation show that our approach can achieve a more\ndesirable risk-adjusted return. The data and codes of our work have been\nreleased at https://github.com/lixiaojieff/HGTAN.",
          "link": "http://arxiv.org/abs/2107.14033",
          "publishedOn": "2021-07-30T02:13:28.966Z",
          "wordCount": 630,
          "title": "Temporal-Relational Hypergraph Tri-Attention Networks for Stock Trend Prediction. (arXiv:2107.14033v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13832",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1\">Prerak Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deleforge_A/0/1/0/all/0/1\">Antoine Deleforge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_E/0/1/0/all/0/1\">Emmanuel Vincent</a>",
          "description": "Knowing the geometrical and acoustical parameters of a room may benefit\napplications such as audio augmented reality, speech dereverberation or audio\nforensics. In this paper, we study the problem of jointly estimating the total\nsurface area, the volume, as well as the frequency-dependent reverberation time\nand mean surface absorption of a room in a blind fashion, based on two-channel\nnoisy speech recordings from multiple, unknown source-receiver positions. A\nnovel convolutional neural network architecture leveraging both single- and\ninter-channel cues is proposed and trained on a large, realistic simulated\ndataset. Results on both simulated and real data show that using multiple\nobservations in one room significantly reduces estimation errors and variances\non all target quantities, and that using two channels helps the estimation of\nsurface and volume. The proposed model outperforms a recently proposed blind\nvolume estimation method on the considered datasets.",
          "link": "http://arxiv.org/abs/2107.13832",
          "publishedOn": "2021-07-30T02:13:28.859Z",
          "wordCount": 595,
          "title": "Blind Room Parameter Estimation Using Multiple-Multichannel Speech Recordings. (arXiv:2107.13832v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00719",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Kao_P/0/1/0/all/0/1\">Po-Yu Kao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kao_S/0/1/0/all/0/1\">Shu-Min Kao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Huang_N/0/1/0/all/0/1\">Nan-Lan Huang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Chu Lin</a>",
          "description": "Drug-target interaction (DTI) prediction plays a crucial role in drug\ndiscovery, and deep learning approaches have achieved state-of-the-art\nperformance in this field. We introduce an ensemble of deep learning models\n(EnsembleDLM) for DTI prediction. EnsembleDLM only uses the sequence\ninformation of chemical compounds and proteins, and it aggregates the\npredictions from multiple deep neural networks. This approach not only achieves\nstate-of-the-art performance in Davis and KIBA datasets but also reaches\ncutting-edge performance in the cross-domain applications across different\nbio-activity types and different protein classes. We also demonstrate that\nEnsembleDLM achieves a good performance (Pearson correlation coefficient and\nconcordance index > 0.8) in the new domain with approximately 50% transfer\nlearning data, i.e., the training set has twice as much data as the test set.",
          "link": "http://arxiv.org/abs/2107.00719",
          "publishedOn": "2021-07-29T02:00:11.235Z",
          "wordCount": 588,
          "title": "Toward Drug-Target Interaction Prediction via Ensemble Modeling and Transfer Learning. (arXiv:2107.00719v2 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dsouza_D/0/1/0/all/0/1\">Daniel D&#x27;souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nussbaum_Z/0/1/0/all/0/1\">Zach Nussbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>",
          "description": "As machine learning models are increasingly employed to assist human\ndecision-makers, it becomes critical to communicate the uncertainty associated\nwith these model predictions. However, the majority of work on uncertainty has\nfocused on traditional probabilistic or ranking approaches - where the model\nassigns low probabilities or scores to uncertain examples. While this captures\nwhat examples are challenging for the model, it does not capture the underlying\nsource of the uncertainty. In this work, we seek to identify examples the model\nis uncertain about and characterize the source of said uncertainty. We explore\nthe benefits of designing a targeted intervention - targeted data augmentation\nof the examples where the model is uncertain over the course of training. We\ninvestigate whether the rate of learning in the presence of additional\ninformation differs between atypical and noisy examples? Our results show that\nthis is indeed the case, suggesting that well-designed interventions over the\ncourse of training can be an effective way to characterize and distinguish\nbetween different sources of uncertainty.",
          "link": "http://arxiv.org/abs/2107.13098",
          "publishedOn": "2021-07-29T02:00:11.209Z",
          "wordCount": 618,
          "title": "A Tale Of Two Long Tails. (arXiv:2107.13098v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">Edward S. Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1\">Oleh Rybkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1\">Dinesh Jayaraman</a>",
          "description": "Training visuomotor robot controllers from scratch on a new robot typically\nrequires generating large amounts of robot-specific data. Could we leverage\ndata previously collected on another robot to reduce or even completely remove\nthis need for robot-specific data? We propose a \"robot-aware\" solution paradigm\nthat exploits readily available robot \"self-knowledge\" such as proprioception,\nkinematics, and camera calibration to achieve this. First, we learn modular\ndynamics models that pair a transferable, robot-agnostic world dynamics module\nwith a robot-specific, analytical robot dynamics module. Next, we set up visual\nplanning costs that draw a distinction between the robot self and the world.\nOur experiments on tabletop manipulation tasks in simulation and on real robots\ndemonstrate that these plug-in improvements dramatically boost the\ntransferability of visuomotor controllers, even permitting zero-shot transfer\nonto new robots for the very first time. Project website:\nhttps://hueds.github.io/rac/",
          "link": "http://arxiv.org/abs/2107.09047",
          "publishedOn": "2021-07-29T02:00:11.202Z",
          "wordCount": 603,
          "title": "Know Thyself: Transferable Visuomotor Control Through Robot-Awareness. (arXiv:2107.09047v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuangjia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Ying Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jiahua Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuedong Yang</a>",
          "description": "Constructing appropriate representations of molecules lies at the core of\nnumerous tasks such as material science, chemistry and drug designs. Recent\nresearches abstract molecules as attributed graphs and employ graph neural\nnetworks (GNN) for molecular representation learning, which have made\nremarkable achievements in molecular graph modeling. Albeit powerful, current\nmodels either are based on local aggregation operations and thus miss\nhigher-order graph properties or focus on only node information without fully\nusing the edge information. For this sake, we propose a Communicative Message\nPassing Transformer (CoMPT) neural network to improve the molecular graph\nrepresentation by reinforcing message interactions between nodes and edges\nbased on the Transformer architecture. Unlike the previous transformer-style\nGNNs that treat molecules as fully connected graphs, we introduce a message\ndiffusion mechanism to leverage the graph connectivity inductive bias and\nreduce the message enrichment explosion. Extensive experiments demonstrated\nthat the proposed model obtained superior performances (around 4$\\%$ on\naverage) against state-of-the-art baselines on seven chemical property datasets\n(graph-level tasks) and two chemical shift datasets (node-level tasks). Further\nvisualization studies also indicated a better representation capacity achieved\nby our model.",
          "link": "http://arxiv.org/abs/2107.08773",
          "publishedOn": "2021-07-29T02:00:11.168Z",
          "wordCount": 649,
          "title": "Learning Attributed Graph Representations with Communicative Message Passing Transformer. (arXiv:2107.08773v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.02951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>",
          "description": "How can deep neural networks encode information that corresponds to words in\nhuman speech into raw acoustic data? This paper proposes two neural network\narchitectures for modeling unsupervised lexical learning from raw acoustic\ninputs, ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN),\nthat combine a Deep Convolutional GAN architecture for audio data (WaveGAN;\narXiv:1705.07904) with an information theoretic extension of GAN -- InfoGAN\n(arXiv:1606.03657), and propose a new latent space structure that can model\nfeatural learning simultaneously with a higher level classification and allows\nfor a very low-dimension vector representation of lexical items. Lexical\nlearning is modeled as emergent from an architecture that forces a deep neural\nnetwork to output data such that unique information is retrievable from its\nacoustic outputs. The networks trained on lexical items from TIMIT learn to\nencode unique information corresponding to lexical items in the form of\ncategorical variables in their latent space. By manipulating these variables,\nthe network outputs specific lexical items. The network occasionally outputs\ninnovative lexical items that violate training data, but are linguistically\ninterpretable and highly informative for cognitive modeling and neural network\ninterpretability. Innovative outputs suggest that phonetic and phonological\nrepresentations learned by the network can be productively recombined and\ndirectly paralleled to productivity in human speech: a fiwGAN network trained\non `suit' and `dark' outputs innovative `start', even though it never saw\n`start' or even a [st] sequence in the training data. We also argue that\nsetting latent featural codes to values well beyond training range results in\nalmost categorical generation of prototypical lexical items and reveals\nunderlying values of each latent code.",
          "link": "http://arxiv.org/abs/2006.02951",
          "publishedOn": "2021-07-29T02:00:11.091Z",
          "wordCount": 756,
          "title": "CiwGAN and fiwGAN: Encoding information in acoustic data to model lexical learning with Generative Adversarial Networks. (arXiv:2006.02951v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sayak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>",
          "description": "Floods wreak havoc throughout the world, causing billions of dollars in\ndamages, and uprooting communities, ecosystems and economies. Accurate and\nrobust flood detection including delineating open water flood areas and\nidentifying flood levels can aid in disaster response and mitigation. However,\nestimating flood levels remotely is of essence as physical access to flooded\nareas is limited and the ability to deploy instruments in potential flood zones\ncan be dangerous. Aligning flood extent mapping with local topography can\nprovide a plan-of-action that the disaster response team can consider. Thus,\nremote flood level estimation via satellites like Sentinel-1 can prove to be\nremedial. The Emerging Techniques in Computational Intelligence (ETCI)\ncompetition on Flood Detection tasked participants with predicting flooded\npixels after training with synthetic aperture radar (SAR) images in a\nsupervised setting. We use a cyclical approach involving two stages (1)\ntraining an ensemble model of multiple UNet architectures with available high\nand low confidence labeled data and, (2) generating pseudo labels or low\nconfidence labels on the unlabeled test dataset, and then, combining the\ngenerated labels with the previously available high confidence labeled dataset.\nThis assimilated dataset is used for the next round of training ensemble\nmodels. This cyclical process is repeated until the performance improvement\nplateaus. Additionally, we post process our results with Conditional Random\nFields. Our approach sets a high score on the public leaderboard for the ETCI\ncompetition with 0.7654 IoU. Our method, which we release with all the code\nincluding trained models, can also be used as an open science benchmark for the\nSentinel-1 released dataset on GitHub. To the best of our knowledge we believe\nthis the first works to try out semi-supervised learning to improve flood\nsegmentation models.",
          "link": "http://arxiv.org/abs/2107.08369",
          "publishedOn": "2021-07-29T02:00:11.083Z",
          "wordCount": 774,
          "title": "Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Navarro_B_J/0/1/0/all/0/1\">J.-Emeterio Navarro-B</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gebert_M/0/1/0/all/0/1\">Martin Gebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielig_R/0/1/0/all/0/1\">Ralf Bielig</a>",
          "description": "This article proposes two different approaches to automatically create a map\nfor valid on-street car parking spaces. For this, we use car sharing park-out\nevents data. The first one uses spatial aggregation and the second a machine\nlearning algorithm. For the former, we chose rasterization and road sectioning;\nfor the latter we chose decision trees. We compare the results of these\napproaches and discuss their advantages and disadvantages. Furthermore, we show\nour results for a neighborhood in the city of Berlin and report a\nclassification accuracy of 91.6\\% on the original imbalanced data. Finally, we\ndiscuss further work; from gathering more data over a longer period of time to\nfitting spatial Gaussian densities to the data and the usage of apps for manual\nvalidation and annotation of parking spaces to improve ground truth data.",
          "link": "http://arxiv.org/abs/2102.06758",
          "publishedOn": "2021-07-29T02:00:11.075Z",
          "wordCount": 621,
          "title": "On automatic extraction of on-street parking spaces using park-out events data. (arXiv:2102.06758v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00773",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Visani_G/0/1/0/all/0/1\">Gian Marco Visani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1\">Alexandra Hope Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Nguyen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kent_D/0/1/0/all/0/1\">David M. Kent</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wong_J/0/1/0/all/0/1\">John B. Wong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cohen_J/0/1/0/all/0/1\">Joshua T. Cohen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hughes_M/0/1/0/all/0/1\">Michael C. Hughes</a>",
          "description": "We address the problem of modeling constrained hospital resources in the\nmidst of the COVID-19 pandemic in order to inform decision-makers of future\ndemand and assess the societal value of possible interventions. For broad\napplicability, we focus on the common yet challenging scenario where\npatient-level data for a region of interest are not available. Instead, given\ndaily admissions counts, we model aggregated counts of observed resource use,\nsuch as the number of patients in the general ward, in the intensive care unit,\nor on a ventilator. In order to explain how individual patient trajectories\nproduce these counts, we propose an aggregate count explicit-duration hidden\nMarkov model, nicknamed the ACED-HMM, with an interpretable, compact\nparameterization. We develop an Approximate Bayesian Computation approach that\ndraws samples from the posterior distribution over the model's transition and\nduration parameters given aggregate counts from a specific location, thus\nadapting the model to a region or individual hospital site of interest. Samples\nfrom this posterior can then be used to produce future forecasts of any counts\nof interest. Using data from the United States and the United Kingdom, we show\nour mechanistic approach provides competitive probabilistic forecasts for the\nfuture even as the dynamics of the pandemic shift. Furthermore, we show how our\nmodel provides insight about recovery probabilities or length of stay\ndistributions, and we suggest its potential to answer challenging what-if\nquestions about the societal value of possible interventions.",
          "link": "http://arxiv.org/abs/2105.00773",
          "publishedOn": "2021-07-29T02:00:11.068Z",
          "wordCount": 782,
          "title": "Approximate Bayesian Computation for an Explicit-Duration Hidden Markov Model of COVID-19 Hospital Trajectories. (arXiv:2105.00773v2 [stat.AP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00533",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hall_G/0/1/0/all/0/1\">Georgina Hall</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Massoulie_L/0/1/0/all/0/1\">Laurent Massouli&#xe9;</a>",
          "description": "In this paper, we consider the graph alignment problem, which is the problem\nof recovering, given two graphs, a one-to-one mapping between nodes that\nmaximizes edge overlap. This problem can be viewed as a noisy version of the\nwell-known graph isomorphism problem and appears in many applications,\nincluding social network deanonymization and cellular biology. Our focus here\nis on partial recovery, i.e., we look for a one-to-one mapping which is correct\non a fraction of the nodes of the graph rather than on all of them, and we\nassume that the two input graphs to the problem are correlated\nErd\\H{o}s-R\\'enyi graphs of parameters $(n,q,s)$. Our main contribution is then\nto give necessary and sufficient conditions on $(n,q,s)$ under which partial\nrecovery is possible with high probability as the number of nodes $n$ goes to\ninfinity. In particular, we show that it is possible to achieve partial\nrecovery in the $nqs=\\Theta(1)$ regime under certain additional assumptions. An\ninteresting byproduct of the analysis techniques we develop to obtain the\nsufficiency result in the partial recovery setting is a tighter analysis of the\nmaximum likelihood estimator for the graph alignment problem, which leads to\nimproved sufficient conditions for exact recovery.",
          "link": "http://arxiv.org/abs/2007.00533",
          "publishedOn": "2021-07-29T02:00:11.061Z",
          "wordCount": 677,
          "title": "Partial Recovery in the Graph Alignment Problem. (arXiv:2007.00533v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamford_C/0/1/0/all/0/1\">Chris Bamford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grela_L/0/1/0/all/0/1\">Lukasz Grela</a>",
          "description": "In recent years, researchers have achieved great success in applying Deep\nReinforcement Learning (DRL) algorithms to Real-time Strategy (RTS) games,\ncreating strong autonomous agents that could defeat professional players in\nStarCraft~II. However, existing approaches to tackle full games have high\ncomputational costs, usually requiring the use of thousands of GPUs and CPUs\nfor weeks. This paper has two main contributions to address this issue: 1) We\nintroduce Gym-$\\mu$RTS (pronounced \"gym-micro-RTS\") as a fast-to-run RL\nenvironment for full-game RTS research and 2) we present a collection of\ntechniques to scale DRL to play full-game $\\mu$RTS as well as ablation studies\nto demonstrate their empirical importance. Our best-trained bot can defeat\nevery $\\mu$RTS bot we tested from the past $\\mu$RTS competitions when working\nin a single-map setting, resulting in a state-of-the-art DRL agent while only\ntaking about 60 hours of training using a single machine (one GPU, three vCPU,\n16GB RAM). See the blog post at\nhttps://wandb.ai/vwxyzjn/gym-microrts-paper/reports/Gym-RTS-Toward-Affordable-Deep-Reinforcement-Learning-Research-in-Real-Time-Strategy-Games--Vmlldzo2MDIzMTg\nand the source code at https://github.com/vwxyzjn/gym-microrts-paper",
          "link": "http://arxiv.org/abs/2105.13807",
          "publishedOn": "2021-07-29T02:00:11.038Z",
          "wordCount": 666,
          "title": "Gym-$\\mu$RTS: Toward Affordable Full Game Real-time Strategy Games Research with Deep Reinforcement Learning. (arXiv:2105.13807v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.11988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fornasier_M/0/1/0/all/0/1\">Massimo Fornasier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pareschi_L/0/1/0/all/0/1\">Lorenzo Pareschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunnen_P/0/1/0/all/0/1\">Philippe S&#xfc;nnen</a>",
          "description": "We investigate the implementation of a new stochastic Kuramoto-Vicsek-type\nmodel for global optimization of nonconvex functions on the sphere. This model\nbelongs to the class of Consensus-Based Optimization. In fact, particles move\non the sphere driven by a drift towards an instantaneous consensus point, which\nis computed as a convex combination of particle locations, weighted by the cost\nfunction according to Laplace's principle, and it represents an approximation\nto a global minimizer. The dynamics is further perturbed by a random vector\nfield to favor exploration, whose variance is a function of the distance of the\nparticles to the consensus point. In particular, as soon as the consensus is\nreached the stochastic component vanishes. The main results of this paper are\nabout the proof of convergence of the numerical scheme to global minimizers\nprovided conditions of well-preparation of the initial datum. The proof\ncombines previous results of mean-field limit with a novel asymptotic analysis,\nand classical convergence results of numerical methods for SDE. We present\nseveral numerical experiments, which show that the algorithm proposed in the\npresent paper scales well with the dimension and is extremely versatile. To\nquantify the performances of the new approach, we show that the algorithm is\nable to perform essentially as good as ad hoc state of the art methods in\nchallenging problems in signal processing and machine learning, namely the\nphase retrieval problem and the robust subspace detection.",
          "link": "http://arxiv.org/abs/2001.11988",
          "publishedOn": "2021-07-29T02:00:11.020Z",
          "wordCount": 747,
          "title": "Consensus-Based Optimization on the Sphere: Convergence to Global Minimizers and Machine Learning. (arXiv:2001.11988v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guesmi_A/0/1/0/all/0/1\">Amira Guesmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alouani_I/0/1/0/all/0/1\">Ihsen Alouani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khasawneh_K/0/1/0/all/0/1\">Khaled Khasawneh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baklouti_M/0/1/0/all/0/1\">Mouna Baklouti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frikha_T/0/1/0/all/0/1\">Tarek Frikha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abid_M/0/1/0/all/0/1\">Mohamed Abid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abu_Ghazaleh_N/0/1/0/all/0/1\">Nael Abu-Ghazaleh</a>",
          "description": "In the past few years, an increasing number of machine-learning and deep\nlearning structures, such as Convolutional Neural Networks (CNNs), have been\napplied to solving a wide range of real-life problems. However, these\narchitectures are vulnerable to adversarial attacks. In this paper, we propose\nfor the first time to use hardware-supported approximate computing to improve\nthe robustness of machine learning classifiers. We show that our approximate\ncomputing implementation achieves robustness across a wide range of attack\nscenarios. Specifically, for black-box and grey-box attack scenarios, we show\nthat successful adversarial attacks against the exact classifier have poor\ntransferability to the approximate implementation. Surprisingly, the robustness\nadvantages also apply to white-box attacks where the attacker has access to the\ninternal implementation of the approximate classifier. We explain some of the\npossible reasons for this robustness through analysis of the internal operation\nof the approximate implementation. Furthermore, our approximate computing model\nmaintains the same level in terms of classification accuracy, does not require\nretraining, and reduces resource utilization and energy consumption of the CNN.\nWe conducted extensive experiments on a set of strong adversarial attacks; We\nempirically show that the proposed implementation increases the robustness of a\nLeNet-5 and an Alexnet CNNs by up to 99% and 87%, respectively for strong\ngrey-box adversarial attacks along with up to 67% saving in energy consumption\ndue to the simpler nature of the approximate logic. We also show that a\nwhite-box attack requires a remarkably higher noise budget to fool the\napproximate classifier, causing an average of 4db degradation of the PSNR of\nthe input image relative to the images that succeed in fooling the exact\nclassifier",
          "link": "http://arxiv.org/abs/2006.07700",
          "publishedOn": "2021-07-29T02:00:11.003Z",
          "wordCount": 761,
          "title": "Defensive Approximation: Enhancing CNNs Security through Approximate Computing. (arXiv:2006.07700v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>",
          "description": "Near out-of-distribution detection (OOD) is a major challenge for deep neural\nnetworks. We demonstrate that large-scale pre-trained transformers can\nsignificantly improve the state-of-the-art (SOTA) on a range of near OOD tasks\nacross different data modalities. For instance, on CIFAR-100 vs CIFAR-10 OOD\ndetection, we improve the AUROC from 85% (current SOTA) to more than 96% using\nVision Transformers pre-trained on ImageNet-21k. On a challenging genomics OOD\ndetection benchmark, we improve the AUROC from 66% to 77% using transformers\nand unsupervised pre-training. To further improve performance, we explore the\nfew-shot outlier exposure setting where a few examples from outlier classes may\nbe available; we show that pre-trained transformers are particularly\nwell-suited for outlier exposure, and that the AUROC of OOD detection on\nCIFAR-100 vs CIFAR-10 can be improved to 98.7% with just 1 image per OOD class,\nand 99.46% with 10 images per OOD class. For multi-modal image-text pre-trained\ntransformers such as CLIP, we explore a new way of using just the names of\noutlier classes as a sole source of information without any accompanying\nimages, and show that this outperforms previous SOTA on standard vision OOD\nbenchmark tasks.",
          "link": "http://arxiv.org/abs/2106.03004",
          "publishedOn": "2021-07-29T02:00:10.994Z",
          "wordCount": 646,
          "title": "Exploring the Limits of Out-of-Distribution Detection. (arXiv:2106.03004v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-29T02:00:10.967Z",
          "wordCount": 700,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Minping Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Q/0/1/0/all/0/1\">Qiuhua Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yudong Cao</a>",
          "description": "The scope of data-driven fault diagnosis models is greatly improved through\ndeep learning (DL). However, the classical convolution and recurrent structure\nhave their defects in computational efficiency and feature representation,\nwhile the latest Transformer architecture based on attention mechanism has not\nbeen applied in this field. To solve these problems, we propose a novel\ntime-frequency Transformer (TFT) model inspired by the massive success of\nstandard Transformer in sequence processing. Specially, we design a fresh\ntokenizer and encoder module to extract effective abstractions from the\ntime-frequency representation (TFR) of vibration signals. On this basis, a new\nend-to-end fault diagnosis framework based on time-frequency Transformer is\npresented in this paper. Through the case studies on bearing experimental\ndatasets, we constructed the optimal Transformer structure and verified the\nperformance of the diagnostic method. The superiority of the proposed method is\ndemonstrated in comparison with the benchmark model and other state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2104.09079",
          "publishedOn": "2021-07-29T02:00:10.915Z",
          "wordCount": 623,
          "title": "A novel Time-frequency Transformer and its Application in Fault Diagnosis of Rolling Bearings. (arXiv:2104.09079v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02604",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Sabando_M/0/1/0/all/0/1\">Mar&#xed;a Virginia Sabando</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ponzoni_I/0/1/0/all/0/1\">Ignacio Ponzoni</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Milios_E/0/1/0/all/0/1\">Evangelos E. Milios</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Soto_A/0/1/0/all/0/1\">Axel J. Soto</a>",
          "description": "With the consolidation of deep learning in drug discovery, several novel\nalgorithms for learning molecular representations have been proposed. Despite\nthe interest of the community in developing new methods for learning molecular\nembeddings and their theoretical benefits, comparing molecular embeddings with\neach other and with traditional representations is not straightforward, which\nin turn hinders the process of choosing a suitable representation for QSAR\nmodeling. A reason behind this issue is the difficulty of conducting a fair and\nthorough comparison of the different existing embedding approaches, which\nrequires numerous experiments on various datasets and training scenarios. To\nclose this gap, we reviewed the literature on methods for molecular embeddings\nand reproduced three unsupervised and two supervised molecular embedding\ntechniques recently proposed in the literature. We compared these five methods\nconcerning their performance in QSAR scenarios using different classification\nand regression datasets. We also compared these representations to traditional\nmolecular representations, namely molecular descriptors and fingerprints. As\nopposed to the expected outcome, our experimental setup consisting of over\n25,000 trained models and statistical tests revealed that the predictive\nperformance using molecular embeddings did not significantly surpass that of\ntraditional representations. While supervised embeddings yielded competitive\nresults compared to those using traditional molecular representations,\nunsupervised embeddings tended to perform worse than traditional\nrepresentations. Our results highlight the need for conducting a careful\ncomparison and analysis of the different embedding techniques prior to using\nthem in drug design tasks, and motivate a discussion about the potential of\nmolecular embeddings in computer-aided drug design.",
          "link": "http://arxiv.org/abs/2104.02604",
          "publishedOn": "2021-07-29T02:00:10.908Z",
          "wordCount": 714,
          "title": "Using Molecular Embeddings in QSAR Modeling: Does it Make a Difference?. (arXiv:2104.02604v2 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_A/0/1/0/all/0/1\">Ananth Mahadevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathioudakis_M/0/1/0/all/0/1\">Michael Mathioudakis</a>",
          "description": "Machine unlearning is the task of updating machine learning (ML) models after\na subset of the training data they were trained on is deleted. Methods for the\ntask are desired to combine effectiveness and efficiency, i.e., they should\neffectively \"unlearn\" deleted data, but in a way that does not require\nexcessive computation effort (e.g., a full retraining) for a small amount of\ndeletions. Such a combination is typically achieved by tolerating some amount\nof approximation in the unlearning. In addition, laws and regulations in the\nspirit of \"the right to be forgotten\" have given rise to requirements for\ncertifiability, i.e., the ability to demonstrate that the deleted data has\nindeed been unlearned by the ML model.\n\nIn this paper, we present an experimental study of the three state-of-the-art\napproximate unlearning methods for linear models and demonstrate the trade-offs\nbetween efficiency, effectiveness and certifiability offered by each method. In\nimplementing the study, we extend some of the existing works and describe a\ncommon ML pipeline to compare and evaluate the unlearning methods on six\nreal-world datasets and a variety of settings. We provide insights into the\neffect of the quantity and distribution of the deleted data on ML models and\nthe performance of each unlearning method in different settings. We also\npropose a practical online strategy to determine when the accumulated error\nfrom approximate unlearning is large enough to warrant a full retrain of the ML\nmodel.",
          "link": "http://arxiv.org/abs/2106.15093",
          "publishedOn": "2021-07-29T02:00:10.886Z",
          "wordCount": 683,
          "title": "Certifiable Machine Unlearning for Linear Models. (arXiv:2106.15093v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14331",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Das_A/0/1/0/all/0/1\">Abhranil Das</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Geisler_W/0/1/0/all/0/1\">Wilson S Geisler</a>",
          "description": "Univariate and multivariate normal probability distributions are widely used\nwhen modeling decisions under uncertainty. Computing the performance of such\nmodels requires integrating these distributions over specific domains, which\ncan vary widely across models. Besides some special cases where these integrals\nare easy to calculate, there exist no general analytical expressions, standard\nnumerical methods or software for these integrals. Here we present mathematical\nresults and open-source software that provide (i) the probability in any domain\nof a normal in any dimensions with any parameters, (ii) the probability\ndensity, cumulative distribution, and inverse cumulative distribution of any\nfunction of a normal vector, (iii) the classification errors among any number\nof normal distributions, the Bayes-optimal discriminability index and relation\nto the operating characteristic, (iv) dimension reduction and visualizations\nfor such problems, and (v) tests for how reliably these methods may be used on\ngiven data. We demonstrate these tools with vision research applications of\ndetecting occluding objects in natural scenes, and detecting camouflage.",
          "link": "http://arxiv.org/abs/2012.14331",
          "publishedOn": "2021-07-29T02:00:10.854Z",
          "wordCount": 681,
          "title": "A method to integrate and classify normal distributions. (arXiv:2012.14331v7 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhengbo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zitang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weilian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamata_S/0/1/0/all/0/1\">Sei-ichiro Kamata</a>",
          "description": "Various deep neural network architectures (DNNs) maintain massive vital\nrecords in computer vision. While drawing attention worldwide, the design of\nthe overall structure lacks general guidance. Based on the relationship between\nDNN design and numerical differential equations, we performed a fair comparison\nof the residual design with higher-order perspectives. We show that the widely\nused DNN design strategy, constantly stacking a small design (usually 2-3\nlayers), could be easily improved, supported by solid theoretical knowledge and\nwith no extra parameters needed. We reorganise the residual design in\nhigher-order ways, which is inspired by the observation that many effective\nnetworks can be interpreted as different numerical discretisations of\ndifferential equations. The design of ResNet follows a relatively simple\nscheme, which is Euler forward; however, the situation becomes complicated\nrapidly while stacking. We suppose that stacked ResNet is somehow equalled to a\nhigher-order scheme; then, the current method of forwarding propagation might\nbe relatively weak compared with a typical high-order method such as\nRunge-Kutta. We propose HO-ResNet to verify the hypothesis of widely used CV\nbenchmarks with sufficient experiments. Stable and noticeable increases in\nperformance are observed, and convergence and robustness are also improved. Our\nstacking strategy improved ResNet-30 by 2.15 per cent and ResNet-58 by 2.35 per\ncent on CIFAR-10, with the same settings and parameters. The proposed strategy\nis fundamental and theoretical and can therefore be applied to any network as a\ngeneral guideline.",
          "link": "http://arxiv.org/abs/2103.15244",
          "publishedOn": "2021-07-29T02:00:10.846Z",
          "wordCount": 727,
          "title": "Rethinking ResNets: Improved Stacking Strategies With High Order Schemes. (arXiv:2103.15244v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarussi_R/0/1/0/all/0/1\">Roei Sarussi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brutzkus_A/0/1/0/all/0/1\">Alon Brutzkus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1\">Amir Globerson</a>",
          "description": "Can a neural network minimizing cross-entropy learn linearly separable data?\nDespite progress in the theory of deep learning, this question remains\nunsolved. Here we prove that SGD globally optimizes this learning problem for a\ntwo-layer network with Leaky ReLU activations. The learned network can in\nprinciple be very complex. However, empirical evidence suggests that it often\nturns out to be approximately linear. We provide theoretical support for this\nphenomenon by proving that if network weights converge to two weight clusters,\nthis will imply an approximately linear decision boundary. Finally, we show a\ncondition on the optimization that leads to weight clustering. We provide\nempirical results that validate our theoretical analysis.",
          "link": "http://arxiv.org/abs/2101.02533",
          "publishedOn": "2021-07-29T02:00:10.759Z",
          "wordCount": 567,
          "title": "Towards Understanding Learning in Neural Networks with Linear Teachers. (arXiv:2101.02533v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Minto_L/0/1/0/all/0/1\">Lorenzo Minto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haller_M/0/1/0/all/0/1\">Moritz Haller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddadi_H/0/1/0/all/0/1\">Hamed Haddadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livshits_B/0/1/0/all/0/1\">Benjamin Livshits</a>",
          "description": "Recommender systems are commonly trained on centrally collected user\ninteraction data like views or clicks. This practice however raises serious\nprivacy concerns regarding the recommender's collection and handling of\npotentially sensitive data. Several privacy-aware recommender systems have been\nproposed in recent literature, but comparatively little attention has been\ngiven to systems at the intersection of implicit feedback and privacy. To\naddress this shortcoming, we propose a practical federated recommender system\nfor implicit data under user-level local differential privacy (LDP). The\nprivacy-utility trade-off is controlled by parameters $\\epsilon$ and $k$,\nregulating the per-update privacy budget and the number of $\\epsilon$-LDP\ngradient updates sent by each user respectively. To further protect the user's\nprivacy, we introduce a proxy network to reduce the fingerprinting surface by\nanonymizing and shuffling the reports before forwarding them to the\nrecommender. We empirically demonstrate the effectiveness of our framework on\nthe MovieLens dataset, achieving up to Hit Ratio with K=10 (HR@10) 0.68 on 50k\nusers with 5k items. Even on the full dataset, we show that it is possible to\nachieve reasonable utility with HR@10>0.5 without compromising user privacy.",
          "link": "http://arxiv.org/abs/2105.03941",
          "publishedOn": "2021-07-29T02:00:10.747Z",
          "wordCount": 666,
          "title": "Stronger Privacy for Federated Collaborative Filtering with Implicit Feedback. (arXiv:2105.03941v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01168",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_G/0/1/0/all/0/1\">Guannan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yujie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Low_S/0/1/0/all/0/1\">Steven Low</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Na Li</a>",
          "description": "With large-scale integration of renewable generation and distributed energy\nresources (DERs), modern power systems are confronted with new operational\nchallenges, such as growing complexity, increasing uncertainty, and aggravating\nvolatility. Meanwhile, more and more data are becoming available owing to the\nwidespread deployment of smart meters, smart sensors, and upgraded\ncommunication networks. As a result, data-driven control techniques, especially\nreinforcement learning (RL), have attracted surging attention in recent years.\nIn this paper, we provide a tutorial on various RL techniques and how they can\nbe applied to decision-making in power systems. We illustrate RL-based models\nand solutions in three key applications, frequency regulation, voltage control,\nand energy management. We conclude with three critical issues in the\napplication of RL, i.e., safety, scalability, and data. Several potential\nfuture directions are discussed as well.",
          "link": "http://arxiv.org/abs/2102.01168",
          "publishedOn": "2021-07-29T02:00:10.704Z",
          "wordCount": 632,
          "title": "Reinforcement Learning for Decision-Making and Control in Power Systems: Tutorial, Review, and Vision. (arXiv:2102.01168v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramabathiran_A/0/1/0/all/0/1\">Amuthan A. Ramabathiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_P/0/1/0/all/0/1\">Prabhu Ramachandran</a>",
          "description": "We introduce a class of Sparse, Physics-based, and partially Interpretable\nNeural Networks (SPINN) for solving ordinary and partial differential equations\n(PDEs). By reinterpreting a traditional meshless representation of solutions of\nPDEs we develop a class of sparse neural network architectures that are\npartially interpretable. The SPINN model we propose here serves as a seamless\nbridge between two extreme modeling tools for PDEs, namely dense neural network\nbased methods like Physics Informed Neural Networks (PINNs) and traditional\nmesh-free numerical methods, thereby providing a novel means to develop a new\nclass of hybrid algorithms that build on the best of both these viewpoints. A\nunique feature of the SPINN model that distinguishes it from other neural\nnetwork based approximations proposed earlier is that it is (i) interpretable,\nin a particular sense made precise in the work, and (ii) sparse in the sense\nthat it has much fewer connections than typical dense neural networks used for\nPDEs. Further, the SPINN algorithm implicitly encodes mesh adaptivity and is\nable to handle discontinuities in the solutions. In addition, we demonstrate\nthat Fourier series representations can also be expressed as a special class of\nSPINN and propose generalized neural network analogues of Fourier\nrepresentations. We illustrate the utility of the proposed method with a\nvariety of examples involving ordinary differential equations, elliptic,\nparabolic, hyperbolic and nonlinear partial differential equations, and an\nexample in fluid dynamics.",
          "link": "http://arxiv.org/abs/2102.13037",
          "publishedOn": "2021-07-29T02:00:10.684Z",
          "wordCount": 718,
          "title": "SPINN: Sparse, Physics-based, and partially Interpretable Neural Networks for PDEs. (arXiv:2102.13037v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05071",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yunsong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stearrett_R/0/1/0/all/0/1\">Ryan Stearrett</a>",
          "description": "A cross-benchmark has been done on three critical aspects, data imputing,\nfeature selection and regression algorithms, for machine learning based\nchemical vapor deposition (CVD) virtual metrology (VM). The result reveals that\nlinear feature selection regression algorithm would extensively under-fit the\nVM data. Data imputing is also necessary to achieve a higher prediction\naccuracy as the data availability is only ~70% when optimal accuracy is\nobtained. This work suggests a nonlinear feature selection and regression\nalgorithm combined with nearest data imputing algorithm would provide a\nprediction accuracy as high as 0.7. This would lead to 70% reduced CVD\nprocessing variation, which is believed to will lead to reduced frequency of\nphysical metrology as well as more reliable mass-produced wafer with improved\nquality.",
          "link": "http://arxiv.org/abs/2107.05071",
          "publishedOn": "2021-07-29T02:00:10.616Z",
          "wordCount": 574,
          "title": "Machine Learning based CVD Virtual Metrology in Mass Produced Semiconductor Process. (arXiv:2107.05071v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1\">Charaf Eddine Benarab</a>",
          "description": "Knowledge is acquired by humans through experience, and no boundary is set\nbetween the kinds of knowledge or skill levels we can achieve on different\ntasks at the same time. When it comes to Neural Networks, that is not the case,\nthe major breakthroughs in the field are extremely task and domain specific.\nVision and language are dealt with in separate manners, using separate methods\nand different datasets. In this work, we propose to use knowledge acquired by\nbenchmark Vision Models which are trained on ImageNet to help a much smaller\narchitecture learn to classify text. After transforming the textual data\ncontained in the IMDB dataset to gray scale images. An analysis of different\ndomains and the Transfer Learning method is carried out. Despite the challenge\nposed by the very different datasets, promising results are achieved. The main\ncontribution of this work is a novel approach which links large pretrained\nmodels on both language and vision to achieve state-of-the-art results in\ndifferent sub-fields from the original task. Without needing high compute\ncapacity resources. Specifically, Sentiment Analysis is achieved after\ntransferring knowledge between vision and language models. BERT embeddings are\ntransformed into grayscale images, these images are then used as training\nexamples for pre-trained vision models such as VGG16 and ResNet\n\nIndex Terms: BERT, Convolutional Neural Networks, Domain Adaptation, image\nclassification, Natural Language Processing, t-SNE, text classification,\nTransfer Learning",
          "link": "http://arxiv.org/abs/2106.12479",
          "publishedOn": "2021-07-29T02:00:10.346Z",
          "wordCount": 706,
          "title": "Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Waller_I/0/1/0/all/0/1\">Isaac Waller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1\">Ashton Anderson</a>",
          "description": "Optimism about the Internet's potential to bring the world together has been\ntempered by concerns about its role in inflaming the 'culture wars'. Via mass\nselection into like-minded groups, online society may be becoming more\nfragmented and polarized, particularly with respect to partisan differences.\nHowever, our ability to measure the social makeup of online communities, and in\nturn understand the social organization of online platforms, is limited by the\npseudonymous, unstructured, and large-scale nature of digital discussion. We\ndevelop a neural embedding methodology to quantify the positioning of online\ncommunities along social dimensions by leveraging large-scale patterns of\naggregate behaviour. Applying our methodology to 5.1B Reddit comments made in\n10K communities over 14 years, we measure how the macroscale community\nstructure is organized with respect to age, gender, and U.S. political\npartisanship. Examining political content, we find Reddit underwent a\nsignificant polarization event around the 2016 U.S. presidential election, and\nremained highly polarized for years afterward. Contrary to conventional wisdom,\nhowever, individual-level polarization is rare; the system-level shift in 2016\nwas disproportionately driven by the arrival of new and newly political users.\nPolitical polarization on Reddit is unrelated to previous activity on the\nplatform, and is instead temporally aligned with external events. We also\nobserve a stark ideological asymmetry, with the sharp increase in 2016 being\nentirely attributable to changes in right-wing activity. Our methodology is\nbroadly applicable to the study of online interaction, and our findings have\nimplications for the design of online platforms, understanding the social\ncontexts of online behaviour, and quantifying the dynamics and mechanisms of\nonline polarization.",
          "link": "http://arxiv.org/abs/2010.00590",
          "publishedOn": "2021-07-29T02:00:10.318Z",
          "wordCount": 746,
          "title": "Quantifying social organization and political polarization in online platforms. (arXiv:2010.00590v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smedt_J/0/1/0/all/0/1\">Johannes De Smedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeshchenko_A/0/1/0/all/0/1\">Anton Yeshchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyvyanyy_A/0/1/0/all/0/1\">Artem Polyvyanyy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weerdt_J/0/1/0/all/0/1\">Jochen De Weerdt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendling_J/0/1/0/all/0/1\">Jan Mendling</a>",
          "description": "Process analytics is an umbrella of data-driven techniques which includes\nmaking predictions for individual process instances or overall process models.\nAt the instance level, various novel techniques have been recently devised,\ntackling next activity, remaining time, and outcome prediction. At the model\nlevel, there is a notable void. It is the ambition of this paper to fill this\ngap. To this end, we develop a technique to forecast the entire process model\nfrom historical event data. A forecasted model is a will-be process model\nrepresenting a probable future state of the overall process. Such a forecast\nhelps to investigate the consequences of drift and emerging bottlenecks. Our\ntechnique builds on a representation of event data as multiple time series,\neach capturing the evolution of a behavioural aspect of the process model, such\nthat corresponding forecasting techniques can be applied. Our implementation\ndemonstrates the accuracy of our technique on real-world event log data.",
          "link": "http://arxiv.org/abs/2105.01092",
          "publishedOn": "2021-07-29T02:00:10.311Z",
          "wordCount": 634,
          "title": "Process Model Forecasting Using Time Series Analysis of Event Sequence Data. (arXiv:2105.01092v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>",
          "description": "This paper argues that training GANs on local and non-local dependencies in\nspeech data offers insights into how deep neural networks discretize continuous\ndata and how symbolic-like rule-based morphophonological processes emerge in a\ndeep convolutional architecture. Acquisition of speech has recently been\nmodeled as a dependency between latent space and data generated by GANs in\nBegu\\v{s} (2020b; arXiv:2006.03965), who models learning of a simple local\nallophonic distribution. We extend this approach to test learning of local and\nnon-local phonological processes that include approximations of morphological\nprocesses. We further parallel outputs of the model to results of a behavioral\nexperiment where human subjects are trained on the data used for training the\nGAN network. Four main conclusions emerge: (i) the networks provide useful\ninformation for computational models of speech acquisition even if trained on a\ncomparatively small dataset of an artificial grammar learning experiment; (ii)\nlocal processes are easier to learn than non-local processes, which matches\nboth behavioral data in human subjects and typology in the world's languages.\nThis paper also proposes (iii) how we can actively observe the network's\nprogress in learning and explore the effect of training steps on learning\nrepresentations by keeping latent space constant across different training\nsteps. Finally, this paper shows that (iv) the network learns to encode the\npresence of a prefix with a single latent variable; by interpolating this\nvariable, we can actively observe the operation of a non-local phonological\nprocess. The proposed technique for retrieving learning representations has\ngeneral implications for our understanding of how GANs discretize continuous\nspeech data and suggests that rule-like generalizations in the training data\nare represented as an interaction between variables in the network's latent\nspace.",
          "link": "http://arxiv.org/abs/2009.12711",
          "publishedOn": "2021-07-29T02:00:10.300Z",
          "wordCount": 761,
          "title": "Local and non-local dependency learning and emergence of rule-like representations in speech data by Deep Convolutional Generative Adversarial Networks. (arXiv:2009.12711v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07757",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Musso_D/0/1/0/all/0/1\">Daniele Musso</a>",
          "description": "Local entropic loss functions provide a versatile framework to define\narchitecture-aware regularization procedures. Besides the possibility of being\nanisotropic in the synaptic space, the local entropic smoothening of the loss\nfunction can vary during training, thus yielding a tunable model complexity. A\nscoping protocol where the regularization is strong in the early-stage of the\ntraining and then fades progressively away constitutes an alternative to\nstandard initialization procedures for deep convolutional neural networks,\nnonetheless, it has wider applicability. We analyze anisotropic, local entropic\nsmoothenings in the language of statistical physics and information theory,\nproviding insight into both their interpretation and workings. We comment some\naspects related to the physics of renormalization and the spacetime structure\nof convolutional networks.",
          "link": "http://arxiv.org/abs/2107.07757",
          "publishedOn": "2021-07-29T02:00:10.291Z",
          "wordCount": 585,
          "title": "Entropic alternatives to initialization. (arXiv:2107.07757v2 [cond-mat.dis-nn] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.00826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gautam_A/0/1/0/all/0/1\">Akshat Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sit_M/0/1/0/all/0/1\">Muhammed Sit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1\">Ibrahim Demir</a>",
          "description": "In this paper, we demonstrated a practical application of realistic river\nimage generation using deep learning. Specifically, we explored a generative\nadversarial network (GAN) model capable of generating high-resolution and\nrealistic river images that can be used to support modeling and analysis in\nsurface water estimation, river meandering, wetland loss, and other\nhydrological research studies. First, we have created an extensive repository\nof overhead river images to be used in training. Second, we incorporated the\nProgressive Growing GAN (PGGAN), a network architecture that iteratively trains\nsmaller-resolution GANs to gradually build up to a very high resolution to\ngenerate high quality (i.e., 1024x1024) synthetic river imagery. With simpler\nGAN architectures, difficulties arose in terms of exponential increase of\ntraining time and vanishing/exploding gradient issues, which the PGGAN\nimplementation seemed to significantly reduce. The results presented in this\nstudy show great promise in generating high-quality images and capturing the\ndetails of river structure and flow to support hydrological research, which\noften requires extensive imagery for model performance.",
          "link": "http://arxiv.org/abs/2003.00826",
          "publishedOn": "2021-07-29T02:00:10.284Z",
          "wordCount": 644,
          "title": "Realistic River Image Synthesis using Deep Generative Adversarial Networks. (arXiv:2003.00826v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Clarice Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kathryn Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_A/0/1/0/all/0/1\">Andrew Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_R/0/1/0/all/0/1\">Rashidul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keya_K/0/1/0/all/0/1\">Kamrun Naher Keya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foulds_J/0/1/0/all/0/1\">James Foulds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shimei Pan</a>",
          "description": "Currently, there is a surge of interest in fair Artificial Intelligence (AI)\nand Machine Learning (ML) research which aims to mitigate discriminatory bias\nin AI algorithms, e.g. along lines of gender, age, and race. While most\nresearch in this domain focuses on developing fair AI algorithms, in this work,\nwe show that a fair AI algorithm on its own may be insufficient to achieve its\nintended results in the real world. Using career recommendation as a case\nstudy, we build a fair AI career recommender by employing gender debiasing\nmachine learning techniques. Our offline evaluation showed that the debiased\nrecommender makes fairer career recommendations without sacrificing its\naccuracy. Nevertheless, an online user study of more than 200 college students\nrevealed that participants on average prefer the original biased system over\nthe debiased system. Specifically, we found that perceived gender disparity is\na determining factor for the acceptance of a recommendation. In other words,\nour results demonstrate we cannot fully address the gender bias issue in AI\nrecommendations without addressing the gender bias in humans.",
          "link": "http://arxiv.org/abs/2106.07112",
          "publishedOn": "2021-07-29T02:00:10.232Z",
          "wordCount": 649,
          "title": "User Acceptance of Gender Stereotypes in Automated Career Recommendations. (arXiv:2106.07112v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09747",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Basak_S/0/1/0/all/0/1\">Subhasish Basak</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Petit_S/0/1/0/all/0/1\">S&#xe9;bastien Petit</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bect_J/0/1/0/all/0/1\">Julien Bect</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vazquez_E/0/1/0/all/0/1\">Emmanuel Vazquez</a>",
          "description": "This article investigates the origin of numerical issues in maximum\nlikelihood parameter estimation for Gaussian process (GP) interpolation and\ninvestigates simple but effective strategies for improving commonly used\nopen-source software implementations. This work targets a basic problem but a\nhost of studies, particularly in the literature of Bayesian optimization, rely\non off-the-shelf GP implementations. For the conclusions of these studies to be\nreliable and reproducible, robust GP implementations are critical.",
          "link": "http://arxiv.org/abs/2101.09747",
          "publishedOn": "2021-07-29T02:00:10.225Z",
          "wordCount": 530,
          "title": "Numerical issues in maximum likelihood parameter estimation for Gaussian process interpolation. (arXiv:2101.09747v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cesari_T/0/1/0/all/0/1\">Tommaso R. Cesari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vecchia_R/0/1/0/all/0/1\">Riccardo Della Vecchia</a>",
          "description": "In this preliminary (and unpolished) version of the paper, we study an\nasynchronous online learning setting with a network of agents. At each time\nstep, some of the agents are activated, requested to make a prediction, and pay\nthe corresponding loss. Some feedback is then revealed to these agents and is\nlater propagated through the network. We consider the case of full, bandit, and\nsemi-bandit feedback. In particular, we construct a reduction to delayed\nsingle-agent learning that applies to both the full and the bandit feedback\ncase and allows to obtain regret guarantees for both settings. We complement\nthese results with a near-matching lower bound.",
          "link": "http://arxiv.org/abs/2106.04982",
          "publishedOn": "2021-07-29T02:00:10.218Z",
          "wordCount": 556,
          "title": "Cooperative Online Learning. (arXiv:2106.04982v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Somanath_G/0/1/0/all/0/1\">Gowri Somanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1\">Daniel Kurz</a>",
          "description": "We present a method to estimate an HDR environment map from a narrow\nfield-of-view LDR camera image in real-time. This enables perceptually\nappealing reflections and shading on virtual objects of any material finish,\nfrom mirror to diffuse, rendered into a real physical environment using\naugmented reality. Our method is based on our efficient convolutional neural\nnetwork architecture, EnvMapNet, trained end-to-end with two novel losses,\nProjectionLoss for the generated image, and ClusterLoss for adversarial\ntraining. Through qualitative and quantitative comparison to state-of-the-art\nmethods, we demonstrate that our algorithm reduces the directional error of\nestimated light sources by more than 50%, and achieves 3.7 times lower Frechet\nInception Distance (FID). We further showcase a mobile application that is able\nto run our neural network model in under 9 ms on an iPhone XS, and render in\nreal-time, visually coherent virtual objects in previously unseen real-world\nenvironments.",
          "link": "http://arxiv.org/abs/2011.10687",
          "publishedOn": "2021-07-29T02:00:10.197Z",
          "wordCount": 658,
          "title": "HDR Environment Map Estimation for Real-Time Augmented Reality. (arXiv:2011.10687v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.12278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castera_C/0/1/0/all/0/1\">Camille Castera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolte_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Bolte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevotte_C/0/1/0/all/0/1\">C&#xe9;dric F&#xe9;votte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1\">Edouard Pauwels</a>",
          "description": "We introduce a new second-order inertial optimization method for machine\nlearning called INNA. It exploits the geometry of the loss function while only\nrequiring stochastic approximations of the function values and the generalized\ngradients. This makes INNA fully implementable and adapted to large-scale\noptimization problems such as the training of deep neural networks. The\nalgorithm combines both gradient-descent and Newton-like behaviors as well as\ninertia. We prove the convergence of INNA for most deep learning problems. To\ndo so, we provide a well-suited framework to analyze deep learning loss\nfunctions involving tame optimization in which we study a continuous dynamical\nsystem together with its discrete stochastic approximations. We prove sublinear\nconvergence for the continuous-time differential inclusion which underlies our\nalgorithm. Additionally, we also show how standard optimization mini-batch\nmethods applied to non-smooth non-convex problems can yield a certain type of\nspurious stationary points never discussed before. We address this issue by\nproviding a theoretical framework around the new idea of $D$-criticality; we\nthen give a simple asymptotic analysis of INNA. Our algorithm allows for using\nan aggressive learning rate of $o(1/\\log k)$. From an empirical viewpoint, we\nshow that INNA returns competitive results with respect to state of the art\n(stochastic gradient descent, ADAGRAD, ADAM) on popular deep learning benchmark\nproblems.",
          "link": "http://arxiv.org/abs/1905.12278",
          "publishedOn": "2021-07-29T02:00:10.190Z",
          "wordCount": 733,
          "title": "An Inertial Newton Algorithm for Deep Learning. (arXiv:1905.12278v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.01845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruijun Xu</a>",
          "description": "One of the most critical problems in weight-sharing neural architecture\nsearch is the evaluation of candidate models within a predefined search space.\nIn practice, a one-shot supernet is trained to serve as an evaluator. A\nfaithful ranking certainly leads to more accurate searching results. However,\ncurrent methods are prone to making misjudgments. In this paper, we prove that\ntheir biased evaluation is due to inherent unfairness in the supernet training.\nIn view of this, we propose two levels of constraints: expectation fairness and\nstrict fairness. Particularly, strict fairness ensures equal optimization\nopportunities for all choice blocks throughout the training, which neither\noverestimates nor underestimates their capacity. We demonstrate that this is\ncrucial for improving the confidence of models' ranking. Incorporating the\none-shot supernet trained under the proposed fairness constraints with a\nmulti-objective evolutionary search algorithm, we obtain various\nstate-of-the-art models, e.g., FairNAS-A attains 77.5% top-1 validation\naccuracy on ImageNet. The models and their evaluation codes are made publicly\navailable online this http URL .",
          "link": "http://arxiv.org/abs/1907.01845",
          "publishedOn": "2021-07-29T02:00:10.158Z",
          "wordCount": 672,
          "title": "FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search. (arXiv:1907.01845v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Habibpour_M/0/1/0/all/0/1\">Maryam Habibpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gharoun_H/0/1/0/all/0/1\">Hassan Gharoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdipour_M/0/1/0/all/0/1\">Mohammadreza Mehdipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajally_A/0/1/0/all/0/1\">AmirReza Tajally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgharnezhad_H/0/1/0/all/0/1\">Hamzeh Asgharnezhad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsi_A/0/1/0/all/0/1\">Afshar Shamsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafie_Khah_M/0/1/0/all/0/1\">Miadreza Shafie-Khah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catalao_J/0/1/0/all/0/1\">Joao P.S. Catalao</a>",
          "description": "Countless research works of deep neural networks (DNNs) in the task of credit\ncard fraud detection have focused on improving the accuracy of point\npredictions and mitigating unwanted biases by building different network\narchitectures or learning models. Quantifying uncertainty accompanied by point\nestimation is essential because it mitigates model unfairness and permits\npractitioners to develop trustworthy systems which abstain from suboptimal\ndecisions due to low confidence. Explicitly, assessing uncertainties associated\nwith DNNs predictions is critical in real-world card fraud detection settings\nfor characteristic reasons, including (a) fraudsters constantly change their\nstrategies, and accordingly, DNNs encounter observations that are not generated\nby the same process as the training distribution, (b) owing to the\ntime-consuming process, very few transactions are timely checked by\nprofessional experts to update DNNs. Therefore, this study proposes three\nuncertainty quantification (UQ) techniques named Monte Carlo dropout, ensemble,\nand ensemble Monte Carlo dropout for card fraud detection applied on\ntransaction data. Moreover, to evaluate the predictive uncertainty estimates,\nUQ confusion matrix and several performance metrics are utilized. Through\nexperimental results, we show that the ensemble is more effective in capturing\nuncertainty corresponding to generated predictions. Additionally, we\ndemonstrate that the proposed UQ methods provide extra insight to the point\npredictions, leading to elevate the fraud prevention process.",
          "link": "http://arxiv.org/abs/2107.13508",
          "publishedOn": "2021-07-29T02:00:10.077Z",
          "wordCount": 663,
          "title": "Uncertainty-Aware Credit Card Fraud Detection Using Deep Learning. (arXiv:2107.13508v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13522",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Hasyim_M/0/1/0/all/0/1\">Muhammad R. Hasyim</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Batton_C/0/1/0/all/0/1\">Clay H. Batton</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Mandadapu_K/0/1/0/all/0/1\">Kranthi K. Mandadapu</a>",
          "description": "A central object in the computational studies of rare events is the committor\nfunction. Though costly to compute, the committor function encodes complete\nmechanistic information of the processes involving rare events, including\nreaction rates and transition-state ensembles. Under the framework of\ntransition path theory (TPT), recent work [1] proposes an algorithm where a\nfeedback loop couples a neural network that models the committor function with\nimportance sampling, mainly umbrella sampling, which collects data needed for\nadaptive training. In this work, we show additional modifications are needed to\nimprove the accuracy of the algorithm. The first modification adds elements of\nsupervised learning, which allows the neural network to improve its prediction\nby fitting to sample-mean estimates of committor values obtained from short\nmolecular dynamics trajectories. The second modification replaces the\ncommittor-based umbrella sampling with the finite-temperature string (FTS)\nmethod, which enables homogeneous sampling in regions where transition pathways\nare located. We test our modifications on low-dimensional systems with\nnon-convex potential energy where reference solutions can be found via\nanalytical or the finite element methods, and show how combining supervised\nlearning and the FTS method yields accurate computation of committor functions\nand reaction rates. We also provide an error analysis for algorithms that use\nthe FTS method, using which reaction rates can be accurately estimated during\ntraining with a small number of samples.",
          "link": "http://arxiv.org/abs/2107.13522",
          "publishedOn": "2021-07-29T02:00:10.028Z",
          "wordCount": 682,
          "title": "Supervised Learning and the Finite-Temperature String Method for Computing Committor Functions and Reaction Rates. (arXiv:2107.13522v1 [cond-mat.stat-mech])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guangyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>",
          "description": "EEG-based emotion recognition often requires sufficient labeled training\nsamples to build an effective computational model. Labeling EEG data, on the\nother hand, is often expensive and time-consuming. To tackle this problem and\nreduce the need for output labels in the context of EEG-based emotion\nrecognition, we propose a semi-supervised pipeline to jointly exploit both\nunlabeled and labeled data for learning EEG representations. Our\nsemi-supervised framework consists of both unsupervised and supervised\ncomponents. The unsupervised part maximizes the consistency between original\nand reconstructed input data using an autoencoder, while simultaneously the\nsupervised part minimizes the cross-entropy between the input and output\nlabels. We evaluate our framework using both a stacked autoencoder and an\nattention-based recurrent autoencoder. We test our framework on the large-scale\nSEED EEG dataset and compare our results with several other popular\nsemi-supervised methods. Our semi-supervised framework with a deep\nattention-based recurrent autoencoder consistently outperforms the benchmark\nmethods, even when small sub-sets (3\\%, 5\\% and 10\\%) of the output labels are\navailable during training, achieving a new state-of-the-art semi-supervised\nperformance.",
          "link": "http://arxiv.org/abs/2107.13505",
          "publishedOn": "2021-07-29T02:00:10.020Z",
          "wordCount": 618,
          "title": "Deep Recurrent Semi-Supervised EEG Representation Learning for Emotion Recognition. (arXiv:2107.13505v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Helou_B/0/1/0/all/0/1\">Bassam Helou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusi_A/0/1/0/all/0/1\">Aditya Dusi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collin_A/0/1/0/all/0/1\">Anne Collin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdipour_N/0/1/0/all/0/1\">Noushin Mehdipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lizarazo_C/0/1/0/all/0/1\">Cristhian Lizarazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belta_C/0/1/0/all/0/1\">Calin Belta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wongpiromsarn_T/0/1/0/all/0/1\">Tichakorn Wongpiromsarn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tebbens_R/0/1/0/all/0/1\">Radboud Duintjer Tebbens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>",
          "description": "Autonomous vehicles must balance a complex set of objectives. There is no\nconsensus on how they should do so, nor on a model for specifying a desired\ndriving behavior. We created a dataset to help address some of these questions\nin a limited operating domain. The data consists of 92 traffic scenarios, with\nmultiple ways of traversing each scenario. Multiple annotators expressed their\npreference between pairs of scenario traversals. We used the data to compare an\ninstance of a rulebook, carefully hand-crafted independently of the dataset,\nwith several interpretable machine learning models such as Bayesian networks,\ndecision trees, and logistic regression trained on the dataset. To compare\ndriving behavior, these models use scores indicating by how much different\nscenario traversals violate each of 14 driving rules. The rules are\ninterpretable and designed by subject-matter experts. First, we found that\nthese rules were enough for these models to achieve a high classification\naccuracy on the dataset. Second, we found that the rulebook provides high\ninterpretability without excessively sacrificing performance. Third, the data\npointed to possible improvements in the rulebook and the rules, and to\npotential new rules. Fourth, we explored the interpretability vs performance\ntrade-off by also training non-interpretable models such as a random forest.\nFinally, we make the dataset publicly available to encourage a discussion from\nthe wider community on behavior specification for AVs. Please find it at\ngithub.com/bassam-motional/Reasonable-Crowd.",
          "link": "http://arxiv.org/abs/2107.13507",
          "publishedOn": "2021-07-29T02:00:09.974Z",
          "wordCount": 693,
          "title": "The Reasonable Crowd: Towards evidence-based and interpretable models of driving behavior. (arXiv:2107.13507v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1901.09997",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Berahas_A/0/1/0/all/0/1\">Albert S. Berahas</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jahani_M/0/1/0/all/0/1\">Majid Jahani</a>, <a href=\"http://arxiv.org/find/math/1/au:+Richtarik_P/0/1/0/all/0/1\">Peter Richt&#xe1;rik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Takac_M/0/1/0/all/0/1\">Martin Tak&#xe1;&#x10d;</a>",
          "description": "We present two sampled quasi-Newton methods (sampled LBFGS and sampled LSR1)\nfor solving empirical risk minimization problems that arise in machine\nlearning. Contrary to the classical variants of these methods that sequentially\nbuild Hessian or inverse Hessian approximations as the optimization progresses,\nour proposed methods sample points randomly around the current iterate at every\niteration to produce these approximations. As a result, the approximations\nconstructed make use of more reliable (recent and local) information, and do\nnot depend on past iterate information that could be significantly stale. Our\nproposed algorithms are efficient in terms of accessed data points (epochs) and\nhave enough concurrency to take advantage of parallel/distributed computing\nenvironments. We provide convergence guarantees for our proposed methods.\nNumerical tests on a toy classification problem as well as on popular\nbenchmarking binary classification and neural network training tasks reveal\nthat the methods outperform their classical variants.",
          "link": "http://arxiv.org/abs/1901.09997",
          "publishedOn": "2021-07-29T02:00:09.967Z",
          "wordCount": 641,
          "title": "Quasi-Newton Methods for Machine Learning: Forget the Past, Just Sample. (arXiv:1901.09997v5 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13545",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Charles Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orbik_J/0/1/0/all/0/1\">J&#x119;drzej Orbik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devin_C/0/1/0/all/0/1\">Coline Devin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Brian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berseth_G/0/1/0/all/0/1\">Glen Berseth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "In this paper, we study how robots can autonomously learn skills that require\na combination of navigation and grasping. Learning robotic skills in the real\nworld remains challenging without large-scale data collection and supervision.\nOur aim is to devise a robotic reinforcement learning system for learning\nnavigation and manipulation together, in an \\textit{autonomous} way without\nhuman intervention, enabling continual learning under realistic assumptions.\nSpecifically, our system, ReLMM, can learn continuously on a real-world\nplatform without any environment instrumentation, without human intervention,\nand without access to privileged information, such as maps, objects positions,\nor a global view of the environment. Our method employs a modularized policy\nwith components for manipulation and navigation, where uncertainty over the\nmanipulation success drives exploration for the navigation controller, and the\nmanipulation module provides rewards for navigation. We evaluate our method on\na room cleanup task, where the robot must navigate to and pick up items of\nscattered on the floor. After a grasp curriculum training phase, ReLMM can\nlearn navigation and grasping together fully automatically, in around 40 hours\nof real-world training.",
          "link": "http://arxiv.org/abs/2107.13545",
          "publishedOn": "2021-07-29T02:00:09.959Z",
          "wordCount": 626,
          "title": "ReLMM: Practical RL for Learning Mobile Manipulation Skills Using Only Onboard Sensors. (arXiv:2107.13545v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13473",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Valenchon_N/0/1/0/all/0/1\">Nicolas Valenchon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bouteiller_Y/0/1/0/all/0/1\">Yann Bouteiller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jourde_H/0/1/0/all/0/1\">Hugo R. Jourde</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coffey_E/0/1/0/all/0/1\">Emily B.J. Coffey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beltrame_G/0/1/0/all/0/1\">Giovanni Beltrame</a>",
          "description": "Electroencephalography (EEG) is a method of measuring the brain's electrical\nactivity, using non-invasive scalp electrodes. In this article, we propose the\nPortiloop, a deep learning-based portable and low-cost device enabling the\nneuroscience community to capture EEG, process it in real time, detect patterns\nof interest, and respond with precisely-timed stimulation. The core of the\nPortiloop is a System on Chip composed of an Analog to Digital Converter (ADC)\nand a Field-Programmable Gate Array (FPGA). After being converted to digital by\nthe ADC, the EEG signal is processed in the FPGA. The FPGA contains an ad-hoc\nArtificial Neural Network (ANN) with convolutional and recurrent units,\ndirectly implemented in hardware. The output of the ANN is then used to trigger\nthe user-defined feedback. We use the Portiloop to develop a real-time sleep\nspindle stimulating application, as a case study. Sleep spindles are a specific\ntype of transient oscillation ($\\sim$2.5 s, 12-16 Hz) that are observed in EEG\nrecordings, and are related to memory consolidation during sleep. We tested the\nPortiloop's capacity to detect and stimulate sleep spindles in real time using\nan existing database of EEG sleep recordings. With 71% for both precision and\nrecall as compared with expert labels, the system is able to stimulate spindles\nwithin $\\sim$300 ms of their onset, enabling experimental manipulation of early\nthe entire spindle. The Portiloop can be extended to detect and stimulate other\nneural events in EEG. It is fully available to the research community as an\nopen science project.",
          "link": "http://arxiv.org/abs/2107.13473",
          "publishedOn": "2021-07-29T02:00:09.840Z",
          "wordCount": 721,
          "title": "The Portiloop: a deep learning-based open science tool for closed-loop brain stimulation. (arXiv:2107.13473v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/1811.11891",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Koelle_S/0/1/0/all/0/1\">Samson Koelle</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1\">Hanyu Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1\">Marina Meila</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Chia Chen</a>",
          "description": "Manifold embedding algorithms map high-dimensional data down to coordinates\nin a much lower-dimensional space. One of the aims of dimension reduction is to\nfind intrinsic coordinates that describe the data manifold. The coordinates\nreturned by the embedding algorithm are abstract, and finding their physical or\ndomain-related meaning is not formalized and often left to domain experts. This\npaper studies the problem of recovering the meaning of the new low-dimensional\nrepresentation in an automatic, principled fashion. We propose a method to\nexplain embedding coordinates of a manifold as non-linear compositions of\nfunctions from a user-defined dictionary. We show that this problem can be set\nup as a sparse linear Group Lasso recovery problem, find sufficient recovery\nconditions, and demonstrate its effectiveness on data.",
          "link": "http://arxiv.org/abs/1811.11891",
          "publishedOn": "2021-07-29T02:00:09.832Z",
          "wordCount": 567,
          "title": "Manifold Coordinates with Physical Meaning. (arXiv:1811.11891v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.00570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tsung-Hui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Ying Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jong-Shi Pang</a>",
          "description": "The non-negative matrix factorization (NMF) model with an additional\northogonality constraint on one of the factor matrices, called the orthogonal\nNMF (ONMF), has been found a promising clustering model and can outperform the\nclassical K-means. However, solving the ONMF model is a challenging\noptimization problem because the coupling of the orthogonality and\nnon-negativity constraints introduces a mixed combinatorial aspect into the\nproblem due to the determination of the correct status of the variables\n(positive or zero). Most of the existing methods directly deal with the\northogonality constraint in its original form via various optimization\ntechniques, but are not scalable for large-scale problems. In this paper, we\npropose a new ONMF based clustering formulation that equivalently transforms\nthe orthogonality constraint into a set of norm-based non-convex equality\nconstraints. We then apply a non-convex penalty (NCP) approach to add them to\nthe objective as penalty terms, leading to a problem that is efficiently\nsolvable. One smooth penalty formulation and one non-smooth penalty formulation\nare respectively studied. We build theoretical conditions for the penalized\nproblems to provide feasible stationary solutions to the ONMF based clustering\nproblem, as well as proposing efficient algorithms for solving the penalized\nproblems of the two NCP methods. Experimental results based on both synthetic\nand real datasets are presented to show that the proposed NCP methods are\ncomputationally time efficient, and either match or outperform the existing\nK-means and ONMF based methods in terms of the clustering performance.",
          "link": "http://arxiv.org/abs/1906.00570",
          "publishedOn": "2021-07-29T02:00:09.825Z",
          "wordCount": 732,
          "title": "Clustering by Orthogonal NMF Model and Non-Convex Penalty Optimization. (arXiv:1906.00570v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polgreen_E/0/1/0/all/0/1\">Elizabeth Polgreen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reynolds_A/0/1/0/all/0/1\">Andrew Reynolds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seshia_S/0/1/0/all/0/1\">Sanjit A. Seshia</a>",
          "description": "In classic program synthesis algorithms, such as counterexample-guided\ninductive synthesis (CEGIS), the algorithms alternate between a synthesis phase\nand an oracle (verification) phase. Many synthesis algorithms use a white-box\noracle based on satisfiability modulo theory (SMT) solvers to provide\ncounterexamples. But what if a white-box oracle is either not available or not\neasy to work with? We present a framework for solving a general class of\noracle-guided synthesis problems which we term synthesis modulo oracles. In\nthis setting, oracles may be black boxes with a query-response interface\ndefined by the synthesis problem. As a necessary component of this framework,\nwe also formalize the problem of satisfiability modulo theories and oracles,\nand present an algorithm for solving this problem. We implement a prototype\nsolver for satisfiability and synthesis modulo oracles and demonstrate that, by\nusing oracles that execute functions not easily modeled in SMT-constraints,\nsuch as recursive functions or oracles that incorporate compilation and\nexecution of code, SMTO and SyMO are able to solve problems beyond the\nabilities of standard SMT and synthesis solvers.",
          "link": "http://arxiv.org/abs/2107.13477",
          "publishedOn": "2021-07-29T02:00:09.813Z",
          "wordCount": 613,
          "title": "Satisfiability and Synthesis Modulo Oracles. (arXiv:2107.13477v1 [cs.LO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kummer_L/0/1/0/all/0/1\">Lorenz Kummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidak_K/0/1/0/all/0/1\">Kevin Sidak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichmann_T/0/1/0/all/0/1\">Tabea Reichmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gansterer_W/0/1/0/all/0/1\">Wilfried Gansterer</a>",
          "description": "Quantization is a technique for reducing deep neural networks (DNNs) training\nand inference times, which is crucial for training in resource constrained\nenvironments or time critical inference applications. State-of-the-art (SOTA)\nquantization approaches focus on post-training quantization, i.e. quantization\nof pre-trained DNNs for speeding up inference. Very little work on quantized\ntraining exists, which neither al-lows dynamic intra-epoch precision switches\nnor em-ploys an information theory based switching heuristic. Usually, existing\napproaches require full precision refinement afterwards and enforce a global\nword length across the whole DNN. This leads to suboptimal quantization\nmappings and resource usage. Recognizing these limits, we introduce MARViN, a\nnew quantized training strategy using information theory-based intra-epoch\nprecision switching, which decides on a per-layer basis which precision should\nbe used in order to minimize quantization-induced information loss. Note that\nany quantization must leave enough precision such that future learning steps do\nnot suffer from vanishing gradients. We achieve an average speedup of 1.86\ncompared to a float32 basis while limiting mean accuracy degradation on\nAlexNet/ResNet to only -0.075%.",
          "link": "http://arxiv.org/abs/2107.13490",
          "publishedOn": "2021-07-29T02:00:09.804Z",
          "wordCount": 612,
          "title": "MARViN -- Multiple Arithmetic Resolutions Vacillating in Neural Networks. (arXiv:2107.13490v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogozhnikov_A/0/1/0/all/0/1\">Alex Rogozhnikov</a>",
          "description": "Without positional information, attention-based transformer neural networks\nare permutation-invariant. Absolute or relative positional embeddings are the\nmost popular ways to feed transformer models positional information. Absolute\npositional embeddings are simple to implement, but suffer from generalization\nissues when evaluating on sequences of different length than those seen at\ntraining time. Relative positions are more robust to length change, but are\nmore complex to implement and yield inferior model throughput. In this paper,\nwe propose an augmentation-based approach (CAPE) for absolute positional\nembeddings, which keeps the advantages of both absolute (simplicity and speed)\nand relative position embeddings (better generalization). In addition, our\nempirical evaluation on state-of-the-art models in machine translation, image\nand speech recognition demonstrates that CAPE leads to better generalization\nperformance as well as increased stability with respect to training\nhyper-parameters.",
          "link": "http://arxiv.org/abs/2106.03143",
          "publishedOn": "2021-07-29T02:00:09.699Z",
          "wordCount": 607,
          "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings. (arXiv:2106.03143v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Terry_J/0/1/0/all/0/1\">J. K. Terry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayakumar_M/0/1/0/all/0/1\">Mario Jayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alwis_K/0/1/0/all/0/1\">Kusal De Alwis</a>",
          "description": "The general approach taken when training deep learning classifiers is to save\nthe parameters after every few iterations, train until either a human observer\nor a simple metric-based heuristic decides the network isn't learning anymore,\nand then backtrack and pick the saved parameters with the best validation\naccuracy. Simple methods are used to determine if a neural network isn't\nlearning anymore because, as long as it's well after the optimal values are\nfound, the condition doesn't impact the final accuracy of the model. However\nfrom a runtime perspective, this is of great significance to the many cases\nwhere numerous neural networks are trained simultaneously (e.g. hyper-parameter\ntuning). Motivated by this, we introduce a statistical significance test to\ndetermine if a neural network has stopped learning. This stopping criterion\nappears to represent a happy medium compared to other popular stopping\ncriterions, achieving comparable accuracy to the criterions that achieve the\nhighest final accuracies in 77% or fewer epochs, while the criterions which\nstop sooner do so with an appreciable loss to final accuracy. Additionally, we\nuse this as the basis of a new learning rate scheduler, removing the need to\nmanually choose learning rate schedules and acting as a quasi-line search,\nachieving superior or comparable empirical performance to existing methods.",
          "link": "http://arxiv.org/abs/2103.01205",
          "publishedOn": "2021-07-29T02:00:09.681Z",
          "wordCount": 691,
          "title": "Statistically Significant Stopping of Neural Network Training. (arXiv:2103.01205v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1\">Spandan Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1\">Timothy Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dozier_J/0/1/0/all/0/1\">Jamell Dozier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_H/0/1/0/all/0/1\">Helen Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandari_N/0/1/0/all/0/1\">Nishchal Bhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1\">Fr&#xe9;do Durand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>",
          "description": "Object recognition and viewpoint estimation lie at the heart of visual\nunderstanding. Recent works suggest that convolutional neural networks (CNNs)\nfail to generalize to out-of-distribution (OOD) category-viewpoint\ncombinations, ie. combinations not seen during training. In this paper, we\ninvestigate when and how such OOD generalization may be possible by evaluating\nCNNs trained to classify both object category and 3D viewpoint on OOD\ncombinations, and identifying the neural mechanisms that facilitate such OOD\ngeneralization. We show that increasing the number of in-distribution\ncombinations (ie. data diversity) substantially improves generalization to OOD\ncombinations, even with the same amount of training data. We compare learning\ncategory and viewpoint in separate and shared network architectures, and\nobserve starkly different trends on in-distribution and OOD combinations, ie.\nwhile shared networks are helpful in-distribution, separate networks\nsignificantly outperform shared ones at OOD combinations. Finally, we\ndemonstrate that such OOD generalization is facilitated by the neural mechanism\nof specialization, ie. the emergence of two types of neurons -- neurons\nselective to category and invariant to viewpoint, and vice versa.",
          "link": "http://arxiv.org/abs/2007.08032",
          "publishedOn": "2021-07-29T02:00:09.674Z",
          "wordCount": 654,
          "title": "When and how do CNNs generalize to out-of-distribution category-viewpoint combinations?. (arXiv:2007.08032v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Wenxuan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haiping Huang</a>",
          "description": "The geometric structure of an optimization landscape is argued to be\nfundamentally important to support the success of deep neural network learning.\nA direct computation of the landscape beyond two layers is hard. Therefore, to\ncapture the global view of the landscape, an interpretable model of the\nnetwork-parameter (or weight) space must be established. However, the model is\nlacking so far. Furthermore, it remains unknown what the landscape looks like\nfor deep networks of binary synapses, which plays a key role in robust and\nenergy efficient neuromorphic computation. Here, we propose a statistical\nmechanics framework by directly building a least structured model of the\nhigh-dimensional weight space, considering realistic structured data,\nstochastic gradient descent training, and the computational depth of neural\nnetworks. We also consider whether the number of network parameters outnumbers\nthe number of supplied training data, namely, over- or under-parametrization.\nOur least structured model reveals that the weight spaces of the\nunder-parametrization and over-parameterization cases belong to the same class,\nin the sense that these weight spaces are well-connected without any\nhierarchical clustering structure. In contrast, the shallow-network has a\nbroken weight space, characterized by a discontinuous phase transition, thereby\nclarifying the benefit of depth in deep learning from the angle of high\ndimensional geometry. Our effective model also reveals that inside a deep\nnetwork, there exists a liquid-like central part of the architecture in the\nsense that the weights in this part behave as randomly as possible, providing\nalgorithmic implications. Our data-driven model thus provides a statistical\nmechanics insight about why deep learning is unreasonably effective in terms of\nthe high-dimensional weight space, and how deep networks are different from\nshallow ones.",
          "link": "http://arxiv.org/abs/2007.08093",
          "publishedOn": "2021-07-29T02:00:09.666Z",
          "wordCount": 754,
          "title": "Data-driven effective model shows a liquid-like deep learning. (arXiv:2007.08093v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1\">Maria Pia Del Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiller_D/0/1/0/all/0/1\">Dario Spiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1\">Pierre Philippe Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "In recent years, the growth of Machine Learning (ML) algorithms has raised\nthe number of studies including their applicability in a variety of different\nscenarios. Among all, one of the hardest ones is the aerospace, due to its\npeculiar physical requirements. In this context, a feasibility study and a\nfirst prototype for an Artificial Intelligence (AI) model to be deployed on\nboard satellites are presented in this work. As a case study, the detection of\nvolcanic eruptions has been investigated as a method to swiftly produce alerts\nand allow immediate interventions. Two Convolutional Neural Networks (CNNs)\nhave been proposed and designed, showing how to efficiently implement them for\nidentifying the eruptions and at the same time adapting their complexity in\norder to fit on board requirements.",
          "link": "http://arxiv.org/abs/2106.15281",
          "publishedOn": "2021-07-29T02:00:09.647Z",
          "wordCount": 602,
          "title": "On Board Volcanic Eruption Detection through CNNs and Satellite Multispectral Imagery. (arXiv:2106.15281v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1808.09670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fouillen_E/0/1/0/all/0/1\">Erwan Fouillen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyer_C/0/1/0/all/0/1\">Claire Boyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangnier_M/0/1/0/all/0/1\">Maxime Sangnier</a>",
          "description": "Gradient boosting is a prediction method that iteratively combines weak\nlearners to produce a complex and accurate model. From an optimization point of\nview, the learning procedure of gradient boosting mimics a gradient descent on\na functional variable. This paper proposes to build upon the proximal point\nalgorithm, when the empirical risk to minimize is not differentiable, in order\nto introduce a novel boosting approach, called proximal boosting. Besides being\nmotivated by non-differentiable optimization, the proposed algorithm benefits\nfrom algorithmic improvements such as controlling the approximation error and\nNesterov's acceleration, in the same way as gradient boosting [Grubb and\nBagnell, 2011, Biau et al., 2018]. This leads to two variants, respectively\ncalled residual proximal boosting and accelerated proximal boosting.\nTheoretical convergence is proved for the first two procedures under different\nhypotheses on the empirical risk and advantages of leveraging proximal methods\nfor boosting are illustrated by numerical experiments on simulated and\nreal-world data. In particular, we exhibit a favorable comparison over gradient\nboosting regarding convergence rate and prediction accuracy.",
          "link": "http://arxiv.org/abs/1808.09670",
          "publishedOn": "2021-07-29T02:00:09.640Z",
          "wordCount": 631,
          "title": "Proximal boosting and variants. (arXiv:1808.09670v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hammoudeh_A/0/1/0/all/0/1\">Ahmad Hammoudeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tedmori_S/0/1/0/all/0/1\">Sara Tedmori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obeid_N/0/1/0/all/0/1\">Nadim Obeid</a>",
          "description": "Although learning from data is effective and has achieved significant\nmilestones, it has many challenges and limitations. Learning from data starts\nfrom observations and then proceeds to broader generalizations. This framework\nis controversial in science, yet it has achieved remarkable engineering\nsuccesses. This paper reflects on some epistemological issues and some of the\nlimitations of the knowledge discovered in data. The document discusses the\ncommon perception that getting more data is the key to achieving better machine\nlearning models from theoretical and practical perspectives. The paper sheds\nsome light on the shortcomings of using generic mathematical theories to\ndescribe the process. It further highlights the need for theories specialized\nin learning from data. While more data leverages the performance of machine\nlearning models in general, the relation in practice is shown to be logarithmic\nat its best; After a specific limit, more data stabilize or degrade the machine\nlearning models. Recent work in reinforcement learning showed that the trend is\nshifting away from data-oriented approaches and relying more on algorithms. The\npaper concludes that learning from data is hindered by many limitations. Hence\nan approach that has an intensional orientation is needed.",
          "link": "http://arxiv.org/abs/2107.13270",
          "publishedOn": "2021-07-29T02:00:09.633Z",
          "wordCount": 626,
          "title": "A Reflection on Learning from Data: Epistemology Issues and Limitations. (arXiv:2107.13270v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1\">Sam Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_R/0/1/0/all/0/1\">Raluca Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1\">Ida Momennejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rzepecki_J/0/1/0/all/0/1\">Jaroslaw Rzepecki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuniga_E/0/1/0/all/0/1\">Evelyn Zuniga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costello_G/0/1/0/all/0/1\">Gavin Costello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leroy_G/0/1/0/all/0/1\">Guy Leroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_A/0/1/0/all/0/1\">Ali Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>",
          "description": "A key challenge on the path to developing agents that learn complex\nhuman-like behavior is the need to quickly and accurately quantify\nhuman-likeness. While human assessments of such behavior can be highly\naccurate, speed and scalability are limited. We address these limitations\nthrough a novel automated Navigation Turing Test (ANTT) that learns to predict\nhuman judgments of human-likeness. We demonstrate the effectiveness of our\nautomated NTT on a navigation task in a complex 3D environment. We investigate\nsix classification models to shed light on the types of architectures best\nsuited to this task, and validate them against data collected through a human\nNTT. Our best models achieve high accuracy when distinguishing true human and\nagent behavior. At the same time, we show that predicting finer-grained human\nassessment of agents' progress towards human-like behavior remains unsolved.\nOur work takes an important step towards agents that more effectively learn\ncomplex human-like behavior.",
          "link": "http://arxiv.org/abs/2105.09637",
          "publishedOn": "2021-07-29T02:00:09.626Z",
          "wordCount": 660,
          "title": "Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation. (arXiv:2105.09637v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Farwa K. Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flanagan_A/0/1/0/all/0/1\">Adrian Flanagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kuan E. Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alamgir_Z/0/1/0/all/0/1\">Zareen Alamgir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammad_Ud_Din_M/0/1/0/all/0/1\">Muhammad Ammad-Ud-Din</a>",
          "description": "We introduce the payload optimization method for federated recommender\nsystems (FRS). In federated learning (FL), the global model payload that is\nmoved between the server and users depends on the number of items to recommend.\nThe model payload grows when there is an increasing number of items. This\nbecomes challenging for an FRS if it is running in production mode. To tackle\nthe payload challenge, we formulated a multi-arm bandit solution that selected\npart of the global model and transmitted it to all users. The selection process\nwas guided by a novel reward function suitable for FL systems. So far as we are\naware, this is the first optimization method that seeks to address item\ndependent payloads. The method was evaluated using three benchmark\nrecommendation datasets. The empirical validation confirmed that the proposed\nmethod outperforms the simpler methods that do not benefit from the bandits for\nthe purpose of item selection. In addition, we have demonstrated the usefulness\nof our proposed method by rigorously evaluating the effects of a payload\nreduction on the recommendation performance degradation. Our method achieved up\nto a 90\\% reduction in model payload, yielding only a $\\sim$4\\% - 8\\% loss in\nthe recommendation performance for highly sparse datasets",
          "link": "http://arxiv.org/abs/2107.13078",
          "publishedOn": "2021-07-29T02:00:09.606Z",
          "wordCount": 674,
          "title": "A Payload Optimization Method for Federated Recommender Systems. (arXiv:2107.13078v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feeney_P/0/1/0/all/0/1\">Patrick Feeney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1\">Michael C. Hughes</a>",
          "description": "The pixelwise reconstruction error of deep autoencoders is often utilized for\nimage novelty detection and localization under the assumption that pixels with\nhigh error indicate which parts of the input image are unfamiliar and therefore\nlikely to be novel. This assumed correlation between pixels with high\nreconstruction error and novel regions of input images has not been verified\nand may limit the accuracy of these methods. In this paper we utilize saliency\nmaps to evaluate whether this correlation exists. Saliency maps reveal directly\nhow much a change in each input pixel would affect reconstruction loss, while\neach pixel's reconstruction error may be attributed to many input pixels when\nlayers are fully connected. We compare saliency maps to reconstruction error\nmaps via qualitative visualizations as well as quantitative correspondence\nbetween the top K elements of the maps for both novel and normal images. Our\nresults indicate that reconstruction error maps do not closely correlate with\nthe importance of pixels in the input images, making them insufficient for\nnovelty localization.",
          "link": "http://arxiv.org/abs/2107.13379",
          "publishedOn": "2021-07-29T02:00:09.599Z",
          "wordCount": 606,
          "title": "Evaluating the Use of Reconstruction Error for Novelty Localization. (arXiv:2107.13379v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yue Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Liangxiu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleerekoper_A/0/1/0/all/0/1\">Anthony Kleerekoper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Sheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tongle Hu</a>",
          "description": "Late blight disease is one of the most destructive diseases in potato crop,\nleading to serious yield losses globally. Accurate diagnosis of the disease at\nearly stage is critical for precision disease control and management. Current\nfarm practices in crop disease diagnosis are based on manual visual inspection,\nwhich is costly, time consuming, subject to individual bias. Recent advances in\nimaging sensors (e.g. RGB, multiple spectral and hyperspectral cameras), remote\nsensing and machine learning offer the opportunity to address this challenge.\nParticularly, hyperspectral imagery (HSI) combining with machine learning/deep\nlearning approaches is preferable for accurately identifying specific plant\ndiseases because the HSI consists of a wide range of high-quality reflectance\ninformation beyond human vision, capable of capturing both spectral-spatial\ninformation. The proposed method considers the potential disease specific\nreflectance radiation variance caused by the canopy structural diversity,\nintroduces the multiple capsule layers to model the hierarchical structure of\nthe spectral-spatial disease attributes with the encapsulated features to\nrepresent the various classes and the rotation invariance of the disease\nattributes in the feature space. We have evaluated the proposed method with the\nreal UAV-based HSI data under the controlled field conditions. The\neffectiveness of the hierarchical features has been quantitatively assessed and\ncompared with the existing representative machine learning/deep learning\nmethods. The experiment results show that the proposed model significantly\nimproves the accuracy performance when considering hierarchical-structure of\nspectral-spatial features, comparing to the existing methods only using\nspectral, or spatial or spectral-spatial features without consider\nhierarchical-structure of spectral-spatial features.",
          "link": "http://arxiv.org/abs/2107.13277",
          "publishedOn": "2021-07-29T02:00:09.592Z",
          "wordCount": 707,
          "title": "A Novel CropdocNet for Automated Potato Late Blight Disease Detection from the Unmanned Aerial Vehicle-based Hyperspectral Imagery. (arXiv:2107.13277v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yihong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Sheng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_S/0/1/0/all/0/1\">Shunmei Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_X/0/1/0/all/0/1\">Xiaoxiao Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chao Yan</a>",
          "description": "Edge computing enabled smart greenhouse is a representative application of\nInternet of Things technology, which can monitor the environmental information\nin real time and employ the information to contribute to intelligent\ndecision-making. In the process, anomaly detection for wireless sensor data\nplays an important role. However, traditional anomaly detection algorithms\noriginally designed for anomaly detection in static data have not properly\nconsidered the inherent characteristics of data stream produced by wireless\nsensor such as infiniteness, correlations and concept drift, which may pose a\nconsiderable challenge on anomaly detection based on data stream, and lead to\nlow detection accuracy and efficiency. First, data stream usually generates\nquickly which means that it is infinite and enormous, so any traditional\noff-line anomaly detection algorithm that attempts to store the whole dataset\nor to scan the dataset multiple times for anomaly detection will run out of\nmemory space. Second, there exist correlations among different data streams,\nwhich traditional algorithms hardly consider. Third, the underlying data\ngeneration process or data distribution may change over time. Thus, traditional\nanomaly detection algorithms with no model update will lose their effects.\nConsidering these issues, a novel method (called DLSHiForest) on basis of\nLocality-Sensitive Hashing and time window technique in this paper is proposed\nto solve these problems while achieving accurate and efficient detection.\nComprehensive experiments are executed using real-world agricultural greenhouse\ndataset to demonstrate the feasibility of our approach. Experimental results\nshow that our proposal is practicable in addressing challenges of traditional\nanomaly detection while ensuring accuracy and efficiency.",
          "link": "http://arxiv.org/abs/2107.13353",
          "publishedOn": "2021-07-29T02:00:09.585Z",
          "wordCount": 715,
          "title": "Fast Wireless Sensor Anomaly Detection based on Data Stream in Edge Computing Enabled Smart Greenhouse. (arXiv:2107.13353v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merlo_E/0/1/0/all/0/1\">Ettore Merlo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marhaba_M/0/1/0/all/0/1\">Mira Marhaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1\">Foutse Khomh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braiek_H/0/1/0/all/0/1\">Houssem Ben Braiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniol_G/0/1/0/all/0/1\">Giuliano Antoniol</a>",
          "description": "Neural network test cases are meant to exercise different reasoning paths in\nan architecture and used to validate the prediction outcomes. In this paper, we\nintroduce \"computational profiles\" as vectors of neuron activation levels. We\ninvestigate the distribution of computational profile likelihood of metamorphic\ntest cases with respect to the likelihood distributions of training, test and\nerror control cases. We estimate the non-parametric probability densities of\nneuron activation levels for each distinct output class. Probabilities are\ninferred using training cases only, without any additional knowledge about\nmetamorphic test cases. Experiments are performed by training a network on the\nMNIST Fashion library of images and comparing prediction likelihoods with those\nobtained from error control-data and from metamorphic test cases. Experimental\nresults show that the distributions of computational profile likelihood for\ntraining and test cases are somehow similar, while the distribution of the\nrandom-noise control-data is always remarkably lower than the observed one for\nthe training and testing sets. In contrast, metamorphic test cases show a\nprediction likelihood that lies in an extended range with respect to training,\ntests, and random noise. Moreover, the presented approach allows the\nindependent assessment of different training classes and experiments to show\nthat some of the classes are more sensitive to misclassifying metamorphic test\ncases than other classes. In conclusion, metamorphic test cases represent very\naggressive tests for neural network architectures. Furthermore, since\nmetamorphic test cases force a network to misclassify those inputs whose\nlikelihood is similar to that of training cases, they could also be considered\nas adversarial attacks that evade defenses based on computational profile\nlikelihood evaluation.",
          "link": "http://arxiv.org/abs/2107.13491",
          "publishedOn": "2021-07-29T02:00:09.561Z",
          "wordCount": 717,
          "title": "Models of Computational Profiles to Study the Likelihood of DNN Metamorphic Test Cases. (arXiv:2107.13491v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13530",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kessler_S/0/1/0/all/0/1\">Samuel Kessler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thomas_B/0/1/0/all/0/1\">Bethan Thomas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karout_S/0/1/0/all/0/1\">Salah Karout</a>",
          "description": "We present a method for continual learning of speech representations for\nmultiple languages using self-supervised learning (SSL) and applying these for\nautomatic speech recognition. There is an abundance of unannotated speech, so\ncreating self-supervised representations from raw audio and finetuning on a\nsmall annotated datasets is a promising direction to build speech recognition\nsystems. Wav2vec models perform SSL on raw audio in a pretraining phase and\nthen finetune on a small fraction of annotated data. SSL models have produced\nstate of the art results for ASR. However, these models are very expensive to\npretrain with self-supervision. We tackle the problem of learning new language\nrepresentations continually from audio without forgetting a previous language\nrepresentation. We use ideas from continual learning to transfer knowledge from\na previous task to speed up pretraining a new language task. Our\ncontinual-wav2vec2 model can decrease pretraining times by 32% when learning a\nnew language task, and learn this new audio-language representation without\nforgetting previous language representation.",
          "link": "http://arxiv.org/abs/2107.13530",
          "publishedOn": "2021-07-29T02:00:09.543Z",
          "wordCount": 634,
          "title": "Continual-wav2vec2: an Application of Continual Learning for Self-Supervised Automatic Speech Recognition. (arXiv:2107.13530v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_D/0/1/0/all/0/1\">Daniel Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stapleton_L/0/1/0/all/0/1\">Logan Stapleton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syrgkanis_V/0/1/0/all/0/1\">Vasilis Syrgkanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiwei Steven Wu</a>",
          "description": "Randomized experiments can be susceptible to selection bias due to potential\nnon-compliance by the participants. While much of the existing work has studied\ncompliance as a static behavior, we propose a game-theoretic model to study\ncompliance as dynamic behavior that may change over time. In rounds, a social\nplanner interacts with a sequence of heterogeneous agents who arrive with their\nunobserved private type that determines both their prior preferences across the\nactions (e.g., control and treatment) and their baseline rewards without taking\nany treatment. The planner provides each agent with a randomized recommendation\nthat may alter their beliefs and their action selection. We develop a novel\nrecommendation mechanism that views the planner's recommendation as a form of\ninstrumental variable (IV) that only affects an agents' action selection, but\nnot the observed rewards. We construct such IVs by carefully mapping the\nhistory -- the interactions between the planner and the previous agents -- to a\nrandom recommendation. Even though the initial agents may be completely\nnon-compliant, our mechanism can incentivize compliance over time, thereby\nenabling the estimation of the treatment effect of each treatment, and\nminimizing the cumulative regret of the planner whose goal is to identify the\noptimal treatment.",
          "link": "http://arxiv.org/abs/2107.10093",
          "publishedOn": "2021-07-29T02:00:09.515Z",
          "wordCount": 671,
          "title": "Incentivizing Compliance with Algorithmic Instruments. (arXiv:2107.10093v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lingam_V/0/1/0/all/0/1\">Vijay Lingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragesh_R/0/1/0/all/0/1\">Rahul Ragesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1\">Arun Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellamanickam_S/0/1/0/all/0/1\">Sundararajan Sellamanickam</a>",
          "description": "Graph Neural Networks (GNNs) exhibit excellent performance when graphs have\nstrong homophily property, i.e. connected nodes have the same labels. However,\nthey perform poorly on heterophilic graphs. Several approaches address the\nissue of heterophily by proposing models that adapt the graph by optimizing\ntask-specific loss function using labelled data. These adaptations are made\neither via attention or by attenuating or enhancing various\nlow-frequency/high-frequency signals, as needed for the task at hand. More\nrecent approaches adapt the eigenvalues of the graph. One important\ninterpretation of this adaptation is that these models select/weigh the\neigenvectors of the graph. Based on this interpretation, we present an\neigendecomposition based approach and propose EigenNetwork models that improve\nthe performance of GNNs on heterophilic graphs. Performance improvement is\nachieved by learning flexible graph adaptation functions that modulate the\neigenvalues of the graph. Regularization of these functions via parameter\nsharing helps to improve the performance even more. Our approach achieves up to\n11% improvement in performance over the state-of-the-art methods on\nheterophilic graphs.",
          "link": "http://arxiv.org/abs/2107.13312",
          "publishedOn": "2021-07-29T02:00:09.507Z",
          "wordCount": 610,
          "title": "Effective Eigendecomposition based Graph Adaptation for Heterophilic Networks. (arXiv:2107.13312v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13157",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Trivedi_A/0/1/0/all/0/1\">Anusua Trivedi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Desbiens_J/0/1/0/all/0/1\">Jocelyn Desbiens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gross_R/0/1/0/all/0/1\">Ron Gross</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dodhia_R/0/1/0/all/0/1\">Rahul Dodhia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferres_J/0/1/0/all/0/1\">Juan Lavista Ferres</a>",
          "description": "Purpose: To demonstrate that retinal microvasculature per se is a reliable\nbiomarker for Diabetic Retinopathy (DR) and, by extension, cardiovascular\ndiseases. Methods: Deep Learning Convolutional Neural Networks (CNN) applied to\ncolor fundus images for semantic segmentation of the blood vessels and severity\nclassification on both vascular and full images. Vessel reconstruction through\nharmonic descriptors is also used as a smoothing and de-noising tool. The\nmathematical background of the theory is also outlined. Results: For diabetic\npatients, at least 93.8% of DR No-Refer vs. Refer classification can be related\nto vasculature defects. As for the Non-Sight Threatening vs. Sight Threatening\ncase, the ratio is as high as 96.7%. Conclusion: In the case of DR, most of the\ndisease biomarkers are related topologically to the vasculature. Translational\nRelevance: Experiments conducted on eye blood vasculature reconstruction as a\nbiomarker shows a strong correlation between vasculature shape and later stages\nof DR.",
          "link": "http://arxiv.org/abs/2107.13157",
          "publishedOn": "2021-07-29T02:00:09.498Z",
          "wordCount": 603,
          "title": "Retinal Microvasculature as Biomarker for Diabetes and Cardiovascular Diseases. (arXiv:2107.13157v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramamonjison_R/0/1/0/all/0/1\">Rindra Ramamonjison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xinyu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiaolong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>",
          "description": "This paper presents a Simple and effective unsupervised adaptation method for\nRobust Object Detection (SimROD). To overcome the challenging issues of domain\nshift and pseudo-label noise, our method integrates a novel domain-centric\naugmentation method, a gradual self-labeling adaptation procedure, and a\nteacher-guided fine-tuning mechanism. Using our method, target domain samples\ncan be leveraged to adapt object detection models without changing the model\narchitecture or generating synthetic data. When applied to image corruptions\nand high-level cross-domain adaptation benchmarks, our method outperforms prior\nbaselines on multiple domain adaptation benchmarks. SimROD achieves new\nstate-of-the-art on standard real-to-synthetic and cross-camera setup\nbenchmarks. On the image corruption benchmark, models adapted with our method\nachieved a relative robustness improvement of 15-25% AP50 on Pascal-C and 5-6%\nAP on COCO-C and Cityscapes-C. On the cross-domain benchmark, our method\noutperformed the best baseline performance by up to 8% AP50 on Comic dataset\nand up to 4% on Watercolor dataset.",
          "link": "http://arxiv.org/abs/2107.13389",
          "publishedOn": "2021-07-29T02:00:09.477Z",
          "wordCount": 612,
          "title": "SimROD: A Simple Adaptation Method for Robust Object Detection. (arXiv:2107.13389v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13419",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Devi_T/0/1/0/all/0/1\">Thangjam Clarinda Devi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thaoroijam_K/0/1/0/all/0/1\">Kabita Thaoroijam</a>",
          "description": "This paper presents a vowel-based dialect identification system for\nMeeteilon. For this work, a vowel dataset is created by using Meeteilon Speech\nCorpora available at Linguistic Data Consortium for Indian Languages (LDC-IL).\nSpectral features such as formant frequencies (F1, F1 and F3) and prosodic\nfeatures such as pitch (F0), energy, intensity and segment duration values are\nextracted from monophthong vowel sounds. Random forest classifier, a decision\ntree-based ensemble algorithm is used for classification of three major\ndialects of Meeteilon namely, Imphal, Kakching and Sekmai. Model has shown an\naverage dialect identification performance in terms of accuracy of around\n61.57%. The role of spectral and prosodic features are found to be significant\nin Meeteilon dialect classification.",
          "link": "http://arxiv.org/abs/2107.13419",
          "publishedOn": "2021-07-29T02:00:09.466Z",
          "wordCount": 579,
          "title": "Vowel-based Meeteilon dialect identification using a Random Forest classifier. (arXiv:2107.13419v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patrick_Evans_J/0/1/0/all/0/1\">James Patrick-Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dannehl_M/0/1/0/all/0/1\">Moritz Dannehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinder_J/0/1/0/all/0/1\">Johannes Kinder</a>",
          "description": "Reverse engineers would benefit from identifiers like function names, but\nthese are usually unavailable in binaries. Training a machine learning model to\npredict function names automatically is promising but fundamentally hard due to\nthe enormous number of classes. In this paper, we introduce eXtreme Function\nLabeling (XFL), an extreme multi-label learning approach to selecting\nappropriate labels for binary functions. XFL splits function names into tokens,\ntreating each as an informative label akin to the problem of tagging texts in\nnatural language. To capture the semantics of binary code, we introduce DEXTER,\na novel function embedding that combines static analysis-based features with\nlocal context from the call graph and global context from the entire binary. We\ndemonstrate that XFL outperforms state-of-the-art approaches to function\nlabeling on a dataset of over 10,000 binaries from the Debian project,\nachieving a precision of 82.5%. We also study combinations of XFL with\ndifferent published embeddings for binary functions and show that DEXTER\nconsistently improves over the state of the art in information gain. As a\nresult, we are able to show that binary function labeling is best phrased in\nterms of multi-label learning, and that binary function embeddings benefit from\nmoving beyond just learning from syntax.",
          "link": "http://arxiv.org/abs/2107.13404",
          "publishedOn": "2021-07-29T02:00:09.448Z",
          "wordCount": 631,
          "title": "XFL: eXtreme Function Labeling. (arXiv:2107.13404v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13361",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_G/0/1/0/all/0/1\">Gary G. Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_V/0/1/0/all/0/1\">Vincent S. Tseng</a>",
          "description": "Arrhythmia detection from ECG is an important research subject in the\nprevention and diagnosis of cardiovascular diseases. The prevailing studies\nformulate arrhythmia detection from ECG as a time series classification\nproblem. Meanwhile, early detection of arrhythmia presents a real-world demand\nfor early prevention and diagnosis. In this paper, we address a problem of\ncardiovascular disease early classification, which is a varied-length and\nlong-length time series early classification problem as well. For solving this\nproblem, we propose a deep reinforcement learning-based framework, namely\nSnippet Policy Network (SPN), consisting of four modules, snippet generator,\nbackbone network, controlling agent, and discriminator. Comparing to the\nexisting approaches, the proposed framework features flexible input length,\nsolves the dual-optimization solution of the earliness and accuracy goals.\nExperimental results demonstrate that SPN achieves an excellent performance of\nover 80\\% in terms of accuracy. Compared to the state-of-the-art methods, at\nleast 7% improvement on different metrics, including the precision, recall,\nF1-score, and harmonic mean, is delivered by the proposed SPN. To the best of\nour knowledge, this is the first work focusing on solving the cardiovascular\nearly classification problem based on varied-length ECG data. Based on these\nexcellent features from SPN, it offers a good exemplification for addressing\nall kinds of varied-length time series early classification problems.",
          "link": "http://arxiv.org/abs/2107.13361",
          "publishedOn": "2021-07-29T02:00:09.439Z",
          "wordCount": 646,
          "title": "Snippet Policy Network for Multi-class Varied-length ECG Early Classification. (arXiv:2107.13361v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13394",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Faruqui_S/0/1/0/all/0/1\">Syed Hasib Akhter Faruqui</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Alaeddini_A/0/1/0/all/0/1\">Adel Alaeddini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fisher_Hoch_S/0/1/0/all/0/1\">Susan P Fisher-Hoch</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mccormic_J/0/1/0/all/0/1\">Joseph B Mccormic</a>",
          "description": "The emergence and progression of multiple chronic conditions (MCC) over time\noften form a dynamic network that depends on patient's modifiable risk factors\nand their interaction with non-modifiable risk factors and existing conditions.\nContinuous time Bayesian networks (CTBNs) are effective methods for modeling\nthe complex network of MCC relationships over time. However, CTBNs are not able\nto effectively formulate the dynamic impact of patient's modifiable risk\nfactors on the emergence and progression of MCC. Considering a functional CTBN\n(FCTBN) to represent the underlying structure of the MCC relationships with\nrespect to individuals' risk factors and existing conditions, we propose a\nnonlinear state-space model based on Extended Kalman filter (EKF) to capture\nthe dynamics of the patients' modifiable risk factors and existing conditions\non the MCC evolution over time. We also develop a tensor control chart to\ndynamically monitor the effect of changes in the modifiable risk factors of\nindividual patients on the risk of new chronic conditions emergence. We\nvalidate the proposed approach based on a combination of simulation and real\ndata from a dataset of 385 patients from Cameron County Hispanic Cohort (CCHC)\nover multiple years. The dataset examines the emergence of 5 chronic conditions\n(Diabetes, Obesity, Cognitive Impairment, Hyperlipidemia, and Hypertension)\nbased on 4 modifiable risk factors representing lifestyle behaviors (Diet,\nExercise, Smoking Habit, and Drinking Habit) and 3 non-modifiable risk factors,\nincluding demographic information (Age, Gender, Education). The results\ndemonstrate the effectiveness of the proposed methodology for dynamic\nprediction and monitoring of the risk of MCC emergence in individual patients.",
          "link": "http://arxiv.org/abs/2107.13394",
          "publishedOn": "2021-07-29T02:00:09.432Z",
          "wordCount": 736,
          "title": "Nonlinear State Space Modeling and Control of the Impact of Patients' Modifiable Lifestyle Behaviors on the Emergence of Multiple Chronic Conditions. (arXiv:2107.13394v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lattanzi_E/0/1/0/all/0/1\">Emanuele Lattanzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calisti_L/0/1/0/all/0/1\">Lorenzo Calisti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freschi_V/0/1/0/all/0/1\">Valerio Freschi</a>",
          "description": "Current guidelines from the World Health Organization indicate that the\nSARSCoV-2 coronavirus, which results in the novel coronavirus disease\n(COVID-19), is transmitted through respiratory droplets or by contact. Contact\ntransmission occurs when contaminated hands touch the mucous membrane of the\nmouth, nose, or eyes. Moreover, pathogens can also be transferred from one\nsurface to another by contaminated hands, which facilitates transmission by\nindirect contact. Consequently, hands hygiene is extremely important to prevent\nthe spread of the SARSCoV-2 virus. Additionally, hand washing and/or hand\nrubbing disrupts also the transmission of other viruses and bacteria that cause\ncommon colds, flu and pneumonia, thereby reducing the overall disease burden.\nThe vast proliferation of wearable devices, such as smartwatches, containing\nacceleration, rotation, magnetic field sensors, etc., together with the modern\ntechnologies of artificial intelligence, such as machine learning and more\nrecently deep-learning, allow the development of accurate applications for\nrecognition and classification of human activities such as: walking, climbing\nstairs, running, clapping, sitting, sleeping, etc. In this work we evaluate the\nfeasibility of an automatic system, based on current smartwatches, which is\nable to recognize when a subject is washing or rubbing its hands, in order to\nmonitor parameters such as frequency and duration, and to evaluate the\neffectiveness of the gesture. Our preliminary results show a classification\naccuracy of about 95% and of about 94% for respectively deep and standard\nlearning techniques.",
          "link": "http://arxiv.org/abs/2107.13405",
          "publishedOn": "2021-07-29T02:00:09.424Z",
          "wordCount": 722,
          "title": "Automatic Unstructured Handwashing Recognition using Smartwatch to Reduce Contact Transmission of Pathogens. (arXiv:2107.13405v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dsouza_A/0/1/0/all/0/1\">Alishiba Dsouza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tempelmeier_N/0/1/0/all/0/1\">Nicolas Tempelmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demidova_E/0/1/0/all/0/1\">Elena Demidova</a>",
          "description": "OpenStreetMap (OSM) is one of the richest openly available sources of\nvolunteered geographic information. Although OSM includes various geographical\nentities, their descriptions are highly heterogeneous, incomplete, and do not\nfollow any well-defined ontology. Knowledge graphs can potentially provide\nvaluable semantic information to enrich OSM entities. However, interlinking OSM\nentities with knowledge graphs is inherently difficult due to the large,\nheterogeneous, ambiguous, and flat OSM schema and the annotation sparsity. This\npaper tackles the alignment of OSM tags with the corresponding knowledge graph\nclasses holistically by jointly considering the schema and instance layers. We\npropose a novel neural architecture that capitalizes upon a shared latent space\nfor tag-to-class alignment created using linked entities in OSM and knowledge\ngraphs. Our experiments performed to align OSM datasets for several countries\nwith two of the most prominent openly available knowledge graphs, namely,\nWikidata and DBpedia, demonstrate that the proposed approach outperforms the\nstate-of-the-art schema alignment baselines by up to 53 percentage points in\nterms of F1-score. The resulting alignment facilitates new semantic annotations\nfor over 10 million OSM entities worldwide, which is more than a 400% increase\ncompared to the existing semantic annotations in OSM.",
          "link": "http://arxiv.org/abs/2107.13257",
          "publishedOn": "2021-07-29T02:00:09.404Z",
          "wordCount": 625,
          "title": "Towards Neural Schema Alignment for OpenStreetMap and Knowledge Graphs. (arXiv:2107.13257v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Struski_L/0/1/0/all/0/1\">&#x141;ukasz Struski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danel_T/0/1/0/all/0/1\">Tomasz Danel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smieja_M/0/1/0/all/0/1\">Marek &#x15a;mieja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1\">Jacek Tabor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zielinski_B/0/1/0/all/0/1\">Bartosz Zieli&#x144;ski</a>",
          "description": "Recent years have seen a surge in research on deep interpretable neural\nnetworks with decision trees as one of the most commonly incorporated tools.\nThere are at least three advantages of using decision trees over logistic\nregression classification models: they are easy to interpret since they are\nbased on binary decisions, they can make decisions faster, and they provide a\nhierarchy of classes. However, one of the well-known drawbacks of decision\ntrees, as compared to decision graphs, is that decision trees cannot reuse the\ndecision nodes. Nevertheless, decision graphs were not commonly used in deep\nlearning due to the lack of efficient gradient-based training techniques. In\nthis paper, we fill this gap and provide a general paradigm based on Markov\nprocesses, which allows for efficient training of the special type of decision\ngraphs, which we call Self-Organizing Neural Graphs (SONG). We provide an\nextensive theoretical study of SONG, complemented by experiments conducted on\nLetter, Connect4, MNIST, CIFAR, and TinyImageNet datasets, showing that our\nmethod performs on par or better than existing decision models.",
          "link": "http://arxiv.org/abs/2107.13214",
          "publishedOn": "2021-07-29T02:00:09.397Z",
          "wordCount": 601,
          "title": "SONG: Self-Organizing Neural Graphs. (arXiv:2107.13214v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yong_B/0/1/0/all/0/1\">Bang Xiang Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathy_Y/0/1/0/all/0/1\">Yasmin Fathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1\">Alexandra Brintrup</a>",
          "description": "Autoencoders are unsupervised models which have been used for detecting\nanomalies in multi-sensor environments. A typical use includes training a\npredictive model with data from sensors operating under normal conditions and\nusing the model to detect anomalies. Anomalies can come either from real\nchanges in the environment (real drift) or from faulty sensory devices (virtual\ndrift); however, the use of Autoencoders to distinguish between different\nanomalies has not yet been considered. To this end, we first propose the\ndevelopment of Bayesian Autoencoders to quantify epistemic and aleatoric\nuncertainties. We then test the Bayesian Autoencoder using a real-world\nindustrial dataset for hydraulic condition monitoring. The system is injected\nwith noise and drifts, and we have found the epistemic uncertainty to be less\nsensitive to sensor perturbations as compared to the reconstruction loss. By\nobserving the reconstructed signals with the uncertainties, we gain\ninterpretable insights, and these uncertainties offer a potential avenue for\ndistinguishing real and virtual drifts.",
          "link": "http://arxiv.org/abs/2107.13249",
          "publishedOn": "2021-07-29T02:00:09.388Z",
          "wordCount": 601,
          "title": "Bayesian Autoencoders for Drift Detection in Industrial Environments. (arXiv:2107.13249v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiyong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qianqian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Shilong Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>",
          "description": "The Area under the ROC curve (AUC) is a well-known ranking metric for\nproblems such as imbalanced learning and recommender systems. The vast majority\nof existing AUC-optimization-based machine learning methods only focus on\nbinary-class cases, while leaving the multiclass cases unconsidered. In this\npaper, we start an early trial to consider the problem of learning multiclass\nscoring functions via optimizing multiclass AUC metrics. Our foundation is\nbased on the M metric, which is a well-known multiclass extension of AUC. We\nfirst pay a revisit to this metric, showing that it could eliminate the\nimbalance issue from the minority class pairs. Motivated by this, we propose an\nempirical surrogate risk minimization framework to approximately optimize the M\nmetric. Theoretically, we show that: (i) optimizing most of the popular\ndifferentiable surrogate losses suffices to reach the Bayes optimal scoring\nfunction asymptotically; (ii) the training framework enjoys an imbalance-aware\ngeneralization error bound, which pays more attention to the bottleneck samples\nof minority classes compared with the traditional $O(\\sqrt{1/N})$ result.\nPractically, to deal with the low scalability of the computational operations,\nwe propose acceleration methods for three popular surrogate loss functions,\nincluding the exponential loss, squared loss, and hinge loss, to speed up loss\nand gradient evaluations. Finally, experimental results on 11 real-world\ndatasets demonstrate the effectiveness of our proposed framework.",
          "link": "http://arxiv.org/abs/2107.13171",
          "publishedOn": "2021-07-29T02:00:09.381Z",
          "wordCount": 650,
          "title": "Learning with Multiclass AUC: Theory and Algorithms. (arXiv:2107.13171v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arora_G/0/1/0/all/0/1\">Geetika Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_R/0/1/0/all/0/1\">Rohit K Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_K/0/1/0/all/0/1\">Kamlesh Tiwari</a>",
          "description": "This paper proposes teeth-photo, a new biometric modality for human\nauthentication on mobile and hand held devices. Biometrics samples are acquired\nusing the camera mounted on mobile device with the help of a mobile application\nhaving specific markers to register the teeth area. Region of interest (RoI) is\nthen extracted using the markers and the obtained sample is enhanced using\ncontrast limited adaptive histogram equalization (CLAHE) for better visual\nclarity. We propose a deep learning architecture and novel regularization\nscheme to obtain highly discriminative embedding for small size RoI. Proposed\ncustom loss function was able to achieve perfect classification for the tiny\nRoI of $75\\times 75$ size. The model is end-to-end and few-shot and therefore\nis very efficient in terms of time and energy requirements. The system can be\nused in many ways including device unlocking and secure authentication. To the\nbest of our understanding, this is the first work on teeth-photo based\nauthentication for mobile device. Experiments have been conducted on an\nin-house teeth-photo database collected using our application. The database is\nmade publicly available. Results have shown that the proposed system has\nperfect accuracy.",
          "link": "http://arxiv.org/abs/2107.13217",
          "publishedOn": "2021-07-29T02:00:09.373Z",
          "wordCount": 632,
          "title": "DeepTeeth: A Teeth-photo Based Human Authentication System for Mobile and Hand-held Devices. (arXiv:2107.13217v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Wei-Wei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1\">Isabelle Guyon</a>",
          "description": "Analyzing better time series with limited human effort is of interest to\nacademia and industry. Driven by business scenarios, we organized the first\nAutomated Time Series Regression challenge (AutoSeries) for the WSDM Cup 2020.\nWe present its design, analysis, and post-hoc experiments. The code submission\nrequirement precluded participants from any manual intervention, testing\nautomated machine learning capabilities of solutions, across many datasets,\nunder hardware and time limitations. We prepared 10 datasets from diverse\napplication domains (sales, power consumption, air quality, traffic, and\nparking), featuring missing data, mixed continuous and categorical variables,\nand various sampling rates. Each dataset was split into a training and a test\nsequence (which was streamed, allowing models to continuously adapt). The\nsetting of time series regression, differs from classical forecasting in that\ncovariates at the present time are known. Great strides were made by\nparticipants to tackle this AutoSeries problem, as demonstrated by the jump in\nperformance from the sample submission, and post-hoc comparisons with\nAutoGluon. Simple yet effective methods were used, based on feature\nengineering, LightGBM, and random search hyper-parameter tuning, addressing all\naspects of the challenge. Our post-hoc analyses revealed that providing\nadditional time did not yield significant improvements. The winners' code was\nopen-sourced https://www.4paradigm.com/competition/autoseries2020.",
          "link": "http://arxiv.org/abs/2107.13186",
          "publishedOn": "2021-07-29T02:00:09.353Z",
          "wordCount": 644,
          "title": "AutoML Meets Time Series Regression Design and Analysis of the AutoSeries Challenge. (arXiv:2107.13186v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lishuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinting Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsui_K/0/1/0/all/0/1\">Kwok Leung Tsui</a>",
          "description": "Short-term forecasting of passenger flow is critical for transit management\nand crowd regulation. Spatial dependencies, temporal dependencies,\ninter-station correlations driven by other latent factors, and exogenous\nfactors bring challenges to the short-term forecasts of passenger flow of urban\nrail transit networks. An innovative deep learning approach, Multi-Graph\nConvolutional-Recurrent Neural Network (MGC-RNN) is proposed to forecast\npassenger flow in urban rail transit systems to incorporate these complex\nfactors. We propose to use multiple graphs to encode the spatial and other\nheterogenous inter-station correlations. The temporal dynamics of the\ninter-station correlations are also modeled via the proposed multi-graph\nconvolutional-recurrent neural network structure. Inflow and outflow of all\nstations can be collectively predicted with multiple time steps ahead via a\nsequence to sequence(seq2seq) architecture. The proposed method is applied to\nthe short-term forecasts of passenger flow in Shenzhen Metro, China. The\nexperimental results show that MGC-RNN outperforms the benchmark algorithms in\nterms of forecasting accuracy. Besides, it is found that the inter-station\ndriven by network distance, network structure, and recent flow patterns are\nsignificant factors for passenger flow forecasting. Moreover, the architecture\nof LSTM-encoder-decoder can capture the temporal dependencies well. In general,\nthe proposed framework could provide multiple views of passenger flow dynamics\nfor fine prediction and exhibit a possibility for multi-source heterogeneous\ndata fusion in the spatiotemporal forecast tasks.",
          "link": "http://arxiv.org/abs/2107.13226",
          "publishedOn": "2021-07-29T02:00:09.345Z",
          "wordCount": 659,
          "title": "Multi-Graph Convolutional-Recurrent Neural Network (MGC-RNN) for Short-Term Forecasting of Transit Passenger Flow. (arXiv:2107.13226v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruhe_D/0/1/0/all/0/1\">David Ruhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forre_P/0/1/0/all/0/1\">Patrick Forr&#xe9;</a>",
          "description": "We perform approximate inference in state-space models that allow for\nnonlinear higher-order Markov chains in latent space. The conditional\nindependencies of the generative model enable us to parameterize only an\ninference model, which learns to estimate clean states in a self-supervised\nmanner using maximum likelihood. First, we propose a recurrent method that is\ntrained directly on noisy observations. Afterward, we cast the model such that\nthe optimization problem leads to an update scheme that backpropagates through\na recursion similar to the classical Kalman filter and smoother. In scientific\napplications, domain knowledge can give a linear approximation of the latent\ntransition maps. We can easily incorporate this knowledge into our model,\nleading to a hybrid inference approach. In contrast to other methods,\nexperiments show that the hybrid method makes the inferred latent states\nphysically more interpretable and accurate, especially in low-data regimes.\nFurthermore, we do not rely on an additional parameterization of the generative\nmodel or supervision via uncorrupted observations or ground truth latent\nstates. Despite our model's simplicity, we obtain competitive results on the\nchaotic Lorenz system compared to a fully supervised approach and outperform a\nmethod based on variational inference.",
          "link": "http://arxiv.org/abs/2107.13349",
          "publishedOn": "2021-07-29T02:00:09.338Z",
          "wordCount": 617,
          "title": "Self-Supervised Hybrid Inference in State-Space Models. (arXiv:2107.13349v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peuter_S/0/1/0/all/0/1\">Sebastiaan De Peuter</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Oulasvirta_A/0/1/0/all/0/1\">Antti Oulasvirta</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Kaski_S/0/1/0/all/0/1\">Samuel Kaski</a> (1 and 3) ((1) Department of Computer Science, Aalto University, Finland, (2) Department of Communications and Networking, Aalto University, Finland, (3) Department of Computer Science, University of Manchester, UK)",
          "description": "AI for supporting designers needs to be rethought. It should aim to\ncooperate, not automate, by supporting and leveraging the creativity and\nproblem-solving of designers. The challenge for such AI is how to infer\ndesigners' goals and then help them without being needlessly disruptive. We\npresent AI-assisted design: a framework for creating such AI, built around\ngenerative user models which enable reasoning about designers' goals,\nreasoning, and capabilities.",
          "link": "http://arxiv.org/abs/2107.13074",
          "publishedOn": "2021-07-29T02:00:09.331Z",
          "wordCount": 550,
          "title": "Toward AI Assistants That Let Designers Design. (arXiv:2107.13074v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_E/0/1/0/all/0/1\">Eric Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jennifer J. Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_A/0/1/0/all/0/1\">Ann Kennedy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yisong Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Swarat Chaudhuri</a>",
          "description": "We present a framework for the unsupervised learning of neurosymbolic\nencoders, i.e., encoders obtained by composing neural networks with symbolic\nprograms from a domain-specific language. Such a framework can naturally\nincorporate symbolic expert knowledge into the learning process and lead to\nmore interpretable and factorized latent representations than fully neural\nencoders. Also, models learned this way can have downstream impact, as many\nanalysis workflows can benefit from having clean programmatic descriptions. We\nground our learning algorithm in the variational autoencoding (VAE) framework,\nwhere we aim to learn a neurosymbolic encoder in conjunction with a standard\ndecoder. Our algorithm integrates standard VAE-style training with modern\nprogram synthesis techniques. We evaluate our method on learning latent\nrepresentations for real-world trajectory data from animal biology and sports\nanalytics. We show that our approach offers significantly better separation\nthan standard VAEs and leads to practical gains on downstream tasks.",
          "link": "http://arxiv.org/abs/2107.13132",
          "publishedOn": "2021-07-29T02:00:09.324Z",
          "wordCount": 576,
          "title": "Unsupervised Learning of Neurosymbolic Encoders. (arXiv:2107.13132v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yong_B/0/1/0/all/0/1\">Bang Xiang Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1\">Alexandra Brintrup</a>",
          "description": "Recent advancements in predictive machine learning has led to its application\nin various use cases in manufacturing. Most research focused on maximising\npredictive accuracy without addressing the uncertainty associated with it.\nWhile accuracy is important, focusing primarily on it poses an overfitting\ndanger, exposing manufacturers to risk, ultimately hindering the adoption of\nthese techniques. In this paper, we determine the sources of uncertainty in\nmachine learning and establish the success criteria of a machine learning\nsystem to function well under uncertainty in a cyber-physical manufacturing\nsystem (CPMS) scenario. Then, we propose a multi-agent system architecture\nwhich leverages probabilistic machine learning as a means of achieving such\ncriteria. We propose possible scenarios for which our proposed architecture is\nuseful and discuss future work. Experimentally, we implement Bayesian Neural\nNetworks for multi-tasks classification on a public dataset for the real-time\ncondition monitoring of a hydraulic system and demonstrate the usefulness of\nthe system by evaluating the probability of a prediction being accurate given\nits uncertainty. We deploy these models using our proposed agent-based\nframework and integrate web visualisation to demonstrate its real-time\nfeasibility.",
          "link": "http://arxiv.org/abs/2107.13252",
          "publishedOn": "2021-07-29T02:00:09.318Z",
          "wordCount": 635,
          "title": "Multi Agent System for Machine Learning Under Uncertainty in Cyber Physical Manufacturing System. (arXiv:2107.13252v1 [cs.MA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canessa_G/0/1/0/all/0/1\">Gianpiero Canessa</a>",
          "description": "Support vector machines (SVM) is one of the well known supervised classes of\nlearning algorithms. Furthermore, the conic-segmentation SVM (CS-SVM) is a\nnatural multiclass analogue of the standard binary SVM, as CS-SVM models are\ndealing with the situation where the exact values of the data points are known.\nThis paper studies CS-SVM when the data points are uncertain or mislabelled.\nWith some properties known for the distributions, a chance-constrained CS-SVM\napproach is used to ensure the small probability of misclassification for the\nuncertain data. The geometric interpretation is presented to show how CS-SVM\nworks. Finally, we present experimental results to investigate the chance\nconstrained CS-SVM's performance.",
          "link": "http://arxiv.org/abs/2107.13319",
          "publishedOn": "2021-07-29T02:00:09.297Z",
          "wordCount": 547,
          "title": "Chance constrained conic-segmentation support vector machine with uncertain data. (arXiv:2107.13319v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13393",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Choe_Y/0/1/0/all/0/1\">Yoonsuck Choe</a>",
          "description": "Brain science and artificial intelligence have made great progress toward the\nunderstanding and engineering of the human mind. The progress has accelerated\nsignificantly since the turn of the century thanks to new methods for probing\nthe brain (both structure and function), and rapid development in deep learning\nresearch. However, despite these new developments, there are still many open\nquestions, such as how to understand the brain at the system level, and various\nrobustness issues and limitations of deep learning. In this informal essay, I\nwill talk about some of the concepts that are central to brain science and\nartificial intelligence, such as information and memory, and discuss how a\ndifferent view on these concepts can help us move forward, beyond current\nlimits of our understanding in these fields.",
          "link": "http://arxiv.org/abs/2107.13393",
          "publishedOn": "2021-07-29T02:00:09.291Z",
          "wordCount": 576,
          "title": "Meaning Versus Information, Prediction Versus Memory, and Question Versus Answer. (arXiv:2107.13393v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chubba_e/0/1/0/all/0/1\">ennifer Chubba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Missaouib_S/0/1/0/all/0/1\">Sondess Missaouib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Concannonc_S/0/1/0/all/0/1\">Shauna Concannonc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maloneyb_L/0/1/0/all/0/1\">Liam Maloneyb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1\">James Alfred Walker</a>",
          "description": "Conversational Artificial Intelligence (CAI) systems and Intelligent Personal\nAssistants (IPA), such as Alexa, Cortana, Google Home and Siri are becoming\nubiquitous in our lives, including those of children, the implications of which\nis receiving increased attention, specifically with respect to the effects of\nthese systems on children's cognitive, social and linguistic development.\nRecent advances address the implications of CAI with respect to privacy,\nsafety, security, and access. However, there is a need to connect and embed the\nethical and technical aspects in the design. Using a case-study of a research\nand development project focused on the use of CAI in storytelling for children,\nthis paper reflects on the social context within a specific case of technology\ndevelopment, as substantiated and supported by argumentation from within the\nliterature. It describes the decision making process behind the recommendations\nmade on this case for their adoption in the creative industries. Further\nresearch that engages with developers and stakeholders in the ethics of\nstorytelling through CAI is highlighted as a matter of urgency.",
          "link": "http://arxiv.org/abs/2107.13076",
          "publishedOn": "2021-07-29T02:00:09.284Z",
          "wordCount": 620,
          "title": "Interactive Storytelling for Children: A Case-study of Design and Development Considerations for Ethical Conversational AI. (arXiv:2107.13076v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_M/0/1/0/all/0/1\">Masahiro Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1\">Takaaki Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>",
          "description": "With the recent rapid progress in the study of deep generative models (DGMs),\nthere is a need for a framework that can implement them in a simple and generic\nway. In this research, we focus on two features of the latest DGMs: (1) deep\nneural networks are encapsulated by probability distributions and (2) models\nare designed and learned based on an objective function. Taking these features\ninto account, we propose a new DGM library called Pixyz. We experimentally show\nthat our library is faster than existing probabilistic modeling languages in\nlearning simple DGMs and we show that our library can be used to implement\ncomplex DGMs in a simple and concise manner, which is difficult to do with\nexisting libraries.",
          "link": "http://arxiv.org/abs/2107.13109",
          "publishedOn": "2021-07-29T02:00:09.276Z",
          "wordCount": 553,
          "title": "Pixyz: a library for developing deep generative models. (arXiv:2107.13109v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Curth_A/0/1/0/all/0/1\">Alicia Curth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1\">Mihaela van der Schaar</a>",
          "description": "The machine learning toolbox for estimation of heterogeneous treatment\neffects from observational data is expanding rapidly, yet many of its\nalgorithms have been evaluated only on a very limited set of semi-synthetic\nbenchmark datasets. In this paper, we show that even in arguably the simplest\nsetting -- estimation under ignorability assumptions -- the results of such\nempirical evaluations can be misleading if (i) the assumptions underlying the\ndata-generating mechanisms in benchmark datasets and (ii) their interplay with\nbaseline algorithms are inadequately discussed. We consider two popular machine\nlearning benchmark datasets for evaluation of heterogeneous treatment effect\nestimators -- the IHDP and ACIC2016 datasets -- in detail. We identify problems\nwith their current use and highlight that the inherent characteristics of the\nbenchmark datasets favor some algorithms over others -- a fact that is rarely\nacknowledged but of immense relevance for interpretation of empirical results.\nWe close by discussing implications and possible next steps.",
          "link": "http://arxiv.org/abs/2107.13346",
          "publishedOn": "2021-07-29T02:00:09.269Z",
          "wordCount": 619,
          "title": "Doing Great at Estimating CATE? On the Neglected Assumptions in Benchmark Comparisons of Treatment Effect Estimators. (arXiv:2107.13346v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenhua Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "In this work, we propose an adversarial unsupervised domain adaptation (UDA)\napproach with the inherent conditional and label shifts, in which we aim to\nalign the distributions w.r.t. both $p(x|y)$ and $p(y)$. Since the label is\ninaccessible in the target domain, the conventional adversarial UDA assumes\n$p(y)$ is invariant across domains, and relies on aligning $p(x)$ as an\nalternative to the $p(x|y)$ alignment. To address this, we provide a thorough\ntheoretical and empirical analysis of the conventional adversarial UDA methods\nunder both conditional and label shifts, and propose a novel and practical\nalternative optimization scheme for adversarial UDA. Specifically, we infer the\nmarginal $p(y)$ and align $p(x|y)$ iteratively in the training, and precisely\nalign the posterior $p(y|x)$ in testing. Our experimental results demonstrate\nits effectiveness on both classification and segmentation UDA, and partial UDA.",
          "link": "http://arxiv.org/abs/2107.13469",
          "publishedOn": "2021-07-29T02:00:09.251Z",
          "wordCount": 605,
          "title": "Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate. (arXiv:2107.13469v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13090",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hambly_B/0/1/0/all/0/1\">Ben Hambly</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xu_R/0/1/0/all/0/1\">Renyuan Xu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yang_H/0/1/0/all/0/1\">Huining Yang</a>",
          "description": "We consider a general-sum N-player linear-quadratic game with stochastic\ndynamics over a finite horizon and prove the global convergence of the natural\npolicy gradient method to the Nash equilibrium. In order to prove the\nconvergence of the method, we require a certain amount of noise in the system.\nWe give a condition, essentially a lower bound on the covariance of the noise\nin terms of the model parameters, in order to guarantee convergence. We\nillustrate our results with numerical experiments to show that even in\nsituations where the policy gradient method may not converge in the\ndeterministic setting, the addition of noise leads to convergence.",
          "link": "http://arxiv.org/abs/2107.13090",
          "publishedOn": "2021-07-29T02:00:09.243Z",
          "wordCount": 559,
          "title": "Policy Gradient Methods Find the Nash Equilibrium in N-player General-sum Linear-quadratic Games. (arXiv:2107.13090v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aalst_W/0/1/0/all/0/1\">Wil van der Aalst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockhoff_T/0/1/0/all/0/1\">Tobias Brockhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghahfarokhi_A/0/1/0/all/0/1\">Anahita Farhang Ghahfarokhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pourbafrani_M/0/1/0/all/0/1\">Mahsa Pourbafrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uysal_M/0/1/0/all/0/1\">Merih Seran Uysal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelst_S/0/1/0/all/0/1\">Sebastiaan van Zelst</a>",
          "description": "Operational processes in production, logistics, material handling,\nmaintenance, etc., are supported by cyber-physical systems combining hardware\nand software components. As a result, the digital and the physical world are\nclosely aligned, and it is possible to track operational processes in detail\n(e.g., using sensors). The abundance of event data generated by today's\noperational processes provides opportunities and challenges for process mining\ntechniques supporting process discovery, performance analysis, and conformance\nchecking. Using existing process mining tools, it is already possible to\nautomatically discover process models and uncover performance and compliance\nproblems. In the DFG-funded Cluster of Excellence \"Internet of Production\"\n(IoP), process mining is used to create \"digital shadows\" to improve a wide\nvariety of operational processes. However, operational processes are dynamic,\ndistributed, and complex. Driven by the challenges identified in the IoP\ncluster, we work on novel techniques for comparative process mining (comparing\nprocess variants for different products at different locations at different\ntimes), object-centric process mining (to handle processes involving different\ntypes of objects that interact), and forward-looking process mining (to explore\n\"What if?\" questions). By addressing these challenges, we aim to develop\nvaluable \"digital shadows\" that can be used to remove operational friction.",
          "link": "http://arxiv.org/abs/2107.13066",
          "publishedOn": "2021-07-29T02:00:09.236Z",
          "wordCount": 660,
          "title": "Removing Operational Friction Using Process Mining: Challenges Provided by the Internet of Production (IoP). (arXiv:2107.13066v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yanan Sun</a>",
          "description": "Neural Architecture Search (NAS) can automatically design well-performed\narchitectures of Deep Neural Networks (DNNs) for the tasks at hand. However,\none bottleneck of NAS is the prohibitively computational cost largely due to\nthe expensive performance evaluation. The neural predictors can directly\nestimate the performance without any training of the DNNs to be evaluated, thus\nhave drawn increasing attention from researchers. Despite their popularity,\nthey also suffer a severe limitation: the shortage of annotated DNN\narchitectures for effectively training the neural predictors. In this paper, we\nproposed Homogeneous Architecture Augmentation for Neural Predictor (HAAP) of\nDNN architectures to address the issue aforementioned. Specifically, a\nhomogeneous architecture augmentation algorithm is proposed in HAAP to generate\nsufficient training data taking the use of homogeneous representation.\nFurthermore, the one-hot encoding strategy is introduced into HAAP to make the\nrepresentation of DNN architectures more effective. The experiments have been\nconducted on both NAS-Benchmark-101 and NAS-Bench-201 dataset. The experimental\nresults demonstrate that the proposed HAAP algorithm outperforms the state of\nthe arts compared, yet with much less training data. In addition, the ablation\nstudies on both benchmark datasets have also shown the universality of the\nhomogeneous architecture augmentation.",
          "link": "http://arxiv.org/abs/2107.13153",
          "publishedOn": "2021-07-29T02:00:09.228Z",
          "wordCount": 631,
          "title": "Homogeneous Architecture Augmentation for Neural Predictor. (arXiv:2107.13153v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Sayantini Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivisonno_R/0/1/0/all/0/1\">Riccardo Trivisonno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carle_G/0/1/0/all/0/1\">Georg Carle</a>",
          "description": "Network automation is gaining significant attention in the development of B5G\nnetworks, primarily for reducing operational complexity, expenditures and\nimproving network efficiency. Concurrently operating closed loops aiming for\nindividual optimization targets may cause conflicts which, left unresolved,\nwould lead to significant degradation in network Key Performance Indicators\n(KPIs), thereby resulting in sub-optimal network performance. Centralized\ncoordination, albeit optimal, is impractical in large scale networks and for\ntime-critical applications. Decentralized approaches are therefore envisaged in\nthe evolution to B5G and subsequently, 6G networks. This work explores\npervasive intelligence for conflict resolution in network automation, as an\nalternative to centralized orchestration. A Q-Learning decentralized approach\nto network automation is proposed, and an application to network slice\nauto-scaling is designed and evaluated. Preliminary results highlight the\npotential of the proposed scheme and justify further research work in this\ndirection.",
          "link": "http://arxiv.org/abs/2107.13268",
          "publishedOn": "2021-07-29T02:00:09.218Z",
          "wordCount": 606,
          "title": "Q-Learning for Conflict Resolution in B5G Network Automation. (arXiv:2107.13268v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellogin_A/0/1/0/all/0/1\">Alejandro Bellog&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomo_C/0/1/0/all/0/1\">Claudio Pomo</a>",
          "description": "Collaborative filtering models based on matrix factorization and learned\nsimilarities using Artificial Neural Networks (ANNs) have gained significant\nattention in recent years. This is, in part, because ANNs have demonstrated\ngood results in a wide variety of recommendation tasks. The introduction of\nANNs within the recommendation ecosystem has been recently questioned, raising\nseveral comparisons in terms of efficiency and effectiveness. One aspect most\nof these comparisons have in common is their focus on accuracy, neglecting\nother evaluation dimensions important for the recommendation, such as novelty,\ndiversity, or accounting for biases. We replicate experiments from three papers\nthat compare Neural Collaborative Filtering (NCF) and Matrix Factorization\n(MF), to extend the analysis to other evaluation dimensions. Our contribution\nshows that the experiments are entirely reproducible, and we extend the study\nincluding other accuracy metrics and two statistical hypothesis tests. We\ninvestigated the Diversity and Novelty of the recommendations, showing that MF\nprovides a better accuracy also on the long tail, although NCF provides a\nbetter item coverage and more diversified recommendations. We discuss the bias\neffect generated by the tested methods. They show a relatively small bias, but\nother recommendation baselines, with competitive accuracy performance,\nconsistently show to be less affected by this issue. This is the first work, to\nthe best of our knowledge, where several evaluation dimensions have been\nexplored for an array of SOTA algorithms covering recent adaptations of ANNs\nand MF. Hence, we show the potential these techniques may have on\nbeyond-accuracy evaluation while analyzing the effect on reproducibility these\ncomplementary dimensions may spark. Available at\ngithub.com/sisinflab/Reenvisioning-the-comparison-between-Neural-Collaborative-Filtering-and-Matrix-Factorization",
          "link": "http://arxiv.org/abs/2107.13472",
          "publishedOn": "2021-07-29T02:00:09.198Z",
          "wordCount": 710,
          "title": "Reenvisioning Collaborative Filtering vs Matrix Factorization. (arXiv:2107.13472v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yubin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Pengyi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>",
          "description": "There has been a growing interest in unsupervised domain adaptation (UDA) to\nalleviate the data scalability issue, while the existing works usually focus on\nclassifying independently discrete labels. However, in many tasks (e.g.,\nmedical diagnosis), the labels are discrete and successively distributed. The\nUDA for ordinal classification requires inducing non-trivial ordinal\ndistribution prior to the latent space. Target for this, the partially ordered\nset (poset) is defined for constraining the latent vector. Instead of the\ntypically i.i.d. Gaussian latent prior, in this work, a recursively conditional\nGaussian (RCG) set is proposed for ordered constraint modeling, which admits a\ntractable joint distribution prior. Furthermore, we are able to control the\ndensity of content vectors that violate the poset constraint by a simple\n\"three-sigma rule\". We explicitly disentangle the cross-domain images into a\nshared ordinal prior induced ordinal content space and two separate\nsource/target ordinal-unrelated spaces, and the self-training is worked on the\nshared space exclusively for ordinal-aware domain alignment. Extensive\nexperiments on UDA medical diagnoses and facial age estimation demonstrate its\neffectiveness.",
          "link": "http://arxiv.org/abs/2107.13467",
          "publishedOn": "2021-07-29T02:00:09.190Z",
          "wordCount": 624,
          "title": "Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation. (arXiv:2107.13467v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuesong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>",
          "description": "Node features and structural information of a graph are both crucial for\nsemi-supervised node classification problems. A variety of graph neural network\n(GNN) based approaches have been proposed to tackle these problems, which\ntypically determine output labels through feature aggregation. This can be\nproblematic, as it implies conditional independence of output nodes given\nhidden representations, despite their direct connections in the graph. To learn\nthe direct influence among output nodes in a graph, we propose the Explicit\nPairwise Factorized Graph Neural Network (EPFGNN), which models the whole graph\nas a partially observed Markov Random Field. It contains explicit pairwise\nfactors to model output-output relations and uses a GNN backbone to model\ninput-output relations. To balance model complexity and expressivity, the\npairwise factors have a shared component and a separate scaling coefficient for\neach edge. We apply the EM algorithm to train our model, and utilize a\nstar-shaped piecewise likelihood for the tractable surrogate objective. We\nconduct experiments on various datasets, which shows that our model can\neffectively improve the performance for semi-supervised node classification on\ngraphs.",
          "link": "http://arxiv.org/abs/2107.13059",
          "publishedOn": "2021-07-29T02:00:09.179Z",
          "wordCount": 613,
          "title": "Explicit Pairwise Factorized Graph Neural Network for Semi-Supervised Node Classification. (arXiv:2107.13059v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Picallo_M/0/1/0/all/0/1\">Mario Alvarez-Picallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghica_D/0/1/0/all/0/1\">Dan R. Ghica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sprunger_D/0/1/0/all/0/1\">David Sprunger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanasi_F/0/1/0/all/0/1\">Fabio Zanasi</a>",
          "description": "We enhance the calculus of string diagrams for monoidal categories with\nhierarchical features in order to capture closed monoidal (and cartesian\nclosed) structure. Using this new syntax we formulate an automatic\ndifferentiation algorithm for (applied) simply typed lambda calculus in the\nstyle of [Pearlmutter and Siskind 2008] and we prove for the first time its\nsoundness. To give an efficient yet principled implementation of the AD\nalgorithm we define a sound and complete representation of hierarchical string\ndiagrams as a class of hierarchical hypergraphs we call hypernets.",
          "link": "http://arxiv.org/abs/2107.13433",
          "publishedOn": "2021-07-29T02:00:09.160Z",
          "wordCount": 523,
          "title": "Functorial String Diagrams for Reverse-Mode Automatic Differentiation. (arXiv:2107.13433v1 [cs.PL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hanxiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1\">Helena Kotthaus</a>",
          "description": "In the field of autonomous driving and robotics, point clouds are showing\ntheir excellent real-time performance as raw data from most of the mainstream\n3D sensors. Therefore, point cloud neural networks have become a popular\nresearch direction in recent years. So far, however, there has been little\ndiscussion about the explainability of deep neural networks for point clouds.\nIn this paper, we propose new explainability approaches for point cloud deep\nneural networks based on local surrogate model-based methods to show which\ncomponents make the main contribution to the classification. Moreover, we\npropose a quantitative validation method for explainability methods of point\nclouds which enhances the persuasive power of explainability by dropping the\nmost positive or negative contributing features and monitoring how the\nclassification scores of specific categories change. To enable an intuitive\nexplanation of misclassified instances, we display features with confounding\ncontributions. Our new explainability approach provides a fairly accurate, more\nintuitive and widely applicable explanation for point cloud classification\ntasks. Our code is available at https://github.com/Explain3D/Explainable3D",
          "link": "http://arxiv.org/abs/2107.13459",
          "publishedOn": "2021-07-29T02:00:09.104Z",
          "wordCount": 610,
          "title": "Surrogate Model-Based Explainability Methods for Point Cloud NNs. (arXiv:2107.13459v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_G/0/1/0/all/0/1\">Guangliang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minglei Li</a>",
          "description": "Channel estimation and signal detection are essential steps to ensure the\nquality of end-to-end communication in orthogonal frequency-division\nmultiplexing (OFDM) systems. In this paper, we develop a DDLSD approach, i.e.,\nData-driven Deep Learning for Signal Detection in OFDM systems. First, the OFDM\nsystem model is established. Then, the long short-term memory (LSTM) is\nintroduced into the OFDM system model. Wireless channel data is generated\nthrough simulation, the preprocessed time series feature information is input\ninto the LSTM to complete the offline training. Finally, the trained model is\nused for online recovery of transmitted signal. The difference between this\nscheme and existing OFDM receiver is that explicit estimated channel state\ninformation (CSI) is transformed into invisible estimated CSI, and the transmit\nsymbol is directly restored. Simulation results show that the DDLSD scheme\noutperforms the existing traditional methods in terms of improving channel\nestimation and signal detection performance.",
          "link": "http://arxiv.org/abs/2107.13423",
          "publishedOn": "2021-07-29T02:00:09.096Z",
          "wordCount": 594,
          "title": "A Signal Detection Scheme Based on Deep Learning in OFDM Systems. (arXiv:2107.13423v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dongchen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-feng Yang</a>",
          "description": "Traditional maximum entropy and sparsity-based algorithms for analytic\ncontinuation often suffer from the ill-posed kernel matrix or demand tremendous\ncomputation time for parameter tuning. Here we propose a neural network method\nby convex optimization and replace the ill-posed inverse problem by a sequence\nof well-conditioned surrogate problems. After training, the learned optimizers\nare able to give a solution of high quality with low time cost and achieve\nhigher parameter efficiency than heuristic full-connected networks. The output\ncan also be used as a neural default model to improve the maximum entropy for\nbetter performance. Our methods may be easily extended to other\nhigh-dimensional inverse problems via large-scale pretraining.",
          "link": "http://arxiv.org/abs/2107.13265",
          "publishedOn": "2021-07-29T02:00:09.087Z",
          "wordCount": 542,
          "title": "Learned Optimizers for Analytic Continuation. (arXiv:2107.13265v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13449",
          "author": "<a href=\"http://arxiv.org/find/nucl-th/1/au:+Sarkar_A/0/1/0/all/0/1\">Avik Sarkar</a>, <a href=\"http://arxiv.org/find/nucl-th/1/au:+Lee_D/0/1/0/all/0/1\">Dean Lee</a>",
          "description": "Emulators that can bypass computationally expensive scientific calculations\nwith high accuracy and speed can enable new studies of fundamental science as\nwell as more potential applications. In this work we focus on solving a system\nof constraint equations efficiently using a new machine learning approach that\nwe call self-learning emulation. A self-learning emulator is an active learning\nprotocol that can rapidly solve a system of equations over some range of\ncontrol parameters. The key ingredient is a fast estimate of the emulator error\nthat becomes progressively more accurate as the emulator improves. This\nacceleration is possible because the emulator itself is used to estimate the\nerror, and we illustrate with two examples. The first uses cubic spline\ninterpolation to find the roots of a polynomial with variable coefficients. The\nsecond example uses eigenvector continuation to find the eigenvectors and\neigenvalues of a large Hamiltonian matrix that depends on several control\nparameters. We envision future applications of self-learning emulators for\nsolving systems of algebraic equations, linear and nonlinear differential\nequations, and linear and nonlinear eigenvalue problems.",
          "link": "http://arxiv.org/abs/2107.13449",
          "publishedOn": "2021-07-29T02:00:09.080Z",
          "wordCount": 625,
          "title": "Self-learning Emulators and Eigenvector Continuation. (arXiv:2107.13449v1 [nucl-th])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13148",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Ullah_A/0/1/0/all/0/1\">A. K. M. Amanat Ullah</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Imtiaz_F/0/1/0/all/0/1\">Fahim Imtiaz</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Ihsan_M/0/1/0/all/0/1\">Miftah Uddin Md Ihsan</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Alam_M/0/1/0/all/0/1\">Md. Golam Rabiul Alam</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Majumdar_M/0/1/0/all/0/1\">Mahbub Majumdar</a>",
          "description": "The unpredictability and volatility of the stock market render it challenging\nto make a substantial profit using any generalized scheme. This paper intends\nto discuss our machine learning model, which can make a significant amount of\nprofit in the US stock market by performing live trading in the Quantopian\nplatform while using resources free of cost. Our top approach was to use\nensemble learning with four classifiers: Gaussian Naive Bayes, Decision Tree,\nLogistic Regression with L1 regularization and Stochastic Gradient Descent, to\ndecide whether to go long or short on a particular stock. Our best model\nperformed daily trade between July 2011 and January 2019, generating 54.35%\nprofit. Finally, our work showcased that mixtures of weighted classifiers\nperform better than any individual predictor about making trading decisions in\nthe stock market.",
          "link": "http://arxiv.org/abs/2107.13148",
          "publishedOn": "2021-07-29T02:00:09.073Z",
          "wordCount": 594,
          "title": "Combining Machine Learning Classifiers for Stock Trading with Effective Feature Extraction. (arXiv:2107.13148v1 [q-fin.TR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Divi_S/0/1/0/all/0/1\">Siddharth Divi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yi-Shan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farrukh_H/0/1/0/all/0/1\">Habiba Farrukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celik_Z/0/1/0/all/0/1\">Z. Berkay Celik</a>",
          "description": "In Federated Learning (FL), the clients learn a single global model (FedAvg)\nthrough a central aggregator. In this setting, the non-IID distribution of the\ndata across clients restricts the global FL model from delivering good\nperformance on the local data of each client. Personalized FL aims to address\nthis problem by finding a personalized model for each client. Recent works\nwidely report the average personalized model accuracy on a particular data\nsplit of a dataset to evaluate the effectiveness of their methods. However,\nconsidering the multitude of personalization approaches proposed, it is\ncritical to study the per-user personalized accuracy and the accuracy\nimprovements among users with an equitable notion of fairness. To address these\nissues, we present a set of performance and fairness metrics intending to\nassess the quality of personalized FL methods. We apply these metrics to four\nrecently proposed personalized FL methods, PersFL, FedPer, pFedMe, and\nPer-FedAvg, on three different data splits of the CIFAR-10 dataset. Our\nevaluations show that the personalized model with the highest average accuracy\nacross users may not necessarily be the fairest. Our code is available at\nhttps://tinyurl.com/1hp9ywfa for public use.",
          "link": "http://arxiv.org/abs/2107.13173",
          "publishedOn": "2021-07-29T02:00:09.053Z",
          "wordCount": 627,
          "title": "New Metrics to Evaluate the Performance and Fairness of Personalized Federated Learning. (arXiv:2107.13173v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yong_B/0/1/0/all/0/1\">Bang Xiang Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pearce_T/0/1/0/all/0/1\">Tim Pearce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1\">Alexandra Brintrup</a>",
          "description": "After an autoencoder (AE) has learnt to reconstruct one dataset, it might be\nexpected that the likelihood on an out-of-distribution (OOD) input would be\nlow. This has been studied as an approach to detect OOD inputs. Recent work\nshowed this intuitive approach can fail for the dataset pairs FashionMNIST vs\nMNIST. This paper suggests this is due to the use of Bernoulli likelihood and\nanalyses why this is the case, proposing two fixes: 1) Compute the uncertainty\nof likelihood estimate by using a Bayesian version of the AE. 2) Use\nalternative distributions to model the likelihood.",
          "link": "http://arxiv.org/abs/2107.13304",
          "publishedOn": "2021-07-29T02:00:09.045Z",
          "wordCount": 550,
          "title": "Bayesian Autoencoders: Analysing and Fixing the Bernoulli likelihood for Out-of-Distribution Detection. (arXiv:2107.13304v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13430",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nishida_K/0/1/0/all/0/1\">Kiheiji Nishida</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Naito_K/0/1/0/all/0/1\">Kanta Naito</a>",
          "description": "This paper studies kernel density estimation by stagewise minimization\nalgorithm with a simple dictionary on U-divergence. We randomly split an i.i.d.\nsample into the two disjoint sets, one to be used for constructing the kernels\nin the dictionary and the other for evaluating the estimator, and implement the\nalgorithm. The resulting estimator brings us data-adaptive weighting parameters\nand bandwidth matrices, and realizes a sparse representation of kernel density\nestimation. We present the non-asymptotic error bounds of our estimator and\nconfirm its performance by simulations compared with the direct plug-in\nbandwidth matrices and the reduced set density estimator.",
          "link": "http://arxiv.org/abs/2107.13430",
          "publishedOn": "2021-07-29T02:00:09.036Z",
          "wordCount": 538,
          "title": "Kernel Density Estimation by Stagewise Algorithm with a Simple Dictionary. (arXiv:2107.13430v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13136",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marino_J/0/1/0/all/0/1\">Joseph Marino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mandt_S/0/1/0/all/0/1\">Stephan Mandt</a>",
          "description": "While recent machine learning research has revealed connections between deep\ngenerative models such as VAEs and rate-distortion losses used in learned\ncompression, most of this work has focused on images. In a similar spirit, we\nview recently proposed neural video coding algorithms through the lens of deep\nautoregressive and latent variable modeling. We present recent neural video\ncodecs as instances of a generalized stochastic temporal autoregressive\ntransform, and propose new avenues for further improvements inspired by\nnormalizing flows and structured priors. We propose several architectures that\nyield state-of-the-art video compression performance on full-resolution video\nand discuss their tradeoffs and ablations. In particular, we propose (i)\nimproved temporal autoregressive transforms, (ii) improved entropy models with\nstructured and temporal dependencies, and (iii) variable bitrate versions of\nour algorithms. Since our improvements are compatible with a large class of\nexisting models, we provide further evidence that the generative modeling\nviewpoint can advance the neural video coding field.",
          "link": "http://arxiv.org/abs/2107.13136",
          "publishedOn": "2021-07-29T02:00:09.028Z",
          "wordCount": 637,
          "title": "Insights from Generative Modeling for Neural Video Compression. (arXiv:2107.13136v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Colin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yining Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>",
          "description": "A common lens to theoretically study neural net architectures is to analyze\nthe functions they can approximate. However, the constructions from\napproximation theory often have unrealistic aspects, for example, reliance on\ninfinite precision to memorize target function values, which make these results\npotentially less meaningful. To address these issues, this work proposes a\nformal definition of statistically meaningful approximation which requires the\napproximating network to exhibit good statistical learnability. We present case\nstudies on statistically meaningful approximation for two classes of functions:\nboolean circuits and Turing machines. We show that overparameterized\nfeedforward neural nets can statistically meaningfully approximate boolean\ncircuits with sample complexity depending only polynomially on the circuit\nsize, not the size of the approximating network. In addition, we show that\ntransformers can statistically meaningfully approximate Turing machines with\ncomputation time bounded by $T$, requiring sample complexity polynomial in the\nalphabet size, state space size, and $\\log (T)$. Our analysis introduces new\ntools for generalization bounds that provide much tighter sample complexity\nguarantees than the typical VC-dimension or norm-based bounds, which may be of\nindependent interest.",
          "link": "http://arxiv.org/abs/2107.13163",
          "publishedOn": "2021-07-29T02:00:09.019Z",
          "wordCount": 619,
          "title": "Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers. (arXiv:2107.13163v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13200",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_B/0/1/0/all/0/1\">Bo Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_P/0/1/0/all/0/1\">Pengfei Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a> (Alzheimer&#x27;s Disease Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing), <a href=\"http://arxiv.org/find/eess/1/au:+Shen_S/0/1/0/all/0/1\">Shuwei Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_P/0/1/0/all/0/1\">Peng Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1\">Ronald X. Xu</a>",
          "description": "Early and accurate diagnosis of Alzheimer's disease (AD) and its prodromal\nperiod mild cognitive impairment (MCI) is essential for the delayed disease\nprogression and the improved quality of patients'life. The emerging\ncomputer-aided diagnostic methods that combine deep learning with structural\nmagnetic resonance imaging (sMRI) have achieved encouraging results, but some\nof them are limit of issues such as data leakage and unexplainable diagnosis.\nIn this research, we propose a novel end-to-end deep learning approach for\nautomated diagnosis of AD and localization of important brain regions related\nto the disease from sMRI data. This approach is based on a 2D single model\nstrategy and has the following differences from the current approaches: 1)\nConvolutional Neural Network (CNN) models of different structures and\ncapacities are evaluated systemically and the most suitable model is adopted\nfor AD diagnosis; 2) a data augmentation strategy named Two-stage Random\nRandAugment (TRRA) is proposed to alleviate the overfitting issue caused by\nlimited training data and to improve the classification performance in AD\ndiagnosis; 3) an explainable method of Grad-CAM++ is introduced to generate the\nvisually explainable heatmaps that localize and highlight the brain regions\nthat our model focuses on and to make our model more transparent. Our approach\nhas been evaluated on two publicly accessible datasets for two classification\ntasks of AD vs. cognitively normal (CN) and progressive MCI (pMCI) vs. stable\nMCI (sMCI). The experimental results indicate that our approach outperforms the\nstate-of-the-art approaches, including those using multi-model and 3D CNN\nmethods. The resultant localization heatmaps from our approach also highlight\nthe lateral ventricle and some disease-relevant regions of cortex, coincident\nwith the commonly affected regions during the development of AD.",
          "link": "http://arxiv.org/abs/2107.13200",
          "publishedOn": "2021-07-29T02:00:08.998Z",
          "wordCount": 760,
          "title": "An explainable two-dimensional single model deep learning approach for Alzheimer's disease diagnosis and brain atrophy localization. (arXiv:2107.13200v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daubechies_I/0/1/0/all/0/1\">Ingrid Daubechies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeVore_R/0/1/0/all/0/1\">Ronald DeVore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dym_N/0/1/0/all/0/1\">Nadav Dym</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faigenbaum_Golovin_S/0/1/0/all/0/1\">Shira Faigenbaum-Golovin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovalsky_S/0/1/0/all/0/1\">Shahar Z. Kovalsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kung-Ching Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Josiah Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrova_G/0/1/0/all/0/1\">Guergana Petrova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sober_B/0/1/0/all/0/1\">Barak Sober</a>",
          "description": "In the desire to quantify the success of neural networks in deep learning and\nother applications, there is a great interest in understanding which functions\nare efficiently approximated by the outputs of neural networks. By now, there\nexists a variety of results which show that a wide range of functions can be\napproximated with sometimes surprising accuracy by these outputs. For example,\nit is known that the set of functions that can be approximated with exponential\naccuracy (in terms of the number of parameters used) includes, on one hand,\nvery smooth functions such as polynomials and analytic functions (see e.g.\n\\cite{E,S,Y}) and, on the other hand, very rough functions such as the\nWeierstrass function (see e.g. \\cite{EPGB,DDFHP}), which is nowhere\ndifferentiable. In this paper, we add to the latter class of rough functions by\nshowing that it also includes refinable functions. Namely, we show that\nrefinable functions are approximated by the outputs of deep ReLU networks with\na fixed width and increasing depth with accuracy exponential in terms of their\nnumber of parameters. Our results apply to functions used in the standard\nconstruction of wavelets as well as to functions constructed via subdivision\nalgorithms in Computer Aided Geometric Design.",
          "link": "http://arxiv.org/abs/2107.13191",
          "publishedOn": "2021-07-29T02:00:08.989Z",
          "wordCount": 635,
          "title": "Neural Network Approximation of Refinable Functions. (arXiv:2107.13191v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bahadori_M/0/1/0/all/0/1\">Mohammad Taha Bahadori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchetgen_E/0/1/0/all/0/1\">Eric Tchetgen Tchetgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1\">David E. Heckerman</a>",
          "description": "We study the problem of observational causal inference with continuous\ntreatment. We focus on the challenge of estimating the causal response curve\nfor infrequently-observed treatment values. We design a new algorithm based on\nthe framework of entropy balancing which learns weights that directly maximize\ncausal inference accuracy using end-to-end optimization. Our weights can be\ncustomized for different datasets and causal inference algorithms. We propose a\nnew theory for consistency of entropy balancing for continuous treatments.\nUsing synthetic and real-world data, we show that our proposed algorithm\noutperforms the entropy balancing in terms of causal inference accuracy.",
          "link": "http://arxiv.org/abs/2107.13068",
          "publishedOn": "2021-07-29T02:00:08.982Z",
          "wordCount": 537,
          "title": "End-to-End Balancing for Causal Continuous Treatment-Effect Estimation. (arXiv:2107.13068v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundgaard_K/0/1/0/all/0/1\">Keld T. Lundgaard</a>",
          "description": "By leveraging large amounts of product data collected across hundreds of live\ne-commerce websites, we construct 1000 unique classification tasks that share\nsimilarly-structured input data, comprised of both text and images. These\nclassification tasks focus on learning the product hierarchy of different\ne-commerce websites, causing many of them to be correlated. Adopting a\nmulti-modal transformer model, we solve these tasks in unison using multi-task\nlearning (MTL). Extensive experiments are presented over an initial 100-task\ndataset to reveal best practices for \"large-scale MTL\" (i.e., MTL with more\nthan 100 tasks). From these experiments, a final, unified methodology is\nderived, which is composed of both best practices and new proposals such as\nDyPa, a simple heuristic for automatically allocating task-specific parameters\nto tasks that could benefit from extra capacity. Using our large-scale MTL\nmethodology, we successfully train a single model across all 1000 tasks in our\ndataset while using minimal task specific parameters, thereby showing that it\nis possible to extend several orders of magnitude beyond current efforts in\nMTL.",
          "link": "http://arxiv.org/abs/2107.13054",
          "publishedOn": "2021-07-29T02:00:08.975Z",
          "wordCount": 623,
          "title": "Exceeding the Limits of Visual-Linguistic Multi-Task Learning. (arXiv:2107.13054v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kesidis_G/0/1/0/all/0/1\">George Kesidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_D/0/1/0/all/0/1\">David J. Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergeron_M/0/1/0/all/0/1\">Maxime Bergeron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferguson_R/0/1/0/all/0/1\">Ryan Ferguson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_V/0/1/0/all/0/1\">Vladimir Lucic</a>",
          "description": "We describe a gradient-based method to discover local error maximizers of a\ndeep neural network (DNN) used for regression, assuming the availability of an\n\"oracle\" capable of providing real-valued supervision (a regression target) for\nsamples. For example, the oracle could be a numerical solver which,\noperationally, is much slower than the DNN. Given a discovered set of local\nerror maximizers, the DNN is either fine-tuned or retrained in the manner of\nactive learning.",
          "link": "http://arxiv.org/abs/2107.13124",
          "publishedOn": "2021-07-29T02:00:08.954Z",
          "wordCount": 511,
          "title": "Robust and Active Learning for Deep Neural Network Regression. (arXiv:2107.13124v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13067",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Ataei_M/0/1/0/all/0/1\">Mohammadmehdi Ataei</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pirmorad_E/0/1/0/all/0/1\">Erfan Pirmorad</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Costa_F/0/1/0/all/0/1\">Franco Costa</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Han_S/0/1/0/all/0/1\">Sejin Han</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Park_C/0/1/0/all/0/1\">Chul B Park</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bussmann_M/0/1/0/all/0/1\">Markus Bussmann</a>",
          "description": "Piecewise Linear Interface Construction (PLIC) is frequently used to\ngeometrically reconstruct fluid interfaces in Computational Fluid Dynamics\n(CFD) modeling of two-phase flows. PLIC reconstructs interfaces from a scalar\nfield that represents the volume fraction of each phase in each computational\ncell. Given the volume fraction and interface normal, the location of a linear\ninterface is uniquely defined. For a cubic computational cell (3D), the\nposition of the planar interface is determined by intersecting the cube with a\nplane, such that the volume of the resulting truncated polyhedron cell is equal\nto the volume fraction. Yet it is geometrically complex to find the exact\nposition of the plane, and it involves calculations that can be a computational\nbottleneck of many CFD models. However, while the forward problem of 3D PLIC is\nchallenging, the inverse problem, of finding the volume of the truncated\npolyhedron cell given a defined plane, is simple. In this work, we propose a\ndeep learning model for the solution to the forward problem of PLIC by only\nmaking use of its inverse problem. The proposed model is up to several orders\nof magnitude faster than traditional schemes, which significantly reduces the\ncomputational bottleneck of PLIC in CFD simulations.",
          "link": "http://arxiv.org/abs/2107.13067",
          "publishedOn": "2021-07-29T02:00:08.947Z",
          "wordCount": 651,
          "title": "A Deep Learning Algorithm for Piecewise Linear Interface Construction (PLIC). (arXiv:2107.13067v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dallmann_A/0/1/0/all/0/1\">Alexander Dallmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoller_D/0/1/0/all/0/1\">Daniel Zoller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hotho_A/0/1/0/all/0/1\">Andreas Hotho</a>",
          "description": "At the present time, sequential item recommendation models are compared by\ncalculating metrics on a small item subset (target set) to speed up\ncomputation. The target set contains the relevant item and a set of negative\nitems that are sampled from the full item set. Two well-known strategies to\nsample negative items are uniform random sampling and sampling by popularity to\nbetter approximate the item frequency distribution in the dataset. Most\nrecently published papers on sequential item recommendation rely on sampling by\npopularity to compare the evaluated models. However, recent work has already\nshown that an evaluation with uniform random sampling may not be consistent\nwith the full ranking, that is, the model ranking obtained by evaluating a\nmetric using the full item set as target set, which raises the question whether\nthe ranking obtained by sampling by popularity is equal to the full ranking. In\nthis work, we re-evaluate current state-of-the-art sequential recommender\nmodels from the point of view, whether these sampling strategies have an impact\non the final ranking of the models. We therefore train four recently proposed\nsequential recommendation models on five widely known datasets. For each\ndataset and model, we employ three evaluation strategies. First, we compute the\nfull model ranking. Then we evaluate all models on a target set sampled by the\ntwo different sampling strategies, uniform random sampling and sampling by\npopularity with the commonly used target set size of 100, compute the model\nranking for each strategy and compare them with each other. Additionally, we\nvary the size of the sampled target set. Overall, we find that both sampling\nstrategies can produce inconsistent rankings compared with the full ranking of\nthe models. Furthermore, both sampling by popularity and uniform random\nsampling do not consistently produce the same ranking ...",
          "link": "http://arxiv.org/abs/2107.13045",
          "publishedOn": "2021-07-29T02:00:08.940Z",
          "wordCount": 740,
          "title": "A Case Study on Sampling Strategies for Evaluating Neural Sequential Item Recommendation Models. (arXiv:2107.13045v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Walsh_R/0/1/0/all/0/1\">Reece Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelpakey_M/0/1/0/all/0/1\">Mohamed H. Abdelpakey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1\">Mohamed S. Shehata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1\">Mostafa M.Mohamed</a>",
          "description": "Classifying and analyzing human cells is a lengthy procedure, often involving\na trained professional. In an attempt to expedite this process, an active area\nof research involves automating cell classification through use of deep\nlearning-based techniques. In practice, a large amount of data is required to\naccurately train these deep learning models. However, due to the sparse human\ncell datasets currently available, the performance of these models is typically\nlow. This study investigates the feasibility of using few-shot learning-based\ntechniques to mitigate the data requirements for accurate training. The study\nis comprised of three parts: First, current state-of-the-art few-shot learning\ntechniques are evaluated on human cell classification. The selected techniques\nare trained on a non-medical dataset and then tested on two out-of-domain,\nhuman cell datasets. The results indicate that, overall, the test accuracy of\nstate-of-the-art techniques decreased by at least 30% when transitioning from a\nnon-medical dataset to a medical dataset. Second, this study evaluates the\npotential benefits, if any, to varying the backbone architecture and training\nschemes in current state-of-the-art few-shot learning techniques when used in\nhuman cell classification. Even with these variations, the overall test\naccuracy decreased from 88.66% on non-medical datasets to 44.13% at best on the\nmedical datasets. Third, this study presents future directions for using\nfew-shot learning in human cell classification. In general, few-shot learning\nin its current state performs poorly on human cell classification. The study\nproves that attempts to modify existing network architectures are not effective\nand concludes that future research effort should be focused on improving\nrobustness towards out-of-domain testing using optimization-based or\nself-supervised few-shot learning techniques.",
          "link": "http://arxiv.org/abs/2107.13093",
          "publishedOn": "2021-07-29T02:00:08.932Z",
          "wordCount": 720,
          "title": "Automated Human Cell Classification in Sparse Datasets using Few-Shot Learning. (arXiv:2107.13093v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Onoufriou_G/0/1/0/all/0/1\">George Onoufriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayfield_P/0/1/0/all/0/1\">Paul Mayfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leontidis_G/0/1/0/all/0/1\">Georgios Leontidis</a>",
          "description": "Fully Homomorphic Encryption (FHE) is a relatively recent advancement in the\nfield of privacy-preserving technologies. FHE allows for the arbitrary depth\ncomputation of both addition and multiplication, and thus the application of\nabelian/polynomial equations, like those found in deep learning algorithms.\nThis project investigates, derives, and proves how FHE with deep learning can\nbe used at scale, with relatively low time complexity, the problems that such a\nsystem incurs, and mitigations/solutions for such problems. In addition, we\ndiscuss how this could have an impact on the future of data privacy and how it\ncan enable data sharing across various actors in the agri-food supply chain,\nhence allowing the development of machine learning-based systems. Finally, we\nfind that although FHE incurs a high spatial complexity cost, the time\ncomplexity is within expected reasonable bounds, while allowing for absolutely\nprivate predictions to be made, in our case for milk yield prediction.",
          "link": "http://arxiv.org/abs/2107.12997",
          "publishedOn": "2021-07-29T02:00:08.920Z",
          "wordCount": 585,
          "title": "Fully Homomorphically Encrypted Deep Learning as a Service. (arXiv:2107.12997v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Timothy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novak_R/0/1/0/all/0/1\">Roman Novak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Lechao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaehoon Lee</a>",
          "description": "The effectiveness of machine learning algorithms arises from being able to\nextract useful features from large amounts of data. As model and dataset sizes\nincrease, dataset distillation methods that compress large datasets into\nsignificantly smaller yet highly performant ones will become valuable in terms\nof training efficiency and useful feature extraction. To that end, we apply a\nnovel distributed kernel based meta-learning framework to achieve\nstate-of-the-art results for dataset distillation using infinitely wide\nconvolutional neural networks. For instance, using only 10 datapoints (0.02% of\noriginal dataset), we obtain over 64% test accuracy on CIFAR-10 image\nclassification task, a dramatic improvement over the previous best test\naccuracy of 40%. Our state-of-the-art results extend across many other settings\nfor MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and SVHN. Furthermore, we\nperform some preliminary analyses of our distilled datasets to shed light on\nhow they differ from naturally occurring data.",
          "link": "http://arxiv.org/abs/2107.13034",
          "publishedOn": "2021-07-29T02:00:08.867Z",
          "wordCount": 580,
          "title": "Dataset Distillation with Infinitely Wide Convolutional Networks. (arXiv:2107.13034v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.01791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uddin_A/0/1/0/all/0/1\">A. F. M. Shahab Uddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monira_M/0/1/0/all/0/1\">Mst. Sirazam Monira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1\">Wheemyung Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1\">TaeChoong Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Sung-Ho Bae</a>",
          "description": "Advanced data augmentation strategies have widely been studied to improve the\ngeneralization ability of deep learning models. Regional dropout is one of the\npopular solutions that guides the model to focus on less discriminative parts\nby randomly removing image regions, resulting in improved regularization.\nHowever, such information removal is undesirable. On the other hand, recent\nstrategies suggest to randomly cut and mix patches and their labels among\ntraining images, to enjoy the advantages of regional dropout without having any\npointless pixel in the augmented images. We argue that such random selection\nstrategies of the patches may not necessarily represent sufficient information\nabout the corresponding object and thereby mixing the labels according to that\nuninformative patch enables the model to learn unexpected feature\nrepresentation. Therefore, we propose SaliencyMix that carefully selects a\nrepresentative image patch with the help of a saliency map and mixes this\nindicative patch with the target image, thus leading the model to learn more\nappropriate feature representation. SaliencyMix achieves the best known top-1\nerror of 21.26% and 20.09% for ResNet-50 and ResNet-101 architectures on\nImageNet classification, respectively, and also improves the model robustness\nagainst adversarial perturbations. Furthermore, models that are trained with\nSaliencyMix help to improve the object detection performance. Source code is\navailable at https://github.com/SaliencyMix/SaliencyMix.",
          "link": "http://arxiv.org/abs/2006.01791",
          "publishedOn": "2021-07-28T02:02:34.744Z",
          "wordCount": 708,
          "title": "SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization. (arXiv:2006.01791v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.14220",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mostaani_A/0/1/0/all/0/1\">Arsham Mostaani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Thang X. Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzinotas_S/0/1/0/all/0/1\">Symeon Chatzinotas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ottersten_B/0/1/0/all/0/1\">Bj&#xf6;rn Ottersten</a>",
          "description": "A collaborative task is assigned to a multiagent system (MAS) in which agents\nare allowed to communicate. The MAS runs over an underlying Markov decision\nprocess and its task is to maximize the averaged sum of discounted one-stage\nrewards. Although knowing the global state of the environment is necessary for\nthe optimal action selection of the MAS, agents are limited to individual\nobservations. The inter-agent communication can tackle the issue of local\nobservability, however, the limited rate of the inter-agent communication\nprevents the agent from acquiring the precise global state information. To\novercome this challenge, agents need to communicate their observations in a\ncompact way such that the MAS compromises the minimum possible sum of rewards.\nWe show that this problem is equivalent to a form of rate-distortion problem\nwhich we call the task-based information compression. We introduce a scheme for\ntask-based information compression titled State aggregation for information\ncompression (SAIC), for which a state aggregation algorithm is analytically\ndesigned. The SAIC is shown to be capable of achieving near-optimal performance\nin terms of the achieved sum of discounted rewards. The proposed algorithm is\napplied to a rendezvous problem and its performance is compared with several\nbenchmarks. Numerical experiments confirm the superiority of the proposed\nalgorithm.",
          "link": "http://arxiv.org/abs/2005.14220",
          "publishedOn": "2021-07-28T02:02:34.699Z",
          "wordCount": 696,
          "title": "Task-Based Information Compression for Multi-Agent Communication Problems with Channel Rate Constraints. (arXiv:2005.14220v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Secci_F/0/1/0/all/0/1\">Francesco Secci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceccarelli_A/0/1/0/all/0/1\">Andrea Ceccarelli</a>",
          "description": "RGB cameras are one of the most relevant sensors for autonomous driving\napplications. It is undeniable that failures of vehicle cameras may compromise\nthe autonomous driving task, possibly leading to unsafe behaviors when images\nthat are subsequently processed by the driving system are altered. To support\nthe definition of safe and robust vehicle architectures and intelligent\nsystems, in this paper we define the failure modes of a vehicle camera,\ntogether with an analysis of effects and known mitigations. Further, we build a\nsoftware library for the generation of the corresponding failed images and we\nfeed them to six object detectors for mono and stereo cameras and to the\nself-driving agent of an autonomous driving simulator. The resulting\nmisbehaviors with respect to operating with clean images allow a better\nunderstanding of failures effects and the related safety risks in image-based\napplications.",
          "link": "http://arxiv.org/abs/2008.05938",
          "publishedOn": "2021-07-28T02:02:34.652Z",
          "wordCount": 611,
          "title": "RGB cameras failures and their effects in autonomous driving applications. (arXiv:2008.05938v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12975",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Severin_B/0/1/0/all/0/1\">B. Severin</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Lennon_D/0/1/0/all/0/1\">D. T. Lennon</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Camenzind_L/0/1/0/all/0/1\">L. C. Camenzind</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Vigneau_F/0/1/0/all/0/1\">F. Vigneau</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Fedele_F/0/1/0/all/0/1\">F. Fedele</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Jirovec_D/0/1/0/all/0/1\">D. Jirovec</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Ballabio_A/0/1/0/all/0/1\">A. Ballabio</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Chrastina_D/0/1/0/all/0/1\">D. Chrastina</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Isella_G/0/1/0/all/0/1\">G. Isella</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kruijf_M/0/1/0/all/0/1\">M. de Kruijf</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Carballido_M/0/1/0/all/0/1\">M. J. Carballido</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Svab_S/0/1/0/all/0/1\">S. Svab</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kuhlmann_A/0/1/0/all/0/1\">A. V. Kuhlmann</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Braakman_F/0/1/0/all/0/1\">F. R. Braakman</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Geyer_S/0/1/0/all/0/1\">S. Geyer</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Froning_F/0/1/0/all/0/1\">F. N. M. Froning</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Moon_H/0/1/0/all/0/1\">H. Moon</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Osborne_M/0/1/0/all/0/1\">M. A. Osborne</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Sejdinovic_D/0/1/0/all/0/1\">D. Sejdinovic</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Katsaros_G/0/1/0/all/0/1\">G. Katsaros</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zumbuhl_D/0/1/0/all/0/1\">D. M. Zumb&#xfc;hl</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Briggs_G/0/1/0/all/0/1\">G. A. D. Briggs</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Ares_N/0/1/0/all/0/1\">N. Ares</a>",
          "description": "The potential of Si and SiGe-based devices for the scaling of quantum\ncircuits is tainted by device variability. Each device needs to be tuned to\noperation conditions. We give a key step towards tackling this variability with\nan algorithm that, without modification, is capable of tuning a 4-gate Si\nFinFET, a 5-gate GeSi nanowire and a 7-gate SiGe heterostructure double quantum\ndot device from scratch. We achieve tuning times of 30, 10, and 92 minutes,\nrespectively. The algorithm also provides insight into the parameter space\nlandscape for each of these devices. These results show that overarching\nsolutions for the tuning of quantum devices are enabled by machine learning.",
          "link": "http://arxiv.org/abs/2107.12975",
          "publishedOn": "2021-07-28T02:02:34.625Z",
          "wordCount": 609,
          "title": "Cross-architecture Tuning of Silicon and SiGe-based Quantum Devices Using Machine Learning. (arXiv:2107.12975v1 [cond-mat.mes-hall])"
        },
        {
          "id": "http://arxiv.org/abs/2005.07473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silveira_B/0/1/0/all/0/1\">B&#xe1;rbara Silveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_H/0/1/0/all/0/1\">Henrique S. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murai_F/0/1/0/all/0/1\">Fabricio Murai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1\">Ana Paula Couto da Silva</a>",
          "description": "In recent years, Online Social Networks have become an important medium for\npeople who suffer from mental disorders to share moments of hardship, and\nreceive emotional and informational support. In this work, we analyze how\ndiscussions in Reddit communities related to mental disorders can help improve\nthe health conditions of their users. Using the emotional tone of users'\nwriting as a proxy for emotional state, we uncover relationships between user\ninteractions and state changes. First, we observe that authors of negative\nposts often write rosier comments after engaging in discussions, indicating\nthat users' emotional state can improve due to social support. Second, we build\nmodels based on SOTA text embedding techniques and RNNs to predict shifts in\nemotional tone. This differs from most of related work, which focuses primarily\non detecting mental disorders from user activity. We demonstrate the\nfeasibility of accurately predicting the users' reactions to the interactions\nexperienced in these platforms, and present some examples which illustrate that\nthe models are correctly capturing the effects of comments on the author's\nemotional tone. Our models hold promising implications for interventions to\nprovide support for people struggling with mental illnesses.",
          "link": "http://arxiv.org/abs/2005.07473",
          "publishedOn": "2021-07-28T02:02:34.616Z",
          "wordCount": 698,
          "title": "Predicting User Emotional Tone in Mental Disorder Online Communities. (arXiv:2005.07473v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12416",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jing_G/0/1/0/all/0/1\">Gangshan Jing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_H/0/1/0/all/0/1\">He Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+George_J/0/1/0/all/0/1\">Jemin George</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chakrabortty_A/0/1/0/all/0/1\">Aranya Chakrabortty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1\">Piyush K. Sharma</a>",
          "description": "Recently introduced distributed zeroth-order optimization (ZOO) algorithms\nhave shown their utility in distributed reinforcement learning (RL).\nUnfortunately, in the gradient estimation process, almost all of them require\nrandom samples with the same dimension as the global variable and/or require\nevaluation of the global cost function, which may induce high estimation\nvariance for large-scale networks. In this paper, we propose a novel\ndistributed zeroth-order algorithm by leveraging the network structure inherent\nin the optimization objective, which allows each agent to estimate its local\ngradient by local cost evaluation independently, without use of any consensus\nprotocol. The proposed algorithm exhibits an asynchronous update scheme, and is\ndesigned for stochastic non-convex optimization with a possibly non-convex\nfeasible domain based on the block coordinate descent method. The algorithm is\nlater employed as a distributed model-free RL algorithm for distributed linear\nquadratic regulator design, where a learning graph is designed to describe the\nrequired interaction relationship among agents in distributed learning. We\nprovide an empirical validation of the proposed algorithm to benchmark its\nperformance on convergence rate and variance against a centralized ZOO\nalgorithm.",
          "link": "http://arxiv.org/abs/2107.12416",
          "publishedOn": "2021-07-28T02:02:34.579Z",
          "wordCount": 641,
          "title": "Asynchronous Distributed Reinforcement Learning for LQR Control via Zeroth-Order Block Coordinate Descent. (arXiv:2107.12416v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Claudia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veitch_V/0/1/0/all/0/1\">Victor Veitch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blei_D/0/1/0/all/0/1\">David Blei</a>",
          "description": "The defining challenge for causal inference from observational data is the\npresence of `confounders', covariates that affect both treatment assignment and\nthe outcome. To address this challenge, practitioners collect and adjust for\nthe covariates, hoping that they adequately correct for confounding. However,\nincluding every observed covariate in the adjustment runs the risk of including\n`bad controls', variables that induce bias when they are conditioned on. The\nproblem is that we do not always know which variables in the covariate set are\nsafe to adjust for and which are not. To address this problem, we develop\nNearly Invariant Causal Estimation (NICE). NICE uses invariant risk\nminimization (IRM) [Arj19] to learn a representation of the covariates that,\nunder some assumptions, strips out bad controls but preserves sufficient\ninformation to adjust for confounding. Adjusting for the learned\nrepresentation, rather than the covariates themselves, avoids the induced bias\nand provides valid causal inferences. We evaluate NICE on both synthetic and\nsemi-synthetic data. When the covariates contain unknown collider variables and\nother bad controls, NICE performs better than adjusting for all the covariates.",
          "link": "http://arxiv.org/abs/2011.12379",
          "publishedOn": "2021-07-28T02:02:34.572Z",
          "wordCount": 637,
          "title": "Invariant Representation Learning for Treatment Effect Estimation. (arXiv:2011.12379v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barbosa_W/0/1/0/all/0/1\">Wendson A. S. Barbosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffith_A/0/1/0/all/0/1\">Aaron Griffith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rowlands_G/0/1/0/all/0/1\">Graham E. Rowlands</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Govia_L/0/1/0/all/0/1\">Luke C. G. Govia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeill_G/0/1/0/all/0/1\">Guilhem J. Ribeill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh-Hai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohki_T/0/1/0/all/0/1\">Thomas A. Ohki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gauthier_D/0/1/0/all/0/1\">Daniel J. Gauthier</a>",
          "description": "We demonstrate that matching the symmetry properties of a reservoir computer\n(RC) to the data being processed dramatically increases its processing power.\nWe apply our method to the parity task, a challenging benchmark problem that\nhighlights inversion and permutation symmetries, and to a chaotic system\ninference task that presents an inversion symmetry rule. For the parity task,\nour symmetry-aware RC obtains zero error using an exponentially reduced neural\nnetwork and training data, greatly speeding up the time to result and\noutperforming hand crafted artificial neural networks. When both symmetries are\nrespected, we find that the network size $N$ necessary to obtain zero error for\n50 different RC instances scales linearly with the parity-order $n$. Moreover,\nsome symmetry-aware RC instances perform a zero error classification with only\n$N=1$ for $n\\leq7$. Furthermore, we show that a symmetry-aware RC only needs a\ntraining data set with size on the order of $(n+n/2)$ to obtain such\nperformance, an exponential reduction in comparison to a regular RC which\nrequires a training data set with size on the order of $n2^n$ to contain all\n$2^n$ possible $n-$bit-long sequences. For the inference task, we show that a\nsymmetry-aware RC presents a normalized root-mean-square error three\norders-of-magnitude smaller than regular RCs. For both tasks, our RC approach\nrespects the symmetries by adjusting only the input and the output layers, and\nnot by problem-based modifications to the neural network. We anticipate that\ngeneralizations of our procedure can be applied in information processing for\nproblems with known symmetries.",
          "link": "http://arxiv.org/abs/2102.00310",
          "publishedOn": "2021-07-28T02:02:34.553Z",
          "wordCount": 739,
          "title": "Symmetry-Aware Reservoir Computing. (arXiv:2102.00310v3 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.05759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Gil_D/0/1/0/all/0/1\">Diego Garc&#xed;a-Gil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_S/0/1/0/all/0/1\">Salvador Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_N/0/1/0/all/0/1\">Ning Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrera_F/0/1/0/all/0/1\">Francisco Herrera</a>",
          "description": "Differences in data size per class, also known as imbalanced data\ndistribution, have become a common problem affecting data quality. Big Data\nscenarios pose a new challenge to traditional imbalanced classification\nalgorithms, since they are not prepared to work with such amount of data. Split\ndata strategies and lack of data in the minority class due to the use of\nMapReduce paradigm have posed new challenges for tackling the imbalance between\nclasses in Big Data scenarios. Ensembles have shown to be able to successfully\naddress imbalanced data problems. Smart Data refers to data of enough quality\nto achieve high performance models. The combination of ensembles and Smart\nData, achieved through Big Data preprocessing, should be a great synergy. In\nthis paper, we propose a novel methodology based on Decision Trees Ensemble\nwith Smart Data for addressing the imbalanced classification problem in Big\nData domains, namely DeTE_SD methodology. This methodology is based on the\nlearning of different decision trees using distributed quality data for the\nensemble process. This quality data is achieved by fusing Random\nDiscretization, Principal Components Analysis and clustering-based Random\nOversampling for obtaining different Smart Data versions of the original data.\nExperiments carried out in 21 binary adapted datasets have shown that our\nmethodology outperforms Random Forest.",
          "link": "http://arxiv.org/abs/2001.05759",
          "publishedOn": "2021-07-28T02:02:34.537Z",
          "wordCount": 691,
          "title": "A Methodology guided by Decision Trees Ensemble and Smart Data for Imbalanced Big Data. (arXiv:2001.05759v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ruohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yuqing Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ricky Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1\">Matthew Guzdial</a>",
          "description": "Procedural content generation via machine learning (PCGML) is the process of\nprocedurally generating game content using models trained on existing game\ncontent. PCGML methods can struggle to capture the true variance present in\nunderlying data with a single model. In this paper, we investigated the use of\nensembles of Markov chains for procedurally generating \\emph{Mega Man} levels.\nWe conduct an initial investigation of our approach and evaluate it on measures\nof playability and stylistic similarity in comparison to a non-ensemble,\nexisting Markov chain approach.",
          "link": "http://arxiv.org/abs/2107.12524",
          "publishedOn": "2021-07-28T02:02:34.523Z",
          "wordCount": 540,
          "title": "Ensemble Learning For Mega Man Level Generation. (arXiv:2107.12524v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>",
          "description": "Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.",
          "link": "http://arxiv.org/abs/2106.13948",
          "publishedOn": "2021-07-28T02:02:34.503Z",
          "wordCount": 654,
          "title": "Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12783",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Khurana_D/0/1/0/all/0/1\">Drona Khurana</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ravichandran_S/0/1/0/all/0/1\">Srinivasan Ravichandran</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jain_S/0/1/0/all/0/1\">Sparsh Jain</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Edakunni_N/0/1/0/all/0/1\">Narayanan Unny Edakunni</a>",
          "description": "A plug-in algorithm to estimate Bayes Optimal Classifiers for fairness-aware\nbinary classification has been proposed in (Menon & Williamson, 2018). However,\nthe statistical efficacy of their approach has not been established. We prove\nthat the plug-in algorithm is statistically consistent. We also derive finite\nsample guarantees associated with learning the Bayes Optimal Classifiers via\nthe plug-in algorithm. Finally, we propose a protocol that modifies the plug-in\napproach, so as to simultaneously guarantee fairness and differential privacy\nwith respect to a binary feature deemed sensitive.",
          "link": "http://arxiv.org/abs/2107.12783",
          "publishedOn": "2021-07-28T02:02:34.496Z",
          "wordCount": 532,
          "title": "Statistical Guarantees for Fairness Aware Plug-In Algorithms. (arXiv:2107.12783v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12102",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cartis_C/0/1/0/all/0/1\">Coralia Cartis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Massart_E/0/1/0/all/0/1\">Estelle Massart</a>, <a href=\"http://arxiv.org/find/math/1/au:+Otemissov_A/0/1/0/all/0/1\">Adilet Otemissov</a>",
          "description": "We propose a random-subspace algorithmic framework for global optimization of\nLipschitz-continuous objectives, and analyse its convergence using novel tools\nfrom conic integral geometry. X-REGO randomly projects, in a sequential or\nsimultaneous manner, the high-dimensional original problem into low-dimensional\nsubproblems that can then be solved with any global, or even local,\noptimization solver. We estimate the probability that the randomly-embedded\nsubproblem shares (approximately) the same global optimum as the original\nproblem. This success probability is then used to show convergence of X-REGO to\nan approximate global solution of the original problem, under weak assumptions\non the problem (having a strictly feasible global solution) and on the solver\n(guaranteed to find an approximate global solution of the reduced problem with\nsufficiently high probability). In the particular case of unconstrained\nobjectives with low effective dimension, that only vary over a low-dimensional\nsubspace, we propose an X-REGO variant that explores random subspaces of\nincreasing dimension until finding the effective dimension of the problem,\nleading to X-REGO globally converging after a finite number of embeddings,\nproportional to the effective dimension. We show numerically that this variant\nefficiently finds both the effective dimension and an approximate global\nminimizer of the original problem.",
          "link": "http://arxiv.org/abs/2107.12102",
          "publishedOn": "2021-07-28T02:02:34.490Z",
          "wordCount": 631,
          "title": "Global optimization using random embeddings. (arXiv:2107.12102v1 [math.OC] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shifeng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>",
          "description": "Cross-speaker style transfer is crucial to the applications of multi-style\nand expressive speech synthesis at scale. It does not require the target\nspeakers to be experts in expressing all styles and to collect corresponding\nrecordings for model training. However, the performances of existing style\ntransfer methods are still far behind real application needs. The root causes\nare mainly twofold. Firstly, the style embedding extracted from single\nreference speech can hardly provide fine-grained and appropriate prosody\ninformation for arbitrary text to synthesize. Secondly, in these models the\ncontent/text, prosody, and speaker timbre are usually highly entangled, it's\ntherefore not realistic to expect a satisfied result when freely combining\nthese components, such as to transfer speaking style between speakers. In this\npaper, we propose a cross-speaker style transfer text-to-speech (TTS) model\nwith explicit prosody bottleneck. The prosody bottleneck builds up the kernels\naccounting for speaking style robustly, and disentangles the prosody from\ncontent and speaker timbre, therefore guarantees high quality cross-speaker\nstyle transfer. Evaluation result shows the proposed method even achieves\non-par performance with source speaker's speaker-dependent (SD) model in\nobjective measurement of prosody, and significantly outperforms the cycle\nconsistency and GMVAE-based baselines in objective and subjective evaluations.",
          "link": "http://arxiv.org/abs/2107.12562",
          "publishedOn": "2021-07-28T02:02:34.483Z",
          "wordCount": 640,
          "title": "Cross-speaker Style Transfer with Prosody Bottleneck in Neural Speech Synthesis. (arXiv:2107.12562v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuzborskij_I/0/1/0/all/0/1\">Ilja Kuzborskij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1\">Csaba Szepesv&#xe1;ri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivasplata_O/0/1/0/all/0/1\">Omar Rivasplata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rannen_Triki_A/0/1/0/all/0/1\">Amal Rannen-Triki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>",
          "description": "Empirically it has been observed that the performance of deep neural networks\nsteadily improves as we increase model size, contradicting the classical view\non overfitting and generalization. Recently, the double descent phenomena has\nbeen proposed to reconcile this observation with theory, suggesting that the\ntest error has a second descent when the model becomes sufficiently\noverparameterized, as the model size itself acts as an implicit regularizer. In\nthis paper we add to the growing body of work in this space, providing a\ncareful study of learning dynamics as a function of model size for the least\nsquares scenario. We show an excess risk bound for the gradient descent\nsolution of the least squares objective. The bound depends on the smallest\nnon-zero eigenvalue of the covariance matrix of the input features, via a\nfunctional form that has the double descent behavior. This gives a new\nperspective on the double descent curves reported in the literature. Our\nanalysis of the excess risk allows to decouple the effect of optimization and\ngeneralization error. In particular, we find that in case of noiseless\nregression, double descent is explained solely by optimization-related\nquantities, which was missed in studies focusing on the Moore-Penrose\npseudoinverse solution. We believe that our derivation provides an alternative\nview compared to existing work, shedding some light on a possible cause of this\nphenomena, at least in the considered least squares setting. We empirically\nexplore if our predictions hold for neural networks, in particular whether the\ncovariance of intermediary hidden activations has a similar behavior as the one\npredicted by our derivations.",
          "link": "http://arxiv.org/abs/2107.12685",
          "publishedOn": "2021-07-28T02:02:34.475Z",
          "wordCount": 711,
          "title": "On the Role of Optimization in Double Descent: A Least Squares Study. (arXiv:2107.12685v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.04896",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Sakthivadivel_D/0/1/0/all/0/1\">Dalton A R Sakthivadivel</a>",
          "description": "We investigate how the activation function can be used to describe neural\nfiring in an abstract way, and in turn, why it works well in artificial neural\nnetworks. We discuss how a spike in a biological neurone belongs to a\nparticular universality class of phase transitions in statistical physics. We\nthen show that the artificial neurone is, mathematically, a mean field model of\nbiological neural membrane dynamics, which arises from modelling spiking as a\nphase transition. This allows us to treat selective neural firing in an\nabstract way, and formalise the role of the activation function in perceptron\nlearning. The resultant statistical physical model allows us to recover the\nexpressions for some known activation functions as various special cases. Along\nwith deriving this model and specifying the analogous neural case, we analyse\nthe phase transition to understand the physics of neural network learning.\nTogether, it is shown that there is not only a biological meaning, but a\nphysical justification, for the emergence and performance of typical activation\nfunctions; implications for neural learning and inference are also discussed.",
          "link": "http://arxiv.org/abs/2102.04896",
          "publishedOn": "2021-07-28T02:02:34.452Z",
          "wordCount": 649,
          "title": "Formalising the Use of the Activation Function in Neural Inference. (arXiv:2102.04896v2 [q-bio.NC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Halle_A/0/1/0/all/0/1\">Alex Halle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campanile_L/0/1/0/all/0/1\">L. Flavio Campanile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasse_A/0/1/0/all/0/1\">Alexander Hasse</a>",
          "description": "Topology optimization is widely used by engineers during the initial product\ndevelopment process to get a first possible geometry design. The\nstate-of-the-art is the iterative calculation, which requires both time and\ncomputational power. Some newly developed methods use artificial intelligence\nto accelerate the topology optimization. These require conventionally\npre-optimized data and therefore are dependent on the quality and number of\navailable data. This paper proposes an AI-assisted design method for topology\noptimization, which does not require pre-optimized data. The designs are\nprovided by an artificial neural network, the predictor, on the basis of\nboundary conditions and degree of filling (the volume percentage filled by\nmaterial) as input data. In the training phase, geometries generated on the\nbasis of random input data are evaluated with respect to given criteria. The\nresults of those evaluations flow into an objective function which is minimized\nby adapting the predictor's parameters. After the training is completed, the\npresented AI-assisted design procedure supplies geometries which are similar to\nthe ones generated by conventional topology optimizers, but requires a small\nfraction of the computational effort required by those algorithms. We\nanticipate our paper to be a starting point for AI-based methods that requires\ndata, that is hard to compute or not available.",
          "link": "http://arxiv.org/abs/2012.06384",
          "publishedOn": "2021-07-28T02:02:34.444Z",
          "wordCount": 664,
          "title": "An AI-Assisted Design Method for Topology Optimization Without Pre-Optimized Training Data. (arXiv:2012.06384v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yejiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Weiming Xiang</a>",
          "description": "In this paper, a robust optimization framework is developed to train shallow\nneural networks based on reachability analysis of neural networks. To\ncharacterize noises of input data, the input training data is disturbed in the\ndescription of interval sets. Interval-based reachability analysis is then\nperformed for the hidden layer. With the reachability analysis results, a\nrobust optimization training method is developed in the framework of robust\nleast-square problems. Then, the developed robust least-square problem is\nrelaxed to a semidefinite programming problem. It has been shown that the\ndeveloped robust learning method can provide better robustness against\nperturbations at the price of loss of training accuracy to some extent. At\nlast, the proposed method is evaluated on a robot arm model learning example.",
          "link": "http://arxiv.org/abs/2107.12801",
          "publishedOn": "2021-07-28T02:02:34.437Z",
          "wordCount": 560,
          "title": "Robust Optimization Framework for Training Shallow Neural Networks Using Reachability Method. (arXiv:2107.12801v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bernini_N/0/1/0/all/0/1\">Nicola Bernini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bessa_M/0/1/0/all/0/1\">Mikhail Bessa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delmas_R/0/1/0/all/0/1\">R&#xe9;mi Delmas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gold_A/0/1/0/all/0/1\">Arthur Gold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goubault_E/0/1/0/all/0/1\">Eric Goubault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pennec_R/0/1/0/all/0/1\">Romain Pennec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putot_S/0/1/0/all/0/1\">Sylvie Putot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sillion_F/0/1/0/all/0/1\">Fran&#xe7;ois Sillion</a>",
          "description": "We explore the reinforcement learning approach to designing controllers by\nextensively discussing the case of a quadcopter attitude controller. We provide\nall details allowing to reproduce our approach, starting with a model of the\ndynamics of a crazyflie 2.0 under various nominal and non-nominal conditions,\nincluding partial motor failures and wind gusts. We develop a robust form of a\nsignal temporal logic to quantitatively evaluate the vehicle's behavior and\nmeasure the performance of controllers. The paper thoroughly describes the\nchoices in training algorithms, neural net architecture, hyperparameters,\nobservation space in view of the different performance metrics we have\nintroduced. We discuss the robustness of the obtained controllers, both to\npartial loss of power for one rotor and to wind gusts and finish by drawing\nconclusions on practical controller design by reinforcement learning.",
          "link": "http://arxiv.org/abs/2107.12942",
          "publishedOn": "2021-07-28T02:02:34.428Z",
          "wordCount": 595,
          "title": "Reinforcement Learning with Formal Performance Metrics for Quadcopter Attitude Control under Non-nominal Contexts. (arXiv:2107.12942v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12421",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Talgorn_B/0/1/0/all/0/1\">Bastien Talgorn</a>, <a href=\"http://arxiv.org/find/math/1/au:+Alarie_S/0/1/0/all/0/1\">St&#xe9;phane Alarie</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kokkolaras_M/0/1/0/all/0/1\">Michael Kokkolaras</a>",
          "description": "We consider computationally expensive blackbox optimization problems and\npresent a method that employs surrogate models and concurrent computing at the\nsearch step of the mesh adaptive direct search (MADS) algorithm. Specifically,\nwe solve a surrogate optimization problem using locally weighted scatterplot\nsmoothing (LOWESS) models to find promising candidate points to be evaluated by\nthe blackboxes. We consider several methods for selecting promising points from\na large number of points. We conduct numerical experiments to assess the\nperformance of the modified MADS algorithm with respect to available CPU\nresources by means of five engineering design problems.",
          "link": "http://arxiv.org/abs/2107.12421",
          "publishedOn": "2021-07-28T02:02:34.421Z",
          "wordCount": 528,
          "title": "Parallel Surrogate-assisted Optimization Using Mesh Adaptive Direct Search. (arXiv:2107.12421v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2004.06230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jiayu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1\">Emma Brunskill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Weiwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_S/0/1/0/all/0/1\">Susan Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1\">Finale Doshi-Velez</a>",
          "description": "Contextual bandits often provide simple and effective personalization in\ndecision making problems, making them popular tools to deliver personalized\ninterventions in mobile health as well as other health applications. However,\nwhen bandits are deployed in the context of a scientific study -- e.g. a\nclinical trial to test if a mobile health intervention is effective -- the aim\nis not only to personalize for an individual, but also to determine, with\nsufficient statistical power, whether or not the system's intervention is\neffective. It is essential to assess the effectiveness of the intervention\nbefore broader deployment for better resource allocation. The two objectives\nare often deployed under different model assumptions, making it hard to\ndetermine how achieving the personalization and statistical power affect each\nother. In this work, we develop general meta-algorithms to modify existing\nalgorithms such that sufficient power is guaranteed while still improving each\nuser's well-being. We also demonstrate that our meta-algorithms are robust to\nvarious model mis-specifications possibly appearing in statistical studies,\nthus providing a valuable tool to study designers.",
          "link": "http://arxiv.org/abs/2004.06230",
          "publishedOn": "2021-07-28T02:02:34.395Z",
          "wordCount": 651,
          "title": "Power Constrained Bandits. (arXiv:2004.06230v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gothankar_R/0/1/0/all/0/1\">Ruchira Gothankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troia_F/0/1/0/all/0/1\">Fabio Di Troia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1\">Mark Stamp</a>",
          "description": "YouTube videos often include captivating descriptions and intriguing\nthumbnails designed to increase the number of views, and thereby increase the\nrevenue for the person who posted the video. This creates an incentive for\npeople to post clickbait videos, in which the content might deviate\nsignificantly from the title, description, or thumbnail. In effect, users are\ntricked into clicking on clickbait videos. In this research, we consider the\nchallenging problem of detecting clickbait YouTube videos. We experiment with\nmultiple state-of-the-art machine learning techniques using a variety of\ntextual features.",
          "link": "http://arxiv.org/abs/2107.12791",
          "publishedOn": "2021-07-28T02:02:34.388Z",
          "wordCount": 517,
          "title": "Clickbait Detection in YouTube Videos. (arXiv:2107.12791v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huaimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haibo Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy M. Hospedales</a>",
          "description": "Federated learning (FL) enables distributed participants to collectively\nlearn a strong global model without sacrificing their individual data privacy.\nMainstream FL approaches require each participant to share a common network\narchitecture and further assume that data are are sampled IID across\nparticipants. However, in real-world deployments participants may require\nheterogeneous network architectures; and the data distribution is almost\ncertainly non-uniform across participants. To address these issues we introduce\nFedH2L, which is agnostic to both the model architecture and robust to\ndifferent data distributions across participants. In contrast to approaches\nsharing parameters or gradients, FedH2L relies on mutual distillation,\nexchanging only posteriors on a shared seed set between participants in a\ndecentralized manner. This makes it extremely bandwidth efficient, model\nagnostic, and crucially produces models capable of performing well on the whole\ndata distribution when learning from heterogeneous silos.",
          "link": "http://arxiv.org/abs/2101.11296",
          "publishedOn": "2021-07-28T02:02:34.373Z",
          "wordCount": 615,
          "title": "FedH2L: Federated Learning with Model and Statistical Heterogeneity. (arXiv:2101.11296v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Solares_J/0/1/0/all/0/1\">Jose Roberto Ayala Solares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yajie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassaine_A/0/1/0/all/0/1\">Abdelaali Hassaine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1\">Shishir Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yikuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamouei_M/0/1/0/all/0/1\">Mohammad Mamouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canoy_D/0/1/0/all/0/1\">Dexter Canoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_K/0/1/0/all/0/1\">Kazem Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimi_Khorshidi_G/0/1/0/all/0/1\">Gholamreza Salimi-Khorshidi</a>",
          "description": "Deep learning models have shown tremendous potential in learning\nrepresentations, which are able to capture some key properties of the data.\nThis makes them great candidates for transfer learning: Exploiting\ncommonalities between different learning tasks to transfer knowledge from one\ntask to another. Electronic health records (EHR) research is one of the domains\nthat has witnessed a growing number of deep learning techniques employed for\nlearning clinically-meaningful representations of medical concepts (such as\ndiseases and medications). Despite this growth, the approaches to benchmark and\nassess such learned representations (or, embeddings) is under-investigated;\nthis can be a big issue when such embeddings are shared to facilitate transfer\nlearning. In this study, we aim to (1) train some of the most prominent disease\nembedding techniques on a comprehensive EHR data from 3.1 million patients, (2)\nemploy qualitative and quantitative evaluation techniques to assess these\nembeddings, and (3) provide pre-trained disease embeddings for transfer\nlearning. This study can be the first comprehensive approach for clinical\nconcept embedding evaluation and can be applied to any embedding techniques and\nfor any EHR concept.",
          "link": "http://arxiv.org/abs/2107.12919",
          "publishedOn": "2021-07-28T02:02:34.352Z",
          "wordCount": 623,
          "title": "Transfer Learning in Electronic Health Records through Clinical Concept Embedding. (arXiv:2107.12919v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12775",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Che_H/0/1/0/all/0/1\">Hui Che</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramanathan_S/0/1/0/all/0/1\">Sumana Ramanathan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Foran_D/0/1/0/all/0/1\">David Foran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nosher_J/0/1/0/all/0/1\">John L Nosher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M Patel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hacihaliloglu_I/0/1/0/all/0/1\">Ilker Hacihaliloglu</a>",
          "description": "With the success of deep learning-based methods applied in medical image\nanalysis, convolutional neural networks (CNNs) have been investigated for\nclassifying liver disease from ultrasound (US) data. However, the scarcity of\navailable large-scale labeled US data has hindered the success of CNNs for\nclassifying liver disease from US data. In this work, we propose a novel\ngenerative adversarial network (GAN) architecture for realistic diseased and\nhealthy liver US image synthesis. We adopt the concept of stacking to\nsynthesize realistic liver US data. Quantitative and qualitative evaluation is\nperformed on 550 in-vivo B-mode liver US images collected from 55 subjects. We\nalso show that the synthesized images, together with real in vivo data, can be\nused to significantly improve the performance of traditional CNN architectures\nfor Nonalcoholic fatty liver disease (NAFLD) classification.",
          "link": "http://arxiv.org/abs/2107.12775",
          "publishedOn": "2021-07-28T02:02:34.345Z",
          "wordCount": 598,
          "title": "Realistic Ultrasound Image Synthesis for Improved Classification of Liver Disease. (arXiv:2107.12775v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.06575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">Robin Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1\">Matthias Rottmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1\">Hanno Gottschalk</a>",
          "description": "Deep neural networks (DNNs) for the semantic segmentation of images are\nusually trained to operate on a predefined closed set of object classes. This\nis in contrast to the \"open world\" setting where DNNs are envisioned to be\ndeployed to. From a functional safety point of view, the ability to detect\nso-called \"out-of-distribution\" (OoD) samples, i.e., objects outside of a DNN's\nsemantic space, is crucial for many applications such as automated driving. A\nnatural baseline approach to OoD detection is to threshold on the pixel-wise\nsoftmax entropy. We present a two-step procedure that significantly improves\nthat approach. Firstly, we utilize samples from the COCO dataset as OoD proxy\nand introduce a second training objective to maximize the softmax entropy on\nthese samples. Starting from pretrained semantic segmentation networks we\nre-train a number of DNNs on different in-distribution datasets and\nconsistently observe improved OoD detection performance when evaluating on\ncompletely disjoint OoD datasets. Secondly, we perform a transparent\npost-processing step to discard false positive OoD samples by so-called \"meta\nclassification\". To this end, we apply linear models to a set of hand-crafted\nmetrics derived from the DNN's softmax probabilities. In our experiments we\nconsistently observe a clear additional gain in OoD detection performance,\ncutting down the number of detection errors by up to 52% when comparing the\nbest baseline with our results. We achieve this improvement sacrificing only\nmarginally in original segmentation performance. Therefore, our method\ncontributes to safer DNNs with more reliable overall system performance.",
          "link": "http://arxiv.org/abs/2012.06575",
          "publishedOn": "2021-07-28T02:02:34.339Z",
          "wordCount": 728,
          "title": "Entropy Maximization and Meta Classification for Out-Of-Distribution Detection in Semantic Segmentation. (arXiv:2012.06575v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08158",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gadermayr_M/0/1/0/all/0/1\">Michael Gadermayr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tschuchnig_M/0/1/0/all/0/1\">Maximilian Tschuchnig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stangassinger_L/0/1/0/all/0/1\">Lea Maria Stangassinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kreutzer_C/0/1/0/all/0/1\">Christina Kreutzer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Couillard_Despres_S/0/1/0/all/0/1\">Sebastien Couillard-Despres</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oostingh_G/0/1/0/all/0/1\">Gertie Janneke Oostingh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hittmair_A/0/1/0/all/0/1\">Anton Hittmair</a>",
          "description": "In contrast to paraffin sections, frozen sections can be quickly generated\nduring surgical interventions. This procedure allows surgeons to wait for\nhistological findings during the intervention to base intra-operative decisions\non the outcome of the histology. However, compared to paraffin sections, the\nquality of frozen sections is typically lower, leading to a higher ratio of\nmiss-classification. In this work, we investigated the effect of the section\ntype on automated decision support approaches for classification of thyroid\ncancer. This was enabled by a data set consisting of pairs of sections for\nindividual patients. Moreover, we investigated, whether a frozen-to-paraffin\ntranslation could help to optimize classification scores. Finally, we propose a\nspecific data augmentation strategy to deal with a small amount of training\ndata and to increase classification accuracy even further.",
          "link": "http://arxiv.org/abs/2012.08158",
          "publishedOn": "2021-07-28T02:02:34.332Z",
          "wordCount": 622,
          "title": "Frozen-to-Paraffin: Categorization of Histological Frozen Sections by the Aid of Paraffin Sections and Generative Adversarial Networks. (arXiv:2012.08158v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.",
          "link": "http://arxiv.org/abs/2107.12651",
          "publishedOn": "2021-07-28T02:02:34.285Z",
          "wordCount": 604,
          "title": "Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1\">Chung-Wei Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yakimenka_Y/0/1/0/all/0/1\">Yauhen Yakimenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsuan-Yin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosnes_E/0/1/0/all/0/1\">Eirik Rosnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kliewer_J/0/1/0/all/0/1\">Joerg Kliewer</a>",
          "description": "We propose to extend the concept of private information retrieval by allowing\nfor distortion in the retrieval process and relaxing the perfect privacy\nrequirement at the same time. In particular, we study the tradeoff between\ndownload rate, distortion, and user privacy leakage, and show that in the limit\nof large file sizes this trade-off can be captured via a novel\ninformation-theoretical formulation for datasets with a known distribution.\nMoreover, for scenarios where the statistics of the dataset is unknown, we\npropose a new deep learning framework by leveraging a generative adversarial\nnetwork approach, which allows the user to learn efficient schemes from the\ndata itself, minimizing the download cost. We evaluate the performance of the\nscheme on a synthetic Gaussian dataset as well as on both the MNIST and\nCIFAR-10 datasets. For the MNIST dataset, the data-driven approach\nsignificantly outperforms a non-learning based scheme which combines source\ncoding with multiple file download, while the CIFAR-10 performance is notably\nbetter.",
          "link": "http://arxiv.org/abs/2012.03902",
          "publishedOn": "2021-07-28T02:02:34.277Z",
          "wordCount": 658,
          "title": "Generative Adversarial User Privacy in Lossy Single-Server Information Retrieval. (arXiv:2012.03902v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tingting Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_R/0/1/0/all/0/1\">Ramy E. Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemi_H/0/1/0/all/0/1\">Hanieh Hashemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangwani_T/0/1/0/all/0/1\">Tynan Gangwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Annavaram_M/0/1/0/all/0/1\">Murali Annavaram</a>",
          "description": "Stragglers, Byzantine workers, and data privacy are the main bottlenecks in\ndistributed cloud computing. Several prior works proposed coded computing\nstrategies to jointly address all three challenges. They require either a large\nnumber of workers, a significant communication cost or a significant\ncomputational complexity to tolerate malicious workers. Much of the overhead in\nprior schemes comes from the fact that they tightly couple coding for all three\nproblems into a single framework. In this work, we propose Verifiable Coded\nComputing (VCC) framework that decouples Byzantine node detection challenge\nfrom the straggler tolerance. VCC leverages coded computing just for handling\nstragglers and privacy, and then uses an orthogonal approach of verifiable\ncomputing to tackle Byzantine nodes. Furthermore, VCC dynamically adapts its\ncoding scheme to tradeoff straggler tolerance with Byzantine protection and\nvice-versa. We evaluate VCC on compute intensive distributed logistic\nregression application. Our experiments show that VCC speeds up the\nconventional uncoded implementation of distributed logistic regression by\n$3.2\\times-6.9\\times$, and also improves the test accuracy by up to $12.6\\%$.",
          "link": "http://arxiv.org/abs/2107.12958",
          "publishedOn": "2021-07-28T02:02:34.258Z",
          "wordCount": 630,
          "title": "Verifiable Coded Computing: Towards Fast, Secure and Private Distributed Machine Learning. (arXiv:2107.12958v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12521",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghojogh_B/0/1/0/all/0/1\">Benyamin Ghojogh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karray_F/0/1/0/all/0/1\">Fakhri Karray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crowley_M/0/1/0/all/0/1\">Mark Crowley</a>",
          "description": "This is a tutorial and survey paper on Boltzmann Machine (BM), Restricted\nBoltzmann Machine (RBM), and Deep Belief Network (DBN). We start with the\nrequired background on probabilistic graphical models, Markov random field,\nGibbs sampling, statistical physics, Ising model, and the Hopfield network.\nThen, we introduce the structures of BM and RBM. The conditional distributions\nof visible and hidden variables, Gibbs sampling in RBM for generating\nvariables, training BM and RBM by maximum likelihood estimation, and\ncontrastive divergence are explained. Then, we discuss different possible\ndiscrete and continuous distributions for the variables. We introduce\nconditional RBM and how it is trained. Finally, we explain deep belief network\nas a stack of RBM models. This paper on Boltzmann machines can be useful in\nvarious fields including data science, statistics, neural computation, and\nstatistical physics.",
          "link": "http://arxiv.org/abs/2107.12521",
          "publishedOn": "2021-07-28T02:02:34.248Z",
          "wordCount": 604,
          "title": "Restricted Boltzmann Machine and Deep Belief Network: Tutorial and Survey. (arXiv:2107.12521v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ittner_J/0/1/0/all/0/1\">Jan Ittner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolikowski_L/0/1/0/all/0/1\">Lukasz Bolikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemker_K/0/1/0/all/0/1\">Konstantin Hemker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_R/0/1/0/all/0/1\">Ricardo Kennedy</a>",
          "description": "We offer a new formalism for global explanations of pairwise feature\ndependencies and interactions in supervised models. Building upon SHAP values\nand SHAP interaction values, our approach decomposes feature contributions into\nsynergistic, redundant and independent components (S-R-I decomposition of SHAP\nvectors). We propose a geometric interpretation of the components and formally\nprove its basic properties. Finally, we demonstrate the utility of synergy,\nredundancy and independence by applying them to a constructed data set and\nmodel.",
          "link": "http://arxiv.org/abs/2107.12436",
          "publishedOn": "2021-07-28T02:02:34.232Z",
          "wordCount": 523,
          "title": "Feature Synergy, Redundancy, and Independence in Global Model Explanations using SHAP Vector Decomposition. (arXiv:2107.12436v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.05754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baudry_D/0/1/0/all/0/1\">Dorian Baudry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautron_R/0/1/0/all/0/1\">Romain Gautron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufmann_E/0/1/0/all/0/1\">Emilie Kaufmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maillard_O/0/1/0/all/0/1\">Odalric-Ambryn Maillard</a>",
          "description": "In this paper we study a multi-arm bandit problem in which the quality of\neach arm is measured by the Conditional Value at Risk (CVaR) at some level\nalpha of the reward distribution. While existing works in this setting mainly\nfocus on Upper Confidence Bound algorithms, we introduce a new Thompson\nSampling approach for CVaR bandits on bounded rewards that is flexible enough\nto solve a variety of problems grounded on physical resources. Building on a\nrecent work by Riou & Honda (2020), we introduce B-CVTS for continuous bounded\nrewards and M-CVTS for multinomial distributions. On the theoretical side, we\nprovide a non-trivial extension of their analysis that enables to theoretically\nbound their CVaR regret minimization performance. Strikingly, our results show\nthat these strategies are the first to provably achieve asymptotic optimality\nin CVaR bandits, matching the corresponding asymptotic lower bounds for this\nsetting. Further, we illustrate empirically the benefit of Thompson Sampling\napproaches both in a realistic environment simulating a use-case in agriculture\nand on various synthetic examples.",
          "link": "http://arxiv.org/abs/2012.05754",
          "publishedOn": "2021-07-28T02:02:34.225Z",
          "wordCount": 637,
          "title": "Optimal Thompson Sampling strategies for support-aware CVaR bandits. (arXiv:2012.05754v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1711.03639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lykouris_T/0/1/0/all/0/1\">Thodoris Lykouris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_K/0/1/0/all/0/1\">Karthik Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tardos_E/0/1/0/all/0/1\">Eva Tardos</a>",
          "description": "We consider the problem of adversarial (non-stochastic) online learning with\npartial information feedback, where at each round, a decision maker selects an\naction from a finite set of alternatives. We develop a black-box approach for\nsuch problems where the learner observes as feedback only losses of a subset of\nthe actions that includes the selected action. When losses of actions are\nnon-negative, under the graph-based feedback model introduced by Mannor and\nShamir, we offer algorithms that attain the so called \"small-loss\" $o(\\alpha\nL^{\\star})$ regret bounds with high probability, where $\\alpha$ is the\nindependence number of the graph, and $L^{\\star}$ is the loss of the best\naction. Prior to our work, there was no data-dependent guarantee for general\nfeedback graphs even for pseudo-regret (without dependence on the number of\nactions, i.e. utilizing the increased information feedback). Taking advantage\nof the black-box nature of our technique, we extend our results to many other\napplications such as semi-bandits (including routing in networks), contextual\nbandits (even with an infinite comparator class), as well as learning with\nslowly changing (shifting) comparators.\n\nIn the special case of classical bandit and semi-bandit problems, we provide\noptimal small-loss, high-probability guarantees of\n$\\tilde{O}(\\sqrt{dL^{\\star}})$ for actual regret, where $d$ is the number of\nactions, answering open questions of Neu. Previous bounds for bandits and\nsemi-bandits were known only for pseudo-regret and only in expectation. We also\noffer an optimal $\\tilde{O}(\\sqrt{\\kappa L^{\\star}})$ regret guarantee for\nfixed feedback graphs with clique-partition number at most $\\kappa$.",
          "link": "http://arxiv.org/abs/1711.03639",
          "publishedOn": "2021-07-28T02:02:34.208Z",
          "wordCount": 765,
          "title": "Small-loss bounds for online learning with partial information. (arXiv:1711.03639v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.07450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tingyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenbing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojoudi_S/0/1/0/all/0/1\">Somayeh Sojoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>",
          "description": "Variants of Graph Neural Networks (GNNs) for representation learning have\nbeen proposed recently and achieved fruitful results in various fields. Among\nthem, Graph Attention Network (GAT) first employs a self-attention strategy to\nlearn attention weights for each edge in the spatial domain. However, learning\nthe attentions over edges can only focus on the local information of graphs and\ngreatly increases the computational costs. In this paper, we first introduce\nthe attention mechanism in the spectral domain of graphs and present Spectral\nGraph Attention Network (SpGAT) that learns representations for different\nfrequency components regarding weighted filters and graph wavelets bases. In\nthis way, SpGAT can better capture global patterns of graphs in an efficient\nmanner with much fewer learned parameters than that of GAT. Further, to reduce\nthe computational cost of SpGAT brought by the eigen-decomposition, we propose\na fast approximation variant SpGAT-Cheby. We thoroughly evaluate the\nperformance of SpGAT and SpGAT-Cheby in semi-supervised node classification\ntasks and verify the effectiveness of the learned attentions in the spectral\ndomain.",
          "link": "http://arxiv.org/abs/2003.07450",
          "publishedOn": "2021-07-28T02:02:34.201Z",
          "wordCount": 655,
          "title": "Spectral Graph Attention Network with Fast Eigen-approximation. (arXiv:2003.07450v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bonet_D/0/1/0/all/0/1\">David Bonet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_A/0/1/0/all/0/1\">Antonio Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Hidalgo_J/0/1/0/all/0/1\">Javier Ruiz-Hidalgo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekkizhar_S/0/1/0/all/0/1\">Sarath Shekkizhar</a>",
          "description": "State-of-the-art neural network architectures continue to scale in size and\ndeliver impressive generalization results, although this comes at the expense\nof limited interpretability. In particular, a key challenge is to determine\nwhen to stop training the model, as this has a significant impact on\ngeneralization. Convolutional neural networks (ConvNets) comprise\nhigh-dimensional feature spaces formed by the aggregation of multiple channels,\nwhere analyzing intermediate data representations and the model's evolution can\nbe challenging owing to the curse of dimensionality. We present channel-wise\nDeepNNK (CW-DeepNNK), a novel channel-wise generalization estimate based on\nnon-negative kernel regression (NNK) graphs with which we perform local\npolytope interpolation on low-dimensional channels. This method leads to\ninstance-based interpretability of both the learned data representations and\nthe relationship between channels. Motivated by our observations, we use\nCW-DeepNNK to propose a novel early stopping criterion that (i) does not\nrequire a validation set, (ii) is based on a task performance metric, and (iii)\nallows stopping to be reached at different points for each channel. Our\nexperiments demonstrate that our proposed method has advantages as compared to\nthe standard criterion based on validation set performance.",
          "link": "http://arxiv.org/abs/2107.12972",
          "publishedOn": "2021-07-28T02:02:34.193Z",
          "wordCount": 631,
          "title": "Channel-Wise Early Stopping without a Validation Set via NNK Polytope Interpolation. (arXiv:2107.12972v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekarchi_Z/0/1/0/all/0/1\">Zahra Shekarchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_S/0/1/0/all/0/1\">Suzanne Stevenson</a>",
          "description": "Children learn word meanings by tapping into the commonalities across\ndifferent situations in which words are used and overcome the high level of\nuncertainty involved in early word learning experiences. We propose a modeling\nframework to investigate the role of mutual exclusivity bias - asserting\none-to-one mappings between words and their meanings - in reducing uncertainty\nin word learning. In a set of computational studies, we show that to\nsuccessfully learn word meanings in the face of uncertainty, a learner needs to\nuse two types of competition: words competing for association to a referent\nwhen learning from an observation and referents competing for a word when the\nword is used. Our work highlights the importance of an algorithmic-level\nanalysis to shed light on the utility of different mechanisms that can\nimplement the same computational-level theory.",
          "link": "http://arxiv.org/abs/2012.03370",
          "publishedOn": "2021-07-28T02:02:34.176Z",
          "wordCount": 619,
          "title": "Competition in Cross-situational Word Learning: A Computational Study. (arXiv:2012.03370v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cesaire_M/0/1/0/all/0/1\">Manon C&#xe9;saire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajri_H/0/1/0/all/0/1\">Hatem Hajri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1\">Sylvain Lamprier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>",
          "description": "This paper introduces stochastic sparse adversarial attacks (SSAA), simple,\nfast and purely noise-based targeted and untargeted $L_0$ attacks of neural\nnetwork classifiers (NNC). SSAA are devised by exploiting a simple small-time\nexpansion idea widely used for Markov processes and offer new examples of $L_0$\nattacks whose studies have been limited. They are designed to solve the known\nscalability issue of the family of Jacobian-based saliency maps attacks to\nlarge datasets and they succeed in solving it. Experiments on small and large\ndatasets (CIFAR-10 and ImageNet) illustrate further advantages of SSAA in\ncomparison with the-state-of-the-art methods. For instance, in the untargeted\ncase, our method called Voting Folded Gaussian Attack (VFGA) scales efficiently\nto ImageNet and achieves a significantly lower $L_0$ score than SparseFool (up\nto $\\frac{2}{5}$ lower) while being faster. Moreover, VFGA achieves better\n$L_0$ scores on ImageNet than Sparse-RS when both attacks are fully successful\non a large number of samples. Codes are publicly available through the link\nhttps://github.com/SSAA3/stochastic-sparse-adv-attacks",
          "link": "http://arxiv.org/abs/2011.12423",
          "publishedOn": "2021-07-28T02:02:34.166Z",
          "wordCount": 644,
          "title": "Stochastic sparse adversarial attacks. (arXiv:2011.12423v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Archit Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1\">Karol Hausman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>",
          "description": "Reinforcement learning (RL) promises to enable autonomous acquisition of\ncomplex behaviors for diverse agents. However, the success of current\nreinforcement learning algorithms is predicated on an often under-emphasised\nrequirement -- each trial needs to start from a fixed initial state\ndistribution. Unfortunately, resetting the environment to its initial state\nafter each trial requires substantial amount of human supervision and extensive\ninstrumentation of the environment which defeats the purpose of autonomous\nreinforcement learning. In this work, we propose Value-accelerated Persistent\nReinforcement Learning (VaPRL), which generates a curriculum of initial states\nsuch that the agent can bootstrap on the success of easier tasks to efficiently\nlearn harder tasks. The agent also learns to reach the initial states proposed\nby the curriculum, minimizing the reliance on human interventions into the\nlearning. We observe that VaPRL reduces the interventions required by three\norders of magnitude compared to episodic RL while outperforming prior\nstate-of-the art methods for reset-free RL both in terms of sample efficiency\nand asymptotic performance on a variety of simulated robotics problems.",
          "link": "http://arxiv.org/abs/2107.12931",
          "publishedOn": "2021-07-28T02:02:34.146Z",
          "wordCount": 605,
          "title": "Persistent Reinforcement Learning via Subgoal Curricula. (arXiv:2107.12931v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12869",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Duong_Q/0/1/0/all/0/1\">Quan Duong</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tran_T/0/1/0/all/0/1\">Tan Tran</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pham_D/0/1/0/all/0/1\">Duc-Thinh Pham</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mai_A/0/1/0/all/0/1\">An Mai</a>",
          "description": "The volume of flight traffic gets increasing over the time, which makes the\nstrategic traffic flow management become one of the challenging problems since\nit requires a lot of computational resources to model entire traffic data. On\nthe other hand, Automatic Dependent Surveillance - Broadcast (ADS-B) technology\nhas been considered as a promising data technology to provide both flight crews\nand ground control staff the necessary information safely and efficiently about\nthe position and velocity of the airplanes in a specific area. In the attempt\nto tackle this problem, we presented in this paper a simplified framework that\ncan support to detect the typical air routes between airports based on ADS-B\ndata. Specifically, the flight traffic will be classified into major groups\nbased on similarity measures, which helps to reduce the number of flight paths\nbetween airports. As a matter of fact, our framework can be taken into account\nto reduce practically the computational cost for air flow optimization and\nevaluate the operational performance. Finally, in order to illustrate the\npotential applications of our proposed framework, an experiment was performed\nusing ADS-B traffic flight data of three different pairs of airports. The\ndetected typical routes between each couple of airports show promising results\nby virtue of combining two indices for measuring the clustering performance and\nincorporating human judgment into the visual inspection.",
          "link": "http://arxiv.org/abs/2107.12869",
          "publishedOn": "2021-07-28T02:02:34.139Z",
          "wordCount": 681,
          "title": "A Simplified Framework for Air Route Clustering Based on ADS-B Data. (arXiv:2107.12869v1 [physics.soc-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12970",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifeng Zhao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_P/0/1/0/all/0/1\">Pei Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Galindo_Torres_S/0/1/0/all/0/1\">S.A. Galindo-Torres</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>",
          "description": "Longitudinal Dispersion(LD) is the dominant process of scalar transport in\nnatural streams. An accurate prediction on LD coefficient(Dl) can produce a\nperformance leap in related simulation. The emerging machine learning(ML)\ntechniques provide a self-adaptive tool for this problem. However, most of the\nexisting studies utilize an unproved quaternion feature set, obtained through\nsimple theoretical deduction. Few studies have put attention on its reliability\nand rationality. Besides, due to the lack of comparative comparison, the proper\nchoice of ML models in different scenarios still remains unknown. In this\nstudy, the Feature Gradient selector was first adopted to distill the local\noptimal feature sets directly from multivariable data. Then, a global optimal\nfeature set (the channel width, the flow velocity, the channel slope and the\ncross sectional area) was proposed through numerical comparison of the\ndistilled local optimums in performance with representative ML models. The\nchannel slope is identified to be the key parameter for the prediction of LDC.\nFurther, we designed a weighted evaluation metric which enables comprehensive\nmodel comparison. With the simple linear model as the baseline, a benchmark of\nsingle and ensemble learning models was provided. Advantages and disadvantages\nof the methods involved were also discussed. Results show that the support\nvector machine has significantly better performance than other models. Decision\ntree is not suitable for this problem due to poor generalization ability.\nNotably, simple models show superiority over complicated model on this\nlow-dimensional problem, for their better balance between regression and\ngeneralization.",
          "link": "http://arxiv.org/abs/2107.12970",
          "publishedOn": "2021-07-28T02:02:34.133Z",
          "wordCount": 691,
          "title": "A Data-driven feature selection and machine-learning model benchmark for the prediction of longitudinal dispersion coefficient. (arXiv:2107.12970v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12978",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1\">Brennan Nichyporuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1\">Justin Szeto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1\">Douglas L. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>",
          "description": "There are many clinical contexts which require accurate detection and\nsegmentation of all focal pathologies (e.g. lesions, tumours) in patient\nimages. In cases where there are a mix of small and large lesions, standard\nbinary cross entropy loss will result in better segmentation of large lesions\nat the expense of missing small ones. Adjusting the operating point to\naccurately detect all lesions generally leads to oversegmentation of large\nlesions. In this work, we propose a novel reweighing strategy to eliminate this\nperformance gap, increasing small pathology detection performance while\nmaintaining segmentation accuracy. We show that our reweighing strategy vastly\noutperforms competing strategies based on experiments on a large scale,\nmulti-scanner, multi-center dataset of Multiple Sclerosis patient images.",
          "link": "http://arxiv.org/abs/2107.12978",
          "publishedOn": "2021-07-28T02:02:34.125Z",
          "wordCount": 584,
          "title": "Optimizing Operating Points for High Performance Lesion Detection and Segmentation Using Lesion Size Reweighting. (arXiv:2107.12978v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sommer_D/0/1/0/all/0/1\">David M. Sommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abfalterer_L/0/1/0/all/0/1\">Lukas Abfalterer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zingg_S/0/1/0/all/0/1\">Sheila Zingg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_E/0/1/0/all/0/1\">Esfandiar Mohammadi</a>",
          "description": "Differentially private (DP) mechanisms face the challenge of providing\naccurate results while protecting their inputs: the privacy-utility trade-off.\nA simple but powerful technique for DP adds noise to sensitivity-bounded query\noutputs to blur the exact query output: additive mechanisms. While a vast body\nof work considers infinitely wide noise distributions, some applications (e.g.,\nreal-time operating systems) require hard bounds on the deviations from the\nreal query, and only limited work on such mechanisms exist. An additive\nmechanism with truncated noise (i.e., with bounded range) can offer such hard\nbounds. We introduce a gradient-descent-based tool to learn truncated noise for\nadditive mechanisms with strong utility bounds while simultaneously optimizing\nfor differential privacy under sequential composition, i.e., scenarios where\nmultiple noisy queries on the same data are revealed. Our method can learn\ndiscrete noise patterns and not only hyper-parameters of a predefined\nprobability distribution. For sensitivity bounded mechanisms, we show that it\nis sufficient to consider symmetric and that\\new{, for from the mean\nmonotonically falling noise,} ensuring privacy for a pair of representative\nquery outputs guarantees privacy for all pairs of inputs (that differ in one\nelement). We find that the utility-privacy trade-off curves of our generated\nnoise are remarkably close to truncated Gaussians and even replicate their\nshape for $l_2$ utility-loss. For a low number of compositions, we also\nimproved DP-SGD (sub-sampling). Moreover, we extend Moments Accountant to\ntruncated distributions, allowing to incorporate mechanism output events with\nvarying input-dependent zero occurrence probability.",
          "link": "http://arxiv.org/abs/2107.12957",
          "publishedOn": "2021-07-28T02:02:34.118Z",
          "wordCount": 686,
          "title": "Learning Numeric Optimal Differentially Private Truncated Additive Mechanisms. (arXiv:2107.12957v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kawakami_H/0/1/0/all/0/1\">Hiroki Kawakami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_H/0/1/0/all/0/1\">Hirohisa Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Keisuke Sugiura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsutani_H/0/1/0/all/0/1\">Hiroki Matsutani</a>",
          "description": "Although high-performance deep neural networks are in high demand in edge\nenvironments, computation resources are strictly limited in edge devices, and\nlight-weight neural network techniques, such as Depthwise Separable Convolution\n(DSC), have been developed. ResNet is one of conventional deep neural network\nmodels that stack a lot of layers and parameters for a higher accuracy. To\nreduce the parameter size of ResNet, by utilizing a similarity to ODE (Ordinary\nDifferential Equation), Neural ODE repeatedly uses most of weight parameters\ninstead of having a lot of different parameters. Thus, Neural ODE becomes\nsignificantly small compared to that of ResNet so that it can be implemented in\nresource-limited edge devices. In this paper, a combination of Neural ODE and\nDSC, called dsODENet, is designed and implemented for FPGAs (Field-Programmable\nGate Arrays). dsODENet is then applied to edge domain adaptation as a practical\nuse case and evaluated with image classification datasets. It is implemented on\nXilinx ZCU104 board and evaluated in terms of domain adaptation accuracy,\ntraining speed, FPGA resource utilization, and speedup rate compared to a\nsoftware execution. The results demonstrate that dsODENet is comparable to or\nslightly better than our baseline Neural ODE implementation in terms of domain\nadaptation accuracy, while the total parameter size without pre- and\npost-processing layers is reduced by 54.2% to 79.8%. The FPGA implementation\naccelerates the prediction tasks by 27.9 times faster than a software\nimplementation.",
          "link": "http://arxiv.org/abs/2107.12824",
          "publishedOn": "2021-07-28T02:02:34.103Z",
          "wordCount": 677,
          "title": "A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs. (arXiv:2107.12824v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12878",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Alle_S/0/1/0/all/0/1\">Shanmukh Alle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Priyakumar_U/0/1/0/all/0/1\">U. Deva Priyakumar</a>",
          "description": "Parkinson's Disease (PD) is a chronic and progressive neurological disorder\nthat results in rigidity, tremors and postural instability. There is no\ndefinite medical test to diagnose PD and diagnosis is mostly a clinical\nexercise. Although guidelines exist, about 10-30% of the patients are wrongly\ndiagnosed with PD. Hence, there is a need for an accurate, unbiased and fast\nmethod for diagnosis. In this study, we propose LPGNet, a fast and accurate\nmethod to diagnose PD from gait. LPGNet uses Linear Prediction Residuals (LPR)\nto extract discriminating patterns from gait recordings and then uses a 1D\nconvolution neural network with depth-wise separable convolutions to perform\ndiagnosis. LPGNet achieves an AUC of 0.91 with a 21 times speedup and about 99%\nlesser parameters in the model compared to the state of the art. We also\nundertake an analysis of various cross-validation strategies used in literature\nin PD diagnosis from gait and find that most methods are affected by some form\nof data leakage between various folds which leads to unnecessarily large models\nand inflated performance due to overfitting. The analysis clears the path for\nfuture works in correctly evaluating their methods.",
          "link": "http://arxiv.org/abs/2107.12878",
          "publishedOn": "2021-07-28T02:02:34.096Z",
          "wordCount": 664,
          "title": "Linear Prediction Residual for Efficient Diagnosis of Parkinson's Disease from Gait. (arXiv:2107.12878v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12910",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Hongpeng Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ibrahim_C/0/1/0/all/0/1\">Chahine Ibrahim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_W/0/1/0/all/0/1\">Wei Xing Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_W/0/1/0/all/0/1\">Wei Pan</a>",
          "description": "This paper proposes a sparse Bayesian treatment of deep neural networks\n(DNNs) for system identification. Although DNNs show impressive approximation\nability in various fields, several challenges still exist for system\nidentification problems. First, DNNs are known to be too complex that they can\neasily overfit the training data. Second, the selection of the input regressors\nfor system identification is nontrivial. Third, uncertainty quantification of\nthe model parameters and predictions are necessary. The proposed Bayesian\napproach offers a principled way to alleviate the above challenges by marginal\nlikelihood/model evidence approximation and structured group sparsity-inducing\npriors construction. The identification algorithm is derived as an iterative\nregularized optimization procedure that can be solved as efficiently as\ntraining typical DNNs. Furthermore, a practical calculation approach based on\nthe Monte-Carlo integration method is derived to quantify the uncertainty of\nthe parameters and predictions. The effectiveness of the proposed Bayesian\napproach is demonstrated on several linear and nonlinear systems identification\nbenchmarks with achieving good and competitive simulation accuracy.",
          "link": "http://arxiv.org/abs/2107.12910",
          "publishedOn": "2021-07-28T02:02:34.086Z",
          "wordCount": 610,
          "title": "Sparse Bayesian Deep Learning for Dynamic System Identification. (arXiv:2107.12910v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baird_A/0/1/0/all/0/1\">Alice Baird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stappen_L/0/1/0/all/0/1\">Lukas Stappen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christ_L/0/1/0/all/0/1\">Lukas Christ</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schumann_L/0/1/0/all/0/1\">Lea Schumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messner_E/0/1/0/all/0/1\">Eva-Maria Me&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>",
          "description": "Emotion is an inherently subjective psychophysiological human-state and to\nproduce an agreed-upon representation (gold standard) for continuous emotion\nrequires a time-consuming and costly training procedure of multiple human\nannotators. There is strong evidence in the literature that physiological\nsignals are sufficient objective markers for states of emotion, particularly\narousal. In this contribution, we utilise a dataset which includes continuous\nemotion and physiological signals - Heartbeats per Minute (BPM), Electrodermal\nActivity (EDA), and Respiration-rate - captured during a stress induced\nscenario (Trier Social Stress Test). We utilise a Long Short-Term Memory,\nRecurrent Neural Network to explore the benefit of fusing these physiological\nsignals with arousal as the target, learning from various audio, video, and\ntextual based features. We utilise the state-of-the-art MuSe-Toolbox to\nconsider both annotation delay and inter-rater agreement weighting when fusing\nthe target signals. An improvement in Concordance Correlation Coefficient (CCC)\nis seen across features sets when fusing EDA with arousal, compared to the\narousal only gold standard results. Additionally, BERT-based textual features'\nresults improved for arousal plus all physiological signals, obtaining up to\n.3344 CCC compared to .2118 CCC for arousal only. Multimodal fusion also\nimproves overall CCC with audio plus video features obtaining up to .6157 CCC\nto recognize arousal plus EDA and BPM.",
          "link": "http://arxiv.org/abs/2107.12964",
          "publishedOn": "2021-07-28T02:02:34.080Z",
          "wordCount": 660,
          "title": "A Physiologically-adapted Gold Standard for Arousal During a Stress Induced Scenario. (arXiv:2107.12964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perekrestenko_D/0/1/0/all/0/1\">Dmytro Perekrestenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eberhard_L/0/1/0/all/0/1\">L&#xe9;andre Eberhard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolcskei_H/0/1/0/all/0/1\">Helmut B&#xf6;lcskei</a>",
          "description": "We show that every $d$-dimensional probability distribution of bounded\nsupport can be generated through deep ReLU networks out of a $1$-dimensional\nuniform input distribution. What is more, this is possible without incurring a\ncost - in terms of approximation error measured in Wasserstein-distance -\nrelative to generating the $d$-dimensional target distribution from $d$\nindependent random variables. This is enabled by a vast generalization of the\nspace-filling approach discovered in (Bailey & Telgarsky, 2018). The\nconstruction we propose elicits the importance of network depth in driving the\nWasserstein distance between the target distribution and its neural network\napproximation to zero. Finally, we find that, for histogram target\ndistributions, the number of bits needed to encode the corresponding generative\nnetwork equals the fundamental limit for encoding probability distributions as\ndictated by quantization theory.",
          "link": "http://arxiv.org/abs/2107.12466",
          "publishedOn": "2021-07-28T02:02:34.062Z",
          "wordCount": 558,
          "title": "High-Dimensional Distribution Generation Through Deep Neural Networks. (arXiv:2107.12466v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mimi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parnell_A/0/1/0/all/0/1\">Andrew Parnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brabazon_D/0/1/0/all/0/1\">Dermot Brabazon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benavoli_A/0/1/0/all/0/1\">Alessio Benavoli</a>",
          "description": "Bayesian optimization (BO) is an approach to globally optimizing black-box\nobjective functions that are expensive to evaluate. BO-powered experimental\ndesign has found wide application in materials science, chemistry, experimental\nphysics, drug development, etc. This work aims to bring attention to the\nbenefits of applying BO in designing experiments and to provide a BO manual,\ncovering both methodology and software, for the convenience of anyone who wants\nto apply or learn BO. In particular, we briefly explain the BO technique,\nreview all the applications of BO in additive manufacturing, compare and\nexemplify the features of different open BO libraries, unlock new potential\napplications of BO to other types of data (e.g., preferential output). This\narticle is aimed at readers with some understanding of Bayesian methods, but\nnot necessarily with knowledge of additive manufacturing; the software\nperformance overview and implementation instructions are instrumental for any\nexperimental-design practitioner. Moreover, our review in the field of additive\nmanufacturing highlights the current knowledge and technological trends of BO.",
          "link": "http://arxiv.org/abs/2107.12809",
          "publishedOn": "2021-07-28T02:02:34.055Z",
          "wordCount": 606,
          "title": "Bayesian Optimisation for Sequential Experimental Design with Applications in Additive Manufacturing. (arXiv:2107.12809v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Strumke_I/0/1/0/all/0/1\">Inga Str&#xfc;mke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slavkovik_M/0/1/0/all/0/1\">Marija Slavkovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madai_V/0/1/0/all/0/1\">Vince Madai</a>",
          "description": "While the demand for ethical artificial intelligence (AI) systems increases,\nthe number of unethical uses of AI accelerates, even though there is no\nshortage of ethical guidelines. We argue that a main underlying cause for this\nis that AI developers face a social dilemma in AI development ethics,\npreventing the widespread adaptation of ethical best practices. We define the\nsocial dilemma for AI development and describe why the current crisis in AI\ndevelopment ethics cannot be solved without relieving AI developers of their\nsocial dilemma. We argue that AI development must be professionalised to\novercome the social dilemma, and discuss how medicine can be used as a template\nin this process.",
          "link": "http://arxiv.org/abs/2107.12977",
          "publishedOn": "2021-07-28T02:02:34.046Z",
          "wordCount": 558,
          "title": "The social dilemma in AI development and why we have to solve it. (arXiv:2107.12977v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12838",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Noman_F/0/1/0/all/0/1\">Fuad Noman</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ting_C/0/1/0/all/0/1\">Chee-Ming Ting</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kang_H/0/1/0/all/0/1\">Hakmook Kang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Phan_R/0/1/0/all/0/1\">Raphael C.-W. Phan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Boyd_B/0/1/0/all/0/1\">Brian D. Boyd</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Taylor_W/0/1/0/all/0/1\">Warren D. Taylor</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ombao_H/0/1/0/all/0/1\">Hernando Ombao</a>",
          "description": "Brain functional connectivity (FC) reveals biomarkers for identification of\nvarious neuropsychiatric disorders. Recent application of deep neural networks\n(DNNs) to connectome-based classification mostly relies on traditional\nconvolutional neural networks using input connectivity matrices on a regular\nEuclidean grid. We propose a graph deep learning framework to incorporate the\nnon-Euclidean information about graph structure for classifying functional\nmagnetic resonance imaging (fMRI)- derived brain networks in major depressive\ndisorder (MDD). We design a novel graph autoencoder (GAE) architecture based on\nthe graph convolutional networks (GCNs) to embed the topological structure and\nnode content of large-sized fMRI networks into low-dimensional latent\nrepresentations. In network construction, we employ the Ledoit-Wolf (LDW)\nshrinkage method to estimate the high-dimensional FC metrics efficiently from\nfMRI data. We consider both supervised and unsupervised approaches for the\ngraph embedded learning. The learned embeddings are then used as feature inputs\nfor a deep fully-connected neural network (FCNN) to discriminate MDD from\nhealthy controls. Evaluated on a resting-state fMRI MDD dataset with 43\nsubjects, results show that the proposed GAE-FCNN model significantly\noutperforms several state-of-the-art DNN methods for brain connectome\nclassification, achieving accuracy of 72.50% using the LDW-FC metrics as node\nfeatures. The graph embeddings of fMRI FC networks learned by the GAE also\nreveal apparent group differences between MDD and HC. Our new framework\ndemonstrates feasibility of learning graph embeddings on brain networks to\nprovide discriminative information for diagnosis of brain disorders.",
          "link": "http://arxiv.org/abs/2107.12838",
          "publishedOn": "2021-07-28T02:02:34.021Z",
          "wordCount": 691,
          "title": "Graph Autoencoders for Embedding Learning in Brain Networks and Major Depressive Disorder Identification. (arXiv:2107.12838v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12889",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Felfeliyan_B/0/1/0/all/0/1\">Banafshe Felfeliyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hareendranathan_A/0/1/0/all/0/1\">Abhilash Hareendranathan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuntze_G/0/1/0/all/0/1\">Gregor Kuntze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jaremko_J/0/1/0/all/0/1\">Jacob L. Jaremko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ronsky_J/0/1/0/all/0/1\">Janet L. Ronsky</a>",
          "description": "Objective assessment of Magnetic Resonance Imaging (MRI) scans of\nosteoarthritis (OA) can address the limitation of the current OA assessment.\nSegmentation of bone, cartilage, and joint fluid is necessary for the OA\nobjective assessment. Most of the proposed segmentation methods are not\nperforming instance segmentation and suffer from class imbalance problems. This\nstudy deployed Mask R-CNN instance segmentation and improved it (improved-Mask\nR-CNN (iMaskRCNN)) to obtain a more accurate generalized segmentation for\nOA-associated tissues. Training and validation of the method were performed\nusing 500 MRI knees from the Osteoarthritis Initiative (OAI) dataset and 97 MRI\nscans of patients with symptomatic hip OA. Three modifications to Mask R-CNN\nyielded the iMaskRCNN: adding a 2nd ROIAligned block, adding an extra decoder\nlayer to the mask-header, and connecting them by a skip connection. The results\nwere assessed using Hausdorff distance, dice score, and coefficients of\nvariation (CoV). The iMaskRCNN led to improved bone and cartilage segmentation\ncompared to Mask RCNN as indicated with the increase in dice score from 95% to\n98% for the femur, 95% to 97% for tibia, 71% to 80% for femoral cartilage, and\n81% to 82% for tibial cartilage. For the effusion detection, dice improved with\niMaskRCNN 72% versus MaskRCNN 71%. The CoV values for effusion detection\nbetween Reader1 and Mask R-CNN (0.33), Reader1 and iMaskRCNN (0.34), Reader2\nand Mask R-CNN (0.22), Reader2 and iMaskRCNN (0.29) are close to CoV between\ntwo readers (0.21), indicating a high agreement between the human readers and\nboth Mask R-CNN and iMaskRCNN. Mask R-CNN and iMaskRCNN can reliably and\nsimultaneously extract different scale articular tissues involved in OA,\nforming the foundation for automated assessment of OA. The iMaskRCNN results\nshow that the modification improved the network performance around the edges.",
          "link": "http://arxiv.org/abs/2107.12889",
          "publishedOn": "2021-07-28T02:02:34.013Z",
          "wordCount": 755,
          "title": "Improved-Mask R-CNN: Towards an Accurate Generic MSK MRI instance segmentation platform (Data from the Osteoarthritis Initiative). (arXiv:2107.12889v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaeckle_F/0/1/0/all/0/1\">Florian Jaeckle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingyue Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">M. Pawan Kumar</a>",
          "description": "Many available formal verification methods have been shown to be instances of\na unified Branch-and-Bound (BaB) formulation. We propose a novel machine\nlearning framework that can be used for designing an effective branching\nstrategy as well as for computing better lower bounds. Specifically, we learn\ntwo graph neural networks (GNN) that both directly treat the network we want to\nverify as a graph input and perform forward-backward passes through the GNN\nlayers. We use one GNN to simulate the strong branching heuristic behaviour and\nanother to compute a feasible dual solution of the convex relaxation, thereby\nproviding a valid lower bound.\n\nWe provide a new verification dataset that is more challenging than those\nused in the literature, thereby providing an effective alternative for testing\nalgorithmic improvements for verification. Whilst using just one of the GNNs\nleads to a reduction in verification time, we get optimal performance when\ncombining the two GNN approaches. Our combined framework achieves a 50\\%\nreduction in both the number of branches and the time required for verification\non various convolutional networks when compared to several state-of-the-art\nverification methods. In addition, we show that our GNN models generalize well\nto harder properties on larger unseen networks.",
          "link": "http://arxiv.org/abs/2107.12855",
          "publishedOn": "2021-07-28T02:02:33.965Z",
          "wordCount": 638,
          "title": "Neural Network Branch-and-Bound for Neural Network Verification. (arXiv:2107.12855v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kenfack_P/0/1/0/all/0/1\">Patrik Joslin Kenfack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Adil Mehmood Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_R/0/1/0/all/0/1\">Rasheed Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazmi_S/0/1/0/all/0/1\">S.M. Ahsan Kazmi</a>,",
          "description": "Training machine learning models with the only accuracy as a final goal may\npromote prejudices and discriminatory behaviors embedded in the data. One\nsolution is to learn latent representations that fulfill specific fairness\nmetrics. Different types of learning methods are employed to map data into the\nfair representational space. The main purpose is to learn a latent\nrepresentation of data that scores well on a fairness metric while maintaining\nthe usability for the downstream task. In this paper, we propose a new fair\nrepresentation learning approach that leverages different levels of\nrepresentation of data to tighten the fairness bounds of the learned\nrepresentation. Our results show that stacking different auto-encoders and\nenforcing fairness at different latent spaces result in an improvement of\nfairness compared to other existing approaches.",
          "link": "http://arxiv.org/abs/2107.12826",
          "publishedOn": "2021-07-28T02:02:33.952Z",
          "wordCount": 568,
          "title": "Adversarial Stacked Auto-Encoders for Fair Representation Learning. (arXiv:2107.12826v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12797",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kepler_M/0/1/0/all/0/1\">Michael E. Kepler</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Koppel_A/0/1/0/all/0/1\">Alec Koppel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bedi_A/0/1/0/all/0/1\">Amrit Singh Bedi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Stilwell_D/0/1/0/all/0/1\">Daniel J. Stilwell</a>",
          "description": "Gaussian processes (GPs) are a well-known nonparametric Bayesian inference\ntechnique, but they suffer from scalability problems for large sample sizes,\nand their performance can degrade for non-stationary or spatially heterogeneous\ndata. In this work, we seek to overcome these issues through (i) employing\nvariational free energy approximations of GPs operating in tandem with online\nexpectation propagation steps; and (ii) introducing a local splitting step\nwhich instantiates a new GP whenever the posterior distribution changes\nsignificantly as quantified by the Wasserstein metric over posterior\ndistributions. Over time, then, this yields an ensemble of sparse GPs which may\nbe updated incrementally, and adapts to locality, heterogeneity, and\nnon-stationarity in training data.",
          "link": "http://arxiv.org/abs/2107.12797",
          "publishedOn": "2021-07-28T02:02:33.936Z",
          "wordCount": 548,
          "title": "Wasserstein-Splitting Gaussian Process Regression for Heterogeneous Online Bayesian Inference. (arXiv:2107.12797v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12626",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiqiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhiwen Pan</a>",
          "description": "Nowadays, multi-sensor technologies are applied in many fields, e.g., Health\nCare (HC), Human Activity Recognition (HAR), and Industrial Control System\n(ICS). These sensors can generate a substantial amount of multivariate\ntime-series data. Unsupervised anomaly detection on multi-sensor time-series\ndata has been proven critical in machine learning researches. The key challenge\nis to discover generalized normal patterns by capturing spatial-temporal\ncorrelation in multi-sensor data. Beyond this challenge, the noisy data is\noften intertwined with the training data, which is likely to mislead the model\nby making it hard to distinguish between the normal, abnormal, and noisy data.\nFew of previous researches can jointly address these two challenges. In this\npaper, we propose a novel deep learning-based anomaly detection algorithm\ncalled Deep Convolutional Autoencoding Memory network (CAE-M). We first build a\nDeep Convolutional Autoencoder to characterize spatial dependence of\nmulti-sensor data with a Maximum Mean Discrepancy (MMD) to better distinguish\nbetween the noisy, normal, and abnormal data. Then, we construct a Memory\nNetwork consisting of linear (Autoregressive Model) and non-linear predictions\n(Bidirectional LSTM with Attention) to capture temporal dependence from\ntime-series data. Finally, CAE-M jointly optimizes these two subnetworks. We\nempirically compare the proposed approach with several state-of-the-art anomaly\ndetection methods on HAR and HC datasets. Experimental results demonstrate that\nour proposed model outperforms these existing methods.",
          "link": "http://arxiv.org/abs/2107.12626",
          "publishedOn": "2021-07-28T02:02:33.901Z",
          "wordCount": 671,
          "title": "Unsupervised Deep Anomaly Detection for Multi-Sensor Time-Series Signals. (arXiv:2107.12626v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ausset_G/0/1/0/all/0/1\">Guillaume Ausset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciffreo_T/0/1/0/all/0/1\">Tom Ciffreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portier_F/0/1/0/all/0/1\">Francois Portier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clemencon_S/0/1/0/all/0/1\">Stephan Cl&#xe9;men&#xe7;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papin_T/0/1/0/all/0/1\">Timoth&#xe9;e Papin</a>",
          "description": "Survival analysis, or time-to-event modelling, is a classical statistical\nproblem that has garnered a lot of interest for its practical use in\nepidemiology, demographics or actuarial sciences. Recent advances on the\nsubject from the point of view of machine learning have been concerned with\nprecise per-individual predictions instead of population studies, driven by the\nrise of individualized medicine. We introduce here a conditional normalizing\nflow based estimate of the time-to-event density as a way to model highly\nflexible and individualized conditional survival distributions. We use a novel\nhierarchical formulation of normalizing flows to enable efficient fitting of\nflexible conditional distributions without overfitting and show how the\nnormalizing flow formulation can be efficiently adapted to the censored\nsetting. We experimentally validate the proposed approach on a synthetic\ndataset as well as four open medical datasets and an example of a common\nfinancial problem.",
          "link": "http://arxiv.org/abs/2107.12825",
          "publishedOn": "2021-07-28T02:02:33.893Z",
          "wordCount": 579,
          "title": "Individual Survival Curves with Conditional Normalizing Flows. (arXiv:2107.12825v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuesheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haizhang Zhang</a>",
          "description": "We explore convergence of deep neural networks with the popular ReLU\nactivation function, as the depth of the networks tends to infinity. To this\nend, we introduce the notion of activation domains and activation matrices of a\nReLU network. By replacing applications of the ReLU activation function by\nmultiplications with activation matrices on activation domains, we obtain an\nexplicit expression of the ReLU network. We then identify the convergence of\nthe ReLU networks as convergence of a class of infinite products of matrices.\nSufficient and necessary conditions for convergence of these infinite products\nof matrices are studied. As a result, we establish necessary conditions for\nReLU networks to converge that the sequence of weight matrices converges to the\nidentity matrix and the sequence of the bias vectors converges to zero as the\ndepth of ReLU networks increases to infinity. Moreover, we obtain sufficient\nconditions in terms of the weight matrices and bias vectors at hidden layers\nfor pointwise convergence of deep ReLU networks. These results provide\nmathematical insights to the design strategy of the well-known deep residual\nnetworks in image classification.",
          "link": "http://arxiv.org/abs/2107.12530",
          "publishedOn": "2021-07-28T02:02:33.802Z",
          "wordCount": 607,
          "title": "Convergence of Deep ReLU Networks. (arXiv:2107.12530v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Souvik Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_G/0/1/0/all/0/1\">Gourav Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedram_M/0/1/0/all/0/1\">Massoud Pedram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beerel_P/0/1/0/all/0/1\">Peter A. Beerel</a>",
          "description": "Deep spiking neural networks (SNNs) have emerged as a potential alternative\nto traditional deep learning frameworks, due to their promise to provide\nincreased compute efficiency on event-driven neuromorphic hardware. However, to\nperform well on complex vision applications, most SNN training frameworks yield\nlarge inference latency which translates to increased spike activity and\nreduced energy efficiency. Hence,minimizing average spike activity while\npreserving accuracy indeep SNNs remains a significant challenge and\nopportunity.This paper presents a non-iterative SNN training technique\nthatachieves ultra-high compression with reduced spiking activitywhile\nmaintaining high inference accuracy. In particular, our framework first uses\nthe attention-maps of an un compressed meta-model to yield compressed ANNs.\nThis step can be tuned to support both irregular and structured channel pruning\nto leverage computational benefits over a broad range of platforms. The\nframework then performs sparse-learning-based supervised SNN training using\ndirect inputs. During the training, it jointly optimizes the SNN weight,\nthreshold, and leak parameters to drastically minimize the number of time steps\nrequired while retaining compression. To evaluate the merits of our approach,\nwe performed experiments with variants of VGG and ResNet, on both CIFAR-10 and\nCIFAR-100, and VGG16 on Tiny-ImageNet.The SNN models generated through the\nproposed technique yield SOTA compression ratios of up to 33.4x with no\nsignificant drops in accuracy compared to baseline unpruned counterparts.\nCompared to existing SNN pruning methods, we achieve up to 8.3x higher\ncompression with improved accuracy.",
          "link": "http://arxiv.org/abs/2107.12445",
          "publishedOn": "2021-07-28T02:02:33.598Z",
          "wordCount": null,
          "title": "Towards Low-Latency Energy-Efficient Deep SNNs via Attention-Guided Compression. (arXiv:2107.12445v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12636",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Fengxiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Detection transformers have recently shown promising object detection results\nand attracted increasing attention. However, how to develop effective domain\nadaptation techniques to improve its cross-domain performance remains\nunexplored and unclear. In this paper, we delve into this topic and empirically\nfind that direct feature distribution alignment on the CNN backbone only brings\nlimited improvements, as it does not guarantee domain-invariant sequence\nfeatures in the transformer for prediction. To address this issue, we propose a\nnovel Sequence Feature Alignment (SFA) method that is specially designed for\nthe adaptation of detection transformers. Technically, SFA consists of a domain\nquery-based feature alignment (DQFA) module and a token-wise feature alignment\n(TDA) module. In DQFA, a novel domain query is used to aggregate and align\nglobal context from the token sequence of both domains. DQFA reduces the domain\ndiscrepancy in global feature representations and object relations when\ndeploying in the transformer encoder and decoder, respectively. Meanwhile, TDA\naligns token features in the sequence from both domains, which reduces the\ndomain gaps in local and instance-level feature representations in the\ntransformer encoder and decoder, respectively. Besides, a novel bipartite\nmatching consistency loss is proposed to enhance the feature discriminability\nfor robust object detection. Experiments on three challenging benchmarks show\nthat SFA outperforms state-of-the-art domain adaptive object detection methods.\nCode has been made available at: https://github.com/encounter1997/SFA.",
          "link": "http://arxiv.org/abs/2107.12636",
          "publishedOn": "2021-07-28T02:02:33.591Z",
          "wordCount": null,
          "title": "Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koren_M/0/1/0/all/0/1\">Mark Koren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassar_A/0/1/0/all/0/1\">Ahmed Nassar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel J. Kochenderfer</a>",
          "description": "Validating the safety of autonomous systems generally requires the use of\nhigh-fidelity simulators that adequately capture the variability of real-world\nscenarios. However, it is generally not feasible to exhaustively search the\nspace of simulation scenarios for failures. Adaptive stress testing (AST) is a\nmethod that uses reinforcement learning to find the most likely failure of a\nsystem. AST with a deep reinforcement learning solver has been shown to be\neffective in finding failures across a range of different systems. This\napproach generally involves running many simulations, which can be very\nexpensive when using a high-fidelity simulator. To improve efficiency, we\npresent a method that first finds failures in a low-fidelity simulator. It then\nuses the backward algorithm, which trains a deep neural network policy using a\nsingle expert demonstration, to adapt the low-fidelity failures to\nhigh-fidelity. We have created a series of autonomous vehicle validation case\nstudies that represent some of the ways low-fidelity and high-fidelity\nsimulators can differ, such as time discretization. We demonstrate in a variety\nof case studies that this new AST approach is able to find failures with\nsignificantly fewer high-fidelity simulation steps than are needed when just\nrunning AST directly in high-fidelity. As a proof of concept, we also\ndemonstrate AST on NVIDIA's DriveSim simulator, an industry state-of-the-art\nhigh-fidelity simulator for finding failures in autonomous vehicles.",
          "link": "http://arxiv.org/abs/2107.12940",
          "publishedOn": "2021-07-28T02:02:33.548Z",
          "wordCount": null,
          "title": "Finding Failures in High-Fidelity Simulation using Adaptive Stress Testing and the Backward Algorithm. (arXiv:2107.12940v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12547",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoyt_C/0/1/0/all/0/1\">Christopher R. Hoyt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owen_A/0/1/0/all/0/1\">Art B. Owen</a>",
          "description": "We use graphical methods to probe neural nets that classify images. Plots of\nt-SNE outputs at successive layers in a network reveal increasingly organized\narrangement of the data points. They can also reveal how a network can diminish\nor even forget about within-class structure as the data proceeds through\nlayers. We use class-specific analogues of principal components to visualize\nhow succeeding layers separate the classes. These allow us to sort images from\na given class from most typical to least typical (in the data) and they also\nserve as very useful projection coordinates for data visualization. We find\nthem especially useful when defining versions guided tours for animated data\nvisualization.",
          "link": "http://arxiv.org/abs/2107.12547",
          "publishedOn": "2021-07-28T02:02:33.516Z",
          "wordCount": null,
          "title": "Probing neural networks with t-SNE, class-specific projections and a guided tour. (arXiv:2107.12547v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yezhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_T/0/1/0/all/0/1\">Tong Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>",
          "description": "Confidence calibration is of great importance to the reliability of decisions\nmade by machine learning systems. However, discriminative classifiers based on\ndeep neural networks are often criticized for producing overconfident\npredictions that fail to reflect the true correctness likelihood of\nclassification accuracy. We argue that such an inability to model uncertainty\nis mainly caused by the closed-world nature in softmax: a model trained by the\ncross-entropy loss will be forced to classify input into one of $K$ pre-defined\ncategories with high probability. To address this problem, we for the first\ntime propose a novel $K$+1-way softmax formulation, which incorporates the\nmodeling of open-world uncertainty as the extra dimension. To unify the\nlearning of the original $K$-way classification task and the extra dimension\nthat models uncertainty, we propose a novel energy-based objective function,\nand moreover, theoretically prove that optimizing such an objective essentially\nforces the extra dimension to capture the marginal data distribution. Extensive\nexperiments show that our approach, Energy-based Open-World Softmax\n(EOW-Softmax), is superior to existing state-of-the-art methods in improving\nconfidence calibration.",
          "link": "http://arxiv.org/abs/2107.12628",
          "publishedOn": "2021-07-28T02:02:33.502Z",
          "wordCount": null,
          "title": "Energy-Based Open-World Uncertainty Modeling for Confidence Calibration. (arXiv:2107.12628v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qingyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boshen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiayi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>",
          "description": "Recently, the problem of inaccurate learning targets in crowd counting draws\nincreasing attention. Inspired by a few pioneering work, we solve this problem\nby trying to predict the indices of pre-defined interval bins of counts instead\nof the count values themselves. However, an inappropriate interval setting\nmight make the count error contributions from different intervals extremely\nimbalanced, leading to inferior counting performance. Therefore, we propose a\nnovel count interval partition criterion called Uniform Error Partition (UEP),\nwhich always keeps the expected counting error contributions equal for all\nintervals to minimize the prediction risk. Then to mitigate the inevitably\nintroduced discretization errors in the count quantization process, we propose\nanother criterion called Mean Count Proxies (MCP). The MCP criterion selects\nthe best count proxy for each interval to represent its count value during\ninference, making the overall expected discretization error of an image nearly\nnegligible. As far as we are aware, this work is the first to delve into such a\nclassification task and ends up with a promising solution for count interval\npartition. Following the above two theoretically demonstrated criterions, we\npropose a simple yet effective model termed Uniform Error Partition Network\n(UEPNet), which achieves state-of-the-art performance on several challenging\ndatasets. The codes will be available at:\nhttps://github.com/TencentYoutuResearch/CrowdCounting-UEPNet.",
          "link": "http://arxiv.org/abs/2107.12619",
          "publishedOn": "2021-07-28T02:02:33.499Z",
          "wordCount": null,
          "title": "Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting. (arXiv:2107.12619v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Menculini_L/0/1/0/all/0/1\">Lorenzo Menculini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marini_A/0/1/0/all/0/1\">Andrea Marini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proietti_M/0/1/0/all/0/1\">Massimiliano Proietti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garinei_A/0/1/0/all/0/1\">Alberto Garinei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozza_A/0/1/0/all/0/1\">Alessio Bozza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moretti_C/0/1/0/all/0/1\">Cecilia Moretti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marconi_M/0/1/0/all/0/1\">Marcello Marconi</a>",
          "description": "Setting sale prices correctly is of great importance for firms, and the study\nand forecast of prices time series is therefore a relevant topic not only from\na data science perspective but also from an economic and applicative one. In\nthis paper we exhamine different techniques to forecast the sale prices of\nthree food products applied by an Italian food wholesaler, as a step towards\nthe automation of pricing tasks usually taken care by human workforce. We\nconsider ARIMA models and compare them to Prophet, a scalable forecasting tool\ndeveloped by Facebook and based on a generalized additive model, and to deep\nlearning models based on Long Short--Term Memory (LSTM) and Convolutional\nNeural Networks (CNNs). ARIMA models are frequently used in econometric\nanalyses, providing a good bechmark for the problem under study. Our results\nindicate that ARIMA performs similarly to LSTM neural networks for the problem\nunder study, while the combination of CNNs and LSTMs attains the best overall\naccuracy, but requires more time to be tuned. On the contrary, Prophet is very\nfast to use, but less accurate.",
          "link": "http://arxiv.org/abs/2107.12770",
          "publishedOn": "2021-07-28T02:02:33.496Z",
          "wordCount": null,
          "title": "Comparing Prophet and Deep Learning to ARIMA in Forecasting Wholesale Food Prices. (arXiv:2107.12770v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Lesley Tan</a>",
          "description": "In this paper, we investigate data-driven parameterized modeling of insertion\nloss for transmission lines with respect to design parameters. We first show\nthat direct application of neural networks can lead to non-physics models with\nnegative insertion loss. To mitigate this problem, we propose two deep learning\nsolutions. One solution is to add a regulation term, which represents the\npassive condition, to the final loss function to enforce the negative quantity\nof insertion loss. In the second method, a third-order polynomial expression is\ndefined first, which ensures positiveness, to approximate the insertion loss,\nthen DeepONet neural network structure, which was proposed recently for\nfunction and system modeling, was employed to model the coefficients of\npolynomials. The resulting neural network is applied to predict the\ncoefficients of the polynomial expression. The experimental results on an\nopen-sourced SI/PI database of a PCB design show that both methods can ensure\nthe positiveness for the insertion loss. Furthermore, both methods can achieve\nsimilar prediction results, while the polynomial-based DeepONet method is\nfaster than DeepONet based method in training time.",
          "link": "http://arxiv.org/abs/2107.12527",
          "publishedOn": "2021-07-28T02:02:33.494Z",
          "wordCount": null,
          "title": "Physics-Enforced Modeling for Insertion Loss of Transmission Lines by Deep Neural Networks. (arXiv:2107.12527v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghu_M/0/1/0/all/0/1\">Maithra Raghu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1\">Jon Kleinberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1\">Samy Bengio</a>",
          "description": "The successes of deep learning critically rely on the ability of neural\nnetworks to output meaningful predictions on unseen data -- generalization. Yet\ndespite its criticality, there remain fundamental open questions on how neural\nnetworks generalize. How much do neural networks rely on memorization -- seeing\nhighly similar training examples -- and how much are they capable of\nhuman-intelligence styled reasoning -- identifying abstract rules underlying\nthe data? In this paper we introduce a novel benchmark, Pointer Value Retrieval\n(PVR) tasks, that explore the limits of neural network generalization. While\nPVR tasks can consist of visual as well as symbolic inputs, each with varying\nlevels of difficulty, they all have a simple underlying rule. One part of the\nPVR task input acts as a pointer, giving the location of a different part of\nthe input, which forms the value (and output). We demonstrate that this task\nstructure provides a rich testbed for understanding generalization, with our\nempirical study showing large variations in neural network performance based on\ndataset size, task complexity and model architecture. The interaction of\nposition, values and the pointer rule also allow the development of nuanced\ntests of generalization, by introducing distribution shift and increasing\nfunctional complexity. These reveal both subtle failures and surprising\nsuccesses, suggesting many promising directions of exploration on this\nbenchmark.",
          "link": "http://arxiv.org/abs/2107.12580",
          "publishedOn": "2021-07-28T02:02:33.449Z",
          "wordCount": null,
          "title": "Pointer Value Retrieval: A new benchmark for understanding the limits of neural network generalization. (arXiv:2107.12580v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raumanns_R/0/1/0/all/0/1\">Ralf Raumanns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schouten_G/0/1/0/all/0/1\">Gerard Schouten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joosten_M/0/1/0/all/0/1\">Max Joosten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pluim_J/0/1/0/all/0/1\">Josien P. W. Pluim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>",
          "description": "We present ENHANCE, an open dataset with multiple annotations to complement\nthe existing ISIC and PH2 skin lesion classification datasets. This dataset\ncontains annotations of visual ABC (asymmetry, border, colour) features from\nnon-expert annotation sources: undergraduate students, crowd workers from\nAmazon MTurk and classic image processing algorithms. In this paper we first\nanalyse the correlations between the annotations and the diagnostic label of\nthe lesion, as well as study the agreement between different annotation\nsources. Overall we find weak correlations of non-expert annotations with the\ndiagnostic label, and low agreement between different annotation sources. We\nthen study multi-task learning (MTL) with the annotations as additional labels,\nand show that non-expert annotations can improve (ensembles of)\nstate-of-the-art convolutional neural networks via MTL. We hope that our\ndataset can be used in further research into multiple annotations and/or MTL.\nAll data and models are available on Github:\nhttps://github.com/raumannsr/ENHANCE.",
          "link": "http://arxiv.org/abs/2107.12734",
          "publishedOn": "2021-07-28T02:02:33.448Z",
          "wordCount": null,
          "title": "ENHANCE (ENriching Health data by ANnotations of Crowd and Experts): A case study for skin lesion classification. (arXiv:2107.12734v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13493",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kabeli_O/0/1/0/all/0/1\">Ori Kabeli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1\">Zhenyu Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_B/0/1/0/all/0/1\">Buye Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1\">Anurag Kumar</a>",
          "description": "Deep neural networks have recently shown great success in the task of blind\nsource separation, both under monaural and binaural settings. Although these\nmethods were shown to produce high-quality separations, they were mainly\napplied under offline settings, in which the model has access to the full input\nsignal while separating the signal. In this study, we convert a non-causal\nstate-of-the-art separation model into a causal and real-time model and\nevaluate its performance under both online and offline settings. We compare the\nperformance of the proposed model to several baseline methods under anechoic,\nnoisy, and noisy-reverberant recording conditions while exploring both monaural\nand binaural inputs and outputs. Our findings shed light on the relative\ndifference between causal and non-causal models when performing separation. Our\nstateful implementation for online separation leads to a minor drop in\nperformance compared to the offline model; 0.8dB for monaural inputs and 0.3dB\nfor binaural inputs while reaching a real-time factor of 0.65. Samples can be\nfound under the following link:\nhttps://kwanum.github.io/sagrnnc-stream-results/.",
          "link": "http://arxiv.org/abs/2106.13493",
          "publishedOn": "2021-07-28T02:02:33.412Z",
          "wordCount": 649,
          "title": "Online Self-Attentive Gated RNNs for Real-Time Speaker Separation. (arXiv:2106.13493v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zandieh_A/0/1/0/all/0/1\">Amir Zandieh</a>",
          "description": "The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely wide\nneural nets trained under least squares loss by gradient descent. However,\ndespite its importance, the super-quadratic runtime of kernel methods limits\nthe use of NTK in large-scale learning tasks. To accelerate kernel machines\nwith NTK, we propose a near input sparsity time algorithm that maps the input\ndata to a randomized low-dimensional feature space so that the inner product of\nthe transformed data approximates their NTK evaluation. Our transformation\nworks by sketching the polynomial expansions of arc-cosine kernels.\nFurthermore, we propose a feature map for approximating the convolutional\ncounterpart of the NTK, which can transform any image using a runtime that is\nonly linear in the number of pixels. We show that in standard large-scale\nregression and classification tasks a linear regressor trained on our features\noutperforms trained Neural Nets and Nystrom approximation of NTK kernel.",
          "link": "http://arxiv.org/abs/2104.00415",
          "publishedOn": "2021-07-28T02:02:33.403Z",
          "wordCount": 611,
          "title": "Learning with Neural Tangent Kernels in Near Input Sparsity Time. (arXiv:2104.00415v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhirui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zekun Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yabang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_A/0/1/0/all/0/1\">Andrew Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>",
          "description": "3D deep learning has been increasingly more popular for a variety of tasks\nincluding many safety-critical applications. However, recently several works\nraise the security issues of 3D deep nets. Although most of these works\nconsider adversarial attacks, we identify that backdoor attack is indeed a more\nserious threat to 3D deep learning systems but remains unexplored. We present\nthe backdoor attacks in 3D with a unified framework that exploits the unique\nproperties of 3D data and networks. In particular, we design two attack\napproaches: the poison-label attack and the clean-label attack. The first one\nis straightforward and effective in practice, while the second one is more\nsophisticated assuming there are certain data inspections. The attack\nalgorithms are mainly motivated and developed by 1) the recent discovery of 3D\nadversarial samples which demonstrate the vulnerability of 3D deep nets under\nspatial transformations; 2) the proposed feature disentanglement technique that\nmanipulates the feature of the data through optimization methods and its\npotential to embed a new task. Extensive experiments show the efficacy of the\npoison-label attack with over 95% success rate across several 3D datasets and\nmodels, and the ability of clean-label attack against data filtering with\naround 50% success rate. Our proposed backdoor attack in 3D point cloud is\nexpected to perform as a baseline for improving the robustness of 3D deep\nmodels.",
          "link": "http://arxiv.org/abs/2103.16074",
          "publishedOn": "2021-07-28T02:02:33.391Z",
          "wordCount": 702,
          "title": "PointBA: Towards Backdoor Attacks in 3D Point Cloud. (arXiv:2103.16074v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chernyak_B/0/1/0/all/0/1\">Bronya Roni Chernyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazan_T/0/1/0/all/0/1\">Tamir Hazan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keshet_J/0/1/0/all/0/1\">Joseph Keshet</a>",
          "description": "This paper proposes an attack-independent (non-adversarial training)\ntechnique for improving adversarial robustness of neural network models, with\nminimal loss of standard accuracy. We suggest creating a neighborhood around\neach training example, such that the label is kept constant for all inputs\nwithin that neighborhood. Unlike previous work that follows a similar\nprinciple, we apply this idea by extending the training set with multiple\nperturbations for each training example, drawn from within the neighborhood.\nThese perturbations are model independent, and remain constant throughout the\nentire training process. We analyzed our method empirically on MNIST, SVHN, and\nCIFAR-10, under different attacks and conditions. Results suggest that the\nproposed approach improves standard accuracy over other defenses while having\nincreased robustness compared to vanilla adversarial training.",
          "link": "http://arxiv.org/abs/2103.08265",
          "publishedOn": "2021-07-28T02:02:33.371Z",
          "wordCount": 594,
          "title": "Constant Random Perturbations Provide Adversarial Robustness with Minimal Effect on Accuracy. (arXiv:2103.08265v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12654",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Da-Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>",
          "description": "Traditional learning systems are trained in closed-world for a fixed number\nof classes, and need pre-collected datasets in advance. However, new classes\noften emerge in real-world applications and should be learned incrementally.\nFor example, in electronic commerce, new types of products appear daily, and in\na social media community, new topics emerge frequently. Under such\ncircumstances, incremental models should learn several new classes at a time\nwithout forgetting. We find a strong correlation between old and new classes in\nincremental learning, which can be applied to relate and facilitate different\nlearning stages mutually. As a result, we propose CO-transport for class\nIncremental Learning (COIL), which learns to relate across incremental tasks\nwith the class-wise semantic relationship. In detail, co-transport has two\naspects: prospective transport tries to augment the old classifier with optimal\ntransported knowledge as fast model adaptation. Retrospective transport aims to\ntransport new class classifiers backward as old ones to overcome forgetting.\nWith these transports, COIL efficiently adapts to new tasks, and stably resists\nforgetting. Experiments on benchmark and real-world multimedia datasets\nvalidate the effectiveness of our proposed method.",
          "link": "http://arxiv.org/abs/2107.12654",
          "publishedOn": "2021-07-28T02:02:33.338Z",
          "wordCount": 616,
          "title": "Co-Transport for Class-Incremental Learning. (arXiv:2107.12654v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dam_T/0/1/0/all/0/1\">Tanmoy Dam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anavatti_S/0/1/0/all/0/1\">Sreenatha G. Anavatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbass_H/0/1/0/all/0/1\">Hussein A. Abbass</a> (Fellow, IEEESchool of Engineering and Information Technology, University of New South Wales Canberra, Australia)",
          "description": "The Latent Space Clustering in Generative adversarial networks (ClusterGAN)\nmethod has been successful with high-dimensional data. However, the method\nassumes uniformlydistributed priors during the generation of modes, which isa\nrestrictive assumption in real-world data and cause loss ofdiversity in the\ngenerated modes. In this paper, we proposeself-augmentation information\nmaximization improved Clus-terGAN (SIMI-ClusterGAN) to learn the distinctive\npriorsfrom the data. The proposed SIMI-ClusterGAN consists offour deep neural\nnetworks: self-augmentation prior network,generator, discriminator and\nclustering inference autoencoder.The proposed method has been validated using\nseven bench-mark data sets and has shown improved performance overstate-of-the\nart methods. To demonstrate the superiority ofSIMI-ClusterGAN performance on\nimbalanced dataset, wehave discussed two imbalanced conditions on MNIST\ndatasetswith one-class imbalance and three classes imbalanced cases.The results\nhighlight the advantages of SIMI-ClusterGAN.",
          "link": "http://arxiv.org/abs/2107.12706",
          "publishedOn": "2021-07-28T02:02:33.331Z",
          "wordCount": 582,
          "title": "Improving ClusterGAN Using Self-AugmentedInformation Maximization of Disentangling LatentSpaces. (arXiv:2107.12706v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kosman_E/0/1/0/all/0/1\">Eitan Kosman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Dotan Di Castro</a>",
          "description": "Autonomous driving gained huge traction in recent years, due to its potential\nto change the way we commute. Much effort has been put into trying to estimate\nthe state of a vehicle. Meanwhile, learning to forecast the state of a vehicle\nahead introduces new capabilities, such as predicting dangerous situations.\nMoreover, forecasting brings new supervision opportunities by learning to\npredict richer a context, expressed by multiple horizons. Intuitively, a video\nstream originated from a front-facing camera is necessary because it encodes\ninformation about the upcoming road. Besides, historical traces of the\nvehicle's states give more context. In this paper, we tackle multi-horizon\nforecasting of vehicle states by fusing the two modalities. We design and\nexperiment with 3 end-to-end architectures that exploit 3D convolutions for\nvisual features extraction and 1D convolutions for features extraction from\nspeed and steering angle traces. To demonstrate the effectiveness of our\nmethod, we perform extensive experiments on two publicly available real-world\ndatasets, Comma2k19 and the Udacity challenge. We show that we are able to\nforecast a vehicle's state to various horizons, while outperforming the current\nstate-of-the-art results on the related task of driving state estimation. We\nexamine the contribution of vision features, and find that a model fed with\nvision features achieves an error that is 56.6% and 66.9% of the error of a\nmodel that doesn't use those features, on the Udacity and Comma2k19 datasets\nrespectively.",
          "link": "http://arxiv.org/abs/2107.12674",
          "publishedOn": "2021-07-28T02:02:33.323Z",
          "wordCount": 672,
          "title": "Vision-Guided Forecasting -- Visual Context for Multi-Horizon Time Series Forecasting. (arXiv:2107.12674v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pasandi_H/0/1/0/all/0/1\">Hannaneh Barahouei Pasandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_T/0/1/0/all/0/1\">Tamer Nadeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirpour_H/0/1/0/all/0/1\">Hadi Amirpour</a>",
          "description": "Over the last decade, the bandwidth expansion and MU-MIMO spectral efficiency\nhave promised to increase data throughput by allowing concurrent communication\nbetween one Access Point and multiple users. However, we are still a long way\nfrom enjoying such MU-MIMO MAC protocol improvements for bandwidth hungry\napplications such as video streaming in practical WiFi network settings due to\nheterogeneous channel conditions and devices, unreliable transmissions, and\nlack of useful feedback exchange among the lower and upper layers'\nrequirements. This paper introduces MuViS, a novel dual-phase optimization\nframework that proposes a Quality of Experience (QoE) aware MU-MIMO\noptimization for multi-user video streaming over IEEE 802.11ac. MuViS first\nemploys reinforcement learning to optimize the MU-MIMO user group and mode\nselection for users based on their PHY/MAC layer characteristics. The video\nbitrate is then optimized based on the user's mode (Multi-User (MU) or\nSingle-User (SU)). We present our design and its evaluation on smartphones and\nlaptops using 802.11ac WiFi. Our experimental results in various indoor\nenvironments and configurations show a scalable framework that can support a\nlarge number of users with streaming at high video rates and satisfying QoE\nrequirements.",
          "link": "http://arxiv.org/abs/2106.15262",
          "publishedOn": "2021-07-28T02:02:33.313Z",
          "wordCount": 637,
          "title": "MU-MIMO Grouping For Real-time Applications. (arXiv:2106.15262v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sohee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungkyu Lee</a>",
          "description": "Continual learning is a concept of online learning with multiple sequential\ntasks. One of the critical barriers of continual learning is that a network\nshould learn a new task keeping the knowledge of old tasks without access to\nany data of the old tasks. In this paper, we propose a neuron activation\nimportance-based regularization method for stable continual learning regardless\nof the order of tasks. We conduct comprehensive experiments on existing\nbenchmark data sets to evaluate not just the stability and plasticity of our\nmethod with improved classification accuracy also the robustness of the\nperformance along the changes of task order.",
          "link": "http://arxiv.org/abs/2107.12657",
          "publishedOn": "2021-07-28T02:02:33.294Z",
          "wordCount": 529,
          "title": "Continual Learning with Neuron Activation Importance. (arXiv:2107.12657v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pahar_M/0/1/0/all/0/1\">Madhurananda Pahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niesler_T/0/1/0/all/0/1\">Thomas Niesler</a>",
          "description": "We present an experimental investigation into the automatic detection of\nCOVID-19 from coughs, breaths and speech as this type of screening is\nnon-contact, does not require specialist medical expertise or laboratory\nfacilities and can easily be deployed on inexpensive consumer hardware.\n\nSmartphone recordings of cough, breath and speech from subjects around the\nglobe are used for classification by seven standard machine learning\nclassifiers using leave-$p$-out cross-validation to provide a promising\nbaseline performance.\n\nThen, a diverse dataset of 10.29 hours of cough, sneeze, speech and noise\naudio recordings are used to pre-train a CNN, LSTM and Resnet50 classifier and\nfine tuned the model to enhance the performance even further.\n\nWe have also extracted the bottleneck features from these pre-trained models\nby removing the final-two layers and used them as an input to the LR, SVM, MLP\nand KNN classifiers to detect COVID-19 signature.\n\nThe highest AUC of 0.98 was achieved using a transfer learning based Resnet50\narchitecture on coughs from Coswara dataset.\n\nThe highest AUC of 0.94 and 0.92 was achieved from an SVM run on the\nbottleneck features extracted from the breaths from Coswara dataset and speech\nrecordings from ComParE dataset.\n\nWe conclude that among all vocal audio, coughs carry the strongest COVID-19\nsignature followed by breath and speech and using transfer learning improves\nthe classifier performance with higher AUC and lower variance across the\ncross-validation folds.\n\nAlthough these signatures are not perceivable by human ear, machine learning\nbased COVID-19 detection is possible from vocal audio recorded via smartphone.",
          "link": "http://arxiv.org/abs/2104.02477",
          "publishedOn": "2021-07-28T02:02:33.288Z",
          "wordCount": 775,
          "title": "Deep Transfer Learning based COVID-19 Detection in Cough, Breath and Speech using Bottleneck Features. (arXiv:2104.02477v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12631",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+He_J/0/1/0/all/0/1\">Jiguang He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wymeersch_H/0/1/0/all/0/1\">Henk Wymeersch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Renzo_M/0/1/0/all/0/1\">Marco Di Renzo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Juntti_M/0/1/0/all/0/1\">Markku Juntti</a>",
          "description": "Inspired by the remarkable learning and prediction performance of deep neural\nnetworks (DNNs), we apply one special type of DNN framework, known as\nmodel-driven deep unfolding neural network, to reconfigurable intelligent\nsurface (RIS)-aided millimeter wave (mmWave) single-input multiple-output\n(SIMO) systems. We focus on uplink cascaded channel estimation, where known and\nfixed base station combining and RIS phase control matrices are considered for\ncollecting observations. To boost the estimation performance and reduce the\ntraining overhead, the inherent channel sparsity of mmWave channels is\nleveraged in the deep unfolding method. It is verified that the proposed deep\nunfolding network architecture can outperform the least squares (LS) method\nwith a relatively smaller training overhead and online computational\ncomplexity.",
          "link": "http://arxiv.org/abs/2107.12631",
          "publishedOn": "2021-07-28T02:02:33.281Z",
          "wordCount": 562,
          "title": "Learning to Estimate RIS-Aided mmWave Channels. (arXiv:2107.12631v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tong_A/0/1/0/all/0/1\">Alexander Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huguet_G/0/1/0/all/0/1\">Guillaume Huguet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natik_A/0/1/0/all/0/1\">Amine Natik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacDonald_K/0/1/0/all/0/1\">Kincaid MacDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuchroo_M/0/1/0/all/0/1\">Manik Kuchroo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coifman_R/0/1/0/all/0/1\">Ronald Coifman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1\">Guy Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_S/0/1/0/all/0/1\">Smita Krishnaswamy</a>",
          "description": "We propose a new fast method of measuring distances between large numbers of\nrelated high dimensional datasets called the Diffusion Earth Mover's Distance\n(EMD). We model the datasets as distributions supported on common data graph\nthat is derived from the affinity matrix computed on the combined data. In such\ncases where the graph is a discretization of an underlying Riemannian closed\nmanifold, we prove that Diffusion EMD is topologically equivalent to the\nstandard EMD with a geodesic ground distance. Diffusion EMD can be computed in\n$\\tilde{O}(n)$ time and is more accurate than similarly fast algorithms such as\ntree-based EMDs. We also show Diffusion EMD is fully differentiable, making it\namenable to future uses in gradient-descent frameworks such as deep neural\nnetworks. Finally, we demonstrate an application of Diffusion EMD to single\ncell data collected from 210 COVID-19 patient samples at Yale New Haven\nHospital. Here, Diffusion EMD can derive distances between patients on the\nmanifold of cells at least two orders of magnitude faster than equally accurate\nmethods. This distance matrix between patients can be embedded into a higher\nlevel patient manifold which uncovers structure and heterogeneity in patients.\nMore generally, Diffusion EMD is applicable to all datasets that are massively\ncollected in parallel in many medical and biological systems.",
          "link": "http://arxiv.org/abs/2102.12833",
          "publishedOn": "2021-07-28T02:02:33.274Z",
          "wordCount": 727,
          "title": "Diffusion Earth Mover's Distance and Distribution Embeddings. (arXiv:2102.12833v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>",
          "description": "We propose using self-supervised discrete representations for the task of\nspeech resynthesis. To generate disentangled representation, we separately\nextract low-bitrate representations for speech content, prosodic information,\nand speaker identity. This allows to synthesize speech in a controllable\nmanner. We analyze various state-of-the-art, self-supervised representation\nlearning methods and shed light on the advantages of each method while\nconsidering reconstruction quality and disentanglement properties.\nSpecifically, we evaluate the F0 reconstruction, speaker identification\nperformance (for both resynthesis and voice conversion), recordings'\nintelligibility, and overall quality using subjective human evaluation. Lastly,\nwe demonstrate how these representations can be used for an ultra-lightweight\nspeech codec. Using the obtained representations, we can get to a rate of 365\nbits per second while providing better speech quality than the baseline\nmethods. Audio samples can be found under the following link:\nspeechbot.github.io/resynthesis.",
          "link": "http://arxiv.org/abs/2104.00355",
          "publishedOn": "2021-07-28T02:02:33.266Z",
          "wordCount": 627,
          "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations. (arXiv:2104.00355v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1\">Zhiyong Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yixuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Huihua Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1\">Hsiao-Dong Chiang</a>",
          "description": "Recent progress on deep learning relies heavily on the quality and efficiency\nof training algorithms. In this paper, we develop a fast training method\nmotivated by the nonlinear Conjugate Gradient (CG) framework. We propose the\nConjugate Gradient with Quadratic line-search (CGQ) method. On the one hand, a\nquadratic line-search determines the step size according to current loss\nlandscape. On the other hand, the momentum factor is dynamically updated in\ncomputing the conjugate gradient parameter (like Polak-Ribiere). Theoretical\nresults to ensure the convergence of our method in strong convex settings is\ndeveloped. And experiments in image classification datasets show that our\nmethod yields faster convergence than other local solvers and has better\ngeneralization capability (test set accuracy). One major advantage of the paper\nmethod is that tedious hand tuning of hyperparameters like the learning rate\nand momentum is avoided.",
          "link": "http://arxiv.org/abs/2106.11548",
          "publishedOn": "2021-07-28T02:02:33.247Z",
          "wordCount": 603,
          "title": "Adaptive Learning Rate and Momentum for Training Deep Neural Networks. (arXiv:2106.11548v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hayes_B/0/1/0/all/0/1\">Ben Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saitis_C/0/1/0/all/0/1\">Charalampos Saitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Fazekas</a>",
          "description": "We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully\ncausal approach to neural audio synthesis which operates directly in the\nwaveform domain, with an accompanying optimisation (FastNEWT) for efficient CPU\ninference. The NEWT uses time-distributed multilayer perceptrons with periodic\nactivations to implicitly learn nonlinear transfer functions that encode the\ncharacteristics of a target timbre. Once trained, a NEWT can produce complex\ntimbral evolutions by simple affine transformations of its input and output\nsignals. We paired the NEWT with a differentiable noise synthesiser and reverb\nand found it capable of generating realistic musical instrument performances\nwith only 260k total model parameters, conditioned on F0 and loudness features.\nWe compared our method to state-of-the-art benchmarks with a multi-stimulus\nlistening test and the Fr\\'echet Audio Distance and found it performed\ncompetitively across the tested timbral domains. Our method significantly\noutperformed the benchmarks in terms of generation speed, and achieved\nreal-time performance on a consumer CPU, both with and without FastNEWT,\nsuggesting it is a viable basis for future creative sound design tools.",
          "link": "http://arxiv.org/abs/2107.05050",
          "publishedOn": "2021-07-28T02:02:33.240Z",
          "wordCount": 631,
          "title": "Neural Waveshaping Synthesis. (arXiv:2107.05050v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1\">Hunter Lang</a>",
          "description": "Deep learning has proven effective for various application tasks, but its\napplicability is limited by the reliance on annotated examples. Self-supervised\nlearning has emerged as a promising direction to alleviate the supervision\nbottleneck, but existing work focuses on leveraging co-occurrences in unlabeled\ndata for task-agnostic representation learning, as exemplified by masked\nlanguage model pretraining. In this chapter, we explore task-specific\nself-supervision, which leverages domain knowledge to automatically annotate\nnoisy training examples for end applications, either by introducing labeling\nfunctions for annotating individual instances, or by imposing constraints over\ninterdependent label decisions. We first present deep probabilistic logic(DPL),\nwhich offers a unifying framework for task-specific self-supervision by\ncomposing probabilistic logic with deep learning. DPL represents unknown labels\nas latent variables and incorporates diverse self-supervision using\nprobabilistic logic to train a deep neural network end-to-end using variational\nEM. Next, we present self-supervised self-supervision(S4), which adds to DPL\nthe capability to learn new self-supervision automatically. Starting from an\ninitial seed self-supervision, S4 iteratively uses the deep neural network to\npropose new self supervision. These are either added directly (a form of\nstructured self-training) or verified by a human expert (as in feature-based\nactive learning). Experiments on real-world applications such as biomedical\nmachine reading and various text classification tasks show that task-specific\nself-supervision can effectively leverage domain expertise and often match the\naccuracy of supervised methods with a tiny fraction of human effort.",
          "link": "http://arxiv.org/abs/2107.12591",
          "publishedOn": "2021-07-28T02:02:33.232Z",
          "wordCount": 675,
          "title": "Combining Probabilistic Logic and Deep Learning for Self-Supervised Learning. (arXiv:2107.12591v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.09630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grassucci_E/0/1/0/all/0/1\">Eleonora Grassucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cicero_E/0/1/0/all/0/1\">Edoardo Cicero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comminiello_D/0/1/0/all/0/1\">Danilo Comminiello</a>",
          "description": "Latest Generative Adversarial Networks (GANs) are gathering outstanding\nresults through a large-scale training, thus employing models composed of\nmillions of parameters requiring extensive computational capabilities. Building\nsuch huge models undermines their replicability and increases the training\ninstability. Moreover, multi-channel data, such as images or audio, are usually\nprocessed by realvalued convolutional networks that flatten and concatenate the\ninput, often losing intra-channel spatial relations. To address these issues\nrelated to complexity and information loss, we propose a family of\nquaternion-valued generative adversarial networks (QGANs). QGANs exploit the\nproperties of quaternion algebra, e.g., the Hamilton product, that allows to\nprocess channels as a single entity and capture internal latent relations,\nwhile reducing by a factor of 4 the overall number of parameters. We show how\nto design QGANs and to extend the proposed approach even to advanced models.We\ncompare the proposed QGANs with real-valued counterparts on several image\ngeneration benchmarks. Results show that QGANs are able to obtain better FID\nscores than real-valued GANs and to generate visually pleasing images.\nFurthermore, QGANs save up to 75% of the training parameters. We believe these\nresults may pave the way to novel, more accessible, GANs capable of improving\nperformance and saving computational resources.",
          "link": "http://arxiv.org/abs/2104.09630",
          "publishedOn": "2021-07-28T02:02:33.225Z",
          "wordCount": 678,
          "title": "Quaternion Generative Adversarial Networks. (arXiv:2104.09630v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12532",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sorochan_K/0/1/0/all/0/1\">Kynan Sorochan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jerry Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yakun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1\">Matthew Guzdial</a>",
          "description": "Machine learning has been a popular tool in many different fields, including\nprocedural content generation. However, procedural content generation via\nmachine learning (PCGML) approaches can struggle with controllability and\ncoherence. In this paper, we attempt to address these problems by learning to\ngenerate human-like paths, and then generating levels based on these paths. We\nextract player path data from gameplay video, train an LSTM to generate new\npaths based on this data, and then generate game levels based on this path\ndata. We demonstrate that our approach leads to more coherent levels for the\ngame Lode Runner in comparison to an existing PCGML approach.",
          "link": "http://arxiv.org/abs/2107.12532",
          "publishedOn": "2021-07-28T02:02:33.218Z",
          "wordCount": 562,
          "title": "Generating Lode Runner Levels by Learning Player Paths with LSTMs. (arXiv:2107.12532v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadasivan_V/0/1/0/all/0/1\">Vinu Sankar Sadasivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1\">Anirban Dasgupta</a>",
          "description": "Curriculum learning is a training strategy that sorts the training examples\nby some measure of their difficulty and gradually exposes them to the learner\nto improve the network performance. Motivated by our insights from implicit\ncurriculum ordering, we first introduce a simple curriculum learning strategy\nthat uses statistical measures such as standard deviation and entropy values to\nscore the difficulty of data points for real image classification tasks. We\nempirically show its improvements in performance with convolutional and\nfully-connected neural networks on multiple real image datasets. We also\npropose and study the performance of a dynamic curriculum learning algorithm.\nOur dynamic curriculum algorithm tries to reduce the distance between the\nnetwork weight and an optimal weight at any training step by greedily sampling\nexamples with gradients that are directed towards the optimal weight. Further,\nwe use our algorithms to discuss why curriculum learning is helpful.",
          "link": "http://arxiv.org/abs/2103.00147",
          "publishedOn": "2021-07-28T02:02:33.211Z",
          "wordCount": 614,
          "title": "Statistical Measures For Defining Curriculum Scoring Function. (arXiv:2103.00147v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wenlong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mingbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zhiling Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shuhang Gu</a>",
          "description": "Generative adversarial networks (GANs) have promoted remarkable advances in\nsingle-image super-resolution (SR) by recovering photo-realistic images.\nHowever, high memory consumption of GAN-based SR (usually generators) causes\nperformance degradation and more energy consumption, hindering the deployment\nof GAN-based SR into resource-constricted mobile devices. In this paper, we\npropose a novel compression framework \\textbf{M}ulti-scale \\textbf{F}eature\n\\textbf{A}ggregation Net based \\textbf{GAN} (MFAGAN) for reducing the memory\naccess cost of the generator. First, to overcome the memory explosion of dense\nconnections, we utilize a memory-efficient multi-scale feature aggregation net\nas the generator. Second, for faster and more stable training, our method\nintroduces the PatchGAN discriminator. Third, to balance the student\ndiscriminator and the compressed generator, we distill both the generator and\nthe discriminator. Finally, we perform a hardware-aware neural architecture\nsearch (NAS) to find a specialized SubGenerator for the target mobile phone.\nBenefiting from these improvements, the proposed MFAGAN achieves up to\n\\textbf{8.3}$\\times$ memory saving and \\textbf{42.9}$\\times$ computation\nreduction, with only minor visual quality degradation, compared with ESRGAN.\nEmpirical studies also show $\\sim$\\textbf{70} milliseconds latency on Qualcomm\nSnapdragon 865 chipset.",
          "link": "http://arxiv.org/abs/2107.12679",
          "publishedOn": "2021-07-28T02:02:33.193Z",
          "wordCount": 610,
          "title": "MFAGAN: A Compression Framework for Memory-Efficient On-Device Super-Resolution GAN. (arXiv:2107.12679v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_B/0/1/0/all/0/1\">Byungjin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yu Xiao</a>",
          "description": "Vehicular fog computing (VFC) pushes the cloud computing capability to the\ndistributed fog nodes at the edge of the Internet, enabling compute-intensive\nand latency-sensitive computing services for vehicles through task offloading.\nHowever, a heterogeneous mobility environment introduces uncertainties in terms\nof resource supply and demand, which are inevitable bottlenecks for the optimal\noffloading decision. Also, these uncertainties bring extra challenges to task\noffloading under the oblivious adversary attack and data privacy risks. In this\narticle, we develop a new adversarial online learning algorithm with bandit\nfeedback based on the adversarial multi-armed bandit theory, to enable scalable\nand low-complexity offloading decision making. Specifically, we focus on\noptimizing fog node selection with the aim of minimizing the offloading service\ncosts in terms of delay and energy. The key is to implicitly tune the\nexploration bonus in the selection process and the assessment rules of the\ndesigned algorithm, taking into account volatile resource supply and demand. We\ntheoretically prove that the input-size dependent selection rule allows to\nchoose a suitable fog node without exploring the sub-optimal actions, and also\nan appropriate score patching rule allows to quickly adapt to evolving\ncircumstances, which reduce variance and bias simultaneously, thereby achieving\na better exploitation-exploration balance. Simulation results verify the\neffectiveness and robustness of the proposed algorithm.",
          "link": "http://arxiv.org/abs/2104.12827",
          "publishedOn": "2021-07-28T02:02:33.186Z",
          "wordCount": 700,
          "title": "Learning-based decentralized offloading decision making in an adversarial environment. (arXiv:2104.12827v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benaroya_L/0/1/0/all/0/1\">Laurent Benaroya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obin_N/0/1/0/all/0/1\">Nicolas Obin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roebel_A/0/1/0/all/0/1\">Axel Roebel</a>",
          "description": "Voice conversion (VC) consists of digitally altering the voice of an\nindividual to manipulate part of its content, primarily its identity, while\nmaintaining the rest unchanged. Research in neural VC has accomplished\nconsiderable breakthroughs with the capacity to falsify a voice identity using\na small amount of data with a highly realistic rendering. This paper goes\nbeyond voice identity and presents a neural architecture that allows the\nmanipulation of voice attributes (e.g., gender and age). Leveraging the latest\nadvances on adversarial learning of structured speech representation, a novel\nstructured neural network is proposed in which multiple auto-encoders are used\nto encode speech as a set of idealistically independent linguistic and\nextra-linguistic representations, which are learned adversariarly and can be\nmanipulated during VC. Moreover, the proposed architecture is time-synchronized\nso that the original voice timing is preserved during conversion which allows\nlip-sync applications. Applied to voice gender conversion on the real-world\nVCTK dataset, our proposed architecture can learn successfully\ngender-independent representation and convert the voice gender with a very high\nefficiency and naturalness.",
          "link": "http://arxiv.org/abs/2107.12346",
          "publishedOn": "2021-07-28T02:02:33.178Z",
          "wordCount": 637,
          "title": "Beyond Voice Identity Conversion: Manipulating Voice Attributes by Adversarial Learning of Structured Disentangled Representations. (arXiv:2107.12346v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Team_Open_Ended_Learning/0/1/0/all/0/1\">Open-Ended Learning Team</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stooke_A/0/1/0/all/0/1\">Adam Stooke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_A/0/1/0/all/0/1\">Anuj Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barros_C/0/1/0/all/0/1\">Catarina Barros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deck_C/0/1/0/all/0/1\">Charlie Deck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_J/0/1/0/all/0/1\">Jakob Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sygnowski_J/0/1/0/all/0/1\">Jakub Sygnowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trebacz_M/0/1/0/all/0/1\">Maja Trebacz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaderberg_M/0/1/0/all/0/1\">Max Jaderberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_M/0/1/0/all/0/1\">Michael Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAleese_N/0/1/0/all/0/1\">Nat McAleese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradley_Schmieg_N/0/1/0/all/0/1\">Nathalie Bradley-Schmieg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_N/0/1/0/all/0/1\">Nathaniel Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porcel_N/0/1/0/all/0/1\">Nicolas Porcel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1\">Roberta Raileanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_Fitt_S/0/1/0/all/0/1\">Steph Hughes-Fitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalibard_V/0/1/0/all/0/1\">Valentin Dalibard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czarnecki_W/0/1/0/all/0/1\">Wojciech Marian Czarnecki</a>",
          "description": "In this work we create agents that can perform well beyond a single,\nindividual task, that exhibit much wider generalisation of behaviour to a\nmassive, rich space of challenges. We define a universe of tasks within an\nenvironment domain and demonstrate the ability to train agents that are\ngenerally capable across this vast space and beyond. The environment is\nnatively multi-agent, spanning the continuum of competitive, cooperative, and\nindependent games, which are situated within procedurally generated physical 3D\nworlds. The resulting space is exceptionally diverse in terms of the challenges\nposed to agents, and as such, even measuring the learning progress of an agent\nis an open research problem. We propose an iterative notion of improvement\nbetween successive generations of agents, rather than seeking to maximise a\nsingular objective, allowing us to quantify progress despite tasks being\nincomparable in terms of achievable rewards. We show that through constructing\nan open-ended learning process, which dynamically changes the training task\ndistributions and training objectives such that the agent never stops learning,\nwe achieve consistent learning of new behaviours. The resulting agent is able\nto score reward in every one of our humanly solvable evaluation levels, with\nbehaviour generalising to many held-out points in the universe of tasks.\nExamples of this zero-shot generalisation include good performance on Hide and\nSeek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks\nwe characterise the behaviour of our agent, and find interesting emergent\nheuristic behaviours such as trial-and-error experimentation, simple tool use,\noption switching, and cooperation. Finally, we demonstrate that the general\ncapabilities of this agent could unlock larger scale transfer of behaviour\nthrough cheap finetuning.",
          "link": "http://arxiv.org/abs/2107.12808",
          "publishedOn": "2021-07-28T02:02:33.166Z",
          "wordCount": 738,
          "title": "Open-Ended Learning Leads to Generally Capable Agents. (arXiv:2107.12808v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gudovskiy_D/0/1/0/all/0/1\">Denis Gudovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishizaka_S/0/1/0/all/0/1\">Shun Ishizaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozuka_K/0/1/0/all/0/1\">Kazuki Kozuka</a>",
          "description": "Unsupervised anomaly detection with localization has many practical\napplications when labeling is infeasible and, moreover, when anomaly examples\nare completely missing in the train data. While recently proposed models for\nsuch data setup achieve high accuracy metrics, their complexity is a limiting\nfactor for real-time processing. In this paper, we propose a real-time model\nand analytically derive its relationship to prior methods. Our CFLOW-AD model\nis based on a conditional normalizing flow framework adopted for anomaly\ndetection with localization. In particular, CFLOW-AD consists of a\ndiscriminatively pretrained encoder followed by a multi-scale generative\ndecoders where the latter explicitly estimate likelihood of the encoded\nfeatures. Our approach results in a computationally and memory-efficient model:\nCFLOW-AD is faster and smaller by a factor of 10x than prior state-of-the-art\nwith the same input setting. Our experiments on the MVTec dataset show that\nCFLOW-AD outperforms previous methods by 0.36% AUROC in detection task, by\n1.12% AUROC and 2.5% AUPRO in localization task, respectively. We open-source\nour code with fully reproducible experiments.",
          "link": "http://arxiv.org/abs/2107.12571",
          "publishedOn": "2021-07-28T02:02:33.147Z",
          "wordCount": 620,
          "title": "CFLOW-AD: Real-Time Unsupervised Anomaly Detection with Localization via Conditional Normalizing Flows. (arXiv:2107.12571v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zisen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1\">Matthew Guzdial</a>",
          "description": "Co-creative Procedural Content Generation via Machine Learning (PCGML) refers\nto systems where a PCGML agent and a human work together to produce output\ncontent. One of the limitations of co-creative PCGML is that it requires\nco-creative training data for a PCGML agent to learn to interact with humans.\nHowever, acquiring this data is a difficult and time-consuming process. In this\nwork, we propose approximating human-AI interaction data and employing transfer\nlearning to adapt learned co-creative knowledge from one game to a different\ngame. We explore this approach for co-creative Zelda dungeon room generation.",
          "link": "http://arxiv.org/abs/2107.12533",
          "publishedOn": "2021-07-28T02:02:33.138Z",
          "wordCount": 545,
          "title": "Toward Co-creative Dungeon Generation via Transfer Learning. (arXiv:2107.12533v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jianxin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bing Yao</a>",
          "description": "The rapid developments in advanced sensing and imaging bring about a\ndata-rich environment, facilitating the effective modeling, monitoring, and\ncontrol of complex systems. For example, the body-sensor network captures\nmulti-channel information pertinent to the electrical activity of the heart\n(i.e., electrocardiograms (ECG)), which enables medical scientists to monitor\nand detect abnormal cardiac conditions. However, the high-dimensional sensing\ndata are generally complexly structured and realizing the full data potential\ndepends to a great extent on advanced analytical and predictive methods. This\npaper presents a physics-constrained deep learning (P-DL) framework for\nhigh-dimensional inverse ECG modeling. This method integrates the physical laws\nof the complex system with the advanced deep learning infrastructure for\neffective prediction of the system dynamics. The proposed P-DL approach is\nimplemented to solve the inverse ECG model and predict the time-varying\ndistribution of electric potentials in the heart from the ECG data measured by\nthe body-surface sensor network. Experimental results show that the proposed\nP-DL method significantly outperforms existing methods that are commonly used\nin current practice.",
          "link": "http://arxiv.org/abs/2107.12780",
          "publishedOn": "2021-07-28T02:02:33.131Z",
          "wordCount": 594,
          "title": "Physics-constrained Deep Learning for Robust Inverse ECG Modeling. (arXiv:2107.12780v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cromp_S/0/1/0/all/0/1\">Sonia Cromp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samadian_A/0/1/0/all/0/1\">Alireza Samadian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruhs_K/0/1/0/all/0/1\">Kirk Pruhs</a>",
          "description": "Many tasks use data housed in relational databases to train boosted\nregression tree models. In this paper, we give a relational adaptation of the\ngreedy algorithm for training boosted regression trees. For the subproblem of\ncalculating the sum of squared residuals of the dataset, which dominates the\nruntime of the boosting algorithm, we provide a $(1 + \\epsilon)$-approximation\nusing the tensor sketch technique. Employing this approximation within the\nrelational boosted regression trees algorithm leads to learning similar model\nparameters, but with asymptotically better runtime.",
          "link": "http://arxiv.org/abs/2107.12373",
          "publishedOn": "2021-07-28T02:02:33.124Z",
          "wordCount": 513,
          "title": "Relational Boosted Regression Trees. (arXiv:2107.12373v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_P/0/1/0/all/0/1\">P.S. Sastry</a>",
          "description": "Deep Neural Networks (DNNs) have been shown to be susceptible to memorization\nor overfitting in the presence of noisily labelled data. For the problem of\nrobust learning under such noisy data, several algorithms have been proposed. A\nprominent class of algorithms rely on sample selection strategies, motivated by\ncurriculum learning. For example, many algorithms use the `small loss trick'\nwherein a fraction of samples with loss values below a certain threshold are\nselected for training. These algorithms are sensitive to such thresholds, and\nit is difficult to fix or learn these thresholds. Often, these algorithms also\nrequire information such as label noise rates which are typically unavailable\nin practice. In this paper, we propose a data-dependent, adaptive sample\nselection strategy that relies only on batch statistics of a given mini-batch\nto provide robustness against label noise. The algorithm does not have any\nadditional hyperparameters for sample selection, does not need any information\non noise rates, and does not need access to separate data with clean labels. We\nempirically demonstrate the effectiveness of our algorithm on benchmark\ndatasets.",
          "link": "http://arxiv.org/abs/2106.15292",
          "publishedOn": "2021-07-28T02:02:33.117Z",
          "wordCount": 641,
          "title": "Adaptive Sample Selection for Robust Learning under Label Noise. (arXiv:2106.15292v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12506",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Halina_E/0/1/0/all/0/1\">Emily Halina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1\">Matthew Guzdial</a>",
          "description": "Generating rhythm game charts from songs via machine learning has been a\nproblem of increasing interest in recent years. However, all existing systems\nstruggle to replicate human-like patterning: the placement of game objects in\nrelation to each other to form congruent patterns based on events in the song.\nPatterning is a key identifier of high quality rhythm game content, seen as a\nnecessary component in human rankings. We establish a new approach for chart\ngeneration that produces charts with more congruent, human-like patterning than\nseen in prior work.",
          "link": "http://arxiv.org/abs/2107.12506",
          "publishedOn": "2021-07-28T02:02:33.105Z",
          "wordCount": 547,
          "title": "TaikoNation: Patterning-focused Chart Generation for Rhythm Action Games. (arXiv:2107.12506v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12455",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aouali_I/0/1/0/all/0/1\">Imad Aouali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_S/0/1/0/all/0/1\">Sergey Ivanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gartrell_M/0/1/0/all/0/1\">Mike Gartrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohde_D/0/1/0/all/0/1\">David Rohde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasile_F/0/1/0/all/0/1\">Flavian Vasile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaytsev_V/0/1/0/all/0/1\">Victor Zaytsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Legrand_D/0/1/0/all/0/1\">Diego Legrand</a>",
          "description": "We consider the problem of slate recommendation, where the recommender system\npresents a user with a collection or slate composed of K recommended items at\nonce. If the user finds the recommended items appealing then the user may click\nand the recommender system receives some feedback. Two pieces of information\nare available to the recommender system: was the slate clicked? (the reward),\nand if the slate was clicked, which item was clicked? (rank). In this paper, we\nformulate several Bayesian models that incorporate the reward signal (Reward\nmodel), the rank signal (Rank model), or both (Full model), for\nnon-personalized slate recommendation. In our experiments, we analyze\nperformance gains of the Full model and show that it achieves significantly\nlower error as the number of products in the catalog grows or as the slate size\nincreases.",
          "link": "http://arxiv.org/abs/2107.12455",
          "publishedOn": "2021-07-28T02:02:33.086Z",
          "wordCount": 603,
          "title": "Combining Reward and Rank Signals for Slate Recommendation. (arXiv:2107.12455v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peizhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_G/0/1/0/all/0/1\">Gao Cong</a>",
          "description": "Cardinality estimation is a fundamental problem in database systems. To\ncapture the rich joint data distributions of a relational table, most of the\nexisting work either uses data as unsupervised information or uses query\nworkload as supervised information. Very little work has been done to use both\ntypes of information, and cannot fully make use of both types of information to\nlearn the joint data distribution. In this work, we aim to close the gap\nbetween data-driven and query-driven methods by proposing a new unified deep\nautoregressive model, UAE, that learns the joint data distribution from both\nthe data and query workload. First, to enable using the supervised query\ninformation in the deep autoregressive model, we develop differentiable\nprogressive sampling using the Gumbel-Softmax trick. Second, UAE is able to\nutilize both types of information to learn the joint data distribution in a\nsingle model. Comprehensive experimental results demonstrate that UAE achieves\nsingle-digit multiplicative error at tail, better accuracies over\nstate-of-the-art methods, and is both space and time efficient.",
          "link": "http://arxiv.org/abs/2107.12295",
          "publishedOn": "2021-07-28T02:02:33.078Z",
          "wordCount": 615,
          "title": "A Unified Deep Model of Learning from both Data and Queries for Cardinality Estimation. (arXiv:2107.12295v1 [cs.DB] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Varela_J/0/1/0/all/0/1\">Jos&#xe9; Su&#xe1;rez-Varela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferriol_Galmes_M/0/1/0/all/0/1\">Miquel Ferriol-Galm&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1\">Albert L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almasan_P/0/1/0/all/0/1\">Paul Almasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardez_G/0/1/0/all/0/1\">Guillermo Bern&#xe1;rdez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujol_Perich_D/0/1/0/all/0/1\">David Pujol-Perich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusek_K/0/1/0/all/0/1\">Krzysztof Rusek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonniot_L/0/1/0/all/0/1\">Lo&#xef;ck Bonniot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_C/0/1/0/all/0/1\">Christoph Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnitzler_F/0/1/0/all/0/1\">Fran&#xe7;ois Schnitzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taiani_F/0/1/0/all/0/1\">Fran&#xe7;ois Ta&#xef;ani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Happ_M/0/1/0/all/0/1\">Martin Happ</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_C/0/1/0/all/0/1\">Christian Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jia Lei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herlich_M/0/1/0/all/0/1\">Matthias Herlich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorfinger_P/0/1/0/all/0/1\">Peter Dorfinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hainke_N/0/1/0/all/0/1\">Nick Vincent Hainke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venz_S/0/1/0/all/0/1\">Stefan Venz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegener_J/0/1/0/all/0/1\">Johannes Wegener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wissing_H/0/1/0/all/0/1\">Henrike Wissing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shihan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barlet_Ros_P/0/1/0/all/0/1\">Pere Barlet-Ros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabellos_Aparicio_A/0/1/0/all/0/1\">Albert Cabellos-Aparicio</a>",
          "description": "During the last decade, Machine Learning (ML) has increasingly become a hot\ntopic in the field of Computer Networks and is expected to be gradually adopted\nfor a plethora of control, monitoring and management tasks in real-world\ndeployments. This poses the need to count on new generations of students,\nresearchers and practitioners with a solid background in ML applied to\nnetworks. During 2020, the International Telecommunication Union (ITU) has\norganized the \"ITU AI/ML in 5G challenge'', an open global competition that has\nintroduced to a broad audience some of the current main challenges in ML for\nnetworks. This large-scale initiative has gathered 23 different challenges\nproposed by network operators, equipment manufacturers and academia, and has\nattracted a total of 1300+ participants from 60+ countries. This paper narrates\nour experience organizing one of the proposed challenges: the \"Graph Neural\nNetworking Challenge 2020''. We describe the problem presented to participants,\nthe tools and resources provided, some organization aspects and participation\nstatistics, an outline of the top-3 awarded solutions, and a summary with some\nlessons learned during all this journey. As a result, this challenge leaves a\ncurated set of educational resources openly available to anyone interested in\nthe topic.",
          "link": "http://arxiv.org/abs/2107.12433",
          "publishedOn": "2021-07-28T02:02:33.071Z",
          "wordCount": 712,
          "title": "The Graph Neural Networking Challenge: A Worldwide Competition for Education in AI/ML for Networks. (arXiv:2107.12433v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2010.02871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ponomarev_E/0/1/0/all/0/1\">Evgeny Ponomarev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matveev_S/0/1/0/all/0/1\">Sergey Matveev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1\">Ivan Oseledets</a>",
          "description": "A lot of deep learning applications are desired to be run on mobile devices.\nBoth accuracy and inference time are meaningful for a lot of them. While the\nnumber of FLOPs is usually used as a proxy for neural network latency, it may\nbe not the best choice. In order to obtain a better approximation of latency,\nresearch community uses look-up tables of all possible layers for latency\ncalculation for the final prediction of the inference on mobile CPU. It\nrequires only a small number of experiments. Unfortunately, on mobile GPU this\nmethod is not applicable in a straight-forward way and shows low precision. In\nthis work, we consider latency approximation on mobile GPU as a data and\nhardware-specific problem. Our main goal is to construct a convenient latency\nestimation tool for investigation(LETI) of neural network inference and\nbuilding robust and accurate latency prediction models for each specific task.\nTo achieve this goal, we build open-source tools which provide a convenient way\nto conduct massive experiments on different target devices focusing on mobile\nGPU. After evaluation of the dataset, we learn the regression model on\nexperimental data and use it for future latency prediction and analysis. We\nexperimentally demonstrate the applicability of such an approach on a subset of\npopular NAS-Benchmark 101 dataset and also evaluate the most popular neural\nnetwork architectures for two mobile GPUs. As a result, we construct latency\nprediction model with good precision on the target evaluation subset. We\nconsider LETI as a useful tool for neural architecture search or massive\nlatency evaluation. The project is available at https://github.com/leti-ai",
          "link": "http://arxiv.org/abs/2010.02871",
          "publishedOn": "2021-07-28T02:02:33.062Z",
          "wordCount": 758,
          "title": "LETI: Latency Estimation Tool and Investigation of Neural Networks inference on Mobile GPU. (arXiv:2010.02871v2 [cs.PF] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laousy_O/0/1/0/all/0/1\">Othmane Laousy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chassagnon_G/0/1/0/all/0/1\">Guillaume Chassagnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1\">Edouard Oyallon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paragios_N/0/1/0/all/0/1\">Nikos Paragios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revel_M/0/1/0/all/0/1\">Marie-Pierre Revel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>",
          "description": "Sarcopenia is a medical condition characterized by a reduction in muscle mass\nand function. A quantitative diagnosis technique consists of localizing the CT\nslice passing through the middle of the third lumbar area (L3) and segmenting\nmuscles at this level. In this paper, we propose a deep reinforcement learning\nmethod for accurate localization of the L3 CT slice. Our method trains a\nreinforcement learning agent by incentivizing it to discover the right\nposition. Specifically, a Deep Q-Network is trained to find the best policy to\nfollow for this problem. Visualizing the training process shows that the agent\nmimics the scrolling of an experienced radiologist. Extensive experiments\nagainst other state-of-the-art deep learning based methods for L3 localization\nprove the superiority of our technique which performs well even with limited\namount of data and annotations.",
          "link": "http://arxiv.org/abs/2107.12800",
          "publishedOn": "2021-07-28T02:02:33.031Z",
          "wordCount": 577,
          "title": "Deep Reinforcement Learning for L3 Slice Localization in Sarcopenia Assessment. (arXiv:2107.12800v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12040",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Du_X/0/1/0/all/0/1\">Xuefeng Du</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_H/0/1/0/all/0/1\">Haohan Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenxi Zhu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangrui Zeng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chang_Y/0/1/0/all/0/1\">Yi-Wei Chang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>",
          "description": "Motivation: Cryo-Electron Tomography (cryo-ET) is a 3D bioimaging tool that\nvisualizes the structural and spatial organization of macromolecules at a\nnear-native state in single cells, which has broad applications in life\nscience. However, the systematic structural recognition and recovery of\nmacromolecules captured by cryo-ET are difficult due to high structural\ncomplexity and imaging limits. Deep learning based subtomogram classification\nhave played critical roles for such tasks. As supervised approaches, however,\ntheir performance relies on sufficient and laborious annotation on a large\ntraining dataset.\n\nResults: To alleviate this major labeling burden, we proposed a Hybrid Active\nLearning (HAL) framework for querying subtomograms for labelling from a large\nunlabeled subtomogram pool. Firstly, HAL adopts uncertainty sampling to select\nthe subtomograms that have the most uncertain predictions. Moreover, to\nmitigate the sampling bias caused by such strategy, a discriminator is\nintroduced to judge if a certain subtomogram is labeled or unlabeled and\nsubsequently the model queries the subtomogram that have higher probabilities\nto be unlabeled. Additionally, HAL introduces a subset sampling strategy to\nimprove the diversity of the query set, so that the information overlap is\ndecreased between the queried batches and the algorithmic efficiency is\nimproved. Our experiments on subtomogram classification tasks using both\nsimulated and real data demonstrate that we can achieve comparable testing\nperformance (on average only 3% accuracy drop) by using less than 30% of the\nlabeled subtomograms, which shows a very promising result for subtomogram\nclassification task with limited labeling resources.",
          "link": "http://arxiv.org/abs/2102.12040",
          "publishedOn": "2021-07-28T02:02:33.023Z",
          "wordCount": 783,
          "title": "Active Learning to Classify Macromolecular Structures in situ for Less Supervision in Cryo-Electron Tomography. (arXiv:2102.12040v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinyi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>",
          "description": "Federated learning (FL) is an emerging practical framework for effective and\nscalable machine learning among multiple participants, such as end users,\norganizations and companies. However, most existing FL or distributed learning\nframeworks have not well addressed two important issues together: collaborative\nfairness and adversarial robustness (e.g. free-riders and malicious\nparticipants). In conventional FL, all participants receive the global model\n(equal rewards), which might be unfair to the high-contributing participants.\nFurthermore, due to the lack of a safeguard mechanism, free-riders or malicious\nadversaries could game the system to access the global model for free or to\nsabotage it. In this paper, we propose a novel Robust and Fair Federated\nLearning (RFFL) framework to achieve collaborative fairness and adversarial\nrobustness simultaneously via a reputation mechanism. RFFL maintains a\nreputation for each participant by examining their contributions via their\nuploaded gradients (using vector similarity) and thus identifies\nnon-contributing or malicious participants to be removed. Our approach\ndifferentiates itself by not requiring any auxiliary/validation dataset.\nExtensive experiments on benchmark datasets show that RFFL can achieve high\nfairness and is very robust to different types of adversaries while achieving\ncompetitive predictive accuracy.",
          "link": "http://arxiv.org/abs/2011.10464",
          "publishedOn": "2021-07-28T02:02:33.007Z",
          "wordCount": 683,
          "title": "A Reputation Mechanism Is All You Need: Collaborative Fairness and Adversarial Robustness in Federated Learning. (arXiv:2011.10464v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azari_B/0/1/0/all/0/1\">Bahar Azari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdogmus_D/0/1/0/all/0/1\">Deniz Erdogmus</a>",
          "description": "Despite the vast success of standard planar convolutional neural networks,\nthey are not the most efficient choice for analyzing signals that lie on an\narbitrarily curved manifold, such as a cylinder. The problem arises when one\nperforms a planar projection of these signals and inevitably causes them to be\ndistorted or broken where there is valuable information. We propose a\nCircular-symmetric Correlation Layer (CCL) based on the formalism of\nroto-translation equivariant correlation on the continuous group $S^1 \\times\n\\mathbb{R}$, and implement it efficiently using the well-known Fast Fourier\nTransform (FFT) algorithm. We showcase the performance analysis of a general\nnetwork equipped with CCL on various recognition and classification tasks and\ndatasets. The PyTorch package implementation of CCL is provided online.",
          "link": "http://arxiv.org/abs/2107.12480",
          "publishedOn": "2021-07-28T02:02:33.000Z",
          "wordCount": 552,
          "title": "Circular-Symmetric Correlation Layer based on FFT. (arXiv:2107.12480v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12525",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Kang_D/0/1/0/all/0/1\">Daniel Kang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Guibas_J/0/1/0/all/0/1\">John Guibas</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bailis_P/0/1/0/all/0/1\">Peter Bailis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sun_Y/0/1/0/all/0/1\">Yi Sun</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>",
          "description": "Given a dataset $\\mathcal{D}$, we are interested in computing the mean of a\nsubset of $\\mathcal{D}$ which matches a predicate. \\algname leverages\nstratified sampling and proxy models to efficiently compute this statistic\ngiven a sampling budget $N$. In this document, we theoretically analyze\n\\algname and show that the MSE of the estimate decays at rate $O(N_1^{-1} +\nN_2^{-1} + N_1^{1/2}N_2^{-3/2})$, where $N=K \\cdot N_1+N_2$ for some integer\nconstant $K$ and $K \\cdot N_1$ and $N_2$ represent the number of samples used\nin Stage 1 and Stage 2 of \\algname respectively. Hence, if a constant fraction\nof the total sample budget $N$ is allocated to each stage, we will achieve a\nmean squared error of $O(N^{-1})$ which matches the rate of mean squared error\nof the optimal stratified sampling algorithm given a priori knowledge of the\npredicate positive rate and standard deviation per stratum.",
          "link": "http://arxiv.org/abs/2107.12525",
          "publishedOn": "2021-07-28T02:02:32.982Z",
          "wordCount": 591,
          "title": "Proof: Accelerating Approximate Aggregation Queries with Expensive Predicates. (arXiv:2107.12525v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10205",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xiao_Y/0/1/0/all/0/1\">Yongqian Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Q/0/1/0/all/0/1\">QianLi Lin</a>",
          "description": "With the development of end-to-end control based on deep learning, it is\nimportant to study new system modeling techniques to realize dynamics modeling\nwith high-dimensional inputs. In this paper, a novel Koopman-based deep\nconvolutional network, called CKNet, is proposed to identify latent dynamics\nfrom raw pixels. CKNet learns an encoder and decoder to play the role of the\nKoopman eigenfunctions and modes, respectively. The Koopman eigenvalues can be\napproximated by eigenvalues of the learned state transition matrix. The\ndeterministic convolutional Koopman network (DCKNet) and the variational\nconvolutional Koopman network (VCKNet) are proposed to span some subspace for\napproximating the Koopman operator respectively. Because CKNet is trained under\nthe constraints of the Koopman theory, the identified latent dynamics is in a\nlinear form and has good interpretability. Besides, the state transition and\ncontrol matrices are trained as trainable tensors so that the identified\ndynamics is also time-invariant. We also design an auxiliary weight term for\nreducing multi-step linearity and prediction losses. Experiments were conducted\non two offline trained and four online trained nonlinear forced dynamical\nsystems with continuous action spaces in Gym and Mujoco environment\nrespectively, and the results show that identified dynamics are adequate for\napproximating the latent dynamics and generating clear images. Especially for\noffline trained cases, this work confirms CKNet from a novel perspective that\nwe visualize the evolutionary processes of the latent states and the Koopman\neigenfunctions with DCKNet and VCKNet separately to each task based on the same\nepisode and results demonstrate that different approaches learn similar\nfeatures in shapes.",
          "link": "http://arxiv.org/abs/2102.10205",
          "publishedOn": "2021-07-28T02:02:32.974Z",
          "wordCount": 729,
          "title": "CKNet: A Convolutional Neural Network Based on Koopman Operator for Modeling Latent Dynamics from Pixels. (arXiv:2102.10205v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sabo_A/0/1/0/all/0/1\">Andrea Sabo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdizadeh_S/0/1/0/all/0/1\">Sina Mehdizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iaboni_A/0/1/0/all/0/1\">Andrea Iaboni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taati_B/0/1/0/all/0/1\">Babak Taati</a>",
          "description": "Drug-induced parkinsonism affects many older adults with dementia, often\ncausing gait disturbances. New advances in vision-based human pose-estimation\nhave opened possibilities for frequent and unobtrusive analysis of gait in\nresidential settings. This work proposes novel spatial-temporal graph\nconvolutional network (ST-GCN) architectures and training procedures to predict\nclinical scores of parkinsonism in gait from video of individuals with\ndementia. We propose a two-stage training approach consisting of a\nself-supervised pretraining stage that encourages the ST-GCN model to learn\nabout gait patterns before predicting clinical scores in the finetuning stage.\nThe proposed ST-GCN models are evaluated on joint trajectories extracted from\nvideo and are compared against traditional (ordinal, linear, random forest)\nregression models and temporal convolutional network baselines. Three 2D human\npose-estimation libraries (OpenPose, Detectron, AlphaPose) and the Microsoft\nKinect (2D and 3D) are used to extract joint trajectories of 4787 natural\nwalking bouts from 53 older adults with dementia. A subset of 399 walks from 14\nparticipants is annotated with scores of parkinsonism severity on the gait\ncriteria of the Unified Parkinson's Disease Rating Scale (UPDRS) and the\nSimpson-Angus Scale (SAS). Our results demonstrate that ST-GCN models operating\non 3D joint trajectories extracted from the Kinect consistently outperform all\nother models and feature sets. Prediction of parkinsonism scores in natural\nwalking bouts of unseen participants remains a challenging task, with the best\nmodels achieving macro-averaged F1-scores of 0.53 +/- 0.03 and 0.40 +/- 0.02\nfor UPDRS-gait and SAS-gait, respectively. Pre-trained model and demo code for\nthis work is available:\nhttps://github.com/TaatiTeam/stgcn_parkinsonism_prediction.",
          "link": "http://arxiv.org/abs/2105.03464",
          "publishedOn": "2021-07-28T02:02:32.967Z",
          "wordCount": 723,
          "title": "Estimating Parkinsonism Severity in Natural Gait Videos of Older Adults with Dementia. (arXiv:2105.03464v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rukundo_O/0/1/0/all/0/1\">Olivier Rukundo</a>",
          "description": "This paper presents the evaluation of effects of image size on deep learning\nperformance via semantic segmentation of magnetic resonance heart images with\nU-net for fully automated quantification of myocardial infarction. Both\nnon-extra pixel and extra pixel interpolation algorithms are used to change the\nsize of images in datasets of interest. Extra class labels, in interpolated\nground truth segmentation images, are removed using thresholding, median\nfiltering, and subtraction strategies. Common class metrics are used to\nevaluate the quality of semantic segmentation with U-net against the ground\ntruth segmentation while arbitrary threshold, comparison of the sums, and sums\nof differences between medical experts and fully automated results are options\nused to estimate the relationship between medical experts-based quantification\nand fully automated quantification results.",
          "link": "http://arxiv.org/abs/2101.11508",
          "publishedOn": "2021-07-28T02:02:32.957Z",
          "wordCount": 592,
          "title": "Effects of Image Size on Deep Learning. (arXiv:2101.11508v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12438",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Gupta_V/0/1/0/all/0/1\">Vishal Gupta</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_M/0/1/0/all/0/1\">Michael Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rusmevichientong_P/0/1/0/all/0/1\">Paat Rusmevichientong</a>",
          "description": "Motivated by the poor performance of cross-validation in settings where data\nare scarce, we propose a novel estimator of the out-of-sample performance of a\npolicy in data-driven optimization.Our approach exploits the optimization\nproblem's sensitivity analysis to estimate the gradient of the optimal\nobjective value with respect to the amount of noise in the data and uses the\nestimated gradient to debias the policy's in-sample performance. Unlike\ncross-validation techniques, our approach avoids sacrificing data for a test\nset, utilizes all data when training and, hence, is well-suited to settings\nwhere data are scarce. We prove bounds on the bias and variance of our\nestimator for optimization problems with uncertain linear objectives but known,\npotentially non-convex, feasible regions. For more specialized optimization\nproblems where the feasible region is ``weakly-coupled\" in a certain sense, we\nprove stronger results. Specifically, we provide explicit high-probability\nbounds on the error of our estimator that hold uniformly over a policy class\nand depends on the problem's dimension and policy class's complexity. Our\nbounds show that under mild conditions, the error of our estimator vanishes as\nthe dimension of the optimization problem grows, even if the amount of\navailable data remains small and constant. Said differently, we prove our\nestimator performs well in the small-data, large-scale regime. Finally, we\nnumerically compare our proposed method to state-of-the-art approaches through\na case-study on dispatching emergency medical response services using real\ndata. Our method provides more accurate estimates of out-of-sample performance\nand learns better-performing policies.",
          "link": "http://arxiv.org/abs/2107.12438",
          "publishedOn": "2021-07-28T02:02:32.948Z",
          "wordCount": 683,
          "title": "Debiasing In-Sample Policy Performance for Small-Data, Large-Scale Optimization. (arXiv:2107.12438v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_R/0/1/0/all/0/1\">Raz Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_Y/0/1/0/all/0/1\">Yuval Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_K/0/1/0/all/0/1\">Kobi Cohen</a>",
          "description": "We consider a distributed learning problem in a wireless network, consisting\nof N distributed edge devices and a parameter server (PS). The objective\nfunction is a sum of the edge devices' local loss functions, who aim to train a\nshared model by communicating with the PS over multiple access channels (MAC).\nThis problem has attracted a growing interest in distributed sensing systems,\nand more recently in federated learning, known as over-the-air computation. In\nthis paper, we develop a novel Accelerated Gradient-descent Multiple Access\n(AGMA) algorithm that uses momentum-based gradient signals over noisy fading\nMAC to improve the convergence rate as compared to existing methods.\nFurthermore, AGMA does not require power control or beamforming to cancel the\nfading effect, which simplifies the implementation complexity. We analyze AGMA\ntheoretically, and establish a finite-sample bound of the error for both convex\nand strongly convex loss functions with Lipschitz gradient. For the strongly\nconvex case, we show that AGMA approaches the best-known linear convergence\nrate as the network increases. For the convex case, we show that AGMA\nsignificantly improves the sub-linear convergence rate as compared to existing\nmethods. Finally, we present simulation results using real datasets that\ndemonstrate better performance by AGMA.",
          "link": "http://arxiv.org/abs/2107.12452",
          "publishedOn": "2021-07-28T02:02:32.929Z",
          "wordCount": 638,
          "title": "Accelerated Gradient Descent Learning over Multiple Access Fading Channels. (arXiv:2107.12452v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12395",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Kahlhoefer_F/0/1/0/all/0/1\">Felix Kahlhoefer</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Korsmeier_M/0/1/0/all/0/1\">Michael Korsmeier</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kramer_M/0/1/0/all/0/1\">Michael Kr&#xe4;mer</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Manconi_S/0/1/0/all/0/1\">Silvia Manconi</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Nippel_K/0/1/0/all/0/1\">Kathrin Nippel</a>",
          "description": "The interpretation of data from indirect detection experiments searching for\ndark matter annihilations requires computationally expensive simulations of\ncosmic-ray propagation. In this work we present a new method based on Recurrent\nNeural Networks that significantly accelerates simulations of secondary and\ndark matter Galactic cosmic ray antiprotons while achieving excellent accuracy.\nThis approach allows for an efficient profiling or marginalisation over the\nnuisance parameters of a cosmic ray propagation model in order to perform\nparameter scans for a wide range of dark matter models. We identify importance\nsampling as particularly suitable for ensuring that the network is only\nevaluated in well-trained parameter regions. We present resulting constraints\nusing the most recent AMS-02 antiproton data on several models of Weakly\nInteracting Massive Particles. The fully trained networks are released as\nDarkRayNet together with this work and achieve a speed-up of the runtime by at\nleast two orders of magnitude compared to conventional approaches.",
          "link": "http://arxiv.org/abs/2107.12395",
          "publishedOn": "2021-07-28T02:02:32.922Z",
          "wordCount": 608,
          "title": "Constraining dark matter annihilation with cosmic ray antiprotons using neural networks. (arXiv:2107.12395v1 [astro-ph.HE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12698",
          "author": "<a href=\"http://arxiv.org/find/gr-qc/1/au:+Moreno_E/0/1/0/all/0/1\">Eric A. Moreno</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Vlimant_J/0/1/0/all/0/1\">Jean-Roch Vlimant</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Spiropulu_M/0/1/0/all/0/1\">Maria Spiropulu</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Borzyszkowski_B/0/1/0/all/0/1\">Bartlomiej Borzyszkowski</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Pierini_M/0/1/0/all/0/1\">Maurizio Pierini</a>",
          "description": "We present an application of anomaly detection techniques based on deep\nrecurrent autoencoders to the problem of detecting gravitational wave signals\nin laser interferometers. Trained on noise data, this class of algorithms could\ndetect signals using an unsupervised strategy, i.e., without targeting a\nspecific kind of source. We develop a custom architecture to analyze the data\nfrom two interferometers. We compare the obtained performance to that obtained\nwith other autoencoder architectures and with a convolutional classifier. The\nunsupervised nature of the proposed strategy comes with a cost in terms of\naccuracy, when compared to more traditional supervised techniques. On the other\nhand, there is a qualitative gain in generalizing the experimental sensitivity\nbeyond the ensemble of pre-computed signal templates. The recurrent autoencoder\noutperforms other autoencoders based on different architectures. The class of\nrecurrent autoencoders presented in this paper could complement the search\nstrategy employed for gravitational wave detection and extend the reach of the\nongoing detection campaigns.",
          "link": "http://arxiv.org/abs/2107.12698",
          "publishedOn": "2021-07-28T02:02:32.904Z",
          "wordCount": 621,
          "title": "Source-Agnostic Gravitational-Wave Detection with Recurrent Autoencoders. (arXiv:2107.12698v1 [gr-qc])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12375",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Atz_K/0/1/0/all/0/1\">Kenneth Atz</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Grisoni_F/0/1/0/all/0/1\">Francesca Grisoni</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schneider_G/0/1/0/all/0/1\">Gisbert Schneider</a>",
          "description": "Geometric deep learning (GDL), which is based on neural network architectures\nthat incorporate and process symmetry information, has emerged as a recent\nparadigm in artificial intelligence. GDL bears particular promise in molecular\nmodeling applications, in which various molecular representations with\ndifferent symmetry properties and levels of abstraction exist. This review\nprovides a structured and harmonized overview of molecular GDL, highlighting\nits applications in drug discovery, chemical synthesis prediction, and quantum\nchemistry. Emphasis is placed on the relevance of the learned molecular\nfeatures and their complementarity to well-established molecular descriptors.\nThis review provides an overview of current challenges and opportunities, and\npresents a forecast of the future of GDL for molecular sciences.",
          "link": "http://arxiv.org/abs/2107.12375",
          "publishedOn": "2021-07-28T02:02:32.889Z",
          "wordCount": 546,
          "title": "Geometric Deep Learning on Molecular Representations. (arXiv:2107.12375v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12915",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Cho_I/0/1/0/all/0/1\">In Ho Cho</a>",
          "description": "Although researchers accumulated knowledge about seismogenesis and\ndecades-long earthquake data, predicting imminent individual earthquakes at a\nspecific time and location remains a long-standing enigma. This study\nhypothesizes that the observed data conceal the hidden rules which may be\nunraveled by a novel glass-box (as opposed to black-box) physics rule learner\n(GPRL) framework. Without any predefined earthquake-related mechanisms or\nstatistical laws, GPRL's two essentials, convolved information index and\ntransparent link function, seek generic expressions of rules directly from\ndata. GPRL's training with 10-years data appears to identify plausible rules,\nsuggesting a combination of the pseudo power and the pseudo vorticity of\nreleased energy in the lithosphere. Independent feasibility test supports the\npromising role of the unraveled rules in predicting earthquakes' magnitudes and\ntheir specific locations. The identified rules and GPRL are in their infancy\nrequiring substantial improvement. Still, this study hints at the existence of\nthe data-guided hidden pathway to imminent individual earthquake prediction.",
          "link": "http://arxiv.org/abs/2107.12915",
          "publishedOn": "2021-07-28T02:02:32.864Z",
          "wordCount": 600,
          "title": "Initial Foundation for Predicting Individual Earthquake's Location and Magnitude by Using Glass-Box Physics Rule Learner. (arXiv:2107.12915v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12723",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Richards_D/0/1/0/all/0/1\">Dominic Richards</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kuzborskij_I/0/1/0/all/0/1\">Ilja Kuzborskij</a>",
          "description": "We revisit on-average algorithmic stability of Gradient Descent (GD) for\ntraining overparameterised shallow neural networks and prove new generalisation\nand excess risk bounds without the Neural Tangent Kernel (NTK) or\nPolyak-{\\L}ojasiewicz (PL) assumptions. In particular, we show oracle type\nbounds which reveal that the generalisation and excess risk of GD is controlled\nby an interpolating network with the shortest GD path from initialisation (in a\nsense, an interpolating network with the smallest relative norm). While this\nwas known for kernelised interpolants, our proof applies directly to networks\ntrained by GD without intermediate kernelisation. At the same time, by relaxing\noracle inequalities developed here we recover existing NTK-based risk bounds in\na straightforward way, which demonstrates that our analysis is tighter.\nFinally, unlike most of the NTK-based analyses we focus on regression with\nlabel noise and show that GD with early stopping is consistent.",
          "link": "http://arxiv.org/abs/2107.12723",
          "publishedOn": "2021-07-28T02:02:32.857Z",
          "wordCount": 593,
          "title": "Stability & Generalisation of Gradient Descent for Shallow Neural Networks without the Neural Tangent Kernel. (arXiv:2107.12723v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bobadilla_J/0/1/0/all/0/1\">Jes&#xfa;s Bobadilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_F/0/1/0/all/0/1\">Fernando Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_A/0/1/0/all/0/1\">Abraham Guti&#xe9;rrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Prieto_A/0/1/0/all/0/1\">&#xc1;ngel Gonz&#xe1;lez-Prieto</a>",
          "description": "Deep learning provides accurate collaborative filtering models to improve\nrecommender system results. Deep matrix factorization and their related\ncollaborative neural networks are the state-of-art in the field; nevertheless,\nboth models lack the necessary stochasticity to create the robust, continuous,\nand structured latent spaces that variational autoencoders exhibit. On the\nother hand, data augmentation through variational autoencoder does not provide\naccurate results in the collaborative filtering field due to the high sparsity\nof recommender systems. Our proposed models apply the variational concept to\ninject stochasticity in the latent space of the deep architecture, introducing\nthe variational technique in the neural collaborative filtering field. This\nmethod does not depend on the particular model used to generate the latent\nrepresentation. In this way, this approach can be applied as a plugin to any\ncurrent and future specific models. The proposed models have been tested using\nfour representative open datasets, three different quality measures, and\nstate-of-art baselines. The results show the superiority of the proposed\napproach in scenarios where the variational enrichment exceeds the injected\nnoise effect. Additionally, a framework is provided to enable the\nreproducibility of the conducted experiments.",
          "link": "http://arxiv.org/abs/2107.12677",
          "publishedOn": "2021-07-28T02:02:32.794Z",
          "wordCount": 637,
          "title": "Deep Variational Models for Collaborative Filtering-based Recommender Systems. (arXiv:2107.12677v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2001.07620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Isufi_E/0/1/0/all/0/1\">Elvin Isufi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gama_F/0/1/0/all/0/1\">Fernando Gama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1\">Alejandro Ribeiro</a>",
          "description": "Driven by the outstanding performance of neural networks in the structured\nEuclidean domain, recent years have seen a surge of interest in developing\nneural networks for graphs and data supported on graphs. The graph is leveraged\nat each layer of the neural network as a parameterization to capture detail at\nthe node level with a reduced number of parameters and computational\ncomplexity. Following this rationale, this paper puts forth a general framework\nthat unifies state-of-the-art graph neural networks (GNNs) through the concept\nof EdgeNet. An EdgeNet is a GNN architecture that allows different nodes to use\ndifferent parameters to weigh the information of different neighbors. By\nextrapolating this strategy to more iterations between neighboring nodes, the\nEdgeNet learns edge- and neighbor-dependent weights to capture local detail.\nThis is a general linear and local operation that a node can perform and\nencompasses under one formulation all existing graph convolutional neural\nnetworks (GCNNs) as well as graph attention networks (GATs). In writing\ndifferent GNN architectures with a common language, EdgeNets highlight specific\narchitecture advantages and limitations, while providing guidelines to improve\ntheir capacity without compromising their local implementation. An interesting\nconclusion is the unification of GCNNs and GATs -- approaches that have been so\nfar perceived as separate. In particular, we show that GATs are GCNNs on a\ngraph that is learned from the features. This particularization opens the doors\nto develop alternative attention mechanisms for improving discriminatory power.",
          "link": "http://arxiv.org/abs/2001.07620",
          "publishedOn": "2021-07-28T02:02:32.769Z",
          "wordCount": 712,
          "title": "EdgeNets:Edge Varying Graph Neural Networks. (arXiv:2001.07620v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1\">Mohit Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "Seemingly simple natural language requests to a robot are generally\nunderspecified, for example \"Can you bring me the wireless mouse?\" When viewing\nmice on the shelf, the number of buttons or presence of a wire may not be\nvisible from certain angles or positions. Flat images of candidate mice may not\nprovide the discriminative information needed for \"wireless\". The world, and\nobjects in it, are not flat images but complex 3D shapes. If a human requests\nan object based on any of its basic properties, such as color, shape, or\ntexture, robots should perform the necessary exploration to accomplish the\ntask. In particular, while substantial effort and progress has been made on\nunderstanding explicitly visual attributes like color and category,\ncomparatively little progress has been made on understanding language about\nshapes and contours. In this work, we introduce a novel reasoning task that\ntargets both visual and non-visual language about 3D objects. Our new\nbenchmark, ShapeNet Annotated with Referring Expressions (SNARE), requires a\nmodel to choose which of two objects is being referenced by a natural language\ndescription. We introduce several CLIP-based models for distinguishing objects\nand demonstrate that while recent advances in jointly modeling vision and\nlanguage are useful for robotic language understanding, it is still the case\nthat these models are weaker at understanding the 3D nature of objects --\nproperties which play a key role in manipulation. In particular, we find that\nadding view estimation to language grounding models improves accuracy on both\nSNARE and when identifying objects referred to in language on a robot platform.",
          "link": "http://arxiv.org/abs/2107.12514",
          "publishedOn": "2021-07-28T02:02:32.743Z",
          "wordCount": 709,
          "title": "Language Grounding with 3D Objects. (arXiv:2107.12514v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stier_J/0/1/0/all/0/1\">Julian Stier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darji_H/0/1/0/all/0/1\">Harshil Darji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granitzer_M/0/1/0/all/0/1\">Michael Granitzer</a>",
          "description": "Sparsity in the structure of Neural Networks can lead to less energy\nconsumption, less memory usage, faster computation times on convenient\nhardware, and automated machine learning. If sparsity gives rise to certain\nkinds of structure, it can explain automatically obtained features during\nlearning.\n\nWe provide insights into experiments in which we show how sparsity can be\nachieved through prior initialization, pruning, and during learning, and answer\nquestions on the relationship between the structure of Neural Networks and\ntheir performance. This includes the first work of inducing priors from network\ntheory into Recurrent Neural Networks and an architectural performance\nprediction during a Neural Architecture Search. Within our experiments, we show\nhow magnitude class blinded pruning achieves 97.5% on MNIST with 80%\ncompression and re-training, which is 0.5 points more than without compression,\nthat magnitude class uniform pruning is significantly inferior to it and how a\ngenetic search enhanced with performance prediction achieves 82.4% on CIFAR10.\nFurther, performance prediction for Recurrent Networks learning the Reber\ngrammar shows an $R^2$ of up to 0.81 given only structural information.",
          "link": "http://arxiv.org/abs/2107.12917",
          "publishedOn": "2021-07-28T02:02:32.718Z",
          "wordCount": 617,
          "title": "Experiments on Properties of Hidden Structures of Sparse Neural Networks. (arXiv:2107.12917v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1905.05976",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Matsuda_T/0/1/0/all/0/1\">Takeru Matsuda</a>, <a href=\"http://arxiv.org/find/math/1/au:+Uehara_M/0/1/0/all/0/1\">Masatoshi Uehara</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hyvarinen_A/0/1/0/all/0/1\">Aapo Hyvarinen</a>",
          "description": "Many statistical models are given in the form of non-normalized densities\nwith an intractable normalization constant. Since maximum likelihood estimation\nis computationally intensive for these models, several estimation methods have\nbeen developed which do not require explicit computation of the normalization\nconstant, such as noise contrastive estimation (NCE) and score matching.\nHowever, model selection methods for general non-normalized models have not\nbeen proposed so far. In this study, we develop information criteria for\nnon-normalized models estimated by NCE or score matching. They are\napproximately unbiased estimators of discrepancy measures for non-normalized\nmodels. Simulation results and applications to real data demonstrate that the\nproposed criteria enable selection of the appropriate non-normalized model in a\ndata-driven manner.",
          "link": "http://arxiv.org/abs/1905.05976",
          "publishedOn": "2021-07-28T02:02:32.710Z",
          "wordCount": 592,
          "title": "Information criteria for non-normalized models. (arXiv:1905.05976v5 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhenfei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haitao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_G/0/1/0/all/0/1\">Guangchun Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Haiwang Zhong</a>",
          "description": "In electricity markets, locational marginal price (LMP) forecasting is\nparticularly important for market participants in making reasonable bidding\nstrategies, managing potential trading risks, and supporting efficient system\nplanning and operation. Unlike existing methods that only consider LMPs'\ntemporal features, this paper tailors a spectral graph convolutional network\n(GCN) to greatly improve the accuracy of short-term LMP forecasting. A\nthree-branch network structure is then designed to match the structure of LMPs'\ncompositions. Such kind of network can extract the spatial-temporal features of\nLMPs, and provide fast and high-quality predictions for all nodes\nsimultaneously. The attention mechanism is also implemented to assign varying\nimportance weights between different nodes and time slots. Case studies based\non the IEEE-118 test system and real-world data from the PJM validate that the\nproposed model outperforms existing forecasting models in accuracy, and\nmaintains a robust performance by avoiding extreme errors.",
          "link": "http://arxiv.org/abs/2107.12794",
          "publishedOn": "2021-07-28T02:02:32.702Z",
          "wordCount": 601,
          "title": "Short-Term Electricity Price Forecasting based on Graph Convolution Network and Attention Mechanism. (arXiv:2107.12794v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Runze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1\">Srikrishna Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1\">Bir Bhanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyan Wu</a>",
          "description": "We consider the problem of estimating frame-level full human body meshes\ngiven a video of a person with natural motion dynamics. While much progress in\nthis field has been in single image-based mesh estimation, there has been a\nrecent uptick in efforts to infer mesh dynamics from video given its role in\nalleviating issues such as depth ambiguity and occlusions. However, a key\nlimitation of existing work is the assumption that all the observed motion\ndynamics can be modeled using one dynamical/recurrent model. While this may\nwork well in cases with relatively simplistic dynamics, inference with\nin-the-wild videos presents many challenges. In particular, it is typically the\ncase that different body parts of a person undergo different dynamics in the\nvideo, e.g., legs may move in a way that may be dynamically different from\nhands (e.g., a person dancing). To address these issues, we present a new\nmethod for video mesh recovery that divides the human mesh into several local\nparts following the standard skeletal model. We then model the dynamics of each\nlocal part with separate recurrent models, with each model conditioned\nappropriately based on the known kinematic structure of the human body. This\nresults in a structure-informed local recurrent learning architecture that can\nbe trained in an end-to-end fashion with available annotations. We conduct a\nvariety of experiments on standard video mesh recovery benchmark datasets such\nas Human3.6M, MPI-INF-3DHP, and 3DPW, demonstrating the efficacy of our design\nof modeling local dynamics as well as establishing state-of-the-art results\nbased on standard evaluation metrics.",
          "link": "http://arxiv.org/abs/2107.12847",
          "publishedOn": "2021-07-28T02:02:32.695Z",
          "wordCount": 708,
          "title": "Learning Local Recurrent Models for Human Mesh Recovery. (arXiv:2107.12847v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.00473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Constantinou_A/0/1/0/all/0/1\">Anthony C. Constantinou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhigao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitson_N/0/1/0/all/0/1\">Neville K. Kitson</a>",
          "description": "Bayesian Networks (BNs) have become a powerful technology for reasoning under\nuncertainty, particularly in areas that require causal assumptions that enable\nus to simulate the effect of intervention. The graphical structure of these\nmodels can be determined by causal knowledge, learnt from data, or a\ncombination of both. While it seems plausible that the best approach in\nconstructing a causal graph involves combining knowledge with machine learning,\nthis approach remains underused in practice. We implement and evaluate 10\nknowledge approaches with application to different case studies and BN\nstructure learning algorithms available in the open-source Bayesys structure\nlearning system. The approaches enable us to specify pre-existing knowledge\nthat can be obtained from heterogeneous sources, to constrain or guide\nstructure learning. Each approach is assessed in terms of structure learning\neffectiveness and efficiency, including graphical accuracy, model fitting,\ncomplexity, and runtime; making this the first paper that provides a\ncomparative evaluation of a wide range of knowledge approaches for BN structure\nlearning. Because the value of knowledge depends on what data are available, we\nillustrate the results both with limited and big data. While the overall\nresults show that knowledge becomes less important with big data due to higher\nlearning accuracy rendering knowledge less important, some of the knowledge\napproaches are actually found to be more important with big data. Amongst the\nmain conclusions is the observation that reduced search space obtained from\nknowledge does not always imply reduced computational complexity, perhaps\nbecause the relationships implied by the data and knowledge are in tension.",
          "link": "http://arxiv.org/abs/2102.00473",
          "publishedOn": "2021-07-28T02:02:32.688Z",
          "wordCount": 719,
          "title": "Information fusion between knowledge and data in Bayesian network structure learning. (arXiv:2102.00473v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Squires_E/0/1/0/all/0/1\">Eric Squires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konda_R/0/1/0/all/0/1\">Rohit Konda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coogan_S/0/1/0/all/0/1\">Samuel Coogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egerstedt_M/0/1/0/all/0/1\">Magnus Egerstedt</a>",
          "description": "This paper demonstrates that in some cases the safety override arising from\nthe use of a barrier function can be needlessly restrictive. In particular, we\nexamine the case of fixed wing collision avoidance and show that when using a\nbarrier function, there are cases where two fixed wing aircraft can come closer\nto colliding than if there were no barrier function at all. In addition, we\nconstruct cases where the barrier function labels the system as unsafe even\nwhen the vehicles start arbitrarily far apart. In other words, the barrier\nfunction ensures safety but with unnecessary costs to performance. We therefore\nintroduce model free barrier functions which take a data driven approach to\ncreating a barrier function. We demonstrate the effectiveness of model free\nbarrier functions in a collision avoidance simulation of two fixed-wing\naircraft.",
          "link": "http://arxiv.org/abs/2107.12871",
          "publishedOn": "2021-07-28T02:02:32.679Z",
          "wordCount": 565,
          "title": "Model Free Barrier Functions via Implicit Evading Maneuvers. (arXiv:2107.12871v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wimmer_P/0/1/0/all/0/1\">Paul Wimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehnert_J/0/1/0/all/0/1\">Jens Mehnert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1\">Alexandru Condurache</a>",
          "description": "State-of-the-art deep neural network (DNN) pruning techniques, applied\none-shot before training starts, evaluate sparse architectures with the help of\na single criterion -- called pruning score. Pruning weights based on a solitary\nscore works well for some architectures and pruning rates but may also fail for\nother ones. As a common baseline for pruning scores, we introduce the notion of\na generalized synaptic score (GSS). In this work we do not concentrate on a\nsingle pruning criterion, but provide a framework for combining arbitrary GSSs\nto create more powerful pruning strategies. These COmbined Pruning Scores\n(COPS) are obtained by solving a constrained optimization problem. Optimizing\nfor more than one score prevents the sparse network to overly specialize on an\nindividual task, thus COntrols Pruning before training Starts. The\ncombinatorial optimization problem given by COPS is relaxed on a linear program\n(LP). This LP is solved analytically and determines a solution for COPS.\nFurthermore, an algorithm to compute it for two scores numerically is proposed\nand evaluated. Solving COPS in such a way has lower complexity than the best\ngeneral LP solver. In our experiments we compared pruning with COPS against\nstate-of-the-art methods for different network architectures and image\nclassification tasks and obtained improved results.",
          "link": "http://arxiv.org/abs/2107.12673",
          "publishedOn": "2021-07-28T02:02:32.672Z",
          "wordCount": 647,
          "title": "COPS: Controlled Pruning Before Training Starts. (arXiv:2107.12673v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12033",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_H/0/1/0/all/0/1\">Hai V. Nguyen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bui_Thanh_T/0/1/0/all/0/1\">Tan Bui-Thanh</a>",
          "description": "Deep Learning (DL), in particular deep neural networks (DNN), by design is\npurely data-driven and in general does not require physics. This is the\nstrength of DL but also one of its key limitations when applied to science and\nengineering problems in which underlying physical properties (such as\nstability, conservation, and positivity) and desired accuracy need to be\nachieved. DL methods in their original forms are not capable of respecting the\nunderlying mathematical models or achieving desired accuracy even in big-data\nregimes. On the other hand, many data-driven science and engineering problems,\nsuch as inverse problems, typically have limited experimental or observational\ndata, and DL would overfit the data in this case. Leveraging information\nencoded in the underlying mathematical models, we argue, not only compensates\nmissing information in low data regimes but also provides opportunities to\nequip DL methods with the underlying physics and hence obtaining higher\naccuracy. This short communication introduces several model-constrained DL\napproaches (including both feed-forward DNN and autoencoders) that are capable\nof learning not only information hidden in the training data but also in the\nunderlying mathematical models to solve inverse problems. We present and\nprovide intuitions for our formulations for general nonlinear problems. For\nlinear inverse problems and linear networks, the first order optimality\nconditions show that our model-constrained DL approaches can learn information\nencoded in the underlying mathematical models, and thus can produce consistent\nor equivalent inverse solutions, while naive purely data-based counterparts\ncannot.",
          "link": "http://arxiv.org/abs/2105.12033",
          "publishedOn": "2021-07-28T02:02:32.631Z",
          "wordCount": 694,
          "title": "Model-Constrained Deep Learning Approaches for Inverse Problems. (arXiv:2105.12033v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varma_K/0/1/0/all/0/1\">Kamala Varma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baracaldo_N/0/1/0/all/0/1\">Nathalie Baracaldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1\">Ali Anwar</a>",
          "description": "Federated learning has arisen as a mechanism to allow multiple participants\nto collaboratively train a model without sharing their data. In these settings,\nparticipants (workers) may not trust each other fully; for instance, a set of\ncompetitors may collaboratively train a machine learning model to detect fraud.\nThe workers provide local gradients that a central server uses to update a\nglobal model. This global model can be corrupted when Byzantine workers send\nmalicious gradients, which necessitates robust methods for aggregating\ngradients that mitigate the adverse effects of Byzantine inputs. Existing\nrobust aggregation algorithms are often computationally expensive and only\neffective under strict assumptions. In this paper, we introduce LayerwisE\nGradient AggregatTiOn (LEGATO), an aggregation algorithm that is, by contrast,\nscalable and generalizable. Informed by a study of layer-specific responses of\ngradients to Byzantine attacks, LEGATO employs a dynamic gradient reweighing\nscheme that is novel in its treatment of gradients based on layer-specific\nrobustness. We show that LEGATO is more computationally efficient than multiple\nstate-of-the-art techniques and more generally robust across a variety of\nattack settings in practice. We also demonstrate LEGATO's benefits for gradient\ndescent convergence in the absence of an attack.",
          "link": "http://arxiv.org/abs/2107.12490",
          "publishedOn": "2021-07-28T02:02:32.510Z",
          "wordCount": 644,
          "title": "LEGATO: A LayerwisE Gradient AggregaTiOn Algorithm for Mitigating Byzantine Attacks in Federated Learning. (arXiv:2107.12490v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maurer_T/0/1/0/all/0/1\">Thomas Maurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1\">Matthew Guzdial</a>",
          "description": "Autonomous game design, generating games algorithmically, has been a longtime\ngoal within the technical games research field. However, existing autonomous\ngame design systems have relied in large part on human-authoring for game\ndesign knowledge, such as fitness functions in search-based methods. In this\npaper, we describe an experiment to attempt to learn a human-like fitness\nfunction for autonomous game design in an adversarial manner. While our\nexperimental work did not meet our expectations, we present an analysis of our\nsystem and results that we hope will be informative to future autonomous game\ndesign research.",
          "link": "http://arxiv.org/abs/2107.12501",
          "publishedOn": "2021-07-28T02:02:32.394Z",
          "wordCount": 558,
          "title": "Adversarial Random Forest Classifier for Automated Game Design. (arXiv:2107.12501v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12460",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rothermel_D/0/1/0/all/0/1\">Danielle Rothermel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Margaret Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1\">Tim Rockt&#xe4;schel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1\">Jakob Foerster</a>",
          "description": "Self-supervised pre-training of large-scale transformer models on text\ncorpora followed by finetuning has achieved state-of-the-art on a number of\nnatural language processing tasks. Recently, Lu et al. (2021, arXiv:2103.05247)\nclaimed that frozen pretrained transformers (FPTs) match or outperform training\nfrom scratch as well as unfrozen (fine-tuned) pretrained transformers in a set\nof transfer tasks to other modalities. In our work, we find that this result\nis, in fact, an artifact of not tuning the learning rates. After carefully\nredesigning the empirical setup, we find that when tuning learning rates\nproperly, pretrained transformers do outperform or match training from scratch\nin all of our tasks, but only as long as the entire model is finetuned. Thus,\nwhile transfer from pretrained language models to other modalities does indeed\nprovide gains and hints at exciting possibilities for future work, properly\ntuning hyperparameters is important for arriving at robust findings.",
          "link": "http://arxiv.org/abs/2107.12460",
          "publishedOn": "2021-07-28T02:02:32.353Z",
          "wordCount": 609,
          "title": "Don't Sweep your Learning Rate under the Rug: A Closer Look at Cross-modal Transfer of Pretrained Transformers. (arXiv:2107.12460v1 [cs.LG])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}